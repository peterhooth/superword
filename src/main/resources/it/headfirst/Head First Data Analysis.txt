It’s about time a straightforward and comprehensive guide to analyzing data was written that makes learning the concepts simple and fun.
It will change the way you think and approach problems using proven techniques and free tools.
Head First Data Analysis does a fantastic job of  giving readers systematic methods to analyze real-world problems.
From coffee, to rubber duckies, to asking for a raise, Head First Data Analysis shows the reader how to find and unlock the power of  data in everyday life.
Buried under mountains of  data? Let Michael Milton be your guide as you fill your toolbox with the analytical skills that give you an edge.
In Head First Data Analysis, you’ll learn how to turn raw numbers into real knowledge.
Kathy and Bert’s Head First Java transforms the printed page into the closest thing to a GUI you’ve ever seen.
Just the right tone for the geeked-out, casual-cool guru coder in all of  us.
There are books you buy, books you keep, books you keep on your desk, and thanks to O’Reilly and the Head First crew, there is the ultimate category, Head First books.
They’re the ones that are dog-eared, mangled, and carried everywhere.
Head First SQL is at the top of  my stack.
Usually when reading through a book or article on design patterns, I’d have to occasionally stick myself in the eye with something just to make sure I was paying attention.
Odd as it may sound, this book makes learning about design patterns fun.
Wouldn’t it be dreamy if there was a book on data analysis that wasn’t just a glorified printout of.
Microsoft Excel help files? But it’s probably just a fantasy...
O’Reilly Media books may be purchased for educational, business, or sales promotional use.
Many of  the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of  a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of  this book, the publisher and the authors assume no responsibility for errors or omissions, or for damages resulting from the use of  the information contained herein.
No data was harmed in the making of  this book.
Dedicated to the memory of  my grandmother, Jane Reese Gibbs.
Michael Milton has spent most of his career helping nonprofit organizations improve their fundraising by interpreting and acting on the data they collect from their donors.
He has a degree in philosophy from New College of  Florida and one in religious ethics from Yale University.
He found reading Head First to be a revelation after spending years reading boring books filled with terribly important stuff  and is grateful to have the opportunity to write an exciting book filled with terribly important stuff.
When he’s not in the library or the bookstore, you can find him running, taking pictures, and brewing beer.
Here you are trying to learn something, while here your brain is doing you a favor by making sure the learning doesn’t stick.
Nowadays, everyone has to deal with mounds of data, whether they call.
Test your theories Can you show what you believe? In a real empirical test? There’s nothing like a good experiment to solve your problems.
Strong empirical data will make your analytical judgments all the more.
And we’re always trying to figure out how to get it.
If the things we want more ofprofit, money, efficiency, speed—can be represented numerically, then chances.
In this chapter, you’ll be using one of those tools and the powerful.
Pictures make you smarter You need more than a table of  numbers.
Your data is brilliantly complex, with more variables than you can shake a stick at.
Mulling over mounds and mounds of spreadsheets isn’t just boring; it can actually be a.
A clear, highly multivariate visualization can, in a small space, show.
Get past first base You’ll always be collecting new data.
And you need to make sure that every analysis you do incorporates the data you have.
But only if those numbers describe your own mental states, expressing.
Subjective probability is a straightforward way of injecting some real.
Analyze like a humanThe real world has more variables than you can handle.
There is always going to be data that you can’t have.
And even when you do have data on most of the things you want to understand, optimizing methods are often elusive.
Fortunately, most of the actual thinking you do in life is not.
How much can a bar graph tell you? There are about a zillion ways of showing data with pictures, but one of them is.
Histograms, which are kind of similar to bar graphs, are a super-fast and.
And you’re about to do it with a new, free, crazypowerful software tool.
Regression is an incredibly powerful statistical tool that, when used correctly, has the.
So it should be no surprise that your predictions rarely hit the target squarely.
Every time you express error, you offer a much richer perspective.
And with the tools in this chapter, you’ll also learn.
How do you structure really, really multivariate data? A spreadsheet has only two dimensions: rows and columns.
With a clear vision of how you need it to look.
But data analysis is a vast and constantly evolving field, and there’s so much left the.
In this appendix, we’ll go over ten items that there wasn’t enough room to cover.
But fortunately, getting R installed and started is something you can accomplish in.
The ToolPak Some of  the best features of  Excel aren’t installed by default.
That’s right, in order to run the optimization from Chapter 3 and the histograms from.
Chapter 9, you need to activate the Solver and the Analysis ToolPak, two extensions.
I can’t believe they put that in a data analysis book.
Is this book for you? This book is for anyone with the money to pay for it.
And it makes a great gift for that special someone.
Do you prefer stimulating dinner party conversation to dry, dull, academic lectures?
Do you want to learn, understand, and remember how to create brilliant graphics, test hypotheses, run a regression, or clean up messy data?
Have you never loaded and used Microsoft Excel or OpenOffice calc?
Do you feel like there’s a world of insights buried in your data that you’d only be able to access if you had the right tools?
Are you a seasoned, brilliant data analyst looking for a survey of bleeding edge data topics?
Are you afraid to try something different? Would you rather have a root canal than mix stripes with plaid? Do you believe that a technical book can’t be serious if it anthropomorphizes control groups and objective functions?
It was built that way, and it helps you stay alive.
So what does your brain do with all the routine, ordinary, normal things you encounter? Everything it can to stop them from interfering with the brain’s real job—recording things that matter.
It doesn’t bother saving the boring things; they never make it past the “this is obviously not important” filter.
How does your brain know what’s important? Suppose you’re out for a day hike and a tiger jumps in front of  you, what happens inside your head and body?
This must be important! Don’t forget it! But imagine you’re at home, or in a library.
Or trying to learn some tough technical topic your boss thinks will take a week, ten days at the most.
It’s trying to make sure that this obviously non-important content doesn’t clutter up scarce resources.
Resources that are better spent storing the really big things.
Like how you should never have posted those “party” photos on your Facebook page.
And there’s no simple way to tell your brain, “Hey brain, thank you very much, but no matter how dull this book is, and how little I’m registering on the emotional Richter scale right now, I really do want you to keep this stuff  around.”
We think of a “Head First” reade r as a learner.
If  you really want to learn, and you want to learn more quickly and more deeply, pay attention to how you pay attention.
Most of  us did not take courses on metacognition or learning theory when we were growing up.
We were expected to learn, but rarely taught to learn.
But we assume that if  you’re holding this book, you really want to learn data analysis.
And you probably don’t want to spend a lot of  time.
If  you want to use what you read in this book, you need to remember what you read.
To get the most from this book, or any book or learning experience, take responsibility for your brain.
The trick is to get your brain to see the new material you’re learning as Really Important.
Otherwise, you’re in for a constant battle, with your brain doing its best to keep the new content from sticking.
So just how DO you get your brain to treat data analysis like it was a hungry tiger? There’s the slow, tedious way, or the faster, more effective way.
You obviously know that you are able to learn and remember even the dullest of  topics if  you keep pounding the same thing into your brain.
With enough repetition, your brain says, “This doesn’t feel important to him, but he keeps looking at the same thing over and over and over, so I suppose it must be.”
The faster way is to do anything that increases brain activity, especially different types of  brain activity.
The things on the previous page are a big part of  the solution, and they’re all things that have been proven to help your brain work in your favor.
For example, studies show that putting words within the pictures they describe (as opposed to somewhere else in the page, like a caption or in the body text) causes your brain to try to makes sense of  how the words and picture relate, and this causes more neurons to fire.
More neurons firing = more chances for your brain to get that this is something worth paying attention to, and possibly recording.
A conversational style helps because people tend to pay more attention when they perceive that they’re in a conversation, since they’re expected to follow along and hold up their end.
The amazing thing is, your brain doesn’t necessarily care that the “conversation” is between you and a book! On the other hand, if  the writing style is formal and dry, your brain perceives it the same way you experience being lectured to while sitting in a roomful of  passive attendees.
Here’s what WE did: We used pictures, because your brain is tuned for visuals, not text.
As far as your brain’s concerned, a picture really is worth a thousand words.
And when text and pictures work together, we embedded the text in the pictures because your brain works more effectively when the text is within the thing the text refers to, as opposed to in a caption or buried in the text somewhere.
We used redundancy, saying the same thing in different ways and with different media types, and multiple senses, to increase the chance that the content gets coded into more than one area of  your brain.
We used concepts and pictures in unexpected ways because your brain is tuned for novelty, and we used pictures and ideas with at least some emotional content, because your brain is tuned to pay attention to the biochemistry of  emotions.
That which causes you to feel something is more likely to be remembered, even if  that feeling is nothing more than a little humor, surprise, or interest.
We used a personalized, conversational style, because your brain is tuned to pay more attention when it believes you’re in a conversation than if  it thinks you’re passively listening to a presentation.
We included more than 80 activities, because your brain is tuned to learn and remember more when you do things than when you read about things.
We used multiple learning styles, because you might prefer step-by-step procedures, while someone else wants to understand the big picture first, and someone else just wants to see an example.
But regardless of  your own learning preference, everyone benefits from seeing the same content represented in multiple ways.
We include content for both sides of  your brain, because the more of  your brain you engage, the more likely you are to learn and remember, and the longer you can stay focused.
Since working one side of  the brain often means giving the other side a chance to rest, you can be more productive at learning for a longer period of  time.
And we included stories and exercises that present more than one point of  view, because your brain is tuned to learn more deeply when it’s forced to make evaluations and judgments.
We included challenges, with exercises, and by asking questions that don’t always have a straight answer, because your brain is tuned to learn and remember when it has to work at something.
Think about it—you can’t get your body in shape just by watching people at the gym.
But we did our best to make sure that when you’re working hard, it’s on the right things.
That you’re not spending one extra dendrite processing a hard-to-understand example, or parsing difficult, jargon-laden, or overly terse text.
In stories, examples, pictures, etc., because, well, because you’re a person.
And your brain pays more attention to people than it does to things.
These tips are a starting point; listen to your brain and figure out what works for you and what doesn’t.
Your brain works best in a nice bath of  fluid.
Dehydration (which can happen before you ever feel thirsty) decreases cognitive function.
Get your hands dirty! There’s only one way to learn data analysis: get your hands dirty.
And that’s what you’re going to do throughout this book.
Data analysis is a skill, and the only way to get good at it is to practice.
We’re going to give you a lot of  practice: every chapter has exercises that pose a problem for you to solve.
Don’t just skip over them—a lot of  the learning happens when you solve the exercises.
But try to solve the problem before you look at the solution.
And definitely get it working before you move on to the next part of  the book.
Groaning over a bad joke is still better than feeling nothing at all.
If  you find yourself  starting to skim the surface or forget what you just read, it’s time for a break.
Once you go past a certain point, you won’t learn faster by trying to shove more in, and you might even hurt the process.
If you’re trying to understand something, or increase your chance of  remembering it later, say it out loud.
Better still, try to explain it out loud to someone else.
You’ll learn more quickly, and you might uncover ideas you hadn’t known were there when you were reading about it.
Part of  the learning (especially the transfer to long-term memory) happens after you put the book down.
Your brain needs time on its own, to do more processing.
If  you put in something new during that processing time, some of  what you just learned will be lost.
They’re not optional sidebars, they’re part of  the core content! Don’t skip them.
Here’s what YOU can do to bend your brain into submission.
We put them in, but if  we did them for you, that would be like having someone else do your workouts for you.
There’s plenty of  evidence that physical activity while learning can increase the learning.
When the book asks you a question, don’t just skip to the answer.
The more deeply you force your brain to think, the better chance you have of  learning and remembering.
The more you understand, the less you have to memorize.
We deliberately stripped out everything that might get in the way of  learning whatever it is we’re working on at that point in the book.
And the first time through, you need to begin at the beginning, because the book makes assumptions about what you’ve already seen and learned.
Many books with “data analysis” in their titles simply go down the list of  Excel functions considered to be related to data analysis and show you a few examples of  each.
Head First Data Analysis, on the other hand, is about how to be a data analyst.
You’ll learn quite a bit about software tools in this book, but they are only a means to the end of  learning how to do good data analysis.
We expect you to know how to use basic spreadsheet formulas.
Have you ever used the SUM formula in a spreadsheet? If  not, you may want to bone up on spreadsheets a little before beginning this book.
While many chapters do not ask you to use spreadsheets at all, the ones that do assume that you know how to use formulas.
If  you are familiar with the SUM formula, then you’re in good shape.
There’s plenty of  statistics in this book, and as a data analyst you should learn as much statistics as you can.
Once you’re finished with Head First Data Analysis, it’d be a good idea to read Head First Statistics as well.
But “data analysis” encompasses statistics and a number of  other fields, and the many non-statistical topics chosen for this book are focused on the practical, nitty-gritty experience of  doing data analysis in the real world.
The exercises and activities are not add-ons; they’re part of  the core content of  the book.
Some of  them are to help with memory, some are for understanding, and some will help you apply what you’ve learned.
One distinct difference in a Head First book is that we want you to really get it.
And we want you to finish the book remembering what you’ve learned.
Most reference books don’t have retention and recall as a goal, but this book is about learning, so you’ll see some of  the same concepts come up more than once.
We love it when you can find fun and useful extra stuff  on book companion sites.
You’ll find extra stuff  on data analysis at the following url: http://www.headfirstlabs.com/books/hfda/
For some of  them, there is no right answer, and for others, part of  the learning experience of  the Brain Power activities is for you to decide if  and when your answers are right.
In some of  the Brain Power exercises, you will find hints to point you in the right direction.
During his time as an undergraduate in DC, he worked at the State Department and at the National Economic Council at the White House.
He completed his graduate work in economics at the University of  Chicago.
He can’t wait to run a data analysis on his golf  stats to help him win on the links.
Anthony Rose has been working in the data analysis field for nearly ten years and is currently the president of Support Analytics, a data analysis and visualization consultancy.
Anthony has an MBA concentrated in Management and Finance degree, which is where his passion for data and analysis started.
When he isn’t working, he can normally be found on the golf  course in Columbia, Maryland, lost in a good book, savoring a delightful wine, or simply enjoying time with his young girls and amazing wife.
Working with Brian is like dancing with a professional ballroom dancer.
All sorts of  important stuff  is happening that you don’t really understand, but you look great, and you’re having a blast.
Ours has been a exciting collaboration, and his support, feedback, and ideas have been invaluable.
Brett McLaughlin saw the vision for this project from the beginning, shepherded it through tough times, and has been a constant support.
Brett’s implacable focus on your experience with the Head First books is an inspiration.
Karen Shaner provided logistical support and a good bit of  cheer on some cold Cambridge mornings.
Brittany Smith contributed some cool graphic elements that we used over and over.
Really smart people whose ideas are remixed in this book:
While many of  big ideas taught in this book are unconventional for books with “data analysis” in the title, few of  them are uniquely my own.
Read them all! The idea of  the anti-resume comes from Nassim Taleb’s The Black Swan (if  there’s a Volume 2, expect to see more of  his ideas)
Richards Heuer kindly corresponded with me about the book and gave me a number of  useful ideas.
Lou Barr’s intellectual, moral, logistical, and aesthetic support of  this book is much appreciated.
Aron Edidin sponsored an awesome tutorial for me on intelligence analysis when I was an undergraduate.
My poker group—Paul, Brewster, Matt, Jon, and Jason—has given me an expensive education in the balance of  heuristic and optimizing decision frameworks.
The technical review team did a brilliant job, caught loads of  errors, made a bunch of  good suggestions, and were tremendously supportive.
As I wrote this book, I leaned heavily on my friend Blair Christian, who is a statistician and deep thinker.
Above all, I appreciate the steadfast support of  my wife Julia, who means everything.
It’s a virtual library that lets you easily search thousands of  top tech books, cut and paste code samples, download chapters, and find quick answers when you need the most accurate, current information.
But people who possess a toolbox of data analysis skills have a massive.
I detect ginger, garlic, paprika, and perhaps a hint of fish sauce…
Acme Cosmetics needs your help It’s your first day on the job as a data analyst, and you were just sent this sales data from the CEO to review.
It’s fine not to know everything—just slow down and take a look.
What do you see? How much does the table tell you about Acme’s business? About Acme’s MoisturePlus moisturizer?
What do you think is going on with these unit prices? Why are they going down?
What has been happening during the last six months with sales? How do their gross sales figures.
The CEO wants data analysis to help increase sales He wants you to “give him an analysis.”
It’s kind of  a vague request, isn’t it? It sounds simple, but will your job be that straightforward? Sure, he wants more sales.
Sure, he thinks something in the data will help accomplish that goal.
Take a look at our data and give me an analysis to help us figure out how to increase sales.
Think about what, fundamentally, the CEO is looking for from you with this question.
The expression “data analysis” covers a lot of different activities and a lot of  different skills.
If someone tells you that she’s a data analyst, you still won’t know much about what specifically she knows or does.
But all good analysts, regardless of  their skills or goals, go through this same basic process during the course of  their work, always using empirical evidence to think carefully about problems.
In every chapter of  this book, you’ll go through these steps over and over again, and they’ll become second nature really quickly.
Ultimately, all data analysis is designed to lead to better decisions, and you’re about to learn how to make better decisions by gleaning insights from a sea of  data.
Data analysis is all about breaking problems and data into smaller pieces.
Here’s the meat of the analysis, where you draw your conclusions about what you’ve learned in the first two steps.
Finally, you put it all back together and make (or recommend) a decision.
You might bet that she knows Excel, but that’s about it!
Define the problem Doing data analysis without explicitly defining your problem or goal is like heading out on a road trip without having decided on a destination.
Sure, you might come across some interesting sights, and sometimes you might want to wander around in the hopes you’ll stumble on something cool, but who’s to say you’ll find anything?
Ever seen an “analytical report” that’s a million pages long, with tons and tons of  charts and diagrams?
Every once in a while, an analyst really does need a ream of  paper or an hourlong slide show to make a point.
But in this sort of  case, the analyst often hasn’t focused enough on his problem and is pelting you with information as a way of  ducking his obligation to solve a problem and recommend a decision.
Sometimes, the situation is even worse: the problem isn’t defined at all and the analyst doesn’t want you to realize that he’s just wandering around in the data.
He is the person your analysis is meant to serve.
Your client might be your boss, your company’s CEO, or even yourself.
Your client is the person who will make decisions on the basis of  your analysis.
You need to get as much information as you can from him to define your problem.
You need to understand more specifically what he means in order to craft an analysis that solves the problem.
There’s a bonus in it for you if you can figure out how to increase MoisturePlus sales.
It’s a really good idea to know your client as well as you can.
The better you understand your client, the more likely your analysis will be able to help.
Keep an eye at the bottom of the page during this chapter for these cues, which show you where you are.
The general problem is that we need to increase sales.
What questions would you ask the CEO to understand better what he means specifically? List five.
Do you mean that I need to have some specific goal in mind before I even look at my data?
A: You don’t need to have a problem in mind just to look at data.
But keep in mind that looking by itself is not yet data analysis.
Data analysis is all about identifying problems and then solving them.
Q: I’ve heard about “exploratory data analysis,” where you explore the data for ideas you might want to evaluate further.
There’s no problem definition in that sort of data analysis!
Your problem in exploratory data analysis is to find hypotheses worth testing.
Tell me more about these clients who aren’t well informed about their problems.
Does that kind of person even need a data analyst?
A: Of course! Q: Sounds to me like that kind of person needs professional help.
A: Actually, good data analysts help their clients think through their problem; they don’t just wait around for their clients to tell them what to do.
Your clients will really appreciate it if you can show them that they have problems they didn’t even know about.
Who wants more problems? A: People who hire data analysts recognize that people with analytical skills have the ability to improve their businesses.
Some people see problems as opportunities, and data analysts who show their clients how to exploit opportunity give them a competitive advantage.
I need to get it back in line with our target sales, which you can see on the table.
All our budgeting is built around those targets, and we’ll be in trouble if we miss them.
You’re going to get sales up with marketing of some sort or another.
How much of a sales increase do you think is feasible? Are the target sales figures reasonable?
I don’t think there’s any limit to what we can make off of selling them MoisturePlus.
I don’t have any hard numbers, but my impression is that they are going to leave us in the dust.
I’d say they’re 50–100 percent ahead of us in terms of gross moisturizer revenue.
What’s the deal with the ads and the social networking marketing budget?
I shudder to think what’d be happening if we’d kept ads at the same level.
Here are some sample questions to get the CEO to define your analytical goals.
This email just came through in response to your questions.
Always ask “how much.” Make your goals and beliefs quantitative.
The next step in data analysis is to take what you’ve learned about your problem from your client, along with your data, and break that information down into the level of granularity that will best serve your analysis.
Divide the problem into smaller problems You need to divide your problem into manageable, solvable chunks.
But by answering the smaller problems, which you’ve analyzed out of  the big problem, you can get your answer to the big one.
Divide the data into smaller chunks Same deal with the data.
People aren’t going to present you the precise quantitative answers you need; you’ll need to extract important elements on your own.
If  the data you receive is a summary, like what you’ve received from Acme, you’ll want to know which elements are most important to you.
If  your data comes in a raw form, you’ll want to summarize the elements to make that data more useful.
Here you have a summary of  Acme’s sales data, and the best way to start trying to isolate the most important elements of  it is to find strong comparisons.
Making good comparisons is at the core of data analysis, and you’ll be doing it throughout this book.
In this case, you want to build a conception in your mind of  how Acme’s MoisturePlus business works by comparing their summary statistics.
How do January’s gross sales compare to February’s? How do the gross and target sales figures compare to each other for October?
How are ad and social network costs changing relative to each other over time?
Does the decrease in unit prices coincide with any change in gross sales?
Break down your summary data by searching for interesting comparisons.
You’ve defined the problem: figure out how to increase sales.
But that problem tells you very little about how you’re expected to do it, so you elicited a lot of  useful commentary from the CEO.
This commentary provides an important baseline set of  assumptions about how the cosmetics business works.
Hopefully, the CEO is right about those assumptions, because they will be the backbone of  your analysis! What are the most important points that the CEO makes?
Summarize what your client believes and your thoughts on the data you’ve received to do the analysis.
Analyze the above email and your data into smaller pieces that describe your situation.
I need to get it back in line with our target sales, which you can see on the table.
All our budgeting is built around those targets, and we’ll be in trouble if we miss them.
You’re going to get sales up with marketing of some sort or another.
How much of a sales increase do you think is feasible? Are the target sales figures reasonable?
I don’t think there’s any limit to what we can make off of selling them MoisturePlus.
I don’t have any hard numbers, but my impression is that they are going to leave us in the dust.
I’d say they’re 50–100 percent ahead of us in terms of gross moisturizer revenue.
What’s the deal with the ads and the social networking marketing budget?
I shudder to think what’d be happening if we’d kept ads at the same level.
Which parts of it are most important? What’s most useful?
MoisturePlus customers are tween girls (where tweens are people aged 11–15)
Acme is trying out reallocating expenses from advertisements to social networking, but so far, the success of the initiative is unknown.
We see no limit to potential sales growth among tween girls.3
Sales are slightly up in February compared to September, but kind of flat.
Sales are way off their targets and began diverging in November.
Cutting the prices does not seem to have helped sales keep pace with targets.
Cutting ad expenses may have hurt Acme’s ability to keep pace with sales targets.
You’ve successfully broken your problem into smaller, more manageable pieces.
Now it’s time to evaluate those pieces in greater detail…
Good…  this is the sort of thing one does nowadays.
You have almost all the right pieces, but one important piece is missing…
You know what you need to figure out, and you know what chunks of  data will enable you to do it.
Now, take a close, focused look at the pieces and form your own judgements about them.
Just as it was with disassembly, the key to evaluating the pieces you have isolated is comparison.
What do you see when you compare these elements to each other?
Sales are slightly up in February compared to September, but kind of flat.
Cutting the prices does not seem to have helped sales keep pace with targets.
Cutting ad expenses may have hurt Acme’s ability to keep pace with sales targets.
MoisturePlus customers are tween girls (where tweens are people aged 11–15)
Acme is trying out reallocating expenses from advertisements to social networking, but so far, the success of the initiative is unknown.
We see no limit to potential sales growth among tween girls.
Pick any two elements and read them next to each other.
Analysis begins when you insert yourself Inserting yourself  into your analysis means making your own assumptions explicit and betting your credibility on your conclusions.
Whether you’re building complex models or making simple decisions, data analysis is all about you: your beliefs, your judgement, your credibility.
You’ll lose track of  how your baseline assumptions affect your conclusions.
Your client won’t trust your analysis, because he won’t know your motives and incentives.
Your client might get a false sense of “objectivity” or detached rationality.
As you craft your final report, be sure to refer to yourself, so that your client knows where your conclusions are coming from.
Your prospects for success are much better if you are an explicit part of your analysis.
Look at the information you’ve collected on the previous pages.
What do you recommend that Acme does to increase sales? Why?
As a data analyst, your job is to empower yourself and your client to make better decisions, using insights gleaned from carefully studying your evaluation of  the data.
Making that happen means you have to package your ideas and judgments together into a format that can be digested by your client.
That means making your work as simple as it can be, but not simpler! It’s your job to make sure your voice is heard and that people make good decisions on the basis of  what you have to say.
An analysis is useless unless it’s assembled into a form that facilitates decisions.
The report you present to your client needs to be focused on making yourself understood and encouraging intelligent, data-based decision making.
MoisturePlus customers are tween girls (where tweens are people aged 11–15)
Acme is trying out reallocating expenses from advertisements to social networking, but so far, the success of  the initiative is unknown.
We see no limit to potential sales growth among tween girls.
Sales are slightly up in February compared to September, but kind of flat.
Cutting ad expenses may have hurt Acme’s ability to keep pace with sales targets.
Cutting the prices does not seem to have helped sales keep pace with targets.
It might be that the decline in sales relative to the target is linked to the decline in advertising relative to past advertising expenses.
We have no good evidence to believe that social networking has been as successful as we had hoped.
I will return advertising to September levels to see if  the tween girls respond.
Advertising to tween girls is the way to get gross sales back in line with targets.
It’s a good idea to state your and your clients’ assumptions in your report.
This is the stuff we got from the CEO at the beginning.
It speaks to the CEO’s needs in a way that’s even clearer than his own way of  describing them.
You looked at the data, got greater clarity from the CEO, compared his beliefs to your own interpretation of  his data, and recommended a decision.
On the face of  it, this sounds good for Acme.
But if  the market’s saturated, more ads to tween girls probably won’t do much good.
MoisturePlus achieves complete market saturation among tween girls Our very own cosmetics industry analysts report that the tween girl moisturizer market is completely dominated by Acme Cosmetics's flagship product, MoisturePlus.
According to the DBD's survey, 95 percent of  tween girls report "Very Frequent" usage of  MoisturePlus, typically twice a day or more.
The Acme CEO was surprised when  our reporter told him of our findings.
I'm delighted to hear that MoisturePlus has achieved so much success with them.
We have basically given up on marketing to tween girls.
The customers that we recruit for viral marketing are made fun of  by their friends for  allegedly using a cheap, inferior product.
The MoisturePlus brand is so powerful that it's a waste of  our marketing dollars to compete.
With any luck, the MoisturePlus brand will take a hit if  something happens like their celebrity endorsement getting caught on video having…
Seems like a nice article, on the face of it.
It’s hard to imagine the tween girl campaign would have worked.
If  the overwhelming majority of  them are using MoisturePlus two or more times a day, what opportunity is there for increasing sales?
But first, you need to get a handle on what just happened to your analysis.
Somewhere along the way, you picked up some bad or incomplete information that left you blind to these facts about tween girls.
You let the CEO’s beliefs take you down the wrong path.
Here’s what the CEO said about how MoisturePlus sales works:
Take a look at how these beliefs fit with the data.
Do the two agree or conflict? Do they describe different things?
He assumes that tween girls are the only buyers and that tween girls have the ability to purchase more MoisturePlus.
MoisturePlus customers are tween girls (where tweens are people aged 11–15)
Acme is trying out reallocating expenses from advertisements to social networking, but so far, the success of  the initiative is unknown.
We see no limit to potential sales growth among tween girls.
In light of the news article, you might want to reassess these beliefs.
Your assumptions and beliefs about the world are your mental model.
If  the newspaper report is true, the CEO’s beliefs about tween girls are wrong.
Those beliefs are the model you’ve been using to interpret the data.
The world is complicated, so we use mental models to make sense of  it.
Your brain is like a toolbox, and any time your brain gets new information, it picks a tool to help interpret that information.
Mental models can be hard-wired, innate cognitive abilities, or they can be theories that you learn.
Either way, they have a big impact on how you interpret data.
Aren’t you supposed to be analyzing data, not people’s minds? What about data models?
Sometimes mental models are a big help, and sometimes they cause problems.
In this book, you’ll get a crash course on how to use them to your advantage.
What’s most important for now is that you always make them explicit and give them the same serious and careful treatment that you give data.
Any time you’re faced with new information, your brain pulls out a tool to make sense of it.
Your statistical model depends on your mental model Mental models determine what you see.
If  you’re aware of  your mental model, you’re more likely to see what’s important and develop the most relevant and useful statistical models.
If  you use the wrong mental model, your analysis fails before it even begins.
You can’t see everything, so your brain has to be selective in what it chooses to focus your attention on.
Your mental model is like the lens you use to view the world.
One mental model will draw your attention to some features of the world…
Let’s take another look at the data and think about what other mental models would fit the data.
List some assumptions that would be true if MoisturePlus is actually the preferred lotion for tweens.
List some assumptions that would be true if MoisturePlus was in serious danger of losing customers to their competition.
You just looked at your summary data with a new perspective: how would different mental models fit?
It's not unusual for your client to have the completely wrong mental model.
In fact, it's really common for people to ignore what might be the most important part of the mental model…
Tween girls spend almost all their moisturizer dollars on MoisturePlus.
Acme needs to find new markets for MoisturePlus to increase sales.
Social networks are the most cost-effective way to sell to people nowadays.
Tween girls shifting to new moisturizer product, and Acme needs to fight back.
Social network marketing is a black hole, and we need to go back to ads.
The “dry” skin look is becoming popular among young people.
Tween girls are willing to spend much more money on moisturizer.
List some assumptions that would be true if MoisturePlus is actually the preferred lotion for tweens.
List some assumptions that would be true if MoisturePlus was in serious danger of losing customers to their competition.
If  you’re explicit about uncertainty, you’ll be on the lookout for ways to use data to fill gaps in your knowledge, and you will make better recommendations.
Thinking about uncertainties and blind spots can be uncomfortable, but the payoff  is huge.
This “anti-resume” talks about what someone doesn’t know rather than what they do know.
If  you want to hire a dancer, say, the dances they don’t know might be more interesting to you than the dances they do know.
What questions would you ask the CEO to find out what he doesn’t know?
The first fifty digits of  Pi How many mobile minutes I used today The meaning of  life.
Make a toast in Urdu Dance merengue Shred on the guitar.
When you hire people, you often find out what they don’t know only when it’s too late.
Specify uncertainty up front, and you won’t get nasty surprises later on.
Where would you say are the biggest gaps in your knowledge about MoisturePlus sales?
I’d always thought we really understood how customers felt about our product.
But since we don’t sell direct to consumers, we really don’t know what happens after we send our product to our resellers.
So, yeah, we don’t really know what happens once MoisturePlus leaves the warehouse.
Well, like they always say, half of it works, half of it doesn’t, and you never know which half is which.
But it’s pretty clear that the MoisturePlus brand is most of what our customers are buying, because MoisturePlus isn’t terribly different from other moisturizers, so ads are key to establishing the brand.
Because the product is so brand-driven we only think about tween girls.
Are there any other lingering uncertainties that I should know about?
I don’t feel like I know anything about my product any more.
Your data analysis makes me think I know less than I ever knew.
Not a lot of certainty here on how well advertising works.
Q: That’s a funny thing the CEO said at the end: data analysis makes you feel like you know less.
Nowadays, more and more problems can be solved by using the techniques of data analysis.
These are problems that, in the past, people would solve using gut instincts, flying by the seat of their pants.
Q: So mental models feel more and more flimsy compared to how they felt in the past?
A: A lot of what mental models do is help you fill in the gaps of what you don’t know.
So the point of the exercise of specifying your uncertainty in great detail is to help you see the blind spots that require hard-nosed empirical data work.
Q: But won’t I always need to use mental models to fill in the gaps of knowledge in how I understand the world?
A: Absolutely… Q: Because even if I get a good understanding of how things work right now, ten minutes from now the world will be different.
That’s why specifying your problem rigorously and managing the uncertainties in your mental model is so important.
You have only so much time and resources to devote to solving your analytical problems, so answering these questions will help you do it efficiently and effectively.
Q: Does stuff you learn from your statistical models make it into your mental models?
The facts and phenomena you discover in today’s research often become the assumptions that take you into tomorrow’s research.
Think of it this way: you’ll inevitably draw wrong conclusions from your statistical models.
And when those conclusions become part of your mental model, you want to keep them explicit, so you can recognize a situation where you need to double back and change them.
Q: So mental models are things that you can test empirically?
You can’t test everything, but everything in your model should be testable.
The CEO ordered more data to help you look for market segments besides tween girls.
Acme just sent you a huge list of raw data When you get new data, and you haven’t done anything to change it yet, it’s considered raw data.
You willl almost always need to manipulate data you get from someone else in order to get it into a useful form for the number crunching you want to do.
And keep them separate from any data manipulation you do.
Even the best analysts make mistakes, and you always need to be able to compare your work to the raw data.
This is a lot of stuff… maybe more than you need.
Just stay focused on what you’re trying to accomplish with the data.
If  you lose track of your goals and assumptions, it’s easy to get “lost”
But good data analysis is all about keeping focused on what you want to learn about the data.
Take a close look at this data and think about the CEO’s mental model.
Does this data fit with the idea that the customers are all tween girls, or might it suggest other customers?
What did you see in the data? Is the CEO right that only tween girls purchase MoisturePlus, or might there be someone else?
We can certainly see that Acme is selling to companies.
Time to drill further into the data You looked at the mass of  data with a very clear task: find out who’s buying besides tween girls.
At Acme’s request, General American Wholesalers sent over this breakdown of their customers for MoisturePlus.
Write down what this data tells you about who’s buying MoisturePlus.
What did General American Wholesaler’s vendor list tell you about who’s buying MoisturePlus?
Yeah, the old guys like it, too, even though they’re embarrassed that it’s a tween product.
It looks like there’s a whole group of  people out there buying MoisturePlus that Acme hasn’t recognized.
With any luck, this group of  people could be where you have the potential to grow Acme’s sales.
You’ve made it to the final stage of this analysis.
Remember, walk your client through your thought process in detail.
Finally, what do you suggest that he do to improve his business on the basis of  your insights? How does this information help him increase sales?
This intelligence might bring about a huge shift in how we do business.
Could you just walk me through how you came to this conclusion? And what should we do with this new information?
How did you recap your work, and what do you recommend that the CEO do in order to increase sales?
I started off trying to figure out how to increase sales to tween girls, because we believed that.
Turns out there are more people than we realized who are enthusiastic.
Q: If I have to get more detailed data to answer my questions, how will I know when to stop? Do I need to go as far as interviewing customers myself?
A: How far to go chasing new and deeper data sources is ultimately a question about your own best judgement.
In this case, you searched until you found a new market segment, and that was enough to enable you to generate a compelling new sales strategy.
We’ll talk more about when to stop collecting data in future chapters.
Q: Is seems like getting that wrong mental model at the beginning was devastating to the first analysis I did.
A: Yeah, getting that assumption incorrect at the beginning doomed your analysis to the wrong answers.
That’s why it’s so important to make sure that your models are based on the right assumptions from the very beginning and be ready to go back and refine them as soon as you get data that upsets your assumptions.
Q: Does analysis ever stop? I’m looking for some finality here.
A: You certainly can answer big questions in data analysis, but you can never know everything.
And even if you knew everything today, tomorrow would be different.
Your recommendation to sell to older men might work today, but Acme will always need analysts chasing sales.
A: On the contrary! Analysts are like detectives, and there are always mysteries to be solved.
That’s what makes data analysis so much fun! Just think of going back, refining your models, and looking at the world through your new models as being a fundamental part of your job as data analyst, not an exception the rule.
Here’s what you did Here’s one last look at the steps you’ve gone through to reach your conclusion about how to increase the sales of  Acme’s MoisturePlus.
You suggested that increasing ads to tweens might bring sales back in line.
Actual Gross SalesHere’s where the CEO decided to start marketing to older men.
After he received your report, the CEO quickly mobilized his marketing team and created a SmoothLeather brand moisturizer, which is just MoisturePlus under a new name.
Sales took off ! Within two months sales figures had exceeded the target levels you saw at the beginning of  the chapter.
Can you show what you believe? In a real empirical test? There’s nothing like a good experiment to solve your problems.
Strong empirical data will make your analytical judgments all the more.
They always say, “Ben, you’ll never learn anything flying that kite all day,” but I’ve got a hunch that I’ll prove them all wrong…
Sales are way down, and we need a plan to get back on track.
It’s up to you to make a recommendation to the board.
Times are tough, and even Starbuzz Coffee has felt the sting.
Starbuzz has been the place to go for premium gourmet coffee, but in the past few months, sales have plummeted relative to their projections.
The Starbuzz CEO has called you in to help figure out how to get sales back up.
That’s not a lot of  time to pull a turnaround plan together, but it must be done.
We don’t totally know why sales are down, but we’re pretty sure the economy has something to do with it.
Regardless, you need to figure out how to get sales back up.
Which do you think would be the best ways to start? Why?
The board is expecting a complete turnaround plan at the next meeting.
If your plan for getting numbers back up is insufficient, we’ll be forced to enact our plan, which first involve the replacement of all high-level staff.
Interview the CEO to figure out how Starbuzz works as a business.
Do a survey of customers to find out what they’re thinking.
Write in the blanks what you think about each of these options.
Where do you think is the best place to start figuring out how to increase Starbuzz sales?
They take a random, representative sample of  their coffee consumers and ask them a bunch of  pertinent questions about how they feel about the coffee and the coffee-buying experience.
What people say in surveys does not always fit with how they behave in reality, but it never hurts to ask people how they feel.
Give them a gander and tell me what you see.
Interview the CEO to figure out how Starbuzz works as a business.
Do a survey of customers to find out what they’re thinking.
Circle the number that corresponds to how you feel about each.
Here it is: the survey the marketing department administers monthly to a large sample of  Starbuzz customers.
If you’re a Starbuzz customer, there’s a good chance someone will hand you one of these to fill out.
The figures represent the average score given to each statement by survey respondents from participating stores.
Starbuzz Survey Thank you for filling out our Starbuzz survey! Once you’re finished, our manager will be delighted to give you a $10 gift card for use at any Starbuzz location.
Circle the number that corresponds to how you feel about each statement.
Starbuzz Survey Thank you for filling out our Starbuzz survey! Once you’re finished, our manager will be delighted to give you a $10 gift card for use at any Starbuzz location.
Circle the number that corresponds to how you feel about each statement.
Starbuzz Survey Thank you for filling out our Starbuzz survey! Once you’re finished, our manager will be delighted to give you a $10 gift card for use at any Starbuzz location.
Circle the number that corresponds to how you feel about each statement.
Starbuzz Survey Thank you for filling out our Starbuzz survey! Once you’re finished, our manager will be delighted to give you a $10 gift card for use at any Starbuzz location.
Circle the number that corresponds to how you feel about each statement.
Starbuzz Survey Thank you for filling out our Starbuzz survey! Once you’re finished, our manager will be delighted to give you a $10 gift card for use at any Starbuzz location.
Circle the number that corresponds to how you feel about each statement.
Starbuzz Survey Thank you for filling out our Starbuzz survey! Once you’re finished, our manager will be delighted to give you a $10 gift card for use at any Starbuzz location.
Circle the number that corresponds to how you feel about each statement.
Starbuzz Survey Thank you for filling out our Starbuzz survey! Once you’re finished, our manager will be delighted to give you a $10 gift card for use at any Starbuzz location.
Circle the number that corresponds to how you feel about each statement.
Always use the method of comparison One of  the most fundamental principles of  analysis and statistics is the method of  comparison, which states that data is interesting only in comparison to other data.
In this case, the marketing department takes the average answer for each question and compares those averages month by month.
Each monthly average is useful only when you compare it to numbers from other months.
The answers to the questions are all averaged and grouped into this table.
If a statistic seems interesting or useful, you need to explain why in terms of how that statistic compares to others.
If you’re not explicit about it, you’re assuming that your client will make the comparison on their own, and that’s bad analysis.
This number is only useful when you compare it to these numbers.
Look at the survey data on the facing page and compare the averages across the months.
And this is true especially in observational studies like the analysis of  Starbuzz’s marketing data.
In observational data, you just watch people and let them decide what groups they belong to, and taking an inventory of observational data is often the first step to getting better data through experiments.
Is there anything that might explain to you why sales are down?
Observational study A study where the people being described decide on their.
In experiments, on the other hand, you decide which groups people go into.
Groups of people might be “big spenders,” “tea drinkers,” etc.
It would make sense to say that, if people on average think that the coffee isn’t a good value for.
Now you’ve looked closely at the data to figure out what patterns the data contains.
Do you think that the decline in perceived value is the reason for the sales decline?
Is there anything that might explain to you why sales are down?
All the variables except for “Coffee value” bounce around within a narrow range.
This variable shows a pretty steady decline over the past six months.
The figures represents the average score given to each statement by survey respondents from participating stores.
According to the data, everything’s going along just fine with Starbuzz customers, except for one variable: perceived Starbuzz coffee value.
It looks like people might be buying less because they don’t think Starbuzz is a good bang for the buck.
Maybe the economy has made people a little more cash-strapped, so they’re more sensitive to prices.
Your so-called “value problem” is no problem at all at my stores! Our Starbuzz is hugely popular, and no one thinks that Starbuzz is a poor value.
The manager of the SoHo stores does not agree SoHo is a wealthy area and the home of  a bunch of  really lucrative Starbuzz stores, and the manager of  those stores does not believe it’s true that there’s a value perception problem.
Are her customers lying? Did someone record the data incorrectly? Or is there something problematic about the observational study itself ?
Q: How do I know that a decline in value actually caused coffee sales to go down?
But right now the perceived value data is the only data you have that is congruent with the decline in sales.
It looks like sales and perceived value are going down together, but you don’t know that the decline in value has caused the decline in sales.
Q: Could there be other factors at play? Maybe the value problem isn’t as simple as it looks.
With observational studies, you should assume that other factors are.
Q: Could it be the other way around? Maybe declining sales caused people to think the coffee is less valuable.
A: That’s a great question, and it could definitely be the other way around.
A good rule of thumb for analysts is, when you’re starting to suspect that causes are going in one direction (like value perception decline causing sales decline), flip the theory around and see how it looks (like sales decline causes value perception decline)
Q: So how do I figure out what causes what?
A: We’re going to talk a lot throughout this book about how to draw conclusions about causes, but for now, you should know that observational studies aren’t that powerful when it comes to drawing causal conclusions.
Generally, you’ll need other tools to get those sorts of conclusions.
A: Not at all! There is a ton of observational data out there, and it’d be crazy to ignore it because of the shortcomings of observational studies.
What’s really important, however, is that you understand the limitations of observational studies, so that you don’t draw the wrong conclusions about them.
Those guys just don’t know how to read the numbers, and numbers don’t lie.
Sometimes the instincts of  the people on the ground tell you more than the statistics.
In fact, I’m tempted to just scrap this entire data set.
Jim: What specific reason do you have to believe that this data is flawed?
Frank: Look, we need to go back to our interpretation of  the typical or average customer.
Frank: Is there any reason why this chain of events wouldn’t apply to people in SoHo?
Frank: How you persuaded someone from the fashionable set to date you I have no idea.
If  you’re doing well moneywise, you’d be less likely to start believing that Starbuzz is a poor value.
It looks like the SoHo Starbuzz customers may be different from all the other Starbuzz customers…
It’s always a good idea to draw pictures of how you think things relate.
Observational studies are full of confounders A confounder is a difference among the people in your study other than the factor you’re trying to compare that ends up making your results less sensible.
In this case, you’re comparing Starbuzz customers to each other at different points in time.
Starbuzz customers are obviously different from each other—they’re different people.
But if  they’re different from each other in respect to a variable you’re trying to understand, the difference is a confounder, and in this case the confounder is location.
Redraw the causal diagram from the facing page to distinguish between SoHo stores and all the other stores., correcting for the location confounder.
Assume that the SoHo manager is correct and that SoHo customers don’t perceive a value problem.
The customers that are located in SoHo might be different from the rest in a way that confounds our results.
How location might be confounding your results Here’s a refined diagram to show how things might be happening.
It’s a really good idea to make your theories visual using diagrams like this, which help both you and your clients keep track of  your ideas.
The arrows show the order in which things are happening and which effects follow from which causes.
What would you do to the data to show whether Starbuzz value perception in SoHo is still going strong? More generally, what would you do to observational study data to keep your confounders under control?
Q: In this case, isn’t it really the wealth of the customers rather than the location that confounds the results?
If you had the data on how much money each customers has, or how much money each customer feels comfortable spending, you could run the analysis again to see what sort of results wealth-based grouping gets you.
But since we don’t have that information, we’re using location.
Besides, location makes sense, because our theory says that wealthier people tend to shop in SoHo.
Q: Could there be other variables that are confounding this data besides location?
Your job as the analyst is always to think about how confounding might be affecting your results.
If you think that the effect of confounders is minimal, that’s great, but if there’s reason to believe that they’re causing problems, you need to adjust your conclusion accordingly.
Q: What if the confounders are hidden? A: That’s precisely the problem.
Your confounders are usually not going to scream out to you.
You have to dig them up yourself as you try to make your analysis as strong as possible.
In this case, we are fortunate, because the location confounder was actually represented in the data, so we can manipulate the data to manage it.
Often, the confounder information won’t be there, which seriously undermines the ability of the entire study to give you useful conclusions.
Q: How far should I go to figure out what the confounders are?
You should ask yourself commonsense questions about what it is you’re studying to imagine what variables might be confounding your results.
As with everything in data analysis and statistics, no matter how fancy your quantitative techniques are, it’s always really important that your conclusions make sense.
If your conclusions make sense, and you’ve thoroughly searched for confounders, you’ve done all you can do for observational studies.
Other types of studies, as you’ll see, enable you to draw some more ambitious conclusions.
Q: Is it possible that location wouldn’t be a confounder in this same data if I were looking at something besides value perception?
Remember, location being a confounder makes sense in this context, but it might not make sense in another context.
We have no reason to believe, for example, that people’s feelings about whether their coffee temperature is right vary from place to place.
Q: I’m still feeling like observational studies have big problems.
This particular study has been useful to you in terms of understanding Starbuzz customers better, and when you control for location in the data the study will be even more powerful.
Manage confounders by breaking the data into chunks To get your observational study confounders under control, sometimes it’s a good idea to divide your groups into smaller chunks.
In other words, they don’t have the internal variation that might skew your results and give you the wrong ideas.
Here is the Starbuzz survey data once again, this time with tables to represent other regions.
The figures represents the average score given to each statement by survey respondents from participating stores.
Take a look at the data on the facing page, which has been broken into groups.
How does perceived coffee value compare among all the groups?
Was the SoHo manager right about her customers being happy with the value of Starbuzz coffee?
How much of a difference is there between the Mid-Atlantic store subgroup average scores and the average scores for all the Starbuzz stores?
When you looked at the survey data that had been grouped by location, what did you see?
Was the SoHo manager right about her customers being happy with the value of Starbuzz coffee?
How much of a difference is there between the Mid-Atlantic store subgroup average scores and the average scores for all the Starbuzz stores? All the scores wiggle around in the same narrow range, except for the value perception score.
How does perceived coffee value compare among all the groups?
Seattle has a precipitous drop, just like the Mid-Atlantic region.
The data definitely confirm the SoHo manager’s beliefs about what her customers think about.
It was certainly a good idea to listen to her feedback and look at the data in a.
CFO: This situation is worse than we had anticipated, by a long shot.
The value perception in our regions other than SoHo has absolutely fallen through the floor.
The first table, which showed all the regions together, actually made the value perception look better than it is.
CFO: When you break out SoHo, where everyone’s rich, you can see that SoHo customers are pleased with the prices but that everyone else is about to jump ship, if they haven’t already.
Marketing: So we need to figure out what to do.
Marketing: I don’t know what planet you’re from, but we have a brand to worry about.
CFO: I come from Planet Business, and we call this supply and demand.
You might want to go back to school to learn what those words mean.
Marketing: We might get a jump in sales in the short term, but we’ll be sacrificing our profit margins forever if  we cut costs.
We need to figure out a way to persuade people that Starbuzz is a value and keep prices the same.
Your fluffy little ideas won’t get us out of  this jam.
The big guns have all come out to deal with the problems you’ve identified.
Is there anything in the data you have that tells you which strategy will increase sales?
Look again at that last question on the previous page:
You have no observational data that will tell you what will happen if  you try out what either the VP of Marketing or the CFO suggests.
If  you want to draw conclusions about things that overlap with your data but aren’t completely described in the data, you need theory to make the connection.
You have no data to support either of  these theories, no matter how passionately the others believe in them and in the strategies that follow from them.
In order to get more clarity about which strategy is better, you’re going to need to run an experiment.
You need to experiment with these strategies in order to know which will increase sales.
These theories might be true or totally false, but your data doesn’t say.
This will cause people to perceive more value in Starbuzz coffee, which will drive sales back up.
Starbuzz really is a good value, if  you think about it in the right way.
Persuading people to change their beliefs will get sales back up.
You need an experiment to say which strategy will work best.
Is there anything in the data you have that tells you which strategy will increase sales?
Observational data by itself can’t tell you what will happen in the future.
The Starbuzz CEO is in a big hurry And he’s going to pull the trigger whether you’re ready or not!
Starbuzz drops its prices Taking a cue from the CFO, the CEO ordered a price drop across the board for the month of February.
All prices in all Starbuzz stores are reduced by $0.25
Great! Our board was upset at lowering prices, but look how well it went.
Now I need to know how much more money we made as a result of the move.
It’d be nice to know how much extra Starbuzz earned in February that they wouldn’t have earned if prices hadn’t been dropped.
Do you think sales would have the data to help figure this out? Why or why not?
Control groups give you a baseline You have no idea how much extra you made.
Sales could have skyrocketed relative to what they would have been had the CEO not cut prices.
You don’t know because by slashing prices across the board the CEO failed to follow the method of  comparison.
Good experiments always have a control group that enables the analyst to compare what you want to test with the status quo.
Sales could have gone through the roof if Starbuzz had maintained status quo.
Does sales have the data that would help you figure out how much more money you made off the cheaper $3.75 coffee?
They only have data for $3.75 coffee and they can’t compare that.
A: Sure, and if all you’re interested in is whether sales in February are higher than January, you’ll have your answer.
But without a control, the data doesn’t say whether your price-cutting had anything to do with it.
Q: What about comparing this February’s sales with last year’s February’s sales?
A: In this question and the last one you’re talking about using historical controls, where you take past data and treat it as the control, as opposed to contemporaneous controls, where your control group has its experience at the same time as your experimental group.
Historical controls usually tend to favor the success of whatever it is you’re trying to test because it’s so hard to select a control that is really like the group you’re testing.
Q: Do you always need a control? Is there ever a case where you can get by without one?
A: A lot of events in the world can’t be controlled.
Say you’re voting in an election: you can’t elect two candidates simultaneously, see which one fares better relative to the other, and then go back and.
That’s just not how elections work, and it doesn’t mean that you can’t analyze the implications of one choice over the other.
But if you could run an experiment like that you’d be able to get a lot more confidence in your choice!
Q: What about medical tests? Say you want to try out a new drug and are pretty sure it works.
Wouldn’t you just be letting people be sick or die if you stuck them in a control group that didn’t receive treatment?
A: That’s a good question with a legitimate ethical concern.
Medical studies that lack controls (or use historical controls) have very often favored treatments that are later shown by contemporaneous controlled experiment to have no effect or even be harmful.
No matter what your feelings are about a medical treatment, you don’t really know that it’s better than nothing until you do the controlled experiment.
In the worst case, you could end up promoting a treatment that actually hurts people.
Q: Like the practice of bleeding people when they were sick?
In fact, some of the first controlled experiments in history compared medical bleeding against just letting people be.
Bleeding was a frankly disgusting practice that persisted for hundreds of years.
We know now that it was the wrong thing to do because of controlled experiments.
Remember the definition of observational studies: they’re studies where the subjects themselves decide what group they’re in, rather than having you decide it.
If you wanted to do a study on smoking, for example, you couldn’t tell some people to be smokers and some people not to be smokers.
People decide issues like smoking on their own, and in this case, people who chose to be nonsmokers would be the control group of your observational study.
Q: I’ve been in all sorts of situations where sales have trended upwards in one month because we supposedly did something in the previous month.
But you’re saying that we have no idea whether we did well?
There’s definitely a place for gut instincts in business, and sometimes you can’t do controlled experiments and have to rely on observational data-based judgements.
There’s nothing like hard data to supplement your judgement and instincts when you make decisions.
In this case, you don’t have the hard data yet, but you have a CEO that expects answers.
The CEO still wants to know how much extra money the new strategy made… How will you answer his request?
Jim: The CEO asked us to figure out how much money we made in February that we wouldn’t have made if  we hadn’t cut costs.
We have no idea how much extra money we made.
It could have been a lot, but we could have lost money.
It might not be statistically perfect, but he’ll be happy.
Frank: A happy client is all that counts? Sounds like you want us to sacrifice the war to win the day.
If  we give him the wrong answers, it’ll eventually come back on us.
Frank: We’re going to have to give it to him straight, and it won’t be pretty.
All we have to do is set up a control group for March and run the experiment again.
Frank: But the CEO is feeling good about what happened in February, and that’s because he has the wrong idea about what happened.
Jim: I think we can get him thinking clearly without being downers about it.
Having to deliver bad news is part of  being a data analyst.
But there are a bunch of  different ways of  going about delivering the same information.
Option 3: There’s bad news, but if  we use it correctly it’s good news.
This event doesn’t give us the information we want, but the good news is that I know how we fix it.
The best data analysts know the right way to deliver potentially upsetting messages.
Just do what you need to do and get it right this time.That was a.
We’re running the experiment again for the month of  March.
This time, Marketing divided the universe of  Starbuzz stores into control and experimental groups.
The experimental group consists of  stores from the Pacific region, and the control group consists of  stores from the SoHo and MidAtlantic regions.
If we’re going to tell the CEO, we’d better be sure there aren’t confounders like we had in the past.
Look at the design on the facing page and the results above.
Remember, a confounder is a difference among the groups in your study other than the factor you’re trying to compare.
Things aren’t looking half  bad! Your experiment might have given you the answer you want about the effectiveness of  price cutting.
Is it possible that these variables are confounding your results?
Location could definitely confound.The culture ought to be the same all over.
Just because you’ve stepped out of  the world of  observational studies to do an experiment you’re not off  the hook with confounders.
In order for your comparison to be valid, your groups need to be the same.
You’re comparing these two, but they’re different in more ways than the treatment.
It could be because people spend more when the coffee cost less.
But since the groups aren’t comparable, it could be for any number of  other reasons.
The weather could be keeping people on the east coast indoors.
The economy could have taken off in the Pacific region.
How do you think each will fare as a method for avoiding confounders.
That way, half of your customers are experimental, half are control, and location isn’t a confounder.
Use historical controls, making all the stores the control group this month and all the stores the experimental group next month.
Divide big geographic regions into small ones and randomly assign the microregions to control and experimental groups.
Just as it was with observational studies, avoiding confounders is all about splitting the stores into groups correctly.
Which method for selecting groups do you think is best?
That way, half of your customers are experimental, half are control, and location isn’t a confounder.
Use historical controls, making all the stores the control group this month and all the stores the experimental group next month.
Divide big geographic regions into small ones and randomly assign the microregions to control and experimental groups.
This looks kind of promising, but it doesn’t quite fit the bill.
People would just go to the cheaper Starbuzz outlets rather.
If your regions were big enough that people wouldn’t travel to.
Randomly selecting members from your pool of  subjects is a great way to avoid confounders.
What ends up happening when you randomly assign subjects to groups is this: the factors that might otherwise become confounders end up getting equal representation among your control and experimental groups.
By selecting members of your groups randomly, the groups will be like each other and therefore comparable.
Each micro-region will tend to have the same characteristics as the other micro-regions.
You’re evidently a big deal in data analysis and it’s great to have you.
Randomness: Well, my schedule from one second to the next is kind of  open.
My being here is, well, like the roll of  the dice.
So you have no real plan or vision for how you do things?
Head First: So why are you useful in experimental design? Isn’t data analysis all about order and method?
Randomness: When an analyst uses my power to select which experimental and control groups people or stores (or whatever) go into, my black magic makes the resulting groups the same as each other.
Randomness: Say half  of  your population is subject to a hidden confounder, called Factor X.
Scary, right? Factor X could mess up your results big time.
You don’t know what it is, and you don’t have any data on it.
Head First: But that’s always a risk in observational studies.
Randomness: Sure, but say in your experiment you use me to divide your population into experimental and control groups.
What’ll happen is that your two groups will end up both containing Factor X to the same degree.
Head First: So Factor X may still affect your results, but it’ll affect both groups in the exact same way, which means you can have a valid comparison in terms of  whatever it is you’re testing for?
You can do analysis without it, but if  you have it at your disposal you’re going to do the best work.
Randomized controlled experiments get you as close as you can get to the holy grail of  data analysis: demonstrating causal relationships.
Head First: You mean that randomized controlled experiments can prove causal relationships?
Randomness: Well, “proof ” is a very, very strong word.
You’re testing two groups that are identical in every way except in the variable you’re testing.
If  there’s any difference in the outcome between the groups, how could it be anything besides that variable?
Head First: So how do I do randomness? Say I have a spreadsheet list I want to split in half, selecting the members of  the list randomly.
In your spreadsheet program, create a column called “Random” and type this formula into the first cell: =RAND()
Copy and paste the formula for each member of  your list.
That’s it! You can then divide your list into your control group and as many experimental groups as you need, and you’re good to go!
Randomness Exposed This week’s interview: OMG that was so random!
Now that you understand observational and experimental studies, control and experimental groups, confounding, and randomization, you should be able to design just the experiment to tell you what you want to know.
What are your control and experimental groups going to be?
What are your control and experimental groups going to be?
The purpose of the experiment is to figure out which will do a better job of increasing sales:
We’re going to run the experiment over the course of one month: March.
The control group will be stores that are functioning as they always function—no specials or.
One experimental group will consist of stores that have a price drop for March.
That way, our three groups will be about the same.
It’s impossible to know until we run the experiment, but what might happen is that one or both of the.
Experimental group #2: persuade customers that Starbuzz is a value for a.
Before we run it, let’s take one last look at the process we’re going through to show once and for all which strategy is best.
Value persuasion appears to result in significantly higher sales than either lowering prices or doing nothing.
Starbuzz set up your experiment and let it run over the course of  several weeks.
The daily revenue levels for the value persuasion group immediately went up compared to the other two groups, and the revenue for the lower prices group actually matched the control.
This chart is so useful because it makes an excellent comparison.
You selected identical groups and gave them separate treatments, so now you can really attribute the differences in revenue from these stores to the factors you’re testing.
There doesn’t seem to be a revenue difference between these two strategies.
I’m really happy about this finding! I’m giving the order to implement this strategy in all our stores.
When you started this adventure in experiments, Starbuzz was in disarray.
You carefully evaluated observational survey data and learned more about the business from several bright people at Starbuzz, which led you to create a randomized controlled experiment.
That experiment made a powerful comparison, which showed that persuading people that Starbuzz coffee is a more effective way to increase sales than lowering prices and doing nothing.
And we’re always trying to figure out how to get it.
You’re now in the bath toy game You’ve been hired by Bathing Friends Unlimited, one of  the country’s premier manufactures of  rubber duckies and fish for bath-time entertainment purposes.
Believe it or not, bath toys are a serious and profitable business.
They want to make more money, and they hear that managing their business through data analysis is all the rage, so they called you!
I’ll give your firm top consideration as I make my toy purchases this year.
The rubber fish is an unconventional choice, but it’s been a big seller.
Some call it the classic, some say it’s too obvious, but one thing is clear: the rubber ducky is here to stay.
Here’s an email from your client at Bathing Friends Unlimited, describing why they hired you.
From: Bathing Friends Unlimited To: Head First Subject: Requested analysis of product mix Dear Analyst, We’re excited to have you! We want to be as profitable as possible, and in order to get our profits up, we need to make sure we’re making the right amount of ducks and the right amount of fish.
What we need you to help us figure out is our ideal product mix: how much of each should we manufacture? Looking forward to your work.
First of all, it’d be nice to have data on just how profitable ducks.
Is one more profitable than the other? But more than.
How much rubber does it take make these products? And how much.
You need the hard numbers on what you can and can’t control.
Take a closer look at what you need to know.
You can divide those data needs into two categories: things you can’t control, and things you can.
And the basic thing the client wants you to find out in order to get the profit as high as possible.
Ultimately, the answers to these two questions you can control.
Dear Analyst, We’re excited to have you! We want to be as profitable as possible, and in order to get our profits up we need to make sure we’re making the right amount of ducks and the right amount of fish.
What we need you to help us figure out is our ideal product mix: how much of each should we manufacture? Looking forward to your work.
These considerations are called constraints, because they will define the parameters for your problem.
What you’re ultimately after is profit, and finding the right product mix is how you’ll determine the right level of  profitability for next month.
But your options for product mix will be limited by your constraints.
Constraints don’t tell you how to maximize profit; they only tell you what you can’t do to maximize profit.
Decision variables, on the other hand, are the things you can control.
You get to choose how many ducks and fish will be manufactured, and as long as your constraints are met, your job is to choose the combination that creates the most profit.
If we did make 400 fish, we wouldn’t have any rubber to make ducks, and vice versa.
That has to do with the time it takes to set the rubber.
So, what do you think you do with constraints and decision variables to figure out how to maximize profit?
You get to choose the values for each of these.
You have an optimization problem When you want to get as much (or as little) of something as possible, and the way you’ll get it is by changing the values of  other quantities, you have an optimization problem.
Here you want to maximize profit by changing your decision variables: the number of  ducks and fish you manufacture.
But to maximize profit, you have to stay within your constraints: the manufacture time and rubber supply for both toys.
To solve an optimization problem, you need to combine your decision variables, constraints, and the thing you want to maximize together into an objective function.
The objective is the thing you want to maximize or minimize, and you use the objective function to find the optimum result.
Here’s what your objective function looks like, if  you state it algebraically:
Don’t be scared! All this equation says is that you should get the highest P (profit) possible by multiplying each decision variable by a constraint.
Your constraints and decision variables in this equation combine to become the profit of ducks and fish, and those together form your objective: the total profit.
You want your objective to be as high as you can get it.
The constraints that you need to put into your objective function are the profit for each toy.
The profit you get from selling fish and ducks is equal to the profit per duck multiplied by the number of  ducks plus the profit per fish multiplied by the number of  fish.
You can fill in this equation with the values you know represent the profit per item along with some hypothetical count amounts.
This objective function projects a $700 profit for next month.
We’ll use the objective function to try out a number of  other product mixes, too.
Hey! What about all those other constraints? Like rubber and time?
If we did make 400 fish, we wouldn’t have any rubber to make ducks, and vice versa.
That has to do with the time it takes to set the rubber.
How would you visualize the constraints on hypothetical product mixes of ducks and fish with one chart?
Bathing Friends Unlimited has time to produce 400 ducks in the next month.
Rubber and time place limits on the count of fish you can manufacture, and the best way to start thinking about these constraints is to envision different hypothetical product mixes.
You can plot the time constraints for that product mix (and two others) on these bar graphs.
Seeing the constraints in this way is progress, but we need a better visualization.
We have yet more constraints to manage, and it’d be clearer if  we could view them both on a single chart.
This line shows the maximum number of ducks you can produce.
This line shows how many fish you have time to produce.
We’ll also be able to use this chart to visualize the rubber constraints.
In fact, you can place any number of  constraints on this chart and get an idea of  what product mixes are possible.
We can plot both time constraints on a single chart, representing each product mix with a dot rather than a bar.
The resulting chart makes it easy to visualize both time constraints together.
Bathing Friends Unlimited has time to produce 400 ducks in the next month.
Plotting ducks on a y-axis and fish on an x-axis makes it easy to see what product mixes are feasible.
In fact, the space where product mixes are within the constraint lines is called the feasible region.
When you add constraints to your chart, the feasible region will change, and you’ll use the feasible region to figure out which point is optimal.
Let’s add our other constraint, which states how many fish and ducks can be produced given the quantity of rubber they have.
Draw a point representing a product mix where you make 400 fish.
As she says, if  you make 400 fish, you won’t have rubber to make any ducks.
Draw a point representing a product mix where you make 500 ducks.
If  you made 500 ducks, you’d be able to make zero fish.
If we did make 400 fish, we wouldn’t have any rubber to make ducks, and vice versa.
That has to do with the time it takes to set the rubber.
Each fish takes a little more rubber to make than each duck.
You have a fixed supply of rubber, so the number of ducks you make will limit the number of fish you can make.
Draw a point representing a product mix where you make 400 fish.
As she says, if  you make 400 fish, you won’t have rubber to make any ducks.
Draw a point representing a product mix where you make 500 ducks.
If  you made 500 ducks, you’d be able to make zero fish.
If we did make 400 fish, we wouldn’t have any rubber to make ducks, and vice versa.
That has to do with the time it takes to set the rubber.
Are they inside the feasible region? Draw a dot for each product mix on the chart.
How much profit will the different product mixes create? Use the equation below to determine the profit for each.
When you added the rubber constraint, you changed the shape of  the feasible region.
But now your rubber scarcity has ruled out that product mix as a possibility.
You can’t use duck/fish combinations that exist in any of these spaces.
Your potential product mixes all need to be inside here.
You just graphed and calculated the profit for three different product mixes of ducks and fish.
Now all you have to do is try every possible product mix and see which one has the most profit, right?
Even in the small space of the feasible region there are tons and tons of possible product mixes.
There’s no way you’re going to get me to try them all.
Because both Microsoft Excel and OpenOffice have a handy little function that makes short order of  optimization problems.
Your spreadsheet does optimization Microsoft Excel and OpenOffice both have a handy little utility called Solver that can make short order of  your optimization problems.
If  you plug in the constraints and write the objective function, Solver does the algebra for you.
Take a look at this spreadsheet, which describes all the information you received from Bathing Friends Unlimited.
First, here are some numbers to quantify your rubber needs.
The bath toys are made out of  rubber pellets, and cells B10:B11 have formulas that calculate how many pellets you need.
Second, cell B20 has a formula that multiplies the count of fish and ducks by the profit for each to get the total profit.
These cells show a product mix where you manufacture 100 ducks and fish each.
Take a look at Appendix iii if you use OpenOffice or if Solver isn’t on your Excel menu.
Let’s take a look at the Solver dialogue box and figure out how it works with the concepts you’ve learned.
Draw an arrow from each element to where it goes in the Solver dialogue box.
Draw an arrow from each element to where it should go on the Solver.
How do the spaces in the Solver dialogue box match up with the optimization concepts you’ve learned?
Draw an arrow from each element to where it goes in the Solver dialogue box.
The decision variables are the values you will change to find your objective.
Constraints go in the constraints box… no big surprise there!
The objective function goes in a cell on the spreadsheet and returns the objective as the result.
The objective that this objective function calculates is the total profit.
Test Drive Now that you’ve defined your optimization model, it’s time to plug the elements of it into Excel and let the Solver do your number crunching for you.
Set your target cell to point to your objective function.
Find your decision variables and add them to the Changing Cells blank.
Solver crunched your optimization problem in a snap Nice work.
Solver took all of  about a millisecond to find the solution to your optimization problem.
What’s more, if  you compare Solver’s result to the graph you created, you can see that the precise point that Solver considers the best is on the outer limit of  your feasible region.
Solver tried out a bunch of Count values and found the ones that maximize profit.
Better explain to the client what you’ve been up to…
How would you explain to the client what you’re up to? Describe each of these visualizations.
The shaded part of this graph shows all the possible.
Profits fell through the floor You just got this note from Bathing Friends Unlimited about the results of  your analysis…
The fish sold out, but no one’s buying the ducks.
We haven’t ever had this sort of experience before with our duck sales, so for the moment we’re not blaming you for this until we can do our own internal evaluation of what happened.
Your analytical tools inevitably simplify reality, but if  your assumptions are accurate and your data’s good the tools can be pretty reliable.
Your goal should be to create the most useful models you can, making the imperfections of  the models unimportant relative to your analytical objectives.
It’s a good idea to keep in mind this cheeky quote from a famous statistician:
Your model tells you how to maximize profits only under the constraints you specified.
Your models approximate reality and are never perfect, and sometimes their imperfections can cause you problems.
So how will I know if my model has the right assumptions?
Calibrate your assumptions to your analytical objectives You can’t specify all your assumptions, but if you miss an important one it could ruin your analysis.
You will always be asking yourself  how far you need to go specifying assumptions.
Write down everything you think you know and everything you.
What assumption do you need to include in order to get your optimization model working again?
Q: What if the bad assumption were true, and people would buy everything we manufactured? Would the optimization method have worked?
If you can assume that everything you make will sell out, then maximizing your profitability is going to be largely about fine-tuning your product mix.
Q: But what if I set up the objective function to figure out how to maximize the amount of ducks and fish we made overall? It would seem that, if everything was selling out, we’d want to figure out how to make more.
Your contact at Bathing Friends Unlimited said that you were limited in the amount of fish and ducks you could produce by both time and rubber supply.
It’s a tool that you only use when you have a single number that you want to maximize and some handy equations that you can use to find the right value.
A: But you can think of optimization more broadly than that.
The optimizing mentality is all about figuring out what you want and carefully identifying the constraints that will affect how you are able to get it.
Often, those constraints will be things you can represent quantitatively, and in that case, an algebraic software tool like Solver will work well.
Q: So Solver will do my optimizations if my problems can be represented quantitatively.
A: A lot of quantitative problems can be handled by Solver, but Solver is a tool that specializes in problems involving linear programming.
There are other types of optimization problems and a variety of algorithms to solve them.
If you’d like to learn more, run a search on the Internet for operations research.
Q: Should I use optimization to deal with this new model, will we sell people what they want?
A: Yes, if we can figure out how to incorporate people’s preferences into our optimization model.
Is there an assumption that would help you refine your model?
There’s nothing in the current model that says what people will actually buy.
But, as we saw, this isn’t happening, so we need an assumption about what people will buy.
Here’s some historical sales data for rubber fish and ducks.
With this information, you might be able to figure out why no one seemed interested in buying all your ducks.
Is there a pattern in the sales over time that hints at why ducks didn’t sell well last month?
This sales data is for the whole rubber toy industry, not just BFU, so it’s a good indicator of what people prefer to buy and when they prefer to buy it.
What do you see when you look at this new data?
Is there a pattern in the sales over time that hints at why Ducks didn’t sell well last month?
Duck sales and fish sales seem to go in opposite.
Here’s switch, where ducks sell well and then fish jump ahaead..
Don’t assume that two variables are independent of  each other.
Any time you create a model, make sure you specify your assumptions about how the variables relate to each other.
We don’t know why rubber duck and fish sales seem to go in opposite directions from each other, but it sure looks like they are negatively linked.
What sort of constraint would you add to your optimization model to account for the negatively linked fish and duck sales?
Together, they have an increasing trend, with holiday season sales spikes, but always one is ahead of the other.
You need a new constraint that estimates demand for ducks and fish for the month in which you hope to sell them.
Looking at the historical sales data, estimate what you think the highest amount of  sales for ducks and fish will be next month.
Assume also that the next month will follow the trend of  the months that precede it.
Run the Solver again, adding your estimates as new constraints.
For both ducks and fish, what do you think is the maximum number of units you could hope to sell?
Which toy do you think will be on top next month?
None of these elements have changed, so you can leave the spreadsheet itself as is.
You ran your optimization model again to incorporate estimates about rubber duck and fish sales.
Looking at the historical sales data, estimate what you think the highest amount of  sales for ducks and fish will be next month.
Assume that the next month will be similar to the months that preceded it.
Run the Solver again, adding your estimates as new constraints.
We probably won’t be able to sell more than 150 ducks.
We probably won’t be able to sell more than 50 fish.
We should prepare for a big drop in January sales, and it looks like ducks will still be on top.
Your specific numbers may vary a little… these are estimates after all.
Looks like you won’t need to use anywhere near all your rubber.
It’s not as large as last month’s estimate, but it’s.
Nearly every duck and fish that comes out of  their manufacturing operation is sold immediately, so they have no excess inventory and every reason to believe that the profit maximization model has them where they need to be.
Good job! One question: the model works because you got the relationship right between duck demand and fish demand.
But what if that relationship changes? What if people start buying them together, or not at all?
Not only have you optimized our profit, you’ve made our operations more intelligent and data-driven.
We’ll definitely use your model for a long time to come.
Please accept this little token of our appreciation, a special Head First edition of our timeless rubber duck.
All your data is observational, and you don’t know what will happen in the future.
Your model is working now, but it might break suddenly.
You need to be ready and able to reframe your analysis as necessary.
If the relationships between your variables change tomorrow, you’ll need to overhaul your model.
Now hold still… we want to get all the variables together in one shot.
Your data is brilliantly complex, with more variables than you can shake a stick at.
A clear, highly multivariate visualization can in a small space show you the.
New Army is an online clothing retailer that just ran an experiment to test web layouts.
For one month, everyone who came to the website was randomly served one of  these three home page designs.
They had their experiment designers put together a series of  tests that promise to answer a lot of  their questions about their website design.
What they want to do is find the best stylesheets to maximize sales and get people returning to their website.
This is their control, because it’s the stylesheet they’ve been using up to now.
The results are in, but the information designer is out.
It could be hard work, because the experiment designers at New Army are an exacting bunch and generated a lot of solid data.
But before we start, let’s take a look at the rejected designs.
We’ll likely learn something by knowing what sort of  visualizations won’t work.
Now that they have a store of  fantastic data from a controlled, randomized experiment, they need a way to visualize it all together.
So they hired a fancy information designer and asked him to pull together something that helped them understand the implications of  their research.
We got a lot of crap back from the information designer we hired.
It didn’t help us understand our data at all, so he got the ax.
Can you create data visualizations for us that help us build a better website?
What we want to see is which stylesheet or stylesheets maximize revenue, the time our visitors spend on the site, and return visits to the site.
The information designer submitted these three designs to New Army.
What are your impressions? Can you see why the client might not have been pleased?
You can make tag clouds like this for free at http://www.wordle.net.
Total page hits by stylesheet Looks like this chart measures how many visits each home page got.
The size of the text must have something to do with the number of clicks.
You care about the quality of  the data and its interpretation, and you’d hate for a flashy design to get in the way of  your own judgments about the analysis.
What sort of data do you think is behind these visualizations?
Show the data! You can’t tell from these visualizations what data is behind them.
If  you’re the client, how could you ever expect to be able to make useful judgments with the visualizations if  they don’t even say clearly what data they describe?
Your first job in creating good data visualizations is to facilitate rigorous thinking and good decision making on the part of  your clients, and good data analysis begins and ends with thinking with data.
And these graphs are not solutions to the problems of New Army.
You just don’t know what’s behind them until the designer tells you.
New Army’s actual data, however, is really rich and has all sorts of great material for your visualizations.
I want to wish you the best of luck on the New Army project.
I didn’t really want to do it anyway, so it’s good for someone else to get a chance to give it a shot.
One word of warning: they have a lot of data.
Once you really dig into it, you’ll know what I mean.
I say, give me a nice little tabular layout, and I’ll make you a pretty chart with it.
But these guys? They have more data than they know what to do with.
And they will expect you to make visuals of all of it for them.
I just made a few nice charts, which I understand not everyone liked, but I’ll tell you they’ve set forward an insurmountable task.
They want to see it all, but there is just too much.
Dan seems to think  that an excess of data is a real problem for someone trying to design good data visualizations.
Do you think that what he is saying is plausible? Why or why not?
Here’s some unsolicited advice from the last designer You didn’t ask for it, but it appears that you’re getting it anyway: the outgoing information designer wants to put in his two cents about the project.
From the looks of the table on the facing page, it appears that Dan is correct.
Is Dan being reasonable when he says it’s too hard to do good visualizations when there is too much data?
It’s easy to get scared by looking at a lot of  data.
But knowing how to deal with what seems like a lot of  data is easy, too.
If  you’ve got a lot of  data and aren’t sure what to do with it, just remember your analytical objectives.
With these in mind, stay focused on the data that speaks to your objectives and ignore the rest.
Some of this stuff is going to be useful to you.
The whole point of data analysis is to summarize data, and summarizing.
So how do you use a big pile of data with a bunch of different variables to evaluate your objectives? Where exactly do you begin?
The problem is not too much data; the problem is figuring out how to make the data visually appealing.
If  the data visualization solves a client’s problem, it’s always attractive, whether it’s something really elaborate and visually stimulating or whether it’s just a plain ol’ table of  numbers.
Making good data visualizations is just like making any sort of  good data analysis.
Oh, really? Do you think it’s your job as a data analyst to create an aesthetic experience for your clients?
To build good visualizations, first identify what are the fundamental comparisons that will address your client’s objectives.
While New Army has more data than these three sheets, these sheets have the comparisons that will speak directly to what they want to know.
What we want to see is which stylesheet or stylesheets maximize revenue, the time our visitors spend on the site, and return visits to the site.
Take look at the statistics that describe the results for Home Page #1
Plot dots to represent each of the users on the axes below.
Use your spreadsheet’s average formula (AVG) to calculate the average Revenue and TimeOnSite figures for Home Page #1, and draw those numbers as horizontal and vertical lines on the chart.
How do the results you see compare to their goals for revenue and time on site?
This value represents the New Army’s goals for the average number of minutes each user spends on the website.
This value represents the goal New Army has for the average amount of money each user spends.
How did you visualize the Revenue and TimeOnSite variables for Home Page #1?
Here’s the average amount of time spent on the website.
How do the results you see compare to their goals for revenue and time on site?
On average, the time people spend looking at the website under Home Page #1
Your visualization is already more useful than the rejected ones.
Now that’s a nice chart, and it’ll definitely be useful to your client.
It’s an example of  a good data visualization because it…
So what kind of chart is that? And what can you actually do with it?
You don’t have to prove that the value of  the independent variable causes the value of  the dependent variable, because after all we’re exploring the data.
Scatterplots are great tools for exploratory data analysis, which is the term statisticians use to describe looking around in a set of  data for hypotheses to test.
Analysts like to use scatterplots when searching for causal relationships, where one variable is affecting the other.
As a general rule, the horizontal x-axis of  the scatterplot represents the independent variable (the variable we imagine to be a cause), and the vertical y-axis of  a scatterplot represents the dependent variable (which we imagine to be the effect)
Each of these dots represents an observation, in this case a user on the website.
It’s a good idea to use little circles for your scatterplots, because they’re easier to see when they overlap than dots.
That’s cool, but there is a lot more data than two variables, and a lot more comparisons to be made.
How would you make the scatterplot visualization you’ve created more multivariate?
A visualization is multivariate if  it compares three or more variables.
And because making good comparisons is fundamental to data analysis, making your visualizations as multivariate as possible makes it most likely that you’ll make the best comparisons.
And in this case you’ve got a bunch of  variables.
The solid lines are the averages for that home page.
One way of  making your visualization more multivariate is just to show a bunch of  similar scatterplots right next to each other, and here’s an example of  such a visualization.
All of  your variables are plotted together in this format, which enables you to compare a huge array of  information right in one place.
Because New Army is really interested in revenue comparisons, we can just stick with the charts that compare TimeOnSite, Pageviews, and ReturnVisits to revenue.
This graphic was created with a open source software program called R, which you’ll learn more about later.
Look at it and think about what it tells you about the stylesheets that New Army decided to test.
Do you think that this visualization does a good job of showing the data? Why or why not?
Just looking at the dots, you can see that Home Page #2 has a very different sort of spread from the other two stylesheets.
What do you think is happening with Home Page #2?
Which of the three stylesheets do you think does the best job of maximizing the variables that New Army cares about? Why?
Does the new visualization help you understand the comparative performance of the stylesheets?
Do you think that this visualization does a good job of showing the data? Why or why not?
Each dot on each of the nine panels represents the experience of a single user, so even.
Just looking at the dots, you can see that Home Page #2 has a very different sort of spread from the other two stylesheets.
What do you think is happening with Home Page #2?
Which of the three stylesheets do you think does the best job of maximizing the variables that New Army cares about? Why?
While #1 performs above average when it comes to the metrics besides.
When it comes to Return Visits, #1 is ahead, and.
Q: What software tool should I use to create this sort of graphic?
A: Those specific graphs are created in a statistical data analysis program called R, which you’re going to learn all about later in the book.
But there are a number of charting tools you can use in statistical programs, and you don’t even have to stop there.
You can use illustration programs like Adobe Illustrator and just draw visualizations, if you have visual ideas that other software tools don’t implement.
They have a limited range of charting tools you can use, and you can probably figure out a way to create a chart like this one in your spreadsheet program, but it’s going to be an uphill battle.
Q: You don’t sound too hot on spreadsheet data visualizations.
A: Many serious data analysts who use spreadsheets all the time for basic calculations and lists nevertheless wouldn’t dream of using spreadsheet charting tools.
They can be a real pain: not only is there a small range of charts you can create in spreadsheet programs, but often, the programs force you into formatting decisions that you might not otherwise make.
It’s not that you can’t make good data graphics in spreadsheet programs; it’s just that there’s more trouble in it than you’d have if you learned how to use a program like R.
Q: So if I’m looking for inspiration on chart types, the spreadsheet menus aren’t the place to look?
A: No, no, no! If you want inspiration on designs, you should probably pick up some books by Edward Tufte, who’s the authority on data visualization by a long shot.
His body of work is like a museum of excellent data visualizations, which he sometimes calls “cognitive art.”
Q: What about magazine, newspapers, and journal articles? A: It’s a good idea to become sensitive to data visualization quality in publications.
Some are better than others when it comes to designing illuminating visualizations, and when you pay attention to the publications, over time, you’ll get a sense of which ones do a better job.
A good way to start would be to count the variables in a graphic.
If there are three or more variables in a chart, the publication is more likely to be making intelligent comparisons than if there’s one variable to a chart.
Q: What should I make of data visualizations that are complex and artistic but not analytically useful?
A: There’s a lot of enthusiasm and creativity nowadays for creating new computer-generated visualizations.
Some of them facilitate good analytical thinking about the data, and some of them are just interesting to look at.
There’s absolutely nothing wrong with what some call data art.
Just don’t call it data analysis unless you can directly use it to achieve a greater understanding of the underlying data.
Q: So something can be visually interesting without being analytically illuminating.
But if you have something at stake in an analysis, and your visualization is illuminating, then it’s hard to imagine that the graphic wouldn’t be visually interesting!
In order to make his website as powerful as possible, he needs some idea of  why people interact with the different home pages the way they do.
And, since he’s the client, we definitely need to address the theories he put forward.
The visualization is great, but the web guru’s not satisfied yet.
Your designs are excellent and we’re pleased we switched to you from the other guy.
But tell me something: why does Home Page #3 perform so much better than the others?
All this looks really reasonable, but I still want to know why we have these results.
First, I think that Home Page #3 loads faster, which makes the experience of the website more snappy.
Second, I think that its cooler color palette is really relaxing and makes for a good shopping experience.
Looks like your client has some ideas of his own about why the data looks the way it looks.Here’s a.
You just got an email from your client, the web guru at New Army, assessing what you created for him.
Your and your client’s preferred model will usually fit the data.
But there are always other possibilities, especially when you are willing to get imaginative about the explanations.
You need to address alternative causal models or explanations as you describe your data visualization.
Doing so is a real mark of  integrity: it shows your client that you’re not just showing the version of  the story that you like best: you’re thinking through possible failure points in your theories.
This model represents your favorite  hypothesis or explanation of the data.
Of course the model fits… that’s why it seems most plausible to you.
On the surface, this model has a different shape, but it totally accommodates the data.
If the data is true, this model can’t be true.
We haven’t taken a look at the data yet to see for sure.
As for the cooler color palette, we kind of doubt it.
There’s research to show that people react differently, but none of it has really persuaded us.
We better take a look at the data to see whether it confirms or disconfirms these hypotheses.
The experiment designers saw the web guru’s theories and sent you some of  their thoughts.
Perhaps their input will enable you to evaluate the web guru’s hypotheses about why some home pages performed better than others.
Here’s what the experiment designers think about the first hypothesis.
Let’s take a look at the data to see whether the bosses hypotheses fit.
How well did you find the web guru’s hypotheses to fit the data?
We’re delighted to hear that #3 is the best, but we really don’t know why.
Who knows what people are thinking? But that is actually OK: as long as we’re showing improvement on the business fundamentals, we don’t need to understand people in a deep way.
Still, it’s interesting to learn as much as we can.
The stylesheets are really different from each other in many ways.
So when it comes to isolating individual features that might account for the performance differential, it’s hard.
In the future, we’d like to take Home Page #3 and test a bunch of subtle permutations.
That way, we might learn things like how button shape or font choice affect user behavior.
We use fonts and a layout that are easy on the eyes.
You can find pretty much everything in three clicks, when for Home Page #1 it takes you more like seven clicks to find what you want.
Both could be affecting our revenue, but we need more testing to say for sure.
They’ve had an opportunity to take a look at your scatterplots and sent you some of  their own thinking about what’s going on.
These people are data junkies, and their hypotheses definitely fit.
On the basis of what you’ve learned, what would you recommend to your client that he do regarding his web strategy?
I agree with your assessments of the hypotheses and your recommendation.
What would you tell your client to do with his website on the bases of the data you visualized and the explanatory theories you evaluated?
Stick with Home Page #3 and test for finer-grained elements of the user’s experience, like variable.
There are a bunch of different possible explanations for #3’s.
You created an excellent visualization that enabled New Army to quickly and simultaneously assess all the variables they tested in their experiment.
And you evaluated that visualization in light of  a bunch of  different hypotheses, giving them some excellent ideas about what to test for in the future.
Orders are coming in from everywhere! Because of  the new website, traffic is greater than ever.
Your visualization of the experimental results showed what they needed to know to spruce up their website.
Even better, New Army has embarked on a continuous program of experimentation to fine-tune their new design, using your visualization to see what works.
And it can be fiendishly difficult when you have to deal with complex, heterogeneous data.
This is why analysts don’t just take the obvious explanations.
Gimme some skin… You’re with ElectroSkinny, a maker of phone skins.
Your assignment is to figure out whether PodPhone is going to release a new phone next month.
PodPhone is a huge product, and there’s a lot at stake.
PodPhone will release a phone at some point in the future, and ElectroSkinny needs to start manufacturing skins a month before the phone is released in order to get in on the first wave of phone sales.
If  they don’t have skins ready for a release, their competitors will beat them to the punch and sell a lot of  skins before ElectroSkinny can put their own on the market.
But if  they manufacture skins and PodPhone isn’t released, they’ll have wasted money on skins that no one knows when they’ll be able to sell.
With my active lifestyle, I need a great skin for my PodPhone, that’s why I’m all about ElectroSkinny!
What sort of data or information would help you get started on this analytical problem?
The decision of  when to start manufacturing a new line of  skins is a big deal.
PodPhone releases are always a surprise, so ElectroSkinny has to figure out when they’re about to happen.
If  they can start manufacturing a month before a PodPhone release, they’re in great shape.
If there’s a delay, but ElectroSkinny hasn’t started manufacturing, they’re in great shape.
What do you need to know in order to get started?
PodPhone wants their releases to be a surprise, so they’ll probably take measures to avoid letting.
We’ll need some sort of insight into how they think.
You need to figure out how to compare the data you do have with your hypotheses about when PodPhone will release their new phone.
But first, let’s take a look at the key pieces of  information we do have about PodPhone…
PodPhone takes surprise seriously: they really don’t want you to know what they’re up to.
So you can’t just look at publicly available data and expect an answer of  when they’re releasing the PodPhone to pop out at you.
These data points really aren’t going to be of much help…
PodPhone knows you’ll see all this information, so they won’t want any of it to let on their release date.
Do you think her hypothesis makes sense in light of the above evidence we have to consider?
PodPhone has invested more in the new phone than any other company ever has.
There is going to be a huge increase in features compared to competitor phones.
There was just a big new phone released from a competitor.
The economy and consumer spending are both up, so it’s a good time to sell phones.
There is a rumor that the PodPhone CEO said there’d be no release for a year.
Here’s what little information ElectroSkinny has been able to piece together about the release.
Some of  it is publicly available, some of  it is secret, and some of  it is rumor.
Internally, we don’t expect a release, because their product line is really strong.
They’ll want to ride out their success with this line as long as possible.
The CEO has a pretty straightforward account of  stepby-step thinking on the part of  PodPhone.
This model or hypothesis fits the evidence, because there is nothing in the evidence that proves the model wrong.
Of  course, there is nothing in the evidence that strongly supports the model either.
PodPhone has invested more in the new phone than any other company ever has.
There is going to be a huge increase in features compared to competitor phones.
There was just a big new phone released from a competitor.
The economy and consumer spending are both up, so it’s a good time to sell phones.
There is a rumor that the PodPhone CEO said there’d be no release for a year.
PodPhone will want to ride out their success for now.
Here’s what the ElectroSkinny CEO thinks is going to be PodPhone’s thinking.
We want to time our releases to maximize sales and to.
First, we watch the economy, because an i ncrease in.
We don’t usually want to release a phone when they have.
ElectroSkinny watches PodPhone really closely, and sometimes stuff  like this just falls in your lap.
This strategy memo outlines a number of  the factors that PodPhone considers when it’s calculating its release dates.
It’s quite a bit more subtle than the reasoning the ElectroSkinny CEO imagined they are using.
Put a “+” in each circle if the two variables rise and fall together.
Write a “–” sign if the variables move in opposite directions.
Think carefully about how PodPhone thinks the variables mentioned in the memo relate.
Do the pairs below rise and fall together, or do they go in opposite directions? Write a “+” or “–” in each circle depending on your answer.
Can this memo help you figure out when a new PodPhone will be released?
In the mind of PodPhone, how are the pairs of variables below linked to each other quantitatively?
If a competitor has a recent product release, PodPhone avoids releasing.
Every phone PodPhone sells is a phone that their competitor doesn’t sell, and vice versa.
Here are a few of the other relationships that can be read from PodPhone’s strategy memo.
When you are looking at data variables, it’s a good idea to ask whether they are positively linked, where more of  one means more of  the other (and vice versa), or negatively linked, where more of one means less of  the other.
On the right are some more of  the relationships PodPhone sees.
How can you use these relationships to develop a bigger model of  their beliefs, one that might predict when they’re going to release their new phone?
Let’s tie those positive and negative links between variables into an integrated model.
Using the relationships specified on the facing page, draw a network that incorporates all of them.
How does your model of PodPhone’s worldview look once you’ve put it in the form of a network?
PodPhone seems to be watching the interaction of a lot of variables.
One of these can’t really change without affecting all the other variables.
A linear explanation of  the causes for why PodPhone might decide to delay their release is simple and straightforward.
PodPhone will want to ride out their success for now.
PodPhone realizes that they are making decisions in the context of  an active, volatile, interlinked system.
As an analyst, you need to see beyond simple models like this and expect to see causal networks.
In the real world causes propagate across a network of related variables… why should your models be any different?
PodPhone’s strategy memo suggests that their thinking is more complex than this.
So how do we use that to figure out when PodPhone is going to release their new phone? What about the data?
Hypothesize PodPhone’s options Sooner or later, PodPhone is going to release a new phone.
And different answers to that question are your hypotheses for this analysis.
Below are a few options that specify when a release might occur, and picking the right hypothesis is what ElectroSkinny needs you to do.
PodPhone has invested more in the new phone than any other company ever has.
There is going to be a huge increase in features compared to competitor phones.
There was just a big new phone released from a competitor.
The economy and consumer spending are both up, so it’s a good time to sell phones.
There is a rumor that the PodPhone CEO said there’d be no release for a year.
Here are a few estimates of when the new PodPhone might be released.
The hypothesis that we consider strongest will determine ElectroSkinny’s manufacturing schedule.
You’ll somehow combine your hypotheses with this evidence and PodPhone’s mental model to get your answer.
You have what you need to run a hypothesis test Between your understanding of  PodPhone’s mental model and the evidence, you have amassed quite a bit of  knowledge about the issue that ElectroSkinny cares about most: when PodPhone is going to release their product.
You just need a method to put all this intelligence together and form a solid prediction.
But how do we do it? We’ve already  seen how complex this problem is… with all that complexity how can we possibly pick the right hypothesis?
Falsification is the heart of hypothesis testing Don’t try to pick the right hypothesis; just eliminate the disconfirmed hypotheses.
This is the method of  falsification, which is fundamental to hypothesis testing.
Picking the first hypothesis that seems best is called satisficing and looks like this:
Satisficing is really simple: it’s picking the first option without ruling out the others.
It looks like both satisficing and falsification get you the same answer, right? They don’t always.
The big problem with satisficing is that when people pick a hypothesis without thoroughly analyzing the alternatives, they often stick with it even as evidence piles up against it.
Falsification enables you to have a more nimble perspective on your hypotheses and avoid a huge cognitive trap.
Use falsification in hypothesis testing and avoid the danger of satisficing.
Give falsification a try and cross out any hypotheses that are falsified by the evidence below.
PodPhone has invested more in the new phone than any other company ever has.
There is going to be a huge increase in features compared to competitor phones.
There was just a big new phone released from a competitor.
The economy and consumer spending are both up, so it’s a good time to sell phones.
There is a rumor that the PodPhone CEO said there’d be no release for a year.
Why do you believe that the hypotheses you picked are falsified by the evidence?
PodPhone has invested more in the new phone than any other company ever has.
There is going to be a huge increase in features compared to competitor phones.
There was just a big new phone released from a competitor.
The economy and consumer spending are both up, so it’s a good time to sell phones.
There is a rumor that the PodPhone CEO said there’d be no release for a year.
Why do you believe that the hypotheses you picked are falsified by the evidence?
H1 is definitely falsified by the evidence, because the CEO has gone on record saying that there was no way.
The CEO might be lying, but that would be so weird that we can still rule out H1
Q: Falsification seems like a really elaborate way to think about analyzing situations.
A: It’s a great way to overcome the natural tendency to focus on the wrong answer and ignore alternative explanations.
By forcing you to think in a really formal way, you’ll be less likely to make mistakes that stem from your ignorance of important features of a situation.
Q: How does this sort of falsification relate to statistical hypothesis testing?
The idea is to identify a situation that, if true, would make the null hypothesis darn near impossible.
A: One of the virtues of this approach is that it enables you to aggregate.
This method is falsification in a very general form, which makes it useful for very complex problems.
But it’s definitely a good idea to bone up on “frequentist” hypothesis testing described above, because for tests where the data fit its parameters, you would not want to use anything else.
Q: I think that if my coworkers saw me reasoning like this they’d think I was crazy.
A: They certainly won’t think you’re crazy if you catch something really important.
The aspiration of good data analysts is to uncover unintuitive answers to complex problems.
Would you hire a conventionally minded data analyst? If you are really interested in learning something new about your data, you’ll go for the person who thinks outside the box!
Q: It seems like not all hypotheses could be falsified definitively.
Like certain evidence might count against a hypothesis without disproving it.
Q: Where’s the data in all this? I’d expect to see a lot more numbers.
Falsification in hypothesis testing lets you take a more expansive view of “data” and aggregate a lot of heterogeneous data.
You can put virtually any sort of data into the falsification framework.
Q: What’s the difference between using falsification to solve a problem and using optimization to solve it?
In certain situations, you’ll want to break out Solver to tweak your variables until you have the optimal values, and in other situations, you’ll want to use falsification to eliminate possible explanations of your data.
What if I can’t use falsification to eliminate all the hypotheses?
Nice work! I definitely know more now than I did when I brought you on board.
But can you do even better than this? What about eliminating two more?
You know that it’s a bad idea to pick the one that looks like it has the most support, and falsification has helped you eliminate only two of  the hypotheses, so what should you do now?
Which one of these will you ultimately consider to be the strongest?
Compare each hypothesis to the evidence and pick the one that has the most confirmation.
Just present all of the hypotheses and let the client decide whether to start manufacturing skins.
Use the evidence to rank hypotheses in the order of which has the fewest evidence-based knocks against it.
Compare each hypothesis to the evidence and pick the one that has the most confirmation.
The problem is that the information I have is incomplete.
Just present all of the hypotheses and let the client decide whether to start manufacturing skins.
This is certainly an option, but the problem with it is that I’m not really taking any responsibility.
In other words, I’m not really acting as a data analyst as much as someone who.
Use the evidence to rank hypotheses in the order of which has the fewest evidence-based knocks against it.
I’ve already used falsification to rule out things that I’m sure can’t be true.
Now, even though I can’t rule out my remaining hypotheses, I can still use the evidence to see which.
Did you pick a hypothesis elimination technique that you like best?
Diagnosticity is the ability of evidence to help you assess the relative likelihood of the hypotheses.
If evidence is diagnostic, it helps you rank your hypotheses.
By putting the hypothesis that seems strongest at the top of the list, don’t we run the risk of satisficing and picking the one we like rather than.
Not if you compare your evidence to your hypotheses by looking at its diagnosticity.
Evidence is diagnostic if  it helps you rank one hypothesis as stronger than another, and so our method will be to look at each hypothesis in comparison to each piece of  evidence and each other and see which has the strongest support.
Diagnosticity helps you find the hypothesis with the least disconfirmation Evidence and data are diagnostic if  they help you assess the relative strengths of  hypotheses.
The tables below compare different pieces of  evidence with several hypotheses.
The “+” symbol indicates that the evidence supports that hypothesis, while the “–” symbol indicates that the evidence counts against the hypothesis.
In the second table, on the other hand, the evidence is not diagnostic.
When you are hypothesis testing, it’s important to identify and seek out diagnostic evidence.
It might seem like an otherwise interesting piece of evidence, but unless it helps us rank our hypotheses, it’s not of much use.
The weights you assign to these values are analytically rigorous but subjective, so use your best judgment.
Take a close look at your evidence in comparison to each of your hypotheses.
Use the plus and minus notation to rank hypotheses with diagnosticity.
The investment from PodPhone is the biggest investment in new phone tech ever.
There is going to be a huge increase in features compared to competitor phones.
Rumor: PodPhone CEO said there’d be no release this year.
Say whether each piece of evidence supports or hurts each hypothesis.1
Say whether each piece of  evidence supports or hurts each hypothesis.1
The investment from PodPhone is the biggest investment in new phone tech ever.
There is going to be a huge increase in features compared to competitor phones.
Rumor: PodPhone CEO said there’d be no release this year.
The first three pieces of evidence are not diagnostic and can be ignored from this point onward.
PodPhone tries to avoid going head-to-head with a competitor’s new phone, as you learned.
The economy could be worse in a year from now, so a strong economy speaks in favor of the release being sooner.
In six months, the competitor’s new phone might have faded in popularity, so it’d be time for PodPhone to make a move.
You can’t rule out all the hypotheses, but you can say which is strongest.
While the evidence you have at your disposal doesn’t enable you to rule out all hypotheses but one, you can take the three remaining and figure out which one has the least disconfirmation from the evidence.
That hypothesis is going to be your best bet until you know more.
I’m completely persuaded by this and have decided not to manufacture skins in the short term.
Hopefully, something will come up that will let us figure out for sure whether it’ll happen in six months or at some point afterward.
Too bad we couldn’t start manufacturing… it’d be exciting and lucrative to be out in front of a PodPhone launch.
You just got a picture message… Your coworker saw this crew of  PodPhone employees at a restaurant just now.
Everyone’s passing around new phones, and although your contact can’t get close enough to see one, he thinks it might be the one.
You can add this new information to your hypothesis test and then run it again.
Maybe this information will help you distinguish among your hypotheses even further.
Why would all these PodPhone employees be out having a bash at a restaurant?
Passing around phones? Everyone’s seen mock-ups, but why throw a party for mock-ups?
Do your hypothesis test again, this time with the new evidence.
Does this new evidence change your assessment of whether PodPhone is about to announce its new phone (and whether ElectroSkinny should start manufacturing)?
There was just a big new phone released from a competitor.
Rumor: PodPhone CEO said there’d be no release this year.
Did your new evidence change your ideas about the relative strengths of your hypotheses? How?
Does this new evidence change your assessment of whether PodPhone is about to announce its new phone (and whether ElectroSkinny should start manufacturing)?
It’s kind of hard to imagine that the team would be celebrating and passing around.
There was just a big new phone released from a competitor.
There is a rumor that CEO isn’t going to release this year at all.
The development team is seen having a huge celebration, holding new phones.
Thanks to you, we totally saw that launch coming and were ready for it with a bunch of awesome new skins.
What’s more, our competitors all thought PodPhone wasn’t going to release a new phone, so we were the only ones ready and now we’re cleaning up!
Your analysis was spot on, and ElectroSkinny was had a line of  cool new skins for the new model of  the PodPhone.
Get past first base He says he’s not like the other guys, but how different is he exactly?
And you need to make sure that every analysis you do incorporates the data you have.
Your doctor is convinced that you have it, but because you’ve become so handy with data, you might want to take a look at the test and see just how accurate it is.
Your doctor has given you a diagnosis of  lizard flu.
The good news is that lizard flu is not fatal and, if  you have it, you’re in for a full recovery after a few weeks of  treatment.
The bad news is that lizard flu is a big pain in the butt.
You’ll have to miss work, and you will have to stay away from your loved ones for a few weeks.
Lizard flu is a tropical disease first observed among lizard researchers in South America.
The disease is highly contagious, and affected patients need to be quarantined in their homes for no fewer than six weeks.
Patients diagnosed with lizard flu have been known to “taste the air” and in extreme cases have developed temporary chromatophores and zygodactylous feet.
A quick web search on the lizard flu diagnostic test has yielded this result: an analysis of the test’s accuracy.
In light of this information, what do you think is the probability that you have lizard flu? How did you come to your decision?
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
You just looked at some data on the efficacy of the lizard flu diagnostic test.
What did you decide were the chances that you have the disease?
In light of this information, what do you think is the probability that you have lizard flu? How did you come to your decision?
It looks like the chances would be 90% if I had the disease.
So I should revise my estimate down a little bit.
Not only is 75% the wrong answer, but it’s not anywhere near the right.
And if you started making decisions with the idea that there’s a 75% chance you have lizard flu, you’d be making an even bigger mistake!
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
There is so much at stake in getting the answer to this question correct.
We are totally going to get to the bottom of  this…
Take a closer look at the second statement and answer the questions below.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
Let’s take the accuracy analysis one claim at a time.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
Lizard flu diagnostic test Accuracy analysisThere are two different and obviously important.
So let’s imagine two different worlds, one where a lot of  people have lizard flu and one where few people have it, and then look at the claim about “positive” scores for people who don’t have lizard flu.
Does the number of people who have the disease affect how many people are wrongly told that they test positive?
At least when it comes to situations where people who don’t have lizard flu test positively, it seems that the prevalence of  lizard flu in the general population makes a big difference.
In fact, unless we know how many people already have lizard flu, in addition to the accuracy analysis of  the test, we simply cannot figure out how likely it is that you have lizard flu.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
We need more data to make sense of that diagnostic test…
You’ve been counting false positives In the previous exercise, you counted the number of  people who falsely got a positive result.
The opposite of a false positive is a true negative.
If someone doesn’t have lizard flu, the probability that the test returns negative for it is 91%
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
In addition to keeping tabs on false positives, you’ve also been thinking about true negatives.
True negatives are situations where people who don’t have the disease get a negative test result.
If you don’t have lizard flu, the test result is either false positive or true negative.
What term do you think describes this statement, and what do you think is the opposite of this statement?
And here’s Scenario 2, where few people have the disease.
Here’s Scenario 1, where lots of people have lizard flu.
Let’s take a look at what each symbol means in this statement:
The probability of  lizard flu given a positive test result.
A conditional probability in the probability of  some event, given that some other event has happened.
Assuming that someone tests positive, what are the chances that he has lizard flu?
Here’s how the statements you’ve been using look in conditional probability notation:
If someone has lizard flu, the probability that he tests negative for it is 10%
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
What term would you use to describe the other part of the lizard flu diagnostic test?
This is the probability of a positive test result, given lizard flu.
This is the probability of a positive test result, given that the person doesn’t have lizard flu.
The tilde symbol means that the statement (L) is not true.
But first you need to know how many people have lizard flu.
Then you can use these percentages to calculate how many people actually fall into these categories.
Figuring out your probability of  having lizard flu is all about knowing how many actual people are represented by these figures.
P(+|~L), the probability at someone tests positive, given that they don’t have lizard flu.
P(+|L), , the probability at someone tests positive, given that they do have lizard flu.
P(-|L), the probability at someone tests negative, given that they do have lizard flu.
P(-|~L), the probability at someone tests negative given that they don’t have lizard flu.
What is the probability of lizard flu, given a positive test result?
How many actual people fit into each of these probability groupings?
Here’s the number you need in order to interpret your test.
Turns out that 1 percent of  the population has lizard flu.
But as a percentage of  the overall population, it’s a pretty small number.
Prior to learning anything new about individuals because of the test, you know that only 1 percent of  the population has lizard flu.
You might not have base rate data in every case, but if  you do have a base rate and don’t use it, you’ll fall victim to the base rate fallacy, where you ignore your prior data and make the wrong decisions because of  it.
I just thought that the 90% true positive rate meant it’s really likely that you have it!
Study finds that 1 percent of national  population has lizard flu.
Assuming you start with 1,000 people, fill in the blanks, dividing them into groups according to your base rates and the specs of the test.
The probability that you have it, given that you tested negative.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
What did you calculate your new probability of having lizard flu to be?
The probability that you have it, given that you tested negative.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
Your chances of having lizard flu are still pretty low.
You’re either a true positive or a false positive, and it’s a lot more likely that you’re a false positive.
Bayes’ rule manages your base rates when you get new data.
Believe it or not, you just did a commonsense implementation of  Bayes’ rule, an incredibly powerful statistical formula that enables you to use base rates along with your conditional probabilities to estimate new conditional probabilities.
If  you wanted to make the same calculation algebraically, you could use this monster of  a formula:
This formula will give you the same result you just calculated.
The true positive rate The base rate (people who have the disease)
The base rate (people who don’t have the disease) The false positive rate.
The probability of lizard flu given a positive test result.
You’ve got tools in here for dealing with whole numbers.
When you imagined that you were looking at 1,000 people, you switched from decimal probabilities to whole numbers.
Because our brains aren’t innately well-equipped to process numerical probabilities, converting probabilities to whole numbers and then thinking through them is a very effective way to avoid making mistakes.
Bayes’ rule is an important tool in data analysis, because it provides a precise way of  incorporating new information into your analyses.
You’re still nine times more likely to have lizard flu than other people.
Yep, you’re 9x more likely to have lizard flu than the regular population.
Lizard flu is a tropical disease first observed among lizard researchers in South America.
The disease is highly contagious, and affected patients need to be quarantined in their homes for no fewer than six weeks.
Patients diagnosed with lizard flu have been known to “taste the air” and in extreme cases have developed temporary chromatophores and zygodactylous feet.
The doctor ordered a slightly different test: the “advanced” lizard flu diagnostic test.
The doctor didn’t order you the more powerful, advanced lizard flu test the first time because it’s kind of  expensive, but now that you tested positively on the first (cheaper, less accurate) test, it’s time to bring out the big guns…
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 1 percent.
If someone has lizard flu, the probability that the test returns positive for it is 99 percent.
Should we use the same base rate as before? You tested.
Using your base rate, you can use the new test’s statistics to calculate the new probability that you have lizard flu.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 9 percent.
If someone has lizard flu, the probability that the test returns positive for it is 90 percent.
But you learned from the test that your probability of  having lizard flu is higher than the base rate.
That probability is your new base rate, because now you’re part of  the group of  people who’ve tested positively.
When you got your first test results back, you used as your base rate the incidence in the population of everybody for lizard flu.
Using the new test and your revised base rate, let’s calculate the probability that you have lizard flu given your results.
The probability that you have it, given that you tested negative.
Remember, 9% of people like you will have lizard flu.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 1 percent.
If someone has lizard flu, the probability that the test returns positive for it is 99 percent.
What do you calculate your new probability of having lizard flu to be?
The probability that you have it, given that you tested negative.
If someone doesn’t have lizard flu, the probability that the test returns positive for it is 1 percent.
If someone has lizard flu, the probability that the test returns positive for it is 99 percent.
What a relief! You took control of  the probabilities using Bayes’ rule and now know how to manage base rates.
The only way to avoid the base rate fallacy is always to be on the lookout for base rates and to be sure to incorporate them into your analyses.
Your probability of having lizard flu is so low that you can pretty much rule it out.
But only if those numbers describe your own mental states, expressing your.
Subjective probability is a straightforward way of injecting some real rigor into.
Backwater Investments needs your help Backwater Investments is a business that tries to make money by seeking out obscure investments in developing markets.
They pick investments that other people have a hard time understanding or even finding.
Their strategy means that they rely heavily on the expertise of  their analysts, who need to have impeccable judgment and good connections to be able to get BI the information they need for good investment decisions.
It’s a cool business, except it’s about to be torn apart by arguments among the analysts.
The disagreements are so acrimonious that everyone’s about to quit, which would be a disaster for the fund.
The analysts at BI are having big disagreements over a number of  geopolitical trends.
And this is a big problem for the people trying to set investment strategy based on their analyses.
There are a bunch of  different issues that are causing splits.
The analysts are in full revolt! If I don’t get them agreeing on something, they’ll all quit.
Where precisely are the disagreements? It would be really great if  you could help figure out the scope of  the dispute and help achieve a consensus among the analysts.
Or, at the very least, it’d be nice if  you could specify the disagreements in a way that will let the BI bosses figure out where they stand.
Take a look at these emails, which the analysts have sent you.
For the past six months, I’ve consistently argued to the staff that the Vietnamese government is probably going to reduce its taxes this year.
And everything that we’ve seen from our people on the ground and in news reports confirms this.
Yet others in the “analytical” community at BI seem to think this is crazy.
I’m a considered dreamer by the higher-ups and told that such a gesture or the part of the government is “highly unlikely.” Well, what do they base this assessment on? Clearly the government is encouraging foreign investment.
I’ll tell you this: if taxes go down, there will be a flood of private investment, and.
The community at BI has become obsessed with these three places.
Yet aren’t the answers to all our questions abundantly clear? Russia will continue to subsidize oil next quarter like it always has, and they’re more likely than not to buy EuroAir next quarter.
Vietnam might decrease taxes this year, and they probably aren’t going to encourage foreign investment.
Indonesia will more likely than not invest in ecotourism this year, but it won’t be of much help.
If BI doesn’t fire the dissenters and troublemakers who dispute these truths, the firm might as well close…
The authors each use a bunch of words to describe what they think the likelihoods of various events are.
From: VP, Economic Research, Backwater Investments To: Head First Subject: Have these people ever even been to Russia? While the analytic stuff in the Economic division has continued to flourish and produce quality work on Russian business and government, the rest of BI has shown a shocking ignorance of the internal dynamics of Russia.
It’s quite unlikely that Russia will puchase EuroAir, and their support of the oil industry next quarter may be the most tentative it’s ever been…
You need to stop listening to the eggheads back at.
Even a top manager is starting to lose his cool!
This guy’s writing from the field, where he’s doing firsthand research.
What are your impressions of the arguments, now that you’ve read the analysts’ emails?
The authors use a bunch of words to describe what they think the likelihoods of various events are.
For the past six months, I’ve consistently argued to the staff that the Vietnamese government is probably going to reduce its taxes this year.
And everything that we’ve seen from our people on the ground and in news reports confirms this.
Yet others in the “analytical” community at BI seem to think this is crazy.
I’m a dreamer by the  higherups and told that such a gesture or the part of the government is “highly unlikely.” Well, what do they base this assessment on? Clearly the government is encouraging foreign investment.
I’ll tell you this: if taxes go down, there will be a flood of private.
From: VP, Economic Research, Backwater Investments To: Head First Subject: Have these people ever even been to Russia? While the analytic stuff in the Economic division has continued to flourish and produce quality work on Russian business and government, the rest of BI has shown a shocking ignorance of the internal dynamics of Russia.
It’s quite unlikely that Russia will puchase EuroAir, and their support of the oil industry next quarter may be the most tentative it’s ever been…
The community at BI has become obsessed with these three places.
Yet aren’t the answers to all our questions abundantly clear? Russia will continue to subsidize oil next quarter like it always has, and they’re more likely than not to buy EuroAir next quarter.
Vietnam might decrease taxes this year, and they probably aren’t going to encourage foreign investment.
Indonesia will more likely than not invest in ecotourism this year, but it won’t be of much help.
If BI doesn’t fire the dissenters and troublemakers who dispute these truths, the firm might as well close…
You need to stop listening to the eggheads back at.
There are a bunch of probability words used in these emails…
Jim: So we’re supposed to come in and tell everyone who’s right and who’s wrong? That shouldn’t be a problem.
They’re highly trained, highly experienced, serious domain experts when it comes to these countries.
The CEO says they have all the data they could ever hope for.
They have access to the best information in the world.
They pay for proprietary data, they have people digging through government sources, and they have people on the ground doing firsthand reporting.
They’re predicting single events that don’t have a big trail of  numerical frequency data that you can just look at and use to make more predictions.
They’re aggregating data from a bunch of  sources and making very highly educated guesses.
Jim: Then what you’re saying is that these guys are smarter than we are, and that there is really nothing we can do to fix these arguments.
Joe: Providing our own data analysis would be just adding more screaming to the argument.
Frank: Actually, all the arguments involve hypotheses about what’s going to happen in the various countries, and the analysts really get upset when it comes to those probability words.
Jim: So you want to help them find better words to describe their feelings? Gosh, that sounds like a waste of  time.
We need to find something that will give these judgments more precision, even though they’re someone’s subjective beliefs… How would you.
When you assign a numerical probability to your degree of  belief  in something, you’re specifying a subjective probability.
Subjective probabilities are a great way to apply discipline to an analysis, especially when you are predicting single events that lack hard data to describe what happened previously under identical conditions.
These figures are much more precise than the words the analysts used to describe their beliefs.
Continued Russian support of the oil industry is highly probable.
I believe there is a 60% chance that Russia will continue to support the oil industry.
Well, I’d say that there is a 40% chance that it’ll happen.
Well, I’d say there is a 35% chance it’ll happen.
Sketch an outline of a spreadsheet that would contain all the subjective probabilities you need from your analysts.
What you want is a subjective probability from each analyst for each of the key areas of dispute.
What does the spreadsheet you want from the analysts to describe their subjective probabilities look like?
The table will take each of the six statements and list them at the top.
We can fill in the blanks of what each analyst thinks about each statement here.
While you haven’t yet figured out how to resolve all their differences, you have definitely succeeded at showing where exactly the disagreements lie.
And from the looks of  some of  the data, it might not be that there is all that much disagreement after all, at least not on some issues.
This analyst from BI is starting to look a little more upbeat!
Let’s see what the CEO has to say about this data…
What we’ve asked you to do is resolve the disagreements among our analysts, and this just seems like a fancy way of listing the disagreements.
What we need you to do is resolve them or at least deal with them in a way that will let us get a better idea of how to structure our investment portfolio in spite of them.
You should defend your choice of subjective probabilities as a tool for analysis here.
The CEO doesn’t see what you’re up to It appears that he doesn’t think these results provide anything that can be used to resolve the disagreements among the analysts.
You should probably explain and defend your reason for collecting this data to the CEO…
The community at BI has become obsessed with these three places.
Yet aren’t the answers to all our questions abundantly clear? Russia will continue to subsidize oil next quarter like it always has, and they’re more likely than not to buy EuroAir next quarter.
Vietnam might decrease taxes this year, and they probably aren’t going to encourage foreign investment.
Indonesia will more likely than not invest in ecotourism this year, but it won’t be of much help.
If BI doesn’t fire the dissenters and troublemakers who dispute these truths, the firm might as well close…
For the past six months I’ve consistently argued to the staff that the Vietnamese government is probably going to reduce its taxes this year.
And everything that we’ve seen from our people on the ground and in news reports confirms this.
Yet others in the “analytical” community at BI seem to think this is crazy.
I’m a dreamer by the  higher-ups and told that such a gesture or the part of the government is “highly unlikely.” Well what do they base this assessment on? Clearly the government is encouraging foreign investment.
I’ll tell you this: if taxes go down, there will be a flood of private investment, and we need.
From: VP, Economic Research, Backwater Investments To: Head First Subject: Have these people ever even been to Russia? While the analytic stuff in the Economic division has continued to flourish and produce quality work on Russian business and government, the rest of BI has shown a shocking ignorance of the internal dynamics of Russia.
It’s quite unlikely that Russia will puchase EuroAir, and their support of the oil industry next quarter may be the most tentative it’s ever been…
You need to stop listening to the eggheads back at.
The subjective probabilities show that some areas are not as contentious as we previously thought.
The subjective probabilities are a precise specification of where there is disagreement and how much.
The analysts can use them to help them figure out what they should focus on to.
But I don’t want to read a big grid of numbers.
Send me a chart that displays this data in a way that is easier for me to understand.
The community at BI has become obsessed with these three places.
Yet aren’t the answers to all our questions abundantly clear? Russia will continue to subsidize oil next quarter like it always has, and they’re more likely than not to buy EuroAir next quarter.
Vietnam might decrease taxes this year, and they probably aren’t going to encourage foreign investment.
Indonesia will more likely than not invest in ecotourism this year, but it won’t be of much help.
If BI doesn’t fire the dissenters and troublemakers who dispute these truths, the firm might as well close…
For the past six months I’ve consistently argued to the staff that the Vietnamese government is probably going to reduce its taxes this year.
And everything that we’ve seen from our people on the ground and in news reports confirms this.
Yet others in the “analytical” community at BI seem to think this is crazy.
I’m a dreamer by the  higher-ups and told that such a gesture or the part of the government is “highly unlikely.” Well what do they base this assessment on? Clearly the government is encouraging foreign investment.
I’ll tell you this: if taxes go down, there will be a flood of private investment, and we need.
From: VP, Economic Research, Backwater Investments To: Head First Subject: Have these people ever even been to Russia? While the analytic stuff in the Economic division has continued to flourish and produce quality work on Russian business and government, the rest of BI has shown a shocking ignorance of the internal dynamics of Russia.
It’s quite unlikely that Russia will puchase EuroAir, and their support of the oil industry next quarter may be the most tentative it’s ever been…
You need to stop listening to the eggheads back at.
For each value, plot a dot corresponding to the subjective probability.
The vertical axis doesn’t really matter, you can just jitter dots around so you can see them all.
Statement 4 Vietnam’s government will encourage foreign investment this year.
Statement 2 Russia will purchase a European airline next quarter.
Statement 4 Vietnam’s government will encourage foreign investment this year.
How do the spreads of analyst subjective probabilities look on your dot plots?
It looks like there is actually some consensus on this statement.
The analysts are all over the place on these statements.
People are within 20% of each other here, except for one person.
Statement 2 Russia will purchase a European airline next quarter.
Subjective probabilities are something that everyone understands but that don’t get nearly enough use.
Great data analysts are great communicators, and subjective probabilities are an illuminating way to convey to others exactly what you think and believe.
I can see that there are a few areas where we really should concentrate our resources to get better information.
And the stuff that doesn’t appear to have real disagreement is just fantastic.
From now on, I don’t want to hear anything from my analysts unless it’s in the form of a subjective probability (or objective probability, if they can come up with one of those)
Can you rank these questions for me by their level of disagreement? I want to know which ones specifically are the most contentious.– CEO.
What metric would measure disagreement and rank the questions so that the CEO can see the most problematic ones first?
The standard deviation measures how far points are from the average.
The unit of  the standard deviation is whatever it is that you’re measuring.
Most points will be 10 percent above or below the mean, although a handful of  points will be two or three standard deviations away.
The larger the standard deviation of  subjective probabilities from the mean, the more disagreement there will be among analysts as to the likelihood that each hypothesis is true.
The standard deviation measures how far typical points are from the average (or mean) of  the data set.
Most of  the points in a data set will be within one standard deviation of  the mean.
STDEV(data range) Use the STDEV formula in Excel to calculate the standard deviation.
Most observations in any data set are going to be within one standard deviation of the mean.
Then, sort the list of questions to rank highest the question with the most disagreement.
What formula would you use to calculate the standard deviation for the first statement?
This data has been turned on its side so that you can sort the statements once you have the standard deviation.
What formula would you use to calculate the standard deviation for the first statement?
Click the Sort Descending button to put the statements in order.
Looks like Statement 3 has the largest standard deviation and the greatest disagreement among analysts.
You might need to hit the “%” button on the toolbar to get the right formatting.
Q: Aren’t subjective probabilities kind of deceptive? A: Deceptive? They’re a lot less deceptive than vague expressions like “really likely.” With probability words, the person listening to you can pour all sorts of possible meanings into your words, so specifying the probabilities is actually a much less deceptive way to communicate your beliefs.
Q: I mean, isn’t it possible or even likely (pardon the expression) that someone looking at these probabilities would get the impression that people are more certain about their beliefs than they actually are?
A: You mean that, since the numbers are in black and white, they might look more certain than they actually are?
But the deal with subjective probabilities is the same as any other tool of data analysis: it’s easy to bamboozle people with them if what you’re trying to do is deceive.
But as long as you make sure that your client knows that your probabilities are subjective, you’re actually doing him a big favor by specifying your beliefs so precisely.
Q: Hey, can Excel do those fancy graphs with the little dots? A: Yes, but it’s a lot of trouble.
These graphs were made in a handy little free program called R using the dotchart function.
You’ll get a taste of the power of R in later chapters.
I’m going to base my trading strategy on this sort of analysis from now on.
If it pans out, you’ll definitely see a piece of the upside.
This is awful! We all predicted that Russia would continue to have confidence in business.
The initial reaction of  the analysts to this news is great concern.
Backwater Investments is heavily invested in Russian oil, largely because of  what you found to be a large consensus on oil’s prospects for continued support from the government.
But this news could cause the value of  these investments to plummet, because people will suddenly expect there to be some huge problem with Russian oil.
Then again, this statement could be a strategem by the Russians, and they might not actually intend to sell their oil fields at all.
We need to go back and revise all the subjective probabilities.
We’ve picked up a lot of analytic tools so far.
Maybe one of them could be useful at figuring out how to revise the subjective probabilities.
Better pick an analytic tool you can use to incorporate this new information into your subjective probability framework.
Why would you or wouldn’t you use each of these?
Better pick an analytic tool you can use to incorporate this new information into your subjective probability framework.
Why would you or wouldn’t you use each of these?
There is definitely a role for hypothesis testing in problems like this one, and the analysts might use.
But our job is to figure out specifically how the new.
There is no hard numerical data! The optimization tools we’ve learned presuppose that you have.
It’s kind of hard to imagine what sort of experiment you could run to get better data.
Using each analyst’s first subjective probability as a base rate, maybe we can.
Bayes’ rule is great for revising subjective probabilities Bayes’ rule is not just for lizard flu! It’s great for subjective probabilities as well, because it allows you to incorporate new evidence into your beliefs about your hypothesis.
Try out this more generic version of  Bayes’ rule, which uses H to refer to your hypothesis (or base rate) and E to refer to your new evidence.
Here’s the formula you used to figure out your chances of having lizard flu.
The probability that you’d see the evidence, given that the hypothesis is true.
The probability that you’d see the evidence, given that the hypothesis is false.
Using Bayes’ rule with subjective probabilities is all about asking for the probability that you’d see the evidence, given that the hypothesis is true.
After you’ve disciplined yourself  to assign a subjective value to this statistic, Bayes’ rule can figure out the rest.
You just need to get the analysts to give you these values:
Why go to this trouble? Why not just go back to the analysts and ask for new subjective probabilities based on their reaction to the events?
The subjective probability that Russia will (and won’t) continue to subsidize oil.
The subjective probability that the news report would (or wouldn’t) happen, given that Russia will continue to subsidize oil.
I don’t see why the analyst wouldn’t just ask me for another subjective probability.
But I still don’t appreciate being kicked to the curb once I’ve given the analyst my first idea.
I still don’t get why I can’t just give you a new subjective probability to describe the chances that Russia will continue to support the oil industry.
Would anyone ever actually think like this? Sure, I can see why someone would use you when he wanted to calculate the chances he had a disease.
I guess I need learn to tell the analyst to use you under the right conditions.
I just wish you made a little more intuitive sense.
You did indeed, and I can’t wait to use your first subjective probability as a base rate.
Oh no! You’re still really important, and we need you to provide more subjective probabilities to describe the chances that we’d see the evidence given that the hypothesis is either true or untrue.
Using me to process these probabilities is a rigorous, formal way to incorporate new data into the analyst’s framework of  beliefs.
Plus, it ensures that analysts won’t overcompensate their subjective probabilities if  they think they had been wrong.
OK, it’s true that analysts certainly don’t have to use me every single time they learn anything new.
But if the stakes are high, they really need me.
If  you think you might have a disease, or you need to invest a ton of  money, you want to use the analytical tools.
Here’s a spreadsheet that lists two new sets of subjective probabilities that have been collected from the analysts.
Write a formula that implements Bayes’ rule to give you P(S1|E)
This is the probability that the hypothesis is true, given the new evidence.
Load this! Here are the two new columns of data.
Put your formula here and copy/paste it for each analyst.
What formula did you use to implement Bayes’ rule and derive new subjective probabilities for Russia’s support of the oil industry?
Those new probabilities look hot! Let’s get them plotted and see how they compare to the base rates!
This formula combines the analysts’ base rate with their judgments about the new data to come up with a new assessment.
Using the data on the facing page, plot the new subjective probabilities of each analyst on the chart below.
As a point of reference, here is the plot of people’s beliefs in the hypothesis that Russia would continue to support the oil industry as they were before the news report.
How do you compare the new distribution of subjective probabilities with the old one?
How does the now distribution of beliefs about Russia’s support for the oil industry look?
The spread of the new set of subjective probabilities is a little wider, but only three people assign.
These three analysts must have lost at least some confidence in the hypothesis after the news report.
The CEO knows exactly what to do with this new information.
Everyone is selling their Russia holdings, but this new data on my analysts’ beliefs leads me to want to hold on to ours.
On close inspection, the analysts concluded that the Russian news is likely to report the selling of  their oil fields whether it’s true that they will stop supporting oil or not.
The analysts were right: Russia was bluffing about selling off  their oil fields.
And the market rally that took place once everyone realized it was very good for Backwater.
Looks like your subjective probabilities kept heads cool at Backwater Investments and resulted in a big payoff  for everyone!
Do a little more work like that, and this will be the start of a long relationship.
The real world has more variables than you can handle.
There is always going to be data that you can’t have.
What is really cool is that these rules can actually.
That last comment is the one we’re really worried about.
It’s starting to look as if LitterGitters will be in big trouble very soon if  you can’t persuade the city council that LitterGitters’ public outreach programs have been a success relative to the city council’s intentions for it.
The LitterGitters are a nonprofit group funded by the Dataville City Council to run public service announcements to encourage people to stop littering.
They just presented the results of  their most recent work to the city council, and the reaction is not what they’d hoped for.
We’re cutting your funding in 1 month, unless you can come up with a way to show you’ve.
Before the LitterGitters came along, Dataville was a total mess.
Some residents didn’t respect their home and polluted it with trash, damaging Dataville’s environment and looks, but all that changed when LitterGitters began their work.
It’d be terrible for the city council to cut their funding.
They need you to help them get better at communicating why their program is successful so that the city council will continue their support.
Brainstorm the metrics you might use to fulfill the mandate.
If the city council cuts LitterGitters’ funding, Dataville will turn back into a big trash dump!
How exactly would you collect the data that would show whether the LitterGitters’ work had resulted in a reduction in litter tonnage?
Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? No.
Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? No.
Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes.
Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
Do you litter in Dataville? No Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
If you saw someone littering, would you tell them to throw their.
Do you think littering is a problem in Dataville? Yes.
Do you litter in Dataville? No Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
LitterGitters have been measuring their results, but they haven’t measured the things you imagined in the previous exercise.
Their tactics, after all, are all about changing people’s behaviors so that they stop littering.
Let’s take a look at a summary of  their results…
We could have garbage men separate litter from normal trash and weigh both repeatedly over time.
Or we could have special collections at places in Dataville that have a reputation for being filled.
Does the LitterGitters’ results show or suggest a reduction in the tonnage of litter in Dataville?
And educating people about why they need to change their behaviors will lead to a reduction in litter tonnage, right? That’s the basic premise of  LitterGitters, and their survey results do seem to show an increase in public awareness.
But the city council was unimpressed by this report, and you need to help LitterGitters figure out whether they have fulfilled the mandate and then persuade the city council that they have done so.
Does the data show or suggest a litter tonnage reduction because of LitterGitters’ work?
It might suggest a reduction, if you believe that people’s reported change in beliefs has an impact on.
But the data itself only discusses public opinion, and there is nothing in it explicitly about.
Actually weighing litter is way too expensive and logistically complicated, and everyone in the field considers that Databurg 10% figure bogus.
What else are we supposed to do besides survey people?
The city council is expecting to hear evidence from LitterGitters that demonstrates that the LitterGitters campaign has reduced litter tonnage, but all we provided them is this opinion survey.
If  it’s true that measuring tonnage directly is logistically unfeasible, then the demand for evidence of  tonnage reduction is dooming LitterGitters to failure.
Give people a hard question, and they’ll answer an easier one instead.
LitterGitters knows that what they are expected to do is reduce litter tonnage, but they decided not to measure tonnage directly because doing so is such an expense.
Reacting to difficult questions in this way is actually a very common and very human thing to do.
We all face problems that are hard to tackle because they’re “expensive” economically—or cognitively (more on this in a moment)—and the natural reaction is to answer a different question.
This simplified approach might seem like the totally wrong way to go about things, especially for a data analyst, but the irony is that in a lot of  situations it really works.
And, as you’re about to see, sometimes it’s the only option.
Here are some of the opinion surveys LitterGitters got back from people.
Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes.
Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes.
Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes.
Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes.
Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
Have you heard of the LitterGitters program? Yes If you saw someone littering, would you tell them to throw their trash away in a trash can? Yes.
Do you think littering is a problem in Dataville? Yes Has LitterGitters helped you better to understand the importance of preventing litter? Yes Would you support continued city funding of LitterGitters’ educational programs? Yes.
You’re going to need a big scale to weigh all this…
The city workers won’t record the data for us, because they already have plenty of work to do.
And staffing the contact points would cost us double what the city already pays us.
If we did nothing but measure litter tonnage, we still wouldn’t have enough money to do it right.
Besides, the city council is all wrong when it focuses on tonnage.
There are lots of people involved, lots of types of litter, and lots of places to find it.
To ignore the system and hyper-focus on one variable is a mistake.
It describes things you might want to measure in the world of  litter.
And here is the director’s explanation of  this big system and the implications that its complexity has for the work of LitterGitters.
This problem would be a beast even if  you had all the data, but as you’ve learned getting all the data is too expensive.
Any sort of  model you created to try to measure or design an optimal litter control program would have an awful lot of  variables to consider.
Not only would you have to come up with a general quantitative theory about how all these elements interact, but you’d also have to know how to manipulate some of  those variables (your decision variables) in order to minimize tonnage reduction.
An objective function shows how you want to maximize your objective in an optimization problem.
In order to optimize, you have to know the entire system.
The city council wants to get litter tonnage as low as possible, and we need to show that LitterGitters’ program does this.
Is giving the city council what they want even possible?
We have a city council asking for something we can’t give them.
And even if  we could provide the tonnage reduction figure, it would not be of much use.
Jill: Yes, we’re not here just to satisfy the council.
Joe: Couldn’t we just make something up? Like do our own “estimate” of  the tonnage?
I mean, the city council seems like a really tough group.
If  we were to make up some subjective metric like that and have it masquerade as a tonnage metric, they might flip out on us.
Jill: Making up something is a sure way to get LitterGitters’ funding eliminated.
Maybe we could persuade the city council that opinion surveys really are a solid proxy for tonnage reduction?
Jill: We could come up with an assessment that incorporates more variables than just public opinion.
Maybe we should try to collect together every variable we can access and just make subjective guesses for all the other variables?
You can indeed go with just a few more variables.
And if  you were to assess the effectiveness of  LitterGitters by picking one or two variables and using them to draw a conclusion about the whole system, you’d be employing a heuristic…
Why can’t we just pick one or two more variables, analyze them too, and leave it at that?
Heuristics are a middle ground between going with your gut and optimization.
If  you’ve solved an optimization problem, you’ve found the answer or answers that represent the maximum or minimum of  your objective function.
And for data analysts, optimization is a sort of ideal.
It would be elegant and beautiful if  all your analytic problems could be definitively solved.
Analysts try to avoid relying on intuition if they can, but decisions you make really quickly or without any data often have to be intuitive.
Do you make decisions impulsively, or with a few well-chosen pieces of  key data, or do you make decisions by building a model that incorporates every scrap of  relevant data and yields the perfect answer?
Your answer is probably “All of  the above,” and it’s important to realize that these are all different ways of  thinking.
Substituting a difficult or confusing attribute for a more accessible one.
Some psychologists even argue that all human reasoning is heuristic and that optimization is an ideal that works only when your problems are ultra-specified.
But if  anyone’s going to have to deal with an ultraspecified problem, it’ll be a data analyst, so don’t throw away your Solver just yet.
Just remember that well-constructed heuristic decision-making protocols need to be part of  your analytic toolkit.
Q: It seems weird that you’d have a decision procedure that didn’t guarantee a correct answer and call it “data analysis.” Shouldn’t you call that sort of thing “guesswork”?
A: Now that wouldn’t be very nice! Look, data analysis is all about breaking down problems into manageable pieces and fitting mental and statistical models to data to make better judgements.
There’s no guarantee that you’ll always get the right answers.
Q: Can’t I just say that I’m always trying to find optimal results? If I’ve got to dabble in heuristic thinking a little, fine, but my goal is optimality?
You certainly don’t want to use heuristic analytical tools when better optimizing tools are available and feasible.
But what is important to recognize is that heuristics are a fundamental part of how you think and of the methods of data analysis.
Q: So what’s the difference between the psychological and the computer science definition of “heuristics”?
In computer science, heuristic algorithms have an ability to solve problems without people being able to prove that the algorithm will always get the right answer.
Many times, heuristic algorithms in computer science can solve problems more quickly and more simply than an algorithm that guarantees the right answer, and often, the only algorithms available for a problem are heuristic.
A: Psychologists have found in experimental research that people use cognitive heuristics all the time.
There is just too much data competing for our attention, so we have to use rules of thumb in order to make decisions.
There are a number of classic ones that are part of the hard-wiring of our brain, and on the whole, they work really well.
Q: Isn’t it kind of obvious that human thinking isn’t like optimization?
People who have a strong sense of humans as rational creatures might be upset by the notion that we use quick and dirty rules of thumb rather than think through all our sensory input in a more thorough way.
Q: So the fact that a lot of reasoning is heuristic means that I’m irrational?
A: It depends on what you take to be the definition of the word “rational.” If rationality is an ability to process every bit of a huge amount of information at lightning speed, to construct perfect models to make sense of that information, and then to have a flawless ability to implement whatever recommendations your models suggest, then yes, you’re irrational.
Q: That’s why we let computers do data analysis for us!
A: Computer programs like Solver live in a cognitive world where you determine the.
And your choice of inputs is subject to all the limitations of your own mind and your access to data.
But within the world of those inputs, Solver acts with perfect rationality.
Q: And since “All models are wrong, but some are useful,” even the optimization problems the computer runs look kind of heuristic in the broader context.
The data you choose as inputs might never cover every variable that has a relationship to your model; you just have to pick the important ones.
A: Think of it this way: with data analysis, it’s all about the tools.
A good data analyst knows how to use his tools to manipulate the data in the context of solving real problems.
There’s no reason to get all fatalistic about how you aren’t perfectly rational.
Learn the tools, use them wisely, and you’ll be able to do a lot of great work.
Q: But there is no way of doing data analysis that guarantees correct answers on all your problems.
A: No, there isn’t, and if you make the mistake of thinking otherwise, you set yourself up for failure.
Analyzing where and how you expect reality to deviate from your analytical models is a big part of data analysis, and we’ll talk about the fine art of managing error in a few chapters.
Q: So heuristics are hard-wired into my brain, but I can make up my own, too?
A: You bet, and what’s really important as a data analyst is that you know it when you’re doing it.
Here’s a heuristic that describes different ways of  dealing with the problem of  having trash you need to get rid of.
It’s a really simple rule: if  there’s a trash can, throw it in the trash can.
This schematic way of  describing a heuristic is called a fast and frugal tree.
It’s fast because it doesn’t take long to complete, and it’s frugal because it doesn’t require a lot of  cognitive resources.
What the city council needs is its own heuristic to evaluate the quality of  the work that LitterGitters has been doing.
Their own heuristic is unfeasible (we’ll have to persuade them of  that), and they reject LitterGitters’ current heuristic.
Can you draw a fast and frugal tree to represent a better heuristic? Let’s talk to LitterGitters to see what they think about a more robust decision procedure.
And, like I’ve said, there is just no way to weigh all the.
But maybe you could just poll the solid waste workers.
The biggest problem is cigarette butts, and if we periodically poll the street sweepers and landfill workers about how many butts they’re seeing, we’d have a not totally complete but still pretty solid grip on what is happening with litter.
Is there a simpler way to assess LitterGitters’ success? Using a heuristic approach to measure LitterGitters’ work would mean picking one or more of  these variables and adding them to your analysis.
What does the LitterGitters director think would be the best approach?
Which of these variables can you add to your analysis to give a fuller picture of LitterGitters’ effectiveness?
Draw a fast and frugal tree to describe how the city council should evaluate the success of LitterGitters.
Be sure to include two variables that LitterGitters considers important.
The final judgment should be whether to maintain or eliminate the funding of LitterGitters.
What heuristic did you create to evaluate whether LitterGitters has been successful?
While your own tree might be different, here’s an example of where you might have ended up.
Do solid waste workers believe there has been a reduction?
First, the city council needs to ask whether the public is reacting positively to LitterGitters.
If not, LitterGitters’ funding should be eliminated.If the public is supportive, do.
This attribute is what we’re substituting for directly measuring tonnage.
If the solid waste workers don’t think there’s been any effect, that’s the end of funding.
It sounds as if  at least one of  the city council members has already made up his mind.
This guy totally has the wrong way of  looking at the work of  LitterGitters.
Draw a diagram to describe his thought process in forming his expectation about LitterGitters.
You need to understand his reasoning if you are going to be able to persuade this guy that your heuristic assessment ideas are valid.
I am looking forward to seeing that report I hear you redid.
But I’m expecting you to be like all the other nonprofits that get Dataville money...
Stereotypes are definitely heuristics: they don’t require a lot of  energy to process, and they’re superfast.
Heck, with a stereotype, you don’t even need to collect data on the thing you’re making a judgement about.
But in this case, and in a lot of  cases, stereotypes lead to poorly reasoned conclusions.
A fast and frugal rule of  thumb might help get answers for some problems while predisposing you to make inadequate judgements in other contexts.
How does it seem this unpleasant city council member is forming his expectations?
It seems as if he isn’t even interested in LitterGitters itself… his other experiences are determining his reaction.
A much better way to judge LitterGitters would be something like this:
Maybe we can get some data to describe what the sanitation workers think about what is happening with litter.
Then we can present our original analysis along with our decisions heuristic and new data to the city council.
Here’s how you decided the city council should assess the work of LitterGitters.
Here’s our original data describing the attitudes of the general public about litter.
Here’s some new data describing the sanitation workers’ impressions of litter in Dataville since LitterGitters began their program.
We can’t compare this figure to last year, because we just started collecting data for this report.
Have you noticed a reduction in litter coming into Da taville.
Are there fewer cigarette butts being collected off the streets.
These numbers represent the percentage of people who answered “yes.”
Your analysis is ready to present Between your heuristic and the data you have, including the just-received responses from the sanitation workers below, you’re ready to start explaining what you see to the city council.
Do solid waste workers believe there has been a reduction?
Can you guarantee that your tactics will continue to work?
Answer the following questions from the city council about your work with LitterGitters.
The problem with doing it, though, is that it’d be too expensive.
It’d cost twice the amount of money you actually pay LitterGitters to do their work.
All the data is observational, so we can’t prove that the increase in awareness of the general public.
But we have good reasons to believe that our program was the cause of these results.
There are never guarantees in life, but as long as we can sustain.
But in that case, your objective wouldn’t be to reduce litter, because you’d.
We can’t speak for other nonprofits, but we have a crystal clear idea of.
Can you guarantee that your tactics will continue to work?
The city council is pleased to renew the contract of  LitterGitters, thanks to the excellent work from the Head First data analyst.
We recognize that our previous assessment of  the work of  LitterGitters did not adequately treat the whole issue of  litter in Dataville, and we discounted the importance of  public opinion and behavior.
The new decision procedure you provided is excellently designed, and we hope the LitterGitters continue to live up to the high bar they have set for themselves.
LitterGitters will receive increased funding from the Dataville City Council this year, which we expect will help…
Now there is so much more we’ll be able to do to get the word out about stopping litter in Dataville.
Thanks to your hard work and subtle insight into these analytical problems, you can pat yourself  on the back for keeping Dataville neat and tidy.
How much can a bar graph tell you? There are about a zillion ways of showing data with pictures, but one of them is special.
Histograms, which are kind of similar to bar graphs, are a super-fast and easy way to.
You’re about to use these powerful little charts to measure your data’s.
Most of the action in this city concentrates right here.
Thank you for filling out our self-review! This document is important for.
Your ability to interpret the meaning and importance of  past events.
Your ability to keep your client well-informed and making good choices.
Your annual review is coming up You’ve been doing some really excellent analytical work lately, and it’s high time you got what’s coming to you.
The powers that be want to know what you think about your own performance.
Not a literal pat on the back, though… something more.
But what kind of  recognition? And how do you go about actually getting it?
Bet you’d score higher now than you would have in chapter 1!
Write down how you’d respond to each of these questions.
Should you just say thanks to your boss and hope for the best? If your boss really believes you’ve been valuable, he’ll reward you, right?
Should you give yourself super-positive feedback, and maybe even exaggerate your talents a little? Then demand a big raise?
Can you envision a data-based way of deciding on how to deal with this situation?
Even though your case is unique to you, it still might make sense to get an idea of  your boss’s baseline expectations.
Right now, you have no idea what your boss will think or do.
Going for more cash could play out in a bunch of different ways People can be skittish about trying to get more money from their bosses.
And who can blame them? There are lots of possible outcomes, and not all of  them are good.
But how do we get the boss to give it to us?
However you answered the questions on the last page, we think you should go for more money.
You’re not doing this hard work for your health, after all.
How would you deal with this data? Could you manage it to make it more useful?
Because you’re so plugged in to Starbuzz’s data, you have access to some sweet numbers: Human Resource’s records about raises for the past three years.
You might be able to wring some powerful insights out of  this data.
If  you assume that your boss will act in a similar way to how previous bosses acted, this data could tell you what to expect.
Problem is, with approximately 3,000 employees, the data set is pretty big.
This data could be of use to you as you figure out what types of raises are reasonable to expect.
You’re going to need to do something to make the data useful.
Each line of the database represents someone’s raise for the specified year.
This column tells you whether the person asked for more money or not.
Here’s the person’s raise amount, measured as a percent increase.
This column says whether the person is male or female… you know, there might be a correlation between gender and raise amount.
Jim: We should forget about the data and just go for as much as we can get.
Nothing in there will tell us what they think we’re worth.
There’s a range of  numbers in the boss’s head, and we need to figure out how to get the upper end of  that range.
Joe: I agree that most of  the data is useless to tell us what they think we are worth, and I don’t see how we find out.
The data will tell us the average raise, and we can’t go wrong shooting for the average.
Frank: I think a more subtle analysis is in order.
There’s some rich information here, and who knows what it’ll tell us?
Joe: We need to stay risk-averse and follow the herd.
Just average the Raise column and ask for that amount.
Frank: Look, the data shows whether people negotiated, the year of  the raise, and people’s genders.
All this information can be useful to us if  we just massage it into the right format.
First we have to figure out how to collapse all these numbers into figures that make more sense…
There’s just too much of  it to read and understand all at once, and until you’ve summarized the data you don’t really know what’s in it.
Start by breaking the data down into its basic constituent pieces.
Once you have those pieces, then you can look at averages or whatever other summary statistic you consider useful.
As you know, much of analysis consists of taking information and breaking it down into smaller, more manageable pieces.
Draw a picture to describe how you would break these data fields down into smaller elements.
What statistics could you use to summarize these elements? Sketch some tables that incorporate your data fields with summary statistics.
Draw pictures here to represent how you’d split the data into smaller pieces.
What sort of pieces would you break your data into?
You can break the data in your columns into pieces…
Here are some examples… your answers might be a little different.
Here are a few ways you might integrate your data chunks with summary statistics.
This table shows the average raise for male and female negotiators.
This chart counts the number of raises in each interval of raise possibilities.
It sure is fun to imagine summarizing these pieces of the data, but here’s a thought.
Using the groupings of data you imagined, you’re ready to start summarizing.
When you need to slice, dice, and summarize a complex data set, you want to use your best software tools to do the dirty work.
So let’s dive in and make your software reveal just what’s going on with all these raises.
Test Drive A visualization of  the number of  people who fall in each category of  raises will enable you to see the whole data set at once.
So let’s create that summary… or even better, let’s do it graphically.
With your data open in Excel, click the Data Analysis button under the Data tab.
In the pop-up window, tell Excel that you want to create a histogram.
If you don’t see the Data Analysis button, see Appendix iii for how to install it.Here it is!
In OpenOffice and older versions of Excel, you can find Data Analysis under the Tools menu.
Be sure to check this box so that Excel makes a chart.
When your raise data is selected, there should be a big “marching ants” selection box all the way from the top…
Histograms are a powerful visualization because, no matter how large your data set is, they show you the distribution of  data points across their range of  values.
For example, the table you envisioned in the last exercise would have told you how many people received raises at about 5 percent.
This histogram shows graphically how many people fall into each raise category, and it concisely shows you what people are getting across the spectrum of  raises.
Looks like a whole lot of people have raises in this area.
What kind of concentration of people get raises around 5%?
On the other hand, there are some problems with what Excel did for you.
The default settings for the bins (or “class intervals”) end up producing messy, noisy x-axis values.
The graph would be easier to read with plain integers (rather than long decimals) on the x-axis to represent the bins.
Sure, you can tweak the settings to get those bins looking more like the data table you initially envisioned.
But it’s chapter 9, and you’ve been kicking serious butt.
You’re ready for a software tool with more power than Excel to manage and manipulate statistics.
It’s a free, open source program that might be the future of statistical computing, and you’re about to dive into it!
Gaps between bars in a histogram mean gaps among the data points.
That’s exactly what the gap should mean, at least if the histogram is written correctly.
If  you assumed this histogram was correct, and that there were gaps between these values, you’d get the totally wrong idea.
You need a software tool to create a better histogram.
In histograms, gaps mean that there is data missing between certain ranges.
If  the histogram showed that, it might really be worth investigating.
In fact, there will always be gaps if  there are more bins than data points (unless your data set is the same number repeated over and over)
The problem with Excel’s function is that it creates these messy, artificial breaks that are really deceptive.
And there’s a technical workaround for the issues (with Excel, there’s almost always a workaround if  you have the time to write code using Microsoft’s proprietary language)
You should have no problem finding a mirror near you that serves R for Windows, Mac, and Linux.
Once you’ve fired up the program, you’ll see a window that looks like this.
And you can always pull up a spreadsheet-style visualization of  your data by typing edit(yourdata)
This little cursor here represents the command prompt and is where you’ll be entering your commands into R.
For your first R command, try loading the Head First Data Analysis script using the source command:
That command will load the raise data you need for R.
You’ll need to be connected to the Internet for it to work.
If  you want to save your R session so that you can come back to the Head First data when you’re not connected to the Internet you can type save.image()
So what did you download? First, take a look at the data frame from your download called.
The output you see on the right is what R gives you in response.
The command returns a listing of all the rows in the data frame.
Type the name of the data frame to get R to display it.
What do you think the various elements of the command mean? Annotate your response.
With histograms, the areas under the bars don’t just measure the count (or frequency) of  the thing being measured; they also show the percentage of  the entire data set being represented by individual segments.
When you run the command, a window pops up showing this.
The second argument tells R how to construct the groupings.
These commands will tell you a little more about your data set and what people’s raises look like.
How does what you see on the histogram compare with what R tells you from these two commands?
Why do you think R responds to each of these the way it does?
Type help(sd) and help(summary) to find out what the commands do.
You just ran some commands to illustrate the summary statistics for your data set about raises.
The right side is a little bigger than the left and pulls the mean to the rgiht.
On average, the raises are 2.43 percentage points from the mean.
How does what you see on the histogram compare with what R tells you from these two commands?
There are two “humps,” a big tall one and this little one on the right.
Joe: If  the histogram were symmetrical, the mean and median would be in the same place—in the dead center.
But in this case, the small hump on the right side is pulling the mean away from the center of  the larger hump, where most of  the observations are.
Frank: Maybe we should take another look at those pieces of  data we identified earlier and see if  they have any relevance to the histogram.
Can you think of any ways that the groups you identified earlier might explain the two humps on the histogram?
Q: So it seems like we have a lot of flexibility when it comes to how the histograms look.
You should think of the very act of creating a histogram as an interpretation, not something you do before interpretation.
Q: Are the defaults that R uses for creating a histogram generally good?
Just as with the summary functions, there’s nothing wrong with running a quick and dirty histogram to see what’s there, but before you draw any big conclusions about what you see, you need to use the histogram (and redraw the histogram) in a way that remains mindful of what you’re looking at and what you hope to gain from your analysis.
Usually, when we think of bell curves, we’re talking about the normal or Gaussian distribution.
But there are other types of bell-shaped distributions, and a lot of other types of distributions that aren’t shaped like a bell.
Q: Then what’s the big deal about the normal distribution?
A: The histogram you’ve been evaluating is definitely not normally distributed.
As long as there’s more than one hump, there’s no way you can call the distribution bell-shaped.
Q: But there are definitely two humps in the data that look like bells!
A: And that shape must have some sort of meaning.
The question is, why is the distribution shaped that way? How will you find out?
Q: Can you draw histograms to represent small portions of the data to evaluate individually? If we do that, we might be able to figure out why there are two humps.
Can you break the raise data down in a way that isolates the two humps and explains why they exist?
How might the groupings of data you identified earlier account for the two humps on your histogram?
There could be variation among years: for example, raises in 2007 could be on average much higher than.
And there could be gender variation, too: men could, on average, get higher raises.
Of course, all the data is observational, so any relationships you discover won’t.
Let’s make a bunch of histograms that describe subsets of the raise data.
Maybe looking at these other histograms will help you figure out what the two humps on the raise histogram mean.
Is there a group of people who are earning more in raises than the rest?
What do you see? The results are on the next page, where you’ll write down your interpretations.
You can make a histogram out of  your entire data set, but you can also split up the data into subsets to make other histograms.
The shape of men’s raises, for example, might tell you something by itself or in comparison to the shape of women’s raises.
To start, look at this histogram command and annotate its syntax.
If you plot the raise values for each subset, you might get a bunch of different shapes.
Inside your data are subsets of data that represent different groups.
These histograms represent the raises for different subgroups of your employee population.
These brackets are the subset operator, which extracts a subset of your data.
You looked at the different histograms in search of answers to help you understand who is getting what raises.
This histogram selects only raises for 2007 and has the same basic.
Once again, we see the big hump and the little hump attached on the.
It looks like splitting those who did and did not negotiate neatly.
Your analysis of  histograms of  different subsets of the raise data shows that getting a larger raise is all about negotiation.
People have a different spread of  outcomes depending on their choice of  whether to negotiate.
If they do, their whole histogram shifts to the right.
If  you run the summary statistics on your negotiation subsets, the results are just as dramatic as what you see with the two curves.
The mean and median are about the same within each distribution.
On average, for both distributions, data points are within a single percentage point of the mean.
What will negotiation mean for you? Now that you’ve analyzed the raise data, it should be pretty clear which strategies will have the best results.
The data suggest that negotiation will tend to create these outcomes.
It’s great to do nothing…  if you don’t want a big raise!
Regression is an incredibly powerful statistical tool that, when used correctly, has the.
I’ve got all this data, but I really need it to tell me what will happen in the future.
What are you going to do with all this money?
With your histograms, you figured out that people who chose to negotiate their salaries got consistently higher outcomes.
So when you went into your boss’s office, you had the confidence that you were pursuing a strategy that tended to pay off, and it did!
These are the histograms you looked at in the final exercises of  the previous chapter, except they’ve been redrawn to show the same scale and bin size.
These folks didn’t negotiate, and their raises were lower than those who did...
Your boss was impressed that you negotiated and gave you 15%
Lots of  people could benefit from your insight about how to get better raises.
Few of  your colleagues took the savvy approach your did, and you have a lot to offer those who didn’t.
You should set up a business that specializes in getting people raises!
Here are a few questions to get you thinking about data-based ways of creating a business around your insights in salary negotiations.
What do you think your clients would want from a business that helps them understand how to negotiate raises?
If you ran such a business, what would be a fair way to compensate you for your knowledge?
What sort of data-based compensation consulting business do you envision?
If you ran such a business, what would be a fair way to compensate you for your knowledge?
Clients will want you to have an incentive to make sure that their experience works out well.
What do you think your clients would want from a business that helps them understand how to negotiate raises?
There are all sorts of ways that people negotiating for a raise could be helped: they might want.
But one question is fundamental: how much do I ask for?
Your slice of the raise.Your client needs you to help her figure out what sort of raise to ask for.
When your client asks her boss for a certain raise level, her boss will respond with a raise.
I want more, but I don’t know what to ask for.
An analysis that tells people what to ask for could be huge What amount of  money is reasonable to ask for? How will a request for a raise translate into an actual raise? Most people just don’t know.
You need a basic outline of your service so you know what you’re shooting for.
And they want to know what they’ll get, given what they’ve asked for.
And you’ve got everything you need to create a kick-butt decision procedure to help people get good raises.
Amount Requested, and perform some steps in order to predict the Amount rewarded.
The algorithm is some sort of decision procedure that says what will happen at different request levels.
It’s all well and good to draw a pretty picture like this, but in order for you to have something that people are willing to pay for—and, just as important, in order for you to have something that works—you’re going to need to do a serious analysis.
Let’s take a look at some data about what negotiators asked for.
Can you predict what sort of  raise you’ll get at various levels of  requests?
Inside the algorithm will be a method to predict raises Prediction is a big deal for data analysis.
Some would argue that, speaking generally, hypothesis testing and prediction together are the definition of  data analysis.
The histograms below describe the amount of money the negotiators received and the amount of money they requested.
You’ll be able to play with it when you turn the page.
This is the chart from the beginning with a different scale.
Do the histograms tell you what people should request in order to get a big raise? Explain how comparing the two histograms might illuminate the relationship between these two variables, so that you might be able to predict how much you would receive for any given request.
Q: Can’t I just overlay two histograms onto the same grid?
But in order to make a good comparison, the two histograms need to describe the same thing.
You made a bunch of histograms in the previous chapter using subsets of the same data, for example, and comparing those subsets to each other made sense.
A: Sure, they’re similar in the sense that they are measured using the same metric: percentage points of one’s salary.
But what you want to know is not so much the distribution of either variable but how, for a single person, one variable relates to the other.
So once we have that information, how will we make use of it?
You should stay focused on the end result of your analysis, which is some sort of intellectual “product” that you can sell to your customers.
What do you need? What will the product look like? But first, you need a visualization that compares these two variables.
Can you tell from looking at these two histograms how much someone should request in order to get the biggest raise?
Or the relationship could be different—because requested and received aren’t plotted together, you just don’t know.
The histograms show spreads of single variables, but they don’t actually compare them.
Scatterplot Magnets Remember scatterplots from chapter 4? They’re a great visualization for looking at two variables together.
In this exercise, take the data from these three people and use it to place them on the graph.
You’ll need to use other magnets to draw your scale and your axis labels.
Use this x-y axis to plot Bob, Fannie, and Julia.
Scatterplot Magnets You just plotted Bob, Fannie, and Julia on the axis to create a scatterplot.
Q: When can I use scatterplots? A: Try to use them as frequently as you can.
They’re a quick way to show rich patterns in your data.
Any time you have data with observations of two variables, you should think about using a scatterplot.
Q: So any two variables can be put together in a scatterplot?
A: As long as the two variables are in pairs that describe the same underlying thing or person.
In this case, each line of our database represents an instance of an employee asking for a raise, and for each employee, we have a received and a requested value.
Q: What should I look for when I see them?
A: For an analyst, scatterplots are ultimately all about looking for causal relationships between variables.
If high requests cause low raises, for example, you’ll see an association between the two variables on the scatterplot.
The scatterplot by itself only shows association, and to demonstrate causation you’ll need more (for starters, you’d need an explanation of why one variable might follow from the other)
Q: What if I want to compare three pieces of data?
For this chapter, we’re going to stick with two, but you can plot three variables using 3D scatterplots and multi-panel lattice visualizations.
If you’d like a taste of multidimensional scatterplots, copy and run some of the examples of the cloud function that can be found in the help file at help(cloud)
Q: So when do we get to look at the 2D scatterplot for the raise data?
Here’s some ready bake code that will grab some new, more detailed data for you and give you a handy scatterplot.
Make sure you’re connected to the Internet when you run this command, because it pulls data off the Web.
This command loads some new data and doesn’t display any results.
Run these commands inside of  R to generate a scatterplot that shows what people requested and what they received.
This command will show you what’s in the data… always a good idea to take a look.
Scatterplots compare two variables Each one of  the points on this scatterplot represents a single observation: a single person.
Like histograms, scatterplots are another quick and elegant way to show data, and they show the spread of  data.
Scatterplots show how the observations are paired to each other, and a good scatterplot can be part of  how you demonstrate causes.
The head command is a quick way to peek inside any new data you load.
Together, these dots represent all the negotiators in the database.
Of  course you can, but why would you? Remember, you’re trying to build an algorithm here.
What would a line through the data do for you?
This scatterplot tells you all sorts of stuff about how people fare at different raise request levels.
A line could tell your clients where to aim A line through the data could be a really powerful way to predict.
Take another look at the algorithm you’ve been thinking about.
If  you had a line, you could take a Request value and then figure out the point on the line that matches a Received value.
If  it was the right line, you might have your missing piece of  the algorithm.
In order to figure out how to get the right line, why not just try answering a specific question about a single raise with your scatterplot? Here’s an example:
Hint: look at the dots in the 8% requested range!
Take a good look at the scatterplot to answer this question.
Using the scatterplot, how do you determine what an 8% raise request is likely to get you?
Just take the average amount received for dots around the range of amount requested you’re.
If you look around 8% on the x-axis (the amount requested), it looks like the.
So you’ve solved the raise question for one group of people: those who ask for 8 percent.
What happens if you look at the average amount received for all the x-axis strips?
This is the y-axis value for receiving an 8% raise.
Predict values in each strip with the graph of averages.
Man, I wanted to draw a line through the first scatterplot.
I’m dying to draw a line through the graph of averages!
Draw a line through the points on the graph of  averages.
Because that line is the one you’re looking for, the line that you can use to predict raises for everybody.
These dots are the predicted received values for different raise requests.
Here’s the point we created to predict the likely value from an 8% raise request.
The graph of  averages is a scatterplot that shows the predicted y-axis value for each strip on the x-axis.
This graph of  averages shows us what people get, on average, when they request each different level of  raise.
The graph of  averages is a lot more powerful than just taking the overall average.
The overall average raise amount, as you know, is 4 percent.
But this graph shows you a much more subtle representation of  how it all shakes out.
Q: Why is it called a regression? A: The guy who discovered the method, Sir Francis Galton (1822-1911), was studying how the height of fathers predicted the height of their sons.
His data showed that, on average, short fathers had taller sons, and tall fathers had shorter sons.
It seems that the word “regression” has more to do with how Galton felt about numbers on boys and their dads than anything statistical.
The word “regression” is more a historical artifact than something analytically illuminating.
Can I predict raise request from raise amount? Can I predict the x-axis from the y-axis?
A: Sure, but in that case, you’d be predicting the value of a past event.
If someone came to you with a raise she received, you’d predict the raise she had requested.
What’s important is that you always do a reality check and make sure you keep track of the meaning of whatever it is that you’re studying.
Q: Would I use the same line to predict the x-axis from the y-axis?
There are two regression lines, one for x given y and one for y given x.
There are two different graphs of averages: one for each of the two variables.
Q: Does the line have to be straight? A: It doesn’t have to be straight, as long as the regression makes sense.
Nonlinear regression is a cool field that’s a lot more complicated and is beyond the scope of this book.
The regression line is just the line that best fits the points on the graph of  averages.
As you’re about to see, you don’t just have to draw them on your graphs.
You can represent them with a simple equation that will allow you to predict the y variable for any x variable in your range.
Are you sure the line is actually useful? I mean, what’s it doing for ya?
There are a lot of  different ways the scatterplot could look, and a lot of  different regression lines.
The question is how useful is the line in your scatterplot.
Are the lines for each one going to be about as useful as the lines for any other? Or do some regression lines seem more powerful?
The line is useful if your data shows a linear correlation A correlation is a linear association between two variables, and for an association to be linear, the scatterplot points need to roughly follow a line.
You can have strong or weak correlations, and they’re measured by a correlation coefficient, which is also known as r (not to be confused with [big] R, the software program)
In order for your regression line to be useful, data must show a strong linear correlation.
The dots on this scatterplot don’t follow a straight line at all, so the regression line isn’t going to predict accurately.
These dots are all over the place, so the regression line might not be of much use here either.
These two scatterplots show tight, strong correlations, and their regression lines will give you good predictions.
Try using R (the program) to calculate r (the correlation coefficient) on your data raise.
How does the output of the correlation function square with your scatterplot? Does the value match what you believe the association between these two variables to be?
You just told R to give you the correlation coefficient of your two variables.
Correlation Up Close How do you got the correlation coefficient? The actual calculation to get the correlation coefficient is simple but tedious.
Here’s one of  the algorithms that can be used to calculate the correlation coefficient:
Standard units show how many standard deviations each value is from the mean.
The cor function tells R to return the correlation of the two variables.
These are the two variables you want to test for correlation.
How does the output of the correlation function square with your scatterplot?
Both the r-value and the scatterplot show a moderate correlation.
A: You just need to use your best judgment on the context.
When you use the regression line, your judgments should always be qualified by the correlation coefficient.
Q: But how will I know how low of a correlation coefficient is too low?
A: As in all questions in statistics and data analysis, think about whether the regression makes sense.
No statistical tool will get you the precisely correct answer all the time, but if you use those tools well, you will know how close they will get you on average.
Q: How can I tell for sure whether my distribution is linear?
A: You should know that there are fancy statistical tools you can use to quantify the linearity of your scatterplot.
Q: If I show a linear relationship between two things, am I proving scientifically that relationship?
You’re specifying a relationship in a really useful mathematical sense, but whether that relationship couldn’t be otherwise is a different matter.
Is your data quality really high? Have other people replicated your results over and over again?
Do you have a strong qualitative theory to explain what you’re seeing? If these elements are all in place, you can say you’ve demonstrated something in a rigorous analytic way, but “proof” might be too strong a word.
A: Like the histogram, a scatterplot is a really high-resolution display.
With the right formatting, you can fit thousands and thousands of dots on it.
The high-res nature of the scatterplot is one of its virtues.
But here’s a question: how do I use it? I want to calculate specific raises precisely.
You’re going to need a mathematical function in order to get your predictions precise…
Straight lines can be described algebraically using the linear equation.
Your regression line can be represented by this linear equation.
If  you knew what yours was, you’d be able to plug any raise request you like into the x variable and get a prediction of  what raise that request would elicit.
You just need to find the numerical values for a and b, which are values called the coefficients.
The first variable of  the right side of the linear equation represents the y-axis intercept, where your line passes the y-axis.
If  you happen to have dots on your scatterplot that are around x=0, you can just find the point of  averages for that strip.
We’re not so lucky, so finding the intercept might be a little trickier.
Wouldn’t it be dreamy if R would just find the slope and intercept for me?
The slope of  a line is a measure of  its angle.
A line with a steep slope will have a large b value, and one with a relatively flat slope will have a b value close to zero.
Once you know the slope and y-axis intercept of  a line, you can easily fill those values into your linear equation to get your line.
If  you give R the variable you want to predict on the basis of  another variable, R will generate a regression for you in a snap.
The basic function to use for this is lm, which stands for linear model.
When you create a linear model, R creates an object in memory that has a long list of  properties, and among those properties are your coefficients for the regression equation.
No software can tell you whether your regression makes sense.
Here’s a list of all the properties R creates inside your linear model.
Using the numerical coefficients that R finds for you, write the regression equation for your data.
Run the formulas that create a linear model to describe your data and display the coefficients of  the regression line.
How did R calculate the slope? It turns out that the slope of the regression line is equal to the correlation coefficient multiplied by the standard deviation of y divided by the standard deviation of x.
Let’s just say that calculating the slope of a regression line is one of those tasks that should make us all happy we have computers to do our dirty work.
As long as you can see a solid association between your two variables, and as long as your regression makes sense, you can trust your software to deal with the coefficients.
Using the coefficients that R found for you, you can write your regression equation like this.
Run the formulas that create a linear model to describe your data and display the coefficients of  the regression line.
What formula did you create using the coefficients that R calculated?
The regression equation goes hand in hand with your scatterplot.
You’ve done a lot of  neat work crafting a regression of  the raise data.
Does your regression equation help you create a product that will provide crafty compensating consulting for your friends and colleagues?
You still haven’t filled in this part of your algorithm.
Take the example of  the person who wanted to know what sort of  raise he’d receive if  he asked for 8 percent.
A few pages back, you made a prediction just by looking at the scatterplot and the vertical strip around 8 percent on the x-axis.
The regression equation your found with the help of  the lm function gives you the same result.
The regression equation is the Raise Reckoner algorithm By taking a hard look at how people in the past have fared at different negotiation levels for their salaries, you identified a regression equation that can be used to predict raises given a certain level of  request.
This equation will be really valuable to people who are stumped about how much of  a raise to request.
It’s a solid, data-based analysis of other people’s success in getting more money from their employers.
Using it is a matter of  simple arithmetic in R.
Say you wanted to find out what sort of raise can be expected from a 5 percent request.
Your clients will use this equation to calculate their expected raise.
Hopefully, your clients will get raises in line with or greater than your prediction.
Q: How do I know that what people ask for tomorrow will be like they received today?
A: That’s one of the big questions in regression analysis.
Not only “Will tomorrow be like today?” but “What happens to my business if tomorrow is different?” The answer is that you don’t know whether tomorrow will be like today.
The likelihood of change and its implications depend on your problem domain.
Q: How so? A: Well, compare medical data versus consumer preferences.
How likely is it that the human body, tomorrow, will suddenly change the way it works? It’s possible, especially if the environment changes in a big way, but unlikely.
How likely is it that consumer preferences will change tomorrow? You can bet that consumer preferences will change, in a big way.
A: In the online world, for example, a good regression analysis can be very profitable for a period of time, even it stops producing good predictions tomorrow.
To an online bookseller, you’re just a set of data points.
A: Not really—it means the bookseller knows how to get you what you want.
You’re a set of data points that the bookseller runs a regression on to predict which books you’ll want to buy.
When they do, and you start buying different books, the bookseller will run the regression again to accommodate the new information.
Q: So when the world changes and the regression doesn’t work any more, I should update the it?
If you have good qualitative reasons to believe that your regression is accurate, you might never have to change it.
But if your data is constantly changing, you should be running regressions constantly and using them in a way that enables you to benefit if the regressions are correct but that doesn’t destroy your business if reality changes and the regressions fail.
Q: Shouldn’t people ask for the raise they think they deserve rather than the raise they see other people getting?
The question is really part of your mental model, and statistics won’t tell you whether what you’re doing is the right or fair approach.
That’s a qualitative question that you, the analyst, need to use your best judgment in evaluating.
But the short answer is: you deserve a big raise.
Meet your first clients! Write down what sort of raise you think is appropriate for them to request, given their feelings about asking, and use R to calculate what they can expect.
What did you recommend to your first two clients, and what did R calculate their expected raises to be?
Your raise predictor didn’t work out as planned… People were falling all over themselves to get your advice, and you got off  your first round of  recommendations smoothly.
Some of  your clients were pleased as punch about the results, but others were not so happy!
What did your clients do with your advice? What went wrong for those who came back unhappy?
You’ll have to get to the bottom of  this situation in the next chapter…
This guy’s request didn’t pan out so well for him.
So it should be no surprise that your predictions rarely hit the target squarely.
And with the tools in this chapter, you’ll also learn about how to get error under.
Your clients are pretty ticked off In the previous chapter, you created a linear regression to predict what sort of  raises people could expect depending on what they requested.
I was so nervous in the meeting that I can’t even remember what I asked for.
Did you hear that? 0.0% I have some ideas for you about what you can do with your algorithm.
My raise was 0.5% lower than expected, but it’s still a solid raise.
I’m pretty sure I wouldn’t have gotten it if I hadn’t negotiated.
I can’t believe it! I got a 5.0% bigger raise than the algorithm predicted! My negotiation must have scared my boss, and he just started throwing money at me!
The statements on the facing page are qualitative data about the effectiveness of your regression.
Everyone used the same formula, which was based on solid empirical data.
But it looks like people had a bunch of different experiences.
You looked closely at your customers’ qualitative responses to your raise prediction algorithm.
Obviously, not everyone is going to be exactly at the average.
This one got a raise that was close but not exactly what you predicted.
It’s kind of hard to draw any conclusion off a statement like this.
I was so nervous in the meeting that I can’t even remembered what I asked for.
Did you hear that? 0.0% I have some ideas for you about what you can do with your algorithm.
I can’t believe it! I got a 5.0% bigger raise than the algorithm predicted! My negotiation must have scared my boss, and he just started throwing money at me!
My raise was 0.3% lower than expected, but it’s still a solid raise.
I’m pretty sure I wouldn’t have gotten it if I hadn’t negotiated.
The ones below are a little more specific than the previous ones.
Draw arrows to point where each of these people would end up if you plotted their request/raise experiences on your scatterplot.
Draw arrows to show where each of these people would show up on the scatterplot.
You just added new dots to your scatterplot to describe where three of your customers would be shown.
This person doesn’t show up on the scatterplot at all.
This person would show up right in the middle of the biggest clump of observations.
This person shows up above the regression line on the far left of the chart.
The guy who asked for 25% went outside the model.
Using a regression equation to predict a value outside your range of  data is called extrapolation.
Extrapolation is different from interpolation, where you are predicting points within your range of  data, which is what regression is designed to do.
Interpolation is fine, but you should be leery of  extrapolation.
But if you’re going to do it, you need to specify additional assumptions that make explicit your ignorance about what happens outside the data range.
Maybe if  you had more data, you could use your equation to predict what a bigger request would get.
But you’d definitely have to run your regression again on the new data to make sure you’re using the right line.
What would you say to a client who is wondering what he should expect if he requested a 30% raise?
Q: So what exactly might happen outside the data range that’s such a problem?
A: There might not even be data outside the range you’re using.
And if there is, the data could look totally different.
Q: I won’t necessarily have all the points within my data range, though.
A: You’re right, and that’s a data quality and sampling issue.
Q: Isn’t there something to be said for thinking about what would happen under different hypothetical, purely speculative situations?
But it takes discipline to make sure your ideas about hypothetical worlds don’t spill over into your ideas (and actions) regarding the real world.
Q: Isn’t any sort of prediction about the future a type of extrapolation?
A: Yes, but whether that’s a problem depends on what you’re studying.
Is what you’re looking at the sort of thing that could totally change its behavior in the future, or is it something that is pretty stable? The physical laws of the universe probably aren’t going to change much next week, but the associations that apparently explain the stock market might.
These considerations should help you know how to use your model.
If you ask for 25%, I have no idea what will happen.
The data won’t really tell us, but this has been a lucrative year, so a 30% request is reasonable.
How to handle the client who wants a prediction outside the data range.
Which of  these responses would be more beneficial to your client? The second one might satisfy your client’s desire to have a specific prediction, but a crummy prediction might be worse than no prediction.
You’ve basically got two options when your clients want a prediction outside your data range: say nothing at all, or introduce an assumption that you can use to find a prediction.
Here’s an assumption you might use to make the prediction.
You may or may not have good reason to believe this assumption!
And when you’re looking at anyone else’s models, always think about how reasonable their assumptions are and whether they might.
Bad assumptions can make your model completely useless at best and dangerously deceptive at worst.
How might each of these change your model, if it were true?
Economic performance has been about the same for all years in the data range, but this year we made a lot less money.
One boss administered all the raises in the company for the data we have, but he’s left the company and been replaced by another boss.
How you ask makes a big difference in what kind of raise you get.
How might each of these change your model, if it were true?
Economic performance has been about the same for all years in the data range, but this year we made a lot less money.
One boss administered all the raises in the company for the data we have, but he’s left the company and been replaced by another boss.
How you ask makes a big difference in what kind of raise you get.
Only tall people have asked for raises in the past.
The new guy might think differently and break the model.
This is surely true, and the data reflects the variation, so the model’s OK.
If this were true, you’d be able to extrapolate the regression equation.
If this were true, the model wouldn’t apply to shorter people.
Shorter people might do better or worse than taller people.
You don’t have data on how to ask for money...
Yikes! That’d be the end of your business, at least until you have data on the new guy!
Now that you’ve thought through how your assumptions affect your model, you need to change your algorithm so that people know how to deal with extrapolation.
You need to tweak your algorithm to instruct your clients to avoid the trap of extrapolation.
How would you explain to your clients that they need to avoid extrapolation?
How did you modify your compensation algorithm to ensure that your clients don’t extrapolate beyond the data range?
How would you change the algorithm to tell your clients to avoid extrapolation?
Beyond a 22% request, you can’t say what will happen.
Your data range for the amount requested only extends to here.
With your new-and-improved regression formula, fewer clients will run with it into the land of  statistical unknowns.
Well, at least you’re fixing your analysis as you go along.
I’ll still hit you up for advice next time I’m up for a raise.
The guy who got fired because of extrapolation has cooled off.
You’ve only solved part of the problem There are still lots of  people who got screwy outcomes, even though they requested raise amounts that were inside your data range.
This guy got more than he asked for by a pretty big margin.
She asked for a common amount and got just a little bit less than she requested.
People who fall below your regression predictions are still pretty ticked off.
You’ve accounted for people who asked for more than 20% in raises.
How do you explain people who received more than the model predicts?
What does the data for the screwy outcomes look like?
You’re always going to be making predictions of  one sort or another, whether you do a full-blown regression or not.
Those predictions are rarely going to be exactly correct, and the amount by which the outcomes deviate from your prediction is called chance error.
In statistics, chance errors are also called residuals, and the analysis of residuals is at the heart of  good statistical modeling.
While you might never have a good explanation for why individuals residuals deviate from the model, you should always look carefully at the residuals on scatterplots.
If  you interpret residuals correctly, you’ll better understand your data and the use of  your model.
This observation is about 8% higher than what the model predicts.
You’ll always have chance errors in your predictions, and you might never learn why they’re in your data.
Better refine your algorithm some more: this time, you should probably say something about error.
Here are some possible provisions to your algorithm about chance error.
Please note that your own results may vary from the prediction because of chance error.
Hate to break it to ya, but your whole business has just fallen apart.
That last line on your compensation algorithm was the difference between people feeling like you were helping them and people feeling like your product was worthless.
Why would I bet my career on an erroneous analysis?
Does the presence of error mean that the analysis is erroneous?
Specifying error does not mean that your analysis is erroneous or wrong.
It’s just being honest about the strength of your predictions.
And the more your clients understand your predictions, the more likely they’ll be to make good decisions using them.
The more forthcoming you are about the chance error that your clients should expect in your predictions, the better off both of  you will be.
I’m not clairvoyant, but I have a good idea of what to expect…
Head First: It’s just that, because of  you, regression will never really be able to make good predictions.
Chance Error: What? I’m an indispensable part of  regression in particular and any sort of measurement generally.
Think of me as someone who’s always there but who isn’t so scary if  you just know how to talk about me.
Chance Error: Not at all! There are so many contexts where error specification is useful.
In fact, the world would be a better place if  people did a better job expressing error often.
Head First: OK, so here’s what I’m saying to clients right now.
Say someone wants to know what they’ll get if  they ask for 7 percent in a raise.
I say, “The model predicts 7 percent, but chance error means that you probably will get something different from it.”
Head First: That doesn’t sound so scary at all! Is it really that simple?
But the most important thing for you to know is that specifying a range for your prediction is a heck of  a lot more useful (and truthful) than just specifying a single number.
Head First: Can I use error ranges to describe subjective probabilities?
The first guy can’t seriously mean he thinks a stock will go up exactly 10 percent.
Head First: Say, where did you say you came from?
Chance Error: OK, the news might not be so good here.
A lot of  times you’ll have no idea where chance error comes from, especially for a single observation.
Head First: Seriously, you mean it’s impossible to explain why observations deviate from model predictions?
Chance Error: Sometimes you can explain some of  the deviation.
For example, you might be able to group some data points together and reduce the chance error.
Head First: So should it be my job to reduce you as much as possible?
Chance Error: It should be your job to make your models and analyses have as much explanatory and predictive power as you can get.
And that means accounting for me intelligently, not getting rid of  me.
It’s a happy coincidence if  your observed outcome is exactly what your predicted outcome is, but the real question is what is the spread of  the chance error (the residual distribution)
What you need is a statistic that shows how far typical points or observations are, on average, from your regression line.
The standard deviation describes how far typical points are from the mean observation.
The spread or distribution of residuals around the regression line says a lot about your model.
The tighter your observations are around your regression line, the more powerful your line will be.
Q: Do I need to memorize that formula? A: As you’ll see in just a second, it’s pretty easy to calculate the R.M.S.
What’s most important for you to know is that error can be described and used quantitatively, and that you should always be able to describe the error of your predictions.
Q: Do all types of regression use this same formula to describe error?
A: If you get into nonlinear or multiple regression, you’ll use different formulas to specify error.
In fact, even within linear regression there are more ways of describing variation than R.M.S.
There are all sorts of statistical tools available to measure error, depending on what you need to know specifically.
The linear model object your created inside of  R in the last chapter doesn’t just know the y-axis intercept and slope of  your regression line.
It has a handle on all sorts of  statistics pertaining to your model, including the R.M.S.
If  you don’t still have the myLm object you created in R, type in this function before the next exercise:
Under the hood, R is using this formula to calculate the R.M.S.
Instead of filling in the algebraic equation to get the R.M.S.
Take a look at R’s summary of your model by entering this command:
Next, color in an error band across your entire regression line to represent your R.M.S.
The error band should follow along the regression line and the thickness above and below the line should be equal to one R.M.S.
These are the slope and intercept of your regression line.
If you draw a band that’s about 2.3 percentage points above and below your regression line, you get a spread that looks like this.
When you ask R to summarize your linear model object, it gives you a bunch of  information about what’s inside the object.
Not only do you see your regression coefficients, like you saw in the previous chapter, but you also see the R.M.S.
You’re ready to have another go at your compensation algorithm.
Can you incorporate a more nuanced conception of chance error?
How would you change this algorithm to incorporate your R.M.S.
Can you give me a prediction with a lower amount of error, please?
Is there anything you can do to make this regression more useful? Can you look at your data in a way that reduces the error?
Let’s take a look at your new algorithm, complete with R.M.S.
Most but not all raises will be within a range of 2.5% more or less than the prediction.
This statement tells your clients the range they should expect their own raise to be inside of.
Do you see segments where the residuals are fundamentally different?
For each strip on the scatterplot, color in what you think the error is within that strip.
Look at the data and think about what it must mean.
Jim: Oh man, that’s nuts! It looks like there’s a different spread of  predictions for every strip along the scatterplot!
How in the world do we explain that to our customers?
Maybe we should ask why the error bands look the way they do.
It might help us understand what’s happening with all these raises.
At the start of  the scale, there’s kind of  a big spread that narrows as soon as we hit 5 percent or so.
Joe: Well, people are being conservative about what they’re asking for.
Your boss might reward you for being so bold, or she might kick your butt for being so audacious.
Jim: Once you start asking for a lot of  money, anything can happen.
Joe: You know, guys, I think we’ve got two different groups of  people in this data.
In fact, I think we may even have two different models.
What would your analysis look like if you split your data?
Segmentation is all about managing error Splitting data into groups is called segmentation, and you do it when having multiple predictive models for subgroups will result in less error over all than one model.
When we looked at the strips, we saw that the error in the two regions is quite different.
In fact, segmenting the data into two groups, giving each a model, would provide a more realistic explanation of  what’s going on.
Segmenting your data into two groups will help you manage error by providing more sensible statistics to describe what happens in each region.
Draw what you think the regression lines are for these two sets of data.
That’s OK—just do your best to estimate where the line goes.
Remember: the regression line is the line that best fits the graph of averages.
This line through the people who make low requests should fit the data much better than the original model.
The regression line through the more aggressive negotiators should have a different slope from the other line.
Two regression lines, huh? Why not twenty? I could draw a separate regression line for each strip… how would you like that?!?
Why stop at two regression lines? Would having more lines—a lot more, say—make your model more useful?
A: If you’ve got a good reason to do it, then go right ahead.
Q: I could go nuts and split the data into 3,000 groups.
And if you did, how powerful do you think your 3,000 regressions would be at predicting people’s raises?
A: If you did that, you’d be able to explain everything.
All your data points would be accounted for, and the R.M.S.
But your models would have lost all ability to predict anything.
Q: So what would an analysis look like that had a whole lot of predictive power but not a lot of explanatory power?
A: Sure, but it’s a model that has incredible predictive power.
The chances are that no one you ever meet will be outside that range.
With a model like that, you sacrifice explanatory power to get predictive power.
Q: So that’s what zero error looks like: no ability to predict anything.
A: That’s it! Your analysis should be somewhere between having complete explanatory power and complete predictive power.
And where you fall between those two extremes has to do with your best judgemnt as an analyst.
Your prediction will be accurate, but it’s not precise enough to be useful.
Two segments in your raise regression will let you fit the data without going to the extreme of  too much explanation or too much prediction.
For each of these two models, color in bands that represent R.M.S.
Draw bands to describe the distribution of residuals for each model.
Your segmented models manage error better than the original model.
They’re more powerful because they do a better job of  describing what’s actually happening when people ask for raises.
Your new model for timid negotiators does a better job fitting the data.
The slope of  the regression line is more on target, and the R.M.S.
Your new model for aggressive negotiators is a better fit, too.
It’s time to implement those new models and segments in R.
Once you have the models created, you’ll be able to use the coefficients to refine your raise prediction algorithm.
This code tells R to look only at the data in your database for negotiators…
Create new linear model objects that correspond to your two segments by typing the following at the command line:
Look at the summaries of both linear model objects using these versions of the summary() function.
When you tell R to create the new models, R doesn’t display anything in the console.
Here are the slope and intercepts for your new regression lines.
You now have everything you need to create a much more powerful algorithm that will help your customers understand what to expect no matter what level of raise they request.
Time to toss out the old algorithm and incorporate everything you’ve learned into the new one.
Using the slopes and intercepts of your new models, write the equations to describe both of them.
How close to the prediction should your client expect her own raise to be, depending on which model she uses?
You used the coefficients to fill in the regression equations.
Your clients are returning in droves Your new algorithm is really starting to pay off, and everyone’s excited about it.
Now people can decide whether they want to take the riskier strategy of  asking for a lot of money or just would rather play it safe and ask for less.
The people who want to play it safe are getting what they want, and the risk-takers understand what they’re getting into when they ask for a lot.
There is just one of me, but so many of them…
How do you structure really, really multivariate data? A spreadsheet has only two dimensions: rows and columns.
The Dataville Dispatch wants to analyze sales The Dataville Dispatch is a popular news magazine, read by most of  Dataville’s residents.
And the Dispatch has a very specific question for you: they want to tie the number of  articles per issue to sales of  their magazine and find an optimum number of  articles to write.
They want each issue be as cost effective as possible.
If  putting a hundred articles in each issue doesn’t get them any more sales than putting fifty articles in each issue, they don’t want to do it.
On the other hand, if  fifty article issues correlate to more sales than ten article issues, they’ll want to go with the fifty articles.
Software Development How a whiteboard could save your next project.
They’ll give you free advertising for your analytics business for a year if  you can give them a thorough analysis of  these variables.
Here’s the data they keep to track their operations The Dispatch has sent you the data they use to manage their operations as four separate spreadsheet files.
The files all relate to each other in some way, and in order to analyze them, you’ll need to figure out how.
What do you need to know in order to compare articles to sales?
Looks like they keep track of a lot of stuff.
You need to know how the data tables relate to each other.
The table or tables you create to get the answers that the Dispatch wants will tie article count to sales.
So you need to know how all these tables relate to each other.
What specific data fields tie them together? And beyond that, what is the meaning of  the relationships?
From: Dataville Dispatch To: Head First Subject: About our data Well, each issue of the magazine has a bunch of articles, and each article has an author, so in our data we tie the authors to the articles.
When we have an issue ready, we call our list of wholesalers.
They place orders for each issue, which we record in our sales table.
The “lot size” in the table you’re looking at counts the number of copies of that issue that we sell—usually in denominations of 100, but sometimes we sell less.
Here is what the Dispatch has to say about how they maintain their data.
They have a lot of stuff to record, which is why they need all these spreadsheets.
Draw arrows and use words to describe the relationship between the things being recorded in each spreadsheet.
Draw arrows between the tables and describe how each relates to the other.
What relationships did you discover among the spreadsheets that the Dataville Dispatch keeps?
Each sale refers to a bundle of copies (usually around 100) of one issue.
A database is a collection of data with well-specified relations to each other A database is a table or collection of  tables that manage data in a way that makes these relationships explicit to each other.
Database software manages these tables, and you have a lot of  different choices of  database software.
What’s really important is that you know the relationships within the software of  the data you want to record.
Out-of-the-box implementation There’s a ton of different kinds of database software out there.
For organizations that collect the same type of data, out-of-the-box databases specifically manage that sort of data.
Other times, people need something really specific to their needs, and they’ll make their own database with Oracle, MySQL, or something else under the hood.
So how do you use this knowledge to calculate article count and sales total for each issue?
Trace a path through the relations to make the comparison you need.
When you have a bunch of tables that are separate but linked through their data, and you have a question you want to answer that involves multiple tables, you need to trace the paths among the tables that are relevant.
This spreadsheet isn’t going to help you compare article count and sales.
Once you know which tables you need, then you can come up with a plan to tie the data together with formulas.
Here, you need a table that compares article count and sales for each issue.
Let’s create a spreadsheet like the one in the facing page and start by calculating the “Article count” for each issue of the Dispatch.
Remember, you don’t want to mess up an original file! Call your new file “dispatch analysis.xls”
Tell your spreadsheet to move the file to your dispatch analysis.xls document.
Create a column for Article count on your issue sheet.
Write a COUNTIF formula to count the number of articles for that issue, and copy and paste that formula for each issue.
Save this file under a new name, so you don’t destroy the original data.
What sort of article count did you find each issue to have?
Remember, you don’t want to mess up an original file! Call your new file “dispatch analysis.xls”
Tell your spreadsheet to move the file to your dispatch analysis.xls document.
Create a column for Article count on your issue sheet.
Write a COUNTIF formula to count the number of articles for that issue, and copy and paste that formula for each issue.
This is the “articles” tab in your dispatch analysis spreadsheet.
It counts the number of times each issue shows up in the list of articles.
The formula looks at the “articles” tab in your spreadsheet.
Create a new column for Sales on the same sheet you used to count the articles.
Copy that formula and then paste it for each of the other issues.
Cool! When you add the sales figures to your spreadsheet, keep in mind that the numbers just refer to units of the magazine, not dollars.
I really just need you to measure sales in terms of the number of magazines sold, not in dollar terms.
Add a field for sales totals to the spreadsheet you are creating.
What formula did you use to add sales to your spreadsheet?
The first argument of the SUMIF formula looks at the issues.
The second argument looks at the specific issue whose sales you want to count.
The third argument points to the actual sales figures you want to sum.
Open R and type the getwd() command to figure out where R keeps its data files.
Then, in your spreadsheet, go to File > Save As...
This is exactly the spreadsheet you need to tell you whether there is a relationship between the number of  articles that the Dataville Dispatch publishes every issue and their sales.
But it’d be a little easier to understand if it.
This function tells you R’s working directory, where it looks for files.
Save your spreadsheet data as a CSV in R’s working directory.
Did you find an optimal value in the data you loaded?
The head command shows you what you have just loaded...
The jitter command adds a little bit of noise to your numbers, which separates them a little and makes them easier to see on the scatterplot.
Try running the same command without adding jitter; isn’t the result hard to read?
Make sure that the field names in your plot formula match the field names that head shows you are contained in the data frame.
When there were five articles in the Dispatch, only 1,000 or fewer copies of the issue sold.
It looks like there’s a pretty steady increase in sales as more articles are added…
From: Dataville DispatchSubject: Thank you Thank you! This is a really big help for us.
I’d kind of suspected that a relationship like this was the case, and your analysis demonstrated it dramatically.And congratulations on a free year of ads! It’ll be our pleasure to help spread the word about your amazing skills.I think I’m going to have a lot more questions for you like this one.
Q: Do people actually store data in linked spreadsheets like that?
Sometimes you’ll receive extracts from larger databases, and sometimes you’ll get data that people have manually kept linked together like that.
Q: Basically, as long as there are those codes that the formulas can read, linking everything with spreadsheets is tedious but not impossible.
A: Well, you’re not always so lucky to recieve data from multiple tables that have neat little codes linking them together.
Often, the data comes to you in a messy state, and in order to make the spreadsheets work together with formulas, you need to do some clean-up work on the data.
You’ll learn more about how to do that in the next chapter.
Q: Is there some better software mechanism for tying data from different tables together?
Copying and pasting all that data was a pain It would suck to go through that process every time someone wanted to query (that is, to ask a question of) their data.
Besides, aren’t computers supposed to be able to do all that grunt work for you?
Wouldn't it be dreamy if there were a way to maintain data relations in a way that’d make it easier to ask the database questions? But I know it's just a fantasy…
Relational databases manage relations for you One of  the most important and powerful ways of  managing data is the RDBMS or relational database management system.
Relational databases are a huge topic, and the more you understand them, the more use you’ll be able to squeeze out of  any data you have stored in them.
What is important for you to know is that the relations that the database enforces among tables are quantitative.
The database doesn’t care what an “issue” or an “author” is; it just knows that one issue has multiple authors.
Each row of  the RDBMS has a unique key, which you’ll often see called IDs, and it it uses the keys to make sure that these quantitative relationships are never violated.
Once you have a RDBMS, watch out: well-formed relational data is a treasure trove for data analysts.
This diagram shows the relationships and data tables inside a relational database.
If the Dataville Dispatch had a RDBMS, it would be a lot easier to come up with analyses like the one you just did.
Circle the tables that you’d need to pull together into a single table in order to show which author has the articles with the most web hits and web comments.
Then draw the table below that would show the fields you’d need in order create those scatterplots.
It was about time that the Dispatch loaded all those spreadsheets into a real RDBMS.
With the diagram you brainstormed, along with the managing editor’s explanation of  their data, a database architect pulled together this relational database.
Now that we’ve found the optimum article count, we should figure out who our most popular authors are so that we can make sure they’re always in each issue.
You could count the web hits and comments that each article gets for each author.
This tables is new… it’s a listing of the comments each article gets in the online edition.
You need a table that draws these three tables from the database together.
In the last table you used, each row represented an issue, but now each row represents an article.
Dataville Dispatch extracted your data using the SQL language SQL, or Structured Query Language, is how data is extracted from relational databases.
You can get your database to respond to your SQL questions either by tying the code directly or using a graphical interface that will create the SQL code for you.
We’re going to use a more powerful function to create scatterplots this time.
Using these commands, load the lattice package and then run the xyplot formula to draw a “lattice” of scatterplots:
What author or authors perform the best, based on these metrics?3
You don’t have to learn SQL, but it’s a good idea.
What’s crucial is that you understand how to ask the right questions of  the database by understanding the tables inside the database and the relations among them.
Make sure you’re connected to the Internet for this command.
Here’s the output from the query that gets you the table you want.
The query that created this data is much more complex than the example on the left.
What do your scatterplots show? Do certain authors get greater sales?
This symbol tells the xyplot function to group the scatterplots by author name.
We’re going to use a more powerful function to create scatterplots this time.
Using these commands, load the lattice package and then run the xyplot formula to draw a “lattice” of scatterplots:
This array of scatterplots shows web hits and comments for each article, grouped by author.
What author or authors perform the best on these metrics?3
The web stats are all over the map, with authors performing very differently from each other.
Comparison possibilities are endless if your data is in a RDBMS The complex visualization you just did with data from the Dispatch’s RDMS just scratches the tip of  the iceberg.
And what that means for you as an analyst is that the range of  comparisons relational databases give you the ability to make is just enormous.
If  you can envision it, a RDBMS can tie data together for powerful comparisons.
I’d always suspected that Rafaela and Destiny were our star writers, but this shows that they’re way ahead of everyone.
Big promotion for them! All this information will make us a much leaner publication while enabling us to better reward our authors’ performance.
Here’s what the managing editor has to say about your most recent analysis.
Think about how far you can reach across this sea of tables to make a brilliant comparison!
The Dataville Dispatch’s database structure isn’t anywhere near this complex, but databases easily get this large.
I can’t believe we had this data all along but never could figure out how to use it.
The authors and editors of  the Dataville Dispatch was so impressed by your work that they decided to feature you in their big data issue! Nice work job.
Looks like you made some friends on the writing staff!
I do my best work when everything’s where it’s supposed to be.
If your data’s not neat, you can’t slice it or dice it, run.
With a clear vision of how you need it to look and a few text.
Your newest client, Head First Head Hunters, just received a list of  job seekers from a defunct competitor.
They had to spend big bucks to get it, but it’s hugely valuable.
The people on this list are the best of  the best, the most employable people around.
The dirty secret of data analysis The dirty secret of  data analysis is that as analyst you might spend more time cleaning data than analyzing it.
Data often doesn’t arrive perfectly organized, so you’ll have to do some heavy text manipulation to get it into a useful format for analysis.
But your work as a data analyst may actually involve a lot of this…
What will be your first step for dealing with this messy data? Take a look at each of these possibilities and write the pros and cons of each.
Ask the client what she wants to do with the data once it’s cleaned.2
Which of these options did you choose as your first step?
Ask the client what she wants to do with the data once it’s cleaned.2
It’ll take forever, and there’s a good chance I’ll transcribe it incorrectly, messing up.
If this is the only way to fix the data, we’d better be sure before going this route.
With an idea of what the client wants to do with the data, I can make.
A powerful formula or two would definitely help out, once we have an idea of what the data.
Head First Head Hunters wants the list for their sales team.
Even though the raw data is a mess, it looks like they just want to extract names and phone numbers.
We need a call list for our sales team to use to contact prospects we don’t know.
The list is of job seekers who have been placed by our old competitor, and we.
Draw a picture that shows some columns and sample data for what you want the messy data to look like.
The data looks like a list of names, which is what we’d expect from the client’s description of it.
What you need is a clean layout of those names.
Add a few lines of sample data to show what you’d like the records to look like.
How would you like your data to look once you’ve cleaned it up?
You can see the information you want that’s been all mashed together in Column A...
You need the name and phone fields separated from each other.
When everything’s separate, you can sort the data by field, filter it, or pipe it to a mail merge or web page or whatever else.
This ID field is useful, since it will let you make sure that the records are unique.
It’s true: thinking about what neat data looks like won’t actually make it neat.
But we needed to previsualize a solution before getting down into the messy data.
Let’s take a look at our general strategy for fixing messy data and then start coding it…
It may go without saying, but cleaning data should begin like any other data project: making sure you have copies of the original data so that you can go back and check your work.
Once you’ve figured out what you need your data to look like, you can then proceed to identify patterns in the messiness.
The last thing you want to do is go back and change the data line by line—that would take forever—so if  you can identify repetition in the messiness you can write formulas and functions that exploit the patterns to make the data neat.
Is there a pattern to how the fields are separated from each other?
Then, patterns in hand, you can get down to the work of actually fixing the data.
As you’ll discover, this process is often iterative, meaning you will clean and restructure the data over and over again until you get the results you need.
Definitely! All the data is in Column A with the fields mashed together.
Excel has a handy tool for splitting data into columns when the fields are separated by a delimiter (the technical term for a character that makes the space between fields)
Select Column A in your data and press the Text to Columns button under the Data tab…
In the first step, tell Excel that your data is split up by a delimiter.
In the second step, tell Excel that your delimiter is the # character.
What’s the pattern you’d use to fix the FirstName column?
Using Excel’s Convert Text to Column Wizard is a great thing to do if  you have simple delimiters separating your fields.
The first and last names, for example, both appear to have junk characters inside the fields.
You’ll have to come up with a way to get rid of  them!
What are you going to do to fix the FirstName field?
Now that the pieces of data are separated, you can manipulate them individually if you want to.
Is there a pattern to the messiness in the FirstName field?
You need some software tool to pull out all the carat characters.
At the beginning of every name there is a ^ character.
We need to get rid of all of them in order.
Returns a numerical value for a number stored as text.
Replaces text you don’t want in a cell with new text that you specify.
Tells you where to find a search string within a cell.
Which function do you think you’ll need to use the clean up the Name column?
Returns a numerical value for a number stored as text.
Replaces text you don’t want in a cell with new text that you specify.
Tells you where to find a search string within a cell.
Which function do you think you’ll need to use the clean up the Name column?
Here’s the formula we need to use to replace the ^ characters in the Name column.
Q: Am I limited to just these formulas? What if I want to take the characters on the left and right of a cell and stick them together? It doesn’t look there’s a formula that does just that.
A: There isn’t, but if you nest the text functions inside of each other you can achieve much more complicated text manipulations.
For example, if you wanted to take the first and last characters inside of cell A1 and stick them together, you’d use this formula:
Q: So I can nest a whole bunch of text formulas inside of each other?
A: You can, and it’s a powerful way to manipulate text.
There’s a problem, though: if your data is really messy and you have to nest a whole bunch of formulas inside of each other, your entire formula can be almost impossible to read.
Q: Who cares? As long as it works, I’m not going to be reading it anyway.
A: Well, the more complex your formula, the more likely you’ll need to do subtle tweaking of it.
And the less readable your formula is, the harder that tweaking will be.
Q: Then how do I get around the problem of formulas that are big and unreadable?
A: Instead of packing all your smaller formulas into one big formula, you can break apart the small formulas into different cells and have a “final” formula that puts them all together.
That way, if something is a little off, it’ll be easier to find the formula that needs to be tweaked.
Q: You know, I bet R has much more powerful ways of doing text manipulation.
A: It does, but why bother learning them? If Excel’s SUBSTITUTE formula handles your issue, you can save your self some time by skipping R.
Copy this formula and paste it all the way down to the end of  the data in Column H.
You cleaned up all the first names Using the SUBSTITUTE formula, you had Excel grab the ^ symbol from each first name and replace it with nothing, which you specified by two quotation marks (“”)
Lots of  different software lets you get rid of  crummy characters by replacing those characters with nothing.
All of these values are outputs from the SUBSTITUTE formula.
To make the original first name data go away forever copy the H column and then Paste Special > Values to turn these values into actual text rather than formula outputs.
After that you can delete the FirstName column so that you never have to see those pesky ^ symbols again.
That first name pattern was easy because it was just a single character at the beginning that had to be removed.
But the last name is going to be harder, because it’s a tougher pattern.
Let’s try using SUBSTITUTE again, this time to fix the last names.
What would you tell SUBSTITUTE to replace? Here’s the syntax again:
SUBSTITUTE(your reference cell, the text you want to replace, what you want to replace it with)
In order to make SUBSTITUTE work, you’d have to write a separate formula for each last name.
And typing a bajillion formulas like this defeats the purpose of using formulas to begin with.
Formulas are supposed to save you the trouble of typing and retyping!
The SUBSTITUTE function looks for a pattern in the form of  a single text string to replace.
The problem with the last names are that each has a different text string to replace.
And that’s not all: the pattern of  messiness in the LastName field is more complex in that the messy strings show up in different positions within each cell and they have different lengths.
You can’t just type in the value you want replaced, because that value changes from cell to cell.
The length of this text is seven characters.The messiness here starts on the.
The formula works, but there’s a problem: it’s starting to get really hard to read.
That’s not a problem if  you write formulas perfectly the first time, but you’d be better off with a tool that has power and simplicity, unlike this nested CONCATENATE formula.
Once you get familiar with Excel text formulas, you can nest them inside of  each other to do complex operations on your messy data.
Wouldn't it be dreamy if there were an easier way to fix complex messes than with long, unreadable formulas like that one.
Here’s a simple regular expression pattern that looks for the letter “a”
When you give this pattern to R, it’ll say whether there’s a match.
Regular expressions are the ultimate tool for cleaning up messy data.
Lots of  platforms and languages implement regular expressions, even though Excel doesn’t.
To learn more about the full regex specification and syntax, type ?regex in R.
Well get on with it! Those prospects are hot and are only getting colder.
I want our sales force to start calling people like yesterday!
Load your data into R and take a look at what you’ve got with the head command.
You can either save your Excel file as a CSV and load the CSV file into R, or you can use the web link below to get the most recent data.
Then take a look at your work by running the head command to see the first few rows of  your table.
This command reads the CSV into a table called hfhh.
The sub command used the pattern you specified and replaced all instances of  it with blank text, effectively deleting every parenthetical text string in the LastName column.
If  you can find a pattern in the messiness of  your data, you can write a regular expression that will neatly exploit it to get you the structure you want.
Your regular expression has three parts: the left parenthesis, the right parenthesis, and everything in between.
This is blank text, which replaces text that matches the pattern with nothing.
The left parenthesis (the backslashes are “escape” characters that tell R that the parenthesis is not itself an R expression)
Q: Some of those regular expression commands look really hard to read.
A: They can be hard to read because they’re really concise.
That economy of syntax can be a real benefit when you have crazy-complex patterns to decode.
Regular expressions are easy to get the hang of but (like anything complex) hard to master.
Just take your time when you read them, and you’ll get the hang of them.
Q: What if data doesn’t even come in a spreadsheet? I might have to extract data from a PDF, a web page, or even XML.
A: Those are the sorts of situations where regular expressions really shine.
As long as you can get your information into some sort of text file, you can parse it with regular expressions.
Web pages in particular are a really common source of information for data analysis, and it’s a snap to program HTML tag patterns into your regex statements.
A: Java, Perl, Python, JavaScript… all sorts of different programming languages use them.
Q: If regular expressions are so common in programming languages, why can’t Excel do regular expressions?
A: On the Windows platform, you can use Microsoft’s Visual Basic for Applications (VBA) programming language inside of Excel to run regular expressions.
But most people would sooner just use a more powerful program like R than take the trouble to learn to program Excel.
Oh, and since VBA was dropped from the recent release of Excel for Mac, you can’t use regex in Excel for Mac, regardless of how badly you might want to.
Better write your new work to a CSV file for your client.
Regardless of  whether your client is using Excel, OpenOffice, or any statistical software package, he’ll be able to read CSV files.
This file will be found in your R working directory, which R will tell you about with the getwd() command.
I can’t use this! Look at all the duplicate entries!
It could be that there are two separate people named Alexia Rasmussen, of course.
But then again, both records here have PersonID equal to 127, which would suggest that they are the same person.
Maybe Alexia is the only duplicate name and the client is just reacting to that one mistake.
To find out, you’ll need to figure out how you can see duplicates more easily than just looking at this huge list as it is.
The client has a bit of  a problem with your work.
Let’s get a better look at the duplicates in your list by sorting the data.
In R, you sort a data frame by using the order function inside of the subset brackets.
Because the PersonID field probably represents a unique number for each person, that makes it a good field to use it to sort.
After all, it’s possible that there’s more than one person in the data named “John Smith.” Next, run the head command to see what you’ve created:
Sort your data to show duplicate values together If  you have a lot of  data, it can be hard to see whether values repeat.
It’s a lot easier to see that repetition if  the lists are sorted.
Sorted It’s hard to see this list’s repetitions, especially if it’s a long list.
That’s because it’s often hard to see all the data at once, and sorting the data by different fields lets you visualize groupings in a way that will help you find duplicates or other weirdness.
Why would our competitor store duplicate data? Is this some sort of joke?
Why do you think the same names show up repeatedly?
If you look at the far right column, you can.
If  elements of  your messy list repeat, then the data probably come from a relational database.
In this case, your data is the output of  a query that consolidated two tables.
Because you understand RDBMS architecture, you know that repetition like what we see here stems from how queries return data rather than from poor data quality.
So you can now remove duplicate names without worrying that something’s fundamentally wrong with your data.
The original database for this data might have looked like this.
Remove duplicate names Now that you know why there are duplicate names, you can start removing them.
Both R and Excel have quick and straightforward functions for removing duplicates.
So now that you have the tool you need to get rid of  those pesky duplicate names, let’s clean up your list and give it back to the client.
Remove the CallID and Time fields, which the client doesn’t need and which are the cause of  your duplicate names:
Take a look at your results and write them to a new CSV:4
Excel will ask you to specify which columns contain the duplicate values, and data from other columns that isn’t duplicated will be deleted.
Make sure your cursor is placed in your data and click this button:
That’s it! Be sure you assign the resulting value to a new name so that you can use the data unique returns.
The unique function returns a vector or data frame like the one you specify, except that the duplicates are removed.
You created nice, clean, unique records This data looks totally solid.
All from following the basic steps of  cleaning a messy data set:
With a clean data set of  live prospects, HFHH is picking up more clients than ever, and they’d never have been able to do it without your data cleaning skills.
This is great! We’ve got more new clients than ever!
We’re sad to see you leave, but there’s nothing like taking what you’ve learned and putting it to use.
You’re just beginning your data analysis journey, and we’ve put you in.
We’re dying to hear how things go, so drop us a line at the Head First Labs.
You’re not finished yet, are you? But there is so much left!
But data analysis is a vast and constantly evolving field, and there’s so much left the learn.
In this appendix, we’ll go over ten items that there wasn’t enough room to cover in this.
Everything else in statistics Statistics is a field that has a huge array of tools and technologies for data analysis.
It’s so important for data analysis, in fact, that many books about “data analysis” are really statistics books.
Here is an incomplete list of  the tools of  statistics not covered in Head First Data Analysis.
Much of  what you have learned in this book, however, has raised your awareness of  deep issues involving assumptions and model-building, preparing you not only to use the tools of  statistics but also to understand their limitations.
The better you know statistics, the more likely you are to do great analytical work.
It’s a great idea to learn about all of these topics if you’re a data analyst.
Excel skills This book has assumed that you have basic spreadsheet skills, but skilled data analysts tend to be spreadsheet ninjas.
Compared to programs like R and subjects like regression, it’s not terribly hard to master Excel.
The best data analysts can do spreadsheets in their sleep.
Good data analysts spend a lot of  time reading and rereading the work of  data great analysts, and Edward Tufte is unique not only in the quality of  his own work but in the quality of  the work of  other analysts that he collects and displays in his books.
His books are a gallery of  the very best in the visualization of data.
What’s more, his book Data Analysis for Public Policy is about as good a book on regression as you’ll ever find, and you can download it for free at this website: http://www.edwardtufte.com/tufte/dapp/
From this raw data, you can create a bunch of different pivot table summaries.
Pivot tables are one of  the more powerful data analysis tools built into spreadsheets and statistical software.
They’re fantastic for exploratory data analysis and for summarizing data extracted from relational databases.
Much of  its power comes from a global community of  user and contributors who contribute free packages with functions you can use for your analytic domain.
You got a taste of  this community when you ran the xyplot function from the lattice, a legendary package for data visualization.
Your installation of R can have any combination of packages that suits your needs.
These data points aren’t linear, but there’s a clear pattern.
Even if  your data do not exhibit a linear pattern, under some circumstances, you can make predictions using regression.
One approach would be to apply a numerical transformation on the data that effectively makes it linear, and another way would be to draw a polynomial rather than linear regression line through the dots.
Also, you don’t have to limit yourself  to predicting a dependent variable from a single independent variable.
Sometimes there are multiple factors that affect the variable, so in order to make a good prediction, you can use the technique of multiple regression.
You use data this equation to predict a dependent variable from a single independent variable.
But you can also write an equation that predicts a dependent verbal from multiple independent variables.
When people are trying to explain events, they do a great job at fitting models to evidence.
But they do a terrible job at deciding against using explanatory models at all.
While the hypothesis testing technique you learned in Chapter 5 is very general and can accommodate a variety of analytical problems, null-alternative testing is the statistical technique many (especially in academia and science) have in mind when they hear the expression.
This tool is used more often than it’s understood, and Head First Statistics is a great place to start if  you’d like to learn it.
Given my data, what is the viability of the null hypothesis?
I never know what this guy has in store for me.
Not only does Google Docs offer a fully functioning online spreadsheet, it has a Gadget feature that offers a large array of  visualizations.
What’s more, Goolge Docs has a variety of  functions that offer access to real-time online data sources.
You can make a lot of different visualizations using the Gadget feature in Google Docs.
It’s fun to explore the different charts that you can do with Google Docs.
Your expertise You’ve learned many tools in this book, but what’s more exciting than any of  them is that you will combine your expertise in your domain of  knowledge with those tools to understand and improve the world.
Yes, I’d like to order up a wordclass statistical software package that will unleash my analytic potential and, uh, no hassles with.
But fortunately, getting R installed and started is something you can accomplish in just a.
Installing the powerful, free, open source statistical software R can be done in these four quick and easy steps.
You should have no problem finding a mirror near you that serves R for Windows, Mac, and Linux.
Once you’ve downloaded the program file for R, double-click on it to start the R installer.
Accept all the default options for loading R by clicking Next through these windows, and let the installer do its work..
Click the R icon on the desktop or Start Menu, and you’re ready to start using R.
Just accept all the default configurations for R by clicking Next.
Here’s what R window looks like when you start it for the first time.
I want to optimize now! I don’t want to have to install.
Some of  the best features of  Excel aren’t installed by default.
That’s right, in order to run the optimization from Chapter 3 and the histograms from.
Chapter 9, you need to activate the Solver and the Analysis ToolPak, two extensions.
Installing the Analysis ToolPak and Solver in Excel is no problem if  you follow these simple steps.
Make sure that the Analysis ToolPak and the Solver Add-in boxes are checked, and then press OK.
Take a look at the Data tab to make sure that the Data Analysis and Solver buttons are there for you to use.
Make sure these buttons can be seen under the Data tab.
Now you’re ready to start running optimizations, histograms, and much more.
Acme Cosmetics needs your help The CEO wants data analysis to help increase sales Data analysis is careful thinking about evidence Define the problem Your client will help you define your problem Acme’s CEO has some feedback for you Break the problem and data into smaller pieces Now take another look at what you know Evaluate the pieces Analysis begins when you insert yourself Make a recommendation Your report is ready The CEO likes your work An article just came across the wire You let the CEO’s beliefs take you down the wrong path Your assumptions and beliefs about the world are your mental model Your statistical model depends on your mental model Mental models should always include what you don’t know The CEO tells you what he doesn’t know Acme just sent you a huge list of raw data Time to drill further into the data General American Wholesalers confirms your impression Here’s what you did Your analysis led your client to a brilliant decision.
You’re now in the bath toy game Constraints limit the variables you control Decision variables are things you can control You have an optimization problem Find your objective with the objective function Your objective function Show product mixes with your other constraints Plot multiple constraints on the same chart Your good options are all in the feasible region Your new constraint changed the feasible region Your spreadsheet does optimization Solver crunched your optimization problem in a snap Profits fell through the floor Your model only describes what you put into it Calibrate your assumptions to your analytical objectives Watch out for negatively linked variables Your new plan is working like a charm Your assumptions are based on an ever-changing reality.
New Army needs to optimize their website The results are in, but the information designer is out The last information designer submitted these three infographics What data is behind the visualizations? Show the data! Here’s some unsolicited advice from the last designer Too much data is never your problem Making the data pretty isn’t your problem either Data visualization is all about making the right comparisons Your visualization is already more useful than the rejected ones Use scatterplots to explore causes The best visualizations are highly multivariate Show more variables by looking at charts together The visualization is great, but the web guru’s not satisfied yet Good visual designs help you think about causes The experiment designers weigh in The experiment designers have some hypotheses of their own The client is pleased with your work Orders are coming in from everywhere!
Gimme some skin… When do we start making new phone skins? PodPhone doesn’t want you to predict their next move Here’s everything we know ElectroSkinny’s analysis does fit the data ElectroSkinny obtained this confidential strategy memo Variables can be negatively or positively linked Causes in the real world are networked, not linear Hypothesize PodPhone’s options You have what you need to run a hypothesis test Falsification is the heart of hypothesis testing Diagnosticity helps you find the hypothesis with the least disconfirmation You can’t rule out all the hypotheses,but you can say which is strongest You just got a picture message… It’s a launch!
The doctor has disturbing news Let’s take the accuracy analysis one claim at a time How common is lizard flu really? You’ve been counting false positives All these terms describe conditional probabilities You need to count false positives, true positives, false negatives, and true negatives Your chances of having lizard flu are still pretty low Do complex probabilistic thinking with simple whole numbers Bayes’ rule manages your base rates when you get new data You can use Bayes’ rule over and over Your second test result is negative The new test has different accuracy statistics New information can change your base rate What a relief!
Backwater Investments needs your help Their analysts are at each other’s throats Subjective probabilities describe expert beliefs Subjective probabilities might show no real disagreement after all The analysts responded with their subjective probabilities The CEO doesn’t see what you’re up to The CEO loves your work The standard deviation measures how far points are from the average You were totally blindsided by this news Bayes’ rule is great for revising subjective probabilities The CEO knows exactly what to do with this new information Russian stock owners rejoice!
LitterGitters submitted their report to the city council The LitterGitters have really cleaned up this town The LitterGitters have been measuring their campaign’s effectiveness The mandate is to reduce the tonnage of litter Tonnage is unfeasible to measure Give people a hard question, and they’ll answer an easier one instead Littering in Dataville is a complex system You can’t build and implement a unified litter-measuring model Heuristics are a middle ground between going with your gut and optimization Use a fast and frugal tree Is there a simpler way to assess LitterGitters’ success? Stereotypes are heuristics Your analysis is ready to present Looks like your analysis impressed the city council members.
Your annual review is coming up Going for more cash could play out in a bunch of different ways Here’s some data on raises Histograms show frequencies of groups of numbers Gaps between bars in a histogram mean gaps among the data points Install and run R Load data into R R creates beautiful histograms Make histograms from subsets of your data Negotiation pays What will negotiation mean for you?
What are you going to do with all this money? An analysis that tells people what to ask for could be huge Behold… the Raise Reckoner! Inside the algorithm will be a method to predict raises Scatterplots compare two variables A line could tell your clients where to aim Predict values in each strip with the graph of averages The regression line predicts what raises people will receive The line is useful if your data shows a linear correlation You need an equation to make your predictions precise Tell R to create a regression object The regression equation goes hand in hand with your scatterplot The regression equation is the Raise Reckoner algorithm Your raise predictor didn’t work out as planned…
Your clients are pretty ticked off What did your raise prediction algorithm do? The segments of customers The guy who asked for 25%went outside the model How to handle the client who wants a prediction outside the data range The guy who got fired because of extrapolation has cooled off You’ve only solved part of the problem What does the data for the screwy outcomes look like? Chance errors are deviations from what your model predicts Error is good for you and your client Specify error quantitatively Quantify your residual distribution with Root Mean Squared error Your model in R already knows the R.M.S.
Just got a client list from a defunct competitor The dirty secret of data analysis Head First Head Hunters wants the list for their sales team Cleaning messy data is all about preparation Once you’re organized, you can fix the data itself Use the # sign as a delimiter Excel split your data into columns using the delimiter Use SUBSTITUTE to replace the carat character You cleaned up all the first names The last name pattern is too complex for SUBSTITUTE Handle complex patterns with nested text formulas R can use regular expressions to crunch complex data patterns The sub command fixed your last names Now you can ship the data to your client Maybe you’re not quite done yet… Sort your data to show duplicate values together The data is probably from a relational database Remove duplicate names You created nice, clean, unique records Head First Head Hunters is recruiting like gangbusters! Leaving town...
