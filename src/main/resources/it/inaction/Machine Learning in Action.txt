The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
Originally my plan was to go back to grad school after two years, but time flies when you are having fun, and two years turned into six.
I realized I had to go back at that point, and I didn’t want to do night school or online learning, I wanted to sit on campus and soak up everything a university has to offer.
The best part of college is not the classes you take or research you do, but the peripheral things: meeting people, going to seminars, joining organizations, dropping in on classes, and learning what you don’t know.
Sometime in 2008 I was helping set up for a career fair.
I began to talk to someone from a large financial institution and they wanted me to interview for a position modeling credit risk (figuring out if someone is going to pay off their loans or not)
At the time, I wasn’t sure I knew what the word stochastic meant.
They were hiring for a geographic location my body xvii.
But this stochastic stuff interested me, so I went to the course catalog and looked for any class being offered with the word “stochastic” in its title.
The class I found was “Discrete-time Stochastic Systems.” I started attending the class without registering, doing the homework and taking tests.
Eventually I was noticed by the professor and she was kind enough to let me continue, for which I am very grateful.
This class was the first time I saw probability applied to an algorithm.
I had seen algorithms take an averaged value as input before, but this was different: the variance and mean were internal values in these algorithms.
The course was about “time series” data where every piece of data is a regularly spaced sample.
I found another course with Machine Learning in the title.
PREFACExviii data was not assumed to be uniformly spaced in time, and they covered more algorithms but with less rigor.
I later realized that similar methods were also being taught in the economics, electrical engineering, and computer science departments.
In early 2009, I graduated and moved to Silicon Valley to start work as a software consultant.
Over the next two years, I worked with eight companies on a very wide range of technologies and saw two trends emerge which make up the major thesis for this book: first, in order to develop a compelling application you need to do more than just connect data sources; and second, employers want people who understand theory and can also program.
A large portion of a programmer’s job can be compared to the concept of connecting pipes—except that instead of pipes, programmers connect the flow of data—and monstrous fortunes have been made doing exactly that.
You could make an application that sells things online—the big picture for this would be allowing people a way to post things and to view what others have posted.
To do this you could create a web form that allows users to enter data about what they are selling and then this data would be shipped off to a data store.
In order for other users to see what a user is selling, you would have to ship the data out of the data store and display it appropriately.
I’m sure people will continue to make money this way; however to make the application really good you need to add a level of intelligence.
This intelligence could do things like automatically remove inappropriate postings, detect fraudulent transactions, direct users to things they might like, and forecast site traffic.
To accomplish these objectives, you would need to apply machine learning.
The end user would not know that there is magic going on behind the scenes; to them your application “just works,” which is the hallmark of a well-built product.
An organization may choose to hire a group of theoretical people, or “thinkers,” and a set of practical people, “doers.” The thinkers may have spent a lot of time in academia, and their day-to-day job may be pulling ideas from papers and modeling them with very high-level tools or mathematics.
The doers interface with the real world by writing the code and dealing with the imperfections of a non-ideal world, such as machines that break down or noisy data.
Separating thinkers from doers is a bad idea and successful organizations realize this.
One of the tenets of lean manufacturing is for the thinkers to get their hands dirty with actual doing.
When there is a limited amount of money to be spent on hiring, who will get hired more readily—the thinker or the doer? Probably the doer, but in reality employers want both.
Things need to get built, but when applications call for more demanding algorithms it is useful to have someone who can read papers, pull out the idea, implement it in real code, and iterate.
I didn’t see a book that addressed the problem of bridging the gap between thinkers and doers in the context of machine learning algorithms.
The goal of this book is to fill that void, and, along the way, to introduce uses of machine learning algorithms so that the reader can build better applications.
First, I would like to thank the folks at Manning.
Above all, I would like to thank my editor Troy Mott; if not for his support and enthusiasm, this book never would have happened.
I would also like to thank Maureen Spencer who helped polish my prose in the final manuscript; she was a pleasure to work with.
Next I would like to thank Jennie Si at Arizona State University for letting me sneak into her class on discrete-time stochastic systems without registering.
Special thanks to the following peer reviewers who read the manuscript at differxix.
My technical proofreaders, Tricia Hoffman and Alex Ott, reviewed the technical content shortly before the manuscript went to press and I would like to thank them.
Alex was a cold-blooded killer when it came to reviewing my code! Thank you for making this a better book.
I want to thank my family for their support during the writing of this book.
I owe a huge debt of gratitude to my wife for her encouragement and for putting up with all the irregularities in my life during the time I spent working on the manuscript.
Finally, I would like to thank Silicon Valley for being such a great place for my wife and me to work and where we can share our ideas and passions.
Tools and applications using these algorithms are introduced to give the reader an idea of how they are used in practice today.
A wide selection of machine learning books is available, which discuss the mathematics, but discuss little of how to program the algorithms.
This book aims to be a bridge from algorithms presented in matrix form to an actual functioning program.
With that in mind, please note that this book is heavy on code and light on mathematics.
Audience What is all this machine learning stuff and who needs it? In a nutshell, machine learning is making sense of data.
So if you have data you want to understand, this book is for you.
If you want to get data and make sense of it, then this book is for you xxi.
It helps if you are familiar with a few basic programming concepts, such as recursion and a few data structures, such as trees.
It will also help if you have had an introduction to linear algebra and probability, although expertise in these fields is not necessary to benefit from this book.
Lastly, the book uses Python, which has been called “executable pseudo code” in the past.
It is assumed that you have a basic working knowledge of Python, but do not worry if you are not an expert in Pythonit is not difficult to learn.
This paper was the result of the award winners from the KDD conference being asked to come up with the top 10 machine learning algorithms.
The general outline of this book follows the algorithms identified in the paper.
I will explain, but let’s first look at the top 10 algorithms.
How the book is organized The book has 15 chapters, organized into four parts, and four appendixes.
This section consists of two chapters which discuss regression or predicting continuous values.
In addition, chapter 8 has a section that deals with the bias-variance tradeoff, which needs to be considered when turning a Machine Learning algorithm.
This part of the book concludes with chapter 9, which discusses tree-based regression and the CART algorithm.
The first two parts focused on supervised learning which assumes you have target values, or you know what you are looking for.
Part 3 begins a new section called “Unsupervised learning” where you do not know what you are looking for; instead we ask the machine to tell us, “what do these data have in common?” The first algorithm discussed is k-Means clustering.
Next we look into association analysis with the Apriori algorithm.
Chapter 12 concludes our discussion of unsupervised learning by looking at an improved algorithm for association analysis called FP-Growth.
The book concludes with a look at some additional tools used in machine learning.
These are principal components analysis and the singular value decomposition.
Finally, we discuss a tool used to scale machine learning to massive datasets that cannot be adequately addressed on a single machine.
Many examples included in this book demonstrate how you can use the algorithms in the real world.
We use the following steps to make sure we have not made any mistakes:
The reason we can’t just jump into step 3 is basic engineering of complex systemsyou want to build things incrementally so you understand when things break, where they break, and why.
If you just throw things together, you won’t know if the implementation of the algorithm is incorrect or if the formatting of the data is incorrect.
Along the way I include some historical notes which you may find of interest.
Code conventions and downloads All source code in listings or in text is in a fixed-width font like this to separate it from ordinary text.
Code annotations accompany many of the listings, highlighting important concepts.
In some cases, numbered bullets link to explanations that follow the listing.
This page provides information on how to get on the forum once you’re registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialog between individual readers and between readers and the author can take place.
It’s not a commitment to any specific amount of participation on the part of the author, whose contribution to the AO remains voluntary (and unpaid)
We suggest you try asking the author some challenging questions lest his interest stray!
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
Prior to joining Zillabyte, he was a machine learning software consultant for two years.
Peter spends his free time competing in programming competitions and building 3D printers.
Hacquet (1739–1815) was an Austrian physician and scientist who spent many years studying the botany, geology, and ethnography of many parts of the Austrian Empire, as well as the Veneto, the Julian Alps, and the western Balkans, inhabited in the past by peoples of the Illyrian tribes.
Hand drawn illustrations accompany the many scientific papers and books that Hacquet published.
The rich diversity of the drawings in Hacquet’s publications speaks vividly of the uniqueness and individuality of the eastern Alpine and northwestern Balkan regions just 200 years ago.
This was a time when the dress codes of two villages separated by a xxvi.
Dress codes have changed since then and the diversity by region, so rich at the time, has faded away.
It is now often hard to tell the inhabitant of one continent from another and today the inhabitants of the picturesque towns and villages in the Slovenian Alps or Balkan coastal towns are not readily distinguishable from the residents of other parts of Europe or America.
We at Manning celebrate the inventiveness, the initiative, and the fun of the computer business with book covers based on costumes from two centuries ago brought back to life by illustrations such as this one.
The first two parts of this book are on supervised learning.
Supervised learning asks the machine to learn from our data when we specify a target variable.
This reduces the machine’s task to only divining some pattern from the input data to get the target variable.
The first case occurs when the target variable can take only nominal values: true or false; reptile, fish, mammal, amphibian, plant, fungi.
Our study of classification algorithms covers the first seven chapters of this book.
Chapter 2 introduces one of the simplest classification algorithms called k-Nearest Neighbors, which uses a distance metric to classify items.
Chapter 3 introduces an intuitive yet slightly harder to implement algorithm: decision trees.
In chapter 4 we address how we can use probability theory to build a classifier.
Next, chapter 5 looks at logistic regression, where we find the best parameters to properly classify our data.
In the process of finding these best parameters, we encounter some powerful optimization algorithms.
Finally, in chapter 7 we see a meta-algorithm, AdaBoost, which is a classifier made up of a collection of classifiers.
We’re not going to attempt to have conversations with computer programs in this book, nor are we going to ask a computer the meaning of life.
With machine learning we can gain insight from a dataset; we’re going to ask the computer to make some sense from data.
This is what we mean by learning, not cyborg rote memorization, and not the creation of sentient beings.
Machine learning is actively being used today, perhaps in many more places than you’d expect.
Here’s a hypothetical day and the many times you’ll encounter machine learning: You realize it’s your friend’s birthday and want to send her a card via snail mail.
You search for funny cards, and the search engine shows you the 10 Machine learning basics.
I was eating dinner with a couple when they asked what I was working on recently.
You click the second link; the search engine learns from this.
Next, you check some email, and without your noticing it, the spam filter catches unsolicited ads for pharmaceuticals and places them in the Spam folder.
Next, you head to the store to buy the birthday card.
When you’re shopping for the card, you pick up some diapers for your friend’s child.
When you get to the checkout and purchase the items, the human operating the cash register hands you a coupon for $1 off a six-pack of beer.
The cash register’s software generated this coupon for you because people who buy diapers also tend to buy beer.
You send the birthday card to your friend, and a machine at the post office recognizes your handwriting to direct the mail to the proper delivery truck.
Next, you go to the loan agent and ask them if you are eligible for loan; they don’t answer but plug some financial information about you into the computer and a decision is made.
Finally, you head to the casino for some late-night entertainment, and as you walk in the door, the person walking in behind you gets approached by security seemingly out of nowhere.
Thorp, we’re going to have to ask you to leave the casino.
What is machine learning? In all of the previously mentioned scenarios, machine learning was present.
Companies are using it to improve business decisions, increase productivity, detect disease, forecast weather, and do many more things.
With the exponential growth of technology, we not only need better tools to understand the data we currently have, but we also need to prepare ourselves for the data we will have.
Are you ready for machine learning? In this chapter you’ll find out what machine learning is, where it’s already being used around you, and how it might help you in the future.
Next, we’ll talk about some common approaches to solving problems with machine learning.
Last, you’ll find out why Python is so great and why it’s a great language for machine learning.
Then we’ll go through a really quick example using a module for Python called NumPy, which allows you to abstract and matrix calculations.
For example, in detecting spam email, looking for the occurrence of a single word may not be very helpful.
But looking at the occurrence of certain words used together, combined with the length of the email and other factors, you could get a much clearer picture of whether the email is spam or not.
Machine learning lies at the intersection of computer science, engineering, and statistics and often appears in other disciplines.
As you’ll see later, it can be applied to many fields from politics to geosciences.
It’s a tool that can be applied to many problems.
Any field that needs to interpret and act on data can benefit from machine learning techniques.
To most people, statistics is an esoteric subject used for companies to lie about how great their products are.
There’s a great manual on how to do this called How to Lie with Statistics by Darrell Huff.
Ironically, this is the best-selling statistics book of all time.
So why do the rest of us need statistics? The practice of engineering is applying science to solve a problem.
In engineering we’re used to solving a deterministic problem where our solution solves the problem all the time.
If we’re asked to write software to control a vending machine, it had better work all the time, regardless of the money entered or the buttons pressed.
That is, we don’t know enough about the problem or don’t have enough computing power to properly model the problem.
For example, the motivation of humans is a problem that is currently too difficult to model.
In the social sciences, being right 60% of the time is considered successful.
If we can predict the way people will behave 60% of the time, we’re doing well.
How can this be? Shouldn’t we be right all the time? If we’re not right all the time, doesn’t that mean we’re doing something wrong?
Let me give you an example to illustrate the problem of not being able to model the problem fully.
So even if our assumptions are correct about people maximizing their own happiness, the definition of happiness is too complex to model.
There are many other examples outside human behavior that we can’t currently model deterministically.
For these problems we need to use some tools from statistics.
We have a tremendous amount of human-created data from the World Wide Web, but recently more nonhuman sources of data have been coming online.
The technology behind the sensors isn’t new, but connecting them to the web is new.
The following is an example of an abundance of free data, a worthy cause, and the need to sort through the data.
Shortly after the Loma Prieta earthquake, a study was published using low-frequency magnetic field measurements claiming to foretell the earthquake.2 A number of subsequent studies showed that the original study was flawed for various reasons.3,4 Suppose we want to redo this study and keep searching for ways to predict earthquakes so we can avoid the horrific consequences and have a better understanding of our planet.
What would be the best way to go about this study? We could buy magnetometers with our own money and buy pieces of land to place them on.
We could ask the government to help us out and give us money and land on which to place these magnetometers.
Who’s going to make sure there’s no tampering with the magnetometers, and how can we get readings from them? There exists another low-cost solution.
The smartphones also come with operating systems where you can execute your own programs; with a few lines of code you can get readings from the magnetometers hundreds of times a second.
Also, the phone already has its own communication system set up; if you can convince people to install and run your program, you could record a large amount of magnetometer data with very little investment.
In addition to the magnetometers, smartphones carry a large number of other sensors including yawrate gyros, three-axis accelerometers, temperature sensors, and GPS receivers, all of which you could use to support your primary measurements.
The two trends of mobile computing and sensor-generated data mean that we’ll be getting more and more data in the future.
In the last half of the twentieth century the majority of the workforce in the developed world has moved from manual labor to what is known as knowledge work.
The clear definitions of “move this from here to there” and “put a hole in this” are gone.
Things are much more ambiguous now; job assignments such as “maximize profits,” “minimize risk,” and “find the best marketing strategy” are all too common.
The fire hose of information available to us from the World Wide Web makes the jobs of knowledge workers even harder.
Making sense of all the data with our job in mind is becoming a more essential skill, as Hal Varian, chief economist at Google, said:
I keep saying the sexy job in the next ten years will be statisticians.
People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s? The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids.
Because now we really do have essentially free and ubiquitous data.
So the complementary scarce factor is the ability to understand that data and extract value from it.
I think statisticians are part of it, but it’s just a part.
You also want to be able to visualize the data, communicate the data, and utilize it effectively.
But I do think those skills—of being able to access, understand, and communicate the insights you get from data analysisare going to be extremely important.
Managers need to be able to access and understand the data themselves.
With so much of the economic activity dependent on information, you can’t afford to be lost in the data.
Machine learning will help you get through all the data and extract some information.
We need to go over some vocabulary that commonly appears in machine learning so it’s clear what’s being discussed in this book.
Before we jump into the machine learning algorithms, it would be best to explain some terminology.
The best way to do so is through an example of a system someone may want to make.
We’ll go through an example of building a bird classification system.
This sort of system is an interesting topic often associated with machine learning called expert systems.
By creating a computer program to recognize birds, we’ve replaced an ornithologist with a computer.
The ornithologist is a bird expert, so we’ve created an expert system.
In table 1.1 are some values for four parts of various birds that we decided to measure.
We chose to measure weight, wingspan, whether it has webbed feet, and the color of its back.
The four things we’ve measured are called features; these are also called attributes, but we’ll stick with the term features in this book.
Each of the rows in table 1.1 is an instance made up of features.
The first two features in table 1.1 are numeric and can take on decimal values.
The fourth feature (back color) is an enumeration over the color palette we’re using, and I just chose some very common colors.
Say we ask the people doing the measurements to choose one of seven colors; then back color would be just an integer.
If you happen to see a Campephilus principalis (Ivory-billed Woodpecker), give me a call ASAP! Don’t tell anyone else you saw it; just call me and keep an eye on the bird until I get there.
There’s a $50,000 reward for anyone who can lead a biologist to a living Ivory-billed Woodpecker.
We want to identify this bird out of a bunch of other birds, and we want to profit from this.
We could set up a bird feeder and then hire an ornithologist (bird expert) to watch it and when they see an Ivory-billed Woodpecker give us a call.
This would be expensive, and the person could only be in one place at a time.
We could also automate this process: set up many bird feeders with cameras and computers attached to them to identify the birds that come in.
We could put a scale on the bird feeder to get the bird’s weight and write some computer vision code to extract the bird’s wingspan, feet type, and back color.
How do we then decide if a bird at our feeder is an Ivory-billed Woodpecker or something else? This task is called classification, and there are many machine learning algorithms that are good at classification.
The class in this example is the bird species; more specifically, we can reduce our classes to Ivory-billed Woodpecker or everything else.
Say we’ve decided on a machine learning algorithm to use for classification.
What we need to do next is train the algorithm, or allow it to learn.
To train the algorithm we feed it quality data known as a training set.
A training set is the set of training examples we’ll use to train our machine learning algorithms.
In table 1.1 our training set has six training examples.
Each training example has four features and one target variable; this is depicted in figure 1.2
The target variable is what we’ll be trying to predict with our machine learning algorithms.
In classification the target variable takes on a nominal value, and in the task of regression its value could be continuous.
The machine learns by finding some relationship between the features and the target variable.
The target variable is the species, and as I mentioned earlier, we can reduce this to take nominal values.
In the classification problem the target variables are called classes, and there is assumed to be a finite number of classes.
This is usually columns in a training or test set.
To test machine learning algorithms what’s usually done is to have a training set of data and a separate dataset, called a test set.
Initially the program is fed the training examples; this is when the machine learning takes place.
The target variable for each example from the test set isn’t given to the program, and the program decides which class each example should belong to.
The target variable or class that the training example belongs to is then compared to the predicted value, and we can get a sense for how accurate the algorithm is.
There are better ways to use all the information in the test set and training set.
In our bird classification example, assume we’ve tested the program and it meets our desired level of accuracy.
Can we see what the machine has learned? This is called knowledge representation.
Some algorithms have knowledge representation that’s more readable by humans than others.
The knowledge representation may be in the form of a set of rules; it may be a probability distribution or an example from the training set.
In some cases we may not be interested in building an expert system but interested only in the knowledge representation that’s acquired from training a machine learning algorithm.
We’ve covered a lot of key terms of machine learning, but we didn’t cover them all.
We’ll introduce more key terms in later chapters as they’re needed.
We’ll now address the big picture: what we can do with machine learning.
In this section we’ll outline the key jobs of machine learning and set a framework that allows us to easily turn a machine learning algorithm into a solid working application.
The example covered previously was for the task of classification.
In classification, our job is to predict what class an instance of data should fall into.
Most people have probably seen an example of regression with a best-fit line drawn through some data points to generalize the data points.
This set of problems is known as supervised because we’re telling the algorithm what to predict.
The opposite of supervised learning is a set of tasks known as unsupervised learning.
In unsupervised learning, there’s no label or target value given for the data.
A task where we group similar items together is known as clustering.
In unsupervised learning, we may also want to find statistical values that describe the data.
Another task of unsupervised learning may be reducing the data from many features to a small number so that we can properly visualize it in two or three dimensions.
Table 1.2 lists some common tasks in machine learning with algorithms used to solve these tasks.
If you noticed in table 1.2 that multiple techniques are used for completing the same task, you may be asking yourself, “If these do the same thing, why are there four different methods? Why can’t I just choose one method and master it?” I’ll answer that question in the next section.
With all the different algorithms in table 1.2, how can you choose which one to use? First, you need to consider your goal.
What are you trying to get out of this? (Do you want a probability that it might rain tomorrow, or do you want to find groups of voters with similar interests?) What data do you have or can you collect? Those are the big questions.
If you’re not trying to predict a target value, then you need to look into unsupervised learning.
Are you trying to fit your data into some discrete groups? If so and that’s all you need, you should look into clustering.
Do you need to have some numerical estimate of how strong the fit is into each group? If you answer yes, then you probably should look into a density estimation algorithm.
The rules I’ve given here should point you in the right direction but are not unbreakable laws.
In chapter 9 I’ll show you how you can use classification techniques for regression, blurring the distinction I made within supervised learning.
The second thing you need to consider is your data.
You should spend some time getting to know your data, and the more you know about it, the better you’ll be able to build a successful application.
Things to know about your data are these: Are the features nominal or continuous? Are there missing values in the features? If there are missing values, why are there missing values? Are there outliers in the data? Are you looking for a needle in a haystack, something that happens very infrequently? All of these features about your data can help you narrow the algorithm selection process.
With the algorithm narrowed, there’s no single answer to what the best algorithm is or what will give you the best results.
You’re going to have to try different algorithms and see how they perform.
There are other machine learning techniques that you can use to improve the performance of a machine learning algorithm.
The relative performance of two algorithms may change after you process the input data.
We’ll discuss these in more detail later, but the point is that finding the best algorithm is an iterative process of trial and error.
Many of the algorithms are different, but there are some common steps you need to take with all of these algorithms when building a machine learning application.
You could collect the samples by scraping a website and extracting data, or you could get information from an RSS feed or an API.
You could have a device collect wind speed measurements and send them to you, or blood glucose levels, or anything you can measure.
To save some time and effort, you could use publicly available data.
Once you have this data, you need to make sure it’s in a useable format.
The format we’ll be using in this book is the Python list.
We’ll talk about Python more in a little bit, and lists are reviewed in appendix A.
The benefit of having this standard format is that you can mix and match algorithms and data sources.
Some algorithms need features in a special format, some algorithms can deal with target variables and features as strings, and some need them to be integers.
We’ll get to this later, but the algorithm-specific formatting is usually trivial compared to collecting data.
This is looking at the data from the previous task.
You can also look at the data to see if you can recognize any patterns or if there’s anything obvious, such as a few data points that are vastly different from the rest of the set.
Plotting data in one, two, or three dimensions can also help.
But most of the time you’ll have more than three features, and you can’t easily plot the data across all features at one time.
You could, however, use some advanced methods we’ll talk about later to distill multiple dimensions down to two or three so you can visualize the data.
If you’re working with a production system and you know what the data should look like, or you trust its source, you can skip this step.
This step takes human involvement, and for an automated system you don’t want human involvement.
The value of this step is that it makes you understand you don’t have garbage coming in.
This step and the next step are where the “core” algorithms lie, depending on the algorithm.
You feed the algorithm good clean data from the first two steps and extract knowledge or information.
This knowledge you often store in a format that’s readily useable by a machine for the next two steps.
In the case of unsupervised learning, there’s no training step because you don’t have a target value.
This is where the information learned in the previous step is put to use.
When you’re evaluating an algorithm, you’ll test it to see how well it does.
In the case of supervised learning, you have some known values you can use to evaluate the algorithm.
In unsupervised learning, you may have to use some other metrics to evaluate the success.
Why Python? you can go back to step 4, change some things, and try testing again.
Here you make a real program to do some task, and once again you see if all the previous steps worked as you expected.
You might encounter some new data and have to revisit steps 1–5
Now we’ll talk about a language to implement machine learning applications.
We need a language that’s understandable by a wide range of people.
We also need a language that has libraries written for a number of tasks, especially matrix math operations.
We also would like a language with an active developer community.
A large number of people and organizations use Python, so there’s ample development and documentation.
The clear syntax of Python has earned it the name executable pseudo-code.
The default install of Python already carries high-level data types like lists, tuples, dictionaries, sets, queues, and so on, which you don’t have to program in yourself.
These high-level data types make abstract concepts easy to implement.
See appendix A for a full description of Python, the data types, and how to install it.
With Python, you can program in any style you’re familiar with: object-oriented, procedural, functional, and so on.
With Python it’s easy to process and manipulate text, which makes it ideal for processing non-numeric data.
You can get by in Python with little to no regular expression usage.
There are a number of libraries for using Python to access web pages, and the intuitive text manipulation makes it easy to extract data from HTML.
Python is popular, so lots of examples are available, which makes learning it fast.
Second, the popularity means that there are lots of modules available for many applications.
The scientific tools in Python work well with a plotting tool called Matplotlib.
Python also has an interactive shell, which allows you to view and inspect elements of the program as you’re developing it.
A new module for Python, called Pylab, seeks to combine NumPy, SciPy, and Matplotlib into one environment and instillation.
At the time of writing, this isn’t yet done but shows great promise for the future.
There are high-level languages that allow you to do matrix math such as MATLAB and Mathematica.
The problem with MATLAB is that to legally use it will cost you a few thousand dollars.
There are third-party add-ons to MATLAB but nothing on the scale of an open source project.
There are matrix math libraries for low-level languages such as Java and C.
The problem with these languages is that it takes a lot of code to get simple things done.
First, you have to typecast variables, and then with Java it seems that you have to write setters and getters every time you sneeze.
You have to subclass methods even if you aren’t going to use them.
At the end of the day, you have written a lot of code—sometimes tedious code—to do simple things.
Java and C aren’t so easy to pick up and much less concise than Python.
All of us learn to write in the second grade.
Perhaps one day I can replace “write” with “write code” in this quote.
But for many people a programming language is simply a tool to accomplish some other task.
Python is a higher-level language; this allows you to spend more time making sense of data and less time concerned with how a machine approximates the data.
The only real drawback of Python is that it’s not as fast as Java or C.
This gives you the best of both worlds and allows you to incrementally develop a program.
If you experiment with an idea in Python and decide it’s something you want to pursue in a production system, it will be easy to make that transition.
If the program is built in a modular fashion, you could first get it up and running in Python and then to improve speed start building portions of the code in C.
Other tools such as Cython and PyPy allow you write typed versions of Python with performance gains over regular Python.
If an idea for a program or application is flawed, then it will be flawed at low speed as well as high speed.
Getting started with the NumPy library large number of users doesn’t change anything.
This makes Python so beautiful that you can quickly see an idea in action and then optimize it if needed.
Now that you know the language we’re going to be using, I’m sure you’re ready to start using it.
In the next section, we’ll walk through use of the Python shell and NumPy.
We’ll use NumPy heavily in this book because we’ll be doing some linear algebra.
Don’t worry about linear algebra—we just want to do the same math operation on lots of different data points.
If we represent our data as a matrix, we can do simple math without a bunch of messy loops.
Before we get into any machine learning algorithms, you should make sure you have Python working and NumPy properly installed.
NumPy is a separate module for Python that doesn’t come with most distributions of Python, so you’ll need to install it after you’ve installed Python.
Start a Python shell by opening a command prompt in Windows or a terminal in Linux and Mac OS.
This imports all of the NumPy modules into the current namespace.
This is shown in figure 1.3 on the Mac OS.
This creates a random array of size 4x4; don’t worry if the numbers you see are different from mine.
These are random numbers, so your numbers should look different from mine.
You can always convert an array to a matrix by calling the mat() function; type in the following:
You will probably have different values than I have here because we’re getting random numbers:
If you don’t remember or never learned how to solve the inverse of a matrix, don’t worry; it was just done for you:
This gives you just the identity matrix, a 4x4 matrix where all elements are zero except the diagonals, which are one.
There are some very small elements left over in the array.
If you got through this example, you have NumPy installed correctly.
You’re now ready to start making some powerful programs using machine learning.
Don’t worry if you haven’t seen all these functions before.
More NumPy functionality will be introduced as it’s needed in further examples in this book.
Machine learning is already being used in your daily lives even though you may not be aware of it.
The amount of data coming at you isn’t going to decrease, and being able to make sense of all this data will be an essential skill for people working in a datadriven industry.
Each instance of data is composed of a number of features.
Classification, one the popular and essential tasks of machine learning, is used to place an unknown piece of data into a known group.
In order to build or train a classifier, you feed it data for which you know the class.
I don’t claim that our expert system used to recognize birds will be perfect or as a good as a human.
But building a machine with accuracy close to that of a human expert could greatly increase the quality of life.
When we build software that can match the accuracy of a human doctor, people can more rapidly get treatment.
Better prediction of weather could lead to fewer water shortages and a greater supply of food.
The examples where machine learning could be useful are endless.
In the next chapter I’ll introduce our first machine learning algorithm.
This will be an example of classification, which is a type of supervised learning.
Have you ever seen movies categorized into genres? What defines these genres, and who says which movie goes into what genre? The movies in one genre are similar but based on what? I’m sure if you asked the people involved with making the movies, they wouldn’t say that their movie is just like someone else’s movie, but in some way you know they’re similar.
What makes an action movie similar to another action movie and dissimilar to a romance movie? Do people kiss in action movies, and do people kick in romance movies? Yes, but there’s probably more kissing in romance movies and more kicking in action movies.
Perhaps if you measured kisses, kicks, and other things per movie, you could automatically figure out what genre a movie belongs to.
I’ll use movies to explain some of the concepts of k-Nearest Neighbors; then we will move on to other applications.
In this chapter, we’ll discuss our first machine-learning algorithm: k-Nearest Neighbors.
We’ll first discuss the theory and how you can use the concept of a distance measurement to classify items.
Next, you’ll see how to easily import and parse data from text files using Python.
We’ll address some common pitfalls when working with distance calculations and data coming from numerous sources.
We’ll put all of this into action in examples for improving results from a dating website and recognizing handwritten digits.
The first machine-learning algorithm we’ll look at is k-Nearest Neighbors (kNN)
It works like this: we have an existing set of example data, our training set.
We have labels for all of this data—we know what class each piece of the data should fall into.
When we’re given a new piece of data without a label, we compare that new piece of data to the existing data, every piece of existing data.
We then take the most similar pieces of data (the nearest neighbors) and look at their labels.
We look at the top k most similar pieces of data from our known dataset; this is where the k comes from.
Lastly, we take a majority vote from the k most similar pieces of data, and the majority is the new class we assign to the data we were asked to classify.
Let’s run through a quick example classifying movies into romance or action movies.
Someone watched a lot of movies and counted the number of kicks and kisses in each movie.
I’ve plotted six movies by the number of kisses and kicks in each movie in figure 2.1
Now, you find a movie you haven’t seen yet and want to know if it’s a romance movie or an action movie.
We find the movie in question and see how many kicks and kisses it has.
It’s plotted as a large question mark along with a few other movies in figure 2.1
We don’t know what type of movie the question mark movie is, but we have a way of figuring that out.
First, we calculate the distance to all the other movies.
I’ve calculated the distances and shown those in table 2.2
Don’t worry about how I did these calculations right now.
Now that we have all the distances to our unknown movie, we need to find the k-nearest movies by sorting the distances in decreasing order.
The kNN algorithm says to take the majority vote from these three movies to determine the class of the mystery movie.
Because all three movies are romances, we forecast that the mystery movie is a romance movie.
We’ll work through a real machine learning algorithm in this chapter, and along the way I’ll introduce the Python tools and machine learning terminology.
First, however, we’ll go over a simple example of the kNN algorithm to make sure we’re using the algorithm correctly.
Table 2.1 Movies with the number of kicks and number of kisses shown for each movie, along with our assessment of the movie type.
First, we’ll create a Python module called kNN.py, where we’ll place all the code used in this chapter.
You can create your own file and enter code as we progress, or you can copy the file kNN.py from the book’s source code.
The best way to learn is to start with a blank module and enter code as it’s used.
First, let’s create kNN.py or copy it from the source code repository.
We’ll create a few support functions before we create the full kNN algorithm.
The first one is NumPy, which is our scientific computing package.
The second module is the operator module, which is used later in the kNN algorithm for sorting; we’ll get to that shortly.
This creates the dataset and labels, as shown in figure 2.1
Let’s try this out: save kNN.py, change to the directory where you’ve stored kNN.py, and launch a Python interactive session.
To get started you need to open a new terminal in Linux/Mac OS or in Windows, so open a command prompt.
Once you’ve started Python to load your module, you need to type.
Use: This application needs to get some input data and output structured numeric values.
Next, the application runs the kNN algorithm on this input data and determines which class the input data should belong to.
The application then takes some action on the calculated class.
To make sure that we’re looking at the same dataset, I created a function called createDataSet.
To inspect each variable, type its name at the Python command prompt:
Each piece of data has two attributes or features, things we know about it.
In the group matrix each row is a different piece of data.
Think of it as a different measurement or entry in some sort of log.
As humans, we can visualize things in one, two, or sometimes three dimensions, but that’s about the limit of our brains; to keep things easy to visualize, we’ll use only two features for each data point.
The label’s vector carries the labels we’ve given to each of the data points.
There should be as many items in this vector as there are rows in the group matrix.
The values in this example are arbitrarily chosen for the purpose of illustration, and the axes are unlabeled.
The four data points with class labels are plotted in figure 2.2
Now that you have an idea of how to parse and load data into Python, and you have an idea of how the kNN algorithm works, let’s put it all together and do some classification.
In this section we’ll build a function, shown in listing 2.1, to run the kNN algorithm on one piece of data.
I’ll first show the function in pseudocode and then in actual Python, followed by a detailed explanation of what everything in the code does.
Remember, the goal of this function is to use the kNN algorithm to classify one piece of data called inX.
For every point in our dataset: calculate the distance between inX and the current point sort the distances in increasing order take k items with lowest distances to inX find the majority class among these items return the majority class as our prediction for the class of inX.
The Python code for the classify0() function is in the following listing.
The function classify0() takes four inputs: the input vector to classify called inX, our full matrix of training examples called dataSet, a vector of labels called labels, and, finally, k, the number of nearest neighbors to use in the voting.
The labels vector should have as many elements in it as there are rows in the dataSet matrix.
You calculate the distances B using the Euclidian distance where the distance between two vectors, xA and xB, with two elements, is given by.
Following the distance calculation, the distances are sorted from least to greatest (this is the default)
Next, C the first k or lowest k distances are used to vote on the class of inX.
Lastly, D you take the classCount dictionary and decompose it into a list of tuples and then sort the tuples by the second item in the tuple using the itemgetter method from the operator module imported in the second line of the program.
This sort is done in reverse so you have largest to smallest.
Finally, you can return the label of the item occurring the most frequently.
To predict the class, type the following text at the Python prompt:
Try to change the [0,0] entry to see how the answer changes.
Congratulations, you just made your first classifier! You can do a lot with this simple classifier.
We built the kNN algorithm and saw that it was giving us answers we would expect.
You may be asking yourself, “At what point does this break?” or “Is it always right?” No, it’s not always right.
There are different ways of exploring how often a classifier is right.
Also, there are different things that impact the performance of a classifier, such as settings of the classifier and the dataset.
To test out a classifier, you start with some known data so you can hide the answer from the classifier and ask the classifier for its best guess.
You can add up the number of times the classifier was wrong and divide it by the total number of tests you gave it.
This will give you the error rate, which is a common measure to gauge how good a classifier is doing on a dataset.
You’ll see this in action with some solid data later.
The example in this section worked, but it wasn’t useful.
We’re going to put kNN to use in real-world examples in the next two sections.
First, we’ll look at improving the results from a dating site with kNN, and then we’ll look at an impressive handwriting recognition example.
We’ll employ testing in the handwriting recognition example to see if this algorithm is working.
My friend Hellen has been using some online dating sites to find different people to go out with.
She realized that despite the site’s recommendations, she didn’t like everyone she was matched with.
After some introspection, she realized there were three types of people she went out with:
After discovering this, Hellen couldn’t figure out what made a person fit into any of these categories.
They all were recommended to her by the dating site.
The people whom she liked in small doses were good to see Monday through Friday, but on the weekend she’d rather spend time with the people she liked in large doses.
Hellen has asked us to help her filter future matches to categorize them.
In addition, Hellen has collected some data that isn’t recorded by the dating site, but she feels it’s useful in selecting people to go out with.
Hellen has been collecting this data for a while and has 1,000 entries.
A new sample is on each line, and Hellen has recorded the following features:
Before we can use this data in our classifier, we need to change it to the format that our classifier accepts.
In order to do this, we’ll add a new function to kNN.py called file2matrix.
This function takes a filename string and outputs two things: a matrix of training examples and a vector of class labels.
Analyze: Use Matplotlib to make 2D plots of our data.
Test: Write a function to use some portion of the data Hellen gave us as test examples.
If the predicted class doesn’t match the real class, we’ll count that as an error.
Use: Build a simple command-line program Hellen can use to predict whether she’ll like someone based on a few inputs.
This code is a great place to demonstrate how easy it is to process text with Python.
Initially, you’d like to know how many lines are in the file.
Next, C you create a NumPy matrix (actually, it’s a 2D array, but don’t worry about that now) to populate and return.
I’ve hard-coded in the size of this to be numberOfLines x 3, but you could add some code to make this adaptable to the various inputs.
Finally, D you loop over all the lines in the file and strip off the return line character with line.strip()
Next, you split the line into a list of elements delimited by the tab character: '\t'
You take the first three elements and shove them into a row of your matrix, and you use the Python feature of negative indexing to get the last item from the list to put into classLabelVector.
You have to explicitly tell the interpreter that you’d like the integer version of the last item in the list, or it will give you the string version.
Usually, you’d have to do this, but NumPy takes care of those details for you.
To use this, type the following at the Python prompt:
Note that before executing the function, I reloaded the kNN.py module.
When you change a module, you need to reload that module or you’ll still be using the old version.
Now that you have the data imported and properly formatted, let’s take a look at it and see if we can make any sense of it.
It can mean look at the values in a text file or look at a plot of the values.
Example: improving matches from a dating site with kNN some of Python’s tools to make plots of the data.
If we make a plot, we may be able to distinguish some patterns.
Let’s look at the data in further detail by making some scatter plots of the data from Matplotlib.
We’ve plotted the second and third columns from the datingDataMat matrix.
NumPy Array and Python’s Array We’ll be using the NumPy array extensively in this book.
In your Python shell you can import this using from numpy import array, or it will be imported when you import all of NumPy.
There’s another array type that comes with Python that we won’t be using.
Don’t make the mistake of importing that array because the NumPy array methods won’t work on it.
From this plot it’s difficult to discern which dot belongs to which group.
It’s hard to see any patterns in this data, but we have additional data we haven’t used yet—the class values.
If we can plot these in color or use some other markers, we can get a better understanding of the data.
The Matplotlib scatter function has additional inputs we can use to customize the markers.
Type the previous code again, but this time use the following for a scatter function:
I provided a different marker size and color that depend on the class labels we have in datingLabels.
You should see a plot similar to the one in figure 2.3
From this image, you can make out three regions where the different classes lie.
Now that you can plot data using Matplotlib, you can get a better idea of exactly what’s going on with our data.
From figure 2.5, you can identify some regions where the different classes lie.
Figure 2.4 Dating data with markers changed by class label.
It’s easier to identify the different classes, but it’s difficult to draw conclusions from looking at this data.
Which term in this equation do you think is going to make the most difference? The largest term, the number of frequent flyer miles earned per year, will have the most effect.
The frequent flyer term will dominate even though the percentage of time spent playing video games and liters of ice cream consumed weekly have the largest differences of any two features in table 2.3
Why should frequent flyer miles be so important just because its values are large? It shouldn’t have any extra importance, unless we want it to, but Hellen believes these terms are equally important.
Table 2.3 Sample of data from improved results on a dating site.
Figure 2.5 Dating data with frequent flier miles versus percentage of time spent playing video games plotted.
The dating data has three features, and these two features show areas where the three different classes lie.
When dealing with values that lie in different ranges, it’s common to normalize them.
In the normalization procedure, the variables min and max are the smallest and largest values in the dataset.
This scaling adds some complexity to our classifier, but it’s worth it to get good results.
In the autoNorm() function, you get the minimum values of each column and place this in minVals; similarly, you get the maximum values.
Next, you calculate the range of possible values seen in our data and then create a new matrix to return.
To get the normalized values, you subtract the minimum values and then divide by the range.
To overcome this, you use the NumPy tile() function to create a matrix the same size as our input matrix and then fill it up with many copies, or tiles.
To try out autoNorm, reload kNN.py, execute the function, and inspect the results at the Python prompt:
You could have returned just normMat, but you need the ranges and minimum values to normalize test data.
Now that you have the data in a format you can use, you’re ready to test our classifier.
After you test it, you can give it to our friend Hellen to use.
One common task in machine learning is evaluating an algorithm’s accuracy.
One way you can use the existing data is to take some portion, say 90%, to train the classifier.
Then you’ll take the remaining 10% to test the classifier and see how accurate it is.
There are more advanced ways of doing this, which we’ll address later, but for now let’s use this method.
The 10% to be held back should be randomly selected.
Earlier, I mentioned that you can measure the performance of a classifier with the error rate.
In classification, the error rate is the number of misclassified pieces of data divided by the total number of data points tested.
In our code, you’ll measure the error rate with a counter that’s incremented every time a piece of data is misclassified.
The total number of errors divided by the total number of data points tested will give you the error rate.
To test the classifier, you’ll create a new function in kNN.py called datingClassTest.
This function is self-contained, so don’t worry if you closed your Python shell earlier.
You won’t have to go back and type the code again.
This uses file2matrix and autoNorm() from earlier to get the data into a form you can use.
Next, the number of test vectors is calculated, and this is used to decide which vectors from normMat will be used for testing and which for training.
The two parts are then fed into our original kNN classifier, classify0
Note that you’re using the original classifier; you spent most of this section manipulating the.
The total error rate for this classifier on this dataset with these settings is 2.4%
You can experiment with different hoRatios and different values of k inside the datingClassTest function.
How does the error change as hoRatio is increased? Note that the results will vary by algorithm, dataset, and settings.
The example showed that we could predict the class with only a 2.4% error.
To our friend Hellen, this means that she can enter a new person’s information, and our system will predict whether she’ll dislike or like the person in large or small doses.
Now that you’ve tested the classifier on our data, it’s time to use it to actually classify people for Hellen.
Hellen will find someone on the dating site and enter his information.
Add the code from the following listing to kNN.py and reload kNN.
The code in listing 2.5 mostly uses things you saw earlier.
This gives the user a text prompt and returns whatever the user enters.
To see the program in action, type in the following:
You’ve seen how to create a classifier with some data.
All of the data is easily read by a human, but how could you use a classifier on data that isn’t easily read by a human? The next section contains another example, this time showing how you can apply kNN to things as diverse as images where the data is in binary form.
We’re going to work through an example of handwriting recognition with our kNN classifier.
The binary images were converted to text format to make this example easier, although it isn’t the most efficient use of memory.
The images are stored in two directories in the chapter 2 source code.
We’ll use the trainingDigits directory to train our classifier and testDigits to.
Prepare: Write a function to convert from the image format to the list format used in our classifier, classify0()
Analyze: We’ll look at the prepared data in the Python shell to make sure it’s correct.
Test: Write a function to use some portion of the data as test examples.
If the predicted class doesn’t match the real class, you’ll count that as an error.
You could build a complete program to extract digits from an image, such a system used to sort the mail in the United States.
Feel free to take a look at the files in those folders.
We’d like to use the same classifier that we used in the previous two examples, so we’re going to need to reformat the images to a single vector.
After we do this, we can apply it to our existing classifier.
The following code is a small function called img2vector, which converts the image to a vector.
Try out the img2vector code with the following commands in the Python shell, and compare the results to a file opened with a text editor:
Now that you have the data in a format that you can plug into our classifier, you’re ready to test out this idea and see how well it works.
Before you add it, make sure to add from os import listdir to the top of the file.
This imports one function, listdir, from the os module, so that you can see the names of files in a given directory.
In listing 2.6, you get the contents for the trainingDigits directory B as a list.
Then you see how many files are in that directory and call this m.
Next, you create a training matrix with m rows and 1024 columns to hold each image as a single row.
You then put this class number in the hwLabels vector and load the image with the function img2vector discussed previously.
Next, you do something similar for all the files in the testDigits directory, but instead of loading them into a big matrix, you test each vector individually with our classify0 function.
Then, when the function begins testing, you can see the results as they come back.
You should have output that’s similar to the following example:
Using the kNN algorithm on this dataset, you were able to achieve an error rate of 1.2%
That way, you can vary the number of training examples and see how that impacts the error rate.
Depending on your computer’s speed, you may think this algorithm is slow, and you’d be right.
Is there a way to make this smaller and take fewer computations? One modification to kNN, called kD-trees, allows you to reduce the number of calculations.
The k-Nearest Neighbors algorithm is a simple and effective way to classify data.
The examples in this chapter should be evidence of how powerful a classifier it is.
The algorithm has to carry around the full dataset; for large datasets, this implies a large amount of storage.
In addition, you need to calculate the distance measurement for every piece of data in the database, and this can be cumbersome.
An additional drawback is that kNN doesn’t give you any idea of the underlying structure of the data; you have no idea what an “average” or “exemplar” instance from each class looks like.
In the next chapter, we’ll address this issue by exploring ways in which probability measurements can help you do classification.
Have you ever played a game called Twenty Questions? If not, the game works like this: One person thinks of some object and players try to guess the object.
Players are allowed to ask 20 questions and receive only yes or no answers.
In this game, the people asking the questions are successively splitting the set of objects they can deduce.
A decision tree works just like the game Twenty Questions; you give it a bunch of data and it generates answers to the game.
The decision tree is one of the most commonly used classification techniques; recent surveys claim that it’s the most commonly used technique.1 You don’t have to know much about machine learning to understand how it works.
If you’re not already familiar with decisions trees, the concept is straightforward.
Chances are good that you’ve already seen a decision tree without knowing it.
Figure 3.1 shows a flowchart, which is a decision tree.
It has decision blocks (rectangles) and terminating blocks (ovals) where some conclusion has been reached.
The right and left arrows coming out of the decision blocks are known as branches, and they can lead to other decision blocks or to a terminating block.
In this particular example, I made a hypothetical email classification system, which first checks the domain of the sending email address.
If this is equal to myEmployer.com, it will classify the email as “Email to read when bored.” If it isn’t from that domain, it checks to see if the body of the email contains the word hockey.
If the email contains the word hockey, then this email is classified as “Email from friends; read immediately”; if the body doesn’t contain the word hockey, then it gets classified as “Spam; don’t read.”
The kNN algorithm in chapter 2 did a great job of classifying, but it didn’t lead to any major insights about the data.
One of the best things about decision trees is that humans can easily understand the data.
The algorithm you’ll build in this chapter will be able to take a set of data, build a decision tree, and draw a tree like the one in figure 3.1
The decision tree does a great job of distilling data into knowledge.
With this, you can take a set of unfamiliar data and extract a set of rules.
The machine learning will take place as the machine creates these rules from the dataset.
Decision trees are often used in expert systems, and the results obtained by using them are often comparable to those from a human expert with decades of experience in a given field.
Now that you know a little of what decision trees are good for, we’re going to get into the process of building them from nothing but a pile of data.
In the first section, we’ll discuss methods used to construct trees and start writing code to construct a tree.
Next, we’ll address some metrics that we can use to measure the algorithm’s success.
Finally, we’ll use recursion to build our classifier and plot it using Matplotlib.
When we have the classifier working, we’ll take some data of a contact lens prescription and use our classifier to try to predict what lenses people will need.
In this section we’re going to walk through the decision tree–building algorithm, with all its fine details.
We’ll first discuss the mathematics that decide how to split a dataset using something called information theory.
We’ll then write some code to apply this theory to our dataset, and finally we’ll write some code to build a tree.
To build a decision tree, you need to make a first decision on the dataset to dictate which feature is used to split the data.
To determine this, you try every feature and measure which split will give you the best results.
The subsets will then traverse down the branches of the first decision node.
If the data on the branches is the same class, then you’ve properly classified it and don’t need to continue splitting it.
If the data isn’t the same, then you need to repeat the splitting process on this subset.
The decision on how to split this subset is done the same way as the original dataset, and you repeat this process until you’ve classified all the data.
Pseudo-code for a function called createBranch() would look like this:
Check if every item in the dataset is in the same class: If so return the class label Else find the best feature to split the data split the dataset create a branch node for each split call createBranch and add the result to the branch node return branch node.
We’ll write this in Python later, but first, we need to address how to split the dataset.
Decision trees Pros: Computationally cheap to use, easy for humans to understand learned results, missing values OK, can deal with irrelevant features.
Some decision trees make a binary split of the data, but we won’t do this.
If we split on an attribute and it has four possible values, then we’ll split the data four ways and create four separate branches.
We’ll follow the ID3 algorithm, which tells us how to split the data and when to stop splitting it.
We’re also going to split on one and only one feature at a time.
If our training set has 20 features, how do we choose which one to use first?
It contains five animals pulled from the sea and asks if they can survive without coming to the surface and if they have flippers.
We would like to classify these animals into two classes: fish and not fish.
Now we want to decide whether we should split the data based on the first feature or the second feature.
To answer this question, we need some quantitative way of determining how to split the data.
We choose to split our dataset in a way that makes our unorganized data more organized.
There are multiple ways to do this, and each has its own advantages and disadvantages.
One way to organize this messiness is to measure the information.
Prepare: This tree-building algorithm works only on nominal values, so any continuous values will need to be quantized.
You should visually inspect the tree after it is built.
Use: This can be used in any supervised learning task.
Tree construction information theory, you can measure the information before and after the split.
Information theory is a branch of science that’s concerned with quantifying information.
The change in information before and after the split is known as the information gain.
When you know how to calculate the information gain, you can split your data across every feature to see which split gives you the highest information gain.
The split with the highest information gain is your best option.
Before you can measure the best split and start splitting our data, you need to know how to calculate the information gain.
The measure of information of a set is known as the Shannon entropy, or just entropy for short.
Its name comes from the father of information theory, Claude Shannon.
If the terms information gain and entropy sound confusing, don’t worry.
They’re meant to be confusing! When Claude Shannon wrote about information theory, John von Neumann told him to use the term entropy because people wouldn’t know what it meant.
Entropy is defined as the expected value of the information.
If you’re classifying something that can take on multiple values, the information for symbol xi is defined as.
To calculate entropy, you need the expected value of all the information of all possible values of our class.
This listing will do entropy calculations on a given dataset for you.
Claude Shannon Claude Shannon is considered one of the smartest people of the twentieth century.
First, you calculate a count of the number of instances in the dataset.
This could have been calculated inline, but it’s used multiple times in the code, so an explicit variable is created for it.
Next, you create a dictionary whose keys are the values in the final column.
For each key, you keep track of how many times this label occurs.
Finally, you use the frequency of all the different labels to calculate the probability of that label.
This probability is used to calculate the Shannon entropy, C and you sum this up for all the labels.
The simple data about fish identification from table 3.1 is provided in the trees.py file by utilizing the createDataSet() function.
The higher the entropy, the more mixed up the data is.
Let’s make the data a little messier and see how the entropy changes.
We’ll add a third class, which is called maybe, and see how the entropy changes:
Listing 3.1 Function to calculate the Shannon entropy of a dataset.
Let’s split the dataset in a way that will give us the largest information gain.
We won’t know how to do that unless we actually split the dataset and measure the information gain.
Another common measure of disorder in a set is the Gini impurity,2 which is the probability of choosing an item from the set and the probability of that item being misclassified.
Instead, we’ll move on to splitting the dataset and building the tree.
You just saw how to measure the amount of disorder in a dataset.
For our classifier algorithm to work, you need to measure the entropy, split the dataset, measure the entropy on the split sets, and see if splitting it was the right thing to do.
You’ll do this for all of our features to determine the best feature to split on.
Think of it as a twodimensional plot of some data.
You want to draw a line to separate one class from another.
Should you do this on the X-axis or the Y-axis? The answer is what you’re trying to find out here.
To see this in action, open your editor and add the following code to trees.py.
The code in listing 3.2 takes three inputs: the dataset we’ll split, the feature we’ll split on, and the value of the feature to return.
Most of the time in Python, you don’t have to worry about memory or allocation.
Python passes lists by reference, so if you modify a list in a function, the list will be modified everywhere.
To account for this, you create a new list at the beginning.
Our dataset is a list of lists; you iterate over every item in the list and if it contains the value you’re looking for, you’ll add it to your newly created list.
Inside the if statement, you cut out the feature that you split on.
You used the extend() and append() methods of the Python list type.
There’s an important difference between these two methods when dealing with multiple lists.
If you do a.append(b), you have a list with four elements, and the fourth element is a list.
Let’s try out the splitDataSet() function on our simple example.
You’re now going to combine the Shannon entropy calculation and the splitDataSet() function to cycle through the dataset and decide which feature is the best to split on.
Using the entropy calculation tells you which split best organizes your data.
Open your text editor and add the code from the following listing to trees.py.
As you can guess, it chooses the feature that, when split on, best organizes your data.
The first assumption is that it comes in the form of a list of lists, and all these lists are of equal size.
The next assumption is that the last column in the data or the last item in each instance is the class label of that instance.
You use these assumptions in the first line of the function to find out how many features you have available in the given dataset.
We didn’t make any assumption on the type of data in the lists.
It could be a number or a string; it doesn’t matter.
The next part of the code in listing 3.3 calculates the Shannon entropy of the whole dataset before any splitting has occurred.
This gives you the base disorder, which you’ll later compare to the post split disorder measurements.
The first for loop loops over all the features in our dataset.
You use list comprehensions to create a list of all the ith entries in our dataset, or all the possible values present in the data.
Sets are like lists, but a value can occur only once.
Creating a new set from a list is one of the fastest ways of getting the unique values out of list in Python.
Next, you go through all the unique values of this feature and split the data for each feature.
The information gain is the reduction in entropy or the reduction in messiness.
I hope entropy makes sense when put in terms of reduction of disorder.
Finally, you compare the information gain among all the features and return the index of the best feature to split on.
After you enter the code from listing 3.3 into trees.py, type the following at your Python shell:
What just happened? The code told you that the 0th feature was the best feature to split on.
The other group will have zero yeses and two nos.
What if you split on the second feature? The first group will have two yeses and two nos.
The second group will have zero yeses and one no.
The first split does a better job of organizing the data.
Now that you can measure how organized a dataset is and you can split the data, it’s time to put all of this together and build the decision tree.
You now have all the components you need to create an algorithm that makes decision trees from a dataset.
It works like this: you start with our dataset and split it based on the best attribute to split.
These aren’t binary trees, so you can handle more than two-way splits.
Once split, the data will traverse down the branches of the tree to another node.
You’re going to use the principle of recursion to handle this.
You’ll stop under the following conditions: you run out of attributes on which to split or all the instances in a branch are the same class.
If all instances have the same class, then you’ll create a leaf node, or terminating block.
Any data that reaches this leaf node is deemed to belong to the class of that leaf node.
The first stopping condition makes this algorithm tractable, and you can even set a bound on the maximum number of splits you can have.
You’ll encounter other decision-tree algorithms later, such as C4.5 and CART.
This creates a problem for these algorithms because they split.
Tree construction the data, but the number of features doesn’t decrease at each split.
You can simply count the number of columns in our dataset to see if you’ve run out of attributes.
If our dataset has run out of attributes but the class labels are not all the same, you must decide what to call that leaf node.
Before you add the next function, you need to add the following line to the top of trees.py: import operator.
This function takes a list of class names and then creates a dictionary whose keys are the unique values in classList, and the object of the dictionary is the frequency of occurrence of each class label from classList.
Finally, you use the operator to sort the dictionary by the keys and return the class that occurs with the greatest frequency.
Open trees.py in your editor and add the code from the following listing.
The code in listing 3.4 takes two inputs: the dataset and a list of labels.
The list of labels contains a label for each of the features in the dataset.
The algorithm could function without this, but it would be difficult to make any sense of the data.
All of the previous assumptions about the dataset still hold.
You first create a list of all the class labels in our dataset and call this classList.
You could have created a special data type, but it’s not necessary.
The myTree dictionary will be used to store the tree, and you’ll see how that works soon.
You get all the unique values from the dataset for our chosen feature: bestFeat.
Finally, you iterate over all the unique values from our chosen feature and recursively call createTree() for each split of the dataset.
This value is inserted into our myTree dictionary, so you end up with a lot of nested dictionaries representing our tree.
Before we get into the nesting, note that the subLabels = labels[:] line makes a copy of labels and places it in a new list called subLabels.
You do this because Python passes lists by reference and you’d like the original list to be the same every time you call createTree()
After you add the code from listing 3.4 to trees.py, enter the following in your Python shell:
The variable myTree contains the nested dictionaries, which you’re using to represent our tree structure.
Reading left to right, the first key, 'no surfacing', is the name of the first feature that was split by the create tree.
This second dictionary’s keys are the splits of the 'no surfacing' feature.
The values of these keys are the children of the 'no surfacing' node.
The values are either a class label or another dictionary.
If the value is a class label, then that child is a leaf node.
If the value is another dictionary, then that child node is a decision node and the format repeats itself.
In our example, we have three leaf nodes and two decision nodes.
Now that you’ve properly constructed the tree, you need to display it so that humans can properly understand the information.
The tree you made in the previous section is great, but it’s a little difficult to visualize.
In this section, we’ll use Matplotlib to create a tree you can look at.
One of the greatest strengths of decision trees is that humans can easily understand them.
The plotting library we used in the previous chapter is extremely powerful.
Unfortunately, Python doesn’t include a good tool for plotting trees, so we’ll make our own.
We’ll write a program to draw a decision tree like the one in figure 3.3
Matplotlib has a great tool, called annotations, that can add text near data in a plot.
Annotations are usually used to explain some part of the data.
But having the text on top of the data looks ugly, so the tool has a built-in arrow that allows you to draw the text a safe distance away from the data yet show what data you’re talking about.
We’re going to hijack the annotations and use them for our tree plotting.
You can color in the box of the text and give it a shape you like.
Next, you can flip the arrow and have it point from the data point to the text box.
Open your text editor and create a new file called treePlotter.py.
Plot or graph? Why use the word plot? Why not use the word graph for talking about showing data in an image? In some disciplines, the word graph has a different meaning.
In applied mathematics, it’s a representation of a set of objects (vertices) connected by edges.
Any combination of the vertices can be connected by edges.
In computer science, a graph is a data structure that’s used to represent the concept from mathematics.
If createPlot() doesn’t look like createPlot() in the example text file, don’t worry.
The code in the listing begins by defining some constants that you’ll use for formatting the nodes.
It needs a plot to draw these on, and the plot is the global variable createPlot.ax1
In Python, all variables are global by default, and if you know what you’re doing, this won’t get you into trouble.
Lastly, you have the createPlot() function, which is the master.
Here, you create a new figure, clear it, and then draw on two nodes to demonstrate the different types of nodes you’ll use in plotting your tree.
To give this code a try, open your Python shell and import the treePlotter file.
You can alter the points in plotNode() C to see how the X,Y position changes.
Now that you can plot the nodes, you’re ready to combine more of these to plot a whole tree.
Now, where do you place all the nodes? You need to know how many leaf nodes you have so that you can properly size things in the X direction, and you need to know how many levels you have so you can properly size the Y direction.
You’re going to create two new functions to get the two items you’re looking for.
Listing 3.6 Identifying the number of leaves in a tree and the depth.
The two functions in listing 3.6 have the same structure, which you’ll use again later.
The structure is built around how you store the tree in a Python dictionary.
The first key is the label of the first split, and the values associated with that key are the children of the first node.
You get out the first key and value, and then you iterate over all of the child nodes.
You test to see if the child nodes are dictionaries by using the Python type() method.
The getNumLeafs() function traverses the entire tree and counts only the leaf nodes; then it returns this number.
The second function, getTreeDepth(), counts the number of times you hit a decision node.
The stopping condition is a leaf node, and once this is reached you back out of your recursive calls and increment the count.
To save you some time, I added a simple function to output premade trees.
This will save you the trouble of making a tree from data every time during testing.
Save treePlotter.py and enter the following into your Python shell:
The retrieveTree() function pulls out a predefined tree for testing.
You can see that getNumLeafs() returns three leaves, which is what tree 0 has.
Now you can put all of these elements together and plot the whole tree.
When you’re finished, the tree will look something like the one in figure 3.6 but without the labels on the X and Y axes.
Open your text editor and enter the code from the following listing into treePlotter.py.
Note that you probably already have a version of treePlotter()
The createPlot() function is the main function you’ll use, and it calls plotTree(), which in turns calls many of the previous functions and plotMidText()
The first thing that happens in plotTree() is the calculation of width and height of the tree.
These variables are used in centering the tree nodes vertically and horizontally.
The plotTree() function gets called recursively like getNumLeafs() and getTreeDepth() from listing 3.6
The width of the tree is used to calculate where to place the decision node.
The idea is to place this in the middle of all the leaf nodes below it, not place it in the middle of its children.
Also note that you use two global variables to keep track of what has already been plotted and the appropriate coordinate to place the next node.
This allows you to split the x-axis into as many segments as you have leaves.
The beautiful thing about plotting everything in terms of the image width is that you can resize the image, and the node will be redrawn in its proper place.
If this was drawn in terms of pixels, that wouldn’t be the case.
Next, you plot the child value or the value for the feature for the split going down that branch.
Next, you decrement the global variable plotTree.yOff to make a note that you’re about to draw children nodes.
You decrement rather than increment because you start drawing from the top of the image and draw downward.
You next recursively go through the tree in a similar fashion as the getNumLeafs() and getTreeDepth() functions.
If a node is a leaf node, you draw a leaf node.
Finally, after you finish plotting the child nodes, you increment the global Y offset.
The last function in listing 3.7 is createPlot(), which handles setting up the image, calculating the global tree size, and kicking off the recursive plotTree() function.
After you add the function to treePlotter.py, type the following in your Python shell:
You should see something like figure 3.6 without the axis labels.
Feel free to play around with the tree data structures and plot them out.
Now that you can build a decision tree and plot out the tree, you can to put it to use and see what you can learn from some data and this algorithm.
The main focus of the first section of this book is on classification.
We’ve done a lot of work in this chapter so far building the tree from data and plotting the tree so a human can make some sense of the data, but we haven’t yet done any classification.
In this section, you’ll build a classifier that uses our tree, and then you’ll see how to persist that classifier on disk for longer storage in a real application.
Finally, you’ll put our decision tree code to use on some real data to see if you can predict what type of contact lenses a person should use.
You want to put our tree to use doing some classification after you’ve learned the tree from our training data, but how do you do that? You need our tree and the label vector that you used in creating the tree.
The code will then take the data under test and compare it against the values in the decision tree.
It will do this recursively until it hits a leaf node; then it will stop because it has arrived at a conclusion.
To see this in action, open your text editor and add the code in the following listing to trees.py.
The code in listing 3.8 follows the same format as the other recursive functions in this chapter.
A problem with storing your data with the label as the feature’s identifier is that you don’t know where this feature is in the dataset.
To clear this up, you first split on the “no surfacing” attribute, but where is that in the dataset? Is it first or second? The Labels list will tell you this.
You use the index method to find out the first item in this list that matches firstStr.
If you reach a leaf node, you’ve made your classification and it’s time to exit.
After you’ve added the code in listing 3.8 to your trees.py file, enter the following in your Python shell:
You have a first node called “no surfacing” that has two children, one called 0, which has a label of “no”, and one that’s another decision node called “flippers”
Is this the same as between the tree you plotted and the tree data structure? Yes.
Now that you’ve built a classifier, it would be nice to be able to store this so you don’t have to rebuild the tree every time you want to do classification.
It may take a few seconds with our small datasets, but, with large datasets, this can take a long time.
When it’s time to classify items with a tree, you can do it quickly.
It would be a waste of time to build the tree every time you wanted to make a classification.
To get around this, you’re going to use a Python module, which is properly named pickle, to serialize objects, as shown in the following listing.
Serializing objects allows you to store them for later use.
Serializing can be done with any object, and dictionaries work as well.
You can experiment with this in your Python shell by typing in the following:
Now you have a way of persisting your classifier so that you don’t have to relearn it every time you want to classify something.
This is another advantage of decision trees over another machine learning algorithm like kNN from chapter 2; you can distill the dataset into some knowledge, and you use that knowledge only when you want to classify something.
Let’s use the tools you’ve learned thus far on the Lenses dataset.
In this section, we’ll go through an example that predicts the contacts lens type that should be prescribed.
You’ll take a small dataset and see if you can learn anything.
You’ll see if a decision tree can give you any insight as to how the eye doctor prescribes contact lenses.
You can predict the type of lenses people will use and understand the underlying processes with a decision tree.
The Lenses dataset3 is one of the more famous datasets.
It’s a number of observations based on patients’ eye conditions and the type of contact lenses the doctor prescribed.
The data is from the UCI database repository and is modified slightly so that it can be displayed easier.
The data is stored in a text file with the source code download.
You can load the data by typing the following into your Python shell:
That tree looks difficult to read as a line of text; it’s a good thing you have a way to plot it.
The tree plotted using our createPlot() function is shown in figure 3.8
If you follow the different branches of the tree, you can see what contact lenses should be prescribed to a given individual.
One other conclusion you can draw from figure 3.8 is that a doctor has to ask at most four questions to determine what type of lenses a patient will need.
Analyze: Quickly review data visually to make sure it was parsed properly.
Test: Write a function to descend the tree for a given instance.
Use: Persist the tree data structure so it can be recalled without building the tree; then use it in any application.
The tree in figure 3.8 matches our data well; however, it probably matches our data too well.
In order to reduce the problem of overfitting, we can prune the tree.
If a leaf node adds only a little information, it will be cut off and merged with another leaf.
In chapter 9 we’ll also investigate another decision tree algorithm called CART.
The algorithm we used in this chapter, ID3, is good but not the best.
We could use continuous values by quantizing them into discrete bins, but ID3 suffers from other problems if we have too many splits.
A decision tree classifier is just like a work-flow diagram with the terminating blocks representing classification decisions.
Starting with a dataset, you can measure the inconsistency of a set or the entropy to find a way to split the set until all the data belongs to the same class.
Recursion is used in tree-building algorithms to turn a dataset into a decision tree.
The tree is easily represented in a Python dictionary rather than a special data structure.
Cleverly applying Matplotlib’s annotations, you can turn our tree data into an easily understood chart.
The Python Pickle module can be used for persisting our tree.
The contact lens data showed that decision trees can try too hard and overfit a dataset.
This overfitting can be removed by pruning the decision tree, combining adjacent leaf nodes that don’t provide a large amount of information gain.
In the first two chapters we asked our classifier to make hard decisions.
We asked for a definite answer for the question “Which class does this data instance belong to?” Sometimes the classifier got the answer wrong.
We could instead ask the classifier to give us a best guess about the class and assign a probability estimate to that best guess.
Probability theory forms the basis for many machine-learning algorithms, so it’s important that you get a good grasp on this topic.
We touched on probability a bit in chapter 3 when we were calculating the probability of a feature taking a given Classifying with probability.
Assume for a moment that we have a dataset with two classes of data inside.
A plot of this data is shown in figure 4.1
We have the data shown in figure 4.1 and we have a friend who read this book; she found the statistical parameters of the two classes of data.
To classify a new measurement with features (x, y), we use the following rules:
Put simply, we choose the class with the higher probability.
That’s Bayesian decision theory in a nutshell: choosing the decision with the highest probability.
If you can represent the data in six floating-point numbers, and the code to calculate the probability is two lines in Python, which would you rather do?
The decision tree wouldn’t be very successful, and kNN would require a lot of calculations compared to the simple probability calculation.
Given this problem, the best choice would be the probability comparison we just discussed.
If you feel that you have a good handle on conditional probability, you can skip the next section.
Let’s spend a few minutes talking about probability and conditional probability.
If you’re comfortable with the p(x,y|c1) symbol, you may want to skip this section.
Let’s assume for a moment that we have a jar containing seven stones.
Three of these stones are gray and four are black, as shown in figure 4.2
If we stick a hand into this jar and randomly pull out a stone, what are the chances that the stone will be gray? There are seven possible stones and three are gray, so the probability is 3/7
Bayes? This interpretation of probability that we use belongs to the category called Bayesian probability; it’s popular and it works well.
Bayesian probability is named after Thomas Bayes, who was an eighteenth-century theologian.
Bayesian probability allows prior knowledge and logic to be applied to uncertain statements.
There’s another interpretation called frequency probability, which only draws conclusions from data and doesn’t allow for logic and prior knowledge.
What if the seven stones were in two buckets? This is shown in figure 4.3
If you want to calculate the P(gray) or P(black), would knowing the bucket change the answer? If you wanted to calculate the probability of drawing a gray stone from bucket B, you could probably figure out how do to that.
We’re calculating the probability of a gray stone, given that the unknown stone comes from bucket B.
To formalize how to calculate the conditional probability, we can say.
This was calculated by taking the number of gray stones in bucket B and dividing by the total number of stones.
Now, P(bucketB) is 3/7 because there are three stones in bucket B of the total seven stones.
This formal definition may seem like too much work for this simple example, but it will be useful when we have more features.
It’s also useful to have this formal definition if we ever need to algebraically manipulate the conditional probability.
Another useful way to manipulate conditional probabilities is known as Bayes’ rule.
Bayes’ rule tells us how to swap the symbols in a conditional probability statement.
Figure 4.2 A collection has seven stones that are gray or black.
If we randomly select a stone from this set, the probability it will be a gray stone is 3/7
Similarly, the probability of selecting a black stone is 4/7
Now that we’ve discussed conditional probability, we need to see how to apply this to our classifier.
The next section will discuss how to use conditional probabilities with Bayesian decision theory.
In section 4.1, I said that Bayesian decision theory told us to find the two probabilities:
The problem is that the equation from our friend is p(x,y|c1), which is not the same.
With these definitions, we can define the Bayesian classification rule:
Using Bayes’ rule, we can calculate this unknown from three known quantities.
We’ll soon write some code to calculate these probabilities and classify items using Bayes’ rule.
Now that we’ve introduced a bit of probability theory, and you’ve seen how you can build a classifier with it, we’re going to put this in action.
The next section will introduce a simple yet powerful application of the Bayesian classifier.
Earlier I mentioned that we’re going to use individual words as features and look for the presence or absence of each word.
How many features is that? Which (human) language are we assuming? It may be more than one language.
In order to generate good probability distributions, we need enough data samples.
In order to get features from our text, we need to split up the text.
But how do we do that? Our features are going to be tokens we get from the text.
You can think of tokens as words, but we may use things that aren’t words such as URLs, IP addresses, or any string of characters.
To see this in action, let’s make a quick filter for an online message board that flags a message as inappropriate if the author uses negative or abusive language.
Filtering out this sort of thing is common because abusive postings make people not come back and can hurt an online community.
We’re going to start looking at text in the form of word vectors or token vectors, that is, transform a sentence into a vector.
We consider all the words in all of our documents and decide what we’ll use for a vocabulary or set of words we’ll consider.
Next, we need to transform each individual document into a vector from our vocabulary.
To get started, open your text editor, create a new file called bayes.py, and add the code from the following listing.
The first function creates some example data to experiment with.
The first variable returned from loadDatSet() is a tokenized set of documents from a Dalmatian (spotted breed of dog) lovers message board.
The text has been broken up into a set of tokens.
The second variable of loadDatSet() returns a set of class labels.
The text has been labeled by a human and will be used to train a program to automatically detect abusive posts.
Next, the function createVocabList() will create a list of all the unique words in all of our documents.
To create this unique list you use the Python set data type.
You can give a list of items to the set constructor, and it will only return a unique list.
Bitwise OR and set union also use the same symbols in mathematical notation.
You then create a vector the same length as the vocabulary list and fill it up with 0s.
If everything goes well, you shouldn’t need to test if a word is in vocabList, but you may use this later.
Save bayes.py, and enter the following into your Python shell:
If you examine this list, you’ll see that there are no repeated words.
The list is unsorted, and if you want to sort it, you can do that later.
This has taken our vocabulary list or list of all the words you’d like to examine and created a feature for each of them.
Now when you apply a given document (a posting to the Dalmatian site), it will be transformed into a word vector.
What’s the word at index 2 in myVocabList? It should be help.
Now check to see that it isn’t in our fourth document.
Now that you’ve seen how to convert from words to numbers, let’s see how to calculate the probabilities with these numbers.
You know whether a word occurs in a document, and you know what class the document belongs to.
Do you remember Bayes’ rule from section 3.2? It’s rewritten here, but I’ve changed the x,y to w.
The bold type means that it’s a vector; that is, we have many values, in our case as many values as words in our vocabulary.
Count the number of documents in each class for every training document:
The code in the following listing will do these calculations for us.
Open your text editor and insert this code into bayes.py.
This function uses some functions from NumPy, so make sure you add from numpy import * to the top of bayes.py.
The function in listing 4.2 takes a matrix of documents, trainMatrix, and a vector with the class labels for each of the documents, trainCategory.
The first thing you do is calculate the probability the document is an abusive document (class=1)
For more than a two-class problem, you’d need to modify this a little.
The numerator is a NumPy array with the same number of elements as you have words in your vocabulary.
In the for loop you loop over all the documents in trainMatrix, or our training set.
Finally, you divide every element by the total number of words for that class.
After you’ve added the code from listing 4.2 to bayes.py, open your Python shell and enter the following:
This for loop populates the trainMat list with word vectors.
Now let’s get the probabilities of being abusive and the two probability vectors:
First, you found the probability that a document was abusive: pAb; this is 0.5, which is correct.
Next, you found the probabilities of the words from our vocabulary given the document class.
If you look at the word in myVocabList at index 26, you’ll see that it’s the word stupid.
This tells you that the word stupid is most indicative of a class 1 (abusive)
Before we can go on to classification with this, we need to address a few flaws in the previous function.
When we attempt to classify a document, we multiply a lot of probabilities together to get the probability that a document belongs to a given class.
Another problem is underflow: doing too many multiplications of small numbers.
One solution to this is to take the natural logarithm of this product.
Doing this allows us to avoid the underflow or round-off error problem.
Do we lose anything by using the natural log of a number rather than the number itself? The answer is no.
It’s quite simple when we’re using vector math with NumPy.
Open your text editor and add the code from the following listing to bayes.py.
This shows that the natural log of a function can be used in place of a function when you’re interested in finding the maximum value of that function.
You next add up the values for all of the words in our vocabulary and add this to the log probability of the class.
Finally, you see which probability is greater and return the class label.
After you’ve added the code from listing 4.3, enter the following into your Python shell:
Up until this point we’ve treated the presence or absence of a word as a feature.
If a word appears more than once in a document, that might convey some sort of information about the document over just the word occurring in the document or not.
A bag of words can have multiple occurrences of each word, whereas a set of words can have only one occurrence of each word.
The code to use the bag-of-words model is given in the following listing.
Now that we have a classifier built, we should be able to put this into action classifying spam.
First, we’ll create some code to parse text into tokens.
Next, we’ll write a function that ties together the parsing and the classification code from earlier in this chapter.
This function will also test the classifier and give us an error rate.
If you have a text string, you can split it using the Python string .split() method.
That works well, but the punctuation is considered part of the word.
You can use regular expressions to split up the sentence on anything that isn’t a word or number:
Analyze: Inspect the tokens to make sure parsing was done correctly.
Test: Use classifyNB() and create a new testing function to calculate the error rate over a set of documents.
Use: Build a complete program that will classify a group of documents and print misclassified documents to the screen.
But you have some empty strings you need to get rid of.
If you were looking at sentences, this would be helpful.
You’re just looking at a bag of words, so you want all the words to look the same whether they’re in the middle, end, or beginning of a sentence.
Python has built-in methods for converting strings to all lowercase (.lower()) or all uppercase (.upper())
Now let’s see this in action with a full email from our email dataset.
The email dataset is in a folder called email, with two subfolders called spam and ham.
The file named 6.txt in the ham folder is quite long.
It’s from a company telling me that they no longer support something.
When we split the URL we got a lot of words.
We’d like to get rid of these words, so we’ll filter out words with less than three characters.
In a real-world parsing program, you should have more advanced filters that look for things like HTML and URIs.
We’ll create a bare-bones function, and you can modify as you see fit.
Let’s put this text parser to work with a whole classifier.
Open your text editor and add the code from this listing to bayes.py.
The first function, textParse(), takes a big string and parses out the text into a list of strings.
It eliminates anything under two characters long and converts everything to lowercase.
There’s a lot more parsing you could do in this function, but it’s good enough for our purposes.
The next for loop iterates through all the items in the test set and creates word vectors from the words of each email and the vocabulary using setOfWords2Vec()
These words are used in traindNB0() to calculate the probabilities needed for classification.
You then iterate through the test set and classify each email in the test set.
After you’ve entered the code from listing 4.5, enter the following into your Python shell:
The function spamTest() displays the error rate from 10 randomly selected emails.
Since these are randomly selected, the results may be different each time.
If there’s an error, it will display the word list for that document to give you an idea of what was misclassified.
To get a good estimate of the error rate, you should repeat this procedure multiple times, say 10, and average the results.
I did that and got an average error rate of 6%
The error that keeps appearing is a piece of spam that was misclassified as ham.
It’s better that a piece of spam sneaks through the filter than a valid email getting shoved into the spam folder.
In this last example, we’ll take some data from personals ads from multiple people for two different cities in the United States.
We’re going to see if people in different cities use different words.
We’re going to use the city that each ad comes from to train a classifier and then see how well it does.
Finally, we’re not going to use this to classify anything.
We’re going to look at the words and conditional probability scores to see if we can learn anything specific to one city over another.
The first thing we’re going to need to do is use Python to download the text.
You should be able to install it like other Python packages, by unzipping the downloaded package, changing your directory to the unzipped package, and then typing >>python setup.py install at the command prompt.
We’re going to use the personal ads from Craigslist, and hopefully we’ll stay Terms Of Service compliant.
To open the RSS feed from Craigslist, enter the following at your Python shell:
I’ve decided to use the step, or strictly platonic, section from Craigslist because other sections can get a little lewd.
You can play around with the feed and check out the great documentation at feedparser.org.
You can create a function similar to spamTest() to automate your testing.
Open your text editor and enter the code from the following listing.
Analyze: Inspect the tokens to make sure parsing was done correctly.
Test: We’ll look at the error rate to make sure this is actually working.
We can make modifications to the tokenizer to improve the error rate and results.
Use: We’ll build a complete program to wrap everything together.
It will display the most common words given in two RSS feeds.
One helper function is included in listing 4.6; the function is called calcMostFreq()
The dictionary is then sorted by frequency from highest to lowest, and the top 100 words are returned.
The reason for doing this is that feeds can change.
Listing 4.6 RSS feed classifier and frequent word removal functions.
You can comment out the three lines that removed the most frequently used words and see the performance before and after.
The size of the vocabList was ~3000 words when I was testing this.
A small percentage of the total words makes up a large portion of the text.
The reason for this is that a large percentage of language is redundancy and structural glue.
Another common approach is to not just remove the most common words but to also remove this structural glue from a predefined list.
This is known as a stop word list, and there are a number of sources of this available.
At the time of writing, http://www.ranks.nl/resources/stopwords.html has a good list of stop words in multiple languages.
After you’ve entered the code from listing 4.6 into bayes.py, you can test it in Python by typing in the following:
To get a good estimate of the error rate, you should do multiple trials of this and take the average.
The error rate here is much higher than for the spam testing.
That is not a huge problem because we’re interested in the word probabilities, not actually classifying anything.
You can play around the number of words removed by caclMostFreq() and see how the error rate changes.
You can sort the vectors pSF and pNY and then print out the words from vocabList at the same index.
There’s one last piece of code that does this for you.
Open bayes.py one more time and enter the code from the following listing.
To see this in action, enter the following in your Python shell after you’ve saved bayes.py.
One thing to note: a lot of stop words appear in the output.
It would be interesting to see how things would change if you removed the fixed stop words.
In my experience, the classification error will also go down.
Using probabilities can sometimes be more effective than using hard rules for classification.
Bayesian probability and Bayes’ rule gives us a way to estimate unknown probabilities from known values.
The probability theory you learned in this chapter will be used again later in the book, and this chapter was a great introduction to the full power of Bayesian probability theory.
You’ll next see a classification method called logistic regression and some optimization algorithms.
If you think about it, many of the things we do in life are optimization problems.
Some examples of optimization from daily life are these: How do we get from point A to point B in the least amount of time? How do we make the most money doing the least amount of work? How do we design an engine to produce the most horsepower while using the least amount of fuel? The things we can do with optimization are powerful.
I’ll introduce a few optimization algorithms to train a nonlinear function for classification.
Perhaps you’ve seen some data points and then someone fit a line called the best-fit line to these points; that’s regression.
What happens in logistic regression is we have a bunch of data, and with Logistic regression.
This is an exciting chapter because this is the first chapter where we encounter.
The exact math behind this you’ll see in the next part of the book, but the regression aspects means that we try to find a best-fit set of parameters.
Finding the best fit is similar to regression, and in this method it’s how we train our classifier.
This best-fit stuff is where the name regression comes from.
We’ll talk about the math behind making this a classifier that puts out one of two values.
In this chapter you’ll first learn what logistic regression is, and then you’ll learn some optimization algorithms.
In our study of optimization algorithms, you’ll learn gradient ascent, and then we’ll look at a modified version called stochastic gradient ascent.
These optimization algorithms will be used to train our classifier.
Next, you’ll see logistic regression in action predicting whether a horse with an illness will live or die.
We’d like to have an equation we can give all of our features and it will predict the class.
Train: We’ll spend most of the time training, where we try to find optimal coefficients to classify our data.
Test: Classification is quick and easy once the training step is done.
Use: This application needs to get some input data and output structured numeric values.
Next, the application applies the simple regression calculation on this input data and determines which class the input data should belong to.
The application then takes some action on the calculated class.
Logistic regression Pros: Computationally inexpensive, easy to implement, knowledge representation easy to interpret.
Classification with logistic regression and the sigmoid function: a tractable step function before; it’s called the Heaviside step function, or sometimes just the step function.
There’s another function that behaves in a similar fashion, but it’s much easier to deal with mathematically.
Two plots of the sigmoid are given in figure 5.1
On a large enough scale (the bottom frame of figure 5.1), the sigmoid looks like a step function.
For the logistic regression classifier we’ll take our features and multiply each one by a weight and then add them up.
You can also think of logistic regression as a probability estimate.
The bottom plot shows a much larger scale where the sigmoid appears similar to a step function at x=0
The question now becomes, what are the best weights, or regression coefficients to use, and how do we find them? The next section will address this question.
The input to the sigmoid function described will be z, where z is given by the following:
All that means is that we have two vectors of numbers and we’ll multiply each element and add them up to get one number.
The vector x is our input data, and we want to find the best coefficients w, so that this classifier will be as successful as possible.
In order to do that, we need to consider some ideas from optimization theory.
We’ll then see how we can use this method of optimization to find the best parameters to model our dataset.
Next, we’ll show how to plot the decision boundary generated with gradient ascent.
This will help you visualize the successfulness of gradient ascent.
Next, you’ll learn about stochastic gradient ascent and how to make modifications to yield better results.
The first optimization algorithm we’re going to look at is called gradient ascent.
Gradient ascent is based on the idea that if we want to find the maximum point on a function, then the best way to move is in the direction of the gradient.
We write the gradient with the symbol and the gradient of a function f(x,y) is given by the equation.
This is one of the aspects of machine learning that can be confusing.
You just need to keep track of what symbols mean.
The function f(x,y) needs to be defined and differentiable around the points where it’s being evaluated.
The gradient ascent algorithm shown in figure 5.2 takes a step in the direction given by the gradient.
The gradient operator will always point in the direction of the greatest increase.
We’ve talked about direction, but I didn’t mention anything to do with magnitude of movement.
The magnitude, or step size, we’ll take is given by the parameter.
In vector notation we can write the gradient ascent algorithm as.
This step is repeated until we reach a stopping condition: either a specified number of steps or the algorithm is within a certain tolerance margin.
Let’s put this into action on our logistic regression classifier and some Python.
Figure 5.2 The gradient ascent algorithm moves in the direction of the gradient evaluated at each point.
The gradient operator always ensures that we’re moving in the best possible direction.
It’s the same thing as gradient ascent, except the plus sign is changed to a minus sign.
With gradient descent we’re trying to minimize some function rather than maximize it.
We’ll try to use gradient ascent to fit the best parameters for the logistic regression model to our data.
We’ll do this by finding the best weights for this given dataset.
Start with the weights all set to 1 Repeat R number of times:
Calculate the gradient of the entire dataset Update the weights vector by alpha*gradient Return the weights vector.
To see it in action, open your text editor and create a new file called logRegres.py.
We’re going to attempt to use gradient descent to find the best weights for a logistic regression classifier on this dataset.
The code in listing 5.1 starts out with a convenience function, loadDataSet()
This opens the text file testSet.txt and reads every line.
The next function, sigmoid(), is our function from section 5.2
The real work is done in the function gradAscent(), which takes two inputs.
The first input, dataMatIn, is a 2D NumPy array, where the columns are the different features and the rows are the different training examples.
In B you take the input arrays and convert them to NumPy matrices.
This is the first time in this book where you’re using NumPy matrices, and if you’re not familiar with matrix math, then some calculations can seem strange.
NumPy can operate on both 2D arrays and matrices, and the results will be different if you assume the wrong data type.
Please see appendix A for an introduction to NumPy matrices.
The input classLabels is a 1x100 row vector, and for the matrix math to work, you need it to be a column vector, so you take the transpose of it and assign that to the variable labelMat.
Next, you get the size of the matrix and set some parameters for our gradient ascent algorithm.
The variable alpha is the step size you’ll take toward the target, and maxCycles is the number of times you’re going to repeat the calculation before stopping.
The for loop iterates over the dataset, and finally you return the weights.
One thing I’d like to stress is that the calculations in C are matrix operations.
Lastly, one thing I’d like to mention is that the first two lines in the formula in C may not be familiar, and I haven’t really derived them.
A little math is needed to derive the equations used here, and I’ll leave it to you to look into that further if desired.
Qualitatively you can see we’re calculating the error between the actual class and the predicted class and then moving in the direction of that error.
Open your text editor and add the code from listing 5.1
We’re solving for a set of weights used to make a line that separates the different classes of data.
How can we plot this line to understand this optimization procedure? In order to make a plot like this, you’ll need the code in the next listing.
The code in listing 5.2 is a straightforward plot using Matplotlib.
Listing 5.2 Plotting the logistic regression best-fit line and dataset.
To use the code in listing 5.2, type the following:
You should get something similar to the plot in figure 5.4
One thing to stress is that this method took a lot of calculations; even our simple example used 300 multiplications on a tiny dataset.
We’ll need to alter the algorithm a little in order for it to work on real-world datasets, and we’ll do that in the next section.
The previous optimization algorithm, gradient ascent, uses the whole dataset on each update.
This was fine with 100 examples, but with billions of data points containing thousands of features, it’s unnecessarily expensive in terms of computational.
An alternative to this method is to update the weights using only one instance at a time.
Stochastic gradient ascent is an example of an online learning algorithm.
This is known as online because we can incrementally update the classifier as new data comes in rather than all at once.
Pseudo-code for the stochastic gradient ascent would look like this:
Start with the weights all set to 1 For each piece of data in the dataset:
Calculate the gradient of one piece of data Update the weights vector by alpha*gradient Return the weights vector.
You can see that stochastic gradient ascent is similar to gradient ascent except that the variables h and error are now single values rather than vectors.
There also is no matrix conversion, so all of the variables are NumPy arrays.
To try this out, enter the code from listing 5.3 into logRegres.py and enter the following into your Python shell:
The resulting best-fit line is OK but certainly not as great as the previous example from gradient ascent.
If we were to use this as our classifier, we’d misclassify one-third of the results.
One way to look at how well the optimization algorithm is doing is to see if it’s converging.
I then plotted the weights, as shown in figure 5.6
Figure 5.5 Our simple dataset with solution from stochastic gradient ascent after one pass through the dataset.
The best-fit line isn’t a good separator of the data.
Figure 5.6 Weights versus iteration number for one pass through the dataset, with this method.
It takes a large number of cycles for the weights to reach a steady-state value, and there are still local fluctuations.
An additional item to notice from this plot is that there are small periodic variations, even though the large variation has stopped.
If you think about what’s happening, it should be obvious that there are pieces of data that don’t classify correctly and cause a large change in the weights.
We’d like to see the algorithm converge to a single value rather than oscillate, and we’d like to see the weights converge more quickly.
Here, you’re randomly selecting each instance to use in updating the weights.
This will reduce the periodic variations that you saw in figure 5.6
An optional argument to the function has also been added.
If no third argument is given, then 150 iterations will be done.
But if a third argument is given, that will override the default.
This is due to the random vector selection of stocGradAscent1()
This is because with stocGradAscent1() we can converge on weights much more quickly.
Let’s see this code in action on the same dataset as the previous examples.
After you’ve entered the code from listing 5.4 into logRegres.py, enter the following in your Python shell:
You should see a plot similar to that in figure 5.8
The results are similar to those of GradientAscent(), but far fewer calculations were involved.
This method is much faster to converge than using a fixed alpha.
A number of books have been written on the subject.
You can also adjust the parameters in our algorithm to give better results for a given dataset.
So far we’ve looked at how the weights change, but we haven’t done a lot of classification, which is the purpose of this section and chapter.
In the next section, we’ll put stochastic gradient ascent to work on a problem of horse colic.
In this section, we’ll use logistic regression to try to predict if a horse with colic will live or die.
From what I’ve read, horse colic is a general term used to describe gastrointestinal pain in horses.
The pain may or may not be from gastrointestinal problems.
Example: estimating horse fatalities from colic measurements from horses seen by a hospital for colic.
Some of the measurements are subjective, and some are difficult to measure, such as the pain level in the horse.
In addition to the obvious problems with the data, there’s another problem: 30% of the values are missing.
We’ll first handle the problem of how to deal with missing values in a dataset, and then we’ll use logistic regression and stochastic gradient ascent to forecast whether a horse will live or die.
Missing values in your data is a big problem, and there have been many pages of text books dedicated to dealing with this problem.
What if a sensor on this machine was broken and one feature was useless? Do you throw out all the data? What about the 19 other features; do they have anything useful to tell you? Yes, they do.
Sometimes data is expensive, and you don’t have the option to throw it out or collect it all over again, so you need a method for handling this problem.
The dataset we’ll use in the next section will be preprocessed so that we can easily use it with our existing algorithm.
During the preprocessing, I decided to do two things from the list.
First, I had to replace all the unknown values with a real number because we’re using NumPy, and in NumPy arrays can’t contain a missing value.
The number chosen was 0, which happens to work out well for logistic regression.
Prepare: Parse a text file in Python, and fill in missing values.
Train: Use an optimization algorithm to find the best coefficients.
Test: To measure the success, we’ll look at error rate.
Depending on the error rate, we may decide to go back to the training step to try to find better values for the regression coefficients by adjusting the number of iterations and step size.
Use: Building a simple command-line program to collect horse symptoms and output live/die diagnosis won’t be difficult.
If dataMatrix is 0 for any feature, then the weight for that feature will simply be.
Also, the error term will not be impacted by this because sigmoid(0)=0.5, which is totally neutral for predicting the class.
For these reasons, replacing missing values with 0 allows us to keep our imperfect data without compromising the learning algorithm.
Also, none of the features take on 0 in the data, so in some sense it’s a special value.
Second, there was a missing class label in the test data.
This solution makes sense given that we’re using logistic regression, but it may not make sense with something like kNN.
You can see the data at http://archive.ics.uci.edu/ml/datasets/Horse+Colic if you want to compare the original data and the preprocessed data.
Now that we have a clean set of data and a good optimization algorithm, we’re going to put all these parts together and build a classifier to see if we can predict whether a horse will die from colic.
We spent a lot of time in the previous sections of this chapter talking about optimization algorithms.
With logistic regression you don’t need to do much to classify an instance.
All you have to do is calculate the sigmoid of the vector under test multiplied by the weights optimized earlier.
To see this in action, open your favorite text editor and enter the following code in logRegres.py.
This takes the weights and an input vector and calculates the sigmoid.
This is a standalone function that opens the test set and training set and properly formats the data.
You use the convention that the last column contains the class value.
Originally, the data had three class values representing what happened to the horse: lived, died, or was euthanized.
For the purposes of this exercise, I bundled died and euthanized into one category called “did not live.” After this data is loaded, the weights vector is calculated using stocGradAscent1()
After the weights are calculated, the test set is loaded and an error rate is calculated.
If you run it multiple times, you’ll get slightly different results because of the random components.
If the weights totally converged in stocGradAscent1(), then there would be no random components.
The last function, multiTest(), runs the function colicTest() 10 times and takes the average.
To see this in action, enter the following at your Python shell:
This wasn’t bad with over 30% of the values missing.
Logistic regression is finding best-fit parameters to a nonlinear function called the sigmoid.
Methods of optimization can be used to find the best-fit parameters.
Among the optimization algorithms, one of the most common algorithms is gradient ascent.
Stochastic gradient ascent can do as well as gradient ascent using far fewer computing resources.
In addition, stochastic gradient ascent is an online algorithm; it can update what it has learned as new data comes in rather than reloading all of the data as in batch processing.
One major problem in machine learning is how to deal with missing values in the data.
It really depends on what you’re doing with the data.
There are a number of solutions, and each solution has its own advantages and disadvantages.
In the next chapter we’re going to take a look at another classification algorithm similar to logistic regression.
The algorithm is called support vector machines and is considered one of the best stock algorithms.
I think if you just read a little bit of the theory and then look at production C++ SVM code, you’re going to have trouble understanding it.
But if we strip out the production code and the speed improvements, the code becomes manageable, perhaps understandable.
Support vector machines are considered by some people to be the best stock classifier.
This means you can take the classifier in its basic form and run it on the data, and the results will have low error rates.
Support vector machines make good decisions for data points that are outside the training set.
In this chapter you’re going to learn what support vector machines are, and I’ll introduce some key terminology.
There are many implementations of support vector Support vector machines.
I’ve seen more than one book follow this pattern when discussing support vector.
After that, you’ll see how to use something called kernels to extend SVMs to a larger number of datasets.
Finally, we’ll revisit the handwriting example from chapter 1 to see if we can do a better job with SVMs.
To introduce the subject of support vector machines I need to explain a few concepts.
There are two groups of data, and the data points are separated enough that you could draw a straight line on the figure with all the points of one class on one side of the line and all the points of the other class on the other side of the line.
If such a situation exists, we say the data is linearly separable.
We’ll later make some changes where the data points can spill over the line.
Support vector machines Pros: Low generalization error, computationally inexpensive, easy to interpret results.
Cons: Sensitive to tuning parameters and kernel choice; natively only handles binary classification.
The line used to separate the dataset is called a separating hyperplane.
Everything on one side belongs to one class, and everything on the other side belongs to a different class.
We’d like to make our classifier in such a way that the farther a data point is from the decision boundary, the more confident we are about the prediction we’ve made.
They all separate the data, but which one does it best? Should we minimize the average distance to the separating hyperplane? In that case, are frames B and C any better than frame D in figure 6.2? Isn’t something like that done with best-fit lines? Yes, but it’s not the best idea here.
We’d like to find the point closest to the separating hyperplane and make sure this is as far away from the separating line as possible.
We want to have the greatest possible margin, because if we made a mistake or trained our classifier on limited data, we’d want it to be as robust as possible.
The points closest to the separating hyperplane are known as support vectors.
Now that we know that we’re trying to maximize the distance from the separating line to the support vectors, we need to find a way to optimize this problem.
Figure 6.2 Linearly separable data is shown in frame A.
Frames B, C, and D show possible valid lines separating the two classes of data.
How can we measure the line that best separates the data? To start with, look at figure 6.3
If we want to find the distance from A to the separating plane, we must measure normal or perpendicular to the line.
The constant b is just an offset like w0 in logistic regression.
All this w and b stuff describes the separating line, or hyperplane, for our data.
I’ve talked about the classifier but haven’t mentioned how it works.
Understanding how the classifier works will help you to understand the optimization problem.
We’ll have a simple equation like the sigmoid where we can enter our data values and get a class label out.
When we’re doing this and deciding where to place the separating line, this margin is calculated by label*(wTx+b)
If a point is far away from the separating plane on the positive side, then wTx+b will be a large positive number, and label*(wTx+b) will give us a large number.
If it’s far from the negative side and has a negative label, label*(wTx+b) will also give us a large positive number.
The goal now is to find the w and b values that will define our classifier.
To do this, we must find the points with the smallest margin.
Then, when we find the points with the smallest margin, we must maximize that margin.
Figure 6.3 The distance from point A to the separating plane is measured by a line normal to the separating plane.
Solving this problem directly is pretty difficult, so we can convert it into another form that we can solve more easily.
Let’s look at the inside of the previous equation, the part inside the curly braces.
Optimizing multiplications can be nasty, so what we do is hold one part fixed and then maximize the other part.
For values farther away from the hyperplane, this product will be larger.
The optimization problem we now have is a constrained optimization problem because we must find the best values, provided they meet some constraints.
There’s a well-known method for solving these types of constrained optimization problems, using something called Lagrange multipliers.
Using Lagrange multipliers, we can write the problem in terms of our constraints.
Because our constraints are our data points, we can write the values of our hyperplane in terms of our data points.
This is great, but it makes one assumption: the data is 100% linearly separable.
We know by now that our data is hardly ever that clean.
With the introduction of something called slack variables, we can allow examples to be on the wrong side of the decision boundary.
Our optimization goal stays the same, but we now have a new set of constraints:
The constant C controls weighting between our goal of making the margin large and ensuring that most of the examples have a functional margin of at least 1.0
The constant C is an argument to our optimization code that we can tune and get different results.
Once we solve for our alphas, we can write the separating hyperplane in terms of these alphas.
The majority of the work in SVMs is finding the alphas.
There have been some large steps taken in coming up with these equations here.
I encourage you to seek a textbook to see a more detailed derivation if you’re interested.1,2
In chapter 1, we defined common steps for building machine learning–based applications.
These steps may change from one machine learning task to another and from one algorithm to another.
It’s worth taking a few minutes to see how these will apply to the algorithm we’re looking at in this chapter.
Now that we have a little bit of the theory behind us, we’d like to be able to program this problem so that we can use it on our data.
The next section will introduce a simple yet powerful algorithm for doing so.
We’ll now discuss the SMO algorithm, and then we’ll write a simplified version of it so that you can properly understand how it works.
In the next section we’ll move from the simplified to the full version, which works much faster than the simplified version.
Train: The majority of the time will be spent here.
Use: You can use an SVM in almost any classification problem.
One thing to note is that SVMs are binary classifiers.
You’ll need to write a little more code to use an SVM on a problem with more than two classes.
Efficient optimization with the SMO algorithm and it takes the large optimization problem and breaks it into many small problems.
The small problems can easily be solved, and solving them sequentially will give you the same answer as trying to solve everything together.
In addition to getting the same answer, the amount of time is greatly reduced.
The SMO algorithm works to find a set of alphas and b.
Once we have a set of alphas, we can easily compute our weights w and get the separating hyperplane.
Here’s how the SMO algorithm works: it chooses two alphas to optimize on each cycle.
Once a suitable pair of alphas is found, one is increased and one is decreased.
To be suitable, a set of alphas must meet certain criteria.
One criterion a pair must meet is that both of the alphas have to be outside their margin boundary.
The second criterion is that the alphas aren’t already clamped or bounded.
Implementing the full Platt SMO algorithm can take a lot of code.
We’ll simplify it in our first example to get an idea of how it works.
After we get the simplified version working, we’ll build on it to see the full version.
The simplification uses less code but takes longer at runtime.
The outer loops of the Platt SMO algorithm determine the best alphas to optimize.
We’ll skip that for this simplified version and select pairs of alphas by first going over every alpha in our dataset.
Then, we’ll choose the second alpha randomly from the remaining alphas.
It’s important to note here that we change two alphas at the same time.
We need to do this because we have a constraint:
Changing one alpha may cause this constraint to be violated, so we always change two at a time.
To do this we’re going to create a helper function that randomly selects one integer from a range.
We also need a helper function to clip values if they get too big.
Open a text editor and add the code to svmMLiA.py.
The data that’s plotted in figure 6.3 is available in the file testSet.txt.
The first function in listing 6.1 is our familiar loadDatSet(), which opens up the file and parses each line into class labels, and our data matrix.
The first one, i, is the index of our first alpha, and m is the total number of alphas.
A value is randomly chosen and returned as long as it’s not equal to the input i.
The last helper function, clipAlpha(), clips alpha values that are greater than H or less than L.
These three helper functions don’t do much on their own, but they’ll be useful in our classifier.
After you’ve entered the code from listing 6.1 and saved it, you can try these out using the following:
Now that we have these working, we’re ready for our first version of the SMO.
Create an alphas vector filled with 0s While the number of iterations is less than MaxIterations:
The code in listing 6.2 is a working version of the SMO algorithm.
In Python, if we end a line with \, the interpreter will assume the statement is continued on the next line.
There are a number of long lines in the following code that need to be broken up, so I’ve used the \ symbol for this.
Open the file svmMLiA.py and enter the code from the following listing.
It’s probably the biggest one you’ll see in this book.
We’ve been building functions in this book with a common interface so you can mix and match algorithms and data sources.
This function takes lists and inputs and transforms them into NumPy matrices so that you can simplify many of the math operations.
The class labels are transposed so that you have a column vector instead of a list.
This makes the row of the class labels correspond to the row of the data matrix.
You also get the constants m and n from the shape of the dataMatIn.
Finally, you create a column matrix for the alphas, initialize this to zero, and create a variable called iter.
This variable will hold a count of the number of times you’ve gone through the dataset without any alphas changing.
When this number reaches the value of the input maxIter, you exit.
In each iteration, you set alphaPairsChanged to 0 and then go through the entire set sequentially.
The variable alphaPairsChanged is used to record if the attempt to optimize any alphas worked.
First, fXi is calculated; this is our prediction of the class.
The error Ei is next calculated based on the prediction and the real class of this instance.
If this error is large, then the alpha corresponding to this data instance can be optimized.
In the if statement, both the positive and negative margins are tested.
In this if statement, you also check to see that the alpha isn’t equal to 0 or C.
Alphas will be clipped at 0 or C, so if they’re equal to these, they’re “bound” and can’t be increased or decreased, so it’s not worth trying to optimize these alphas.
Next, you randomly select a second alpha, alpha[j], using the helper function described in listing 6.1 C.
You calculate the “error” for this alpha similar to what you did for the first alpha, alpha[i]
The next thing you do is make a copy of alpha[i] and alpha[j]
You do this with the copy() method, so that later you can compare the new alphas and the old ones.
Python passes all lists by reference, so you have to explicitly tell Python to give you a new memory location for alphaIold and alphaJold.
Otherwise, when you later compare the new and old values, we won’t see the change.
You then calculate L and H D, which are used for clamping alpha[j] between 0 and C.
If L and H are equal, you can’t change anything, so you issue the continue statement, which in Python means “quit this loop now, and proceed to the next item in the for loop.”
If eta is 0, you also quit the current iteration of the for loop.
This step is a simplification of the real SMO algorithm.
If eta is 0, there’s a messy way to calculate the new alpha[j], but we won’t get into that here.
You can read Platt’s original paper if you really want to know how that works.
It turns out this seldom occurs, so it’s OK if you skip it.
You calculate a new alpha[j] and clip it using the helper function from listing 6.1 and our L and H values.
Next, you check to see if alpha[j] has changed by a small amount.
Next, alpha[i] is changed by the same amount as alpha[j] but in the opposite direction E.
After you optimize alpha[i] and alpha[j], you set the constant term b for these two alphas F.
Finally, you’ve finished the optimization, and you need to take care to make sure you exit the loops properly.
If you’ve reached the bottom of the for loop without hitting a continue statement, then you’ve successfully changed a pair of alphas and you can increment alphaPairsChanged.
Outside the for loop, you check to see if any alphas have been updated; if so you set iter to 0 and continue.
You’ll only stop and exit the while loop when you’ve gone through the entire dataset maxIter number of times without anything changing.
You can look at the alphas matrix by itself, but there’ll be a lot of 0 elements inside.
To see the number of elements greater than 0, type in the following:
Your results may differ from these because of the random nature of the SMO algorithm.
The command alphas[alphas>0] is an example of array filtering, which is specific to NumPy and won’t work with a regular list in Python.
If you type in alphas>0, you’ll get a Boolean array with a true in every case where the inequality holds.
The original dataset with these points circled is shown in figure 6.4
Using the previous settings, I ran this 10 times and took the average time.
This wasn’t bad, but this is a small dataset with only 100 points.
On larger datasets, this would take a long time to converge.
In the next section we’re going speed this up by building the full SMO algorithm.
The simplified SMO works OK on small datasets with a few hundred points but slows down on larger datasets.
Now that we’ve covered the simplified version, we can move on to the full Platt version of the SMO algorithm.
The optimization portion where we change alphas and do all the algebra stays the same.
The only difference is how we select which alpha to use in the optimization.
The full Platt uses some heuristics that increase the speed.
Perhaps in the previous section when executing the example you saw some room for improvement.
The Platt SMO algorithm has an outer loop for choosing the first alpha.
This alternates between single passes over the entire dataset and single passes over non-bound alphas.
The non-bound alphas are alphas that aren’t bound at the limits 0 or C.
The pass over the entire dataset is easy, and to loop over the non-bound alphas we’ll first create a list of these alphas and then loop over the list.
The second alpha is chosen using an inner loop after we’ve selected the first alpha.
This alpha is chosen in a way that will maximize the step size during optimization.
In the simplified SMO, we calculated the error Ej after choosing j.
This time, we’re going to create a global cache of error values and choose from the alphas that maximize step size, or Ei-Ej.
Before we get into the improvements, we’re going to need to clean up the code from the previous section.
The following listing has a data structure we’ll use to clean up the code and three helper functions for caching the E values.
The first thing you do is create a data structure to hold all of the important values.
You don’t use it for object-oriented programming; it’s used as a data structure in this example.
I moved all the data into a structure to save typing when you pass values into functions.
I could have done this just as easily with a Python dictionary, but that takes more work trying to access member variables; compare myObject.X to myObject['X']
To accomplish this, you create the class optStruct, which only has the init method.
All of these are the same as in the simplified SMO.
The first column is a flag bit stating whether the eCache is valid, and the second column is the actual E value.
The first helper function, calcEk(), calculates an E value for a given alpha and returns the E value.
This was previously done inline, but you must take it out because it occurs more frequently in this version of the SMO algorithm.
The next function, selectJ(), selects the second alpha, or the inner loop alpha C.
Recall that the goal is to choose the second alpha so that we’ll take the maximum step during each optimization.
This function takes the error value associated with the first choice alpha (Ei) and the index i.
You first set the input Ei to valid in the cache.
The NumPy function nonzero() returns a list containing indices of the input list that are—you guessed it—not zero.
The nonzero() statement returns the alphas corresponding to non-zero E values, not the E values.
You loop through all of these values and choose the value that gives you a maximum change D.
If this is your first time through the loop, you randomly select an alpha.
There are more sophisticated ways of handling the first-time case, but this works for our purposes.
This calculates the error and puts it in the cache.
The code in listing 6.3 doesn’t do much on its own.
But when combined with the optimization and the outer loop, it forms the powerful SMO algorithm.
Next, I’ll briefly present the optimization routine, to find our decision boundary.
Open your text editor and add the code from the next listing.
But it has been written to use our data structure.
The second important change is that selectJ() from listing 6.3 is used to select the second alpha rather than selectJrand()B.
Lastly C, you update the Ecache after alpha values change.
The final piece of code that wraps all of this up is shown in the following listing.
This is the outer loop where you select the first alpha.
Open your text editor and add the code from this listing to svmMLiA.py.
The code in listing 6.5 is the full Platt SMO algorithm.
Initially you create the data structure that will be used to hold.
Next, you initialize some variables you’ll use to control when you exit the function.
The majority of the code is in the while loop, similar to smoSimple() but with a few more exit conditions.
You’ll exit from the loop whenever the number of iterations exceeds your specified maximum or you pass through the entire set without changing any alpha pairs.
The maxIter variable has a different use from smoSimple() because in that function you counted an iteration as a pass through the entire set when no alphas were changed.
In this function an iteration is defined as one pass through the loop regardless of what was done.
This method is superior to the counting used in smoSimple() because it will stop if there are any oscillations in the optimization.
The first for loop goes over any alphas in the dataset B.
We call innerL() to choose a second alpha and do optimization if possible.
A 1 will be returned if any pairs get changed.
The second for loop goes over all the non-bound alphas, the values that aren’t bound at 0 or C.
You next toggle the for loop to switch between the non-bound loop and the full pass, and print out the iteration number.
To see this in action, type the following in your Python shell:
You can inspect b and alphas similarly to what you did here.
Was this method faster? On my humble laptop I did this algorithm with the settings listed previously 10 times and took the average.
Compare this to smoSimple()on the same dataset, which took an average of 14.5 seconds.
The results will be even better on larger datasets, and there are many ways to make this even faster.
What happens if you change the tolerance value? How about if you change the value of C? I mentioned briefly at the end of section 6.2 that the constant C gives weight to different parts of the optimization problem.
If C is large, the classifier will try to make all of the examples properly classified by the separating hyperplane.
The results from this optimization run are shown in figure 6.5
If you recall, figure 6.4 was generated by our simplified algorithm, which randomly picked pairs of alphas.
This method worked, but it wasn’t as good as the full version of the algorithm, which covered the entire dataset.
You may also think that the support vectors chosen should always be closest to the separating hyperplane.
Speeding up optimization with the full Platt SMO solution that satisfies the algorithm.
When you have a dataset that isn’t linearly separable, you’ll see the support vectors bunch up closer to the hyperplane.
You might be thinking, “We just spent a lot of time figuring out the alphas, but how do we use this to classify things?” That’s not a problem.
You first need to get the hyperplane from the alphas.
The small function listed here will do that for you:
The most important part of the code is the for loop, which just multiplies some things together.
If you looked at any of the alphas we calculated earlier, remember that most of the alphas are 0s.
This for loop goes over all the pieces of data in our dataset, but only the support vectors matter.
You could just as easily throw out those other data points because they don’t contribute to the w calculations.
To use the function listed previously, type in the following:
Now to classify something, say the first data point, type in this:
Figure 6.5 Support vectors shown after the full SMO algorithm is run on the dataset.
The results are slightly different from those in figure 6.4
Now check to make sure other pieces of data are properly classified:
Compare these results to figure 6.5 to make sure it makes sense.
Now that we can successfully train our classifier, I’d like to point out that the two.
If you look at figure 6.1, you can probably find shapes that would separate the two classes.
What if you want your classes to be inside a circle or outside a circle? We’ll next talk about a way you can change the classifier to account for different shapes of regions separating your data.
This is similar to the data in figure 6.1, frame C.
Earlier, this was used to describe data that isn’t linearly separable.
Clearly there’s some pattern in this data that we can recognize.
Is there a way we can use our powerful tools to capture this pattern in the same way we did for the linear data? Yes, there is.
We’re going to use something called a kernel to transform our data into a form that’s easily understood by our classifier.
This section will explain kernels and how we can use them to support vector machines.
Next, you’ll see one popular type of kernel called the radial bias function, and finally we’ll apply this to our existing classifier.
If we just plugged in our X and Y coordinates, we wouldn’t get good results.
You can probably think of some ways to change the circle data so that instead of X and Y, you’d have some new variables that would be better on the greater-than- or less-than-0 test.
This is an example of transforming the data from one feature space to another so that you can deal with it easily with your existing tools.
Mathematicians like to call this mapping from one feature space to another feature space.
Usually, this mapping goes from a lowerdimensional feature space to a higher-dimensional space.
This mapping from one feature space to another is done by a kernel.
You can think of the kernel as a wrapper or interface for the data to translate it from a difficult formatting to an easier formatting.
Using kernels for more complex data space sounds confusing, you can think of it as another distance metric.
There were many different ways to measure the distance, and the same is true with kernels, as you’ll see soon.
After making the substitution, we can go about solving this linear problem in high-dimensional space, which is equivalent to solving a nonlinear problem in low-dimensional space.
One great thing about the SVM optimization is that all operations can be written in terms of inner products.
Inner products are two vectors multiplied together to yield a scalar or single number.
We can replace the inner products with our kernel functions without making simplifications.
Replacing the inner product with a kernel is known as the kernel trick or kernel substation.
A popular kernel is the radial bias function, which we’ll introduce next.
The radial bias function is a kernel that’s often used with support vector machines.
A radial bias function is a function that takes a vector and outputs a scalar based on the vector’s distance.
This distance can be either from 0,0 or from another vector.
We’ll use the Gaussian version, which can be written as.
Figure 6.6 This data can’t be easily separated with a straight line in two dimensions, but it’s obvious that some pattern exists separating the squares and the circles.
This Gaussian version maps the data from its feature space to a higher feature space, infinite dimensional to be specific, but don’t worry about that for now.
This is a common kernel to use because you don’t have to figure out exactly how your data behaves, and you’ll get good results with this kernel.
In our example we have data that’s basically in a circle; we could have looked over the data and realized we only needed to measure the distance to the origin; however, if we encounter a new dataset that isn’t in that format, then we’re in big trouble.
We’ll get great results with this Gaussian kernel, and we can use it on many other datasets and get low error rates there too.
If you add one function to our svmMLiA.py file and make a few modifications, you’ll be able to use kernels with our existing code.
Also, modify our class, optStruct, so that it looks like the code given in the following listing.
I think it’s best to look at our new version of optStruct.
This has everything the same as the previous optStruct with one new input: kTup.
Using kernels for more complex data contains the information about the kernel.
At the end of the initialization method a matrix K gets created and then populated by calling a function kernelTrans()
Then, when you want to use the kernel, you call it.
When the matrix K is being computed, the function kernelTrans() is called multiple times.
This takes three inputs: two numeric types and a tuple.
The first argument in the tuple is a string describing what type of kernel should be used.
The other arguments are optional arguments that may be needed for a kernel.
The function first creates a column vector and then checks the tuple to see which type of kernel is being evaluated.
Here, only two choices are given, but you can expand this to many more by adding in other elif statements.
In the case of the linear kernel, a dot product is taken between the two inputs, which are the full dataset and a row of the dataset.
In the case of the radial bias function, the Gaussian function is evaluated for every element in the matrix in the for loop.
After the for loop is finished, you apply the calculations over the entire vector.
It’s worth mentioning that in NumPy matrices the division symbol means element-wise rather than taking the inverse of a matrix, as would happen in MATLAB.
Lastly, you raise an exception if you encounter a tuple you don’t recognize.
This is important because you don’t want the program to continue in this case.
Code was changed to use the kernel functions in two preexisting functions: innerL() and calcEk()
But relisting the entire functions would take over 90 lines, and I don’t think anyone would be happy with that.
You can copy the code from the source code download to get these changes without manually adding them.
Now that you see how to apply a kernel during training, let’s see how you’d use it during testing.
The input is the user-defined variable for the Gaussian radial bias function.
The code is mostly a collection of stuff you’ve done before.
Then, you run the Platt SMO algorithm on this, with the option 'rbf' for a kernel.
After the optimization finishes, you make matrix copies of the data to use in matrix math later, and you find the non-zero alphas, which are our support vectors.
You also take the labels corresponding to the support vectors and the alphas.
Those are the only values you’ll need to do classification.
The most important lines in this whole listing are the first two lines in the for loops.
You first use the kernelTrans() function you used in the structure initialization method.
After you get the transformed data, you do a multiplication with the alphas and the labels.
Listing 6.8 Radial bias test function for classifying with a kernel.
Using kernels for more complex data thing to note in these lines is how you use only the data for the support vectors.
The second for loop is a repeat of the first one but with a different dataset—the test dataset.
You now can compare how different settings perform on the test set and the training set.
To test out the code from listing 6.8, enter the following at the Python shell:
The userdefined parameter reduces the influence of each support vector, so you need more support vectors.
The optimization algorithm found it needed these points in order to properly classify the data.
This should give you the intuition that the reach of the radial bias is too small.
You can increase sigma and see how the error rate changes.
I increased sigma and made another plot, shown in figure 6.8
If you watch the output of the function testRbf(), you’ll see that the test error has gone down too.
If you make the sigma smaller, you’ll get a lower training error but a higher testing error.
The beauty of SVMs is that they classify things efficiently.
If you have too few support vectors, you may have a poor decision boundary (this will be demonstrated in the next example)
If you have too many support vectors, you’re using the whole dataset every time you classify something—that’s called k-Nearest Neighbors.
Feel free to play around with other settings in the SMO algorithm or to create new kernels.
We’re now going to put our support vector machines to use with some larger data and compare it with a classifier you saw earlier.
Here we have fewer support vectors than in figure 6.7
The support vectors are bunching up around the decision boundary.
At the time of writing there’s a 10 MB limit on certain applications downloaded over the air.
I’m sure this will be laughable at some point in the future.
We need you to keep the same performance with less memory used.
I told the CEO you’d have this ready in a week.
The k-Nearest Neighbors algorithm used in chapter 2 works well, but you have to carry around all the training examples.
With support vector machines, you can carry around far fewer examples (only your support vectors) and achieve comparable performance.
Using some of the code from chapter 2 and the SMO algorithm, let’s build a system to test a classifier on the handwritten digits.
Train: Run the SMO algorithm with two different kernels and different settings for the radial bias kernel.
Test: Write a function to test the different kernels and calculate the error rate.
Use: A full application of image recognition requires some image processing, which we won’t get into.
The only big difference is that in kNN.py this code directly applied the class label.
Creating a multiclass classifier with SVMs has been studied and compared.
It’s almost the exact same code as testRbf(), except it calls loadImages() to get the class labels and data.
The other small difference is that the kernel tuple kTup is now an input, whereas it was assumed that you were using the rbf kernel in testRbf()
After you’ve entered the code from listing 6.9, save svmMLiA.py and type in the following:
I tried different values for sigma as well as trying the linear kernel and summarized them in table 6.1
This is much larger than our previous example, where our minimum test error was roughly 1.3
Why is there such a huge difference? The data is different.
How can you tell what settings to use? To be honest, I didn’t know when I was writing this example.
The answer is also sensitive to the settings of C.
There are other formulations of the SVM that bring C into the optimization procedure, such as v-SVM.
It’s interesting to note that the minimum training error doesn’t correspond to a minimum number of support vectors.
Also note that the linear kernel doesn’t have terrible performance.
It may be acceptable to trade the linear kernel’s error rate for increased speed of classification, but that depends on your application.
They’re called machines because they generate a binary decision; they’re decision machines.
Support vectors have good generalization error: they do a good job of learning and generalizing on what they’ve learned.
These benefits have made support vector machines popular, and they’re considered by some to be the best stock algorithm in unsupervised learning.
Kernel, settings Training error (%) Test error (%) # Support vectors.
Support vector machines try to maximize margin by solving a quadratic optimization problem.
In the past, complex, slow quadratic solvers were used to train support vector machines.
John Platt introduced the SMO algorithm, which allowed fast training of SVMs by optimizing only two alphas at one time.
We discussed the SMO optimization procedure first in a simplified version.
We sped up the SMO algorithm a lot by using the full Platt version over the simplified version.
There are many further improvements that you could make to speed it up even further.
A commonly cited reference for further speed-up is the paper titled “Improvements to Platt’s SMO Algorithm for SVM Classifier Design.”6
Kernel methods, or the kernel trick, map data (sometimes nonlinear data) from a low-dimensional space to a high-dimensional space.
In a higher dimension, you can solve a linear problem that’s nonlinear in lower-dimensional space.
Kernel methods can be used in other algorithms than just SVM.
The radial-bias function is a popular kernel that measures the distance between two vectors.
Support vector machines are a binary classifier and additional methods can be extended to classification of classes greater than two.
The performance of an SVM is also sensitive to optimization parameters and parameters of the kernel used.
Our next chapter will wrap up our coverage of classification by focusing on something called boosting.
A number of similarities can be drawn between boosting and support vector machines, as you’ll soon see.
If you were going to make an important decision, you’d probably get the advice of multiple experts instead of trusting one person.
Why should the problems you solve with machine learning be any different? This is the idea behind a metaalgorithm.
We’ll focus on one of the most popular meta-algorithms called AdaBoost.
This is a powerful tool to have in your toolbox because AdaBoost is considered by some to be the best-supervised learning algorithm.
In this chapter we’re first going to discuss different ensemble methods of classification.
We’ll next focus on boosting and AdaBoost, an algorithm for boosting.
We’ll then build a decision stump classifier, which is a single-node decision tree.
The AdaBoost algorithm will be applied to our decision stump classifier.
We’ll put our classifier to work on a difficult dataset and see how it quickly outperforms other classification methods.
Finally, before we leave the subject of classification, we’re going to talk about a general problem for all classifiers: classification imbalance.
This occurs when we’re trying to classify items but don’t have an equal number of examples.
Detecting fraudulent credit card use is a good example of this: we may have 1,000 negative examples for every positive example.
How do classifiers work in such a situation? You’ll see that you may need to use alternate metrics to evaluate a classifier’s performance.
This subject isn’t unique to AdaBoost, but because this is the last classification chapter, it’s a good time to discuss it.
Methods that do this are known as ensemble methods or meta-algorithms.
Ensemble methods can take the form of using different algorithms, using the same algorithm with different settings, or assigning different parts of the dataset to different classifiers.
We’ll next talk about two methods that use multiple instances of the same classifier and alter the dataset applied to these classifiers.
Finally, we’ll discuss how to approach AdaBoost with our general framework for approaching machine-learning problems.
Bootstrap aggregating, which is known as bagging, is a technique where the data is taken from the original dataset S times to make S new datasets.
Each dataset is built by randomly selecting an example from the original with replacement.
By “with replacement” I mean that you can select the same example more than once.
This property allows you to have values in the new dataset that are repeated, and some values from the original won’t be present in the new set.
After the S datasets are built, a learning algorithm is applied to each one individually.
When you’d like to classify a new piece of data, you’d apply our S classifiers to the new piece of data and take a majority vote.
AdaBoost Pros: Low generalization error, easy to code, works with most classifiers, no parameters to adjust.
There are more advanced methods of bagging, such as random forests.
We’ll now turn our attention to boosting: an ensemble method similar to bagging.
In boosting and bagging, you always use the same type of classifier.
Each new classifier is trained based on the performance of those already trained.
Boosting makes new classifiers focus on data that was previously misclassified by previous classifiers.
Boosting is different from bagging because the output is calculated from a weighted sum of all classifiers.
The weights aren’t equal as in bagging but are based on how successful the classifier was in the previous iteration.
There are many versions of boosting, but this chapter will focus on the most popular version, called AdaBoost.
We’re now going to discuss some of the theory behind AdaBoost and why it works so well.
An interesting theoretical question is can we take a weak classifier and use multiple instances of it to create a strong classifier? By “weak” I mean the classifier does a better job than randomly guessing but not by much.
That is to say, its error rate is greater than 50% in the two-class case.
The “strong” classifier will have a much lower error rate.
Prepare: It depends on which type of weak learner you’re going to use.
In this chapter, we’ll use decision stumps, which can take any type of data.
You could use any classifier, so any of the classifiers from chapters 2–6 would work.
Train: The majority of the time will be spent here.
The classifier will train the weak learner multiple times over the same dataset.
Use: Like support vector machines, AdaBoost predicts one of two classes.
If you want to use it for classification involving more than two classes, then you’ll need to apply some of the same methods as for support vector machines.
The AdaBoost algorithm can be seen schematically in figure 7.1
After D is calculated, AdaBoost starts on the next iteration.
The AdaBoost algorithm repeats the training and weight-adjusting iterations until the training error is 0 or until the number of weak classifiers reaches a user-defined value.
We’re going to build up to the full AdaBoost algorithm.
But, before we can do that, we need to first write some code to create a weak classifier and to accept weights for the dataset.
Now, we’re going to make a decision stump that makes a decision on one feature only.
It’s a tree with only one split, so it’s a stump.
While we’re building the AdaBoost code, we’re going to first work with a simple dataset to make sure we have everything straight.
You can create a new file called adaboost.py and add the following code:
Try choosing one value on one axis that totally separates the circles from the squares.
This is the famous 45 problem that decision trees are notorious for having difficulty with.
AdaBoost will need to use multiple decision stumps to properly classify this dataset.
By using multiple decision stumps, we’ll be able to build a classifier to completely classify the data.
You can load the dataset and class labels by typing in.
Now that you have the dataset loaded, we can create a few functions to build our decision stump.
The first one will be used to test if any of values are less than or greater than the threshold value we’re testing.
The second, more involved function will loop over a weighted version of the dataset and find the stump that yields the lowest error.
Enter the code from the following listing into adaboost.py and save the file.
Figure 7.2 Simple data used to check the AdaBoost building functions.
It’s not possible to choose one threshold on one axis that separates the squares from the circles.
AdaBoost will need to combine multiple decision stumps to classify this set without error.
The first function, stumpClassify(), performs a threshold comparison to classify data.
You can make this comparison on any feature in the dataset, and you can also switch the inequality from greater than to less than.
The next function, buildStump(), will iterate over all of the possible inputs to stumpClassify() and find the best decision stump for our dataset.
Best here will be with respect to the data weight vector D.
The function starts out by making sure the input data is in the proper format for matrix math.
Then, it creates an empty dictionary called bestStump, which you’ll use to store the classifier information corresponding to the best choice of a decision stump given this weight vector D.
The variable numSteps will be used to iterate over the possible values of the features.
You also initialize the variable minError to positive infinity; this variable is used in finding the minimum possible error later.
The main portion of the code is three nested for loops.
The first one goes over all the features in our dataset.
You’re considering numeric values, and you calculate the minimum and maximum to see how large your step size should be.
It might make sense to set the threshold outside the extremes of your range, so there are two extra steps outside the range.
The last for loop toggles your inequality between greater than and less than.
Inside the nested three for loops, you call stumpClassify() with the dataset and your three loop variables.
You next create the column vector errArr, which contains a 1 for any value in predictedVals that isn’t equal to the actual class in labelMat.
You multiply these errors by the weights in D and sum the results to give you a single number: weightedError.
You’re evaluating your classifier based on the weights D, not on another error measure.
If you want to use another classifier, you’d need to include this calculation to define the best classifier for D.
This line can be commented out later, but it’s helpful in understanding how this function works.
Last, you compare the error to your known minimum error, and if it’s below it, you save this decision stump in your dictionary bestStump.
The dictionary, the error, and the class estimates are all returned to the AdaBoost algorithm.
To see this in action, enter the following in the Python shell:
As buildStump iterates over all of the possible values, you can see the output, and finally you can see the dictionary returned.
Does this dictionary correspond to the lowest possible weighted error? Are there other settings that have this same error?
The decision stump generator that you made is a simplified version of a decision tree.
It’s what you’d call the weak learner, which means a weak classification algorithm.
Now that you’ve built the decision stump–generating code, we’re ready to move on to the full AdaBoost algorithm.
In the next section, we’ll create the AdaBoost code to use multiple weak learners.
In the last section, we built a classifier that could make decisions based on weighted input values.
We now have all we need to implement the full AdaBoost algorithm.
For each iteration: Find the best stump using buildStump() Add the best stump to the stump array Calculate alpha Calculate the new weight vector – D Update the aggregate class estimate If the error rate ==0.0 : break out of the for loop.
To put this function into Python, open adaboost.py and add the code from the following listing.
The AdaBoost algorithm takes the input dataset, the class labels, and one parameter, numIt, which is the number of iterations.
This is the only parameter you specify for the whole AdaBoost algorithm.
But the algorithm reached a total error of 0 after the third iteration and quit, so you didn’t get to see all nine iterations.
Intermediate output from each of the iterations comes from the print statements.
You’ll comment these out later, but for now let’s look at the output to see what’s going on under the hood of the AdaBoost algorithm.
The DS at the end of the function names stands for decision stump.
Decision stumps are the most popular weak learner in AdaBoost.
This function is built for decision stumps, but you could easily modify it for other base classifiers.
You could use any of the algorithms we explored in the first part of this book.
The algorithm will output an array of decision stumps, so you first create a new Python list to store these.
You next get m, the number of data points in your dataset, and create a column vector, D.
On subsequent iterations, the AdaBoost algorithm will increase the weight of the misclassified pieces of data and decrease the weight of the properly classified data.
To meet this requirement, you initialize every element to 1/m.
You also create another column vector, aggClassEst, which gives you the aggregate estimate of the class for every data point.
The first thing that is done in this loop is to build a decision stump with the buildStump() function described earlier.
This function takes D, the weights vector, and returns the stump with the lowest error using D.
The lowest error value is also returned as well as a vector with the estimated classes for this iteration D.
This will tell the total classifier how much to weight the output from this stump.
The statement max(error,1e-16) is there to make sure you don’t have a divide-by-zero error in the case where there’s no error.
The alpha value is added to the bestStump dictionary, and the dictionary is appended to the list.
The next three lines B are used to calculate new weights D for the next iteration.
In the case that you have 0 training error, you want to exit the for loop early.
This is calculated C by keeping a running sum of the estimated class in aggClassEst.
This value is a floating point number, and to get the binary class you use the sign() function.
If the total error is 0, you quit the for loop with the break statement.
In the first iteration, all the D values were equal; then only one value, the first data point, was misclassified.
So, in the next iteration, the D vector puts 0.5 weight on the first data point because it was misclassified previously.
You can see the total class by looking at the sign of aggClassEst.
After the second iteration, you can see that the first data point is correctly classified, but the last data point is now wrong.
The D value now becomes 0.5 for the last element, and the other values in the D vector are much smaller.
Finally, in the third iteration the sign of all the values in aggClassEst matches your class labels and the training error becomes 0, so you can quit.
This array contains three dictionaries, which contain all of the information you’ll need for classification.
You’ve now built a classifier, and the classifier will reduce the training error to 0 if you wish.
How does the test error look? In order to see the test error, you need to write some code for classification.
Once you have your array of weak classifiers and alphas for each classifier, testing is easy.
You’ve already written most of the code in adaBoostTrainDS() in listing 7.2
All you need to do is take the train of weak classifiers from your training function and apply these to an instance.
The result of each weak classifier is weighted by its alpha.
The weighted results from all of these weak classifiers are added together, and you take the sign of the final weighted sum to get your final answer.
The code to do this is given in the next listing.
Add the following code to adaboost.py, and then you can use it to classify data with the classifier array from adaboostTrainDS()
The function in listing 7.3 is adaClassify(), which, as you may have guessed, classifies with a train of weak classifiers.
The function adaClassify() first converts datToClass to a NumPy matrix and gets m, the number of instances in datToClass.
Then it creates aggClassEst, which is a column vector of all 0s.
Next, you look over all of the weak classifiers in classifierArr, and for each of them you get a class estimate from stumpClassify()
At that time you iterated over all of the possible stump values and chose the stump with the lowest weighted error.
This class estimate is multiplied by the alpha value for each stump and added to the total: aggClassEst.
I’ve added a print statement so you can see how aggClassEst evolves with each iteration.
After you’ve added the code from listing 7.3, type the following at the Python shell:
If you don’t have the classifier array, you can enter the following:
You can see that the answer for point [0,0] gets stronger with each iteration.
The answer for both points gets stronger with each iteration.
In the next section we’re going to apply this to a much bigger and harder dataset from the real world.
In chapter 4 we tried to predict whether a horse with colic would live or die by using logistic regression.
Let’s see if we can do better with AdaBoost and the decision stumps.
Before you use the functions from the previous code listings in this chapter, you need to have a way to load data from a file.
The function in listing 7.4 is loadDataSet(), which you’ve seen many times before.
It’s slightly improved this time because you don’t have to specify the number of features in each file.
The function also assumes that the last feature is the class label.
To use it, enter the following in your Python shell after you’ve saved adaboost.py:
Train: We’ll train a series of classifiers on the data using the adaBoostTrainDS() function.
With no randomization, we can have an apples-toapples comparison of the AdaBoost results versus the logistic regression results.
Use: We’ll look at the error rates in this example.
But you could create a website that asks a trainer for the horse’s symptoms and then predicts whether the horse will live or die.
If you remember, in chapter 5 we looked at this dataset with logistic regression.
With AdaBoost we never have an error rate that high, and with only 50 weak learners we achieved high performance.
If you look at the Test Error column in table 7.1, you’ll see that the test error reaches a minimum and then starts to increase.
It has been claimed in literature that for well-behaved datasets the test error for AdaBoost reaches a plateau and won’t increase with more classifiers.
Perhaps this dataset isn’t “well behaved.” It did start off with 30% missing values, and the assumptions made for the missing values were valid for logistic regression but they may not work for a decision tree.
If you went back to our dataset and replaced all the 0s with other values—perhaps averages for a given class—would you have better performance?
AdaBoost and support vector machines are considered by many to be the most powerful algorithms in supervised learning.
You can draw a number of similarities between the two.
You can think of the weak learner in AdaBoost as a kernel in support vector machines.
You can also write the AdaBoost algorithm in terms of maximizing a minimum margin.
The way these margins are calculated is different and can lead to different results, especially with higher dimensions.
In the next section we’re going to leave AdaBoost and talk about a problem common to all classifiers.
Before we leave the subject of classification, there’s a topic that needs to be addressed.
In all six chapters on classification, we assumed that the cost of classifying things is equal.
In chapter 5, for example, we built a system to detect whether a horse with.
Table 7.1 AdaBoost test and training errors for a range of weak classifiers.
Usually AdaBoost reaches a test error plateau, and the error doesn’t increase with more classifiers.
Classification imbalance stomach pain would end up living or dying.
We built the classifier but didn’t talk about what happens after classification.
Let’s say someone brings a horse to us and asks us to predict whether the horse will live or die.
We say die, and rather than delay the inevitable, making the animal suffer and incurring veterinary bills, they have it euthanized.
Perhaps our prediction was wrong, and the horse would have lived.
If we predicted this incorrectly, then an expensive animal would have been destroyed, not to mention that a human was emotionally attached to the animal.
How about spam detection? Is it OK to let a few spam messages arrive in your inbox as long as real email never gets put into the spam folder? What about cancer detection? Is it better to tell someone to go for a second opinion as long as you never let someone with a disease go untreated?
The examples for this abound, and it’s safe to say that in most cases the costs aren’t equal.
In this section, we’ll examine a different method for measuring performance of our classifiers and some graphical techniques for visualizing the performance of different classifiers with respect to this problem.
Then we’ll look at two methods of altering our classification algorithms to take into account the costs of making different decisions.
So far in this book we’ve measured the success of the classification tasks by the error rate.
The error rate was the number of misclassified instances divided by the total number of instances tested.
There’s a tool commonly used in machine learning that gives you a better view of classification errors called a confusion matrix.
A confusion matrix for a three-class problem involving predicting animals found around the house is shown in table 7.2
With a confusion matrix you get a better understanding of the classification errors.
If the off-diagonal elements are all zero, then you have a perfect classifier.
Let’s consider another confusion matrix, this time for the simple two-class problem.
In the two-class problem, if you correctly classify something as positive, it’s called a True Positive, and it’s called a True Negative when you properly classify the negative class.
The other two possible cases (False Negative and False Positive) are labeled in table 7.3
With these definitions we can define some new metrics that are more useful than error rate when detection of one class is more important than another class.
Precision tells us the fraction of records that were positive from the group that the classifier predicted to be positive.
The second term we care about is Recall = TP/(TP+FN)
Recall measures the fraction of positive examples the classifier got right.
Classifiers with a large recall don’t have many positive examples classified incorrectly.
You can easily construct a classifier that achieves a high measure of recall or precision but not both.
If you predicted everything to be in the positive class, you’d have perfect recall but poor precision.
Creating a classifier that maximizes both precision and recall is a challenge.
Another tool used for measuring classification imbalance is the ROC curve.
The ROC curve in figure 7.3 has two lines, a solid one and a dashed one.
The x-axis in figure 7.3 is the number of false positives, and the y-axis is the number of true positives.
The ROC curve shows how the two rates change as the threshold changes.
The leftmost point corresponds to classifying everything as the negative class, and the rightmost point corresponds to classifying everything in the positive class.
The dashed line is the curve you’d get by randomly guessing.
Different classifiers may perform better for different threshold values, and it may make sense to combine them in some way.
You wouldn’t get this type of insight from simply looking at the error rate of a classifier.
Ideally, the best classifier would be in upper left as much as possible.
This would mean that you had a high true positive rate for a low false positive rate.
For example, in spam classification this would mean you catch all the spam and don’t allow any legitimate emails to get put in the spam folder.
One metric to compare different ROC curves is the area under the curve (AUC)
The AUC gives an average value of the classifier’s performance and doesn’t substitute for looking at the curve.
This procedure probably sounds confusing but it will become clear when you look at the code in the following listing.
The code in listing 7.5 takes two inputs; the first is a NumPy array or matrix in a row vector form.
Our classifier and our training functions generate this before they apply it to the sign() function.
You’ll see this function in action in a bit, but let’s discuss the code first.
You first input pyplot and then create a tuple of floats and initialize it to 1.0,1.0
You next calculate the number of positive instances you have, by using array filtering, and set this value to numPosClas.
This will give you the number of steps you’re going to take in the y direction.
The next three lines set up the plot, and then you loop over all the sorted values.
The values were sorted in a NumPy array or matrix, but Python needs a list to iterate over, so you call the tolist() method.
As you’re going through the list, you take a step down in the y direction every time you get a class of 1.0, which decreases the true positive rate.
Similarly, you take a step backward in the x direction (false positive rate) for every other class.
To compute the AUC, you need to add up a bunch of small rectangles.
The width of each of these rectangles will be xStep, so you can add the heights of all the rectangles and multiply the sum of the heights by xStep once to get the total area.
The height sum (ySum) increases every time you move in the x direction.
Once you’ve decided whether you’re going to move in the x or y direction, you draw a small, straight-line segment from the current point to the new point.
Finally, you make the plot look nice and display it by printing the AUC to the terminal.
To see this in action, you’ll need to alter the last line of adaboostTrainDS() to.
You should also see an ROC plot identical to figure 7.3
This is the performance of our AdaBoost classifier with 10 weak learners.
Remember, we had the best performance with 40 weak learners? How does the ROC curve compare? Is the AUC better?
Besides tuning the thresholds of our classifier, there are other approaches you can take to aid with uneven classification costs.
The top table encodes the costs of classification as we’ve been using it up to this point.
Now consider the cost matrix in the bottom frame of table 7.4
Using the second cost matrix, the two types of incorrect classification will have different costs.
Similarly, the two types of correct classification will have different benefits.
If you know these costs when you’re building the classifier, you can select a classifier with the minimum cost.
Another way to tune classifiers is to alter the data used to train the classifier to deal with imbalanced classification tasks.
This is done by either undersampling or oversampling the data.
Oversample means to duplicate examples, whereas undersample means to delete examples.
Either way, you’re altering the data from its original form.
The sampling can be done either randomly or in a predetermined fashion.
Usually there’s a rare case that you’re trying to identify, such as credit card fraud.
As mentioned previously, the rare case is the positive class.
You want to preserve as much information as possible about the rare case, so you should keep all of the examples from the positive class and undersample or discard examples from the negative class.
One drawback of this approach is deciding which negative examples to toss out.
The examples you choose to toss out could carry valuable information that isn’t contained in the remaining examples.
One solution for this is to pick samples to discard that aren’t near the decision boundary.
If you wanted to undersample the legitimate transactions to make the dataset equally balanced, you’d need to throw out 4,950 examples, which may also contain valuable information.
This may seem extreme, so an alternative is to use a hybrid approach of undersampling the negative class and oversampling the positive class.
To oversample the positive class, you could replicate the existing examples or add new points similar to the existing points.
One approach is to add a data point interpolated between existing data points.
Ensemble methods are a way of combining the predictions of multiple classifiers to get a better answer than simply using one classifier.
There are ensemble methods that use different types of classifiers, but we chose to look at methods using only one type of classifier.
Combining multiple classifiers exploits the shortcomings of single classifiers, such as overfitting.
Combining multiple classifiers can help, as long as the classifiers are significantly different from each other.
This difference can be in the algorithm or in the data applied to that algorithm.
The two types of ensemble methods we discussed are bagging and boosting.
In bagging, datasets the same size as the original dataset are built by randomly sampling examples for the dataset with replacement.
Boosting takes the idea of bagging a step further by applying a different classifier sequentially to a dataset.
An additional ensemble method that has shown to be successful is random forests.
Random forests aren’t as popular as AdaBoost, so they aren’t discussed in this book.
We discussed the most popular variant of boosting, called AdaBoost.
AdaBoost uses a weak learner as the base classifier with the input data weighted by a weight vector.
Summary data is weighted more strongly if it was incorrectly classified previously.
This adapting to the errors is the strength of AdaBoost.
We built functions to create a classifier using AdaBoost and the weak learner, decision stumps.
The AdaBoost functions can be applied to any classifier, as long as the classifier can deal with weighted data.
The AdaBoost algorithm is powerful, and it quickly handled datasets that were difficult using other classifiers.
The classification imbalance problem is training a classifier with data that doesn’t have an equal number of positive and negative examples.
The problem also exists when the costs for misclassification are different from positive and negative examples.
We looked at ROC curves as a way to evaluate different classifiers.
We introduced precision and recall as metrics to measure the performance classifiers when classification of one class is more important than classification of the other class.
We introduced oversampling and undersampling as ways to adjust the positive and negative examples in a dataset.
Another, perhaps better, technique was introduced for dealing with classifiers with unbalanced objectives.
This method takes the costs of misclassification into account when training a classifier.
We’ve introduced a number of powerful classification techniques so far in this book.
This is the last chapter on classification, and we’ll move on to regression next to complete our study of supervised learning algorithms.
Regression is much like classification, but instead of predicting a nominal class, we’ll be predicting a continuous value.
Recall that supervised learning is machine learning when we have a target variable, or something we want to predict.
The difference between regression and classification is that in regression our target variable is numeric and continuous.
Chapter 8 covers an introduction to linear regression, locally weighted linear regression, and shrinkage methods.
The previous chapters focused on classification that predicts only nominal values for the target variable.
With the tools in this chapter you’ll be able to start predicting target values that are continuous.
You may be asking yourself, “What can I do with these tools?” “Just about anything” would be my answer.
Companies may use this for boring things such as sales forecasts or forecasting manufacturing defects.
One creative example I’ve seen recently is predicting the probability of celebrity divorce.
In this chapter, we’ll first discuss linear regression, where it comes from, and how to do it in Python.
We’ll next look at a technique for locally smoothing our estimates to better fit the data.
We’ll explore shrinkage and a technique for getting a regression Predicting numeric values: regression.
Finally, we’ll put all of these techniques to use in forecasting the age of abalone and the future selling price of antique toys.
To get the data on the antique toys, we’ll first use Python to do some screen scraping.
Our goal when using regression is to predict a numeric target value.
One way to do this is to write out an equation for the target value with respect to the inputs.
For example, assume you’re trying to forecast the horsepower of your sister’s boyfriend’s automobile.
The process of finding these regression weights is called regression.
Once you’ve found the regression weights, forecasting new values given a set of inputs is easy.
All you have to do is multiply the inputs by the regression weights and add them together to get a forecast.
When we talk about regression, we often mean linear regression, so the terms regression and linear regression are used interchangeably in this chapter.
Linear regression means you can add up the inputs multiplied by some constants to get the output.
There’s another type of regression called nonlinear regression in which this isn’t true; the output may be a function of the inputs multiplied together.
Also, we can visualize the regression weights if we apply shrinkage methods.
How can we go from a bunch of data to our regression equation? Our input data is in the matrix X, and our regression weights in the vector w.
We have the Xs and ys, but how can we find the ws? One way is to find the ws that minimize the error.
We define error as the difference between predicted y and the actual y.
Using just the error will allow positive and negative values to cancel out, so we use the squared error.
We can also write this in matrix notation as (y-Xw)T(y-Xw)
If we take the derivative of this with respect to w, we’ll get XT(y-Xw)
We can set this to zero and solve for w to get the following equation:
The little symbol on top of the w tells us that this is the best solution we can come up with for w at the moment.
The value we have for w is based on the data we have and may not perfectly describe the data, so we use a “hat” to describe our best estimate given the data.
Test: We can measure the R2, or correlation of the predicted value and data, to measure the success of our models.
Use: With regression, we can forecast a numeric value for a number of inputs.
This is an improvement over classification because we’re predicting a continuous value rather than a discrete category.
The origins of regression What we know today as regression was invented by the cousin of Charles Darwin, Francis Galton.
Galton did his first regression in 1877 to estimate the size of pea seeds based on the size of their parents’ seeds.
Galton performed regression on a number of things, including the heights of humans.
He noticed that if parents were above average in height, their children also tended to be above average but not as much as their parents.
The heights of children were regressing toward a mean value.
Galton noticed this behavior in a number of things he studied, and so the technique is called regression, despite the English word having no relationship to predicting numeric values.†
Something else to note about the equation is that it uses XTX-1, which is a matrix inverse.
The matrix inverse may not exist, and we’ll need to check for this when putting this into code.
Solving this problem is one of the most common applications of statistics, and there are a number of ways to do it other than the matrix method.
By using the matrix method with NumPy, we can write a few lines and get an answer.
This method is also known as OLS, which stands for “ordinary least squares.”
To see this in action, look at the plot in figure 8.1
We’d like to see how to create a best-fit line for this data.
The code in the following listing will allow you to create a best-fit line for the data in figure 8.1
Open a text editor and create a new file called regression.py, and then add the following code.
This function opens a text file with tab-delimited values and assumes the last value is the target value.
The second function, standRegres(), is the function that computes the best-fit line.
You first load the x and y arrays and then convert them into matrices.
Next you compute XTX and then test if its determinate is zero.
If the determinate is zero, then you’ll get an error when you try to compute the inverse.
NumPy has a linear algebra library called linalg, which has a number of useful functions; you can call linalg.det() to compute the determinate.
If the determinate is nonzero, you compute the ws and return them.
If you didn’t check to see if the determinate was zero before attempting to compute the inverse, you’d get an error.
Using loadDataSet(), you can import the data into two arrays, one for the X values and one for Y values.
The Y values are our target values similar to our class labels in all of the classification algorithms.
The second value, X1, is our value in the plot.
The variable ws is now our weights, which we multiply by our constant tern, and the second one we multiply by our input variable X1
We also want to call this predicted y something other than the actual data, so this is called yHat.
Now we can plot this to see a plot of our data and our best-fit line:
To plot the best-fit line we’ve calculated, we need to plot yHat.
Pyplot will have a problem if the points on our line are out of order, so we first sort the points in ascending order:
You should see a plot similar to the one in figure 8.2
You can make a model of almost any dataset, but how good is the model? Consider.
If you do a linear regression on both of the plots, you’ll get the exact same results.
The plots are obviously not the same, but how can you measure the difference? One way you can calculate how well the predicted value, yHat, matches our actual data, y, is with the correlation between the two series.
In Python the NumPy library comes with a command to generate the correlation coefficients.
Let’s try this out on the data points from the previous example.
First, you need to get an estimate, as we did at the beginning of the example:
You need to transpose yMat so that you have both of the vectors as row vectors:
This gives you the correlation between all possible pairs; elements on the diagonal are 1.0 because the correlation between yMat and yMat is perfect.
The best-fit line does a great job of modeling the data as if it were a straight line.
But it looks like the data has some other patterns we may want to take advantage of.
How can we take advantage of these patterns? One way is to locally adjust our forecast based on the data.
One problem with linear regression is that it tends to underfit the data.
It gives us the lowest mean-squared error for unbiased estimators.
With the model underfit, we aren’t getting the best predictions.
There are a number of ways to reduce this meansquared error by adding some bias into our estimator.
One way to reduce the mean-squared error is a technique known as locally weighted linear regression (LWLR)
In LWLR we give a weight to data points near our data point of interest; then we compute a least-squares regression similar to section 8.1
This type of regression uses the dataset each time a calculation is needed, similar to kNN.
This builds the weight matrix W, which has only diagonal elements.
The closer the data point x is to the other points, the larger w(i,i) will be.
There also is a user-defined constant k that will determine how much to weight nearby points.
This is the only parameter that we have to worry about with LWLR.
You can see how different values of k change the weights matrix in figure 8.4
To see this in action, open your text editor and add the code from the following listing to regression.py.
The code in listing 8.2 is used to generate a yHat estimate for any point in the x space.
The function lwlr() creates matrices from the input data similar to the code in listing 8.1; then it creates a diagonal weights matrix called weights.
The function next iterates over all of the data points and computes a value, which decays exponentially as you move away from the testPoint.
After you’ve populated the weights matrix, you can find an estimate for testPoint similar to standRegres()
The other function in listing 8.2 is lwlrTest(), which will call lwlr() for every point in the dataset.
After you’ve entered the code from listing 8.2 into regression.py, save it and type the following in the Python shell:
If you need to reload the dataset, you can type in.
To get an estimate for all the points in our dataset, you can use lwlrTest():
You can inspect yHat, so now let’s plot these estimates with the original values.
Plot needs the data to be sorted, so let’s sort xArr:
You should see something similar to the plot in the bottom frame of figure 8.5
Figure 8.5 has plots for three different values of k.
With k=1.0, the weights are so large that they appear to weight all the data equally, and you have the same best-fit line as using standard regression.
Using k=0.01 does a much better job of capturing the underlying pattern in the data.
This is too noisy and fits the line closely to the data.
The bottom panel is an example of overfitting, whereas the top panel is an example of underfitting.
You’ll see how to quantitatively measure overfitting and underfitting in the next section.
Figure 8.5 Plot showing locally weighted linear regression with three smoothing values.
The top value of k is no better than least squares.
The middle value captures some of the underlying data pattern.
The bottom frame fits the best-fit line to noise in the data and results in overfitting.
One problem with locally weighted linear regression is that it involves numerous computations.
You have to use the entire dataset to make one estimate.
You could save a lot of computing time by avoiding these calculations.
Now that you’ve seen two methods of finding best-fit lines, let’s put it to use predicting the age of an abalone.
Let’s see our regression example in action on some real live data.
In the data folder there is some data from the UCI data repository describing the age of a shellfish called abalone.
The year is known by counting the number of layers in the shell of the abalone.
The function rssError() will give us a single number describing the error of our estimate:
Using a smaller kernel will give us a lower error, so why don’t we use the smallest kernel all the time? Using the smallest kernel will overfit our data.
This may or may not give us the best results on new data.
Let’s see how well these predictions work on new data:
The kernel size of 10 gave us the largest training error.
Simple linear regression did almost as well as the locally weighted linear regression.
This demonstration illustrates one fact, and that is that in order to choose the best model you have to see how the model does on unknown data.
This example showed how one method—locally weighted linear regression—can be used to build a model that may be better at forecasting than regular regression.
The problem with locally weighted linear regression is that you need to “carry around” the dataset.
You need to have the training data available to make predictions.
We’ll now explore a second class of methods for improving forecasting accuracy.
These methods have some added benefits, as you’ll soon see.
What if we have more features than data points? Can we still make a prediction using linear regression and the methods we’ve seen already? Then answer is no, not using the methods we’ve seen already.
The reason for this is that when we try to compute (XTX)-1 we’ll get an error.
If we have more features than data points (n>m), we say that our data matrix X isn’t full rank.
When the data isn’t full rank, we’ll have a difficult time computing the inverse.
To solve this problem, statisticians introduced the concept of ridge regression, which is the first of two shrinkage methods we’ll look at in this section.
We’ll then discuss the lasso, which is better but difficult to compute.
We’ll finally examine a second shrinkage method called forward stagewise regression, which is an easy way to approximate the lasso.
Shrinkage methods allow us to throw out unimportant parameters so that we can get a better feel and human understanding of the data.
Additionally, shrinkage can give us a better prediction value than linear regression.
First, open regression.py and add the code from the following listing.
The code in listing 8.3 contains two functions: one to calculate weights, ridgeRegres(), and one to test this over a number of lambda values, ridgeTest()
The first function, ridgeRegres(), implements ridge regression for any given value of lambda.
Lambda is a reserved keyword in Python, so you use the variable lam instead.
Next, you add on the ridge term multiplied by our scalar lam.
The identity matrix is created by the NumPy function eye()
If the matrix isn’t singular, the last thing the code does is calculate the weights and return them.
To use ridge regression and all shrinkage methods, you need to first normalize your features.
If you read chapter 2, you’ll remember that we normalized our data to give each feature equal importance regardless of the units it was measured in.
The second function in listing 8.3, ridgeTest(), shows an example of how to normalize the data.
This is done by subtracting off the mean from each feature and dividing by the variance.
After the regularization is done, you call ridgeRegres() with 30 different lambda values.
The values vary exponentially so that you can see how very small values of lambda and very large values impact your results.
We now have the weights for 30 different values of lambda.
To plot them out, enter the following commands in your Python shell:
Shrinking coefficients to understand our data the full values of our coefficients, which are the same as linear regression.
Somewhere in the middle, you have some coefficient values that will give you better prediction results.
To find satisfactory answers, you’d need to do cross-validation testing.
A plot, shown in figure 8.6, also tells you which variables are most descriptive in predicting your output, by the magnitude of these coefficients.
There are other shrinkage methods such as the lasso, LAR, PCA regression,1 and subset selection.
These methods can be used to improve prediction accuracy and improve your ability to interpret regression coefficients similarly to ridge regression.
It can be shown that the equation for ridge regression is the same as our regular leastsquares regression and imposing the following constraint:
Similar to ridge regression, there’s another shrinkage technique called the lasso.
There’s an easier algorithm than the lasso that gives close results: stagewise linear regression.
This algorithm is a greedy algorithm in that at each step it makes the decision that will reduce the error the most at that step.
The decision that’s made at each step is increasing or decreasing a weight by some small amount.
To see this in action, open regression.py and add the code from the following listing.
The function stageWise() in listing 8.4 is a demonstration of the stagewise linear regression algorithm, which approaches the lasso solution but is much easier to compute.
The function takes the following inputs: our input data, xArr; and the variable we’re forecasting, yArr.
One is eps, the step size to take at each iteration, and the second is numIt, which is the number of iterations.
You start off by converting the input data into matrices and normalizing the features to 0 mean and unit variance.
You next make a vector, ws, to hold our w values, and you create two copies for use in the greedy optimization.
Shrinking coefficients to understand our data optimization procedure numIt times.
In each of these iterations you print out the w vector so you have some idea what’s going on inside.
After you’ve entered the code from listing 8.4 into regression.py, save it and type the following in your Python shell:
With the eps variable set to 0.01, after some time the coefficients will all saturate and oscillate between certain values because the step size is too large.
Let’s try again with a smaller step size and many more steps:
You can get the least-squares weights by typing in the following:
You can see after 5,000 iterations that the results from the stagewise linear regression algorithm are close to the results using regular least squares.
The practical benefit of the stagewise linear regression algorithm isn’t that you can make these cool plots like the one in figure 8.7
The benefit is this algorithm allows you to better understand your models and build better models.
When building a model, you’d want to run this algorithm and find out which features are important.
You may choose to stop collecting data for unimportant features.
Ultimately, you’d want to build many models with w values from the algorithm and after every 100 iterations test these.
To test these models, you’d do something like tenfold cross validation and choose the model that minimizes error.
When we apply a shrinkage method such as stagewise linear regression or ridge regression, we say we’re adding bias to our model.
The next section will explain the relationship and how these affect our results.
Anytime you have a difference between your model and your measurements, you have an error.
When thinking about “noise” or error in our model, you have to consider the sources.
This will create socalled noise or errors between your model and your measurements so that you won’t be able to understand the true process that’s generating your data.
Figure 8.7 Coefficient values from the abalone dataset versus iteration of the stagewise linear regression algorithm.
Stagewise linear regression gives values close to the lasso values with a much simpler algorithm.
There could also be noise or problems with your measurement process.
We were trying to model this with a straight line.
That structure was hard to understand, so we used different amounts of local weights to find a solution that gave us the smallest test error.
A plot of training and test error is shown in figure 8.8
The top curve is the test error, and the bottom curve is training error.
If you remember from section 8.3, as we decreased the size of the kernel, our training error got smaller.
This corresponds to starting on the left side of figure 8.8 and then moving to the right as the kernel becomes smaller.
Figure 8.8 The bias variance tradeoff illustrated with test error and training error.
The training error is the top curve, which has a minimum in the middle of the plot.
In order to create the best forecasts, we should adjust our model complexity where the test error is at a minimum.
It’s popular to think of our errors as a sum of three components: bias, error, and random noise.
In section 8.4, when we applied our shrinkage methods, some of the coefficients became small and some became zero.
This is an example of adding bias to our model.
By shrinking some of our components to exactly zero, we were reducing the complexity of our model.
When we eliminated two of them, our model was easier for a human to understand, but it could give us smaller prediction error also.
The left side of figure 8.8 shows our coefficients taking on very small values.
The right side of the figure shows our coefficients totally unconstrained.
If we took a random sample of our abalone data, say, 100 points, and generated a linear model, we’d have a set of weights.
Say we took another set of random points and generated another linear model.
Comparing the amount that the weights change will tell us the variance in our model.
This concept of a bias and variance tradeoff is popular in machine learning and will come up again and again.
Now let’s see if we can put some of these ideas to use.
We’ll next examine some data from a real-world auction site and experiment with some regression methods.
You’ll see the bias/variance trade-off in action as we find the best ridge regression model for our data.
Are you familiar with the LEGO brand of toys? If not, LEGO makes construction toys that are composed of many small plastic blocks of varying size.
Because of the high quality of the parts, the blocks hold together without any adhesives.
Beyond being a simple toy, LEGO sets are popular with many adults.
Usually the blocks are sold as sets, which include all the pieces to make something specific such as a boat, castle, or famous building.
A LEGO set will typically be available for a few years and then will be discontinued.
After the sets are discontinued, they continue to be traded by collectors.
My friend Dangler would like to predict how much LEGO sets will sell for.
We’ll help him by building a model using the regression techniques in this chapter.
Train: We’ll build different models with stagewise linear regression and straightforward linear regression.
Test: We’ll use cross validation to test the different models to see which one performs the best.
Use: The resulting model will be the object of this exercise.
In this example we’ll get some data on the prices for different datasets.
The first thing we need to figure out is how we can get the data.
The wonderful people at Google have provided us with an API for retrieving prices via the Search API for Shopping.
Before you can use the API, you’ll need to sign up for a Google account and then visit the Google API console to enable this Shopping API.
Now you can make HTTP requests and get back information about available products in JSON form.
Python comes with a module for parsing JSON, so all you have to do is sort through the returned JSON for the information you want.
Open regression.py and add the code from the following listing to implement code to retrieve this information.
The first function in listing 8.5 is searchForSet(), which will call the Google Shopping API and extract the correct data.
You need to import a few modules: time.sleep(), json, and urllib2
But first you sleep for 10 seconds, which is just a precaution to prevent making too many API calls too quickly.
Next, you format the search URL with your API key and the set you’re looking for.
All you have to do is find the price and the condition.
The final function in listing 8.5 is setDataCollect(), which calls searchForSet() multiple times.
After you’ve added the code from listing 8.5, save regression.py and enter the following commands in your Python shell:
Inspect lgX and lgY to make sure they’re not empty.
We’re next going to use the data to build a regression equation that will forecast the selling price of antique LEGO sets.
Now that we’ve collected some real data from the internet, we’d like to use it to build a model.
It can also be used to give us a better understanding of the forces driving the data.
To do this, you create a matrix of all 1s:
Next, copy over our data to the first through fifth columns:
Check to make sure that data was copied over correctly:
The model says the price of a set will be.
The predictions were pretty good, but the model isn’t satisfactory.
It may fit the data, but it doesn’t seem to make sense.
It seems that sets with more pieces will sell for less, and there’s a penalty for a set being new.
Let’s try this again but with one of our shrinkage methods, say ridge regression.
Earlier you saw how to shrink coefficients, but now you’ll see how to determine the best coefficients.
The function crossValidation() in listing 8.6 takes three arguments; the first two, lgX and lgY, are assumed to be lists of the X and Y values of a dataset in question.
The third argument, crossValidation(), takes the number of cross validations to run.
Both lgX and lgY are assumed to have the same length.
The crossValidation() function starts by measuring the number of data points, m.
This will be used to split the data into two sets: one test set and one training set.
You first create containers for the training and test sets.
You’ll use this to randomly select a set of data points for the training or test set.
To see all of this in action, enter the code from listing 8.6, save regression.py, and type in the following:
What you were looking for was a model that was easier to interpret.
In order to get that, you need to look at how the regularized coefficients change as you apply the shrinkage.
These are the regularized coefficients for different levels of shrinkage.
These results tell you that if you had to choose one feature to predict the future, you should choose the fourth feature, which is the original price.
If you had to choose two features, you should choose the fourth and the second terms.
This sort of analysis allows you to digest a large amount of data.
Regression is the process of predicting a target value similar to classification.
The difference between regression and classification is that the variable forecasted in regression is continuous, whereas it’s discrete in classification.
Regression is one of the most useful tools in statistics.
Minimizing the sum-of-squares error is used to find the best weights for the input features in a regression equation.
Regression can be done on any set of data provided that for an input matrix X, you can compute the inverse of XTX.
Just because you can compute a regression equation for a set of data doesn’t mean that the results are very good.
One test of how “good” or significant the results are is the correlation between the predicted values yHat and the original data y.
When you have more features than data points, you can’t compute the inverse of XTX.
Ridge regression is a regression method that allows you to compute regression coefficients despite being unable to compute the inverse of XTX.
Shrinkage methods impose a constraint on the size of the regression coefficients.
The lasso is difficult to compute, but stagewise linear regression is easy to compute and gives results close to those of the lasso.
Shrinkage methods can also be viewed as adding bias to a model and reducing the variance.
The bias/variance tradeoff is a powerful concept in understanding how altering a model impacts the success of a model.
Sometimes our data will have complex interactions, perhaps nonlinear interactions that will be difficult to model with linear models.
The next chapter explores a few techniques that use trees to create forecasts for our data.
When the data has many features that interact in complicated ways, building a global model can be difficult if not foolish.
How can we expect to model everything with a global linear model?
One way to build a model for our data is to subdivide the data into sections that can be modeled easily.
If we first partition the data and the results don’t fit a linear model, then we can partition the partitions.
Trees and recursion are useful tools for this sort of portioning.
We’ll first examine a new algorithm for building trees, called CART.
It can be applied to regression or classification, so this is a valuable tool to learn.
The linear regression methods we looked at in chapter 8 contain some powerful methods.
These methods create a model that needs to work for all of the data.
We’ll make the code flexible enough that it can be used for multiple problems.
We’ll next apply the CART algorithm to create regression trees.
We’ll explore a technique called tree pruning, which helps to prevent overfitting our trees to our data.
Next, we’ll explore a more advanced algorithm called model trees.
In a model tree, we build a linear model at each leaf node instead of using mean values as in regression trees.
The algorithms to build these trees have a few adjustable parameters, so we’ll next see how to create a GUI in Python with the Tkinter module.
Finally, we’ll use this GUI to explore the impact of various tree-building parameters.
Decision trees work by successively splitting the data into smaller segments until all of the target variables are the same or until the dataset can no longer be split.
Decision trees are a type of greedy algorithm that makes the best choice at a given time without concern for global optimality.
ID3 chooses the best feature on which to split the data and then splits the data into all possible values that the feature can take.
If a feature can take on four possible values, then there will be a four-way split.
After the data is split on a given feature, that feature is consumed or removed from future splitting opportunities.
There is some argument that this type of splitting separates the data too quickly.
Another way to split the data is to do binary splits.
If a piece of data has a feature equal to the desired split value, then it will go down the left side of the tree; otherwise, it will go down the right.
The ID3 algorithm had another limitation: it couldn’t directly handle continuous features.
Continuous features can be handled in ID3 if they’re first made into discrete features.
This quantization destroys some of the inherent information in a continuous variable.
Using binary splits allows us to easily adapt our tree-building algorithm to handle continuous features.
To handle continuous variables, we choose a feature; values greater than the desired value go on the left side of the tree and all the other values go on the right side.
Binary splits also save time during the tree construction, but this is a moot point because we usually build the tree offline and time isn’t a huge concern.
In chapter 3, we used the Shannon entropy as our measure of how.
Building trees with continuous and discrete features unorganized the sets were.
If we replace the Shannon entropy with some other measure, we can use a tree-building algorithm for regression.
We’ll first build the CART algorithm with regression trees in mind.
Regression trees are similar to trees used for classification but with the leaves representing a numeric value rather than a discrete one.
With an idea of how to approach the problem, we can start writing some code.
In the next section we’ll discuss the best way to build a tree with the CART algorithm in Python.
As we build a tree, we’ll need to have a way of storing the different types of data making up the tree.
In chapter 3, we had a dictionary to store every split.
In the CART algorithm, only binary splits are allowed, so we can fix our tree data structure.
The tree will have a right key and a left key that will store either another branch or a value.
The dictionary will also have two more keys: feature and value.
These will tell us what feature of our data to split on and the value of that feature to make the split.
You could also create this data structure using object-oriented programming patterns.
You’d create the tree node in Python using the following code:
If you have nominal values, it’s a good idea to map them into binary values.
Analyze: We’ll visualize the data in two-dimensional plots and generate trees as dictionaries.
Train: The majority of the time will be spent building trees with models at the leaf nodes.
Test: We’ll use the R2 value with test data to determine the quality of our models.
When working with a less-flexible language like C++, you’d probably want to implement your trees using object-oriented patterns.
Python is flexible enough that you can use dictionaries as your tree data structure and write less code than if you used a specific class.
Python isn’t strongly typed, so our branches can contain other trees, numeric values, or vectors, as you’ll see later.
The first type is called a regression tree, and it contains a single value for each leaf of the tree.
The second type of tree we’ll create is called a model tree, and it has a linear equation at each leaf node.
We’ll try to reuse as much code as possible when creating the two types of trees.
Let’s start by making some tree-building code that can be used for either type of tree.
Find the best feature to split on: If we can’t split the data, this node becomes a leaf node Make a binary split of the data Call createTree() on the right split of the data Call createTree() on the left split of the data.
Open your favorite text editor and create a file called regTrees.py; then add the following code.
The first one, loadDataSet(), is similar to the previous versions of this function from other chapters.
In previous chapters you broke the target variable off into its own list, but here you’ll keep the data together.
This function takes a file with tab-delimited values and breaks each line into a list of floats.
This takes three arguments: a dataset, a feature on which to split, and a value for that feature.
The two sets are created using array filtering for the given feature and value.
The last function in listing 9.1 is createTree(), which builds a tree.
There are four arguments to createTree(): a dataset on which to build the tree and three optional arguments.
The three optional arguments tell the function which type of tree to create.
The argument leafType is the function used to create a leaf.
The argument errType is a function used for measuring the error on the dataset.
The last argument, ops, is a tuple of parameters for creating a tree.
The function createTree() is a recursive function that first attempts to split the dataset into two parts.
The split is determined by the function chooseBestSplit(), which we haven’t written yet.
If chooseBestSplit() hits a stopping condition, it will return None and the value for a model type C.
In the case of regression trees, this model is a constant value; in the case of model trees, this model is a linear equation.
If a stopping condition isn’t hit, then you create a new dictionary and split the dataset into two portions.
The function createTree() gets recursively called on the two splits.
You can’t see createTree() in action until we write chooseBestSplit(), but you can test out the other two functions.
After you’ve entered the code from listing 9.1 into regTrees.py, save it and type in the following:
Now let’s split it by the value in a given column.
Entertaining isn’t it? To see this do some more exciting stuff, let’s fill out the chooseBestSplit() function for the case of regression trees.
By filling out chooseBestSplit() with code specific to regression, we’ll be able to use the CART code from listing 9.1 to build regression trees.
In the next section, we’ll finish this function and build regression trees.
In order to model the complex interactions of our data, we’ve decided to use trees to partition the data.
How should we split up the partitions? How will we know when we’ve split up the data enough? The answer depends on how we’re modeling the final values.
The regression tree method breaks up data using a tree with constant values on the leaf nodes.
This strategy assumes that the complex interactions of the data can be summarized by the tree.
In order to construct a tree of piecewise constant values, we need to be able to measure the consistency of data.
In chapter 3, when we used trees for classification, we measured the disorder of the values at a given node.
How can we measure the disorder of continuous values? Measuring this disorder for a set of data is quite easy.
We first calculate the mean value of a set and then find how much each piece of data deviates from this mean value.
In order to treat positive and negative deviations equally, we need to get the magnitude of the deviation from the mean.
We can get this magnitude with the absolute value or the squared value.
I’ve described something commonly done in statistics, and that’s calculating the variance.
The only difference is the variance is the mean squared error and we want the total error.
We can get this total squared error by multiplying the variance of a dataset by the number of elements in a dataset.
With this error rule and the tree-building algorithm from the previous section, we can now write code to construct a regression tree from a dataset.
In order to build a regression tree, we need to create a few pieces of code to get createTree() from listing 9.1 to work.
The first thing we need is a function, chooseBestSplit(), that given an error metric will find the best binary split for our data.
The function chooseBestSplit() also needs to know when to stop splitting given our error metric and a dataset.
When chooseBestSplit() does decide to stop splitting, we need to generate a leaf node.
If you noticed in listing 9.1, chooseBestSplit() has three arguments in addition to the data set.
The leafType argument is a reference to a function that we use to create the leaf node.
The errType argument is a reference to a function that will be used to calculate the squared deviation from the mean described earlier.
Finally, ops is a tuple of user-defined parameters to help with tree building.
The function chooseBestSplit() is the most involved; this function finds the best place to split the dataset.
It looks over every feature and every value to find the threshold that minimizes error.
To create the code for these three functions, open regTrees.py and enter the code from the following listing.
The first function in listing 9.2 is regLeaf(), which generates the model for a leaf node.
When chooseBestSplit() decides that you no longer should split the data, it will call regLeaf() to get a model for the leaf.
The model in a regression tree is the mean value of the target variables.
The second function in listing 9.2 is our error estimate, regErr()
This function returns the squared error of the target variables in a given dataset.
You could have first calculated the mean, then calculated the deviation, and then squared it, but it’s easier.
You want the total squared error, not the mean, so you can get it by multiplying by the number of instances in a dataset.
The function chooseBestSplit() starts out by assigning the values of ops to tolS and tolN.
These two values are user-defined settings that tell the function when to quit creating new splits.
The variable tolS is a tolerance on the error reduction, and tolN is the minimum data instances to include in a split.
The next thing chooseBestSplit() does is check the number of unique values by creating a set from all the target variables.
If this set is length 1, then you don’t need to try to split the set and you can return.
This error S will be checked against new values of the error to see if splitting reduces the error.
A few variables that will be used to find the best split are created and initialized.
You next iterate over all the possible features and all the possible values of those features to find the best split.
The best split is determined by the lowest error of the sets after the split.
If splitting the dataset improves the error by only a small amount, you choose not to split and create a leaf node.
If this is less than our user-defined parameter tolN, you choose not to split and return a leaf node.
Finally, if none of these early exit conditions have been met, you return the feature on which to split and the value to perform the split.
After you’ve entered the code from listing 9.2 into regTrees.py, save it and enter the following commands in your Python shell:
Now let’s try this out on some data with more splits.
Figure 9.1 Simple dataset for evaluating regression trees with the CART algorithm.
To build a tree from this data, enter the following commands in your Python shell:
Check the tree data structure to make sure that there are five leaf nodes.
Try out the regression trees on some more complex data and see what happens.
Now that we’re able to build regression trees, we need to find a way to check if we’ve been doing something wrong.
We’ll next examine tree pruning, which modifies our decision trees so that we can make better predictions.
Trees with too many nodes are an example of a model overfit to the data.
How do we know when we’re overfitting? In the previous chapters we used some form of crossvalidation with a test set to find out when we’re overfitting.
This section will talk about ways around this and ways to combat overfitting our data.
The procedure of reducing the complexity of a decision tree to avoid overfitting is known as pruning.
By using the early stopping conditions in chooseBestSplit(), you were employing prepruning.
Another form of pruning involves a test set and a training set.
This is known as postpruning, and we’ll investigate its effectiveness in this section, but first let’s discuss some of the drawbacks of prepruning.
The results for these two simple experiments in the previous section were satisfactory, but there was something going on behind the scenes.
The trees built are sensitive to the settings we used for tolS and tolN.
For other values of our data we may not get such a nice answer as we did in the last section.
To see what I mean, enter the following command in your Python shell:
This creates a much bigger tree than the two-leaf tree in the previous section.
It creates a leaf node for every instance in the dataset.
This plot looks similar to the one in figure 9.1
The data is stored in a text file called ex2.txt.
To create a tree, enter the following commands in your Python shell.
What do you notice? The tree we built from figure 9.1 had only two leaf nodes.
The problem is that one of our stopping conditions, tolS, is sensitive to the magnitude of the errors.
If we mess around with the options and square the error tolerance, perhaps we can get a tree with two leaves:
We shouldn’t have to mess around with the stopping conditions to give us the tree we’re looking for.
In fact, we’re often not sure what we’re looking for.
The machine is supposed to tell us the big picture.
The next section will discuss postpruning, which uses a test set to prune the tree.
This is a more idealistic method of pruning because it doesn’t use any user-defined parameters.
The method we’ll use will first split our data into a test set and a training set.
First, you’ll build the tree with the setting that will give you the largest, most complex tree you can handle.
You’ll next descend the tree until you reach a node with only leaves.
You’ll test the leaves against data from a test set and measure if merging the leaves would give you less error on the test set.
If merging the nodes will reduce the error on the test set, you’ll merge the nodes.
Split the test data for the given tree: If the either split is a tree: call prune on that split Calculate the error associated with merging two leaf nodes Calculate the error without merging If merging results in lower error then merge the leaf nodes.
To see this in action, open regTrees.py and enter the code from the following listing.
The function isTree() tests if a variable is a tree.
You can use this to find out when you’ve found a branch with only leaf nodes.
The function getMean() is a recursive function that descends a tree until it hits only leaf nodes.
When it finds two leaf nodes, it takes the average of these two nodes.
The main function in listing 9.3 is prune(), which takes two inputs: a tree to prune and testData to use for pruning the tree.
The first thing you do in prune() is check to see if the test data is empty B.
The function prune() gets called recursively and splits the data based on the tree.
Our tree is generated with a different set of data from our test data, and there will be instances where the test data doesn’t contain values in the same range as the original dataset.
In this case, what should you do? Is the data overfit, in which case it would get pruned, or is the model correct and no pruning would be done? We’ll assume it’s overfit and prune the tree.
Next, you test to see if either branch is a tree.
If so, you attempt to prune it by calling prune on that branch.
After you’ve attempted to prune the left and right branches, you test to see if they’re still trees.
If the two branches aren’t trees, then they can be merged.
If the error from merging the two branches is less than the error from not merging, you merge the branches.
If there’s no measurable benefit to merging, you return the original tree.
After you’ve entered the code from listing 9.3 into regTrees.py, save it and enter the following commands in your Python shell:
To create the largest possible tree, type in the following:
A large number of nodes were pruned off the tree, but it wasn’t reduced to two nodes as we had hoped.
It turns out that postpruning isn’t as effective as prepruning.
You can employ both to give the best possible model.
In the next section we’ll reuse a lot of the tree-building code to create a new type of tree.
This tree will still have binary splits, but the leaf nodes will contain linear models of the data instead of constant values.
An alternative to modeling the data as a simple constant value at each leaf node is to model it as a piecewise linear model at each leaf node.
Piecewise linear means that you have a model that consists of multiple linear segments.
If you aren’t clear, you’ll see what it means in a second.
Consider for a moment the data plotted in figure 9.4
Do you think it would be better to model this dataset as a bunch of constant values or as two straight lines? I say two straight lines.
One of the advantages of decision trees over other machine learning algorithms is that humans can understand the results.
Two straight lines are easier to interpret than a big tree of constant values.
The interpretability of model trees is one reason why you’d choose them over regression trees.
With a simple change, we can use the functions written earlier to generate linear models at the leaf nodes instead of constant values.
We’ll use the tree-generating algorithm to break up the data into segments that can easily be represented by a linear model.
The most important part of the algorithm is the error measurement.
But we need to write some code to determine the error of a proposed split.
Do you remember when we were writing createTree() there were two arguments that we never changed? We made those arguments instead of hard coding them for regression trees so that we could change those arguments for a model tree and reuse the code for model trees.
How do we measure the error and determine the best split? We can’t use the same error we used in the regression tree.
For a given dataset, we first fit a linear model to the data, and then we measure how much the actual target values differ from values forecasted by our linear model.
The first function in listing 9.4 is called linearSolve(), which is used by the other two functions.
This formats the dataset into the target variable Y and the independent variable X B.
In this function, you also raise an exception if the inverse of the matrix can’t be determined.
The next function, modelLeaf(), is used to generate a model for a leaf node once you’ve determined to no longer split the data.
All this does is call linearSolve() on the dataset and return the regression coefficients: ws.
The final function, modelErr(), computes the error for a given dataset.
This is used in chooseBestSplit() to determine which split to take.
This calls linearSolve() on the dataset and computes the squared error of yHat and Y.
To see this in action, save regTrees.py and enter the following into your Python shell:
Now to use createTree() with our model tree functions, you enter the functions as arguments to createTree():
The code created two models, one for the values less than 0.285477 and one for values greater.
These are close to the actual values I used to generate the data in figure 9.4
You can calculate the correlation coefficients in NumPy by the command corrcoef(yHat,y, rowvar=0), where yHat is the predicted values and y is the actual values of the target variable.
You’ll now see an example where we compare the results from the previous chapter using standard linear regression to the methods in this chapter using tree-based methods.
We’ll compare the results using corrcoef() to see which one is the best.
Now that you can create model trees and regression trees and do regular regression, let’s test them to see which one is the best.
We’re going to first write a few quick functions to give us a forecast for any given input once we have a tree.
Next, we’re going to use this to calculate the test error for three different regression models.
We’ll test these models on some data relating a person’s intelligence with the number of speeds on their bicycle.
Also, the data used in this example is purely fictional.
First, let’s write some functions to give us a value, for a given input and a given tree.
Open regTrees.py and enter the code from the following listing.
The function treeForeCast() takes a single data point or row vector and will return a single floating-point value.
This gives one forecast for one data point, for a given tree.
You have to tell treeForeCast() what type of tree you’re using so that it can use the proper model at the leaf.
The argument modelEval is a reference to a function used to evaluate the data at a leaf node.
When a leaf node is hit, it calls modelEval() on the input data.
To evaluate a regression tree leaf node, you call regTreeEval(), which returns the value at the leaf node.
The function modelTreeEval() reformats the input data to account for the 0th order term and then calculates the forecasted value and returns it.
This function is useful when you want to evaluate a test set, because it returns a vector of forecasted values.
I plotted data collected for various bicycle riders, relating their IQ to the number of speeds on their bicycle.
We’re going to build multiple models for this dataset and then test them against a test set.
Figure 9.6 Data relating the number of speeds on a person’s bicycle to their intelligence quotient; this data will be used to test tree-based regression against standard linear regression.
We’re going to build three models for the data in figure 9.6
First, save the code from listing 9.5 in regTrees.py and enter the following in your Python shell:
Next, you need to build a tree for the data:
Now try it with a model tree with the same settings, but you’ll make a model tree instead of a regression tree:
You don’t need to import any code from chapter 8; you have a linear equation solver already written in linearSolve():
To get the yHat values, you can loop over the test data:
The R2 value is lower than the other two methods, so we’ve shown that the trees do a better job at predicting complex data than a simple linear model.
I’m sure you’re not surprised, but I wanted to show you how to qualitatively compare the different regression models.
We’ll now explore a framework for building graphical user interfaces in Python.
You can use this GUI to explore different regression tools.
Machine learning gives us some powerful tools to extract information from poorly understood data.
Being able to present this information to people in an easily understood manner is important.
In addition, if you can give people the ability to interact with the data and algorithms, you’ll have an easier time explaining things.
If all you do is generate static plots and output numbers to the Python shell, you’re going to have a harder time communicating your results.
If you can write some code that allows people to explore the data, on their own terms, without instruction, you’ll have much less explaining to do.
One way to help present the data and give people a way to interact with it is to build a GUI, or graphical user interface, as shown in figure 9.7
Prepare: We need to parse the file with Python, and get numeric values.
Analyze: We’ll build a GUI with Tkinter to display the model and the data.
Train: We’ll train a regression tree and a model tree and display the models with the data.
Use: The GUI will allow people to play with different settings for prepruning and to choose different types of models to use.
In this final section, we’re going to look at how to build a GUI in Python.
You’ll first see how to use an existing module called Tkinter to build a GUI.
Next, you’ll see how to interface Tkinter with the library we’ve been using for making plots.
We’ll create a GUI to give people the ability to explore regression trees and model trees.
Tkinter is one framework that’s easy to work with and comes with the standard Python build.
Let’s get started with the overly simple Hello World example.
At this point, a small window will appear, or something is wrong.
To fill out this window with our text, enter the following commands:
Now your text box should display the text you entered.
That was pretty easy, wasn’t it? To be complete, you should add the following line:
This kicks off the event loop, which handles mouse clicks, keystrokes, and redrawing, among other things.
Widgets are things like text boxes, buttons, labels, and check buttons.
The label we made, myLabel, is the only widget in our overly simple Hello World example.
When we called the .grid() method of myLabel, we were telling the geometry manager where to put myLabel.
You can specify the row and column of each widget.
The code in listing 9.6 sets up the proper Tkinter modules and arranges them using the grid geometry manager.
The format of this code is the same as our simple example of how we first created a root widget of type Tk and then inserted a label.
You can see how we use the grid() method, with the row and column settings.
You can also specify columnspan and rowspan to tell the geometry manager to allow a widget to span more than one row or column.
New widgets that you haven’t seen yet are Entry, Checkbutton, and IntVar.
The Entry widget is a text box where a single line of text can be entered.
In order to read the state of Checkbutton, you need to create a variable, which is why you have IntVar.
Now that we have the GUI working the way we want it to, let’s make it create plots.
We’re going to plot out a dataset and on the same chart plot out forecasted values from our tree-based regression methods.
You’ll see how to do this in the next subsection.
How can we put one of those plots in our GUI? To do this, I’m going to introduce the concept of a backend, and then we’ll alter the Matplotlib backend (only in our GUI) to display in the Tkinter GUI.
The creators of Matplotlib have a frontend, which is the user-facing code such as the plot() and scatter() methods.
They’ve also created a backend, which interfaces the plot with many different applications.
You could alter the backend to have your plots displayed in PNG, PDF, SVG, and so on.
Agg is a C++ library to make raster images from a figure.
TkAgg allows us to use Agg with our selected GUI framework, Tk.
We can place a canvas in our Tk GUI and arrange it with .grid()
Let’s replace the Plot Place Holder label with our canvas.
Delete the Plot Place Holder label and add in the following code:
This code creates a Matplotlib figure and assigns it to the global variable reDraw.f.
It next creates a canvas widget, similar to the other widgets.
We can now connect this canvas with our tree-creating functions.
To see this in action, open treeExplore.py and add the following code.
Remember that we previously made stubs for reDraw() and drawTree(), so make sure that you don’t have two copies of the same function.
The first thing you do in listing 9.7 is import Matplotlib and set the backend to TkAgg.
There are two more import statements that glue together TkAgg and a Matplotlib figure.
If you recall from listing 9.6, this is the function that gets called when someone clicks the button labeled ReDraw.
This function does two things: first, it calls getInputs(), which gets values from the entry boxes.
Next, it calls reDraw() with the values from the entry boxes, and a beautiful plot is made.
The function getInputs() tries to figure out what the user entered, without crashing the program.
We’re expecting a float for tolS and an integer for tolN.
To get the text a user entered, you call the .get() method on the Entry widget.
Form validation can consume a lot of your time when GUI programming, but it’s important to have a successful user experience.
If Python can interpret the text as an integer, you use that integer.
If it can’t recognize it, you print an error, clear the entry box, and restore the default value.
The function reDraw() is where the tree drawing takes place.
The first thing that’s done is to clear the previous figure, so you don’t have two plots on top of each other.
When the function is cleared, the subplot is deleted, so you need to add a new one.
Next, you see whether the check box has been checked.
After the tree is built, you create forecasted values from our testDat, which are evenly spaced points in the same range as our data.
Finally, the actual data and the forecasted values are plotted.
I plotted the actual data with scatter() and the forecasted values with plot()
The scatter() method creates discrete points and the plot() method creates a continuous line.
If you’re writing code in an IDE, you can execute it with a run command.
From the command line, you can execute it with python treeExplore.py.
You should see something like figure 9.7 at the beginning of this section.
The default value shows a regression tree with eight leaf nodes.
Click the Model Tree text box and click the ReDraw button.
Oftentimes your data contains complex interactions that lead to nonlinear relationships between the input data and the target variables.
One method to model these complex relationships is to use a tree to break up the predicted value into piecewise constant segments or piecewise linear segments.
A tree structure modeling the data with piecewise constant segments is known as a regression tree.
When the models are linear regression equations, the tree is known as a model tree.
The CART algorithm builds binary trees and can handle discrete as well as continuous split values.
Model trees and regression trees can be built with the CART algorithm as long as you use the right error measurements.
When building a tree, there’s a tendency for the tree-building algorithm to build the tree too closely to the data, resulting in an overfit model.
An overfit tree is often more complex that it needs to be.
To make the tree less complex, a process of pruning is applied to the tree.
The model tree does a better job of forecasting the data than the regression tree.
It’s not the only one, but it’s the most commonly used.
Tkinter allows you to build widgets and arrange those widgets.
You can make a special widget for Tkinter that allows you to display Matplotlib plots.
The integration of Matplotlib and Tkinter allows you to build powerful GUIs where people can explore machine learning algorithms in a more natural way.
We’ll now leave behind the security of supervised learning and head to the unknown waters of unsupervised learning.
In regression and classification (supervised learning), we had a target variable.
This isn’t the case in unsupervised learning, as you’ll see shortly.
This third part of Machine Learning in Action deals with unsupervised learning.
This is a break from what was covered in the first two sections.
In unsupervised learning we don’t have a target variable as we did in classification and regression.
Instead of telling the machine “Predict Y for our data X,” we’re asking “What can you tell me about X?” Things we ask the machine to tell us about X may be “What are the six best groups we can make out of X?” or “What three features occur together most frequently in X?”
Association analysis can help us answer the question “What items are mostly commonly bought together?” We finish our study of unsupervised learning in chapter 12 with a more efficient algorithm for association analysis: the FP-growth algorithm.
If a percentage of the voters were to have switched sides, the outcome of the elections would have been different.
There are small groups of voters who, when properly appealed to, will switch sides.
These groups may not be huge, but with such close races, they may be big enough to change the outcome of the election.1 How do you find these groups of people, and how do you appeal to them with a limited budget? The answer is clustering.
First, you collect information on people either with or without their consent: any sort of information that might give some clue about what is important to them and what will influence how they vote.
Then you put this information into some sort of clustering algorithm.
Next, for each cluster (it would be smart to choose the largest one first) you craft a message that will appeal to these voters.
Finally, you deliver the campaign and measure to see if it’s working.
Clustering is a type of unsupervised learning that automatically forms clusters of similar things.
You can cluster almost anything, and the more similar the items are in the cluster, the better your clusters are.
In this chapter, we’re going to study one type of clustering algorithm called k-means.
It’s called kmeans because it finds k unique clusters, and the center of each cluster is the mean of the values in that cluster.
You’ll see this in more detail in a little bit.
Before we get into k-means, let’s talk about cluster identification.
Clustering is sometimes called unsupervised classification because it produces the same result as classification but without having predefined classes.
With cluster analysis we’re trying to put similar things in a cluster and dissimilar things in a different cluster.
You’ve seen different similarity measures in previous chapters, and they’ll come up in later chapters as well.
The type of similarity measure used depends on the application.
We’ll build the k-means algorithm and see it in action.
We’ll next discuss some drawbacks of the simple k-means algorithm.
To improve some of these problems, we can apply postprocessing to produce better clusters.
Next, you’ll see a more efficient version of k-means called bisecting k-means.
Finally, you’ll see an example where we’ll use bisecting k-means to find optimal parking locations while visiting multiple nightlife hotspots.
Each cluster is described by a single point known as the centroid.
Centroid means it’s at the center of all the points in the cluster.
Cons: Can converge at local minima; slow on very large datasets.
First, the k centroids are randomly assigned to a point.
Next, each point in the dataset is assigned to a cluster.
The assignment is done by finding the closest centroid and assigning the point to that cluster.
After this step, the centroids are all updated by taking the mean value of all the points in that cluster.
Create k points for starting centroids (often randomly) While any point has changed cluster assignment for every point in our dataset: for every centroid calculate the distance between the centroid and point assign the point to the cluster with the lowest distance for every cluster calculate the mean of the points in that cluster assign the centroid to the mean.
The performance of k-means on a dataset will be determined by the distance measure you use.
First, create a file called kMeans.py and enter the code from the following listing.
Prepare: Numeric values are needed for a distance calculation, and nominal values can be mapped into binary values for distance calculations.
Quantitative error measurements such as sum of squared error (introduced later) can be used.
Often, the clusters centers can be treated as representative data of the whole cluster to make decisions.
The code in listing 10.1 contains a few helper functions you’ll need for the k-means algorithm.
The first function, loadDataSet(), is the same as in previous chapters.
It loads a text file containing lines of tab-delimited floats into a list.
Each of these lists is appended to a list called dataMat, which is returned.
The return value is a list containing many other lists.
This format allows you to easily pack values into a matrix.
The next function, distEclud(), calculates the Euclidean distance between two vectors.
This is our initial distance function, which you can replace with other distance metrics.
Finally, the last function in listing 10.1 is randCent(), which creates a set of k random centroids for a given dataset.
The random centroids need to be within the bounds of the dataset.
This is accomplished by finding the minimum and maximum values of each dimension in the dataset.
Save kMeans.py and enter the following code in your Python shell:
To create a data matrix from a text file, enter the following (testSet.txt is included with the source code for chapter 10):
We’ll use it later to test the full k-means algorithm.
First, let’s see what the minimum and maximum values are in our matrix:
Now let’s see if randCent() produces a value between min and max.
Now that we have the support functions working, we’re ready to implement the full k-means algorithm.
The algorithm will create k centroids, then assign each point to the closest centroid, and then recalculate the centroids.
This process will repeat until the points stop changing clusters.
Open kMeans.py and enter the code from the following listing.
The dataset and the number of clusters to generate are the only required parameters.
A function to use as the distance metric is optional, and a function to create the initial centroids is also optional.
The function starts out by finding the number of items in the dataset and then creates a matrix to store cluster assignments.
The cluster assignment matrix, called clusterAssment, has two columns; one column is for the index of the cluster and the second column is to store the error.
This error is the distance from the cluster centroid to the current point.
We’ll use this error later on to measure how good our clusters are.
You iterate until none of the data points changes its cluster.
You create a flag called clusterChanged, and if this is True you continue iterating.
You next loop over all the data points to find the closest centroid.
This is done by looping over all the centroids and measuring the distance to each one.
The default is distEclud(), which we wrote in listing 10.1
If any of these clusters changes, you update the clusterChanged flag.
Finally, you loop over all the centroids and update their values.
Next, you take the mean values of all these points.
The option axis=0 in the mean calculation does the mean calculation down the columns.
After saving kMeans.py, enter the following in your Python shell:
If you don’t have a copy of datMat from the previous example, you can enter the following command (remember to import NumPy):
I have a hunch there should be four clusters, because the image looks like there could be four clusters, so enter the following:
A plot of these four centroids, along with the original data, is given in figure 10.1
Everything went smoothly with this clustering, but things don’t always go that way.
We’ll next talk about some possible problems with k-means clustering and how to fix those problems.
We talked about putting data points in k clusters where k is a user-defined parameter.
How does the user know that k is the right number? How do you know that the clusters are good clusters? In the matrix with the cluster assignments is a value representing the error of each point.
It’s the squared distance of the point to the cluster center.
We’ll discuss ways you can use this error to find out the quality of your clusters.
This is the result of running kmeans on a dataset with three clusters.
The reason that k-means converged but we had poor clustering was that k-means converges on a local minimum, not a global minimum.
A local minimum means that the result is good but not necessarily the best possible.
Figure 10.2 Cluster centroids incorrectly assigned because of poor initialization with random initialization in k-means.
One metric for the quality of your cluster assignments you can use is the SSE, or sum of squared error.
A lower SSE means that points are closer to their centroids, and you’ve done a better job of clustering.
Because the error is squared, this places more emphasis on points far from the centroid.
One sure way to reduce the SSE is to increase the number of clusters.
This defeats the purpose of clustering, so assume that you have to increase the quality of your clusters while keeping the number of clusters constant.
How can you fix the situation in figure 10.2? You can postprocess the clusters.
One thing you can do is take the cluster with the highest SSE and split it into two clusters.
To get your cluster count back to the original value, you could merge two clusters.
From looking at figure 10.2, it seems obvious to merge the incorrectly placed centroids at the bottom of the figure.
That was easy to visualize in two dimensions, but how could you do that in 40 dimensions?
Two quantifiable ideas are merging the closest centroids or merging the two centroids that increase the total SSE the least.
You could calculate distances between all centroids and merge the closest two.
The second method would require merging two clusters and then calculating the total SSE.
You’d have to repeat this for all pairs of clusters to find the best pair to merge.
We’ll next discuss an algorithm that uses these cluster-splitting techniques to form better clusters.
To overcome the problem of poor clusters because of k-means getting caught in a local minimum, another algorithm has been developed.
This algorithm, known as bisecting k-means, starts out with one cluster and then splits the cluster in two.
The cluster to split is decided by minimizing the SSE.
This splitting based on the SSE is repeated until the user-defined number of clusters is attained.
Start with all the points in one cluster While the number of clusters is less than k for every cluster measure total error perform k-means clustering with k=2 on the given cluster measure total error after k-means has split the cluster in two choose the cluster split that gives the lowest error and commit this split.
Another way of thinking about this is to choose the cluster with the largest SSE and split it and then repeat until you get to the user-defined number of clusters.
This doesn’t sound too difficult to code, does it? To see this in action, open kMeans.py and enter the code from the following listing.
You give it a dataset, the number of clusters you want, and a distance measure, and it gives you the clusters.
Similar to kMeans() in listing 10.2, you can change the distance metric used.
The function starts out by creating a matrix to store the cluster assignment and squared error for each point in the dataset.
Next, one centroid is calculated for the entire dataset, and a list is created to hold all the centroids.
Next, you enter the while loop, which splits clusters until you have the desired number of clusters.
You can measure the number of clusters you have by measuring the number of items in the cluster list.
You’re going to iterate over all the clusters and find the best cluster to split.
To do this, you need to compare the SSE after each split.
You first initialize the lowest SSE to infinity; then you start looping over each cluster in the centList cluster list.
For each of these clusters, you create a dataset of only the.
This dataset is called ptsInCurrCluster and is fed into kMeans()
The k-means algorithm gives you two new centroids as well as the squared error for each of those centroids.
If this split produces the lowest SSE, then it’s saved.
After you’ve decided which cluster to split, it’s time to apply this split.
Applying the split is as easy as overwriting the existing cluster assignments for the cluster you’ve decided to split.
You need to change these cluster numbers to the cluster number you’re splitting and the next cluster to be added.
When the while loop ends, the centroid list and the cluster assignments are returned, the same way that they’re done in kMeans()
After you’ve entered the code from listing 10.3, save kMeans.py and enter the following in your Python shell:
You can run it on our original dataset or you can load the “difficult” dataset in figure 10.2 by entering the following:
You can run this multiple times and the clustering will converge to the global minimum, whereas the original kMeans() would occasionally get stuck in the local minimum.
A plot of the data points and centroids after running biKmeans() is shown in figure 10.3
Now that you have the bisecting k-means algorithm working, you’re ready to put it to use on some real data.
In the next section, we’ll take some geographic coordinates on a map and create clusters from that.
Here’s the situation: your friend Drew wants you to take him out on the town for his birthday.
A number of other friends are going to come also, so you need to provide a plan that everyone can follow.
Drew has given you a list of places he wants to go.
This list is long; it has 70 establishments in it.
The list contains similar establishments in the greater Portland, Oregon, area.
Seventy places in one night! You decide the best strategy is to cluster these places together.
You can arrange transportation to the cluster centers and then hit the places on foot.
Drew’s list includes addresses, but addresses don’t give you a lot of information about how close two places are.
Then, you can cluster these places together and plan your trip.
Figure 10.3 Cluster assignment after running the bisecting k-means algorithm.
Analyze: Use Matplotlib to make 2D plots of our data, with clusters and map.
Use: The final product will be your map with the clusters and cluster centers.
You need a service that will convert an address to latitude and longitude.
We’re going to explore how to use the Yahoo! PlaceFinder API.
Then, we’ll cluster our coordinates and plot the coordinates along with cluster centers to see how good our clustering job was.
The wonderful people at Yahoo! have provided a free API that will return a latitude and longitude for a given address.
You can read more about it at the following URL: http://developer.yahoo.com/geo/placefinder/guide/
In order to use it, you need to sign up for an API key.
To do that, you have to sign up for the Yahoo! Developer Network: http://developer.yahoo.com/
You’re going to need the appid to use the geocoder.
A geocoder takes an address and returns the latitude and longitude of that address.
Open kMeans.py and add the code from the following listing.
The code in listing 10.4 contains two functions: geoGrab() and massPlaceFind()
The function geoGrab() gets a dictionary of values from Yahoo, while massPlaceFind() automates this and saves the relevant information to a file.
In geoGrab(), you first set the apiStem for Yahoo APIs; then you create a dictionary.
You’ll set various values of this dictionary, including flags=J, so that the output will be returned in the JSON format.
It’s a format for serializing arrays and dictionaries, but we won’t look at any JSON.
You next use the urlencode() function from urllib to pack up your dictionary in a format you can pass on in a URL.
The return value is in JSON format, so you use the JSON Python module to decode it into a dictionary.
The decoded dictionary is returned, and you’ve finished geocoding one address.
This opens a tab-delimited text file and gets the second and third fields.
The output dictionary from geoGrab() is then checked to see if there are any errors.
If not, then the latitude and longitude are read out of the dictionary.
These values are appended to the original line and written to a new file.
If there’s an error, you don’t attempt to extract the latitude and longitude.
Last, the sleep function is called to delay massPlaceFind() for one second.
This is done to ensure that you don’t make too many API calls too quickly.
If you do, you may get blocked, so it’s a good idea to put in this delay.
After you’ve saved kMeans.py, enter the following in your Python shell:
To try out geoGrab, enter a street address and a city string such as.
The actual URL used is printed so you can see exactly what’s going on.
If you get sick of seeing the URL, feel free to comment out that print statement in listing 10.4
This is a dictionary with one key, ResultSet, which contains another dictionary with the following keys: Locale, ErrorMessage, Results, version, Error, Found, and Quality.
You can explore all these things, but the two we’re interested in are Error and Results.
Anything else means that we didn’t get the address we were looking for.
These are strings, and you’ll have to get them as floats using float() to use them as numbers.
Now, to see this in action on multiple lines, execute the second function in listing 10.4:
This generates a text file called places.txt in your working directory.
We’ll plot the clubs along with their cluster centers on a map of the city.
Now that we have a list properly formatted with geographic coordinates, we can cluster the clubs together.
We used the Yahoo! PlaceFinder API to get the latitude and longitude of each point.
Now we need to use that to calculate a distance between points and for cluster centers.
The clubs we’re trying to cluster in this example are given to us in latitude and longitude, but this isn’t enough to tell us a distance.
Near the North Pole, you can walk a few meters, and your longitude will vary by tens of degrees.
Walk the same distance at the equator, and your longitude varies a fraction of a degree.
You can use something called the spherical law of cosines to compute the distance between two sets of latitude and longitude.
To see this implemented in code along with a function for plotting the clustered clubs, open kMeans.py and add the code from the following listing.
Example: clustering points on a map return arccos(a + b)*6371.0
The first one, distSLC(), is the a distance metric for two points on the earth’s surface.
The second one, clusterClubs(), clusters the clubs from a text file and plots them.
The function distSLC() returns the distance in miles for two points on the earth’s surface.
Two points are given in latitude and longitude, and you use the spherical law of cosines to calculate the distance between these two points.
Our latitudes and longitudes are given in degrees, but sin() and cos() take radians as inputs.
You convert from degrees to radians by dividing by 180 and multiplying by pi.
The second function, clusterClubs(), takes one input that’s the number of clusters you’d like to create.
This function wraps up parsing a text file, clustering, and plotting.
You first create an empty list and then open places.txt and get the fourth and fifth fields, which contain the latitude and longitude.
A matrix is then created from the list of latitude/longitude pairs.
You next run biKmeans() on these data points and use the distSLC() distance measure for your clustering.
In order to plot the clusters, you first create a figure and a rectangle.
You’re going to use this rectangle to determine the amount of the figure to dedicate to plotting.
Next, you create a list of all the available marker types for scatter plotting.
You’ll use this later to give a unique marker to each cluster.
Next, you create a new plot on the same figure as the image you just plotted.
This allows you to use two coordinate systems without any scaling or shifting.
Next, you loop over every cluster and plot these out.
A marker type is chosen from scatterMarkers, which you created earlier.
Finally, the clusters are plotted as crosses, and you show the plot.
To see this in action, enter the following in your Python shell after you’ve saved kMeans.py:
A figure similar to figure 10.4 should appear after you’ve executed this command.
With unsupervised learning you don’t know what you’re looking for, that is, there are no target variables.
Clustering groups data points together, with similar data points in one cluster and dissimilar points in a different group.
A number of different measurements can be used to measure similarity.
One widely used clustering algorithm is k-means, where k is a user-specified number of clusters to create.
The k-means clustering algorithm starts with k-random cluster centers known as centroids.
Next, the algorithm computes the distance from every point to the cluster centers.
The cluster centers are then recalculated based on the new points in the cluster.
This process is repeated until the cluster centers no longer move.
This simple algorithm is quite effective but is sensitive to the initial cluster placement.
To provide better clustering, a second algorithm called bisecting k-means can be used.
In the next iteration, the cluster with the largest error is chosen to be split.
This process is repeated until k clusters have been created.
Another type of clustering, known as hierarchical clustering, is also a widely used clustering algorithm.
In the next chapter, we’ll examine the Apriori algorithm for finding association rules in a dataset.
A trip to the grocery store provides many examples of machine learning in action today and future uses of it.
The way items are displayed, the coupons offered to you after you purchase something, and loyalty programs all are driven by massive amounts of data crunching.
The store wants to get as much money as possible from you, and they certainly will use technology for this purpose.
Loyalty programs, which give the customer a discount by using a loyalty card, can give the store a glimpse at what one consumer is purchasing.
If you don’t use a loyalty card, the store can also look at the credit card you used to make the purchases.
If you don’t use a loyalty card and pay with cash, a store can look at the items purchased together.
For more ideas on possible uses of technology in the grocery store, see The Numerati by Stephen Baker.
Looking at items commonly purchased together can give stores an idea of customers’ purchasing behavior.
This knowledge, extracted from the sea of data, can be used for pricing, marketing promotions, inventory management, and so on.
Looking for hidden relationships in large datasets is known as association analysis or association rule learning.
The problem is, finding different combinations of items can be a time-consuming task and prohibitively expensive in terms of computing power.
Brute-force solutions aren’t capable of solving this problem, so a more intelligent approach is required to find frequent itemsets in a reasonable amount of time.
In this chapter we’ll focus on the Apriori algorithm to solve this problem.
We’ll first discuss association analysis in detail, and then we’ll discuss the Apriori principle, which leads to the Apriori algorithm.
We’ll next create functions to efficiently find frequent items sets, and then we’ll extract association rules from the frequent items sets.
We’ll finish up with an example of extracting association rules from congressional voting records and an example of finding common features in poisonous mushrooms.
Association analysis is the task of finding interesting relationships in large datasets.
These interesting relationships can take two forms: frequent item sets or association rules.
Frequent item sets are a collection of items that frequently occur together.
The second way to view interesting relationships is association rules.
Association rules suggest that a strong relationship exists between two items.
A list of transactions from a grocery store is shown in figure 11.1
How do we define these so-called interesting relationships? Who defines what’s interesting? When we’re looking for frequent item sets, what’s the definition of frequent? There are a number of concepts we can use to select these things, but the two most important are support and confidence.
The support and confidence are ways we can quantify the success of our association analysis.
Let’s assume we wanted to find all sets of items with a support greater than 0.8
How would we do that? We could generate a list of every combination of items and then count how frequently that occurs.
It turns out that doing this can be very slow when we have thousands of items for sale.
In the next section we’ll address this in detail, and we’ll look at something called the Apriori principle, which will allow us to reduce the number of calculations we need to do to learn association rules.
Let’s assume that we’re running a grocery store with a very limited selection.
We’re interested in finding out which items were purchased together.
What are all the possible combinations in which can be purchased? We can have one item, say item0, alone, or two items, or three items, or all of the items together.
We’re concerned only that they purchased one or more of an item.
Prepare: Any data type will work as we’re storing sets.
Use: This will be used to find frequent itemsets and association rules between items.
The rule turned around says that if an itemset is infrequent, then its supersets are also infrequent, as shown in figure 11.3
In the next section, you’ll see the Apriori algorithm based on this Apriori principle.
We’ll code it in Python and put it to use on a simple data set from our fictional grocery store, Hole Foods.
We discussed in section 11.1 that in association analysis we’re after two things: frequent item sets and association rules.
We first need to find the frequent itemsets, and then we can find association rules.
In this section, we’ll focus only on finding the frequent itemsets.
The way to find frequent itemsets is the Apriori algorithm.
The Apriori algorithm needs a minimum support level as an input and a data set.
The algorithm will generate a list of all candidate itemsets with one item.
The transaction data set will then be scanned to see which sets meet the minimum support level.
Sets that don’t meet the minimum support level will get tossed out.
The remaining sets will then be combined to make itemsets with two elements.
Again, the transaction dataset will be scanned and itemsets not meeting the minimum support level will get tossed.
This procedure will be repeated until all sets are tossed out.
When defining a problem, it’s common to state prior knowledge, or assumptions.
This is written as “a priori.” In Bayesian statistics, it’s common to make inferences conditional upon this a priori knowledge.
A priori knowledge can come from domain knowledge, previous measurements, and so on.
Before we code the whole algorithm in Python, we’ll need to create a few helper functions.
We’ll create a function to create an initial set, and we’ll create a function to scan the dataset looking for items that are subsets of transactions.
For each transaction in tran the dataset: For each candidate itemset, can: Check to see if can is a subset of tran If so increment the count of can For each candidate itemset: If the support meets the minimum, keep this item Return list of frequent itemsets.
To see this in action, create a file called apriori.py and add the following code.
In the Apriori algorithm, we create C1, and then we’ll scan the dataset to see if these one itemsets meet our minimum support requirements.
You need a special function for the first list of candidate itemsets because initially you’re reading from input, whereas later lists will be properly stored formatted.
Frozensets are sets that are frozen, which means they’re immutable; you can’t change them.
You need to use the type frozenset instead of set because you’ll later use these sets as the key in a dictionary; you can do that with frozensets but not with sets.
This will be used to store all our unique values.
Next, you iterate over all the transactions in our dataset.
For each transaction, you iterate over all the items in that transaction.
You don’t simply add the item; you add a list containing just one item.
You do this to create a set of each item, because later in the Apriori algorithm you’ll be doing set operations.
You can’t create a set of just one integer in Python.
Finally, you sort the list and then map every item in the list to frozenset() and return this list of frozensets.
This function takes three arguments: a dataset, Ck, a list of candidate sets, and minSupport, which is the minimum support you’re interested in.
Additionally, this function returns a dictionary with support values for use later.
This function creates an empty dictionary, ssCnt, and then goes over all the transactions in the dataset and all the candidate sets in C1
If the sets of C1 are part of the dataset, then you’ll increment the count in the dictionary.
After you’ve scanned over all the items in the dataset and all the candidate sets, you need to calculate the support.
Sets that don’t meet your minimum support levels won’t be output.
First, you create an empty list that will hold the sets that do meet the minimum support.
The next loop goes over every element in the dictionary and measures the support.
It isn’t necessary to insert at the beginning; it just makes the list look organized.
You also return supportData, which holds the support values for your frequent itemsets.
Let’s see this in action! After you’ve saved apriori.py, enter the following in your Python shell:
C1 contains a list of all the items in frozenset.
Now let’s create D, which is a dataSet in the set form:
Now that you have everything in set form, you can remove items that don’t meet our minimum support.
For this example use 0.5 as our minimum support level:
By removing it, you’ve removed more work from when you find the list of two-item sets.
Pseudo-code for the whole Apriori algorithm would look like this:
Now that you can filter out sets, it’s time to build the full Apriori algorithm.
Open apriori.py and add the code from the following listing.
You give this a dataset and a support number, and it will generate a list of candidate itemsets.
This works by first creating C1 and then taking the dataset and turning that into D, which is a list of sets.
You use the map function to map set() to every item in the dataSet list.
This is done with a while loop, which creates larger lists of larger itemsets until the next-largest itemset is empty.
If this sounds confusing, hold on a second, and you’ll see how it works.
Ck is a list of candidate itemsets, and then scanD() goes through Ck and throws out itemsets that don’t meet the minimum support levels.
Finally, when Lk is empty, you return L and exit.
To see this in action, enter the following after you’ve saved apriori.py:
Each of these itemsets was generated in apriori() with aprioriGen()
Four of these items are in L[1], and the other two items get filtered out by scanD()
The variable suppData is a dictionary with the support values of our itemsets.
We don’t care about those values right now, but we’ll use them in the next section.
You now know which items occur in 70% of all transactions, and you can begin to draw conclusions from this.
You can take this data and begin to draw conclusions, which many applications do, or you can take it and generate association rules to try to get an if-then understanding of the data.
Back in section 11.2, I mentioned that you can look for many interesting things with association analysis.
Two common things that people look for are frequent itemsets and association rules.
You just saw how you can find frequent itemsets with the Apriori algorithm.
Now we need to figure out how to find association rules.
We can use this property of association rules to reduce the number of rules we need to test.
Similar to the Apriori algorithm in listing 11.2, we can start with a frequent itemset.
We’ll then create a list of sets with one item on the right-hand side and test all of those.
Next, we’ll merge the remaining rules to create a list of rules with two items on the right-hand side.
To see this in action, open apriori.py and add the following code.
The first one, generateRules(), is the main command, which calls the other two.
The other two functions, rulesFromConseq() and calcConf(), generate a set of candidate rules and evaluate those rules, respectively.
You’re interested in calculating the confidence of a rule and then finding out which rules meet the minimum confidence.
You’ll return a list of rules that meet the minimum confidence; to hold this you create an empty list, prunedH.
Next, you iterate over all the itemsets in H and calculate the confidence.
By importing these support values, you save a lot of computing time.
If a rule does meet the minimum confidence, then you print the rule to the screen.
The passing rule is also returned and will be used in the next function, rulesFromConseq()
You also fill in the list brl, which is the bigRuleList passed in earlier.
To generate more association rules from our initial itemset, you use the rulesFromConseq() function.
This takes a frequent itemset and H, which is a list of items that could be on the right-hand side of a rule.
The code then measures m, which is the size of the itemsets in H.
You use the aprioriGen() function from listing 11.2 to generate combinations of the items in H without repeating.
You want to see if any of these make sense by testing their confidence in calcConf()
If more than one rule remains, then you recursively call rulesFromConseq() with Hmp1 to see if you could combine those rules further.
To see this in action, save apriori.py and enter the following in your Python shell:
We got a lot more rules (11) once we lowered the confidence.
Now that you see this works on a trivial dataset, let’s put it to work on a bigger, real-life dataset.
In the next section we’ll examine the voting records of the U.S.
Now that we can find frequent itemsets and association rules, it’s time to put these tools to use on a real-life dataset.
What can we use? Shopping is a good example, but it’s played out.
That sounds interesting, but a more interesting example I saw was voting by members of the U.S.
This is a little old, and the issues don’t mean much to me.
There are a number of organizations devoted to making government data public.
You’ll see how to get the data from Votesmart.org into a format that you can use for generating frequent itemsets and association rules.
This data could be used for campaign purposes or to forecast how politicians will vote.
Next, we’ll take the voting records and create a transaction database.
Finally, we’ll use the code written earlier in this chapter to generate a list of frequent itemsets and association rules.
Prepare: Write a function to process votes into a series of transaction records.
Analyze: We’ll look at the prepared data in the Python shell to make sure it’s correct.
Train: We’ll use the apriori() and generateRules() functions written earlier in this chapter to find the interesting information in the voting records.
Use: For entertainment purposes, but you could use the results for a political campaign or to forecast how elected officials will vote.
Project Vote Smart has collected a large amount of government data.
They have also provided a public API to access this data at http://api.votesmart.org/docs/terms.html.
Sunlight Labs has written a Python module to access this data.
We’re going to get some recent voting data from the U.S.
We eventually want the data to be in the same form as shown in figure 11.1
Let’s start by trying to get some things that they voted on recently.
If you haven’t installed python-votesmart and gotten an API key, you’ll need to do that now.
You can see appendix A for how to install python-votesmart.
To get started with the votesmart API, you need to import votesmart:
You can get further information about each of these bills by using the getBill() method.
This returns a BillDetail object with a whole lot of information.
You can investigate all the information there, but what we’re interested in are the actions taken on the bill.
This will give you a number of actions—one when the bill is introduced and another when the bill is voted on.
We’re interested in the one where the voting took place.
You can get this by typing in the following commands:
A bill is introduced, voted on by congress, and voted on by the House of Representatives before it goes through the executive office.
The Passage stage can be deceptive because it could be in the Passage stage at the executive office, where there is no vote.
Now that we’ve played around with all the relevant APIs, we can put all this together.
We’re going to write a function to go from the billIds in the text file to an actionId.
As I mentioned earlier, not every bill has been voted on, and some bills have been voted on in multiple places.
We have to filter out the actionIds to get actionIds that will give us some vote data.
We’ll write one function called getActionIds() to handle filtering out the actionIds.
Open apriori.py and enter the code from the following listing.2
Listing 11.4 Functions for collecting action IDs for bills in Congress.
The code in listing 11.4 imports sleep so that you can delay the API calls, and it imports the votesmart module.
You start by importing API key and then creating two empty lists.
The lists will be used to return the actionsIds and titles.
It’s good practice to use these when dealing with outside APIs because you may get an error, and you don’t want an error to waste all the time you spent fetching data.
So you first try to get a billDetail object using the getBill() method.
You next iterate over all the actions in this bill looking for something with some voting data.
There’s voting data on the Passage stage and the Amendment Vote stage, so you look for those.
Now there’s also a Passage stage at the executive level and that doesn’t contain any voting data, so you make sure the level is House B.
If this is true, you print the actionId to the screen and append it to actionIdList.
At this time, you also append the bill title to billTitleList.
This way, if there is an error with the API call, you don’t append billTitleList.
If there is an error, the except block is called, which prints the error to the screen.
Finally, there’s a sleep of one second to be polite and not bombard Votesmart.org with a bunch of rapid API calls C.
After you enter the code from listing 11.4 into apriori.py, enter the following commands:
The actionId is displayed, but it’s also being added to the output, actionIdList, so that you can use it later.
If there is an error, then the try..except code will catch it.
I had one error when I was getting all the actiondIds.
Now you can move on to getting votes on these actionIds.
The candidates can vote Yea or Nay, or they can choose not to vote.
We need a way of encoding this into something like an itemset and a transaction database.
Example: uncovering patterns in congressional voting that a transaction data set has only the absence or presence of an item, not the quantity in it.
With our voting data, we can treat the presence of a Yea or Nay as an item.
There are two major political parties in the United States: Republicans and Democrats.
We’d like to encode this information in our transaction dataset.
This is how we’ll construct the transaction dataset: we’ll create a dictionary with the politician’s name as the key.
We’ll use zero for a Democrat, and one for a Republican.
Now how do we encode the votes? For each bill we’ll make two items: bill+'Yea' and bill+'Nay'
This method will allow us to properly encode if a politician didn’t vote at all.
The translation from votes to items is shown in figure 11.5
Now that we have a system for encoding the votes to items, it’s time to generate our transaction dataset.
Once we have the transaction dataset, we can use the Apriori code written earlier.
We’re going to write a function to take in a series of actionIds and fetch the voting records from Votesmart’s API.
Then we’ll encode the voting for each candidate into an itemset.
Each candidate is going to be a row or a transaction in the transaction dataset.
To see this in action, open apriori.py and add the code from the following listing.
The getTransList() function will create the transaction dataset so that you can use the Apriori code written earlier to generate frequent itemsets and association rules.
It also creates a title list so that you can easily see what each item means.
The first thing you do is create the meaning list, itemMeaning, with the first two elements.
When you want to know what something means, all you have to do is enter the item number as the index to itemMeaning.
Next, you loop over all the bills you have and add Nay or Yea to the bill title and then add it to the itemMeaning list.
You then start going over every actionId you obtained from getActionIds()
The first thing you do is sleep; this is a delay placed in the for loop so that you don’t make too many API calls too quickly.
Next, you print to screen what you’re trying to do so that you can see this is working.
You now have the try.except block, which tries to use the Votesmart API to get all the votes on a particular actionId.
When you loop over all the votes, you fill up transDict by using the politician’s name as the dictionary key.
If you haven’t encountered a politician before, you get his/her party affiliation.
Each politician in the dictionary has a list to store the items they voted on or their party affiliation.
Then you see if this politician voted Nay or Yea on this bill.
If they voted either way, then you add this to the list.
If something goes wrong during the API call, the except block is called, and it prints an error message to the screen and the function continues.
Finally, the transDict transaction dictionary and the item meaning list, itemMeaning, are returned.
Let’s try this out by getting the first two items voted on and see if our code is working:
Don’t be alarmed if many of these lists look similar.
Now, given a list of items, you can quickly decode what it means with the itemMeaning list:
Your output may be different depending on the results returned from the Votesmart server.
Now, before you’re ready to use the Apriori algorithm we developed earlier, you need to make a list of all the transactions.
You can do that with a list comprehension similar to the previous for loop:
Doing this throws out the keys, which are the politicians’ names.
We’re now going to mine the frequent itemsets and association rules using the Apriori algorithm.
Now you can apply the Apriori algorithm from section 11.3
If you try the default support setting of 50%, you won’t get many frequent itemsets:
Using a lower minimum support of 30% gives you many more frequent itemsets:
With a support of 30%, you have lots of frequent itemsets.
We could stop here, but let’s try to generate association rules using the code we wrote in section 11.4
You can first try the default minimum confidence of 0.7:
To find out what each rule means, enter the rule number as the index to itemMeaning:
Sometimes you don’t want to look for the frequent itemsets; you may only be interested in itemsets containing a certain item.
In the final example, we’re going to look for common features in poisonous mushrooms.
You can then use these common features to help you avoid eating mushrooms that are poisonous.
The UCI Machine Learning Repository has a dataset with 23 features taken from species of gilled mushrooms.
We’re going to need to transform these nominal values into a set similar to what we did with the votes in the previous example.
Luckily, this transformation was already done for us.3 Roberto Bayardo has parsed the UCI mushrooms dataset into a set of features for each sample of mushroom.
Each possible value for each feature is enumerated, and if a sample contains that feature, then its integer value is included in the dataset.
It’s included in the source repository under the name mushroom.dat.
Take a look at the first few lines of the prepared file mushroom.dat:
The next feature is cap shape, which has six possible values that are represented with the integers 3–8
Now you can search the frequent itemsets for the poisonous feature 2:
Now you need to look up these features so you know what to look for in wild mushrooms.
If you see any of these features, avoid eating the mushroom.
One final disclaimer: although these features may be common in poisonous mushrooms, the absence of these features doesn’t make a mushroom edible.
Association analysis is a set of tools used to find interesting relationships in a large set of data.
There are two ways you can quantify the interesting relationships.
The first way is a frequent itemset, which shows items that commonly appear in the data together.
The second way of measuring interesting relationships is association rules.
Finding different combinations of items can be a time-consuming task and prohibitively expensive in terms of computing power.
More intelligent approaches are needed to find frequent itemsets in a reasonable amount of time.
One such approach is the Apriori algorithm, which uses the Apriori principle to reduce the number of sets that are checked against the database.
The Apriori principle states that if an item is infrequent, then supersets containing that item will also be infrequent.
The Apriori algorithm starts from single itemsets and creates larger sets by combining sets that meet the minimum support measure.
Support is used to measure how often a set appears in the original data.
Once frequent itemsets have been found, you can use the frequent itemsets to generate association rules.
The significance of an association rule is measured by confidence.
Confidence tells you how many times this rule applies to the frequent itemsets.
Some common examples are items in a store and pages visited on a website.
Association analysis has also been used to look at the voting history of elected officials and judges.
The Apriori algorithm scans over the dataset each time you increase the length of your frequent itemsets.
When the datasets become very large, this can drastically reduce the speed of finding frequent itemsets.
The next chapter introduces the FPgrowth algorithm.4 In contrast to Apriori, it only needs to go over the dataset twice, which can lead to a significant increase in speed.
Have you ever gone to a search engine, typed in a word or part of a word, and the search engine automatically completed the search term for you? Perhaps it recommended something you didn’t even know existed, and you searched for that instead.
That has happened to me, sometimes with comical results when I started a search with “why does....” To come up with those search terms, researchers at the search company used a version of the algorithm we’ll discuss in this chapter.
They looked at words used on the internet and found pairs of words that frequently occur together.1 This requires a way to find frequent itemsets efficiently.
This chapter expands on the topics in the previous chapter.
This chapter covers a great algorithm for uncovering frequent itemsets.
The algorithm, called FP-growth, is faster than Apriori in the previous chapter.
It builds from Apriori but uses some different techniques to accomplish the same task.
That task is finding frequent itemsets or pairs, sets of things that commonly occur together, by storing the dataset in a special structure called an FP-tree.
This results in faster execution times than Apriori, commonly with performance two orders of magnitude better.
In the last chapter, we discussed ways of looking at interesting things in datasets.
Two of the most common ways of looking at things in the dataset are frequent itemsets and association rules.
We’ll dive deeper into that task, exploring the FP-growth algorithm, which allows us to mine data more efficiently.
This algorithm does a better job of finding frequent itemsets, but it doesn’t find association rules.
The FP-growth algorithm is faster than Apriori because it requires only two scans of the database, whereas Apriori will scan the dataset to find if a given pattern is frequent or not—Apriori scans the dataset for every potential frequent item.
On small datasets, this isn’t a problem, but when you’re dealing with larger datasets, this will be a problem.
The basic approach to finding frequent itemsets using the FP-growth algorithm is as follows:
We’ll discuss the FP-tree data structure and then look at how to encode a dataset in this structure.
We’ll next look at how we can mine frequent itemsets from the FPtree.
Finally, we’ll look at an example of mining commonly used words from a stream of Twitter text and an example of mining common patterns in people’s webbrowsing behavior.
The FP-growth algorithm stores data in a compact data structure called an FP-tree.
The FP stands for “frequent pattern.” An FP-tree looks like other trees in computer science, but it has links connecting similar items.
The linked items can be thought of as a linked list.
Unlike a search tree, an item can appear multiple times in the same tree.
The FPtree is used to store the frequency of occurrence for sets of items.
The FP-tree looks like a generic tree with links connecting similar items.
Sets with similar items will share part of the tree.
A node identifies a single item from the set and the number of times it occurred in this sequence.
A path will tell you how many times a sequence occurred.
The links between similar items, known as node links, will be used to rapidly find the location of similar items.
Don’t worry if this sounds a little confusing right now; we’ll work through a simple example.
We used the term support in chapter 11, which was a minimum threshold, below which we considered items infrequent.
If you set the minimum support to 3 and apply frequent item analysis, you’ll get only itemsets that appear three or more times.
First, you build the FP-tree, and then you mine it for frequent itemsets.
To build the tree, you scan the original dataset twice.
The first pass counts the frequency of occurrence of all the items.
If an item is infrequent, supersets containing that item will also be infrequent, so you don’t have to worry about them.
Build an FP-tree pass to count the frequency of occurrence and then address only the frequent items in the second pass.
In the second pass of the dataset, you build the FP-tree.
In order to build a tree you need a container to hold the tree.
The tree in this chapter is more involved than the other trees in this book, so you’ll create a class to hold each node of the tree.
Create a file called fpGrowth.py and add the code from the following listing.
The code in listing 12.1 is a class definition for the nodes of the FP-tree.
It has variables to hold the name of the node, a count.
The nodeLink variable will be used to link similar items (the dashed lines in figure 12.1)
Next, the parent variable is used to refer to the parent of this node in the tree.
Often, you don’t need this in trees because you’re recursively accessing nodes.
Later in this chapter, you’ll be given a leaf node.
If you have continuous data, it will need to be quantized into discrete values.
Use: This can be used to identify commonly occurring items that can be used to make decisions, suggest items, make forecasts, and so on.
Lastly, the node contains an empty dictionary for the children of this node.
There are two methods in listing 12.1; inc()increments the count variable by a given amount.
The last method, disp(), is used to display the tree in text.
It isn’t needed to create the tree, but it’s useful for debugging.
Add another node to see how two child nodes are displayed:
Now that you have the tree data structure built, you can construct the FP-tree.
In addition to the FP-tree shown in figure 12.1, you need a header table to point to the first instance of a given type.
The header table will allow you to quickly access all of the elements of a given type in the FP-tree.
You’ll use a dictionary as your data structure to store the header table.
In addition to storing pointers, you can use the header table to keep track of the total count of every type of element in the FP-tree.
The header table serves as a starting point to find similar items.
Build an FP-tree this, you need to sort each set before it’s added to the tree.
Now that you have an idea of how to go from a transaction dataset to an FP-tree, let’s write some code to create the tree.
Open fpGrowth.py and add the code from the following listing.
Table 12.2 Transaction dataset with infrequent items removed and items reordered.
To grow the FP-tree (this is where the growth in FP-growth comes from), you call updateTree with an itemset.
Build an FP-tree first tests if the first item in the transaction exists as a child node.
If the item doesn’t exist, it creates a new treeNode and adds it as a child.
At this time, the header table is also updated to point to this new node.
The header table is updated with the updateHeader() function, which we’ll discuss next.
The last thing updateTree() does is recursively call itself with the first element in the list removed.
The last function in listing 12.2 is updateHeader(), which makes sure the node links point to every instance of this item in the tree.
You start with the first nodeLink in the header table and then follow the nodeLinks until you find the end.
When working with trees, the natural reaction is to do everything recursively.
This can get you in trouble when working with a linked list because if the list is long enough, you’ll hit the limits of recursion.
Before you can run this example, you need a dataset.
You can get this from the code repo, or you can enter it by hand.
These are the same as the transactions in table 12.1
The createTree() function will be used later when you’re mining the tree so it doesn’t take the input data as lists.
It expects a dictionary with the itemsets as the dictionary keys and the frequency as the value.
A createInitSet() function does this conversion for you, so add these to fpGrowth.py, as shown in the following listing.
After you’ve entered the code from listing 12.3 to fpGrowth.py, enter the following in your Python shell:
You can display a text representation of the tree with the disp() method:
The item and its frequency count are displayed with indentation representing the depth of the tree.
Verify that this tree is the same as the one in figure 12.2
Now that you’ve created the FP-tree, it’s time to mine it for the frequent items.
We won’t be writing as much code as we did in section 12.1
Now that you have the FP-tree, you can extract the frequent itemsets.
You’ll follow something similar to the Apriori algorithm where you start with the smallest sets containing one item and build larger sets from there.
But you’ll do this with the FP-tree, and you’ll no longer need the original dataset.
There are three basic steps to extract the frequent itemsets from the FP-tree, as follows:
Now, you’ll focus on the first step, which is finding the conditional pattern base.
After that you’ll create conditional FP-trees from each of the conditional pattern bases.
You’ll finally write a little code to wrap these two functions together and get the frequent itemsets from the FP-tree.
You’ll start with the single items you found to be frequent in the last section.
For each of these items, you’ll get the conditional pattern base.
The conditional pattern base is a collection of paths that end with the item you’re looking for.
In short, a prefix path is anything on the tree between the item you’re looking for and the tree root.
The prefix paths will be used to create a conditional FP-tree, but don’t worry about that for now.
To get these prefix paths, you could exhaustively search the tree until you hit your desired frequent item, or you could use a more efficient method.
The more efficient method you’ll use takes advantage of the header table created earlier.
The header table is the starting point for a linked list containing items of the same type.
Once you get to each item, you can ascend the tree until you hit the root node.
Code to find the prefix paths is shown in the following listing.
Listing 12.4 A function to find all paths ending with a given item.
The code in listing 12.4 is used to generate a conditional pattern base given a single item.
This is accomplished by visiting every node in the tree that contains the given item.
When you were creating the tree, you used the header table to point to the first item of this type and successive item to link together.
The findPrefixPath() function iterates through the linked list until it hits the end.
For each item it encounters, it calls ascendTree(), which ascends the tree, collecting the names of items it encounters.
Let’s see this in action with the tree you made earlier:
Check to see if these values match the values in table 12.3
Now that you have the conditional pattern bases, we can move on to creating conditional FP-trees.
In figure 12.4, note that items s and r are part of the conditional pattern bases, but they don’t make it to the conditional FP-tree.
The process of creating conditional trees and prefix paths and conditional bases sounds complex, but the code to do this is relatively simple.
The code starts by sorting the items in the header table by their frequency of occurrence.
Next, you recursively call findPrefixPath() from listing 12.4, to create a conditional base.
This conditional base is treated as a new dataset and fed to createTree()
Finally, if the tree has any items in it, you’ll recursively call mineTree()
Let’s see the code from listing 12.5 in action by putting the whole program together.
After you’ve added the code from listing 12.5 to fpGrowth.py, save the file, and enter the following your Python shell:
Now create an empty list to store all the frequent itemsets:
Now, run mineTree() and all the conditional trees will be displayed.
To get the output like the previous code, I added two lines to mineTree():
These were added in the last if statement: if myHead != None: before call to the mineTree function.
Now, let’s check to see if the itemsets returned matched the condition trees:
The itemsets match the conditional FP-trees, which is what you’d expect.
Now that you have the full FP-growth algorithm working, let’s try it out on a real-world example.
You’ll see if you can get some common words from the microblogging site, Twitter.
As you may have guessed, it allows you to access the microblogging site, Twitter, with Python.
If you aren’t familiar with Twitter.com, it’s a channel for communicating with others.
The documentation for the Twitter API can be found at http://dev.twitter.com/doc.
The keywords aren’t exactly the same between the API documentation and the Python module.
I recommend looking at the Python file twitter.py to fully understand how to use the library.
You’ll use only one small portion of the library, but you can do much more with the API, so I encourage you to explore all functionality of the API.
You need two sets of credentials before you can start using the API.
These keys are specific to the app you’re going to be writing.
This is a command-line Python script that uses OAuth to tell Twitter that this application has the right to post on behalf of this user.
Once that’s done, you can put those values into the previous code and get moving.
You’re going to use the FP-growth algorithm to find frequent words in tweets for a given search term.
You’ll retrieve as many tweets as you can (1,400) and then put the tweets through the FP-growth algorithm.
Prepare: Write a function to remove URLs, remove punctuation, convert to lowercase, and create a set of words from a string.
Analyze: We’ll look at the prepared data in the Python shell to make sure it’s correct.
Train: We’ll use createTree() and mineTree(), developed earlier in this chapter, to perform the FP-growth algorithm.
You could do sentiment analysis or provide search query suggestion.
There are three library imports you’ll need to add: one for the twitter library, one for regular expressions, and the sleep function.
You’ll use the regular expressions to help parse the text later.
The getLotsOfTweets() function handles authentication and then creates an empty list.
The search API allows you to get 100 tweets at a time.
Each of those is considered a page, and you’re allowed 14 pages.
After you make the search call, there’s a six-second sleep to be polite by not making too many requests too quickly.
There’s also a print statement to let you know the program is still running and not dead.
I’m going to search for a stock symbol named RIMM:
As you can see, some people put URLs in the tweets, and when you parse them, you’ll get a mess.
You need to remove URLs, so you can get at the words in the tweet.
We’ll now write some code to parse the tweets into a list of strings, and a function to run the FP-growth algorithm on the dataset.
This calls the regular expression module and removes any URLs.
The other function in listing 12.7, mineTweets(), calls textParse on every tweet.
Lastly, mineTweets() wraps up some commands we used in section 12.2 to build the FP-tree and mine it.
The day before I wrote this, a company that trades under the RIMM ticker symbol had a conference call that didn’t please investors.
The stock opened 22% lower than it had closed the previous day.
It would be interesting to try out some other values for minSupport and some other search terms.
Recall that the FP-trees are built by applying one instance at a time.
But you assume that all the data is present and you iterate over all the available data.
You could rewrite the createTree() function to take in one instance at a time and grow the tree with inputs from the Twitter stream.
There’s a good map-reduce version of FP-growth that can be used to scale this to multiple machines.
Google has used it to find frequent cooccurring words, running a large body of text through it, similar to the example we did here.2
In the source repository is a file called kosarak.dat, which contains close to one million records.3 Each line of this file contains news stories viewed by a user.
Some users viewed only a single story, whereas someone viewed 2,498 stories.
The users and the stories are anonymized as integers, so there won’t be much you can get from viewing the frequent itemsets, but this does a good job of demonstrating the speed of the FP-growth algorithm.
Now, create the FP tree and look for stories or sets of stories that at least 100,000 people viewed:
Creating this tree and scanning the one million lines took only a few seconds on my humble laptop.
Now you need to create an empty list to hold the frequent itemsets:
Let’s see how many stories or sets of stories were viewed by 100,000 or more people:
Try this out with some other settings, perhaps lowering the support level.
The FP-growth algorithm is an efficient way of finding frequent patterns in a dataset.
The FP-growth algorithm works with the Apriori principle but is much faster.
The Apriori algorithm generates candidate itemsets and then scans the dataset to see if they’re frequent.
FP-growth is faster because it goes over the dataset only twice.
The dataset is stored in a structure called an FP-tree.
After the FP-tree is built, you can find frequent itemsets by finding conditional bases for an item and building a conditional FP-tree.
This process is repeated, conditioning on more items until the conditional FPtree has only one item.
The FP-growth algorithm can be used to find frequent words in a series of text documents.
The microblogging site Twitter provides a number of APIs for developers to use their services.
Applying the FP-growth algorithm to a Twitter feed on a certain topic can give you some summary information for that topic.
There are a number of other uses for frequent itemset generation such as shopping transactions, medical diagnosis, and study of the atmosphere.
In the next few chapters we’ll be looking at some additional tools.
You can use these techniques to distill your data down to only the important information and remove the noise.
Chapter 15 will cover MapReduce, which you’ll need when your data exceeds the processing abilities of one machine.
This fourth and final part of Machine Learning in Action covers some additional tools that are commonly used in practice and can be applied to the material from the first three parts of the book.
This part also covers map reduce, which is a technique for distributing jobs to thousands of machines.
Dimensionality reduction is the task of reducing the number of inputs you have; this can reduce noise and improve the performance of machine learning algorithms.
Chapter 13 is the first chapter on dimensionality reduction; we look at principal component analysis, an algorithm for realigning our data in the direction of the most variance.
Chapter 14 is the second chapter on dimensionality reduction; we look at the singular value decomposition, which is a matrix factorization technique that you can use to approximate your original data and thereby reduce its dimensionality.
Chapter 15 is the final chapter in this book, and it discusses machine learning on big data.
The term big data refers to datasets that are larger than the main memory of the machine you’re using.
If you can’t fit the data in main memory, you’ll waste a lot of time moving data between memory and a disk.
To avoid this, you can split a job into multiple segments, which can be performed in parallel on multiple machines.
One popular method for doing this is map reduce, which breaks jobs into map tasks and reduce tasks.
Some common tools for doing map reduce in Python are discussed in chapter 15, along with a discussion of how to break up machine learning algorithms to fit the map reduce paradigm.
Assume for a moment that you’re watching a sports match involving a ball on a flat monitor, not in person.
The monitor probably contains a million pixels, and the ball is represented by, say, a thousand pixels.
In most sports, we’re concerned with the position of the ball at a given time.
For your brain to follow what’s going on, you need to follow the position of the ball on the playing field.
Behind the scene, you’re converting the million pixels on the monitor into a three-dimensional image showing the ball’s position on the playing field, in real time.
You’ve reduced the data from one million dimensions to three.
In this sports match example, you’re presented with millions of pixels, but it’s the ball’s three-dimensional position that’s important.
You’re reducing data from more than one million values to the three relevant values.
It’s much easier to work with data in fewer dimensions.
In addition, the relevant features may not be explicitly presented in the data.
Often, we have to identify the relevant features before we can begin to apply other machine learning algorithms.
This chapter is the first of two that cover dimensionality reduction.
After we’ve preprocessed the data, we can proceed with other machine learning techniques.
This chapter begins with a survey of dimensionality reduction techniques and then moves to one of the more common techniques called principal component analysis.
We’ll next work through an example showing how principal component analysis can be used to reduce a dataset from 590 features to six.
Throughout this book one of the problems has been displaying data and results because the book is only two dimensional, but our data frequently isn’t.
Sometimes, we can show three-dimensional plots or show only the relevant features, but frequently we have more features than we can display.
Displaying data isn’t the only problem with having a large number of features.
A short list of other reasons we want to simplify our data includes the following:
There are dimensionality reduction techniques that work on labeled and unlabeled data.
Here we’ll focus on unlabeled data because it’s applicable to both types.
The first method for dimensionality reduction is called principal component analysis (PCA)
In PCA, the dataset is transformed from its original coordinate system to a new coordinate system.
The new coordinate system is chosen by the data itself.
The first new axis is chosen in the direction of the most variance in the data.
The second axis is orthogonal to the first axis and in the direction of an orthogonal axis with the largest variance.
This procedure is repeated for as many features as we had in the original data.
We’ll find that the majority of the variance is contained in the first few axes.
Therefore, we can ignore the rest of the axes, and we reduce the dimensionality of our data.
In factor analysis, we assume that some unobservable latent variables are generating the data we observe.
The data we observe is assumed to be a linear combination of the latent variables and some noise.
The number of latent variables is possibly lower than the amount of observed data, which gives us the dimensionality reduction.
Factor analysis is used in social sciences, finance, and other areas.
Another common method for dimensionality reduction is independent component analysis (ICA)
The data is assumed to be a mixture of observations of the sources.
The sources are assumed to be statically independent, unlike PCA, which assumes the data is uncorrelated.
As with factor analysis, if there are fewer sources than the amount of our observed data, we’ll get a dimensionality reduction.
Of the three methods of dimensionality reduction, PCA is by far the most commonly used.
We’ll focus on PCA in this chapter, and we won’t cover ICA or factor analysis.
In the next section, I’ll describe PCA and then you’ll write some code to perform PCA in Python.
We’ll first discuss some of the theory behind PCA, and then you’ll see how to do PCA in Python with NumPy.
Consider for a moment the mass of data in figure 13.1
If I asked you to draw a line covering the data points, what’s the longest possible line you could draw? I’ve drawn a few choices.
The first axis is rotated to cover the largest variation in the data: line B in figure 13.1
The largest variation is the data telling us what’s most important.
After choosing the axis covering the most variability, we choose the next axis, which has the second most variability, provided it’s perpendicular to the first axis.
On this two-dimensional plot, perpendicular and orthogonal are the same.
In figure 13.1, line C would be our second axis.
Principal component analysis Pros: Reduces complexity of data, indentifies most important features.
Cons: May not be needed, could throw away useful information.
Figure 13.1 Three choices for lines that span the entire dataset.
Line B is the longest and accounts for the most variability in the dataset.
Now that you have the axis rotation down, let’s talk about dimensionality reduction.
If we want to separate the classes, we could use a decision tree.
Remember that decision trees make a decision based on one feature at a time.
We could find some values on the x-axis that do a good job of separating the different classes.
The support vector machine may give us better margin than the decision tree, but the hyperplane is harder to interpret.
By doing dimensionality reduction with PCA on our dataset, we can have the best of both worlds: we can have a classifier as simple as a decision tree, while having margin as good as the support vector machine.
In this frame, I took the data from the top frame and plotted it after the PCA.
The margin on this will be larger than the decision tree margin when using only the original data.
Also, because we have only one dimension to worry about, we can have rules to separate the classes that are much simpler than the support vector machine.
In figure 13.2, we have only one axis because the other axis was just noise and didn’t contribute to the separation of the classes.
This may seem trivial in two dimensions, but it can make a big difference when we have more dimensions.
Now that we’ve gone over some of what goes on in PCA, let’s write some code to do this.
Earlier, I mentioned that we take the first principal component to be in the direction of the largest variability of the data.
The second principal component will be in the direction of the second largest variability, in a direction orthogonal to the first principal component.
We can get these values by taking the covariance matrix of the dataset and doing eigenvalue analysis on the covariance matrix.
Once we have the eigenvectors of the covariance matrix, we can take the top N eigenvectors.
The top N eigenvectors will give us the true structure of the N.
When the PCA is applied to this dataset, we can throw out one dimension, and the classification problem becomes easier.
We can then multiply the data by the top N eigenvectors to transform our data into the new space.
Pseudocode for transforming out data into the top N principal components would look like this:
Remove the mean Compute the covariance matrix Find the eigenvalues and eigenvectors of the covariance matrix Sort the eigenvalues from largest to smallest Take the top N eigenvectors Transform the data into the new space created by the top N eigenvectors.
Create a file called pca.py and add the code from the following listing to compute the PCA.
The code in listing 13.1 contains the usual NumPy import and loadDataSet()
The function loadDataSet() is slightly different from the versions we used in previous chapters because it uses two list comprehensions to create the matrix.
The pca() function takes two arguments: the dataset on which we’ll perform PCA and the second optional argument, topNfeat, which is the top N features to use.
If you don’t provide a value for topNfeat, it will return the top 9,999,999 features, or as many as the original dataset has.
First, you calculate the mean of the original dataset and remove it.
You use argsort() to get the order of the eigenvalues.
You can now use the order of the eigenvalues to sort the eigenvectors in reverse order and get the topNfeat largest eigenvectors.
That wasn’t bad, was it? Let’s take a look at this in action to make sure you have it right before we get into a big example.
I included a dataset with 1000 points in the testSet.txt file.
The reconstructed data should overlap the original data because no features are removed.
You should see something similar to figure 13.3 but without the straight line.
Now that we have PCA working on a simple dataset, let’s move to a real-world example.
We’ll reduce the dimensionality of a dataset from a semiconductor factory.
The factories or fabrications (fabs) cost billions of dollars and take an army to operate.
The fab is only modern for a few years, after which it needs to be replaced.
The processing time for a single integrated circuit takes more than a month.
With a finite lifetime and a huge cost to operate, every second in the fab is extremely valuable.
If there’s some flaw in the manufacturing process, we need to know as soon as possible, so that precious time isn’t spent processing a flawed product.
Some common engineering solutions find failed products, such as test early and test often.
If machine learning techniques can be used to further reduce errors, it will save the manufacturer a lot of money.
We’ll now look at some data for such a task.
These values are recorded as NaN, which stands for Not a Number.
We can do several things to work with the missing values (see chapter 5)
With 590 features, almost every instance has a NaN, so throwing out incomplete instances isn’t a realistic approach.
We could replace all the NaNs with 0s, but that may be a bad idea because we don’t know what these values mean.
If they’re things like temperature in kelvins, setting the values to zero is a bad idea.
Let’s try to set the missing values to the mean.
We’ll calculate the mean from the values that aren’t NaN.
The code in listing 13.2 opens the dataset and counts the number of features.
For each feature, you first find the mean value, where there are values to measure.
Now that you have the NaN values removed, you can look at the PCA of this dataset.
First, let’s find out how many features you need and how many you can drop.
I’d like to emphasize that there’s a major difference between data and information.
Data is the raw material that you take in, which may contain noise and irrelevant information.
These aren’t just abstract quantities; you can measure the amount of information contained in your data and decide how much to keep.
First, replace the NaN values in the dataset with mean values using the code we just wrote:
Next, borrow some code from the pca() function because we want to look at the intermediate values, not the output.
You see a lot of values, but what do you notice? Did you notice there are a lot of zeros? Over 20% of the eigenvalues are zero.
That means that these features are copies of other features in the dataset, and they don’t provide any extra information.
Second, let’s look at the magnitude of some of these numbers.
This tells you that there are a few important features, but the number of important features drops off quickly.
These are caused by numerical errors and should be rounded to zero.
I’ve plotted the percentage of total variance in figure 13.4
You can see how quickly the variance drops off after the first few principal components.
From this plot, you can see that most of the variance is contained in the first few principal components, and little information would be lost by dropping the higher ones.
I also recorded the percentage of variance and the cumulative percentage of variance for these principal components in table 13.1
Additionally, dropping the higher principal components may make the data cleaner because we’re throwing out noisy components.
Now that you know how much of the information in our dataset is contained in the first few principal components, you can try some cutoff values and see how they perform.
I can’t tell you exactly how many principal components to use.
The number of effective principal components will depend on your dataset and your application.
The analysis tells you how many principal components you can use.
You can then plug this number into the PCA algorithm, and you’ll have reduced data for use in a classifier.
Dimensionality reduction techniques allow us to make data easier to use and often.
It’s often a preprocessing step that can be done to clean up data before applying it to some other algorithm.
A number of techniques can be used to reduce the dimensionality of our data.
Among these, independent component analysis, factor analysis, and principal component analysis are popular methods.
Principal component analysis allows the data to identify the important features.
It does this by rotating the axes to align with the largest variance in the data.
Other axes are chosen orthogonal to the first axis in the direction of largest variance.
Eigenvalue analysis on the covariance matrix can be used to give us a set of orthogonal axes.
The PCA algorithm in this chapter loads the entire dataset into memory.
If this isn’t possible, other methods for finding the eigenvalues can be used.
Restaurants get rolled into a handful of categories: American, Chinese, Japanese, steak house, vegan, and so on.
Have you ever thought that these categories weren’t enough? Perhaps you like a hybrid of these categories or a subcategory like Chinese vegetarian.
How can we find out how many categories there are? Maybe we could ask some human experts? What if one expert tells us we should divide the restaurants by sauces, and another expert tells us we should divide restaurants by the ingredients? Instead of asking an expert, let’s ask the data.
We can take data that records people’s opinions of restaurants and distill it down into underlying factors.
These may line up with our restaurants categories, a specific ingredient used in cooking, or anything.
We can then use these factors to estimate what people will think of a restaurants they haven’t yet visited.
The method for distilling this information is known as the singular value decomposition (SVD)
It’s a powerful tool used to distill information in a number of applications, from bioinformatics to finance.
In this chapter, you’re going to learn what the singular value decomposition is and how it can be used to reduce the dimensionality of our data.
You’ll then see how to do the SVD in Python and how to map our data from the low-dimensional space.
Next, you’ll learn what recommendation engines are and see them in action.
You’ll see how you can apply the SVD to recommendation engines to improve their accuracy.
We’ll use this recommendation engine to help people find a restaurant to visit.
We’ll conclude by looking at an example of how the SVD could be used for image compression.
We can use the SVD to represent our original data set with a much smaller data set.
When we do this, we’re removing noise and redundant information.
Those are noble goals when we’re trying to save bits, but we’re trying to extract knowledge from data.
When viewed from that perspective, we can think of the SVD as extracting the relevant features from a collection of noisy data.
I’m going to show a few examples of where and how this is used to explain the power of the SVD.
First, we’ll discuss how the SVD is used in search and information retrieval using latent semantic indexing.
Next, we’ll discuss how the SVD is used in recommendation systems.
The history of the SVD is over a century old.
But it has found more use with the adoption of computers in the last several decades.
One of first uses was in the field of information retrieval.
The method that uses SVD is called latent semantic indexing (LSI) or latent semantic analysis.
In LSI, a matrix is constructed of documents and words.
When the SVD is done on this matrix, it creates a set of singular values.
The singular values represent concepts or topics contained in the documents.
This was developed to allow more efficient searching of documents.
A simple search that looks only for the existence of words.
The singular value decomposition (SVD) Pros: Simplifies data, removes noise, may improve algorithm results.
Another problem with a simple search is that synonyms may be used, and looking for the existence of a word wouldn’t tell you if a synonym was used to construct the document.
If a concept is derived from thousands of similar documents, both of the synonyms will map to the same concept.
Simple versions of recommendation systems compute similarity between items or people.
More advanced methods use the SVD to create a theme space from the data and then compute similarities in the theme space.
Consider for a moment the matrix of restaurant dishes and reviewers’ opinions of these dishes in figure 14.1
If we did the SVD of this matrix, we’d have noticed two singular values.
So there appears to be two concepts or themes associated with the dataset.
Let’s see if we can figure out what these concepts are by looking for the 0s in the figure.
It looks like Ed, Peter, and Tracy rated Tri Tip and Pulled Pork, but it also appears that these three people didn’t rate any other dishes.
We can think of the singular values as a new space.
Instead of being five or seven dimensional like the matrix in figure 14.1, our matrix is now two dimensional.
What are these two dimensions, and what can they tell us about the data? The two dimensions would correspond to the two groupings in the figure.
I shaded one of the groups in the matrix on the right.
We could name these two dimensions after the common features of the groups.
We’d have an American BBQ dimension and a Japanese food dimension.
The SVD of this matrix can condense the data into a few concepts.
One concept is shaded in gray on the right side.
How can we get from our original data to this new space? In the next section we’ll discuss the SVD in more detail and see how it gives us two matrices called U and VT.
The VT matrix maps from users into the BBQ/Japanese food space.
Similarly the U matrix maps from the restaurant dishes into the BBQ/Japanese food space.
Real data usually isn’t as dense or as well formatted as the data in figure 14.1
A recommendation engine can take noisy data, such someone’s rating of certain dishes, and distill that into these basic themes.
With respect to these themes, the recommendation engine can make better recommendations than using the original data set.
In the next section we’ll discuss some background material leading up to the SVD, and we’ll show how to perform the SVD in Python with NumPy.
When you have a good understanding of recommendations engines, we’ll build a recommendation engine that uses the SVD.
The SVD is a type of matrix factorization, which will break down our data matrix into separate parts.
Often, a few pieces of data in our dataset can contain most of the information in our dataset.
The other information in the matrix is noise or irrelevant.
In linear algebra, there are many techniques for decomposing matrices.
The decomposition is done to put the original matrix in a new form that’s easier to work with.
The new form is a product of two or more matrices.
This decomposition can be thought of like factoring in algebra.
Don’t worry about how we’re going to break down this matrix.
NumPy’s linear algebra library has a method for doing the SVD, which you’ll see in the next section.
If you’re interested in how to program the SVD, I’d suggest you check the book Numerical Linear Algebra.2
If the SVD is so great, how can we do it? The linear algebra of doing it is beyond the scope of this book.
There are a number of software packages that will do the factorization for us.
Let’s see this in action to do the SVD on the matrix.
To do this in Python, enter the following commands in your Python shell:
This is done internally by NumPy because the matrix is all zeros except for the diagonal elements, so it saves space to return just the diagonal elements.
Keep that in mind when you see Sigma as a vector.
OK, let’s do some more decomposing, this time on a bigger matrix.
Create a new file called svdRec.py and enter the following code:
After you’ve saved svdRec.py, enter the following at the Python prompt:
The first three values are much greater than the others in value.
Don’t worry if the last two values are slightly different from the ones listed here; they’re so small that running this on different machines will produce slightly different results.
The order of magnitude should be similar to those listed here.
Now our original data set is approximated by the following:
A schematic representation of this approximation can be seen in figure 14.2
The light gray areas show the original data, and the dark gray areas show the only data used in the matrix approximation.
How did we know to keep only the first three singular values? There are a number of heuristics for the number of singular values to keep.
You typically want to keep 90% of the energy expressed in the matrix.
To calculate the total energy, you add up all the squared singular values.
You can then add squared singular values until you reach 90% of the total.
This is a little less elegant than the energy method, but it’s easier to implement in practice.
Usually you’ll know your data well enough that you can make an assumption like this.
We’ve now approximated the original matrix closely with three matrices.
We can represent a big matrix with a much smaller one.
There are a number of applications that can be improved with the SVD.
I’ll discuss one of the more popular uses, recommendation engines, next.
Recommendation engines are nothing new to people who’ve been using the internet in the last decade.
Amazon recommends items to customers based on their past purchases.
There are a number of approaches for how to do this, but the approach we’re going to use is called collaborative filtering.
Collaborative filtering works by taking a data set of users’ data and comparing it to the data of other users.
The data is conceptually organized in a matrix like the one in figure 14.2
When the data is organized this way, you can compare how similar users are or how similar items are.
Both approaches use a notion of similarity, which we’ll discuss in more detail in a moment.
When you know the similarity between two users or two items, you can use existing data to forecast unknown preferences.
The recommendation engine will see that there’s a movie you haven’t viewed yet.
It will then compute the similarity between the movies you did see and the movie you didn’t see.
If there’s a high similarity, the algorithm will infer that you’ll like this movie.
The only real math going on behind the scenes is the similarity measurement, and that isn’t difficult, as you’ll see next.
We’ll first talk about how the similarity between items is measured.
Next, we’ll discuss the tradeoffs between item-based and user-based similarity measurements.
Finally, we’ll discuss how to measure the success of a recommendation engine.
We’d like to have some quantitative measurement of how similar two items are.
How would you find that? What if you ran a food-selling website? Maybe you can compare food by ingredients, calorie count, someone’s definition of the cuisine type, or something similar.
Now let’s say you wanted to expand your business into eating utensils.
Would you use calorie count to describe a fork? The point is that the attributes you use to describe a food will be different from the attributes you use to describe tableware.
What if you took another approach at comparing items? Instead of trying to describe the similarity between items based on some attributes that an expert tells you are important, you compare the similarity by what people think of these items.
It doesn’t care about the attributes of the items; it compares similarity strictly by the opinions of many users.
Figure 14.3 contains a matrix of some users and ratings they gave some of the dishes mentioned earlier in the chapter.
This is the correlation that we used in chapter 8 to measure the accuracy of our regression equations.
One benefit of this over the Euclidian distance is that it’s insensitive to the magnitude of users’ ratings.
The Pearson correlation tells us that these two vectors are equal.
The Pearson correlation is built into NumPy as the function corrcoef()
The cosine similarity measures the cosine of the angle between two vectors.
To calculate this, we can take the definition of the cosine of two vectors A and B as.
The two parallel lines around A and B means the L2 norm of the vectors.
You can have a norm of any number, but if no number is given, it’s assumed to be the L2 norm.
Once again the linear algebra toolbox in NumPy can compute the norm for you with linalg.norm()
The three functions are the three similarity measures we just discussed.
The NumPy linalg (Linear Algebra) toolbox is imported as la to make the code more readable.
The function assumes that inA and inB are column vectors.
The perasSim() function checks to see if there are three points or more.
If not, it returns 1.0 because the two vectors are perfectly correlated.
After you’ve saved svdRec.py, enter the following in your Python shell:
The Euclidian similarity measure seems to work; now let’s try the cosine:
All of these metrics assumed the data was in column vectors.
We’ll have problems if we try to use row vectors with these functions.
It would be easy to change the functions to compute on row vectors.
Column vectors imply that we’re going to use item-based similarity.
A second method that compares users is known as user-based similarity.
If you refer to figure 14.3, comparing rows (users) is known as user-based similarity; comparing columns is known as item-based similarity.
Which one should you use? The choice depends on how many users you may have or how many items you may have.
Item-based scales with the number of items, and user-based scales with the number of users you have.
If you have something like a store, you’ll have a few thousand items at the most.
The biggest stores at the time of writing have around 100,000 items.
If you have a lot of users, then you’ll probably want to go with item-based similarity.
For most product-driven recommendation engines, the number of users outnumbers the number of items.
There are more people buying items than unique items for sale.
How can we evaluate a recommendation engine? We don’t have a target value to predict, and we don’t have the user here to ask if our prediction is right or wrong.
We can do a form of cross-validation that we’ve done multiple times in other problems.
The way we do that is to take some known rating and hold it out of the data and then make a prediction for that value.
We can compare our predicted value with the real value from the user.
Usually the metric used to evaluate a recommendation engine is root mean squared error (RMSE)
This metric computes the mean of the squared error and then takes the square root of that.
If you’re rating things on a scale of one to five stars and you have an RMSE of 1.0, it means that your predictions are on average one star off of what people really think.
The topic we’re going to apply this to is restaurant food.
Say you’re sitting at home and you decide to go out to eat, but you don’t know where you should go or what you should order.
We’re first going to create the basic recommendation engine, which looks for things you haven’t yet tried.
The second step is to improve our recommendations by using the SVD to reduce the feature space.
We’ll then wrap up this program with a human-readable UI so that people can use it.
The recommendation engine will work like this: given a user, it will return the top N best recommendations for that user.
Look for things the user hasn’t yet rated: look for values with 0 in the user-item matrix.
Sort the list in descending order and return the first N items.
Open your svdRec.py and add the code from the following listing.
The first one, called standEst(), calculates the estimated rating a user would give an item for a given similarity measure.
The second function, called recommend(), is the recommendation engine, and it calls standEst()
The function standEst() takes a data matrix, a user number, an item number, and a similarity measure as its arguments.
You first get the number of items in the dataset, and then you initialize two variables that will be used to calculate an estimated rating.
If an item is rated 0, it means that this user has not rated it, and you’ll skip it.
The big picture of this loop is that you’re going to loop over every item that the user has rated and compare it with other items.
The variable overLap captures the elements that have been rated between two items.
But if there are overlapping items, you calculate the similarity based on the overlapping items.
This similarity is then accumulated, along with the product of the similarity and this user’s rating.
Finally, you normalize the similarity rating product by dividing it by the sum of all the ratings.
The other arguments to this function are a similarity measurement and an estimation method.
You can use any of the similarity measurements from listing 14.1
Right now you have only one option for the estimation method, but you’ll add another one in the next subsection.
The first thing you do is create a list of unrated items for a given user.
For each unrated item, it calls stanEst(), which generates a forecasted score for that item.
The item’s index and the estimated score are placed in a list of tuples called itemScores.
Finally, this list is sorted by the estimated score and returned.
After you’ve saved svdRec.py, enter the following into your Python shell:
You can use the same matrix from earlier in the chapter with a few modifications.
This matrix was great for illustrating the SVD, but it’s not that interesting, so let’s alter a few values:
Try this out with multiple users, and change the data set a little to see how it changes the results.
This example illustrates how the recommendations are done using item-based similarity and a number of similarity measures.
You’ll now see how you can apply the SVD to your recommendations.
Real data sets are much sparser than the version of myMat we used to demonstrate the recommend() function.
The presence of the many unrated items is more realistic than a completely filled-in matrix.
You can enter this matrix or you can copy the function loadExData2() from the code download.
Now let’s compute the SVD of this matrix to see how many dimensions you need.
Now, let’s find the number of singular values that give you 90% of the total energy.
Let’s see how much energy is contained in the first two elements:
Now let’s create a function to calculate similarities in our 3-dimensional space.
We’re going to use the SVD to map our dishes into a lowerdimensional space.
In the lower-dimensional space, we’ll make recommendations based on the same similarity metrics we used earlier.
We’ll create a function similar to standEst() in listing 14.2
Open svdRec.py and add the code from the following listing.
This will be used in place of standEst() when you call recommend()
This function creates an estimated rating for a given item for a given user.
If you compare it to standEst() in listing 14.2, you’ll see that many of the lines are similar.
Something unique to this function is that it does an SVD on the dataset on the third line.
After the SVD is done, you use only the singular values that give you 90% of the energy.
The singular values are given to you in the form of a NumPy array, so to do matrix math you need to build a diagonal matrix with these singular values on the diagonal.
The for loop iterates over all the elements in a row for a given user.
This serves the same purpose as the for loop in standEst() except you’re calculating the similarities in a lower dimension.
The similarity measure used is passed into this function as an argument.
Next, you sum up the similarities and the product of the similarities and the rating that this user gave this item.
I’ve included one print statement in the for loop so you can see what’s going on with the similarity measurements.
You can comment it out if the output gets annoying.
After you’ve entered the code from listing 14.3, save svdRec.py and enter the following in your Python shell:
The code in this section works and does an effective job of demonstrating how recommendation engines work and how the SVD distills data into its essential components.
I wrote the code to be as easy to understand as possible, not necessarily the most efficient.
For one thing, you don’t need to do the SVD every time you want a projected value.
The SVD could be done once when the program is launched.
On large systems, the SVD is done once a day or less often and is done offline.
There are a number of other scaling challenges, such as representing our matrix.
The matrix in this example had a lot of 0s, and in a real system it would have many more.
Perhaps we could save some memory and computations by storing only the nonzero values? Another potential source of computing waste is the similarity scores.
In our program, we calculated the similarity scores for multiple items each time we wanted a recommendation score.
The scores are between items, so we reuse them if another user needs them.
Another thing commonly done in practice is to compute the similarity scores offline and store them.
Another problem with recommendation engines is how to make good recommendations with no data.
This is known as the cold-start problem and can be difficult.
Another way to phrase this problem is users won’t like this unless it works, and it won’t work unless users like it.
If recommendations are a nice-to-have feature, then this may not be a big problem, but if the success of your application is linked to the success of the recommendations, then this is a serious problem.
One solution to the cold-start problem is to treat recommendations as a search problem.
Under the hood these are different solutions, but they can be presented to the user in a transparent manner.
To treat these recommendations as a search problem you could use properties of the items you’re trying to recommend.
In our restaurant dish example we could tag dishes along a number of parameters such as vegetarian, American BBQ, expensive, and so on.
You could also treat these properties as data for our similarity calculations.
In this last section you’ll see a great example of how the SVD can be used for image compression.
This example allows you to easily visualize how well the SVD is approximating our data.
In the code repository, I included a handwritten digit image.
Can we represent the same image with fewer numbers? If we’re able to compress an image, we can save disk space or bandwidth.
We can use the SVD to reduce the dimensionality of the data and compress an image.
The following listing contains some code for reading the digit and compressing it.
The first function in listing 14.4, printMat(), prints a matrix.
The matrix will have floating point values, so you need to define what’s light and what’s dark.
It allows you to reconstruct an image with any given number of singular values.
This creates a list and then opens the text file and loads the characters from the file as numeric values.
After the matrix is loaded, you print it to the screen.
Next, you take the SVD of the original image and reconstruct the image.
Sigma is a diagonal matrix, so you create a matrix with all 0s and then fill in the diagonal elements with the first singular values.
Finally, SigRecon is used with the truncated U and VT matrices to build the reconstructed matrix.
With as few as two singular values, the image is reconstructed quite accurately.
How many numbers did we use to reconstruct this image? Each of the U and VT matrices was 32x2, and there were two singular values.
The singular value decomposition (SVD) is a powerful tool for dimensionality reduction.
You can use the SVD to approximate a matrix and get out the important features.
The SVD is employed in a number of applications today.
Collaborative filtering is one way of creating recommendations based on data of users’ preferences or actions.
At the heart of collaborative filtering is a similarity metric.
A number of similarity metrics can be used to calculate the similarity between items or users.
The SVD can be used to improve recommendation engines by calculating similarities in a reduced number of dimensions.
Calculating the SVD and recommendations can be a difficult engineering problem on massive datasets.
Taking the SVD and similarity calculations offline is one method of reducing redundant calculations and reducing the time required to produce a recommendation.
In the next chapter, we’ll discuss some tools for working with massive datasets and doing machine learning on these datasets.
With so many devices connected to the internet and people interested in making data-driven decisions, the amount of data we’re collecting has outpaced our ability to process it.
Fortunately, a number of open source software projects allow us to process large amounts of data.
One project, called Hadoop, is a Java framework for distributing data processing to multiple machines.
Imagine for a second that you work for a store that sells items on the internet, and you get many visitors—some purchasing items, some leaving before they purchase items.
You’d like to be able to identify the ones who make purchases.
How do you do this? You can look at the web server logs and see what pages each person went to.
Perhaps some other actions are recorded; if so, you can train a classifier on Big data and MapReduce.
I often hear “Your examples are nice, but my data is big, man!” I have no doubt that.
The only problem is that this dataset may be huge, and it may take multiple days to train this classifier on a single machine.
This chapter will show you some tools you can use to solve a problem like this: Hadoop and some Python tools built on top of Hadoop.
Hadoop is a free, open source implementation of the MapReduce framework.
You’re going to first learn what MapReduce and the Hadoop project are.
You’ll next see how you can write MapReduce jobs in Python.
You’ll test these jobs on a single machine, and then you’ll see how you can use Amazon Web Services to run full Hadoop jobs on many machines at one time.
Once you’re comfortable running MapReduce jobs, we’ll discuss common solutions to do machine learning jobs in MapReduce.
You’ll then see a framework for automating MapReduce jobs in Python called mrjob.
Finally, you’ll write a distributed SVM with mrjob that could be used to train a classifier on multiple machines.
MapReduce is a software framework for spreading a single computing job across multiple computers.
It’s assumed that these jobs take too long to run on a single computer, so you run them on multiple computers to shorten the time.
Some of these jobs are summaries of daily statistics where running them on a single machine would take longer than one day.
MapReduce is done on a cluster, and the cluster is made up of nodes.
MapReduce works like this: a single job is broken down into small sections, and the input data is chopped up and distributed to each node.
The code that’s run on each node is called the mapper, and this is known as the map step.
The output from the individual mappers is combined in some way, usually sorted.
The sorted data is then broken into smaller portions and distributed to the nodes for further processing.
This second processing step is known as the reduce step, and the code.
MapReduce Pros: Processes a massive job in a short period of time.
Cons: Algorithms must be rewritten; requires understanding of systems engineering.
MapReduce: a framework for distributed computing run is known as the reducer.
The output of the reducer is the final answer you’re looking for.
The advantage of MapReduce is that it allows programs to be executed in parallel.
For example, say we want to know the maximum temperature in China from the last 100 years.
Assume we have valid data from each province for each day over this period.
We could break up the data by the number of nodes we have, and each node could look for the maximum among its data.
All mappers would produce the same key, which is a string “max.” We’d then need just one reducer to compare the outputs of the mappers and get our global maximum temperature.
NOTE: At no point do the individual mappers or reducers communicate with each other.
Each node minds its own business and computes the data it has been assigned.
Depending on the type of job, we may require different numbers of reducers.
Now we’d need to make sure all values with the same year go to the same reducer.
This is done in the sort between the map and reduce steps.
This example illustrates another point: the way data is passed around.
In our second Chinese temperature example, the year was the key and the temperature was the value.
We sorted by the year, so we correctly combined similar years; each reducer would receive common key (year) values.
From these examples you may have noticed that the number of reducers isn’t fixed.
There are a number of flexible options in a MapReduce implementation.
The orchestration of this is handled by a master node.
The master node handles the orchestration of the whole MapReduce job, including which data is placed on which node, and it handles the timing of the map, sort, and reduce steps, and so on.
Often, multiple copies of the mapper input data are sent to multiple nodes in the event of a failure.
Consider the schematic representation of a MapReduce cluster in figure 15.1
Each machine in figure 15.1 has two processors and can handle two map or reduce jobs simultaneously.
If Machine 0 was destroyed by a robot airplane during the map phase, the master node would recognize this.
When the master node sensed a failure, it would remove Machine 0 from the cluster and continue with the job.
In some implementations of MapReduce, additional data is stored on each machine.
In some MapReduce implementations, the nodes need to communicate with the master node, indicating that they’re alive and functioning.
Some key points about MapReduce from the previous example are these:
One implementation of the MapReduce framework is the Apache Hadoop project.
In the next section we’ll discuss Hadoop and how you can use it with Python.
The Hadoop project has a large set of features for running MapReduce jobs.
In addition to distributed computing, Hadoop has a distributed filesystem.
This isn’t a Java book, nor is it a Hadoop book.
You’re going to see just enough of Hadoop to run MapReduce jobs in Python.
If you’re interested in knowing more about Hadoop, I suggest you pick up Hadoop in Action2 or read documentation on.
In this example there are three machines with two processors each in the cluster.
The book Mahout in Action3 is also a great source of information if you’re interested in machine learning with MapReduce.
Hadoop has code that allows you to run distributed programs that are written in languages other than Java.
Because this book is written in Python, you’ll write some MapReduce jobs in Python and then run them in Hadoop Streaming.
If you aren’t familiar with pipes, they use the symbol | and take the output from one command and direct it to the input of another command.
If our mapper was called mapper.py and our reducer was called reducer.py, Hadoop Streaming would run something similar to the following Linux command:
In Hadoop Streaming something like this is done over multiple machines.
We can use the Linux command to test our MapReduce scripts written in Python.
We’re going to create a MapReduce job that calculates the mean and variance of a bunch of numbers.
This is for demonstration purposes, so we’ll use a small amount of data.
Create a file called mrMeanMapper.py in your favorite editor, and add in the code from the following listing.
You loop over all the input lines and first create a list of floats.
Next, you get the length of that list and then create a NumPy matrix from that list.
Finally, you send out the mean and the mean of the squared values.
These values will be used to calculate the global mean and variance.
If jobs don’t report something to standard error every 10 minutes, then they’ll get killed in Hadoop.
There’s an input file with 100 numbers in a file called inputFile.txt in the source code download.
You can experiment with the mapper before you even touch Hadoop by typing the following command in a Linux window:
The first line is the standard output, which we’ll feed into the reducer.
The second line, which was written to standard error, will be sent to the master node to report to the master that the node is still alive.
Now that we have the mapper working, let’s work on the reducer.
The mapper took raw numbers and collected them into intermediate values for our reducer.
We’ll have many of these mappers doing this in parallel, and we’ll need to combine all those outputs into one value.
We now need to write the reducer so we can combine the intermediate key value pairs.
These values are then combined to form a global mean and variance, which was the goal of this exercise.
You can practice with this on your local machine by typing the following at the command prompt:
Perhaps you don’t have 10 servers sitting around your house.
You’ll learn where you can rent them by the hour in the next section.
You could buy them yourself or you could rent them from someone else.
Amazon rents out parts of its massive computing infrastructure to developers through Amazon Web Services (AWS) at http://aws.amazon.com/
Storage, bandwidth, and compute power are metered, and you’re billed only for your use of each of these, by the hour with no long-term contracts.
This is what makes AWS so attractive—you pay only for what you use.
Say you have an idea that takes 1,000 computers per day.
You can get set up on AWS and experiment for a few days.
Then, if you decide it’s a bad idea, you can shut it down and you don’t have to pay for those 1,000 computers any longer.
We’ll next talk about a few services currently available on AWS, then we’ll walk through getting set up on AWS, and finally we’ll run a Hadoop Streaming job on AWS.
All of them have names that to the informed seem totally logical and to the uninformed seem totally cryptic.
Some basic, stable services that you’ll be using are these:
S3—Simple Storage Service is used for storing data on the internet and is used in conjunction with other AWS products.
Here you’re renting some sort of storage device, so you pay by the amount of data you store and the length of time you store that data.
EC2—Elastic Compute Cloud is a service for using server instances.
You can configure the server to run almost any operating system.
A server can be started from a machine image in a matter of minutes.
This is built on a slightly older release of Hadoop.
Amazon wanted to have a stable release, and so they made a few modifications, which prevents them from having the bleeding-edge release of Hadoop.
It has a nice GUI and simplifies the setup of Hadoop jobs.
You don’t have to mess with adding files to Hadoop’s filesystem or configuring Hadoop machines.
A large number of other services are available for use.
We’ll focus on EMR, although to use it we need to use S3
To get started with AWS, you need to create an account on AWS.
To use AWS you’ll need a credit card; the exercises in the rest of the chapter will cost around $1 USD.
Follow the instructions on the next three pages to get signed up for Amazon Web Services.
If you aren’t signed up for a service, you’ll see something similar to figure 15.3
You’re now ready to run a Hadoop job on Amazon’s computers.
Once you’ve signed up for all the required Amazon services, log into the AWS console and click the S3 tab.
You’ll need to upload our files to S3 in order for the AWS version of Hadoop to find our files.
Create a new bucket; for example, I created one called rustbucket.
Note: bucket names are unique and shared among all users.
Now create two new folders, one called mrMeanCode and one called mrMeanInput.
You’re going to upload the Python MapReduce files you created earlier to mrMeanCode.
The other folder, mrMeanInput, is where you’ll store the input to your Hadoop job.
Upload the file inputFile.txt to the folder mrMeanInput in your bucket (for example, rustbucket)
Figure 15.3 AWS Console showing a service I’m not signed up for yet.
Upload the files mrMeanMapper.py and mrMeanReducer.py to the folder mrMeanCode in your bucket.
Now that you have all the files uploaded, you’re ready to launch your first Hadoop job on multiple machines.
Below that are two check boxes and a drop-down box.
In this step, you give the input arguments to Hadoop.
Enter the values in the following fields on the Specify Parameters screen (be sure to include the quotes):
This is where you’d specify extra arguments such as restricting the number of reducers.
The next window is the Configure EC2 Instances window, where you specify the number of servers that will crunch your data.
You can also specify the type of EC2 instance you want to use.
You can use a more powerful machine with larger memory, but it will cost more.
In practice, big jobs are usually run on Large (or better) instances.
For this trivial demonstration, you can use one Small machine.
Here you set the size of the servers and number of servers you’ll use on your MapReduce job.
The next screen is Advanced Options, where you can set options for debugging.
You won’t be able to enable Hadoop debugging unless you’ve signed up for SimpleDB.
SimpleDB is an Amazon tool for easy access to nonrelational databases.
We should be able to debug our Hadoop Streaming jobs without it.
When a Hadoop job fails, a lot of information is written to this directory.
If your job does fail, you can go back and read some of this information to see what went wrong.
You can select the defaults on the Bootstrap Actions page and continue to the Review page.
Make sure everything looks right; then click the Create Job Flow button at the bottom.
Click the Close button on the next page, and you’ll be brought back to the Elastic MapReduce console.
As your job runs, its progress will be displayed in this console.
Don’t worry if it takes a while to run such a trivial job.
You probably won’t have as many failed jobs as shown here.
Here you set the path of where the debug files will be placed.
You can also set Keep Alive and a key pair to log into the machine if a job fails.
This is a good idea when you want to inspect the exact environment your code will be run on.
A few minutes after your job has started running, it will finish.
When the S3 console comes up, click the bucket you created earlier (rustbucket in this example)
Double-click this file to download it to your local machine.
This is the same output that we got when testing the job on our local machine using pipes.
Things worked on this job, but if they didn’t work, how would you know what went wrong? Back on the Elastic MapReduce tab, if you click a completed job, you’ll see a Debug button with the cartoon of a small green insect on it.
Clicking this will bring up a debug window, which gives you access to different log files.
Click the Controller link, and you’ll see the Hadoop command.
Now that you have a taste of what it’s like to run a Hadoop Streaming job, we’re going to discuss how to execute machine learning algorithms on Hadoop.
MapReduce is a system that allows you to run many programs on many computers, but these programs need to be written a little differently for MapReduce.
The MapReduce job in this chapter has started in this figure.
Skipping AWS If you don’t want to get your credit card out or are afraid that someone on the internet will steal your information, you can run this same task on a local machine.
If the MapReduce jobs are written properly, it may feel like this.
But you can’t take every program and get an instant speedup.
The map and reduce tasks need to be properly written.
Many machine learning algorithms don’t intuitively fit in a MapReduce framework.
As the old adage goes, “Necessity is the mother of invention.” Inventive scientists and engineers have written MapReduce solutions to almost every popular machine learning algorithm.
The following brief list identifies popular machine learning algorithms from this book and their MapReduce implementations:
In a massive dataset, it can limit daily business cycles.
One approach to speed this up is to build a tree, such as a tree to narrow the search for closest vectors.
A popular method for performing a nearest neighbor search on higher-dimensional items such as text, images, and video is locality-sensitive hashing.
Support vector machines—The Platt SMO algorithm that we used in chapter 6 may be difficult to implement in a MapReduce framework.
There are other implementations of SVMs that use a version of stochastic gradient descent such as the Pegasos algorithm.
There’s also an approximate version of SVM called proximal SVM, which computes a solution much faster and is easily applied to a MapReduce framework.4
You can calculate the k-means clusters by using canopy clustering first and using the canopies as the k initial clusters.
The book is especially good at explaining common implementation details for dealing with massive datasets.
Now let’s explore a Python tool for running MapReduce jobs.
Of the algorithms listed previously, a number of them are iterative.
They can be completed in a few MapReduce jobs but not one.
The simple example we looked at in section 15.3 on Amazon’s EMR ran in a single job.
What if we wanted to run AdaBoost on a very large dataset? What if we wanted to run 10 MapReduce jobs?
There are some frameworks for automating MapReduce job flow, such as Cascading and Oozie, but none of these run on Amazon’s EMR.
Amazon’s EMR supports Pig, which can use Python scripts, but doing this would require learning another scripting language.
Pig is an Apache project which provides a higher-level language for data processing.
Pig turns the data processing commands into Hadoop MapReduce jobs.
There are a few tools for running MapReduce jobs from within Python, and one I’m particularly fond of is called mrjob.
You can see appendix A for how to install it.
We’re going to show how to use mrjob next, and then we’ll rewrite the global mean and variance calculation we did earlier in mrjob.
Mrjob is useful as a learning tool, but it’s still Python.
Mrjob runs Hadoop Streaming on Elastic MapReduce as we did in section 15.3
The main difference is that you don’t have to worry about uploading your data to S3 and then typing in the commands correctly.
With mrjob you can also run MapReduce jobs on your own Hadoop cluster or in nondistributed.
The switch from running a job locally to running it on EMR is easy.
For example, to run a job locally, you’d enter something like this:
Now to run that same job on EMR you’d type.
All of the uploading and form filling that we did in section 15.3 is done automatically from mrjob.
You can add another command to run the job on your local Hadoop cluster if you happen to have one.
You can also add numerous command-line arguments to specify the number of servers you want on EMR or the type of server.
In section 15.3 we had two separate files for our mapper and reducer.
In mrjob the mapper and reducer can reside in the same script.
We’ll now look inside a script to see how it works.
You can do a lot of things with mrjob, but to get started we’ll go over a typical MapReduce job.
The best way to explain this is with an example.
We’ll solve the same mean/ variance problem so that we can focus on the subtleties of the framework.
Open a text editor, create a new file called mrMean.py, and enter the code from the following listing.
The input text is broken up into multiple mappers and these calculate intermediate values, which are accumulated in the reducer to give a global mean and variance.
You need to create a new class that inherits from the class MRjob.
There’s another method called steps(), which defines the steps taken.
You’ll see an example of this in the next section.
In the steps() method you tell mrjob the names of your mapper and reducer.
If you don’t specify anything, it will look for methods called mapper and reducer.
The mapper acts like the inside of a for loop and will get called for every line of input.
If you want to do something after you’ve received all the lines of the input, you can do that in mapper_final.
This may seem strange at first, but it’s convenient in practice.
So in our example we accumulate the input values in mapper(), and when we have all the values we compute the mean and the mean of the squared values and send these out.
Values are sent out of the mapper via the yield statement.
If you want to send out multiple values, a good idea is to pack them up in a list.
Values will be sorted after the map step by the key.
Hadoop has options for changing how things are sorted, but the default sort should work for most applications.
Values with the same key value will be sent to the same reducer.
You need to think through what you use for the key so that similar values will be collected together after the sort phase.
I used the key 1 for all the mapper outputs because I want one reducer, and I want all of the mapper outputs to wind up at the same reducer.
At the reducer, the inputs are presented as iterable objects.
To iterate over these, you need to use something like a for loop.
You can’t share state between the mapper or mapper_final and the reducer.
The reason for this is that the Python script isn’t kept alive from the map and reduce steps.
If you want to communicate anything between the mapper and reducer, it should be done through key/value pairs.
At the bottom of the reducer, I added a yield without a key because these values are destined for output.
If they were going to another mapper, I’d put in a key value.
To run the mapper only, enter the following commands in your Linux/DOS window, not in your Python shell.
You’ll get a lot of text describing the intermediate steps, and finally the output will be displayed to the screen:
Finally, to run on Amazon’s Elastic MapReduce, enter this command.
Now that you know how to use mrjob, let’s put this to use on a machine learning problem.
In the next section we’ll do an iterative algorithm with mrjob, something that we couldn’t do with just Elastic MapReduce.
Doing text classification on a large number of documents presents a large machine learning challenge.
How do we train our classifier over so much data? The MapReduce framework can help if we can break up our algorithm into parallel tasks.
If you remember from chapter 6, the SMO algorithm optimized two support vectors at a time.
The SMO algorithm also looped through the entire dataset, stopping at values that needed attention.
One alternative to the SMO algorithm is the Pegasos algorithm.
We’ll investigate the Pegasos algorithm next, and then you’ll see how to write a distributed version of Pegasos.
This algorithm uses a form of stochastic gradient descent to solve the optimization problem defined by support vector machines.
It’s shown that the number of iterations required is determined by the accuracy you desire, not the size of the dataset.
Please see the original paper for more detail.6 There are two versions, a long and a short version: I recommend the long version of the paper.
Recall from chapter 6 that in a support vector machine we’re trying to find a separating hyperplane.
In our two-dimensional examples we’re trying to find a line that properly separates the two classes of data.
The Pegasos algorithm works like this: A set of randomly selected points from our training data is added to a batch.
Each of these points is tested to see if it’s properly classified.
If it’s not properly classified, it’s added to the update set.
At the end of the batch, the weights vector is updated with the improperly classified vectors.
Set w to all zeros For each batch Choose k data vectors randomly For each vector.
Prepare: The input data is in a useable format already, so preparation isn’t needed.
If you need to prepare a massive data set, it probably would be a good idea to write this as a map job so you could parse in parallel.
Train: With SVMs, we spend most of the time and effort on training.
Test: Visually inspect the hyperplane in two dimensions to see if the algorithm is working.
Use: This example won’t build a full application, but it demonstrates how to train an SVM on a massive dataset.
An application of this is text classification, where we have tens of thousands of features and many documents.
If the vector is incorrectly classified: Change the weights vector: w Accumulate the changes to w.
To show you this algorithm in action, I’ve include a working Python version of it in the following listing.
The code in listing 15.4 is the sequential version of the Pegasos algorithm.
The inputs T and k set the number of iterations and the batch size, respectively.
In each of the T iterations you recalculate eta, which determines the learning rate or how much the weights can change.
In the outer loop you also select a new set of data points to use in the next batch.
The inner loop is the batch, where you accumulate B the values of the incorrectly classified values and then update the weights vector.
You can run this example with some of the data from chapter 6 if you wish to try it out.
We aren’t going to do much with this code except use it for a starting point for a MapReduce version.
In the next section we’ll build and run a MapReduce version of Pegasos in mrjob.
We’ll implement the Pegasos algorithm from listing 15.4 in MapReduce.
We’ll use the mrjob framework explored in section 15.5 to implement the algorithm.
First, we have to decide how to break up the algorithm into map and reduce steps.
What can we do in parallel? What can’t be done in parallel?
If you looked at all the computations going on when running the code from listing 15.4, you’d see that a lot of the time is spent doing the inner product.
We can parallelize these inner products, but we can’t parallelize the creation of a new w vector.
This gives us a good starting point for writing the MapReduce job.
Before we write the mapper and reducer, let’s write some supporting code.
Open your text editor and create a new file called mrSVM.py; then add the code from the following listing.
Example: the Pegasos algorithm for distributed SVMs from mrjob.job import MRJob import pickle from numpy import *
The code from listing 15.5 sets up everything so you can do the map and reduce steps properly.
You have an include statement for mrjob, NumPy, and Pickle.
The __init__() method initializes some variables you’ll use in the map and reduce steps.
The Python module Pickle doesn’t like to load to files pickled with different versions of Python.
These are the number of iterations (T) and the batch size (k)
Both of those arguments are optional, and if they aren’t set, they’ll default to the values in listing 15.5
Finally, the steps() method tells mrjob what jobs to do and in what order.
It creates a Python list with map, map_fin, and reduce steps, and then multiplies this by the number of iterations, which repeats the list for each iteration.
In order for this big chain of jobs to work properly, the mapper has to be able to read the data coming out of the reducer.
We didn’t have this requirement in our single MapReduce job, so we’ll have to be more careful with our inputs and outputs.
Let’s take a second to define the inputs and outputs: Mapper.
The first element in the list is a string detailing what type of data is stored in the rest of the list.
Every mapper_final will emit the same key; this is to make sure that all of the key/value pairs come to one reducer.
With our inputs and outputs defined, let’s write the mapper and reducer methods.
Open mrSVM.py and add the following methods to the MRsvm class.
The map_fin() method is executed when all of the inputs have arrived.
At this point you’ll have your weights vector w and a list of x values for this batch.
The data is stored on disk and loaded into memory when the script is executed.
When map_fin() starts, it splits the data into labels and data.
Next, it iterates over all of the values in the current batch, which are stored in self.dataList.
If any of these values is incorrectly classified, then it’s emitted to the reducer.
To keep state between the mapper and reducer, the w vector and the t values are sent to the reducer.
This first iterates over all the key/value pairs and unpacks the values to local variables.
Any value in dataList will be used to update the w vector.
After wMat has been updated, it’s time to start the whole process over.
A new batch of random vectors is chosen and emitted.
The key of all these values is the mapper number.
To see this in action, you need to start the job with some data that looks like it came out of the reducer.
I’ve attached a file called kickStart.txt that will do this.
To execute the previous code on your local machine, enter the following command:
If you want to run the job on EMR, add the -r emr command.
To see all of the options available, enter %python mrSVM.py –h.
Now that you know how to write and launch machine learning jobs on many machines, we’ll talk about whether doing this is really necessary.
Figure 15.9 Results from the distributed Pegasos algorithm after multiple iterations.
The algorithm converges quickly, and further iterations give only a slightly better solution.
Debugging mrjob Debugging a mrjob script can be much more frustrating that debugging a simple Python script.
Make sure you have all the dependencies installed: boto, simplejson, and optionally PyYAML.
You can see the input and output of your jobs there; this is helpful for debugging.
These big-data tools were made by the Googles, Yelps, and Facebooks of the world, but how many of those companies are there?
Making the most of your resources can save time and energy.
If you find your computing jobs taking too long, ask yourself the following questions: Could you rewrite your code in a more efficient language like C or Java? If you already are using one of those languages, are you writing your code in the most memory-efficient manner? Is your processing constrained by memory or the processor? Perhaps you don’t know the answer to these questions.
Most people don’t realize how much number crunching they can do on a single computer.
If you don’t have big data problems, you don’t need MapReduce and Hadoop.
It’s great to know they exist and to know what you could do if you had big-data problems.
When your computing needs have exceeded the capabilities of your computing resources, you may consider buying a better machine.
It may happen that your computing needs have exceeded the abilities of reasonably priced machines.
One solution to this is to break up your computing into parallel jobs.
In MapReduce you break your jobs into map and reduce steps.
A typical job can use the map step to process data in parallel and then combine the data in the reduce step.
This many-to-one model is typical but not the only way of combining jobs.
Data is passed between the mapper and reducers with key/value pairs.
Typically, data is sorted by the value of the keys after the map step.
Hadoop is a popular Java project for running MapReduce jobs.
Hadoop has an application for running non-Java jobs called Hadoop Streaming.
Amazon Web Services allows you to rent computing resources by the hour.
Simple one-step MapReduce jobs can be written and run from the Elastic MapReduce management console.
With minimal setup, mrjob handles the dirty steps associated with Amazon Web Services.
A number of machine learning algorithms can be easily written as MapReduce jobs.
Some machine learning jobs need to be creatively redefined in order to use them in MapReduce.
Support vector machines are a powerful tool for text classification, but training a classifier on a large number of documents can involve a large amount of computing resources.
One approach to creating a distributed classifier for support vector machines is the Pegasos algorithm.
Machine learning algorithms that may require multiple MapReduce jobs such as Pegasos are easily implemented in mrjob.
This concludes the main body of material for this book.
I hope this book opened many new doors for you.
There’s much more to be explored in the mathematics of machine learning or the practical implementation in code.
I look forward to seeing what interesting applications you create with the tools and techniques from this book.
In this appendix we’ll go through instructions for installing Python on the three most popular operating systems.
In addition, there’s a short introduction to Python and instructions for installing the Python modules used in this book.
A discussion on NumPy is saved for appendix B, where it more appropriately fits in with a discussion on linear algebra.
The examples aren’t guaranteed to work with Python 3.X, because Python doesn’t provide backward compatibility.
The easiest way to get these modules is through package installers.
To get NumPy you can get the binary from http://sourceforge.net/projects/ numpy/files/NumPy/
Once the installer is finished, you can start a Python shell.
To do this, first open a command prompt by typing cmd in the find window.
This should start a Python shell telling you which version you’re using, when it was built, and so on.
I’ll leave the details of creating an alias up to you.
You can find the latest version at the Matplotlib home page: http://matplotlib.sourceforge.net/
Installing this binary is relatively easy; just download the installer and click through the installation steps.
MacPorts is a free tool that simplifies compiling and installing software on your Mac.
The best way to do that is to download the appropriate .dmg file.
On this site you can select the .dmg file that corresponds to the version of Mac OS X that you have.
Then when you’ve installed MacPorts, open a new terminal window and enter the following command:
It will take a while depending on how fast your machine and internet connection are.
You can install Python, NumPy, and Matplotlib separately if you don’t want to install MacPorts.
There are now Mac OS X binary installers for all three of these libraries, which make installing super easy.
The best way to get Python, NumPy, and Matplotlib in Debian/Ubuntu is to use aptget, or the corresponding package manager in other distributions.
If you install Matplotlib, then it will check to see if you have all the dependencies.
Since Python and NumPy are dependencies of Matplotlib, installing Matplotlib will ensure that you have the other two.
To install Matplotlib, open a command shell and enter the following command:
This will take some time depending on how fast your machine and internet connection are.
Now that you’ve installed Python, let’s discuss some of the data types used in Python.
A.2 A quick introduction to Python Now we can go over a few of the features of the language that we use in this book.
This is not an exhaustive description of Python; for that I suggest you try “How to Think Like a Computer Scientist” by Elkner, Downey, and Meyers at http://openbookproject.net/ /thinkCSpy/; the contents are available for free online.
We’ll go over collection types and control structures, something found in almost every programming language.
We’ll just review them to see how Python handles them.
Finally, in this section we’ll review list comprehensions, which I think are the most confusing part of getting started with Python.
Python has a number of ways of storing a collection of items, and you can add many modules to create more container types.
Following is a short list of the commonly used containers in Python:
You can have anything in a list: numbers, bool, strings, and so on.
The following code illustrates the creation of a list called jj and the addition of an integer and a string:
You could create the list jj in one pass with the following statement:
Python also has an array data type, which, similar to other programming languages, can contain only one type of data.
This array type is faster than lists when you’re looping.
We won’t use this structure in this book because it could be confused with the array type in NumPy.
Dictionaries—A dictionary is an unordered key/value type of storage container.
In other languages, a dictionary may be called an associative array or map.
In the following code we create a dictionary and adds two items to it:
You can also create this dictionary in one line with the following command:
If you aren’t familiar with that, it means a unique collection of items.
You can create a set from a list by entering the following:
Sets can then do math operations on sets, such as the union, intersection, and difference.
Or, for multiple lines, you can use an indent to tell the interpreter you have more than one line.
You can use this indent with just one line of code if you prefer.
Multiple conditionals, like else if, are written as elif, and the keyword else is used for a default condition.
For—A for loop in Python is like the enhanced for loop in Java or C++0x.
If you’re not familiar with those, it simply means that the for loop goes over every item in a collection.
Let me give you some examples from lists, sets, and dictionaries:
I think the most confusing thing for people new to Python is list comprehensions.
List comprehensions are an elegant way of generating a list without writing a lot of code.
But the way they work is a little bit backwards.
The resulting myList is the same, but we used less code with the list comprehension.
The confusing part is that the item that gets appended to the list is in front of the for loop.
This is contrary to the way the English text is read, from left to right.
You can get really creative with list comprehensions, and if at some point they become difficult to read, you’ll be better off writing out the code.
Now that we’ve reviewed some basics, the next section discusses how to install Python modules used in this book.
For most pure Python modules (modules that don’t have bindings to other languages), you can change the directory to where you’ve unzipped the code and type > python setup.py install.
This is the default, and if you’re ever unsure how to install something, try this command.
A.3 A quick introduction to NumPy Having installed the NumPy library, you may be wondering, “What good is this?” Officially, NumPy is a matrix type for Python, and a large number of functions to operate on these matrices.
Unofficially, it’s a library that makes doing calculations easy and faster to execute, because the calculations are done in C rather than Python.
That would have required a for loop in regular Python.
Here are some more operations that would require a loop in regular Python:
You can now access the elements in the array like it was a list:
When you multiply two arrays together, you multiply the elements in the first array by the elements in the second array:
Now let’s talk about matrices: Similar to arrays, you need to import matrix or mat from NumPy:
You can access the individual elements of a matrix like this:
The NumPy matrix data type has a transpose method, so you can do this multiplication quite easily:
We took the transpose of ss with the .T method.
What if you wanted to multiply every element in matrix mm by every element in ss? This is known as element-wise multiplication and can be done with the NumPy multiply function:
The matrix and array data types have a large number of other useful methods available such as sorting:
Be careful; this method does sort in place, so if you want to keep the original order of your data, you must make a copy first.
You can also use the argsort() method to give you the indices of the matrix if a sort were to happen:
You can also calculate the mean of the numbers in a matrix:
This is a matrix of shape 2x3; to get all the elements in one row, you can use the colon (:) operator with the row number.
For example, to get all the elements in row 1, you’d enter.
Beyond the array and matrix data types, a large number of other functions in.
Next, unzip it and cd into the directory where you’ve unzipped it.
If you’re on Linux and it tells you that you don’t have permission to install it, type in the following:
With most Python modules this is how you’ll install them.
Be sure to read the README.txt included with each module you download.
Installing mrjob is as easy as installing other modules in Python.
There’s a button on the left side that says ZIP.
Unzip and untar the file, and then cd into the directory that you just unzipped.
The GitHub listing has a lot of code samples, and there’s a good page with official Python documentation here: http://packages.python.org/mrjob/
These are values unique to your account (I’m assuming you have an account), and you get them when you log into AWS under Account > Security Credentials.
To set these in Windows, open a command prompt and enter the following:
To set these on Mac OS X (newer versions of OS X use the bash shell), open a terminal window and enter the following:
Ubuntu Linux also uses the bash shell by default, so the Mac OS X instructions.
If you’re using another shell, you’ll have to research how to set the environment variables yourself, but it isn’t difficult.
Sunlight Labs has written a well-documented Python interface for this API.
On the left side is a download button labeled ZIP.
Once you’ve downloaded that file, change your directory to the downloaded and unzipped file folder.
Be patient because the API key takes some time to become active.
My API key wasn’t active until 30 minutes after I applied for it.
You’ll get an email saying your API key has been approved.
Then you’re ready to start finding out what those greasy politicians are up to!
A.7 Python-Twitter Python-Twitter is a module to interface with data from Twitter.
To install this module untar the tarball, and then change to untarred directory and enter.
That should be it; you’ll need to get a Twitter API key, and then you’ll be able to start getting and posting data to Twitter from your Python code.
To understand advanced machine learning topics, you need to know some linear algebra.
If you want to take an algorithm from an academic paper and implement it in code or investigate algorithms outside of this book, you’ll probably need a basic understanding of linear algebra.
This appendix should serve as a light refresher or introduction if you’ve had this material before but it’s been a while and you need a reminder.
If you’ve never had this material before, I recommend that you take a course at a university, work through a self-study book, or watch a video.
Working through examples on your own is necessary to reinforce what you’ve watched others do in a book or video.
We’ll first discuss the basic building block of linear algebra, the matrix.
Then we’ll discuss some basic operations on matrices, including taking the matrix inverse.
We’ll address the vector norm, which often appears in machine learning, and we’ll conclude by discussing how we can apply calculus to linear algebra.
B.1 Matrices The most basic data type in linear algebra is the matrix.
A simple example of this is shown in figure B.1
The row numbering usually begins at the top, and column.
Gilbert Strang has some lectures that are free to view at http://www.youtube.com/watch?v=ZK3O402wf1c.
His lectures aren’t difficult to follow and communicate the key points of linear algebra.
In every chapter in this book we’ve used vectors, which are special cases of matrices containing only a single column or row.
Often, a vector will be mentioned without specifying row or column.
If this is the case, assume it’s a column vector.
Figure B.2 shows a column vector on the left side.
Keeping track of the shape will be important when we’re doing matrix operations such as multiplication.
One of the most basic matrix operations is the transpose.
The transpose is written with a capital T in superscript.
The transpose is often used to manipulate matrices to make calculations easier.
These are called scalar operations because the relative values of the elements in the matrix don’t change; only the scale changes.
If you want to scale your data by a constant or add a constant offset, you’ll need scalar multiplication and addition.
How would you add two matrices together? First of all, the matrices must be the same size.
If the matrices are the same size, you can add them together.
To do this you add the elements in the same position.
Matrix subtraction is similar, but you subtract the elements rather than add them.
Figure B.2 A column vector on the left side and a row vector on the right side.
Figure B.3 The transpose rotates a matrix, and the rows become columns.
Figure B.4 Scalar operations on our matrix result in every element being multiplied or added by a scalar value.
To multiply two matrices, they must have a matching inner dimension.
A quick way to check if you can multiply two matrices and the resulting size is to write the dimensions next to each other like this: (3x4)(4x1)
Because the middle terms match, you can do the multiplication.
By dropping the middle terms, you can see the size of the resulting matrix: 3x1
Another way to think of matrix multiplication is a sum of columns.
In this second method of matrix multiplication, the same result was achieved but we reorganized how we were looking at the multiplication.
Rethinking matrix multiplication as the sum of columns in figure B.7 will be helpful in certain algorithms, such as map reduce versions of matrix multiplication.
In general, the definition of matrix multiplication for a matrix X and matrix Y is.
If you doubt that two operations are equal, you can always write them out using this summation form.
A common operation in machine learning is the dot product between two vectors.
This was done in chapter 6 with support vector machines.
The dot product is an element-wise multiplication, then summing up every element in the resulting vector.
Often there’s a physical meaning associated with a dot product, such as how much one vector moves in the direction of another vector.
The dot product can be used to find the cosine between two vectors.
In any program that supports matrix multiplication, you can get the dot product of two vectors X and Y by multiplying the transpose of X by Y.
B.2 Matrix inverse The matrix inverse comes up a lot when you’re manipulating algebraic equations of matrices.
The matrix X is the inverse of matrix Y if XY=I where I is the identity matrix.
You can multiply other matrices by the identity matrix and get the original matrix.
The practical drawback to the matrix inverse is that it becomes messy for matrices larger than a few elements and is rarely computed by hand.
It helps to know when you can’t take the inverse of a matrix.
Knowing this will help you avoid making errors in your programs.
You write the inverse of a matrix B as B-1
By square, I mean the number of rows and columns has to be equal.
Even if the matrix is square, it may not be invertible.
If a matrix is not invertible, we say that it’s singular or degenerate.
A matrix can be singular if you can express one column as a linear combination of other columns.
If you could do this, you could reduce a column in the matrix to all 0s.
An example of such a matrix is shown in figure B.9
This becomes a problem when computing the inverse of the matrix, because you’ll try to divide by zero.
There are many ways to calculate the inverse of a matrix.
One way is to rearrange some terms in the matrix and divide every element by the determinant.
The determinant is special value associated with a square matrix that can tell you a number of things about the matrix.
Figure B.8 An illustration of the dot product of two vectors.
This matrix has a column of 0s, which means you won’t be able to take the inverse of this matrix.
This leads to dividing by 0, which isn’t possible, so you can’t take the inverse of such a matrix.
This is why you must have a matrix that’s full rank in order to take the inverse.
You’ve seen how the inverse of a 2x2 matrix is calculated.
Now, let’s look at how the inverse of a 3x3 matrix would be calculated; you’ll see it gets much more complex.
The take-home lesson is that computing the inverse gets really messy after two or three terms because the determinant has n! elements.
You don’t deal with such small matrices often, so calculating the inverse is usually done by a computer.
B.3 Norms The norm is something that comes up often in machine learning literature.
The norm of a matrix is written with two vertical lines on each side of the matrix like this: ||A||
The vector norm is an operation that assigns a positive scalar value to any vector.
You can think of this as the length of the vector, which is useful in many machine learning algorithms such as k-Nearest Neighbors.
Figure B.10 Calculating the inverse of a square matrix B.
Since we’re multiplying every element by 1/det(B), det(B) can’t be zero.
If we have a singular matrix, det(B) will be zero and we can’t take the inverse.
With larger matrices the inverse becomes more difficult to calculate by hand.
The determinant of a matrix with size n contains n! elements.
Using a different norm can give better results in some machine learning algorithms, such as the lasso for regression.
The L1 norm is zalso popular, and this is sometimes known as the Manhattan distance.
You can have a norm of any number, and in general they’re defined as.
The vector norms are used when determining the magnitude or significance of vectors, as in an input.
In addition to the function defined here, you could create a vector norm any way you wanted as long as it converted from a vector to a scalar value.
B.4 Matrix calculus In addition to adding, subtracting, and dividing matrices and vectors, you can do calculus operations, such as the derivative on vectors and matrices.
If you have a vector , you can take the derivative of A with respect to.
In this appendix we’ll go through some of the basic concepts of probability.
The subject deserves more treatment than this appendix provides, so think of this as a quick refresher if you’ve had this material in the past but need to be reminded about some of the details.
For someone who hasn’t had this material before, I recommend studying more than this humble appendix.
A number of good tutorials and videos are available from the Khan Academy that can be used for self-study.1
C.1 Intro to probability Probability is defined as how likely something is to occur.
You can calculate the probability of an event occurring from observed data by dividing the number of times this event occurred by the total number of events.
From this table we can calculate the probability the weather is snowing.
The data in table C.1 is limited to seven measurements, and some days are missing in the sequence.
Let’s calculate the probability the weather is snowing as P(weather = snowing):
But weather is the only variable that can take the value of snowing, so we can write this as P(snowing) to save some writing.
With this basic definition of probability we calculate the probabilities of weather = rainy and weather = clear.
You’ve seen how to calculate the probability of one variable taking one specific value, but what if we’re concerned with more than one variable?
C.2 Joint probability What if we want to see the probability of two events happening at the same time, such as weather = snowing and day of week = 2? You can probably figure out how to calculate this; you count the number of examples where both of these events are true and divide it by the total number of events.
The vertical bar is used to represent conditional probability, so this statement is asking for the probability of X AND Y conditioned on the event Z.
A quick refresher on conditional probability is given in chapter 4 if you want to review it.
You just need a few basic rules to manipulate probabilities.
Once you have a firm grasp on these, you can manipulate probabilities like algebraic expressions and infer unknown quantities from known quantities.
C.3 Basic rules of probability The basic rules (axioms) of probability allow us to do algebra with probabilities.
These are as fundamental as the rules of algebra and should not be ignored.
I’ll discuss each of them in turn and show how it relates to our weather data in table C.1
It shouldn’t be a huge surprise then that for any event X, 0?P(x)?1.0
These events aren’t mutually exclusive; this just means that they can happen at the same time.
There’s an area where these two regions overlap, but they don’t completely overlap.
The area of overlap in figure C.2 can be thought of as the intersection of the two events.
This is written as (weather = snowing) AND (day of week = 2)
Figure C.1 The top frame shows the event snowing in the circle while all other events are outside the circle.
The bottom frame shows not snowing or all other events.
The sum of snowing and not snowing makes up all known events.
Figure C.2 A Venn diagram showing the intersection of two non–mutually exclusive events.
We have that last subtracted part to avoid double counting the intersection.
This also leads us to an interesting result, a way of algebraically moving between ANDs and ORs of probabilities.
With these basic rules of probability, we can accomplish a lot.
With assumptions or prior knowledge, we can calculate the probabilities of events we haven’t directly observed.
Collecting data can be a lot of fun, but if you have a good idea for an algorithm or want to try something out, finding data can be a pain.
This appendix contains a collection of links to known datasets.
These sets range in size from 20 lines to trillions of lines, so you should have no problem finding a dataset to meet your needs:
Many of these datasets are used to compare the performance of algorithms so that researchers can have an objective comparison of performance.
The site was intended to make all government data public as long as the data was not private or restricted for security reasons.
In 2011, the federal government reduced funding for the Electronic Government Fund, which pays for Data.gov.
The datasets range from products recalled to a list of failed banks.
Currently, they have more than 14,000 datasets available to download.
A machine is said to learn when its performance improves with experience.
Learning requires algorithms and programs that capture data and ferret out the interesting.
Once the specialized domain of analysts and mathematicians, machine learning is becoming a skill needed by many.
Machine Learning in Action is a clearly written tutorial for developers.
It avoids academic language and takes you straight to the techniques you’ll use in your day-to-day work.
Many (Python) examples present the core algorithms of statistical data processing, data analysis, and data visualization in code you can reuse.
You’ll understand the concepts and how they fi t in with tactical tasks like classifi cation, forecasting, recommendations, and higher-level features like summarization and simplifi cation.
Readers need no prior experience with machine learning or statistical processing.
He holds fi ve US patents and his work has been published in numerous academic journals.
