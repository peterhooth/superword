The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
During a coffee break, a colleague from Deutsche Telekom told me that the local university had a teacher who was very much into OSGi.
This teacher was the author of Oscar, one of the first open source OSGi frameworks.
In 2003, wholeheartedly adopting OSGi was rare, so I was intrigued.
Also around that time, Eclipse was investigating moving to a new module system, and I was asked to participate as an OSGi expert.
I thought Richard could be valuable for this, so I asked him to join the Equinox committee.
That innocent invitation started an enormously long email thread that hasn’t ended yet and, I hope, never will.
Richard is often abrasive when specifications aren’t clear,  or worse, when we attempt to violate modular purity.
Sometimes I think he physically feels pain if we have to compromise on a dirty feature.
As an invited OSGi researcher, he has became one of the key people behind the specifications, making sure we don’t xiv.
When Manning sent a flattering email proposing an OSGi in Action book to the key.
This email triggered intense discussions about collectively writing this book; the idea to write a book had been discussed many times before.
We went into negotiations with Manning, but in the end I withdrew  from the group, urging the others to continue.
Why did I bail out? As the editor of the OSGi specifications, I was aware of how much work it is to write a book in collaboration with other opinionated people.
To extend my day job into the night and weekends for free wasn’t something I was looking forward to, regardless of how much I liked and appreciated these guys.
Each of these authors is a great contributor to the open source world as well as to the OSGi specifications: Karl for his work on Felix and his testimony to modularity by doing Felix security as a separate bundle, proving that even the framework architecture is modular; Stuart for his work on the Maven bundle plugin, the popular Ops4J work, and the Peaberry extension to Guice; and David for the excellent work he is doing with Sigil at Apache and his work at Paremus.
It would be hard to come up with a team that knows more about how OSGi is used in the real world.
All this experience radiates from the chapters they’ve written in this impressive book.
While this team undertook the Herculean effort to write this book, I was in close contact with them all along the way—not only because of our work in the OSGi Alliance, but also because authoring a book about OSGi is likely to expose weakness or deficiencies in the specifications, which then obviously results in another, often heated argument over Skype or email.
Unfortunately, to my chagrin, the team was too often right.
They also asked me to provide the text about the history of OSGi, an effort that resulted in probably the highest compression rate ever achieved.
Of the 4,356 words I wrote, I think the word OSGi remained.
But this is exactly what I like: the quest for quality drove this book, not only in its details but also in its form.
It isn’t like many books today, full of listings outlining in minute steps how to achieve a result.
No, this is a book exactly the way I like it: not only showing in detail how to use OSGi, but also going to great length to point out the rationale.
Although it builds on an object-oriented foundation, it adds a new set of design primitives to address the shortcomings of object-oriented design that were uncovered when applications became humongous assemblies of multiple open source projects and proprietary code.
Objects remain an invaluable technique for building software, but the object-oriented paradigm isn’t well suited to allowing large building blocks (components) collaborate without causing too much coupling.
We desperately fight objects with patterns like factories and class-loading hacks, but at a certain scale the work to prevent coupling becomes a significant part of our efforts.
Dependency injection alleviated much of the coding pain but moved a lot of the code into XML, a language that has the most ill-suited syntax imaginable for human programming tasks.
Annotations provide another level of support for dealing with coupling-—but cause a coupling problem in themselves.
Many of the painkillers we use to alleviate coupling are largely cosmetic because boundaries aren’t enforced at execution time in traditional Java.
Back then, OSGi was targeting the embedded market niche, but that wasn’t my area of interest.
I wanted to create highly dynamic, modular applications, and OSGi gave me the possibility of doing so.
At the time, there weren’t any freely available OSGi framework implementations; so I started working on my own open source implementation, called Oscar, back in December 2000 while I was working at Free University Berlin.
Oscar moved with me when I moved to Grenoble to work at Josef Fourier University, where the work really started to flourish.
OSGi technology has moved beyond the embedded market into a full-blown module system for Java.
This transformation was significantly helped along in 2004 when the Eclipse IDE refactored its plugin system to run on top of OSGi, and it has continued with the adoption of the technology in enterprise circles by Spring and all the major application servers.
Although the future of Java modularity is still evolving, OSGi technology looks to play a role for a long time to come.
PREFACExviii I’d been kicking around the idea of writing an OSGi book for a couple of years, but given the enormity of the task and my life-long time deficit, I never got around to it.
In the summer of 2008, I finally got serious and began writing, only to find myself quickly bogged down.
It wasn’t until Karl and Stuart offered to help, and later David, that we were finally able to slay the beast.
Even then, it’s taken us two years, a few career changes, and the birth of several children to see it to an end.
Thanks also to all the early readers of the manuscript and the book forum posters who provided valuable feedback throughout the writing process.
We’d also like to single out Norman Richards for his technical proofreadxix.
The staff at Manning have been supportive throughout this lengthy ordeal; we’d.
Last, we’d like to thank the Apache Felix community for their contributions to all the code and discussions over the years.
Individually, Richard thanks his wife and daughter and apologizes for the many distractions this book caused.
Karl thanks his wife Doreen and his children Elisabeth and Holger for all the love, support, and understanding.
Stuart thanks his dear wife Hayfa for the motivation to finish this book.
David thanks his wonderful family, and especially his wife Imogen, for the support and encouragement to finish this book.
If you do, you’ll discover that they were written for someone who is going to implement the specifications, not use them.
This book started out as an attempt to remedy this situation by creating a user-oriented companion guide for the specifications.
Our goal wasn’t to create an OSGi cookbook but to thoroughly describe the important aspects of OSGi and show how to use them.
Our main idea was to more simply explain the OSGi specifications by ignoring the implementation details and including additional usage information.
To that end, we’ve tried to limit ourselves to discussing the most common concepts, features, and mechanisms needed to work with OSGi technology throughout the book.
That doesn’t mean we were able to avoid all the esoteric details.
In the end, you need to understand what’s going on under the covers in some places to be able to effectively debug and diagnose the situations in which you find yourself.
As our writing progressed, the book chapters began to separate naturally into three parts:
We introduce OSGi according to its three-layer architecture: module, lifecycle, and services.
This isn’t the only approach to take in explaining OSGi; most explanations of OSGi start out with a simple bundle implementing a simple service.
The downside of this type of approach, in our view, is that it cuts across all three OSGi layers at once, which would require us to explain all three layers at once.
The advantage of following a layered approach is that doing so creates a clear division among the concepts we need to discuss.
For example, the modularity chapter focuses on modularity concepts and can largely ignore lifecycle and services.
This approach also creates a natural progression, because modularity is the foundation of OSGi, lifecycle builds on it, and services are on top of lifecycle.
We can also highlight how to use lower layers of the OSGi architecture without using the upper layers, which is sometimes worthwhile.
We look into converting existing JAR files to bundles as well as testing, debugging, and managing bundles.
These first two parts of the book should be of general interest to anyone wanting to learn more about using OSGi.
Part 3 covers various advanced topics, such as service-oriented component models, framework launching, security, and distributed computing technologies.
This last part serves as a springboard to the world of possibilities available to you in the OSGi universe.
Roadmap Chapter 1 presents a high-level view of OSGi technology and the issues it’s intended to address.
To keep the chapter from being totally abstract, we present a few “Hello, world!” examples to illustrate the different layers of the OSGi framework, but the real meat of our OSGi discussion is in the following chapters.
We also look at the state of modularity support in Java as well as in some related technologies.
Chapter 2 explores the module layer of the OSGi framework.
We start with a general discussion of modularity in computing and then continue by describing OSGi’s module concept, called a bundle.
We present OSGi’s declarative metadata-based approach for creating modules and show how to use it to modularize a simple paint program.
We also investigate one of the key OSGi tasks: bundle dependency resolution.
Chapter 3 looks at the lifecycle layer of the OSGi framework.
We discuss lifecycle management in general and describe how OSGi provides dynamic lifecycle management of bundles.
We present OSGi’s lifecycle-related APIs by creating a simple OSGi shell and also adapt our paint program to make it lifecycle aware.
Chapter 4 examines the services layer of the OSGi framework.
We describe what services are and discuss why and when you need them.
We walk you through providing and using services with some toy examples and then take an iterative approach to describing how to deal with the unique aspect of service dynamism.
We finish our service discussion by adapting the paint program, this time to use dynamic services.
We describe additional ways for bundles to deal with dependencies and content using bundle-level dependencies and bundle fragments.
You also learn how bundles can deal with execution environments and native libraries.
Chapter 6 gives practical advice for converting JAR files into bundles, including how to define bundle metadata, package your bundle content, and add lifecycle support.
We also describe how to go about dividing an application into bundles, demonstrating techniques on an existing open source project.
Chapter 7 shows how to test bundles and OSGi-based applications.
We look into running your existing tests in OSGi and mocking OSGi APIs.
In addition to unit and integration testing, we discuss management testing and explore some tools to help you along the way.
Chapter 8 follows testing by describing how to debug your bundles.
We look into simple, command-line debugging as well as debugging with the Eclipse IDE.
We show how to set up your development environment to get you up to speed quickly.
We also explain some of the typical issues you encounter when working with OSGi and how to deal with them.
Chapter 9 switches gears and discusses how to manage your bundles.
We explain how to meaningfully define version numbers for packages and bundles.
We look into managing bundle configuration data and in the process describe a handful of related OSGi services.
We also cover an option for triggering automatic bundle startup and initialization.
Chapter 10 continues investigating management topics, but moves from singlebundle issues to multi-bundle ones.
We look at a couple of approaches for deploying bundles and their dependencies.
We also explain how you can control bundle startup order.
As a concrete example, we look at a standard OSGi component framework called Declarative Services.
We show how Declarative Services allows you to work with POJOs and simplifies some aspects of dealing with service dynamism.
Chapter 12 continues investigating more advanced component frameworks for OSGi.
We look at Blueprint, which is targeted toward enterprise developers familiar with Spring technology.
We show that one of the benefits of OSGi-based component frameworks is they can all work together via services.
Chapter 13 turns away from developing bundles and looks at launching the OSGi framework.
We describe the standard approach for configuring and creating OSGi frameworks.
We also show how you can use the standard API to embed an OSGi framework into an existing application.
Chapter 14 delves into operating OSGi in a secure environment.
We describe the issues involved and approaches to alleviating them.
We explain how OSGi extends the standard Java security architecture to make it more flexible and easier to manage.
Chapter 15 closes the book with a quick look at using web-related technologies in OSGi.
We discuss using some common web applications technologies, such as servlets, JSPs, and WAR files.
We also look into how to publish and consume web services.
In the text, Courier typeface is used to denote code as well as JAR file manifest headers.
References to methods generally don’t include the signature, except when it’s necessary to differentiate.
The coding style adopts two-space indents and same-line braces to keep everything condensed and isn’t otherwise recommended.
When presenting command or shell interaction, normal Courier typeface is used to indicate program output, while bold is used to indicate user input.
Code annotations accompany many of the listings, highlighting important concepts.
In some cases, numbered bullets link to explanations that follow the listing.
Author Online Purchase of OSGi in Action includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the authors and from other users.
This page provides information on how to get on the forum once you are registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialog between individual readers and between readers and the authors can take place.
It is not a commitment to any specific amount of participation on the part of the authors, whose contribution to the book’s forum remains voluntary (and unpaid)
We suggest you try asking them some challenging questions lest their interest stray!
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
About the title By combining introductions, overviews, and how-to examples, the In Action books are designed to help learning and remembering.
According to research in cognitive science, the things people remember are things they discover during self-motivated exploration.
Although no one at Manning is a cognitive scientist, we are convinced that for learning to become permanent it must pass through stages of exploration, play, and, interestingly, re-telling of what is being learned.
People understand and remember new things, which is to say they master them, only after actively exploring them.
An essential part of an In Action book is that it is exampledriven.
It encourages the reader to try things out, to play with new code, and explore new ideas.
There is another, more mundane, reason for the title of this book: our readers are busy.
They use books to do a job or solve a problem.
They need books that allow them to jump in and jump out easily and learn just what they want, just when they want it.
The books in this series are designed for such readers.
The title page is missing from the collection and we have been unable to track it down to date.
The book’s table of contents identifies the figures in both English and French, and each illustration bears the names of two artists who worked on it, both of whom would no doubt be surprised to find their art gracing the front cover of a computer programming book...two hundred years later.
The collection was purchased by a Manning editor at an antiquarian flea market in the “Garage” on West 26th Street in Manhattan.
The seller was an American based in Ankara, Turkey, and the transaction took place just as he was packing up his stand for the day.
The Manning editor did not have on his person the substantial amount of cash that was required for the purchase and a credit card and check were both politely turned down.
With the seller flying back to Ankara that evening the situation was getting hopeless.
What was the solution? It turned out to be nothing more than an oldfashioned verbal agreement sealed with a handshake.
The seller simply proposed that the money be transferred to him by wire and the editor walked out with the bank information on a piece of paper and the portfolio of images under his arm.
Needless to say, we transferred the funds the next day, and we remain grateful and impressed by this unknown person’s trust in one of us.
It recalls something that might have happened a long time ago.
The pictures from the Ottoman collection, like the other illustrations that appear on our covers, bring to life the richness and variety of dress customs of two centuries ago.
They recall the sense of isolation and distance of that period—and of every other historic period except our own hyperkinetic present.
Dress codes have changed since then and the diversity by region, so rich at the time, has faded away.
It is now often hard to tell the inhabitant of one continent from another.
Perhaps, trying to view it optimistically, we have traded a cultural and visual diversity for a more varied personal life.
Or a more varied and interesting intellectual and technical life.
We at Manning celebrate the inventiveness, the initiative, and, yes, the fun of the computer business with book covers based on the rich diversity of regional life of two centuries ago‚ brought back to life by the pictures from this collection.
Richard is a member of the Apache Software Foundation and works for Oracle on the GlassFish team, helping out out on OSGi issues or anything else, if he can.
He is a member of the Apache Software Foundation and is involved in various Apache and other open source projects.
Stuart is a consultant at Sonatype, working on dependency injection and modularization.
He contributes to the Apache Felix project especially in the area of development tooling via the Sigil subproject.
He is also directly involved in developing specifications for the OSGi Alliance.
The OSGi framework defines a dynamic module system for Java.
It gives you better control over the structure of your code, the ability to dynamically manage your code’s lifecycle, and a loosely coupled approach for code collaboration.
Even better, it’s fully documented in a very elaborate specification.
Unfortunately, the specification was written for people who are going to implement it rather than use it.
In the first part of this book, we’ll remedy this situation by effectively creating a user-oriented companion guide to the OSGi framework specification.
We’ll delve into its details by breaking it into three layers: module, lifecycle, and services.
We’ll explain what you need to understand from the specification to effectively use OSGi technology.
But this success has come in spite of the fact that Java doesn’t have explicit support for building modular systems beyond ordinary object-oriented data encapsulation.
What does this mean to you? If Java is a success despite its lack of advanced modularization support, then you may wonder if that absence is a problem.
Most wellmanaged projects have to build up a repertoire of project-specific techniques to compensate for the lack of modularization in Java.
It’s used to develop applications for everything from small mobile devices to massive enterprise endeavors.
But these techniques are inherently brittle and error prone because they aren’t enforceable via any compile-time or execution-time checks.
The end result has detrimental impacts on multiple stages of an application’s lifecycle:
Development—You’re unable to clearly and explicitly partition development into independent pieces.
Deployment—You’re unable to easily analyze, understand, and resolve requirements imposed by the independently developed pieces composing a complete system.
Execution—You’re unable to manage and evolve the constituent pieces of a running system, nor minimize the impact of doing so.
It’s possible to manage these issues in Java, and lots of projects do so using the custom techniques mentioned earlier, but it’s much more difficult than it should be.
We’re tying ourselves in knots to work around the lack of a fundamental feature.
If Java had explicit support for modularity, then you’d be freed from such issues and could concentrate on what you really want to do, which is developing the functionality of your application.
The OSGi Service Platform is an industry standard defined by the OSGi Alliance to specifically address the lack of support for modularity in the Java platform.
As a continuation of its modularity support, it introduces a service-oriented programming model, referred to by some as SOA in a VM, to help you clearly separate interface from implementation.
This chapter will give you an overview of the OSGi Service Platform and how it helps you create modular and manageable applications using an interface-based development model.
When we’ve finished this chapter, you’ll understand what role OSGi technology plays among the arsenal of Java technologies and why Java and/or other Java-related technologies don’t address the specific features provided by OSGi technology.
The $64,000 question is, “What is OSGi?” The simplest answer to this question is that it’s a modularity layer for the Java platform.
Of course, the next question that may spring to mind is, “What do you mean by modularity?” Here we use modularity more or less in the traditional computer-science sense, where the code of your software application is divided into logical parts representing separate concerns, as shown in figure 1.1
If your software is modular, you can simplify development and.
Figure 1.1 Modularity refers to the logical decomposition of a large system into smaller collaborating pieces.
OSGi technology is cropping up all over the place—for example, as the runtime for the Eclipse IDE and the GlassFish application server.
Why is it gaining popularity now? To better understand why OSGi is an increasingly important Java technology, it’s worthwhile to understand some of Java’s limitations with respect to creating modular applications.
When you understand that, then you can see why OSGi technology is important and how it can help.
Java provides some aspects of modularity in the form of object orientation, but it was never intended to support coarse-grained modular programming.
Although it’s not fair to criticize Java for something it wasn’t intended to address, the success of Java has resulted in difficulty for developers who ultimately have to deal with their need for better modularity support.
Java is promoted as a platform for building all sorts of applications for domains ranging from mobile phone to enterprise applications.
Most of these endeavors require, or could at least benefit from, broader support for modularity.
Although Java provides a fair complement of access modifiers to control visibility (such as public, protected, private, and package private), these tend to address low-level object-oriented encapsulation and not logical system partitioning.
Java has the notion of a package, which is typically used for partitioning code.
For code to be visible from one Java package to another, the code must be declared public (or protected if using inheritance)
Sometimes, the logical structure of your application calls for specific code to belong in different packages; but this means any dependencies among the packages must be exposed as public, which makes them accessible to everyone else, too.
Often, this can expose implementation details, which makes future evolution more difficult because users may end up with dependencies on your nonpublic API.
To illustrate, let’s consider a trivial “Hello, world!” application that provides a public interface in one package, a private implementation in another, and a main class in yet another.
Listing 1.1 Example of the limitations of Java’s object-orientated encapsulation.
Listing 1.1’s author may have intended a third party to only interact with the application via the Greeting interface B.
They may mention this in Javadoc, tutorials, blogs, or even email rants, but nothing stops a third party from constructing a new GreetingImpl using its public constructor C as is done at D.
You may argue that the constructor shouldn’t be public and that there is no need to split the application into multiple packages, which could well be true in this trivial example.
But in real-world applications, class-level visibility when combined with packaging turns out to be a crude tool for ensuring API coherency.
Because supposedly private implementation details can be accessed by third-party developers, you need to worry about changes to private implementation signatures as well as to public interfaces when making updates.
This problem stems from the fact that although Java packages appear to have a logical relationship via nested packages, they don’t.
A common misconception for people first learning Java is to assume that the parent-child package relationship bestows special visibility privileges on the involved packages.
Two packages involved in a nested relationship are equivalent to two packages that aren’t.
Nested packages are largely useful for avoiding name clashes, but they provide only partial support for the logical code partitioning.
What this all means is that, in Java, you’re regularly forced to decide between the following:
Impairing your application’s logical structure by lumping unrelated classes into the same package to avoid exposing nonpublic APIs.
Keeping your application’s logical structure by using multiple packages at the expense of exposing nonpublic APIs so they can be accessed by classes in different packages.
Why does the class path pose problems for modularity? Largely due to all the issues it hides, such as code versions, dependencies, and consistency.
Applications are generally composed of various versions of libraries and components.
The class path pays no attention to code versions—it returns the first version it finds.
Even if it did pay attention, there is no way to explicitly specify dependencies.
The process of setting up your class path is largely trial and error; you just keep adding libraries until the VM stops complaining about missing classes.
Figure 1.2 shows the sort of “class path hell” often found when more than one JAR file provides a given set of classes.
Even though each JAR file may have been compiled to work as a unit, when they’re merged at execution time, the Java class path pays no attention to the logical partitioning of the components.
This tends to lead to hard-topredict errors, such as NoSuchMethodError, when a class from one JAR file interacts with an incompatible class version from another.
In large applications created from independently developed components, it isn’t uncommon to have dependencies on different versions of the same component, such as logging or XML parsing mechanisms.
The class path forces you to choose one version in such situations, which may not always be possible.
Worse, if you have multiple versions of the same package on the class path, either on purpose or accidentally, they’re treated as split packages by Java and are implicitly merged based on order of appearance.
Overall, the class path approach lacks any form of consistency checking.
You get whatever classes have been made available by the system administrator, which is likely only an approximation of what the developer expected.
Java also lacks support when it comes to deploying and managing your application.
There is no easy way in Java to deploy the proper transitive set of versioned code dependencies and execute your application.
The same is true for evolving your application and its components after deployment.
Figure 1.2 Multiple JARs containing overlapping classes and/or packages are merged based on their order of appearance in the class path, with no regard to logical coherency among archives.
Consider the common requirement of wanting to support a dynamic plugin mechanism.
The only way to achieve such a benign request is to use class loaders, which are low level and error prone.
Class loaders were never intended to be a common tool for application developers, but many of today’s systems require their use.
A properly defined modularity layer for Java can deal with these issues by making the module concept explicit and raising the level of abstraction for code partitioning.
With this better understanding of Java’s limitations when it comes to modularity, we can ponder whether OSGi is the right solution for your projects.
Nearly all but the simplest of applications can benefit from the modularity features OSGi provides, so if you’re wondering if OSGi is something you should be interested in, the answer is most likely, “Yes!” Still not convinced? Here are some common scenarios you may have encountered where OSGi can be helpful:
OSGi can help by ensuring that code dependencies are satisfied before allowing the code to execute.
Execution-time errors from your application due to the wrong version of a dependent library on the class path.
OSGi verifies that the set of dependencies are consistent with respect to required versions and other constraints.
Type inconsistencies when sharing classes among modules: put more concretely, the dreaded appearance of foo instanceof Foo == false.
With OSGi, you don’t have to worry about the constraints implied by hierarchical classloading schemes.
Packaging an application as logically independent JAR files and deploying only those pieces you need for a given installation.
Packaging an application as logically independent JAR files, declaring which code is accessible from each JAR file, and having this visibility enforced.
OSGi enables a new level of code visibility for JAR files that allows you to specify what is and what isn’t visible externally.
Defining an extensibility mechanism for an application, like a plugin mechanism.
OSGi modularity is particularly suited to providing a powerful extensibility mechanism, including support for execution-time dynamism.
As you can see, these scenarios cover a lot of use cases, but they’re by no means exhaustive.
The simple and non-intrusive nature of OSGi tends to make you discover more ways to apply it the more you use it.
Having explored some of the limitations of the standard Java class path, we’ll now properly introduce you to OSGi.
The OSGi Service Platform is composed of two parts: the OSGi framework and OSGi standard services (depicted in figure 1.3)
The framework is the runtime that implements and provides OSGi functionality.
The standard services define reusable APIs for common tasks, such as Logging and Preferences.
The framework specification is now on its fourth major revision and is stable.
Technology based on this specification is in use in a range of large-scale industry applications, including (but not limited to) automotive, mobile devices, desktop applications, and more recently enterprise application servers.
This acronym highlights the lineage of the technology but has fallen out of favor.
After the third specification release, the OSGi Alliance officially dropped the acronym, and OSGi is now a trademark for the technology.
In the bulk of this book, we’ll discuss the OSGi framework, its capabilities, and how to use these capabilities.
Because there are so many standard services, we’ll discuss only the most relevant and useful services, where appropriate.
For any service we miss, you can get more information from the OSGi specifications.
For now, we’ll continue our overview of OSGi by introducing the broad features of the OSGi framework.
The OSGi framework plays a central role when you create OSGi-based applications, because it’s the application’s execution environment.
The OSGi Alliance’s framework specification defines the proper behavior of the framework, which gives you a welldefined API to program against.
This ultimately benefits you, because you aren’t tied to a particular vendor and can program against the behavior defined in the specification.
It’s sort of like the reassuring feeling you get by knowing you can go into any McDonald’s anywhere in the world and get the same meal!
You may not know it, but if you use an IDE to do your Java development, it’s possible you already have experience with OSGi.
The Equinox OSGi framework implementation is the underlying runtime for.
Figure 1.3 The OSGi Service Platform specification is divided into halves, one for the OSGi framework and one for standard services.
Likewise, if you use the GlassFish v3 application server, you’re also using OSGi, because the Apache Felix OSGi framework implementation is its runtime.
The diversity of use cases attests to the value and flexibility provided by the OSGi framework through three conceptual layers defined in the OSGi specification (see figure 1.4):
Lifecycle layer—Concerned with providing execution-time module management and access to the underlying OSGi framework.
Service layer—Concerned with interaction and communication among modules, specifically the components contained in them.
Like typical layered architectures, each layer is dependent on the layers beneath it.
Therefore, it’s possible for you to use lower OSGi layers without using upper ones, but not vice versa.
The next three chapters discuss these layers in detail, but we’ll give an overview of each here.
The module layer defines the OSGi module concept, called a bundle, which is a JAR file with extra metadata (data about data)
A bundle contains your class files and their related resources, as depicted in figure 1.5
Bundles typically aren’t an entire application packaged into a single JAR file; rather, they’re the logical modules that combine to form a given application.
Bundles are more powerful than standard JAR files, because you can explicitly declare which contained packages are externally visible (that is, exported packages)
In this sense, bundles extend the normal access modifiers (public, private, and protected) associated with the Java language.
Another important advantage of bundles over standard JAR files is the fact that you can explicitly declare on which external packages the bundles depend (that is, imported packages)
The main benefit of explicitly declaring your bundles’ exported and imported packages is that the OSGi framework can manage and verify their consistency automatically; this.
An architectural overview of OSGi process is called bundle resolution and involves matching exported packages to imported packages.
The lifecycle layer defines how bundles are dynamically installed and managed in the OSGi framework.
If you were building a house, the module layer would provide the foundation and structure, and the lifecycle layer would be the electrical wiring.
External to your application, the lifecycle layer precisely defines the bundle lifecycle operations (install, update, start, stop, and uninstall)
These lifecycle operations allow you to dynamically administer, manage, and evolve your application in a well-defined way.
This means bundles can be safely added to and removed from the framework without restarting the application process.
Internal to your application, the lifecycle layer defines how your bundles gain access to their execution context, which provides them with a way to interact with the OSGi framework and the facilities it provides during execution.
This overall approach to the lifecycle layer is powerful because it lets you create externally (and remotely) managed applications or completely self-managed applications (or any combination)
Finally, the service layer supports and promotes a flexible application programming model incorporating concepts popularized by service-oriented computing (although these concepts were part of the OSGi framework before service-oriented computing became popular)
The main concepts revolve around the service-oriented publish, find, and bind interaction pattern: service providers publish their services into a service registry, while service clients search the registry to find available services to use (see figure 1.6)
Nowadays, this service-oriented architecture (SOA) is largely associated with web services; but OSGi services are local to a single VM, which is why some people refer to it as SOA in a VM.
The OSGi service layer is intuitive, because it promotes an interface-based development approach, which is generally considered good practice.
OSGi services are Java interfaces representing a conceptual contract between service providers and service clients.
This makes the service layer lightweight, because service providers are just Java objects accessed via direct method invocation.
Providers publish services into a registry where requesters can discover which services are available for use.
The result is a programming model eschewing the monolithic and brittle approaches of the past, in favor of being modular and flexible.
This sounds well and good, but you may still be wondering how these three layers fit together and how you go about using them to create an application on top of them.
In the next couple of sections, we’ll explore how these layers fit together using some small example programs.
The OSGi framework is made up of layers, but how do you use these layers in application development? We’ll make it clearer by outlining the general approach you’ll use when creating an OSGi-based application:
Design your application by breaking it down into service interfaces (normal interface-based programming) and clients of those interfaces.
Implement your service provider and client components using your preferred tools and practices.
If you’re already following an interface-based approach, the OSGi approach will feel familiar.
The main difference will be how you locate your interface implementations (that is, your services)
Normally, you might instantiate implementations and pass around references to initialize clients.
In the OSGi world, your services will publish themselves in the service registry, and your clients will look up available services in the registry.
After your bundles are installed and started, your application will start and execute as normal, but with several advantages.
Underneath, the OSGi framework provides more rigid modularity and consistency checking, and its dynamic nature opens up a world of possibilities.
Don’t fret if you don’t or can’t use an interfaced-based approach for your development.
The first two layers of the OSGi framework still provide a lot of functionality; in truth, the bulk of OSGi framework functionality lies in these first two layers, so keep reading.
Because OSGi functionality is divided over the three layers mentioned previously (modularity, lifecycle, and service), we’ll show you three different “Hello, world!” examples that illustrate each of these layers.
The module layer isn’t related to code creation as such; rather, it’s related to the packaging of your code into bundles.
For example, suppose you want to share the following class.
During the build process, you compile the source code and put the generated class file into a JAR file.
This is followed by the symbolic name and version bundle identifier.
In this example, the bulk of the metadata is related to bundle identification.
The important part is the Export-Package statement, because it extends the functionality of a typical JAR file with the ability for you to explicitly declare which packages contained in the JAR are visible to its users.
In this example, only the contents of the org.foo.hello package are externally visible; if the example included other packages, they wouldn’t be externally visible.
This means that when you run your application, other modules won’t be able to accidentally (or intentionally) depend on packages your module doesn’t explicitly expose.
To use this shared code in another module, you again add metadata.
This time, you use the Import-Package statement to explicitly declare which external packages are required by the code contained in the client JAR.
In this case, the last line specifies a dependency on an external package.
Although the example is simple, it illustrates that creating OSGi bundles out of existing JAR files is a reasonably non-intrusive process.
In addition, there are tools that can help you create your bundle metadata, which we’ll discuss in appendix A; but in reality, no special tools are required to create a bundle other than what you normally use to create a JAR file.
Chapter 2 will go into all the juicy details of OSGi modularity.
In the last subsection, you saw that it’s possible to take advantage of OSGi functionality in a non-invasive way by adding metadata to your existing JAR files.
Such a simple approach is sufficient for most reusable libraries, but sometimes you need or want to go further to meet specific requirements or to use additional OSGi features.
The lifecycle layer moves you deeper into the OSGi world.
Perhaps you want to create a module that performs some initialization task, such as starting a background thread or initializing a driver; the lifecycle layer makes this possible.
Bundles may declare a given class as an activator, which is the bundle’s hook into its own lifecycle management.
We’ll discuss the full lifecycle of a bundle in chapter 3, but first let’s look at a simple example to give you an idea of what we’re talking about.
The following listing extends the previous Greeting class to provide a singleton instance.
Listing 1.4 implements a bundle activator to initialize the Greeting class singleton when the bundle is started and clear it when it’s stopped.
The client can now use the preconfigured singleton instead of creating its own instance.
A bundle activator must implement a simple OSGi interface, which in this case is composed of the two methods start() and stop()
At execution time, the framework constructs an instance of this class and invokes the start() method when the bundle is started and the stop() method when the bundle is stopped.
Because the framework uses the same activator instance while the bundle is active, you can share member variables between the start() and stop() methods.
You may wonder what the single parameter of type BundleContext in the start() and stop() methods is all about.
This is how the bundle gets access to the OSGi framework in which it’s executing.
From this context object, the module has access to all the OSGi functionality for modularity, lifecycle, and services.
In short, it’s a fairly important object for most bundles, but we’ll defer a detailed introduction of it until later when we discuss the lifecycle layer.
The important point to take away from this example is that bundles have a simple way to hook into their lifecycle and gain access to the underlying OSGi framework.
Of course, it isn’t sufficient to just create this bundle activator implementation; you have to tell the framework about it.
If you have an existing JAR file you’re converting to be a module, you must add the activator implementation to the existing project so the class is included in the resulting JAR file.
If you’re creating a bundle from scratch, you need to compile the class and put the result in a JAR file.
You must also tell the OSGi framework about the bundle activator by adding another piece of metadata to the JAR file manifest.
For this section’s example, you add the following metadata to the JAR manifest:
We’ve now introduced how to create OSGi bundles out of existing JAR files using the module layer and how to make your bundles lifecycle aware so they can use framework functionality.
The last example in this section demonstrates the service-oriented programming approach promoted by OSGi.
If you follow an interfaced-based approach in your development, the OSGi service approach will feel natural to you.
For any given implementation of the Greeting interface, when the sayHello() method is invoked, a greeting will be displayed.
In general, a service represents a contract between a provider and prospective clients; the semantics of the contract are typically described in a separate, human-readable document, like a specification.
The previous service interface represents the syntactic contract of all Greeting implementations.
The notion of a contract is necessary so that clients can be assured of getting the functionality they expect when using a Greeting service.
The precise details of how any given Greeting implementation performs its task aren’t known to the client.
For example, one implementation may print its greeting textually, whereas another may display its greeting in a GUI dialog box.
Your may be thinking that nothing in the service interface or listing 1.5 indicates that you’re defining an OSGi service.
That’s what makes the OSGi’s service approach so natural if you’re already following an interface-based approach; your code will largely stay the same.
Your development will be a little different in two places: how you make a service instance available to the rest of your application, and how the rest of your application discovers the available service.
All service implementations are ultimately packaged into a bundle, and that bundle must be lifecycle aware in order to register the service.
This means you need to create a bundle activator for the example service, as shown next.
This time, in the start() method, instead of storing the Greeting implementation as a singleton, you use the provided bundle context to register it as a service in the service registry.
The first parameter you need to provide is the interface name(s) that the service implements, followed by the actual service instance, and finally the service properties.
In the stop() method, you could unregister the service implementation before stopping the bundle; but in practice, you don’t need to do this.
The OSGi framework automatically unregisters any registered services when a bundle stops.
You’ve seen how to register a service, but what about discovering a service? The following listing shows a simplistic client that doesn’t handle missing services and that suffers from potential race conditions.
Notice that accessing a service in OSGi is a two-step process.
First, an indirect reference is retrieved from the service registry B.
The service reference can be safely stored in a member variable; but in general it isn’t a good idea to hold on to references to service object instances, because services may be unregistered dynamically, resulting in stale references that prevent garbage collection of uninstalled bundles.
Both the service implementation and the client should be packaged into separate bundle JAR files.
The metadata for each bundle declares its corresponding activator, but the service implementation exports the org.foo.hello package, whereas the client imports it.
Note that the client bundle’s metadata only needs to declare an import for the Greeting interface package—it has no direct dependency on the service implementation.
This makes it easy to swap service implementations dynamically without restarting the client bundle.
Now that you’ve seen some examples, you can better understand how each layer of the OSGi framework builds on the previous one.
Each layer gives you additional capabilities when building your application, but OSGi technology is flexible enough for you to adopt it according to your specific needs.
If you only want better modularity in your project, use the module layer.
If you want a way to initialize modules and interact with the module layer, use both the module and lifecycle layers.
If you want a dynamic, interface-based development approach, use all three layers.
To help introduce the concepts of each layer in the OSGi framework in the next three chapters, we’ll use a simple paint program; its user interface is shown in figure 1.7
The paint program isn’t intended to be independently useful; rather, it’s used to demonstrate common issues and best practices.
From a functionality perspective, the paint program only allows the user to paint various shapes, such as circles, squares, and triangles.
Available shapes are displayed as buttons in the main window’s toolbar.
To draw a shape, the user selects it in the toolbar and then clicks anywhere in the canvas to draw it.
The same shape can be drawn repeatedly by clicking in the canvas numerous times.
The real value of using a visual program for demonstrating these concepts will become evident when we start introducing execution-time dynamism.
We’ve finished our overview of the OSGi framework and are ready to delve into the details; but before we do, we’ll put OSGi in context by discussing similar or related technologies.
Although no Java technology fills the exact same niche as OSGi, several tread similar ground, and it’s worth understanding their relevance before moving forward.
OSGi is often mentioned in the same breath with many other technologies, but it’s in a fairly unique position in the Java world.
Over the years, no single technology has addressed OSGi’s exact problem space, but there have been overlaps, complements, and offshoots.
Although it isn’t possible to cover how OSGi relates to every conceivable technology, we’ll address some of the most relevant in roughly chronological order.
After reading this section, you should have a good idea whether OSGi replaces your familiar technologies or is complementary to them.
Only within the last couple of years has OSGi technology begun to take root in the enterprise space.
The Enterprise JavaBeans (EJB) specification is probably the closest comparable technology from the Java EE space, because it defines a component model and packaging format.
But its component model focuses on providing a standard way to implement enterprise applications that must regularly handle issues of persistence, transactions, and security.
The EJB deployment descriptors and packaging formats are relatively simplistic and don’t address the full component lifecycle, nor do they support clean modularity concepts.
OSGi is also used in the Java EE domain to provide a more sophisticated module layer beneath these existing technologies.
Because the two ignored each other for so long, there are some challenges in moving existing Java EE concepts to OSGi, largely due to different assumptions about how class loading is performed.
An often-overlooked Java technology is Jini, which is definitely a conceptual sibling of OSGi.
Jini targets OSGi’s original problem space of networked environments with a variety of connected devices.
The goal of Jini is to make it possible to administer a networked environment as a flexible, dynamic group of services.
Jini introduces the concepts of service providers, service consumers, and a service lookup registry.
All of this sounds completely isomorphic to OSGi; where Jini differs is its focus on distributed systems.
Consumers access clients through some form of proxy using a remote procedure call mechanism, such as Remote Method Invocation (RMI)
The servicelookup registry is also a remotely accessible, federated service.
The Jini model assumes remote access across multiple VM processes, whereas OSGi assumes everything occurs in a single VM process.
But in stark contrast to OSGi, Jini doesn’t define any modularity mechanisms and relies on the execution-time code-loading features of RMI.
The open source project Newton is an example of combining OSGi and Jini technologies in a single framework.
NetBeans, an IDE and application platform for Java, has a long history of having a modular design.
Sun purchased NetBeans in 1999 and has continued to evolve it.
The NetBeans platform has a lot in common with OSGi.
It defines a fairly sophisticated module layer and also promotes interface-based programming using a lookup pattern that is similar to the OSGi service registry.
Whereas OSGi focused on embedded devices and dynamism, the NetBeans platform was originally an implementation layer for the IDE.
Eventually, the platform was promoted as a separate tool in its own right, but it focused on being a complete GUI application platform with abstractions for file systems, windowing systems, and much more.
NetBeans has never been seen as comparable to OSGi, even though it is; perhaps OSGi’s more narrow focus is an asset in this case.
Why did the comparisons arise in the first place? There are probably three reasons: the JMX component model was sufficiently generic that it was possible to use it for building applications; the specification defined a mechanism for dynamically loading code into its server; and certain early adopters pushed JMX in this direction.
One major perpetrator was JBoss, which adopted and extended JMX for use as a module layer in its.
Putting OSGi in context application server (since eliminated in JBoss 5)
Nowadays, JMX isn’t (and shouldn’t be) confused with a module system.
Around 2003, lightweight or inversion of control (IoC) containers started to appear, such as PicoContainer, Spring, and Apache Avalon.
The main idea behind this crop of IoC containers was to simplify component configuration and assembly by eliminating the use of concrete types in favor of interfaces.
This was combined with dependency injection techniques, where components depend on interface types and implementations of the interfaces are injected into the component instance.
OSGi services promote a similar interface-based approach but employ a service-locator pattern to break a component’s dependency on component implementations, similar to Apache Avalon.
At the same time, the Service Binder project was creating a dependency injection framework for OSGi components.
Regardless, OSGi’s use of interface-based services and the service-locator pattern long predated this trend, and none of these technologies offer a sophisticated dynamic module layer like OSGi.
There is now significant movement from IoC vendors to port their infrastructures to the OSGi framework, such as the work by VMware (formerly SpringSource) on the OSGi Blueprint specification (discussed in chapter 12)
In JBI, plugin components provide and consume services after they’re plugged in to the JBI framework.
Components don’t directly interact with services, as in OSGi; instead, they communicate indirectly using normalized Web Services Description Language (WSDL)-based messages.
Due to the inherent similarities to OSGi’s architecture, it was easy to think JBI was competing for a similar role.
On the contrary, its fairly simplistic modularity mechanisms mainly addressed basic component integration into the framework.
It made more sense for JBI to use OSGi’s more sophisticated modularity, which is ultimately what happened in Project Fuji from Sun and ServiceMix from Apache.
From the perspective of the OSGi Alliance, this was a major case of reinventing the wheel, because the effort was starting from scratch rather than building on the experience gained from OSGi.
Its goal was to focus on necessary language changes for modularity.
The original idea was to introduce the notion of a superpackage into the Java language—a package of packages.
The specification of superpackages got bogged down in details until it was scrapped in favor of adding a module-access modifier keyword to the language.
With JSR 277 on hold, Sun introduced an internal project, called Project Jigsaw, to modularize the JDK.
The details of Jigsaw are still evolving after the acquisition of Sun by Oracle.
Its component model is more advanced because it defines composite components (components made of other components) for a fully recursive component model.
The SCA specification leaves open the possibility of other packaging formats, which makes it possible to use OSGi as a packaging and module layer for Java-based SCA implementations; Apache Tuscany and Newton are examples of an SCA implementation that use OSGi.
In addition, bundles could be used to implement SCA component types, and SCA could be used as a mechanism to provide remote access to OSGi services.
Although Microsoft’s .NET (released in 2002) isn’t a Java technology, it deserves mention because it was largely inspired by Java and did improve on it in ways that are similar.
Microsoft not only learned from Java’s example but also learned from the company’s own history of dealing with DLL hell.
As a result, .NET includes the notion of an assembly, which has modularity aspects similar to an OSGi bundle.
All .NET code is packaged into an assembly, which takes the form of a DLL or EXE file.
Assemblies provide an encapsulation mechanism for the code contained inside of them; an access modifier, called internal, is used to indicate visibility within an assembly but not external to it.
Assemblies also contain metadata describing dependencies on other assemblies, but the overall model isn’t as flexible as OSGi’s.
Because dependencies are on specific assembly versions, the OSGi notion of provider substitutability isn’t attainable.
At execution time, assemblies are loaded into application domains and can only be unloaded by unloading the entire application domain.
This makes the highly dynamic and lightweight nature of OSGi hard to achieve, because multiple assemblies loaded into the same application domain must be unloaded at the same time.
It’s possible to load assemblies into separate domains; but then communication across domains must use interprocess communication to collaborate, and type sharing is greatly complicated.
There have been research efforts to create OSGi-like environments for the .NET platform, but the innate differences between the .NET and Java platforms results in the two not having much in common.
Regardless, .NET deserves credit for improving on standard Java in this area.
In this chapter, we’ve laid the foundation for everything we’ll cover in the rest of the book.
The Java platform is great for developing applications, but its support for modularity is largely limited to fine-grained object-oriented mechanisms, rather than more coarse-grained modularity features needed for project management.
The OSGi Service Platform, through the OSGi framework, addresses the modularity shortcomings of Java to create a powerful and flexible solution.
The declarative, metadata-based approach employed by OSGi provides a noninvasive way to take advantage of its sophisticated modularity capabilities by modifying how projects are packaged with few, if any, changes to the code.
The OSGi framework defines a controlled, dynamic module lifecycle to simplify management.
Following good design principles, OSGi promotes an interface-based programming approach to separate interfaces from implementations.
This is the foundation of everything else in the OSGi world.
In the previous chapter, we took a whistle-stop tour of the OSGi landscape.
We made a number of observations about how standard Java is broken with respect to modularity and gave you examples where OSGi can help.
We also introduced you to some OSGi concepts, including the core layers of the OSGi framework: module, lifecycle, and service.
In this chapter, we’ll deal specifically with the module layer, because its features are the initial attraction for most Java developers to OSGi.
The module layer is the foundation on which everything else rests in the OSGi world.
We’ll provide you with a full understanding of what OSGi modularity is, why modularity is important in a general sense, and how it can help you in designing, building, and maintaining Java applications in the future.
What is modularity? The goal of this chapter is to get you thinking in terms of modules rather than JAR files.
We’ll teach you about OSGi module metadata, and you’ll learn how to describe your application’s modularity characteristics with it.
To illustrate these concepts, we’ll continue the simple paint program example that we introduced in chapter 1; you’ll convert it from a monolithic application into a modular one.
The more experience you have with system design, the more you know good designs tend to be modular—but what precisely does that mean? In short, it means designing a complete system from a set of logically independent pieces; these logically independent pieces are called modules.
You may be thinking, “Is that it?” In the abstract, yes, that is it; but of course there are a lot of details underneath these simple concepts.
A module defines an enforceable logical boundary: code either is part of a module (it’s on the inside) or it isn’t part of a module (it’s on the outside)
The internal (implementation) details of a module are visible only to code that is part of a module.
For all other code, the only visible details of a module are those that it explicitly exposes (the public API), as depicted in figure 2.1
This aspect of modules makes them an integral part of designing the logical structure of an application.
You may wonder, “Hey, doesn’t object orientation give you these things?” That’s correct: object orientation is intended to address these issues too.
You’ll find that modularity provides many of the same benefits as object orientation.
One reason these two programming concepts are similar is because both are forms of separation of concerns.
The idea behind separation of concerns is you should break down a system into minimally overlapping functionality or concerns, so that each concern can be independently reasoned about, designed, implemented, and used.
Modularity is one of the earliest forms of separation of concerns.
With that said, you may now be wondering, “If I already have object orientation in Java, why do I need modularity too?” Another good question.
The module itself is explicitly in control of which classes are completely encapsulated and which are exposed for external use.
You sit down and start writing Java classes to implement the desired functionality.
Do you typically implement all your functionality in a single class? No.
If the functionality is even remotely complicated, you implement it as a set of classes.
You may also use existing classes from other parts of your project or from the JRE.
When you’re done, a logical relationship exists among the classes you created—but where is this relationship captured? Certainly it’s captured in the low-level details of the code, because there are compilation dependencies that won’t be satisfied if all classes aren’t available at compilation time.
Likewise, at execution time, these dependencies will fail if all classes aren’t present on the class path when you try to execute your application.
Unfortunately, these relationships among classes can only be known through lowlevel source code inspection or trial and error.
Classes allow you to encapsulate the state and behavior of a single, logical concept.
But numerous classes are generally necessary to create a well-designed application.
Modules encapsulate classes, allowing you to express the logical relationship among the classes—or concepts—in your application.
Figure 2.2 illustrates how modules encapsulate classes, and the resulting inter-module relationships.
You may think that Java packages allow you to capture such logical code relationships.
Packages are a form of built-in modularity provided by Java, but they have some limitations, as discussed in section 1.1.1
So packages are a good starting point in understanding how modularity helps you encapsulate code, but you need a mechanism that goes further.
In the end, object orientation and modularity serve different but complementary purposes (see figure 2.3)
When you’re developing in Java, you can view object orientation as the implementation approach for modules.
As such, when you’re developing classes, you’re programming in the small, which means you aren’t thinking about the overall structure of your application, but instead are thinking in terms of specific functionality.
After you begin to logically organize related classes into modules, then you start to concern yourself with programming in the large, which means you’re focusing on the larger logical pieces of your system and the relationships among those pieces.
In addition to capturing relationships among classes via module membership, modules also capture logical system structure by explicitly declaring dependencies on external code.
Figure 2.2 Classes have explicit dependencies due to the references contained in the code.
Modules have implicit dependencies due to the code they contain.
Figure 2.3 Even though object orientation and modularity provide similar capabilities, they address them at different levels of granularity.
Why modularize? this in mind, we now have all the pieces in place to more concretely define what we mean by the term module in the context of this book.
Although this definition implies that modules contain classes, at this point this sense of containment is purely logical.
Another aspect of modularity worth understanding is physical modularity, which refers to the container of module code.
The OSGi module layer allows you to properly express the modularity characteristics of applications, but it’s not free.
Let’s look in more depth at why you should modularize your applications, so you can make up your own mind.
In fact, you may be thinking, “If modularity has been around for almost 40 years and it’s so important, why isn’t everyone already doing it?” That’s a great question, and one that probably doesn’t have any single answer.
The computer industry is driven by the next best thing, so we tend to throw out the old when the new comes along.
And in fairness, as we discussed in the last section, the new technologies and approaches (such as object orientation and component orientation) do provide some of the same benefits that modularity was intended to address.
Java also provides another important reason why modularity is once again an important concern.
Traditionally, programming languages were the domain of logical modularity mechanisms, and operating systems and/or deployment packaging systems were the domain of physical modularity.
Java blurs this distinction because it’s both a language and a platform.
To compare to a similar situation, look at the .NET platform.
Microsoft, given its history of operating system development and the pain of.
Physical modularity refers to how code is packaged and/or made available for deployment.
In OSGi, these two concepts are largely conflated; a logical module is referred to as a bundle, and so is the physical module (that is, the JAR file)
Even though these two concepts are nearly synonymous in OSGi, for modularity in general they aren’t, because it’s possible to have logical modularity without physical modularity or to package multiple logical modules into a single physical module.
Physical modules are sometimes also referred to as deployment modules or deployment units.
Finally, the size of applications continues to grow, which makes modularity an important part of managing their complexity—divide and conquer!
This discussion provides some potential explanations for why modularity is coming back in vogue, but it doesn’t answer this section’s original question: Why should you modularize your applications? Modularity allows you to reason about the logical structure of applications.
Cohesion measures how closely aligned a module’s classes are with each other and with achieving the module’s intended functionality.
For example, a module shouldn’t address many different concerns (network communication, persistence, XML parsing, and so on): it should focus on a single concern.
Coupling, on the other hand, refers to how tightly bound or dependent different modules are on each other.
For example, you don’t want every module to depend on all other modules.
As you start to use OSGi to modularize your applications, you can’t avoid these issues.
Modularizing your application will help you see your application in a way that you couldn’t before.
By keeping these principles of cohesion and coupling in mind, you’ll create more reusable code, because it’s easier to reuse a module that performs a single function and doesn’t have a lot of dependencies on other code.
More specifically, by using OSGi to modularize your applications, you’ll be able to address the Java limitations discussed in section 1.1.1
Additionally, because the modules you’ll create will explicitly declare their external code dependencies, reuse is further simplified because you’ll no longer have to scrounge documentation or resort to trial and error to figure out what to put on the class path.
This results in code that more readily fits into collaborative, independent development approaches, such as in multiteam, multilocation projects or in large-scale open source projects.
Now you know what modularity is and why you want it, so let’s begin to focus on how OSGi provides it and what you need to do to use it in your own applications.
The example paint program will help you understand the concepts.
The functionality provided by OSGi’s module layer is sophisticated and can seem overwhelming when taken in total.
You’ll use a simple paint program, as discussed in chapter 1, to learn how to use OSGi’s module layer.
You’ll start from an existing paint program, rather than creating one from scratch.
The existing implementation follows an interfaced-based approach with logical package structuring, so it’s amenable to modularization, but it’s currently packaged as a single JAR file.
The following listing shows the contents of the paint program’s JAR file.
Then come the application classes, followed by various shape implementations.
The main classes composing the paint program are described in table 2.1
For those familiar with Swing, PaintFrame extends JFrame and contains a JToolBar and a JPanel canvas.
PaintFrame maintains a list of available SimpleShape implementations, which it displays in the toolbar.
When the user selects a shape in the toolbar and clicks in the canvas to draw the shape, a ShapeComponent (which extends JComponent) is added to the canvas at the location where the user clicked.
A ShapeComponent is associated with a specific SimpleShape implementation by name, which it retrieves from a reference to its PaintFrame.
Figure 2.4 highlights some of the UI elements in the paint program GUI.
Table 2.1 Overview of the classes in the paint program.
It also has a static main() method to launch the program.
A static main() method on PaintFrame launches the paint program, which creates an instance of the PaintFrame and each shape implementation, adding each shape instance to the created PaintFrame instance.
For further explanation, figure 2.5 captures the paint program classes and their interrelationships.
To run this nonmodular version of the paint program, go into the chapter02/ paint-nonmodular/ directory of the companion code.
Type ant to build the program, and then type java -jar main.jar to run it.
Feel free to click around and see how it works; we won’t go into any more details of the program’s implementation, because GUI programming is beyond the scope of this book.
The important point is to understand the structure of the program.
Using this understanding, you’ll divide the program into bundles so you can enhance and enforce its modularity.
Currently, the paint program is packaged as a single JAR file, which we’ll call version 1.0.0 of the program.
Because everything is in a single JAR file, this implies that the program isn’t already modularized.
Of course, single-JAR-file applications can still be implemented in a modular way—just because an application is composed of multiple JAR files, that doesn’t mean it’s modular.
The paint program example could have both its logical and physical modularity improved.
First, we’ll examine the program’s logical structure and define modules to enhance this structure.
One low-hanging fruit you can look for is public APIs.
It’s good practice in OSGi (you’ll see why later) to separate your public APIs into packages so they can be easily shared without worrying about exposing implementation details.
The paint program has a good example of a public API: its SimpleShape interface.
Figure 2.4 The paint program is a simple Swing application.
Introducing bundles easy to implement new, possibly third-party shapes for use with the program.
Unfortunately, SimpleShape is in the same package as the program’s implementation classes.
To remedy this situation, you’ll shuffle the package structure slightly.
These changes divide the paint program into three logical pieces according to the package structure:
Given this structure (logical modularity), you could package each of these packages as separate JAR files (physical modularity)
To have OSGi verify and enforce the modularity, it isn’t sufficient to package the code as JAR files: you must package them as bundles.
To do this, you need to understand OSGi’s bundle concept, which is its logical and physical unit of modularity.
If you’re going to use OSGi technology, you may as well start getting familiar with the term bundle, because you’ll hear and say it a lot.
Bundle is how OSGi refers to its specific realization of the module concept.
Throughout the remainder of this book, the terms module and bundle will be used interchangeably; but in most cases we’re specifically referring to bundles and not modularity in general, unless otherwise noted.
Enough fuss about how we’ll use the term bundle—let’s define it.
The contents of a bundle are graphically depicted in figure 2.6
The main thing that makes a bundle JAR file different than a normal JAR file is its module metadata, which is used by the OSGi framework to manage its modularity characteristics.
All JAR files, even if they aren’t bundles, have a place for metadata, which is in their manifest file or, more specifically, in the.
Figure 2.6 A bundle can contain all the usual artifacts you expect in a standard JAR file.
The only major difference is that the manifest file contains information describing the bundle’s modular characteristics.
Whenever we refer to a bundle’s manifest file, we’re specifically referring to the module-related metadata in this standard JAR manifest file.
Note that this definition of a bundle is similar to the definition of a module, except that it combines both the physical and logical aspects of modularity into one concept.
So before we get into the meat of this chapter, which is defining bundle metadata, let’s discuss the bundle’s role in physical and logical modularity in more detail.
The main function of a bundle with respect to physical modularity is to determine module membership.
No metadata is associated with making a class a member of a bundle.
A given class is a member of a bundle if it’s contained in the bundle JAR file.
The benefit for you is that you don’t need to do anything special to make a class a member of a bundle: just put it in the bundle JAR file.
This physical containment of classes leads to another important function of bundle JAR files as a deployment unit.
The bundle JAR file is tangible, and it’s the artifact you share, deploy, and use when working with OSGi.
The final important role of the bundle JAR file is as the container of bundle metadata, because, as we mentioned, the JAR manifest file is used to store it.
These aspects of the bundle are shown in figure 2.7
The issue of metadata placement is part of an ongoing debate, which we address in the sidebar for those interested in the issue.
Figure 2.7 A class is a member of a bundle if it’s packaged in it, the bundle carries its module metadata inside it as part of its manifest data, and the bundle can be deployed as a unit into a runtime environment.
Where should metadata go? Is it a good thing to store the module metadata in the physical module and not in the classes themselves? There are two schools of thought on this subject.
One says it’s better to include the metadata alongside the code it’s describing (in the source file itself), rather than in a separate file where it’s more difficult to see the connection to the code.
Similar to how the bundle JAR file physically encapsulates the member classes, the bundle’s role in logical modularity is to logically encapsulate member classes.
What precisely does this mean? It specifically relates to code visibility.
Imagine that you have a utility class in a util package that isn’t part of your project’s public API.
To use this utility class from different packages in your project, it must be public.
Unfortunately, this means anyone can use the utility class, even though it’s not part of your public API.
The logical boundary created by a bundle changes this, giving classes inside the bundle different visibility rules to external code, as shown in figure 2.8
This means public classes inside your bundle JAR file aren’t necessarily externally visible.
You may be thinking, “What?” This isn’t a misstatement: it’s a major differentiator between bundles and standard JAR files.
Only code explicitly exposed via bundle metadata is visible externally.
Unfortunately, when OSGi work started back in 1999, annotations weren’t an option because they didn’t exist yet.
Besides, there are some good reasons to keep the metadata in a separate file, which brings us to the second school of thought.
This school of thought argues that it’s better to not bake metadata into the source code, because it becomes harder to change.
Having metadata in a separate file offers you greater flexibility.
Regardless of whether your preferred approach is annotations, you can see that you gain a good deal of flexibility by maintaining the module metadata in the manifest file.
Figure 2.8 Packages (and therefore the classes in them) contained in a bundle are private to that bundle unless explicitly exposed, allowing them to be shared with other bundles.
If you’re familiar with .NET, this is similar to the internal access modifier, which marks something as being visible in an assembly but private from other assemblies.
As you can see, the bundle concept plays important roles in both physical and logical modularity.
Now we can start to examine how you use metadata to describe bundles.
In this section, we’ll discuss OSGi bundle metadata in detail, and you’ll use the paint program as a use case to understand the theory.
The main purpose of bundle metadata is to precisely describe the modularity-related characteristics of a bundle so the OSGi framework can handle it appropriately, such as resolving dependencies and enforcing encapsulation.
The module-related metadata captures the following pieces of information about the bundle:
We’ll look at each of these areas in the following subsections.
But because OSGi relies on the manifest file, we’ve included a sidebar to explain its persnickety syntax details and OSGi’s extended manifest value syntax.
Luckily, there are tools for editing and generating bundle metadata, so you don’t have to create it manually, but it’s still worthwhile to understand the syntax details.
The name isn’t case sensitive and can contain alphanumeric, underscore, and hyphen characters.
Values can contain any character information except carriage returns and line feeds.
The name and the value must be separated by a colon and a space.
If a line must exceed this length, you must continue it on the next line, which you do by starting the next line with a single space character followed by the continuation of the value.
Manifest lines in OSGi can grow quite long, so it’s useful to know this.
You define an attribute group by placing attribute declarations on successive lines (one line after the other) in the manifest file.
An empty or blank line between attribute declarations signifies different attribute groups.
OSGi uses the first group of attributes, called the main attributes, for module metadata.
If you look in a manifest file, you may see something like this:
Most bundle metadata is intended to be read and interpreted by the OSGi framework in its effort to provide a general module layer for Java.
But some bundle metadata serves no purpose other than helping humans understand what a bundle does and.
We’ll get into the exact meaning of most of these attributes throughout the remainder of this section.
Whereas the standard Java manifest syntax is a name-value pair, OSGi defines a common structure for OSGispecified attribute values.
Most OSGi manifest attribute values are a list of clauses separated by commas, such as.
Each clause is further broken down into a target and a list of name-value pair parameters separated by semicolons:
Parameters are divided into two types, called attributes and directives.
Directives alter framework handling of the associated information and are explicitly defined by the OSGi specification.
Slightly different syntax is used to differentiate directives (:=) from attributes (=), which looks something like this:
Keep in mind that you can have any number of directives and attributes assigned to each target, all with different values.
Values containing whitespace or separator characters should be quoted to avoid parsing errors.
Sometimes you’ll have lots of targets with the same set of directives and attributes.
In such a situation, OSGi provides a shorthand way to avoid repeating all the duplicated directives and attributes, as follows:
This is equivalent to listing the targets separately with their own directives and attributes.
This is pretty much everything you need to understand the structure of OSGi manifest attributes.
Not all OSGi manifest values conform to this common structure, but the majority do, so it makes sense for you to become familiar with it.
The OSGi specification defines several pieces of metadata for this purpose, but none of it is required, nor does it have any impact on modularity.
The following code snippet shows human-readable bundle metadata for the paint program’s org.foo.shape bundle (the other program bundles are described similarly):
The Bundle-Name attribute is intended to be a short name for the bundle.
Even though it’s supposed to be a short name, there’s no enforcement of this; just use your best judgment.
The BundleDescription attribute lets you be a little more long-winded in describing the purpose of your bundle.
To provide even more documentation about your bundle, BundleDocURL allows you to specify a URL to refer to documentation.
OSGi doesn’t define any standard category names, so you’re free to choose your own.
The fact that the OSGi framework ignores it means you can pretty much do what you want to with it.
But don’t fall into a laissez-faire approach just yet—the remaining metadata requires more precision.
Next, we’ll look at how you use metadata to uniquely identify bundles.
The human-readable metadata from the previous subsection helps you understand what a bundle does and where it comes from.
Some of this human-readable metadata also appears to play a role in identifying a bundle.
For example, Bundle-Name seems like it could be a form of bundle identification.
Earlier versions of the OSGi specification didn’t provide any means to uniquely identify a given bundle.
Bundle-Name was purely informational, and therefore no constraints were placed on its value.
As part of the OSGi R4 specification process, the idea of a unique bundle identifier was proposed.
The value of the symbolic name follows rules similar to Java package naming: it’s a series of dotseparated strings, where reverse domain naming is recommended to avoid name.
You’re free to choose a different scheme; but if you do, keep in mind that the main purpose is to provide unique identification, so try to choose a scheme that won’t lead to name clashes.
For the public API bundle, you declare the symbolic name in manifest file as.
This is possible, but it’s cumbersome; and worse, this versioning information would be opaque to the OSGi framework, which means the modularity layer couldn’t take advantage of it.
This pair of attributes not only forms an identifier, it also allows the framework to capture the time-ordered relationship among versions of the same bundle.
For example, the following metadata uniquely identifies the paint program’s public API bundle:
Starting with the R4 specification, it became mandatory for bundles to specify BundleSymbolicName.
The following example shows the complete OSGi R4 metadata to identify the shape bundle:
This is the complete identification metadata for the public API bundle.
The identification metadata for the other paint program bundles are defined in a similar fashion.
Now that bundle identification is out of the way, we’re ready to look at code visibility, which is perhaps the most important area of metadata.
The OSGi specification defines a common version number format that’s used in a number of places throughout the specification.
For this reason, it’s worth spending a few paragraphs exploring exactly what a version number is in the OSGi world.
A version number is composed of three separate numerical component values separated by dots; for example, 1.0.0 is a valid OSGi version number.
The first value is referred to as the major number, the second value as the minor number, and the third value as the micro number.
A fourth version component is possible, which is called a qualifier.
The qualifier can contain alphanumeric characters; for example, 1.0.0.alpha is a valid OSGi version number with a qualifier.
When comparing version numbers, the qualifier is compared using string comparison.
In places in the metadata where a version is expected, if it’s omitted, it defaults to 0.0.0
If a numeric component of the version number is omitted, it defaults to 0, and the qualifier defaults to an empty string.
One tricky aspect is that it isn’t possible to have a qualifier without specifying all the numeric components of the version.
OSGi uses this common version-number format for versioning both bundles and Java packages.
In chapter 9, we’ll discuss high-level approaches for managing version numbers for your packages, bundles, and applications.
The OSGi specification defines metadata for comprehensively describing which code is visible internally in a bundle and which internal code is visible externally.
Each of these areas captures separate but related information about which Java classes are reachable in your bundle and by your bundle.
We’ll cover each in detail; but before we do that, let’s step back and dissect how you use JAR files and the Java class path in traditional Java programming.
This will give you a basis for comparison to OSGi’s approach to code visibility.
IMPORTANT! Standard JAR files typically fail as bundles since they were written under the assumption of global type visibility (i.e., if it’s on the class path, you can use it)
If you’re going to create effective bundles, you have to free yourself from this old assumption and fully understand and accept that type visibility for bundles is based purely on the primitives we describe in this section.
To make this point very clear, we’ll go into intricate details about type visibility rules for standard JAR files versus bundle JAR files.
Although this may appear to be a lesson in the arcane, it’s critical to understand these differences.
Generally speaking, you compile Java source files into classes and then use the jar tool to create a JAR file from the generated classes.
If the JAR file has a Main-Class attribute in the manifest file, you can run the application like this:
If not, you add it to the class path and start the application something like this:
First it searches for the class specified in the Main-Class attribute or the one specified on the command line.
If it finds the class, it searches it for a static public void main(String[]) method.
If such a method is found, it invokes it to start the application.
As the application executes, any additional classes needed by the application are found by searching the class path, which is composed of the application classes in the JAR file and the standard JRE classes (and anything you may have added to the class path)
This represents a high-level understanding of how Java executes an application from a JAR file.
But this high-level view conceals a few implicit decisions made by standard JAR file handling, such as these:
Where to search inside the JAR file for a requested class.
With respect to the second decision, JAR files have an implicit policy of exposing all classes in root-relative packages to all requesters.
This is a highly deconstructed view of the behavior of JAR files, but it helps to illustrate the implicit modularity decisions of standard JAR files.
These implicit code-visibility decisions are put into effect when you place a JAR file on the class path for execution.
While executing, the JVM finds all needed classes by searching the class path, as shown in figure 2.10
But what is the exact purpose of the class path with respect to modularity? The class path defines which external classes are visible to the JAR file’s internal classes.
Every class reachable on the class path is visible to the application classes, even if they aren’t needed.
With this view of how standard JAR files and the class path mechanism work, let’s look into the details of how OSGi handles these same code-visibility concepts, which is quite a bit different.
Figure 2.9 Flow diagram showing the steps the JVM goes through to execute a Java program from the class path.
Figure 2.10 Flow diagram showing the steps the JVM goes through to load a class from the class path.
Defining bundles with metadata start with how OSGi searches bundles internally for code, followed by how OSGi externally exposes internal code, and finally how external code is made visible to internal bundle code.
Whereas standard JAR files are implicitly searched for internal classes in all directories from the root of the JAR file as if they were package names, OSGi uses a more explicit approach called the bundle class path.
Like the standard Java class path concept, the bundle class path is a list of locations to search for classes.
The difference is the bundle class path refers to locations inside the bundle JAR file.
BUNDLE-CLASSPATH An ordered, comma-separated list of relative bundle JAR file locations to be searched for class and resource requests.
When a given bundle class needs another class in the same bundle, the entire bundle class path of the containing bundle is searched to find the class.
Classes in the same bundle have access to all code reachable on their bundle class path.
Bundles declare their internal class path using the Bundle-ClassPath manifest header.
The bundle class path behaves in the same way as the global class path in terms of the search algorithm, so you can refer to figure 2.10 to see how this behaves; but in this case, the scope is limited to classes contained in the bundle.
With BundleClassPath, you can specify a list of paths in the bundle where the class loader should look for classes or resources.
For this example, the bundle is searched first for root-relative packages, then in the folder called other-classes, and finally in the embedded JAR in the bundle.
The ordering is important, because the bundle class path entries are searched in the declared order.
Bundle-ClassPath is somewhat unique, because OSGi manifest headers don’t normally have default values.
Why does Bundle-ClassPath have a default value? The answer is related to how standard JAR files are searched for classes.
In other words, if you specify a value for Bundle-ClassPath, then.
If you specify a value and don’t include ., then root-relative directories aren’t searched when looking for classes in the bundle JAR file.
As you can see, the internal bundle class path concept is powerful and flexible when it comes to defining the contents and internal search order of bundles; refer to the sidebar “Bundle class path flexibility” for some examples of when this flexibility is useful.
Next, you’ll learn how to expose internal code for sharing with other bundles.
Bundle-ClassPath affects the visibility of classes in a bundle, but how do you share classes among bundles? The first stage is to export the packages you wish to share with others.
Externally useful classes are those composing the public API of the code contained in the JAR file, whereas non-useful classes form the implementation details.
Standard JAR files don’t provide any mechanism to differentiate externally useful classes from non-useful ones, but OSGi does.
A standard JAR file exposes everything relative to the root by default, but an OSGi bundle exposes nothing by default.
A bundle must use the Export-Package manifest header to explicitly expose internal classes it wishes to share with other bundles.
EXPORT-PACKAGE A comma-separated list of internal bundle packages to expose for sharing with other bundles.
Instead of exposing individual classes, OSGi defines sharing among bundles at the package level.
Although this makes the task of exporting code a little simpler, it can still be a major undertaking for large projects; we’ll discuss some tools to simplify this in appendix A.
When you include a package in an Export-Package declaration, every public class contained in the package is exposed to other bundles.
Bundle class path flexibility You may wonder why you’d want to package classes in different directories or embed JAR files in the bundle JAR file.
First, the bundle class path mechanism doesn’t apply only to classes, but also to resources.
A common use case is to place images in an image/ directory to make it explicit in the JAR file where certain content can be found.
Also, in web applications, nested JAR files are embedded in the JAR file under the WEB-INF/lib/ directory and classes can be placed in the WEB-INF/classes/ directory.
In other situations, you may have a legacy or proprietary JAR file that you can’t change.
By embedding the JAR file into your bundle and adding bundle metadata, you can use it without changing the original JAR.
It may also be convenient to embed a JAR file when you want your bundle to have a private copy of some library; this is especially useful when you want to avoid sharing static library members with other bundles.
Embedding JAR files isn’t strictly necessary, because you can also unpack a standard JAR file into your bundle to achieve the same effect.
As an aside, you can also see a performance improvement by not embedding JAR files, because OSGi framework implementations must extract the embedded JAR files to access them.
Defining bundles with metadata program shape API bundle is as follows (figure 2.11 shows how we’ll graphically represent exported module packages):
You’ll likely want to export more than one package at a time from your bundles.
You can export multiple packages by separating them with commas:
Because it’s possible for different bundles to export the same packages, a given bundle can use attributes to differentiate its exports from other bundles.
This attaches the vendor attribute with the value "Manning" to the exported packages.
In this particular example, vendor is an arbitrary attribute because it has no special meaning to the framework—it’s something we made up.
When we talk about importing code, you’ll get a better idea of how arbitrary attributes are used in package sharing to differentiate among exported packages.
As we mentioned previously in the sidebar “JAR file manifest syntax,” OSGi also supports a shorthand format when you want to attach the same attributes to a set of target packages, like this:
This shorthand comes in handy, but it can be applied only if all attached attributes are the same for all packages.
Using arbitrary attributes allows a bundle to differentiate its exported packages, but there’s a more meaningful reason to use an attribute for differentiation: version management.
Version management isn’t a part of standard Java development, but it’s inherent in OSGi-based Java development.
In particular, OSGi supports not only bundle versioning, as discussed previously, but also package versioning, which means every shared package has a version number.
Attributes are used to associate a version number with a package:
Here, you attach the version attribute with the value "2.0.0" to the exported packages, using OSGi’s common version-number format.
In this case, the attribute isn’t arbitrary, because this attribute name and value format is defined by the OSGi specification.
You may have noticed that some of the earlier Export-Package examples don’t specify a version.
In that case, the version defaults to "0.0.0", but it isn’t a good idea to use this version number.
With Bundle-ClassPath and Export-Package, you have a pretty good idea how to define and control the visibility of the bundle’s internal classes; but not all the code you need will be contained in the bundle JAR file.
Next, you’ll learn how to specify the bundle’s dependencies on external code.
Both Bundle-ClassPath and Export-Package deal with the visibility of internal bundle code.
You need some way to declare which external classes are needed by the bundle so the OSGi framework can make them visible to it.
Typically, the standard Java class path is used to specify which external code is visible to classes in your JAR files, but OSGi doesn’t use this mechanism.
OSGi requires all bundles to include metadata explicitly declaring their dependencies on external code, referred to as importing.
You must declare imports for all packages required by your bundle but not contained in your bundle.
The only exception to this rule is for classes in the java.* packages, which are automatically made visible to all bundles by the OSGi framework.
The manifest header you use for importing external code is appropriately named Import-Package.
IMPORT-PACKAGE A comma-separated list of packages needed by internal bundle code from other bundles.
The value of the Import-Package header follows the common OSGi manifest header syntax.
Consider the main paint program bundle, which has a dependency on the org.foo.shape package.
It needs to declare an import for this package as follows (figure 2.12 shows how we’ll graphically represent imported module packages):
This specifically tells the OSGi framework that the bundle requires access to org.foo.shape in addition to the internal.
Conceptually, the import keyword and declaring OSGi imports are similar, but they serve different purposes.
You can import classes from any other package to use their short name, but it doesn’t grant any visibility.
In fact, you never need to use import, because you can use the fully qualified class name instead.
For OSGi, the metadata for importing external code is important, because it’s how the framework knows what your bundle needs.
Defining bundles with metadata code visible to it from its bundle class path.
Be aware that importing a package doesn’t import its subpackages; remember, there’s no relationship among nested packages.
Your bundles can import any number of packages by listing them on ImportPackage and separating them using commas.
It’s not uncommon in larger projects for the Import-Package declaration to grow large (although you should strive to minimize this)
Recall how Export-Package declarations can include attributes to differentiate a bundle’s exported packages.
You can use these export attributes as matching attributes when importing packages.
For example, we previously discussed the following export and associated attribute:
A bundle with this metadata exports the two packages with the associated vendor attribute and value.
It’s possible to narrow your bundle’s imported packages using the same matching attribute:
The bundle with this metadata is declaring a dependency on the package org.foo.shape with a vendor attribute matching the "Manning" value.
The attributes attached to Export-Package declarations define the attribute’s value, whereas attributes attached to Import-Package declarations define the value to match; essentially, they act like a filter.
The details of how imports and exports are matched and filtered is something we’ll defer until section 2.7
For now, it’s sufficient to understand that attributes attached to imported packages are matched against the attributes attached to exported packages.
In other words, it either matches the specified value or it doesn’t.
You learned about one non-arbitrary attribute when we discussed Export-Package and the version attribute.
Because this attribute is defined by the OSGi specification, more flexible matching is supported.
In the simple case, it treats a value as an infinite range starting from the specified version number.
In some cases, such as specification packages, it’s reasonable to expect backward compatibility.
In situations where you wish to limit your assumptions about backward compatibility, OSGi allows.
Table 2.2 illustrates the meaning of the various combinations of the version range syntax.
You may wonder why a single value like "1.0.1" is an infinite range rather than a precise version.
In the OSGi specifications prior to R4, all packages were assumed to be specification packages where backward compatibility was guaranteed.
Because backward compatibility was assumed, it was only necessary to specify a minimum version.
When the R4 specification added support for sharing implementation packages, it also needed to add support for arbitrary version ranges.
It would have been possible at this time to redefine a single version to be a precise version, but that would have been unintuitive for existing OSGi programmers.
Also, the specification would have had to define syntax to represent infinity.
In the end, the OSGi Alliance decided it made the most sense to define versions ranges as presented here.
You may have noticed that some of the earlier Import-Package examples didn’t specify a version range.
When no version range is specified, it defaults to the value "0.0.0", which you may expect from past examples.
Now you understand how to use Import-Package to express dependencies on external packages and Export-Package to expose internal packages for sharing.
The decision to use packages as the basis for interbundle sharing isn’t an obvious choice to everyone, so we discuss some arguments for doing so in the sidebar “Depending on packages, not bundles.”
We’ve now covered the major constituents of the OSGi module layer: BundleClassPath, Export-Package, and Import-Package.
We’ve discussed these in the context of the paint program you’ll see running in the next section, but the final piece of the puzzle we need to look at is how these various code-visibility mechanisms fit together in a running application.
Depending on packages, not bundles Importing packages seems fairly normal for most Java programmers, because you import the classes and packages you use in the source files.
But the import statements in the source files are for managing namespaces, not dependencies.
OSGi’s choice of using package-level granularity for expressing dependencies among bundles is novel, if not controversial, for Java-based module-oriented technologies.
Other approaches typically adopt module-level dependencies, meaning dependencies are expressed in terms of one module depending on another.
The OSGi choice of package-level dependencies has created some debate about which approach is better.
The main criticisms leveled against package-level dependencies is that they’re too complicated or fine-grained.
Some people believe it’s easier for developers to think in terms of requiring a whole JAR file rather than individual packages.
This argument doesn’t hold water, because a Java developer using any given technology must know something about its package naming.
For example, if you know enough to realize you want to use the Servlet class in the first place, you probably have some idea about which package it’s in, too.
Package-level dependencies are more fine-grained, which does result in more metadata.
But bundles rarely depend on all exported packages of a given bundle, and this is more of a condemnation of tooling support.
Remember how much of a nuisance it was to maintain import declarations before IDEs started doing it for you? This is starting to change for bundles, too; in appendix A, we describe tools for generating bundle metadata.
Let’s look at some of the benefits of package-level dependencies.
The difference between module- and package-level dependencies is one of who versus what.
Module-level dependencies express which specific module you depend on (who), whereas package-level dependencies express which specific packages you depend on (what)
Module-level dependencies are brittle, because they can only be satisfied by a specific bundle even if another bundle offers the same packages.
Some people argue that this isn’t an issue, because they want the specific bundle they’ve tested against, or because the packages are implementation packages and won’t be provided by another bundle.
Although these arguments are reasonable, they usually break down over time.
For example, if your bundle grows too large over time, you may wish to refactor it by splitting its various exported packages into multiple bundles.
If you use modulelevel dependencies, such a refactoring will break existing clients, which tends to be a real bummer when the clients are from third parties and you can’t easily change them.
Also, a bundle doesn’t usually depend on everything in another bundle, only a subset.
As a result, module-level dependencies are too broad and cause transitive fanout.
You end up needing to deploy a lot of extra bundles you don’t use, just to satisfy all the dependencies.
We’ve talked a lot about code visibility, but in the end all the metadata we’ve discussed allows the OSGi framework to perform sophisticated searching on behalf of bundles for their contained and needed classes.
Under the covers, when an importing bundle needs a class from an exported package, it asks the exporting bundle for it.
The framework uses class loaders to do this, but the exact details of how it asks are unimportant.
Still, it’s important to understand the ordering of this class-search process.
When a bundle needs a class at execution time, the framework searches for the class in the following order:
If the class is from a package starting with java., the parent class loader is asked for the class.
If there is no such class, the search ends with an exception.
If the class is from a package imported by the bundle, the framework asks the exporting bundle for the class.
If there is no such class, the search ends with an exception.
If there is no such class, the search ends with an exception.
These steps are important because they also help the framework ensure consistency.
We haven’t covered everything you can possibly do, but we’ve discussed the most important bundle.
Package-level dependencies represent a higher-level view of the code’s real class dependencies.
It’s possible to analyze a bundle’s code and generate its set of imported packages, similar to how IDEs maintain import declarations in source files.
Modulelevel dependencies can’t be discovered in such a fashion, because they don’t exist in the code.
Package-level dependencies sound great, right? You may now wonder if they have any issues.
The main issue is that OSGi must treat a package as an atomic unit.
If this assumption weren’t made, then the OSGi framework wouldn’t be free to substitute a package from one bundle for the same package from another bundle.
This means you can’t split a package across bundles; a single package must be contained in a single bundle.
If packages were split across bundles, there would be no easy way for the OSGi framework to know when a package was complete.
Other than this, you can do anything with package-level dependencies that you can with module-level dependencies.
Next, you’ll put all the metadata in place for the paint program and then step back to review the current design.
Before moving on, if you’re wondering if it’s possible to have a JAR file that is both a bundle and an ordinary JAR file, see the sidebar “Is a bundle a JAR file or a JAR file a bundle?”
Is a bundle a JAR file or a JAR file a bundle? Maybe you’re interested in adding OSGi metadata to your existing JAR files or you want to create bundles from scratch, but you’d still like to use them in non-OSGi situations too.
We’ve said before that a bundle is just a JAR file with additional module-related metadata in its manifest file, but how accurate is this statement? Does it mean you can use any OSGi bundle as a standard JAR file? What about using a standard JAR file as a bundle? Let’s answer the second question first, because it’s easier.
A standard JAR file can be installed into an OSGi framework unchanged.
Why? The main reason is that a standard JAR file doesn’t expose any of its content; in OSGi terms, it doesn’t export any packages.
The default Bundle-ClassPath for a JAR file is ., but the default for Export-Package is nothing.
So even though a standard JAR file is a bundle, it isn’t a useful bundle.
At a minimum, you need to add an Export-Package declaration to its manifest file to explicitly expose some (or all) of its internal content.
What about bundle JAR files? Can they be used as a standard JAR file outside of an OSGi environment? The answer is, it depends.
It’s possible to create bundles that function equally well in or out of an OSGi environment, but not all bundles can be used as standard JAR files.
It comes down to which features of OSGi your bundle uses.
Of the metadata features you’ve learned about so far, only one can cause issues: Bundle-ClassPath.
Recall that the internal bundle class path is a comma-separated list of locations inside the bundle JAR file and may contain.
If a bundle specifies an embedded JAR file or directory, it requires special handling that’s available only in an OSGi environment.
Luckily, it isn’t too difficult to avoid using embedded JAR files and directories.
It’s a good idea to try to keep your bundle JAR files compatible with standard JAR files if you can, but it’s still best to use them in an OSGi environment.
Without OSGi, you lose dependency checking, consistency checking, and boundary enforcement, not to mention all the cool lifecycle and service stuff we’ll discuss in the coming chapters.
So far, you’ve defined three bundles for the paint program: a shape API bundle, a shape implementation bundle, and a main paint program bundle.
The shape API bundle is described by the following manifest metadata:
The bundle containing the shape implementations is described by the following manifest metadata:
And the main paint program bundle is described by the following manifest metadata:
As you can see in figure 2.13, these three bundles directly mirror the logical package structure of the paint program.
This approach is reasonable, but can it be improved? To some degree, you can answer this question only if you know more about the intended uses of the paint program; but let’s look more closely at it anyway.
Is there a downside to keeping all shape implementations in a single package and a single bundle? Perhaps it’s better to reverse the question.
Is there any advantage to separating the shape implementations into separate bundles? Imagine use cases where not all shapes are necessary; for example, small devices may not have enough resources to support all shape implementations.
If you separate the shape implementations into separate packages and separate bundles, you have more flexibility when it comes to creating different configurations of the application.
This is a good issue to keep in mind when you’re modularizing applications.
Optional components or components with the potential to have multiple alternative implementations are good candidates to be in separate bundles.
Breaking your application into multiple bundles gives you more flexibility, because you’re limited to deploying configurations of your application based on the granularity of your defined bundles.
Sounds good, right? You may then wonder why you don’t divide your applications into as many bundles as you can.
You pay a price for the flexibility afforded by dividing an application into multiple bundles.
Lots of bundles mean you have lots of artifacts that are versioning independently, creating lots of dependencies and configurations to manage.
So it’s probably not a good idea to create a bundle out of each of your project’s packages, for example.
You need to analyze and understand your needs for flexibility when deciding how best to divide an application.
Returning to the paint program, let’s assume the ultimate goal is to enable the possibility for creating different configurations of the application with different sets of shapes.
The metadata for the square and triangle bundles is nearly identical, except with the correct shape name substituted where appropriate.
These changes also require changes to the program’s metadata implementation bundle; you modify its metadata as follows:
The paint program implementation bundle depends on Swing, the public API bundle, and all three shape bundles.
Figure 2.14 depicts the new structure of the paint program.
Now you have five bundles (shape API, circle, square, triangle, and paint)
But what do you do with these bundles? The initial version of the paint program had a static main() method on PaintFrame to launch it; do you still use it to launch the program? You could use it by putting all the bundle JAR files on the class path, because all the example bundles can function as standard JAR files, but this would defeat the purpose of modularizing the application.
There’d be no enforcement of modular boundaries or consistency checking.
To get these benefits, you must launch the paint program using the OSGi framework.
The focus of this chapter is on using the module layer, but you can’t launch the application without a little help from the lifecycle layer.
Instead of putting the cart before the horse and talking about the lifecycle layer now, we created a generic OSGi bundle launcher to launch the paint program for you.
This launcher is simple: you execute it from the command line and specify a path to a directory containing bundles; it creates an OSGi framework and deploys all bundles in the specified directory.
The cool part is that this generic launcher hides all the details and OSGi-specific API from you.
Just deploying the paint bundles into an OSGi framework isn’t sufficient to start the paint program; you still need some way to kick-start it.
You can reuse the paint program’s original static main() method to launch the new modular version.
To get this to work with the bundle launcher, you need to add the following metadata from the original paint program to the paint program bundle manifest:
As in the original paint program, this is standard JAR file metadata for specifying the class containing the application’s static main() method.
Figure 2.14 Logical structure of the paint program with separate modules for each shape implementation.
OSGi dependency resolution defined by the OSGi specification but is a feature of the bundle launcher.
Doing so compiles all the code and packages the modules.
The program starts up as it apparently always has; but underneath, the OSGi framework is resolving the bundles’ dependencies, verifying their consistency, and enforcing their logical boundaries.
You’ve now used the OSGi module layer to create a nicely modular application.
OSGi’s metadata-based approach didn’t require any code changes to the application, although you did move some classes around to different packages to improve logical and physical modularity.
The goal of the OSGi framework is to shield you from a lot of the complexities; but sometimes it’s beneficial to peek behind the curtain, such as to help you debug the OSGi-based applications when things go wrong.
In the next section, we’ll look at some of the work the OSGi framework does for you, to give you a deeper understanding of how everything fits together.
Afterward, we’ll close out the chapter by summarizing the benefits of modularizing the paint program.
You’ve learned how to describe the internal code composing the bundles with BundleClassPath, expose internal code for sharing with Export-Package, and declare dependencies on external code with Import-Package.
Although we hinted at how the OSGi framework uses the exports from one bundle to satisfy the imports of another, we didn’t go into detail.
The Export-Package and Import-Package metadata declarations included in bundle manifests form the backbone of the OSGi bundle dependency model, which is predicated on package sharing among bundles.
In this section, we’ll explain how OSGi resolves bundle package dependencies and ensures package consistency among bundles.
After this section, you’ll have a clear understanding of how bundle modularity metadata is used by the OSGi framework.
You may wonder why this is necessary, because bundle resolution seems like an OSGi framework implementation detail.
Admittedly, this section covers some of the more complex details of the OSGi specification; but it’s helpful when defining bundle metadata if you understand a little of what’s going on behind the scenes.
Further, this information can come in handy when you’re debugging OSGi-based applications.
Adding OSGi metadata to your JAR files represents extra work for you as a developer, so why do it? The main reason is so you can use the OSGi framework to support and enforce the bundles’ inherent modularity.
One of the most important tasks performed by the OSGi framework is automating dependency management, which is called bundle dependency resolution.
A bundle’s dependencies must be resolved by the framework before the bundle can be used, as shown in figure 2.15
The framework’s dependency resolution algorithm is sophisticated; we’ll get into its gory details, but let’s start with a simple definition.
Resolving a bundle may cause the framework to resolve other bundles transitively, if exporting bundles themselves haven’t yet been resolved.
The resulting set of resolved bundles are conceptually wired together in such a fashion that any given imported package from a bundle is wired to a matching exported package from another bundle, where a wire implies having access to the exported package.
The final result is a graph of all bundles wired together, where all imported package dependencies are satisfied.
If any dependency can’t be satisfied, then the resolve fails, and the instigating bundle can’t be used until its dependencies are satisfied.
This description likely makes you want to ask three questions:
The first two questions are related, because they both involve the lifecycle layer, which we’ll discuss in the next chapter.
For the first question, it’s sufficient to say that the framework resolves a bundle automatically when another bundle attempts to use it.
To answer the second question, we’ll say that all bundles must be installed into the framework in order to be resolved (we’ll discuss bundle installation in more depth in chapter 3)
For the discussion in this section, we’ll always be talking about installed bundles.
As for the third question, we won’t answer it fully because the technical details of wiring bundles together isn’t important; but for the curious, we’ll explain it briefly before looking into the resolution process in more detail.
At execution time, each OSGi bundle has a class loader associated with it, which is how the bundle gains access to all the classes to which it should have access (the ones determined by the resolution process)
When an importing bundle is wired to an exporting bundle, the importing class loader is given a reference to the exporting class loader so it can delegate requests for classes in the exported package to it.
You don’t need to worry about how this happens—relax and let OSGi worry about it for you.
Now, let’s look at the resolution process in more detail.
Figure 2.15 Transitive dependencies occur when bundle A depends on packages from bundle B and bundle B in turn depends on packages from bundle C.
To use bundle A, you need to resolve the dependencies of both bundle B and bundle C.
At first blush, resolving dependencies is fairly straightforward; the framework just needs to match exports to imports.
From this, you know that the paint program has a single dependency on the org.foo.shape package.
If only this bundle were installed in the framework, it wouldn’t be usable, because its dependency wouldn’t be satisfiable.
To use the paint program bundle, you must install the shape API bundle, which contains the following metadata:
When the framework tries to resolve the paint program bundle, it knows it must find a matching export for org.foo.shape.
In this case, it finds a candidate in the shape API bundle.
When the framework finds a matching candidate, it must determine whether the candidate is resolved.
If the candidate is already resolved, the candidate can be chosen to satisfy the dependency.
If the candidate isn’t yet resolved, the framework must resolve it first before it can select it; this is the transitive nature of resolving dependencies.
If the shape API bundle has no dependencies, it can always be successfully resolved.
But you know from the example that it does have some dependencies, namely javax.swing: Bundle-Name: Paint API Import-Package: javax.swing Export-Package: org.foo.shape.
What happens when the framework tries to resolve the paint program? By default, in OSGi it wouldn’t succeed, which means the paint program can’t be used.
Why? Because even though the org.foo.shape package from the API bundle satisfies the main program’s import, there’s no bundle to satisfy the shape API’s import of javax.swing.
In general, to resolve this situation, you can conceptually install another bundle exporting the required package:
Now, when the framework tries to resolve the paint program, it succeeds.
The main paint program bundle’s dependency is satisfied by the shape API bundle, and its dependency is satisfied by the Swing bundle, which has no dependencies.
After resolving the main paint program bundle, all three bundles are marked as resolved, and the framework won’t try to resolve them again (until certain conditions require it, as we’ll describe in the next chapter)
The framework ends up wiring the bundles together, as shown in figure 2.16
What does the wiring in figure 2.16 tell you? It says that when the main bundle needs a class in package org.foo.shape, it gets it from the shape API bundle.
It also says when the shape API bundle needs a class in package javax.swing, it gets it from the Swing bundle.
Even though this example is simple, it’s largely what the framework tries to do when it resolves bundle dependencies.
You’ve learned that you can have attributes attached to exported and imported packages.
At the time, we said it was sufficient to understand that attributes attached to imported packages are matched against attributes attached to exported packages.
Let’s modify the bundle metadata snippets to get a deeper understanding of how attributes factor into the resolution process.
Assume you modify the Swing bundle to look like this:
Here, you modify the Swing bundle to export javax.swing with an attribute vendor with value "Sun"
If the other bundles’ metadata aren’t modified and you perform the resolve process from scratch, what impact does this change have? This minor change has no impact at all.
Everything resolves as it did before, and the vendor attribute never comes into play.
Depending on your perspective, this may or may not seem confusing.
As we previously described attributes, imported attributes are matched against exported attributes.
In this case, no import declarations mention the vendor attribute, so it’s ignored.
Let’s revert the change to the Swing bundle and instead change the API bundle to look like this:
System class path delegation In actuality, the javax.swing case in the previous example is a little misleading if you’re running your OSGi framework with a JRE that includes javax.swing.
In such a case, you may want bundles to use Swing from the JRE.
The framework can provide access using system class path delegation.
We’ll look at this area a little in chapter 13, but this highlights a deficiency with the heavyweight JRE approach.
If it’s possible to install a bundle to satisfy the Swing dependencies, why are they packaged in the JVM by default? Adoption of OSGi patterns could massively trim the footprint of future JVM implementations.
Attempting to resolve the paint program bundle now fails because no bundle is exporting the package with a matching vendor attribute for the API bundle.
Putting the vendor attribute back on the Swing bundle export allows the main paint program bundle to successfully resolve again with the same wiring, as shown earlier in figure 2.16
Attributes on exported packages have an impact only if imported packages specify them, in which case the values must match or the resolve fails.
Other than the more expressive interval notation for specifying ranges, it works the same way as arbitrary attributes.
For example, you can modify the shape API bundle as follows:
And you can modify the paint program bundle as follows:
This particular example has multiple matching attributes on the import declaration, which is treated like a logical AND by the framework.
Therefore, if any of the matching attributes on an import declaration don’t match a given export, the export doesn’t match at all.
Overall, attributes don’t add much complexity to the resolution process, because they add additional constraints to the matching of imported and exported package names already taking place.
In the previous section, dependency resolution is fairly straightforward because there’s only one candidate to resolve each dependency.
The OSGi framework doesn’t restrict bundles from exporting the same package.
Actually, one of the benefits of the OSGi framework is its support for side-by-side versions, meaning it’s possible to use different versions of the same package in the same running JVM.
In highly collaborative environments of independently developed bundles, it’s difficult to limit which versions of packages are used.
Likewise, in large systems, it’s possible for different teams to use different versions of libraries in their subsystems; the use of different XML parser versions is a prime example.
Let’s consider what happens when multiple candidates are available to resolve the same dependency.
Consider a case in which a web application needs to import the javax.servlet package and both a servlet API bundle and a Tomcat bundle provide the package (see figure 2.17)
When the framework tries to resolve the dependencies of the web application, it sees that the web application requires javax.servlet with a minimum version of 2.4.0 and both the servlet API and Tomcat bundles meet this requirement.
Because the web application can be wired to only one version of the package, how does the framework choose between the candidates? As you may intuitively expect, the framework favors the highest matching version, so in this case it selects Tomcat to resolve the web application’s dependency.
What happens if both bundles export the same version, say 2.4.0?
In this case, the framework chooses between candidates based on the order in which they’re installed in the framework.
Bundles installed earlier are given priority over bundles installed later; as we mentioned, the next chapter will show you what it means to install a bundle in the framework.
If you assume the servlet API was installed before Tomcat, the servlet API will be selected to resolve the web application’s dependency.
The framework makes one more consideration when prioritizing matching candidates: maximizing collaboration.
So far, you’ve been working under the assumption that you start the resolve process on a cleanly installed set of bundles.
But the OSGi framework allows bundles to be dynamically installed at any time during execution.
In other words, the framework doesn’t always start from a clean slate.
It’s possible for some bundles to be installed, resolved, and already in use when new bundles are installed.
This creates another means to differentiate among exporters: already-resolved exporters and not-yetresolved exporters.
The framework gives priority to already-resolved exporters, so if it must choose between two matching candidates where one is resolved and one isn’t, it chooses the resolved candidate.
If the servlet API is already resolved, the framework will choose it to resolve the web application’s dependency, even though it isn’t exporting the highest version, as shown in figure 2.18
It has to do with maximizing the potential for collaboration.
Bundles can only collaborate if they’re using the same version of a shared package.
Figure 2.17 How does the framework choose between multiple exporters of a package?
OSGi dependency resolution framework favors already-resolved packages as a means to minimize the number of different versions of the same package being used.
Highest priority is given to already-resolved candidates, where multiple matches of resolved candidates are sorted according to version and then installation order.
Next priority is given to unresolved candidates, where multiple matches of unresolved candidates are sorted according to version and then installation order.
It looks like we have all the bases covered, right? Not quite.
Next, we’ll look at how an additional level of constraint checking is necessary to ensure that bundle dependency resolution is consistent.
From the perspective of any given bundle, a set of packages is visible to it, which we’ll call its class space.
Given your current understanding, you can define a bundle’s class space as its imported packages combined with the packages accessible from its bundle class path, as shown in figure 2.19
A bundle’s class space must be consistent, which means only a single instance of a given package must be visible to the bundle.
Here, we define instances of a package as those with the same name, but from different providers.
For example, consider the previous example, where both the servlet API and Tomcat bundles exported the javax.servlet package.
The OSGi framework strives to ensure that the class spaces of all bundles remain consistent.
Prioritizing how exported packages are selected for imported packages, as described.
Figure 2.18 If a bundle is already resolved because it’s in use by another bundle, this bundle is preferred to bundles that are only installed.
Figure 2.19 Bundle A’s class space is defined as the union of its bundle class path with its imported packages, which are provided by bundle B’s exports.
Why not? Let’s consider the simple API in the following code snippet:
The details of what it does are unimportant at the moment; for now, you just need to know its method signature.
This means it has some metadata in its manifest like this:
Let’s assume the framework has the HTTP service bundle and a servlet library bundle installed, as shown in figure 2.20
Given these two bundles, the framework makes the only choice available, which is to select the version of javax.servlet provided by the Servlet API bundle.
When the framework resolves these two new bundles, it does so as shown in figure 2.21
It seems that everything is fine: all bundles have their dependencies resolved, right? Not quite.
There’s an issue with these choices for dependency resolution—can you see what it is?
The class spaces of the HTTP service and client bundles aren’t consistent; two different versions of javax.servlet are reachable from both.
At execution time, this results in class cast exceptions when the HTTP service and client bundles interact.
The framework made the best choices at the time it resolved the bundle dependencies; but due to the incremental nature of the resolve process, it couldn’t make the best overall choice.
If you install all four bundles together, the framework resolves the dependencies in a consistent way using its existing rules.
Figure 2.22 shows the dependency resolution when all four bundles are resolved together.
Because only one version of javax.servlet is in use, you know the class spaces of the HTTP service and client bundles are consistent, allowing them to interact without issue.
But is this a general remedy to class-space inconsistencies? Unfortunately, it isn’t, as you’ll see in chapter 3, because OSGi allows you to dynamically install and uninstall bundles at any time.
Moreover, inconsistent class spaces don’t only result from incremental resolving of dependencies.
It’s also possible to resolve a static set of bundles into inconsistent class spaces due to inconsistent constraints.
For example, imagine that the HTTP service bundle requires precisely version 2.3.0 of javax.
These constraints are clearly inconsistent, but the framework will happily resolve the example bundles given the current set of dependency resolution rules.
The difficulty is that Export-Package and Import-Package only capture inter-bundle dependencies, but class-space consistency conflicts result from intra-bundle dependencies.
Figure 2.23 shows this intra-bundle uses relationship between the HTTP service bundle’s exported and imported packages.
Figure 2.22 Consistent dependency resolution of HTTP service and client bundles.
How do these uses relationships arise? The example shows the typical way, which is when the method signatures of classes in an exported package expose classes from other packages.
This seems obvious, because the used types are visible, but it isn’t always the case.
You can also expose a type via a base class that’s downcast by the consumer.
Because these types of uses relationships are important, how do you capture them in the bundle metadata?
The sidebar “JAR file manifest syntax” in section 2.5 introduced the concept of a directive, but this is the first example of using one.
Directives are additional metadata to alter how the framework interprets the metadata to which the directives are attached.
The syntax for capturing directives is similar to arbitrary attributes.
For example, the following modified metadata for the HTTP service example shows how to use the uses directive:
Notice that directives use the := assignment syntax, but the ordering of the directives and the attributes isn’t important.
How exactly does the framework use this information? uses relationships among packages act like grouping constraints for the packages.
In this specific case, the exported package expresses a uses relationship with an imported package, but it could use other exported packages.
These sorts of uses relationships constrain which choices the framework can make when resolving dependencies, which is why they’re also referred to as constraints.
Abstractly, if package foo uses package bar, importers of foo are constrained to the same bar if they use bar at all.
Figure 2.24 depicts how this would impact the original incremental dependency resolutions.
Figure 2.24 Uses constraints detect class-space inconsistencies, so the framework can determine that it isn’t possible to resolve the HTTP client bundle.
For the incremental case, the framework can now detect inconsistencies in the class spaces, and resolution fails when you try to use the client bundle.
Early detection is better than errors at execution time, because it alerts you to inconsistencies in the deployed set of bundles.
In the next chapter, you’ll learn how to cause the framework to re-resolve the bundle dependencies to remedy this situation.
You can further modify the example, to illustrate how uses constraints help find proper dependency resolutions.
Typically, the framework tries to select the highest version of a package to resolve a dependency; but due to the uses constraint, the framework ends up selecting a lower version instead, as shown in figure 2.25
If you look at the class space of the HTTP client, you can see how the framework ends up with this solution.
Because the HTTP service bundle can only use version 2.3.0 of javax.servlet, this eliminates the Tomcat bundle as a possibility for the client bundle.
The end result is a consistent class space where a lower version of a needed package is correctly selected even though a higher version is available.
Let’s finish the discussion of uses constraints by touching on some final points.
First, uses constraints are transitive, which means that if a given bundle exports package foo that uses imported package bar, and the selected exporter of bar uses package baz, then the associated class space for a bundle importing foo is constrained to have the same providers for both bar and baz, if they’re used at all.
Also, even though uses constraints are important to capture, you don’t want to create blanket uses constraints, because doing so overly constrains dependency resolution.
The framework has more leeway when resolving dependency on packages not listed in uses constraints, which is necessary to support side-by-side versions.
For example, in larger applications, it isn’t uncommon for independently developed subsystems.
If you specify uses constraints too broadly, this isn’t possible.
Accurate uses constraints are important, but luckily tools exist for generating them for exported packages.
OK! You made it through the most difficult part and survived.
Don’t worry if you didn’t understand every detail, because some of it may make more sense after you have more experience creating and using bundles.
Let’s turn our attention back to the paint program to review why you did all this in the first place.
Even though the amount of work required to create the modular version of the paint program wasn’t great, it was still more effort than if you left the paint program as it was.
Why did you create this modular version? Table 2.3 lists some of the benefits.
If you do this, you’ll see an exception like this:
The exact syntax of this message will become familiar to you when you read chapter 4; but ignoring the syntax, it tells you the application is missing the org.foo.shape package, which is provided by the API bundle.
Due to the on-demand nature of Java class loading, such errors are typically only discovered during application execution when the missing classes are used.
With OSGi, you can immediately discover such issues with missing bundles or incorrect versions.
In addition to detecting errors, let’s look at how OSGi modularity helps you create different configurations of the application.
You can keep the implementation details private, because you’re only exposing what you want to expose in the org.foo.shape public API package.
The code is more reusable because you explicitly declare what each bundle depends on via Import-Package statements.
This means you know what you need when using the code in different projects.
You no longer have to guess if you’ve deployed the application properly, because OSGi verifies whether all needed pieces are present when launching the application.
Similar to configuration verification, OSGi also verifies whether you have the correct versions of all the application pieces when launching the application.
You can more easily tailor the application to different scenarios by creating new configurations.
Creating a different configuration of the paint program is as simple as creating a new static main() method for the launcher to invoke.
Currently, you’re using the original static main() method provided by PaintFrame.
In truth, it isn’t modular to have the static main() on the implementation class; it’s better to create a separate class so you don’t need to recompile the implementation classes when you want to change the application’s configuration.
The following listing shows the existing static main() method from the PaintFrame class.
You create a PaintFrame instance B and add a listener C to cause the VM to exit when the PaintFrame window is closed.
You inject the various shape implementations into the paint frame D and make the application window visible.
The important aspect from the point of view of modularity is at D.
Because the configuration decision of which shapes to inject is hardcoded into the method, if you want to create a different configuration, you must recompile the implementation bundle.
For example, assume you want to run the paint program on a small device only capable of supporting a single shape.
You’d also need to modify the metadata for the bundle so it would no longer depend on the other shapes.
Of course, after making these changes, you’d lose the first configuration.
These types of issues are arguments why the static main() method should be in a separate bundle.
The main paint program bundle no longer has any dependencies on the various shape implementations, but it now needs to export the package containing the paint frame.
To launch this full version of the paint program, use the bundle launcher to deploy all the associated bundles, including this FullPaint bundle.
The metadata for the bundle containing the small paint program configuration is as follows:
This small configuration only depends on Swing, the public API, the paint program implementation, and the circle implementation.
When you launch the full configuration, all shape implementations are required; but for the small configuration, only the circle implementation is required.
Now you can deploy the appropriate configuration of the application based on the target device and have OSGi verify the correctness of it all.
For completeness, figure 2.26 shows the before and after views of the paint program.
Figure 2.26 Modular and nonmodular versions of the paint program.
Modularity is a form of separation of concerns that provides both logical and physical encapsulation of classes.
Modularity is desirable because it allows you to break applications into logically independent pieces that can be independently changed and reasoned about.
It’s a JAR file containing code, resources, and modularity metadata.
Modularity metadata details human-readable information, bundle identification, and code visibility.
Bundle code visibility is composed of an internal class path, exported packages, and imported packages, which differs significantly from the global type assumption of standard JAR files.
The OSGi framework uses the metadata about imported and exported packages to automatically resolve bundle dependencies and ensure type consistency before a bundle can be used.
Imported and exported packages capture inter-bundle package dependencies, but uses constraints are necessary to capture intra-bundle package dependencies to ensure complete type consistency.
From here, we’ll move on to the lifecycle layer, where we enter execution time aspects of OSGi modularity.
This chapter was all about describing bundles to the OSGi framework; the lifecycle layer is all about using bundles and the facilities provided by the OSGi framework at execution time.
In the last chapter, we looked at the OSGi module layer and introduced you to bundles: a bundle is OSGi terminology for a module, which is a JAR file with the extra modularity metadata.
You use bundles to define both the logical (code encapsulation and dependencies) and physical (deployable units) modularity of an application.
The OSGi module layer goes to great lengths to ensure that class loading happens in a consistent and predictable way.
But to avoid putting the cart before the horse, in chapter 2 we glossed over the details of how you install bundles into an OSGi framework.
No longer: in this chapter, we’ll look at the next layer of the OSGi stack—the lifecycle layer.
As we mentioned in chapter 2, to use a bundle you install it into a running instance of the OSGi framework.
So creating a bundle is the first half of leveraging OSGi’s modularity features; the second half is using the OSGi framework as a runtime to manage and execute bundles.
The lifecycle layer is unique in allowing you to create externally (and remotely) managed applications or completely self-managed applications (or any combination of the two)
It also introduces dynamism that isn’t normally part of an application.
This chapter will familiarize you with the features of the lifecycle layer and explain how to effectively use them.
In the next section, we’ll take a closer look at what lifecycle management is and why you should care about it, followed by the definition of the OSGi bundle lifecycle.
In subsequent sections, you’ll learn about the API for managing the lifecycle of bundles.
Throughout this chapter, we’ll bring all the points home via examples of a simple OSGi shell and a lifecycle-aware version of the paint program.
The OSGi lifecycle layer provides a management API and a well-defined lifecycle for bundles at execution time in the OSGi framework.
External to your application, the lifecycle layer precisely defines the bundle lifecycle operations.
These lifecycle operations allow you to manage and evolve your application by dynamically changing the composition of bundles inside a running framework.
Internal to your application, the lifecycle layer defines how your bundles gain access to their execution context, which provides them with a way to interact with the OSGi framework and the facilities it provides at execution time.
It’s fine to state what the OSGi lifecycle layer does, but this won’t necessarily convince you of its worth.
Instead, let’s look at a quick example of how it can improve your applications with a real-world scenario.
Imagine you have a business application that can report management events via JMX.
Do you always want to enable or even install the JMX layer? Imagine running in a lightweight configuration and only enabling the JMX notifications on demand.
The lifecycle layer allows you to install, start, update, stop, and uninstall different bundles externally, to customize your application’s configuration at execution time.
Further, imagine that a critical failure event in your application must trigger the JMX layer to send out a notification regardless of whether the administrator previously enabled or installed the layer.
The lifecycle layer also provides programmatic access to bundles so they can internally modify their application’s configuration at execution time.
Generally speaking, programs (or parts of a program) are subject to some sort of lifecycle, which may or may not be explicit.
The lifecycle of software typically has four distinct phases, as shown in figure 3.1
If you’re creating an application, think about the typical lifecycle of the application as a whole.
Assuming all its dependencies are satisfied, you can execute it, which allows it to begin acquiring resources.
When the application is no longer needed, you stop it, which allows it to release any resources and perhaps persist any important state.
Over time, you may want to update the application to a newer version.
Ultimately, you may remove the application because you no longer need it.
For nonmodular applications, the lifecycle operates on the application as a whole; but as you’ll see, for modular applications, fine-grained lifecycle management is possible for individual pieces of the application.
The following are two of the more popular models for creating applications in Java and how they manage software lifecycle:
Standard Java—For the purposes of this discussion, we’ll equate an application in standard Java to a JAR file containing the Main-Class header in its manifest, which allows it to be easily executed.
In standard Java development, the lifecycle of an application is simple.
It’s executed when the user launches a JVM process, typically by double-clicking it.
Updating is usually done by replacing the JAR with a newer version.
Removal is achieved by deleting the JAR from the file system.
Servlet—In servlet development, the lifecycle of the web application is managed by the servlet container.
The application is installed via a container-specific process; sometimes this involves dropping a WAR file containing the application in a certain directory in the file system or uploading a WAR file via a web-management interface.
To update the application, a completely new WAR file is generated.
The existing WAR must be stopped and the new WAR file started in its place.
The application is removed by a container-specific process, again sometimes removing the WAR from the file system or interacting with a management interface.
In traditional Java applications, the lifecycle is largely managed by the platform-specific mechanism of the underlying operating system via installers and double-clicking.
Later, you can update it to a newer version or, ultimately, remove it if you no longer need it.
For modular development approaches, such as servlets, Java EE, and NetBeans, each has its own specific mechanism of handling the lifecycle of its components.
This leads us to the question of why you need lifecycle management at all.
Cast your mind back to the earlier discussion about why you should modularize your application code into separate bundles.
We talked about the benefits of separating different concerns into bundles and avoiding tight coupling among them.
The OSGi module layer provides the necessary means to do this at the class level, but it doesn’t address when a particular set of classes or objects is needed in an application.
An explicit lifecycle API lets the providing application take care of how to configure, initialize, and maintain a piece of code that’s installed so it can decide how it should operate at execution time.
For example, if a database driver is in use, should it start any threads or initialize any cache tables to improve performance? If it does any of these things, when are these resources released? Do they exist for the lifetime of the application as a whole? And if not, how are they removed? Because the OSGi specification provides an explicit lifecycle API, you can take any bundle providing the functionality you need and let it worry about how to manage its internal functions.
Because you can architect your application such that parts of it may come and go at any point in time, the application’s flexibility is greatly increased.
You can easily manage installation, update, and removal of an application and its required modules.
You can configure or tailor applications to suit specific needs, breaking the monolithic approach of standard development approaches.
Instead of “you get what you get,” wouldn’t it be great if you could offer “you get what you need”?
Another great benefit of the standard lifecycle API is that it allows for a diverse set of management applications that can manage your application.
There’s no magic going on; lifecycle management can be done completely using the provided API.
Now, let’s focus specifically on defining the OSGi bundle lifecycle and the management API associated with it.
The OSGi lifecycle layer is how you use the bundles; it’s where the rubber meets the road.
The module metadata from chapter 2 is all well and good, but creating bundles in and of itself is useful only if you use them.
You need to interact with the OSGi lifecycle layer in order to use the bundles.
Unlike the module layer, which relies on metadata, the lifecycle layer relies on APIs.
Because introducing APIs can be a boring endeavor (Javadoc, anyone?), we’ll move in a top-down fashion and use an example to show what the lifecycle layer API allows you to do.
OSGi bundle lifecycle extremely powerful, because it makes it possible to design as many different ways of managing the OSGi framework as you can think of; in the end, you’re limited only by your imagination as a developer.
Because there’s no standard way for users to interact with the lifecycle API, you could use a framework-specific mechanism.
But using this approach here would be a disservice to you, because it’s a great opportunity for learning.
Instead of reusing someone else’s work in this chapter, we’ll lead you through some basic steps for developing your own command line interface for interacting with the OSGi framework.
This gives you the perfect tool, alongside the paint program, to explore the rich capabilities provided by the OSGi lifecycle API.
Enough with the talk—let’s see the lifecycle API in action by kicking off the shell application and using it to install the paint program.
To do this, type the following into your operating system console (Windows users, substitute \ for /):
The shell is created as a bundle that, on starting, begins listening for user input on a telnet socket.
This allows clients to connect and perform install, start, stop, update, and uninstall actions on bundles.
Here’s a session that connects to the newly launched framework and uses the shell to install the paint example:
Shells, shells, everywhere If you have some familiarity with using OSGi frameworks, you’re likely aware that most OSGi framework implementations (such as Apache Felix, Eclipse Equinox, and Knopflerfish) have their own shells for interacting with a running framework.
The OSGi specification doesn’t define a standard shell (although there has been some work toward this goal recently; see http://felix.apache.org/site/apache-felix-gogo.html), but shells need not be tied to a specific framework and can be implemented as bundles, just as you’ll do here.
This causes an empty paint frame to appear with no available shapes, which makes sense because you haven’t installed any other bundles yet.
In step 2, you install and start the circle and square bundles.
As if by magic, the two shapes dynamically appear in the paint frame’s toolbar and are available for drawing.
In step 3, you install and start the triangle bundle; then, you draw some shapes on the paint canvas.
What happens if you stop a bundle? In step 4, you stop the circle bundle, which you see is replaced on the canvas with the placeholder icon (a construction worker) from DefaultShape.
Figure 3.2 Execution-time evolution: dynamically adding shapes to and removing shapes from the paint program as if by magic.
This shows you in practice that you can use the lifecycle API to build a highly dynamic application, but what’s going on in this example? To understand, we’ll take a topdown approach, using the shell and paint example for context:
In standard Java programming, you use JAR files by placing them on the class path.
A bundle can only be used when it’s installed into a running instance of the OSGi framework.
Conceptually, you can think of installing a bundle into the framework as being similar to putting a JAR file on the class path in standard Java programming.
This simplified view hides some important differences from the standard class path, as you can see in figure 3.3
One big difference is the fact that the OSGi framework supports full lifecycle management of bundle JAR files, including install, resolve, start, stop, update, and uninstall.
At this point, we’ve only touched on installing bundles and resolving their dependencies.
The remainder of this chapter will fully explain the lifecycle activities and how they’re related to each other.
For example, we’ve already mentioned that the framework doesn’t allow an installed bundle to be used until its dependencies (Import-Package declarations) are satisfied.
Figure 3.3 Class path versus OSGi framework with full lifecycle management.
Another huge difference from the standard class path is inherent dynamism.
The OSGi framework supports the full bundle lifecycle at execution time.
This is similar to modifying what’s on the class path dynamically.
As part of lifecycle management, the framework maintains a persistent cache of installed bundles.
This means the next time you start the framework, any previously installed bundles are automatically reloaded from the bundle cache, and the original JAR files are no longer necessary.
Perhaps we can characterize the framework as a fully manageable, dynamic, and persistent class path.
Sounds cool, huh? Let’s move on to how you have to modify the metadata to allow bundles to hook into the lifecycle layer API.
How do you tell the framework to kick-start the bundles at execution time? The answer, as with the rest of the modularity information, is via the bundle metadata.
Here’s the JAR file manifest describing the shell bundle you’ll create:
But to recap, most of the entries are related to the class-level modularity of the bundle.
This is the first sighting of the OSGi lifecycle API in action! The Bundle-Activator header specifies the name of a reachable class (that is, either imported or on the bundle class path) implementing the org.
This interface provides the bundle with a hook into the lifecycle layer and the ability to customize what happens when it’s started or stopped.
Is an activator necessary? Keep in mind that not all bundles need an activator.
If you’re creating a simple library bundle, it isn’t necessary to give it an activator because it’s possible to share classes without one.
To understand what’s going on in the shell example, we’ll now introduce you to three interfaces (BundleActivator, BundleContext, and Bundle) that are the heart and soul of the lifecycle layer API.
The last section described how the shell bundle declares a BundleActivator to hook into the framework at execution time.
We can now look into the details of this interface and the other lifecycle APIs made available from it to the bundle.
This is the bundle’s hook into the world of OSGi.
As you’ve seen, adding an activator to the bundle is straightforward, because you only need to create a class implementing the BundleActivator interface, which looks like this:
For the shell example, the activator allows it to become lifecycle aware and gain access to framework facilities.
The following listing shows the activator for the shell bundle.
Bundles don’t necessarily need to be started in order to do useful things.
Remember the paint program you created in chapter 2: none of the bundles had activators, nor did any of them need to be started, but you still created a fully functioning application.
When the bundle is installed and started, the framework constructs an instance of this activator class and invokes the start() method.
When the bundle is stopped, the framework invokes the stop() method.
The start() method is the starting point for your bundle, sort of like the static main() method in standard Java.
After it returns, your bundle is expected to function until the stop() method is invoked at some later point.
The stop() method should undo anything done by the start() method.
We need to mention a few technical but potentially important details about the handling of the BundleActivator instance:
The activator instance on which start() is called is the same instance on which stop() is called.
As you can see, the rest of the activator isn’t complicated.
In the start() method, you get the port on which the bundle listens for connection requests and the number of allowed concurrent connections.
You also create a TelnetBinding, which does the work of listening on a socket for user input and processes it; the details of creating the telnet binding are omitted here for reasons of simplicity and brevity.
The next step is to start the binding, which creates a new Thread to run the shell.
How this happens is left to the binding, which you start next E.
The point about the binding starting its own thread is important because the activator methods shouldn’t do much work.
This is best practice as with most callback patterns, which are supposed to return quickly, allowing the framework to carry on managing other bundles.
But it’s also important to point out that the OSGi specification doesn’t mandate you start a new thread if your application’s startup doesn’t warrant it—the ball is in your court.
For the activator stop() method, all you do is tell the binding to stop listening to user input and cease to execute.
You should make sure it does stop by waiting until its thread is finished; the binding method waits for its thread to stop.
Sometimes, you may have special cases for certain situations because, as you’ll see later, the shell thread itself may call.
OSGi bundle lifecycle the stop() method, which will cause the bundle to freeze.
In general, if you use threads in your bundles, do so in such a way that all threads are stopped when the stop() method returns.
Now you’ve seen how you can handle starting and stopping a bundle, but what if you want to interact with the OSGi framework? We’ll now switch the focus to the BundleContext object passed into the start() and stop() methods of the activator; this allows a bundle to interact with the framework and manage other bundles.
As you learned in the previous section, the framework calls the start() method of a bundle’s activator when it’s started and the stop() method when it’s stopped.
The methods of the BundleContext interface can be roughly divided into two categories:
We’re interested in the first category of methods, because they give you the ability to install and manage the lifecycle of other bundles, access information about the framework, and retrieve basic configuration properties.
Threading OSGi is designed around the normal Java thread abstraction.
Unlike other, more heavyweight frameworks, it assumes that you do your own thread management.
You gain a lot of freedom by doing this, but at the same time you have to make sure your programs are correctly synchronized and thread safe.
In this simple example, nothing special is needed; but in general, it’s likely that stop() will be called on a different thread than start() (for this reason, you make the member at B volatile)
The OSGi libraries are thread safe, and callbacks are normally done in a way to give you some guarantees.
For example, in the case of the bundle activator, start() and stop() are guaranteed to be called in order and not concurrently.
So, technically, in this particular case the volatile might not be necessary, but in general your code must take thread visibility into account.
The second category of BundleContext methods related to services will be covered in the next chapter.
The shell activator in listing 3.1 uses the bundle context to get its configuration property values C.
It also passes the context into the telnet binding D, which client connections will use to interact with the running framework.
Finally, it uses the context to obtain the bundle’s Bundle object to access the identification information.
For each installed bundle, the framework creates a Bundle object to logically represent it.
The Bundle interface defines the API to manage an installed bundle’s lifecycle; a portion of the interface is presented in the following listing.
As we discuss the Bundle interface, you’ll see that most lifecycle operations have a corresponding method in it.
Unique context One important aspect of the bundle context object is its role as the unique execution context of its associated bundle.
Because it represents the execution context, it’s only valid while the associated bundle is active, which is explicitly from the moment the activator start() method is invoked until the activator stop() method completes and the entire time in between.
Most bundle context methods throw an exception if used when the associated bundle isn’t active.
It’s a unique execution context because each activated bundle receives its own context object.
The framework uses this context for security and resource allocation purposes for each individual bundle.
Given this capability of BundleContext objects, they should be treated as sensitive or private objects and not passed freely among bundles.
Each installed bundle is uniquely identified in the framework by its Bundle object.
From the Bundle object, you can also access two additional forms of bundle identification: the bundle identifier and the bundle location.
You might be thinking, “Didn’t we talk about bundle identification metadata back in chapter 2?” Yes, we did, but don’t get confused.
The identification metadata in chapter 2 was for static identification of the bundle JAR file.
The bundle identifier and bundle location are for execution-time identification, meaning they’re associated with the Bundle object.
You may wonder why you need two different execution-time identifiers.
The main difference between the two is who defines the identifier; see figure 3.4
The bundle identifier is a Java language long value assigned by the framework in ascending order as bundles are installed.
The bundle location is a String value assigned by the installer of the bundle.
Both the bundle identifier and location values uniquely identify the Bundle object and persist across framework executions when the installed bundles are reloaded from the framework’s cache.
Bundle location interpretation The bundle location has a unique characteristic because most OSGi framework implementations interpret it as a URL pointing to the bundle JAR file.
The framework then uses this URL to download the contents of the bundle JAR file during bundle installation.
The specification doesn’t define the location string as an URL, nor is it required, because you can install bundles from an input stream as well.
This means the bundle symbolic name and version pair also act as an execution-time identifier.
Although one instance of Bundle exists for each bundle installed into the framework, at execution time there’s also a special instance of Bundle to represent the framework itself.
This special bundle is called the system bundle; and although the API is the same, it merits its own discussion.
At execution time, the framework is represented as a bundle with an identifier of 0, called the system bundle.
You don’t install the system bundle—it always exists while the framework is running.
The system bundle follows the same lifecycle as normal bundles, so you can manipulate it with the same operations as normal bundles.
But lifecycle operations performed on the system bundle have special meanings when compared to normal bundles.
One example of the special meaning is evident when you stop the system bundle.
Intuitively, stopping the system bundle shuts down the framework in a wellbehaved manner.
It stops all other bundles first and then shuts itself down completely.
With that, we conclude our high-level look at the major API players in the lifecycle layer (BundleActivator, BundleContext, and Bundle)
BundleActivator is the entry point for the bundles, much like static main() in a standard Java application.
BundleContext provides applications with the methods to manipulate the OSGi framework at execution time.
Bundle represents an installed bundle in the framework, allowing state manipulations to be performed on it.
Why so many forms of identification? History plays a role here.
Therefore, prior to R4, it made sense to have internally and externally assigned identifiers.
Now it makes less sense, because the bundle’s symbolic name and version pair are externally defined and explicitly recognized internally by the framework.
There’s still a role for the bundle identifier because in some cases the framework treats a lower identifier value as being better than a higher one when deciding between two otherwise equal alternatives, such as when two bundles export the same version of a given package.
The real loser here is the bundle location, which doesn’t serve a useful purpose other than potentially giving the initial URL of the bundle JAR file.
With this knowledge in hand, we’ll complete the top-down approach by defining the overall bundle lifecycle state diagram and see how these interfaces relate to it.
Until now, we’ve been holding off on explicitly describing the complete bundle lifecycle in favor of getting a high-level view of the API forming the lifecycle layer.
This allowed you to quickly get your hands a little dirty.
Now you can better understand how these APIs relate to the complete bundle lifecycle state diagram, shown in figure 3.5
From figure 3.5, you can see that there’s no direct path from INSTALLED to STARTING.
This is because the framework ensures all dependencies of a bundle are satisfied before it can be used (that is, no classes can be loaded from it)
The transition from the INSTALLED to the RESOLVED state represents this guarantee.
The framework won’t allow a bundle to transition to RESOLVED unless all its dependencies are satisfied.
If it can’t transition to RESOLVED, by definition it can’t transition to STARTING.
Often, the transition to RESOLVED happens implicitly when the bundle is started or another bundle tries to load a class from it, but you’ll see later in this chapter that it’s also possible to explicitly resolve a bundle.
The transition from the STARTING to the ACTIVE state is always implicit.
A bundle is in the STARTING state while its activator’s start() method executes.
Upon successful completion of the start() method, the bundle’s state transitions to ACTIVE; but if the activator throws an exception, it transitions back to RESOLVED.
An ACTIVE bundle can be stopped, which also results in a transition back to the RESOLVED state via the STOPPING state.
The STOPPING state is an implicit state like STARTING, and the bundle is in this state while its activator’s stop() method executes.
A stopped bundle transitions back to RESOLVED instead of INSTALLED because its dependencies are still satisfied and don’t need to be resolved again.
It’s possible to force the framework to resolve a bundle again by refreshing it or updating it, which we’ll discuss later.
Refreshing or updating a bundle transitions it back to the INSTALLED state.
A bundle in the INSTALLED state can be uninstalled, which transitions it to the UNINSTALLED state.
If you uninstall an active bundle, the framework automatically stops the bundle first, which results in the appropriate state transitions to the RESOLVED state and then transitions it to the INSTALLED state before uninstalling it.1 A bundle in the UNINSTALLED state remains there as long as it’s still needed (we’ll explain later what this means), but it can no longer transition to another state.
Now that you understand the complete bundle lifecycle, let’s discuss how these operations impact the framework’s bundle cache and subsequent restarts of the framework.
To use bundles, you have to install them into the OSGi framework.
In doing so, you must specify a location typically interpreted as a URL to the bundle JAR file or an input stream from which the bundle JAR file is read.
In either case, the framework reads the bundle JAR file and saves a copy in a private area known as the bundle cache.
The exact details of the bundle cache are dependent on the framework implementation; the specification doesn’t dictate the format nor structure other than that it must be persistent across framework executions.
If you start an OSGi framework, install a bundle, shut down the framework, and then restart it, the bundle you installed will still be there, as shown in figure 3.6
If you compare this approach to using the class path, where you manually manage everything, having the framework cache and manage the artifacts relieves you of a lot of effort.
In terms of your application, you can think of the bundle cache as the deployed configuration of the application.
This is similar to the chapter 2 discussion of creating different configurations of the paint program.
Your application’s configuration is whichever bundles you install into the framework.
You maintain and manage the configuration using the APIs and techniques discussed in this chapter.
This is a change in the R4.2 version of the OSGi specification.
You can’t go to UNINSTALLED from RESOLVED; you have to go to INSTALLED first, and only INSTALLED goes to UNINSTALLED.
Bundle installation isn’t the only lifecycle operation to impact the bundle cache.
When a bundle is persistently marked as started, subsequent executions of the framework not only reinstall the bundle but also attempt to start it.
From a management perspective, you deploy a configuration of your application by installing a set of bundles and activating them.
If you stop a bundle using Bundle.stop(), this removes the persistently started status of the bundle; subsequent framework executions no longer restart the bundle, although it’s still reinstalled.
You may want to ask, “What about updating and uninstalling a bundle? These must impact the bundle cache, right?” The short answer is, yes, but this isn’t the whole answer.
We’ll explain these oddities when we discuss the relationship between the modularity and lifecycle layers in section 3.5
Next, we’ll delve into the details of the shell bundle as we more fully explore how to use the lifecycle layer API.
So far, you haven’t implemented much functionality for the shell—you just created the activator to start it up and shut it down.
In this section, we’ll show you how to implement the bulk of its functionality.
You’ll use a simple command pattern to provide the executable actions to let you interactively install, start, stop, update, and uninstall bundles.
You’ll even add a persistent history to keep track of previously executed commands.
A high-level understanding of the approach will be useful before you start.
The main piece is the telnet binding, which listens to the configured port for.
The client sends command lines to its thread, where a command line consists of a command name and the arguments for the command.
The thread parses the command line, selects the appropriate command, and invokes it with any specified arguments, as shown in figure 3.7
We won’t discuss the implementation of the telnet binding and the connection thread, but full source code is available in the companion code.
We’ll dissect the command implementations to illustrate how to use Bundle and BundleContext.
Let’s get the ball rolling by showing how you configure the bundle.
The shell needs two configuration properties: one for the port and one for the maximum number of concurrent connections.
When creating a bundle, you can use the BundleContext object to retrieve configuration properties instead.
The OSGi specification doesn’t specify a user-oriented way to set bundle configuration properties, so different frameworks handle this differently; typically, they provide a configuration file where the properties are set.
This listing continues the activator implementation from listing 3.1; in the activator, you use these two methods to get configuration properties.
This method looks in the framework properties to find the value of the specified property.
If it can’t find the property, it searches the system properties, returning null if the property isn’t found.
For the shell, you return default values if no configured value is found.
The OSGi specification also defines some standard framework properties, shown in table 3.1
There you have it: your first real interaction with the OSGi framework.
This is only a small part of the API that you can use in your bundles, but we’ll cover a lot of ground in the next section, so don’t worry.
And those of you thinking, “Hey, this configuration mechanism seems overly simplistic!” are correct.
Bundle properties are the simplest mechanism available and should only be used for properties that.
In this regard, they may not be the best choice for the shell, but it depends on what you want to achieve; for example, it makes it difficult to change the shell’s port dynamically.
For now, we’ll keep things simple, so this is sufficient.
Each bundle installed into the framework is represented by a Bundle object and can be identified by its bundle identifier, location, or symbolic name.
For most of the shell commands you’ll implement, you’ll use the bundle identifier to retrieve a Bundle object, because the bundle identifier is nice and concise.
Most of the commands accept a bundle identifier as a parameter, so let’s look at how you can use it and the bundle context to access Bundle objects associated with other bundles.
As part of the design, you create an abstract BasicCommand class to define a shared method, getBundle(), to retrieve bundles by their identifier, as shown here:
The only special case you need to worry about is when no bundle with the given identifier exists.
With this basic functionality in place, you can start the first command.
The next listing shows the implementation of an install command, and figure 3.8 reminds you which portion of the bundle lifecycle is involved.
Figure 3.8 The installrelated portion of the bundle lifecycle state diagram.
In most framework implementations, the argument to installBundle() is conveniently interpreted as a URL in String form from which the bundle JAR file can be retrieved.
Because the user enters the URL argument as a String, you can use it directly to install the bundle.
If the install succeeds, then a new Bundle object corresponding to the newly installed bundle is returned.
The bundle is uniquely identified by this URL, which is used as its location.
This location value will also be used in the future to determine if the bundle is already installed.
If a bundle is already associated with this location value, the Bundle object associated with the previously installed bundle is returned instead of installing it again.
If the install operation is successful, the command outputs the installed bundle’s identifier.
The bundle context also provides an overloaded installBundle() method for installing a bundle from an input stream.
We won’t show this method here, but the other form of installBundle() accepts a location and an open input stream.
When you use this other form of the method, the location is used purely for identification, and the bundle JAR file is read from the passed-in input stream.
Now you have a command to install bundles, so the next operation you’ll want to do is start bundles.
You use the method from the base command class to get the Bundle object associated with the user-supplied identifier, and then you invoke Bundle.start() to start the bundle associated with the identifier.
The result of Bundle.start() depends on the current state of the associated bundle.
If the bundle is INSTALLED, it transitions to ACTIVE via the RESOLVED and STARTING states.
If the bundle is either STARTING or STOPPING, start() blocks until the bundle enters either ACTIVE or RESOLVED.
If the bundle is already ACTIVE, calling start() again has no effect.
A bundle must be resolved before it can be started.
Figure 3.9 The start-related portion of the bundle lifecycle state diagram.
If the bundle’s dependencies can’t be resolved, start() throws a BundleException and the bundle can’t be used until its dependencies are satisfied.
If this happens, you’ll typically install additional bundles to satisfy the missing dependencies and try to start the bundle again.
Any exceptions thrown from the activator result in a failed attempt to start the bundle and an exception being thrown from Bundle.start()
Now we can look at stopping bundles, which is similar to starting them; see the next listing and figure 3.10
Like starting a bundle, stopping a bundle takes a simple call to Bundle.stop() on the Bundle object retrieved from the specified identifier.
As before, you must be mindful of the bundle’s state.
In the ACTIVE state, the bundle transitions to RESOLVED via the STOPPING state.
If the bundle has an activator and the activator’s stop() method throws an exception, a BundleException is thrown.
Let’s continue with the update command in the following listing (see figure 3.11)
Figure 3.10 The stop-related portion of the bundle lifecycle state diagram.
Figure 3.11 The update-related portion of the bundle lifecycle state diagram.
By now, you may have noticed the pattern we mentioned in the beginning.
Most lifecycle operations are methods on the Bundle and BundleContext objects.
The update() method is available in two forms: one with no parameters (shown) and one taking an input stream (not shown)
The update command uses the form without parameters here, which reads the updated bundle JAR file using the original location value as a source URL.
If the bundle being updated is in the ACTIVE state, it needs to be stopped first, as required by the bundle lifecycle.
You don’t need to do this explicitly, because the framework does it for you, but it’s still good to understand that this occurs because it impacts the application’s behavior.
The update happens in either the RESOLVED or INSTALLED state and results in a new revision of the bundle in the INSTALLED state.
As in the stop command, a bundle shouldn’t try to update itself.
You can now wrap up the lifecycle operations by implementing the uninstall command, as shown next (see figure 3.12)
The OSGi specification provides a third option for updating bundles based on bundle metadata.
If it’s present, Bundle.update() with no parameters uses the update location value specified in the metadata as the URL for retrieving the updated bundle JAR file.
Using this approach is discouraged because it’s confusing if you forget it’s set, and it doesn’t make sense to bake this sort of information into the bundle.
Figure 3.12 The uninstall-related portion of the bundle lifecycle state diagram.
As with the other lifecycle operations, a bundle shouldn’t attempt to uninstall itself.
You’ve created a telnet-based shell bundle that you can use in any OSGi framework.
Most of the shell commands require the bundle identifier to perform their action, but how does the shell user know which identifier to use? You need some way to inspect the state of the framework’s installed bundle set.
You need one more command to display information about the bundles currently installed in the framework.
The next listing shows a simple implementation of a bundles command.
The rest of the implementation loops through the returned array and prints out information from each Bundle object.
Here you print the bundle identifier, lifecycle state, name, location, and symbolic name for each bundle.
With this command in place, you have everything you need for the simple shell.
You can install, start, stop, update, and uninstall bundles and list the currently installed bundles.
That was fairly simple, wasn’t it? Think about the flexibility at your fingertips versus the amount of effort needed to create the shell.
Now you can create applications as easily deployable configurations of bundles that you can manage and evolve as necessary over time.
Before you move back to the paint program, two final lifecycle concepts are worth exploring in order to fully appreciate the approach you’ll take to make the paint program dynamically extensible: persistence and events.
We’ll describe them in the context of the shell example; but as you’ll see in the paint example in a couple of pages, they’re generally useful tools to have in mind when building OSGi applications.
As we mentioned when discussing bundle activators, the framework creates an instance of a bundle’s activator class and uses the same instance for starting and subsequently stopping the bundle.
An activator instance is used only once by the framework to start and stop a bundle, after which it’s discarded.
If the bundle is subsequently restarted, a new activator instance is created.
Given this situation, how does a bundle persist state across stops and restarts? Stepping back even further, we mentioned how the framework saves installed bundles into a cache so they can be reloaded the next time the framework starts.
How does a bundle persist state across framework sessions? There are several possibilities.
One possibility is to store the information outside the framework, such as in a database or a file, as shown in figure 3.13
The disadvantage of this approach is that the state isn’t managed by the framework and may not be cleaned up when the bundle is uninstalled.
Another possibility is for a bundle to give its state to another bundle that isn’t being stopped; then, it can get the state back after it restarts, as shown in figure 3.14
This is a workable approach, and in some cases it makes the most sense.
For simplicity, it would be nice to be able to use files, but have them managed by the framework.
The framework maintains a private data area in the file system for each installed bundle.
When using the private data area, you don’t need to worry about where it is on the file system because the framework takes care of that for you, as well as cleaning up in the event of your bundle being uninstalled (see figure 3.15)
It may seem odd to not directly use files to store your data; but if you did, it would be impossible for your bundle to clean up during an uninstall.
This is because a bundle isn’t notified when it’s uninstalled.
Further, this method simplifies running with security enabled, because bundles can be granted permission to access their private area by the framework.
For the shell example, you want to use the private area to persistently save the command history.
Here’s how the history command should work; it prints the commands issued via the shell in reverse order:
Listing 3.11 shows how you use the bundle’s private storage area to save the command history.
The bundle activator’s start() and stop() methods also need to be modified to invoke these methods, but these changes aren’t shown here, so please refer to the companion code for complete implementation details.
The method takes a relative path as a String and returns a valid File object in the storage area.
After you get the File object, you can use it normally to create the file, make a subdirectory, or do whatever you want.
It’s possible for a framework to return null when a bundle requests a file; so as you can see C, you need to handle this possibility.
This can happen because the OSGi framework was designed to run on a variety of devices, some of which may not support a file system.
For the shell, you ignore it if there’s no file system support, because the history command is noncritical functionality.
If you want to retrieve a File object for the root directory of your bundle’s storage area, you can call getDataFile() with an empty string.
Your bundle is responsible for managing the content of its data area, but you don’t need to worry about cleaning up when it’s uninstalled, because the framework takes care of this.
You could finish the history command, but let’s try to make it a little more interesting by keeping track of what’s going on inside the framework.
You can record not only the issued commands, but also the impact they have on the framework.
The next section shows how you can achieve this using the framework’s eventnotification mechanism.
To create bundles and, ultimately, applications that are flexible enough to not only cope with but also take advantage of this dynamism, you need to pay attention to execution-time changes.
The lifecycle layer API provides access to a lot of information, but it isn’t easy to poll for changes; it’s much more convenient if you can be notified when changes occur.
To make this possible, the OSGi framework supports two types of events: BundleEvents and FrameworkEvents.
The former event type reports changes in the lifecycle of bundles, whereas the latter reports framework-related issues.
You can use the normal Java listener pattern in your bundles to receive these events.
The BundleContext object has methods to register BundleListener and FrameworkListener objects for receiving BundleEvent and FrameworkEvent notifications, respectively.
The following listing shows how you implement the history command.
You record all executed commands as well as the events they cause during execution.
Plan ahead Keep in mind that your bundle may be updated.
Due to this possibility, you should design your bundles so they properly deal with previously saved state, because they may start with a private area from an older version of the bundle.
The best approach is for your bundles to seamlessly migrate old state formats to new state formats if possible.
One tricky issue, though, is that the update lifecycle operation may also be used to downgrade a bundle.
In this case, your bundle may have difficulty dealing with the newer state formats, so it’s probably best if you implement your bundles to delete any existing state if they can’t understand it.
Otherwise, you can always uninstall the newer bundle first and then install the older version instead of downgrading.
You use an interceptor pattern to wrap the commands so you can record the issued commands.
The wrapper also records any events in the history by implementing the BundleListener and FrameworkListener interfaces.
You maintain a list of all issued commands and received events in the m_history member defined at B.
The history wrapper command forwards the command execution to the command C and stores it in the history list.
Here, you record the event information in the history list.
The most important part of the event is its type.
Here, you also record the event information in the history list.
You register the listeners using the bundle context as follows:
It’s not necessary to remove the listeners, because the framework will do so automatically when the bundle is stopped; this makes sense because the bundle context is no longer valid after the bundle is stopped.
You only need to explicitly remove your listeners if you want to stop listening to events while your bundle is active.
It’s possible for framework implementations to deliver them synchronously, but typically they don’t because it complicates concurrency handling.
Sometimes you need synchronous delivery because you need to perform an action as the event is happening, so to speak.
This allows you to take action when a certain operation is triggered; for example, you can give permissions to a bundle at the moment it’s installed.
Synchronous bundle listeners are sometimes necessary (as you’ll see in the paint example in the next section), but should be used with caution.
They can lead to concurrency issues if you try to do too much in the callback; as always, keep your callbacks as short and simple as possible and don’t call foreign code while holding a lock.
In all other cases, the thread invoking the listener callback method is undefined.
Events become much more important when you start to write more sophisticated bundles that take full advantage of the bundle lifecycle.
We’ve mentioned it numerous times: a bundle isn’t supposed to change its own state.
But what if a bundle wants to change its own state? Good question.
This is one of the more complicated aspects of the lifecycle layer, and there are potentially negative issues involved.
The central issue is that if a bundle stops itself, it finds itself in a state it shouldn’t be in.
Additionally, the framework has cleaned up its bookkeeping for the bundle and has released any framework facilities it was using, such as unregistering all of its event listeners.
The situation is even worse if a bundle tries to uninstall itself, because the framework will likely release its class loader.
In short, the bundle is in a hostile environment, and it may not be able to function properly.
Because its bundle context is no longer valid, a stopped bundle can no longer use the functionality provided by the framework.
Even if the bundle’s class loader is released, this may not pose a serious issue if the bundle doesn’t need any new classes, because the class loader won’t be garbage collected until the bundle stops using it.
But you’re not guaranteed to be able to load new classes if the bundle was uninstalled.
In this case, the framework may have closed the JAR file associated with the bundle.
Alreadyloaded classes continue to load, but all bets are off when attempting to load new classes.
Depending on your bundle, you may run into other issues too.
If the bundle tries to stop itself on its own thread, that same thread can end up in a cycle waiting for other sibling threads to complete.
For example, the simple shell uses a thread to listen for telnet connections and then uses secondary threads to execute the commands issued on those connections.
Because the calling thread is one of the secondary threads, it’ll end up waiting forever for the connection thread to complete.
You have to be careful of these types of situations, and they’re not always obvious.
Under normal circumstances, you shouldn’t try to stop, uninstall, or update your own bundle.
Let’s look at a case where you may need to do it anyway.
We’ll use the shell as an example, because it provides a means to update bundles, and it may need to update itself.
What do you have to do to allow a user to update the shell bundle via the shell command line? You must do two things to be safe:
Use a new thread when you stop, update, or uninstall your own bundle.
Do nothing in the new thread after calling stop, update, or uninstall.
You need to do this to prevent yourself from waiting forever for the shell thread to return when you get stopped and to avoid the potential ugliness of the hostile environment in which the thread will find itself.
The following listing shows the changes to the implementation of the stop command to accommodate this scenario.
Listing 3.13 Example of how a bundle can stop itself.
Dynamically extending the paint program bundle, you need to stop it using a different thread.
For this reason, you create and start a new thread of type SelfStopThread, which executes the Bundle.stop() method C.
There’s one final point to note in this example: you change the behavior of stopping a bundle in this case from synchronous to asynchronous.
Ultimately, this shouldn’t matter much, because the bundle will be stopped anyway.
You should also modify the implementation of the update and uninstall commands the same way.
Using the shell to stop the framework (the system bundle) also requires special consideration.
Why? Because stopping the system bundle causes the framework to stop, which stops every other bundle.
This means you’ll stop your bundle indirectly, so you should make sure you’re using a new thread.
We hope you now have a good understanding of what is possible with OSGi’s lifecycle layer.
Let’s look at how you can use the individual parts of the lifecycle layer to dynamically extend the paint program.
As you’ll recall from the last chapter, you first converted a nonmodular version of the paint program into a modular one using an interfacebased programming approach for the architecture.
This is great because you can reuse the resulting bundles with minimal extra work.
The bundles containing the shape implementations don’t need to change, except for some additional metadata in their manifest.
You just need to modify the paint program to make it possible for shapes to be added and removed at execution time.
The approach you’ll take is a well-known pattern in the OSGi world, called the extender pattern.
The main idea behind the extender pattern is to model dynamic extensibility on the lifecycle events (installing, resolving, starting, stopping, and so on) of other bundles.
Typically, some bundle in the application acts as the extender: it listens for bundles being started and/or stopped.
When a bundle is started, the extender probes it to see if it’s an extension bundle.
If the bundle does contain an extension, the extension is described by the metadata.
The extender reads the metadata and performs the necessary tasks on behalf of the extension bundle to integrate it into the application.
The extender also listens for extension bundles to be stopped, in which case it removes the associated extensions from the application.
That’s the general description of the extender pattern, which is shown in figure 3.16
Let’s look at how you’ll use it in the paint program.
The extension metadata will be contained in the bundle manifest and will describe which class implements the shape contained in the shape bundle.
The extender will use this information to load the shape class from the bundle, instantiate it, and inject it into the application when an extension bundle is activated.
If a shape bundle is stopped, the extender will remove it from the application.
The first thing you need to do is define the extension metadata for shape bundles to describe their shape implementation.
In the following snippet, you add a couple of constants to the SimpleShape interface for extension metadata property names; it’s not strictly necessary to add these, but it’s good programming practice to use constants:
For the reverse, if the shape bundle is stopped, tracker removes its associated shape.
Figure 3.7 Paint program as an implementation of the extender pattern.
The constants indicate the name of the shape, the bundle resource file for the shape’s icon, and the bundle class name for the shape’s class.
From the constants, it’s fairly straightforward to see how you’ll describe a specific shape implementation.
You only need to know the name, an icon, and the class implementing the shape.
As an example, for the circle implementation you add the following entries to its bundle manifest:
The name is just a string, and the icon and class refer to a resource file and a class inside the bundle JAR file, respectively.
You add similar metadata to the manifests of all shape implementation bundles, which converts them all to extensions.
Next, you need to tweak the architecture of the paint program to make it cope with dynamic addition and removal of shapes.
Comparing the new design to the original, you add two new classes: ShapeTracker and DefaultShape.
They help you dynamically adapt the paint frame to deal with SimpleShape implementations dynamically appearing and disappearing.
In a nutshell, the ShapeTracker is used to track when extension bundles start or stop, in which case it adds or removes DefaultShapes to/from the PaintFrame, respectively.
The concrete implementation of the ShapeTracker is a subclass of another class, called BundleTracker.
The latter class is a generic class for tracking when bundles are started or stopped.
Because BundleTracker is somewhat long, we’ll divide it across multiple listings; the first part is shown next.
The bundle tracker is constructed with a BundleContext object, which is used to listen for bundle lifecycle events.
You need to react on the STOPPING event instead of the STOPPED event because it’s still possible to use the stopping bundle, which hasn’t been stopped yet; a potential subclass might need to do this if it needed to access the stopping bundle’s BundleContext object.
The bundle listener’s single method B makes sure the tracker is tracking bundles C.
If so, for started events, it adds the associated bundle to its bundle list D and invokes the abstract addedBundle() method.
Likewise, for stopping events, it removes the bundle from its bundle list and invokes the abstract removedBundle() method.
The following listing shows the next portion of the BundleTracker.
To start a BundleTracker instance tracking bundles, you must invoke its open() method.
This methods registers a bundle event listener and processes any existing ACTIVE bundles by adding them to its bundle list and invoking the abstract addedBundle() method.
The getBundles() method provides access to the current list of active bundles being tracked.
Because BundleTracker is abstract, subclasses must provide implementations of addedBundle() and removedBundle() to perform custom processing of added and removed bundles, respectively.
This removes its bundle listener, removes each currently tracked bundle from its bundle list, and invokes the abstract removedBundle() method.
Now that you know how the BundleTracker works, let’s return to its subclass, ShapeTracker.
The heart of this subclass is the processBundle() method shown next, which processes added and removed bundles.
Standardizing bundle trackers Tracking bundles is a useful building block.
It’s so useful that the OSGi Alliance decided to create a standard BundleTracker for the R4.2 specification.
ShapeTracker overrides BundleTracker’s addedBundle() and removedBundle() abstract methods to invoke processBundle() in either case.
You determine whether the bundle is an extension by probing its manifest for the Extension-Name property B.
Any bundle without this property in its manifest is ignored.
If the bundle being added contains a shape, the code grabs the metadata from the bundle’s manifest headers and adds the shape to the paint frame wrapped as a DefaultShape C.
If the bundle being removed contains a shape, you remove the shape from the paint frame D.
It implements the SimpleShape interface and is responsible for lazily creating the shape implementation using the Extension-Class metadata.
It also serves as a placeholder for the shape if and when the shape is removed from the application.
You didn’t have to deal with this situation in the original paint program, but now shape implementations can appear or disappear at any time when bundles are installed, started, stopped, and uninstalled.
In such situations, the DefaultShape draws a placeholder icon on the paint canvas for any departed shape implementations.
In summary, when the paint application is started, its activator creates and opens a ShapeTracker.
This tracks STARTED and STOPPED bundle events, interrogating the associated bundle for extension metadata.
For every started extension bundle, it adds a new DefaultShape for the bundle to the paint frame, which creates the shape implementation, if needed, using the extension metadata.
When the bundle stops, the ShapeTracker removes the shape from the paint frame.
When a drawn shape is no longer available, the DefaultShape is used to draw a placeholder shape on the canvas instead.
If the departed shape reappears, the placeholder is removed and the real shape is drawn on the canvas again.
Now you have a dynamically extensible paint program, as demonstrated in section 3.2.1
Although we didn’t show the activator for the paint program, it’s reasonably simple and only creates the framework and shape tracker on start and disposes of them on stop.
Overall, this is a good example of how easy it is to make a modularized application take advantage of the lifecycle layer to make it dynamically extensible.
As a bonus, you no longer need to export the implementation packages of the shape implementations.
What you’re still missing at this point is a discussion about how the lifecycle and module layers interact with each other, which we’ll get into next.
A two-way relationship exists between OSGi’s lifecycle and module layers.
The lifecycle layer manages which bundles are installed into the framework, which obviously impacts how the module layer resolves dependencies among bundles.
The module layer uses the metadata in bundles to make sure all their dependencies are satisfied before they can be used.
This symbiotic relationship creates a chicken-and-egg situation when you want to use your bundles; to use a bundle you have to install it, but to install a bundle you must have a bundle context, which are only given to bundles.
This close relationship is also obvious in how the framework resolves bundle dependencies, especially when bundles are dynamically installed and/or removed.
Let’s explore this relationship by first looking into bundle dependency resolution.
The act of resolving a bundle happens at the discretion of the framework, as long as it happens before any classes are loaded from the bundle.
Often, when resolving a given bundle, the framework ends up resolving another bundle to satisfy a dependency of the original bundle.
This can lead to cascading dependency resolution, because in order for the framework to use a bundle to satisfy the requirements of another bundle, the satisfying bundle too must be resolved, and so on.
Because the framework resolves dependencies when needed, it’s possible to mostly ignore transitioning bundles to the RESOLVED state; you can start a bundle and know the framework will resolve it before starting it, if possible.
This is great compared to the standard Java way, where you can run into missing dependencies at any point during the lifetime of your application.
But what if you want to make sure a given bundle resolves correctly? For example, maybe you want to know in advance whether an installed bundle can be started.
In this case, there’s a way to ask the framework to resolve the bundle directly, but it’s not a method on Bundle like most other lifecycle operations.
The Package Admin Service is represented as an interface and is shown here:
You can explicitly resolve a bundle with the resolveBundles() method, which takes an array of bundles and returns a Boolean flag indicating whether the bundles could.
The Package Admin Service can do a bit more than resolving bundles, and it’s a fairly important part of the framework; it also supports the following operations, among others:
Determines which bundle owns a particular class—In rare circumstances, you may need to know which bundle owns a particular class.
You can accomplish this with the getBundle() method, which takes a Class and returns the Bundle to which it belongs.
Refreshes the dependency resolution for bundles—Because the installed set of bundles can evolve over time, sometimes you need to have the framework recalculate bundle dependencies.
The most important feature of the Package Admin Service isn’t the ability to resolve bundles or introspect dependencies; it’s the ability to refresh bundle dependencies, which is another tool needed for managing bundles.
But before we get into the details of refreshing bundles, let’s finish the discussion of explicitly resolving bundles.
To demonstrate how to use the Package Admin Service to explicitly resolve a bundle, you’ll create a new resolve command for the shell to instigate bundle resolution, as shown next.
If the resolve command is executed with no arguments, you invoke resolveBundles() with null, which causes the framework to attempt to resolve all unresolved bundles.
For each identifier, you get its associated Bundle object and add it to a list.
After you’ve retrieved the complete list of bundles, you pass them in as an array to resolveBundles()
The framework attempts to resolve any unresolved bundles of those specified.
It’s worthwhile to understand that the framework may resolve bundles in addition to those that were specified.
The specified bundles are the root of the framework’s resolve process; the framework will resolve any additional unresolved bundles necessary to resolve the specified roots.
Resolving a bundle is a fairly easy process, because the framework does all the hard work for you.
As long as your bundle’s dependencies are resolved, you have nothing to worry about, right? It turns out the dynamic nature of the bundle lifecycle makes this an invalid assumption.
Sometimes you need to have the framework recalculate a bundle’s dependencies.
You’re probably wondering, “Why?” We’ll tell you all about it in the next section.
The lifecycle layer allows you to deploy and manage your application’s bundles.
Up until now we’ve focused on installing, resolving, and starting bundles, but there are other interesting bundle lifecycle operations.
How about updating or uninstalling a bundle? In and of themselves, these operations are as conceptually simple as the other lifecycle operations.
We certainly understand what it means to update or uninstall a bundle.
When you update or uninstall a resolved bundle, you stand a good chance of disrupting your system.
This is the place where you can start to see the impact of the framework’s dynamic lifecycle management.
The simple case is updating or uninstalling a self-contained bundle.
In this case, the disruption is limited to the specific bundle.
Even if the bundle imports packages from other bundles, the disruption is limited to the specific bundle being updated or uninstalled.
In either case, the framework stops the bundle if it’s active.
In the case of updating, the framework updates the bundle’s content and restarts it if it was previously active.
Complications arise if other bundles depend on the bundle being updated or uninstalled.
Such dependencies can cause a cascading disruption to your application, if the dependent bundles also have bundles depending on them.
Why do dependencies complicate the issue? Consider updating a given bundle.
Other dependent bundles have potentially loaded classes from the old version of the bundle.
They can’t just start loading classes from the new version of the bundle, because they would see old versions of the classes they already loaded mixed with new versions of classes loaded after the update.
In the case of an uninstalled bundle, the situation is more dire, because you can’t pull the rug out from under the dependent bundles.
It’s worthwhile to limit the disruptions caused by bundle updates or uninstalls.
The framework provides such control by making updating and uninstalling bundles a twostep process.
Conceptually, the first step prepares the operation; and the second step, called refreshing, enacts its.
How does this help? It allows you to control when the changeover to the new bundle version or removal of a bundle occurs for updates and uninstalls, respectively, as shown in figure 3.19
We say this is a two-step process, but what happens in the first step? For updates, the new bundle version is put in place, but the old version is still kept around so bundles depending on it can continue loading classes from it.
You may be thinking, “Does this mean two versions of the bundle are installed at the same time?” Effectively, the answer is, yes.
And each time you perform an update without a refresh, you introduce yet another version.
For uninstalls, the bundle is removed from the installed list of bundles, but it isn’t removed from memory.
Again, the framework keeps it around so dependent bundles can continue to load classes from it.
For example, imagine you want to update a set of bundles.
It would be fairly inconvenient if the framework refreshed all dependent bundles after each individual update.
With this two-step approach, you can update all bundles in the set and then trigger one refresh of the framework at the end.
You can experience a similar situation if you install a bundle providing a newer version of a package.
Existing resolved bundles importing an older version of the package won’t be automatically rewired to the new bundle unless they’re refreshed.
Again, it’s nice to be able to control the point in time when this happens.
It’s a fairly common scenario when updating your application that some of your bundles are updated, some are uninstalled, and some are installed; so a way to control when these changes are enacted is helpful.
You trigger a refresh by using the Package Admin Service again.
To illustrate how to use it, let’s add a refresh command to the shell, as shown next.
Figure 3.19 Updating and refreshing bundles is a two-step process.
Most of the work normally takes place in the second step during the framework refresh operation.
Just as in the resolve command, you rely on the magic method to get the Package Admin Service.
If no arguments are given to the command, you pass in null to the Package Admin Service.
This results in the framework refreshing all previously updated and uninstalled bundles since the last refresh.
This captures the update and uninstall cases presented earlier, but it doesn’t help with the rewiring case.
You achieve that by passing in the specific bundles you want refreshed.
You parse their identifiers out of the supplied argument, retrieve their associated Bundle object, and add them to a list to be refreshed B.
You then pass in the array of bundles to refresh to the Package Admin Service C.
The method returns to the caller immediately and performs the following steps on a separate thread:
It computes the graph of affected dependent bundles, starting from the specified bundles (or from all updated or uninstalled bundles if null is specified)
Any bundle wired to a package currently exported by a bundle in the graph is added to the graph.
The graph is fully constructed when there is no bundle outside the graph wired to a bundle in the graph.
Each bundle in the graph in the RESOLVED state, including those that were stopped, is unresolved and moved to the INSTALLED state.
Each bundle in the graph in the UNINSTALLED state is removed from the graph and completely removed from the framework (is free to be garbage collected)
You’re back to a fresh starting state for the affected bundles.
As a result of these steps, it’s possible that some of the previously ACTIVE bundles can no longer be resolved; maybe a bundle providing a required package was uninstalled.
The following shell session shows how you can use the resolve and refresh commands in combination to manage a system:
You install a bundle and resolve it using the resolve command B, which transitions it to the RESOLVED state.
Using the refresh command C, you transition it back to the INSTALLED state.
At this point, you’ve achieved a lot in understanding the lifecycle layer; but before you can finish, we need to explain some nuances about updating bundles.
One of the gotchas many people run into when updating a bundle is the fact that it may or may not use its new classes after the update operation.
We said previously that updating a bundle is a two-step process, where the first step prepares the operation and the second step enacts it, but this isn’t entirely accurate when you update a bundle.
The specification says the framework should enact the update immediately, so after the update the bundle should theoretically be using its new classes; but it doesn’t necessarily start using them immediately.
In some situations, after a bundle is updated, new classes are used; in other situations, old classes are used.
Why not just wait until a refresh to enact the new revision completely?
The original R1 specification defined the update operation to update a bundle.
With experience, it became clear that the specified definition of update was insufficient.
Too many details were left for framework implementations to decide, such as when to dispose of old classes and start using new classes.
This led to inconsistencies, which made it difficult to manage bundle lifecycles across different framework implementations.
This situation resulted in the introduction of the Package Admin Service in the R2 specification, to resolve the inconsistencies around update once and for all.
These concerns leave you with the less-than-clean approach to bundle update that we have today, but at least it’s fairly consistent across framework implementations.
Back to the issue of an updated bundle sometimes using old or new classes.
As arcane as it may be, there is a way to understand what’s going on.
Whether your bundle’s new classes or the old classes are used after an update depends on two factors:
If the classes come from a private bundle package (one that isn’t exported), the new classes become available immediately no matter what.
The old versions of the classes are no longer needed.
If any other bundles are using the exported packages, the new classes don’t.
In this case, the new classes aren’t made available until the PackageAdmin.
In chapter 5, you’ll learn that bundles can also import the same packages they export.
If a bundle imports a package it exports, and the imported package from the updated bundle matches the exported package from the old version, the updated bundle’s import is wired to the old exported packages.
This may work out well in some cases—when you’re fixing a bug in a private package, for example.
But it can potentially lead to odd situations, because the updated bundle is using new versions of private classes alongside old versions of exported classes.
If you need to avoid this situation, you should specify version ranges when your bundle imports its own packages.
If the updated bundle imports its own package, but the import doesn’t match the old version of the exported package, you have a different situation.
It’s similar to the case where the bundle only exports the package.
In this case, the new classes from the exported packages become available immediately to the updated exporting bundle and for future resolves of other bundles, but not to existing importer bundles, which continue to see the old version.
You can avoid some of these issues through interface-based programming and bundle partitioning.
For example, if you can separate shared APIs (the APIs through which bundles interact) into interfaces, and you place those interfaces into a separate set of packages contained in a separate bundle, you can sometimes simplify this situation.
In such a setup, both the client bundles and the bundles implementing the interfaces have dependencies on the shared API bundle, but not on each other.
In other words, you limit the coupling between clients and the providers of the functionality.
In this chapter, you’ve seen that whether your desire is to deploy the bundles needed to execute your application or to create a sophisticated auto-adaptive system, the lifecycle layer provides everything you need.
A bundle can only be used by installing it into a running instance of the OSGi framework.
The lifecycle layer API is composed of three main interfaces: BundleActivator, BundleContext, and Bundle.
The framework associates a lifecycle state with each installed bundle, and the BundleContext and Bundle lifecycle interfaces make it possible to transition bundles though these states at execution time.
Monitoring bundle lifecycle events is a form of dynamic extensibility available in the OSGi framework based on the dynamically changing installed set of bundles (also known as the extender pattern)
The lifecycle and module layers have a close relationship, which is witnessed when bundles are updated and uninstalled.
You use the Package Admin Service to manage this interaction.
Now we’ll move on to the next layer of the OSGi framework: the service layer.
Services promote interface-based programming among bundles and provide another form of dynamic extensibility.
So far, you’ve seen two layers of the OSGi framework.
The module layer helps you separate an application into well-defined, reusable bundles, and the lifecycle layer builds on the module layer to help you manage and evolve bundles over time.
Now we’ll make things even more dynamic with the third and final layer of OSGi: services.
We’ll start this chapter with a general discussion about services to make sure we’re all thinking about the same thing.
We’ll then look at when you should (and shouldn’t) use services and walk through an example to demonstrate the OSGi service model.
At this point, you should understand the basics, so we’ll take a closer look at how best to handle the dynamics of OSGi services, including common pitfalls and how to avoid them.
With these techniques in mind, you’ll update the ongoing paint program to use services and see how the service layer relates to the module and lifecycle layers.
We’ll conclude with a review of standard OSGi framework services and tell you more about the compendium.
As you can see, we have many useful and interesting topics to cover, so let’s get started and talk about services.
Before looking at OSGi services, we should first explain what we mean by a service, because the term can mean different things to different people depending on their background.
When you know the “what,” you also need to know why and when to use services, so we’ll get to that, too.
You may think a service is something you access across the network, like retrieving stock quotes or searching Google.
But the classical view of a service is something much simpler: “work done for another.” This definition can easily apply to a simple method call between two objects, because the callee is doing work for the caller.
How does a service differ from a method call? A service implies a contract between the provider of the service and its consumers.
Consumers typically aren’t worried about the exact implementation behind a service (or even who provides it) as long as it follows the agreed contract, suggesting that services are to some extent substitutable.
Using a service also involves a form of discovery or negotiation, implying that each service has a set of identifying features (see figure 4.1)
If you think about it, Java interfaces provide part of a contract, and Java class linking is a type of service lookup because it “discovers” methods based on signatures and class hierarchy.
Different method implementations can also be substituted by changing the JAR files on the class path.
So a local method call could easily be seen as a service, although it would be even better if you could use a high-level abstraction to find services or if there was a more dynamic way to switch between implementations at execution time.
Thankfully, OSGi helps with both by recording details of the service contract, such as interface names and metadata, and by providing a registry API to publish.
Figure 4.1 Services follow a contract and involve some form of discovery.
The what, why, and when of services and discover services.
You’ll hear more about this later, in section 4.2; for now, let’s continue to look at services in general.
You may be thinking that a Java method call in the same process can’t possibly be a service, because it doesn’t involve a remote connection or a distributed system.
In reality, as you’ll see throughout this chapter, services do not have to be remote, and there are many benefits to using a service-oriented approach in a purely local application.
The main drive behind using services is to get others to do work on your behalf, rather than attempting to do everything yourself.
The key semantic difference between these two approaches is as follows:
Typically, in a component-oriented approach, the architect is focused on ensuring that the component they provide is packaged in such a way that it makes their life easier.
You know that when it comes to packaging and deploying Java code, the code will often be used in a range of different scenarios.
For example, a stock-quote program can be deployed as a console, GUI, or web application by combining different components.
A component design approach tries to make it as easy as possible for the architect to select what functionality they want to deploy without hardcoding this into their application.
This contrasts with a service-oriented approach, where the architect is focused on supplying a function or set of functions to consumers who typically have little interest in how the internals of the individual component are constructed, but have specific requirements for how they want the function to behave.
With this in mind, let’s continue our introduction to services by considering the benefits of services.
Think of this like a game of pass-the-parcel (see figure 4.2), where each developer is trying to pass parcels of work to other developers—except in this game, when the music stops, you want the smallest pile of parcels!
In other words, it encourages a plug-and-play approach to software development, which means much more flexibility during development, testing, deployment, and maintenance.
You don’t mind where a service comes from, as long as it does what you want.
Still not convinced? Let’s see how each of these points helps you build a better application.
One of the most important aspects of a service is the contract.
Putting too much detail in a contract tightens the coupling between provider and consumer and limits the possibility of swapping in other implementations.
Figure 4.2 Using CRC to place responsibilities can be like playing pass-the-parcel.
To put it in clothing terms, you want it nice and stretchy to give your application room to breathe.
A good service contract clearly and cleanly defines the boundary between major components and helps with development and maintenance.
After the contract is defined, you can work on implementing service providers and consumers in parallel to reduce development time, and you can use scripted or mock services to perform early testing of key requirements.
Contracts are good news for everyone—but how do you define one in Java?
They list the various methods that make up a service along with expected parameters and return types.
After they’re defined, you can begin programming against the agreed-on set of interfaces without having to wait for others to finish their implementations (see figure 4.4)
A Java class can implement several interfaces, whereas it can only extend one concrete class.
This is essential if you want flexibility over how you implement related services.
Interfaces also provide a higher level of encapsulation because you’re forced to put logic and state in the implementing class, not the interface.
You could stop at this point, assemble your final application by creating the various components with new, and wire their dependencies manually.
Or you could use a dependency injection framework to do the construction and wiring for you.
If you did, you’d have a pluggable application and all the benefits it entails, but you’d also miss out on two other benefits of a service-oriented approach: rich metadata and the ability to switch between implementations at execution time in response to events.
Interfaces alone can’t easily capture certain characteristics of a service, such as the quality of a particular implementation or configuration settings like supported locales.
Such details are often best recorded as metadata alongside the service interface, and to do this you need some kind of framework.
Semantics, which describe what a service does, are also hard to capture.
Simple semantics like pre- and post-conditions can be recorded using metadata or may even be enforced by the service framework.
Other semantics can only be properly described in documentation, but even here metadata can help provide a link to the relevant information.
Think about your current application: what characteristics may you want to record outside of classes and interfaces? To get you started, table 4.1 describes some characteristics from real-world services that could be recorded as metadata.
Figure 4.4 Programming to interfaces means teams can work in parallel.
As you can see, metadata can capture fine-grained information about your application in a structured way.
This is helpful when you’re assembling, supporting, and maintaining an application.
Recording metadata alongside a service interface also means you can be more exact about what you need.
The service framework can use this metadata to filter out services you don’t want, without having to load and access the service itself.
But why would you want to do this? Why not just call a method on the service to ask if it does what you need?
A single Java interface can have many implementations; one may be fast but use a lot of memory, another may be slow but conserve memory.
How do you know which one to use when they both implement the same interface? You could add a query method to the interface that tells you more about the underlying implementation, but that would lead to bloat and reduce maintainability.
Because service frameworks help you record metadata alongside services, they can also help you query and filter on this metadata when discovering services.
This is different from classic dependency injection frameworks, which look up implementations based solely on the interfaces used at a given dependency point.
Figure 4.5 shows how services can help you get exactly what you want.
We hope that, by now, you agree that services are a good thing—but as.
Supported locales A price-checking service may only be available for certain currencies.
Transaction cost You may want to use the cheapest service, even if it takes longer.
Throughput You may want to use the fastest service, regardless of cost.
Security You may only want to use services that are digitally signed by certain providers.
You may only want to use a service that guarantees to store your data in such a way that it won’t be lost if the JVM restarts.
The what, why, and when of services the saying goes, you can have too much of a good thing! How can you know when you should use a service or when it would be better to use another approach, such as a static factory method or simple dependency injection?
The best way to decide when to use a service is to consider the benefits: less coupling, programming to interfaces, additional metadata, and multiple implementations.
If you have a situation where any of these make sense or your current design provides similar benefits, you should use a service.
The most obvious place to use a service is between major components, especially if you want to replace or upgrade those components over time without having to rewrite other parts of the application.
Similarly, anywhere you look up and choose between implementations is another candidate for a service, because it means you can replace your custom logic with a standard, recognized approach.
Services can also be used as a substitute for the classic listener pattern.3 With this pattern, one object offers to send events to other objects, known as listeners.
The event source provides methods to subscribe and unsubscribe listeners and is responsible for maintaining the list of listeners.
Each listener implements a known interface to receive events and is responsible for subscribing to and unsubscribing from the event source (see figure 4.6)
Implementing the listener pattern involves writing a lot of code to manage and register listeners, but how can services help? You can see a service as a more general form of listener, because it can receive all kinds of requests, not just events.
Why not save time and get the service framework to manage listeners for you by registering them as services?
To find the current list of listeners, the sender queries the service framework for matching services (see figure 4.7)
You can use service metadata to further define and filter the interesting events for a listener.
In OSGi, this is known as the whiteboard pattern; you’ll use this pattern when you update the paint example to use services in section 4.4
One downside of the whiteboard pattern is that it may not be clear that listeners should register a particular interface with the registry, but you can solve this by highlighting the interface in the event source’s documentation.
It also introduces a dependency to the service framework, which you may not want for components that you want to reuse elsewhere.
Finally, the service registry must be able to scale to large numbers of services, for situations where you have lots of sources and listeners.
Another way to decide if you should use services is to consider when you wouldn’t want to use them.
That said, the overhead when calling a service in OSGi can be next to zero.
You may have a one-time start-up cost, but calling a service is then just a direct method call.
You should also consider the work required to define and maintain the service contract.
There’s no point in using a service between two tightly coupled pieces of code that are always developed and updated in tandem (unless of course you need to keep choosing between multiple implementations)
What if you’re still not sure whether to use a service? Fortunately, you can use an approach that makes development easier and helps you migrate to services later: programming to interfaces.
If you use interfaces, you’re already more than halfway to using services, especially if you also take advantage of dependency injection.
Of course, interfaces can be taken to extremes; there’s no point in creating an interface for a class if there will only ever be one implementation.
But for outward-facing interaction between components, it definitely makes sense to use interfaces wherever possible.
What have you learned? You saw how interfaces reduce coupling and promote faster development, regardless of whether you end up using services.
You also saw how services help capture and describe dependencies and how they can be used to switch between different implementations.
More importantly, you learned how a service-oriented approach makes developers think more about where work should be done, rather than lump code all in one place.
And finally, we went through a whole section about services without once mentioning remote or distributed systems.
Is OSGi just another service model? Should we end the chapter here with an overview of the API and move on to other topics? No, because one aspect is unique to the OSGi service model: services are completely dynamic.
What do we mean by dynamic? After a bundle has discovered and started using a service in OSGi, it can disappear at any time.
Perhaps the bundle providing it has been stopped or even uninstalled, or perhaps a piece of hardware has failed; whatever the reason, you should be prepared to cope with services coming and going over time.
This is different from many other service frameworks, where after you bind to a service it’s fixed and never changes—although it may throw a runtime exception to indicate a problem.
OSGi doesn’t try to hide this dynamism: if a bundle wants to stop providing a service, there’s little point in trying to hold it back or pretend the service still exists.
This is similar to many of the failure models used in distributed computing.
Hardware problems in particular should be acknowledged and dealt with promptly rather than ignored.
But before we can discuss the best way to handle dynamic services, you first need to understand how OSGi services work at the basic level, and to do that we need to introduce the registry.
The OSGi framework has a centralized service registry that follows a publish-findbind model (see figure 4.8)
You access the OSGi service registry through the BundleContext interface, which you saw in section 3.2.4
Back then, we looked at its lifecycle-related methods; now we’ll look into its service-related methods, as shown in the following listing.
As long as your bundle has a valid context (that is, when it’s active), it can use services.
Let’s see how easy it is to use a bundle’s BundleContext to publish a service.
Before you can publish a service, you need to describe it so others can find it.
In other words, you need to take details from the implemented contract and record them in the registry.
To publish a service in OSGi, you need to provide a single interface name (or an array of them), the service implementation, and an optional dictionary of metadata (see figure 4.9)
Here’s what you can use for a service that provides both stock listings and stock charts for the London Stock Exchange (LSE):
Note that metadata must be in the Dictionary type and can contain any Java type.
Figure 4.9 Publishing a service that provides both stock listings and stock charts.
When everything’s ready, you can publish your service by using the bundle context:
The registry returns a service registration object for the published service, which you can use to update the service metadata or to remove the service from the registry.
They shouldn’t be shared with other bundles, because they’re tied to the lifecycle of the publishing bundle.
It doesn’t need to extend or implement any specific OSGi types or use any annotations; it just has to match the provided service details.
You don’t even have to use interfaces if you don’t want to—OSGi will accept services registered under concrete class names, but this isn’t recommended.
After you’ve published a service, you can change its metadata at any time by using its service registration:
This makes it easy for your service to adapt to circumstances and inform consumers about any such changes by updating its metadata.
The only pieces of metadata that you can’t change are service.id and objectClass, which are maintained by the framework.
Other properties that have special meaning to the OSGi framework are shown in table 4.2
The publishing bundle can also remove a published service at any time:
Services are sorted by their ranking (highest first) and then by their ID (lowest first)
What happens if your bundle stops before you’ve removed all your published services? The framework keeps track of what you’ve registered, and any services that haven’t yet been removed when a bundle stops are automatically removed by the framework.
You don’t have to explicitly unregister a service when your bundle is stopped, although it’s prudent to unregister before cleaning up required resources.
Otherwise, someone could attempt to use the service while you’re trying to clean it up.
You’ve successfully published the service in only a few lines of code and without any use of OSGi types in the service implementation.
Now let’s see if it’s just as easy to discover and use the service.
As with publishing, you need to take details from the service contract to discover the right services in the registry.
The simplest query takes a single interface name, which is the main interface you expect to use as a consumer of the service:
This time the registry returns a service reference, which is an indirect reference to the discovered service (see figure 4.10)
This service reference can safely be shared with other bundles, because it isn’t tied to the lifecycle of the discovering bundle.
But why does the registry return an indirect reference and not the actual service implementation?
To make services fully dynamic, the registry must decouple the use of a service from its implementation.
By using an indirect reference, it can track and control access to the service, support laziness, and tell consumers when the service is removed.
If multiple services match the given query, the framework chooses what it considers to be the “best” services.
It determines the best service using the ranking property mentioned in table 4.2, where a larger numeric value denotes a higher-ranked service.
If multiple services have the same ranking, the framework chooses the service with the lowest service identifier, also covered in table 4.2
Because the service identifier is an increasing number assigned by the framework, lower identifiers are associated with older services.
So if multiple services have equal ranks, the framework effectively chooses the oldest service, which guarantees some stability and provides an affinity to existing services (see figure 4.11)
Figure 4.11 OSGi service ordering (by highest service.ranking and then lowest service.id)
A quick guide to using LDAP queries Perform attribute matching:
Here’s how you can find all stock listing services using the GBP currency:
You can also use the objectClass property, mentioned in table 4.2, to query for.
Here, you narrow the search to those stock listing services that use a currency of GBP and also provide a chart service:
You can look up all sorts of service references based on your needs, but how do you use them? You need to dereference each service reference to get the actual service object.
Before you can use a service, you must bind to the actual implementation from the registry, like this:
The implementation returned is typically exactly the same POJO instance previously registered with the registry, although the OSGi specification doesn’t prohibit the use of proxies or wrappers.
Each time you call getService(), the registry increments a usage count so it can keep track of who is using a particular service.
To be a good OSGi citizen, you should tell the registry when you’ve finished with a service:
You’ve now seen how to publish simple Java POJOs as OSGi services, how they can be discovered, and how the registry tracks their use.
But if you remember one thing from this section, it should be that services can disappear at any time.
If you want to write a robust OSGi-based application, you shouldn’t rely on services always being around or even appearing in a particular order when starting your application.
Of course, we don’t want to scare you with all this talk of dynamism.
It’s important to realize that dynamism isn’t created or generated by OSGi—it just enables it.
A service is never arbitrarily removed; either a bundle has decided to remove it or an agent has stopped a bundle.
You have control over how much dynamism you need to deal with, but it’s always good to code defensively in case things change in the future or your bundles are used in different scenarios.
Now you have enough knowledge to see what was happening behind the scenes:
This method is simple—probably too simple, as you’ll find out later in section 4.3.1
You use the BundleContext to find a service implementing the Package Admin Service interface.
This returns a service reference, which you use to get the service implementation.
Services aren’t proxies In general in OSGi, when you’re making method calls on a service, you’re holding a reference to the Java object supplied by the providing bundle.
For this reason, you should also remember to null variables referring to the service instance when you’re done using it, so it can be safely garbage collected.
The actual service implementation should generally never be stored in a long-lived variable such as a field; instead, you should try to access it temporarily via the service reference and expect that the service may go away at any time.
What’s the best way to cope with potential dynamism? How can you get the most from dynamic services without continual checking and rechecking? The next section discusses potential pitfalls and recommended approaches when you’re programming with dynamic services.
In the last section, we covered the basics of OSGi services, and you saw how easy it is to publish and discover services.
In this section, we’ll look more closely at the dynamics of services and techniques to help you write robust OSGi applications.
The Log Service is a standard OSGi service, one of the so-called compendium or noncore services.
For now, all you need to know is that the Log Service provides a simple logging facade, with various flavors of methods accepting a logging level and a message, as shown in the following listing.
With OSGi, you can use any number of possible Log Service implementations in the example, such as those written by OSGi framework vendors or others written by thirdparty bundle vendors.
To keep things simple and to help you trace what’s happening inside the framework, you’ll use your own dummy Log Service that implements only one method and outputs a variety of debug information about the bundles using it.
To keep these explanatory code snippets focused and to the point, they occasionally avoid using proper programming techniques such as encapsulation.
You should be able to join the dots between the patterns we show you in these examples and real-world OO design.
You picked up the basics of discovering services in section 4.2.2
In the following section, you’ll take that knowledge and use it to look up and call the Log Service; we’ll point out and help you solve potential problems as we go along.
When people start using OSGi, they often write code that looks similar to the following listing.
Because you store the Log Service instance in a field, the test code can be simple:
The Log Service implementation is stored directly in a field, which means the consumer won’t know when the service is retracted by its providing bundle.
It only finds out when the implementation starts throwing exceptions after removal, when the implementation becomes unstable.
This hard reference to the implementation also keeps it from being garbage collected while the bundle is active, even if the providing bundle is uninstalled.
To fix this, let’s replace the Log Service field with the indirect service reference, as shown in the following listing.
Listing 4.3 Broken lookup example—service instance stored in a field.
Listing 4.4 Broken lookup example—service is only discovered on startup.
You also need to change the test method to always dereference the service, as in the following listing.
This is slightly better, but there’s still a problem with the bundle activator.
You discover the Log Service only once in the start() method, so if there is no Log Service when the bundle starts, the reference is always null.
Similarly, if there is a Log Service at startup, but it subsequently disappears, the reference always returns null from that point onward.
Perhaps you want this one-off check, so you can revert to another (nonOSGi) logging approach based on what’s available at startup.
It would be much better if you could react to changes in the Log Service and always use the active one.
A simple way of reacting to potential service changes is to always look up the service just before you want to use it, as in the following listing.
With this change, the bundle activator becomes trivial and just records the context:
Unfortunately, you’re still not done, because there’s a problem in the test method—can you see what it is? Here’s a clue: remember that services can disappear at any time, and with a multithreaded application this can even happen between single statements.
The current code assumes that when you have a reference, you can safely dereference it immediately afterward.
This is a common mistake made when starting with OSGi and an example of what’s more generally known as a race condition in computing.
Let’s make the lookup more robust by adding a few more checks and a try-catch block, as in the following listing.
You react to changes in the Log Service and fall back to other logging methods when there are problems finding or using.
The getService() call returns null, and you end up not using any Log Service, even though a valid replacement is available.
This particular race condition can’t be solved by adding checks or loops because it’s an inherent problem with the two-stage “find-then-get” discovery process.
Instead, you must use another facility provided by the service layer to avoid this problem: service listeners.
The OSGi framework supports a simple but flexible listener API for service events.
We briefly discussed the listener pattern back in section 4.1.3: one object (in this case, the framework) offers to send events to other objects, known as listeners.
For services, there are currently three different types of event, shown in figure 4.13:
Every service listener must implement this interface in order to receive service events:
How can you use such an interface in the current example? You can use it to cache service instances on REGISTERED events and avoid the cost of repeatedly looking up the Log Service, as you did in section 4.3.1
A simple caching implementation may go something like the following listing.
It’s safe to call the getService() method during the REGISTERED event, because the framework delivers service events synchronously using the same thread.
This means you know the service won’t disappear, at least from the perspective of the framework, until the listener method returns.
Of course, the service could still throw a runtime exception at any time, but using getService() with a REGISTERED event always returns a valid service instance.
For the same reason, you should make sure the listener method is relatively short and won’t block or deadlock; otherwise, you block other service events from being processed.
You have the service listener, but how do you tell the framework about it? The answer is, as usual, via the bundle context, which defines methods to add and remove service listeners.
You must also choose an LDAP filter to restrict events to services implementing the Log Service; otherwise, you can end up receiving events for hundreds of different services.
The LDAP filter matches LogService instances, and you add a listener for future Log Service events.
Notice that you don’t explicitly remove the service listener when you stop the bundle.
This is because the framework keeps track of what listeners you’ve.
You saw something similar in section 4.2.1 when the framework removed any leftover service registrations.
The test method is now simple, because you’re caching the service instance:
This looks much better, doesn’t it? You don’t have to do as much checking or polling of the service registry.
Instead, you wait for the registry to tell you whenever a Log Service appears or disappears.
First, there are some minor issues with the test method; you don’t catch runtime exceptions when using the service; and because of the caching, you don’t unget the service when you’re not using it.
The cached Log Service could also change between the non-null test and when you use it.
More importantly, there’s a significant error in the listener code, because it doesn’t check that the UNREGISTERING service is the same as the Log Service currently being used.
Imagine that two Log Services (A and B) are available at the same time, where the test method uses Log Service A.
If Log Service B is unregistered, the listener will clear the cached instance even though Log Service A is still available.
Similarly, as new Log Services are registered, the listener will always choose the newest service regardless of whether it has a better service ranking.
To make sure you use the highestranked service and to be able to switch to alternative implementations whenever a service is removed, you must keep track of the current set of active service referencesnot just a single instance.
The bundle activator in listing 4.9 has another subtle error, which you may not have noticed at first.
This error may never show up in practice, depending on how you start your application.
Think back to how listeners work: the event source sends events to the listener as they occur.
What about events that happened in the past? What about already-published services? In this case, the service listener doesn’t receive events that happened in the dim and distant past and remains oblivious to existing Log Service implementations.
You have two problems to fix: you must keep track of the active set of Log Services and take into account already-registered Log Services.
The first problem requires the use of a sorted set and relies on the natural ordering of service references, as defined in the specification of the compareTo() method.
You’ll also add a helper method to decide which Log Service to pass to the client, based on the cached set of active service references; see the following listing.
Listing 4.10 Correct listener example—keeping track of active Log Services.
You deliberately lock the listener before passing it to the framework, so the pseudoregistration events are processed first.
Only when the listener has been added do you check for existing services, to make sure you don’t miss any intervening registrations.
You could potentially end up with duplicate registrations by doing the checks in this order, but that’s better than missing services.
The test method now only needs to call the helper method to get the best Log Service, as shown in the following listing.
You may have noticed that the finished listener example still doesn’t unget the service after using it; this is left as an exercise for you.
Here’s a hint to get you started: think about moving responsibility for logging into the listener.
This will also help you reduce the time between binding the service and using it.
Service listeners reduce the need to continually poll the service registry.
They let you react to changes in services as soon as they occur and get around the inherent race condition of the find-then-get approach.
The downside of listeners is the amount of code you need to write.
Imagine having to do this for every service you want to use and having to repeatedly test for synchronization issues.
Listing 4.12 Correct listener example—using the listener to get the best Log Service.
Dealing with dynamics utility class to do all this for you—a class that has been battle hardened and tested in many applications, that you can configure and customize as you require? It does, and the class’s name is ServiceTracker.
The OSGi ServiceTracker class provides a safe way for you to get the benefits of service listeners without the pain.
To show how easy it can be, let’s take the bundle activator from the last example and adapt it in the following listing to use the service tracker.
In this example, you use the basic ServiceTracker constructor that takes a bundle context, the service type you want to track, and a customizer object.
We’ll look at customizers in a moment; for now, you don’t need any customization, so you pass null.
If you need more control over what services are tracked, there’s another constructor that accepts a filter.
This is often the thing people forget to do when they first use a service tracker, and then they wonder why they don’t see any services.
Similarly, when you’re finished with the tracker, you must close it.
Although the framework automatically removes the service listener when the bundle stops, it’s best to explicitly call close() so that all the tracked resources can be properly cleared.
And that’s all you need to do to track instances of the Log Service—you don’t need to write your own listener or worry about managing long lists of references.
When you need to use the Log Service, you ask the tracker for the current instance:
Other tracker methods get all active instances and access the underlying service references; there’s even a method that helps you wait until a service appears.
Often, a raw service tracker is all you need, but sometimes you’ll want to extend it.
Perhaps you want to decorate a service with additional behavior, or you need to acquire or release resources as services appear and disappear.
You could extend the ServiceTracker class, but you’d have to be careful not to break the behavior of any methods you override.
Thankfully, there’s a way to extend a service tracker without subclassing it: with a customizer object.
Like a service listener, a customizer is based on the three major events in the life of a service: adding, modifying, and removing.
The addingService() method is where most of the customization normally occurs.
The associated tracker calls this whenever a matching service is added to the OSGi service registry.
You’re free to do whatever you want with the incoming service; you can initialize some resources or wrap it in another object, for example.
The object you return is tied to the service by the tracker and returned wherever the tracker would normally return the service instance.
If you decide you don’t want to track a particular service instance, return null.
The other two methods in the customizer are typically used for housekeeping tasks like updating or releasing resources.
Suppose you want to decorate the Log Service, such as adding some text around the log messages.
The service tracker customizer may look something like the following listing.
All you have to do to decorate the Log Service is pass the customizer to the tracker:
Now any Log Service returned by this tracker will add angle brackets to the logged message.
This is a trivial example, but we hope you can see how powerful customizers can be.
Service tracker customizers are especially useful in separating code from OSGispecific interfaces, because they act as a bridge connecting your application code to the service registry.
You’ve seen three different ways to access OSGi services: directly through the bundle context, reactively with service listeners, and indirectly using a service tracker.
Which way should you choose? If you only need to use a service intermittently and don’t mind using the raw OSGi API, using the bundle context is probably the best option.
At the other end of the spectrum, if you need full control over service dynamics and don’t mind the potential complexity, a service listener is best.
In all other situations, you should use a service tracker, because it helps you handle the dynamics of OSGi services with the least amount of effort.
Now that you know all about OSGi services and their dynamics, let’s look again at the paint program and see where it may make sense to use services.
You last saw the paint example back in section 3.4, where you used an extender pattern to collect shapes.
Why don’t you try using a service instead? A shape service makes a lot of sense, because you can clearly define what responsibilities belong to a.
What? No abstractions? If none of these options suit you, and you prefer to use a higher-level abstraction, such as components, this is fine too.
As we mentioned at the start of this chapter, it’s possible to build component models on top of these core APIs.
But remember, all these component frameworks make subtle but important semantic choices when mapping components to the OSGi service model.
If you need to cut through these abstractions and get to the real deal, now you know how.
Remember that the first thing to define when creating a new service is the contract.
Let’s use the previous interface as the basis of the new service contract—but this time, instead of extension names, you’ll declare service property names.
These names will tell the client where to find additional metadata about the shape:
This isn’t much different from the interface defined in section 3.4
You can see how easy it is to switch over to services when you’re programming with interfaces.
With this contract in hand, you now need to update each shape bundle to publish its implementation as a service, and update the paint frame bundle to track and consume these shape services.
Before you can publish a shape implementation as a service, you need a bundle context.
To get the bundle context, you need to add a bundle activator to each shape bundle, as shown in the following listing.
You record the name and icon under their correct service properties.
The shape bundles will now publish their shape services when they start and remove them when they stop.
To use these shapes when painting, you need to update the paint frame bundle so it uses services instead of bundles, as shown in figure 4.14
Remember the DefaultShape class that acted as a simple proxy to an underlying shape bundle in section 3.4? When the referenced shape bundle was active, the DefaultShape used its classes and resources to paint the shape.
When the shape bundle wasn’t active, the DefaultShape drew a placeholder image instead.
You can use the same approach with services, except that instead of a bundle identifier, you use a service reference as shown here:
This code gets the referenced shape service and draws a shape with a simple method call.
A placeholder image is drawn instead if there’s a problem.
You can also add a dispose() method to tell the framework when you’re finished with the service:
The new DefaultShape is now based on an underlying service reference, but how do you find such a reference? Remember the advice from section 4.3.3: you want to use several instances of the same service and react as they appear and disappear, but you don’t want detailed control—you need a ServiceTracker.
In the previous chapter, you used a BundleTracker to react as shape bundles came and went.
This proved to be a good design choice, because it meant the ShapeTracker class could process shape events and trigger the necessary Swing actions.
All you need to change is the source of shape events, as shown in the following listing; they now come from the ServiceTracker methods.
The processing code also needs to use service properties rather than extension metadata:
The fact that you needed to change only a few files is a testament to the non-intrusiveness of OSGi services if you already use an interface-based approach.
We hope you can also see how easy it would be to do this in reverse and adapt a service-based example to use extensions.
Imagine being able to decide when and where to use services in your application, without having to factor them into the initial design.
The OSGi service layer gives you that ability, and the previous layers help you manage and control it.
But how can the module and lifecycle layers help; how do they relate to the service layer?
The service layer builds on top of the module and lifecycle layers.
You’ve already seen one example, where the framework automatically unregisters services when their registering bundle stops.
But the layers interact in other ways, such as providing bundlespecific (also known as factory) services and specifying when you should unget and unregister services, and how you should bundle up services.
But let’s start with how modularity affects what services you can see.
Sometimes you may ask yourself this question and wonder why, even though the OSGi framework shows a particular service as registered, you can’t access it from your bundle.
Because multiple versions of service interface packages may be installed at any given time, the OSGi framework only shows your bundle services using the same version.
The reasoning behind this is that you should be able to cast service instances to any of their registered interfaces without causing a ClassCastException.
But what if you want to query all services, regardless of what interfaces you can see? Although this approach isn’t common, it’s useful in management scenarios where you want to track third-party services even if they aren’t compatible with your bundle.
This returns references to all services currently registered in the OSGi service registry.
Similarly, for service listeners there’s an All* extension of the ServiceListener interface, which lets you receive all matching service events.
The ServiceTracker is the odd one out, with no All* variant—to ignore visibility, you start the tracker with open(true)
You’ve seen that although one bundle can see a service, another bundle with different imports may not.
How about two bundles with the same imports? They see the same service instances.
What if you want them to see different instances—is it possible to customize services for each consumer?
You may have noticed that throughout this chapter, you’ve assumed that service instances are created first and then published, discovered, and finally used.
Or, to put it another way, creation of service instances isn’t related to their use.
But sometimes you want to create services lazily or customize a service specifically for each bundle using it.
An example is the simple Log Service implementation from section 4.3
None of the Log Service methods accept a bundle or bundle context, but you may want to record details of the bundle logging the message.
How is this possible in OSGi? Doesn’t the registerService() method expect a fully constructed service instance?
The OSGi framework defines a special interface to use when registering a service.
The ServiceFactory interface acts as a marker telling the OSGi framework to treat the provided instance not as a service, but as a factory that can create service instances on demand.
The OSGi service registry uses this factory to create instances just before they’re needed, when the consumer first attempts to use the service.
A factory can potentially create a number of instances at the same time, so it must be thread safe:
The framework caches factory-created service instances, so a bundle requesting the same service twice receives the same instance.
This cached instance is removed only when the bundle has completely finished with a service (that is, the number of calls to get it match the calls to unget it), when the bundle has stopped, or when the service factory is unregistered.
Should you always unget a service after you use it, like closing an I/O stream?
You just saw that instances created from service factories are cached until the consuming bundle has finished with the service.
This is determined by counting the calls to getService() compared to ungetService()
Forgetting to call unget can lead to instances being kept around until the bundle is stopped.
Similarly, agents interrogating the framework will assume the bundle is using the service when it isn’t.
Should you always unget after using a service, perhaps something like the following?
This code records exactly when you use the service, but what happens if you want to use it again and again in a short space of time? Services backed by factories will end up creating and destroying a new instance on every call, which can be costly.
You may also want to keep the instance alive between calls if it contains session-related data.
In these circumstances, it makes more sense to get at the start of the session and unget at the end of the session.
For long-lived sessions, you still need to track the service in case it’s removed, probably using a service tracker customizer to close the session.
In all other situations, you should unget the service when you’re finished with it.
But what about the other side of the equation? Should bundles let the framework unregister their services when they stop, or should they be more proactive and unregister services as soon as they don’t want to or can’t support them?
The OSGi framework does a lot of tidying up when a bundle stops—it removes listeners, releases used services, and unregisters published services.
Standard services don’t need to do anything yourself; indeed, many bundle activators have empty stop() methods.
Perhaps you’ve received a hardware notification and need to tell bundles not to use your service.
Perhaps you need to perform some processing before shutting down and don’t want bundles using your service while this is going on.
At times like this, remember that you’re in control, and it’s often better to be explicit than to rely on the framework to clean up after you.
After that salutary message, let’s finish this section with a modularity topic that has caused a lot of heated discussion on OSGi mailing lists: where to package service interfaces.
Should they be packaged separately in their own bundle or duplicated inside each implementation bundle? OSGi supports both options, because as long as the metadata is correct, it can wire the various bundles together to share the same interface.
But why would you want to copy the interface inside each implementation bundle? Surely that would lead to duplicated content.
Think about deploying a set of services into a framework.
If each service has both an API and an implementation bundle, that doubles the number of bundles to manage.
Putting the interface inside the implementation bundle means you need to provide only one JAR file.
Similarly, users don’t have to remember to install the API—if they have the implementation, they automatically get the API for free.
Putting interfaces inside an implementation bundle means the OSGi framework may decide to use that bundle as the provider of the API package.
If you then want to upgrade and refresh the implementation bundle, all the consuming bundles will end up being refreshed, causing a wave of restarting bundles.
Similarly, if you decide to uninstall the implementation, the implementation classes will be unloaded by the garbage collector only when the interface classes are no longer being used (because they share the same class loader)
Just as with other topics we’ve discussed—service visibility, service factories, and using unget and unregister—you need to know the possibilities to make an informed choice.
We’ll come back to this topic in the next chapter, because packaging service interfaces with the implementation bundle also requires you to define the bundle metadata a little differently.
Whatever you decide, we can all agree that services are an important feature of OSGi.
Services are such an important feature that they’re used throughout the OSGi specification.
By using services to extend the framework, the core API can be kept lean.
Almost all extensions to OSGi have been specified as optional add-on services without requiring any changes to the core specification.
These standard OSGi services are divided into two categories: core and compendium.
We’ll take a quick look at some of the core and compendium services in the next two subsections (see table 4.3)
The following core services are generally implemented by the OSGi framework itself, because they’re intimately tied to framework internals.
The OSGi Package Admin Service, which we discussed in chapter 3, provides a selection of methods to discover details about exported packages and the bundles that export and/or import them.
You can use this service to trace dependencies between bundles at execution time, which can help when upgrading because you can see what bundles may be affected by the update.
The Package Admin Service also provides methods to refresh exported packages, which may have been removed or updated since the last refresh, and to explicitly resolve specific bundles.
The OSGi Start Level Service lets you programmatically query and set the start level for individual bundles as well as the framework itself, which allows you to control the relative order of bundle activation.
You can use start levels to deploy an application or roll out a significant update in controlled stages.
Package Admin Core Manages bundle updates and discovers who exports what.
Start Level Core Queries and controls framework and bundle start levels.
Two OSGi services deal with permissions: the Permission Admin Service, which deals with permissions granted to specific bundles, and the Conditional Permission Admin Service, which provides a more general-purpose and fine-grained permission model based on conditions.
Both of these services build on the standard Java 2 security architecture.
You now know which core services you can expect to see in an OSGi framework, but what about the non-core compendium services? What sort of capabilities do they cover?
In addition to the core services, the OSGi Alliance defines a set of non-core standard services called the compendium services.
Whereas the core services are typically available by default in a running OSGi framework, the compendium services aren’t.
Keeping with the desire for modularity, you wouldn’t want them to be included by default because this would lead to bloated systems.
Instead, these services are provided as separate bundles by framework implementers or other third parties and typically work on all frameworks.
You’ve already seen one example of a compendium service: the Log Service from section 4.3, which provides a simple logging API.
The OSGi HTTP Service supports registration of servlets and resources under named aliases.
These aliases are matched against incoming URI requests, and the relevant servlet or resource is used to construct the reply.
You can authenticate incoming requests using standard HTTP/HTTPS, the OSGi User Admin Service, or your own custom approach.
Each event consists of a topic, which is basically a semistructured string, and a set of properties.
Event handlers are registered as services and can use metadata to describe which topics and properties they’re interested in.
Events can be sent synchronously or asynchronously and are delivered to matching event handlers by using the whiteboard.
Other types of OSGi events (like framework, bundle, service, and log events) are mapped and republished by the Event Admin Service implementation.
These so-called configuration targets accept configuration data in the form of a dictionary of properties.
Management bundles, which have been granted permission to configure services, can use the Configuration Admin Service to initialize and update configurations for other bundles.
Chapter 9 provides detailed examples of how to supply and consume configuration data.
The OSGi User Admin Service provides a role-based model for authentication (checking credentials) and authorization (checking access rights)
An authenticating bundle uses the User Admin Service to prepopulate the required roles, groups, and users along with identifying properties and credentials.
This bundle can query the User Admin Service at a later date to find users, check their credentials, and confirm their authorized roles.
It can then decide how to proceed based on the results of these checks.
This is a short sample of the compendium services; you can find a complete table in appendix B.
You can also read detailed specifications of each service in OSGi Service Platform Service Compendium.7
That was a lot of information to digest, so don’t worry if you got a bit woozy.
There’s a huge benefit to applying a service-oriented design to a purely local, single-JVM application, and we hope you get the opportunity to experience this in your next project.
In the preceding chapters, we covered a myriad of details about the three layers of the OSGi framework.
Instead, we focused on explaining the common specification features, best practices, and framework behavior necessary to get you started with OSGi technology.
Depending on the project, the aforementioned features and best practices may not be sufficient.
This can be especially true when it comes to legacy situations, where you’re not able to make sweeping changes to your existing code base.
Sometimes the clean theory of modularity conflicts with the messiness of reality, so occasionally compromises are needed to get things moving or to meet objectives.
In this chapter, we’ll investigate more aspects of OSGi’s module layer.
We’ll look into simple features, such as making imported packages a little more flexible, and into more complicated ones, such as splitting Java packages across multiple bundles or breaking a single bundle into pieces.
You probably won’t need to use most of the features described in this chapter as often as the preceding ones; if you do, you should review your design, because it may not be sufficiently modular.
With that said, it’s worthwhile to be aware of these advanced features of the OSGi module layer and the circumstances under which they’re useful.
To assist in this endeavor, we’ll introduce use cases and examples to help you understand when and how to apply them.
This chapter isn’t strictly necessary for understanding subsequent chapters, so feel free to postpone reading it until later.
From what you’ve learned so far, exporting a package from a bundle is fairly simple: include it in the Export-Package header, and potentially include some attributes.
In the following subsections, we’ll discuss other aspects, like importing exported packages, implicit attributes, mandatory attributes, class filtering, and duplicate exports.
In chapter 2, you learned how Export-Package exposes internal bundle classes and how Import-Package makes external classes visible to internal bundle classes.
This seems to be a nice division of labor between the two.
In other words, you may assume a bundle exporting a given package can’t import it also and vice versa.
In many module systems, this would be a reasonable assumption, but for OSGi it’s incorrect.
A bundle importing a package it exports is a special case in OSGi, but what exactly does it mean? The answer to this question is both philosophical and technical.
The original vision of the OSGi service platform was to create a lightweight execution environment where dynamically downloaded bundles collaborate.
In an effort to meet the “lightweight” aspect of this vision, these bundles collaborated by sharing direct references to service objects.
Using direct references means that bundles collaborate via normal method calls, which is lightweight.
As a byproduct of using direct references, bundles must share the Java class associated with shared service objects.
As you’ve learned, OSGi has code sharing covered in spades with Export-Package and Import-Package.
Still, there’s an issue lurking here, so let’s examine a collaboration scenario more closely.
As you now understand, in their respective metadata, bundle A will import package javax.servlet, and bundle B will export it.
Include a copy of package javax.servlet in its bundle JAR file, and also export it.
The answer is technical, so we’ll only briefly explain it.
To use a class, Java must load it into the JVM using a class loader.
The identity of a class at execution time is not only associated with its fully qualified class name, it’s also scoped by the class loader that loaded it.
The exact same class loaded by two different class loaders is loaded twice by the Java VM and is treated as two different and incompatible types.
This means if you try to cast an instance of one to the other, you receive a ClassCastException.
Combine this knowledge with the fact that the OSGi specification requires each bundle to have its own class loader for loading its classes, and you begin to understand the issue with the second option we described.
Bundle A can’t use both instances, because they come from different class loaders and are incompatible.
Due to this incompatibility, the OSGi framework only allows bundle A to see one copy, which means A can’t collaborate with both B and C at the same time.
Figure 5.1 If bundle C imports from B, both can share servlet instances with A because there’s only one copy of the Servlet class; but this creates a dependency for C on B.
It’s not important for you to completely understand these arcane details of Java class loaders, especially because OSGi technology tries to relieve you from having to worry about them in the first place.
This gets us to the crux of OSGi’s special case for allowing a bundle to import a package it exports.
The solution devised by the OSGi specification is to allow a bundle to both import and export the same package (see figure 5.3)
In this case, the bundle contains the given package and its associated classes, but it may not end up using its copy.
A bundle importing and exporting the same package is offering the OSGi framework a choice; it allows the framework to treat it as either an importer or an exporter of the package, whichever seems appropriate at the time the framework makes the decision.
Here’s another way to think about this: it defines a substitutable export, where the framework is free to substitute the bundle’s export with an imported package from another bundle.
Returning to the example, both bundles B and C can include a copy of package javax.servlet, with both importing and exporting it, knowing they’ll work independently or together.
Admittedly, this may seem odd; but as the discussion here illustrates, to simplify the OSGi vision of a collaborative environment, it’s necessary to make sure bundles use the same class definitions.
The R4 specification removed this implicitness, making it a requirement to have a separate Import-Package to get a substitutable export; but this didn’t lessen the importance of doing so in cases where collaboration is desired.
An interesting side effect of this is the possibility of metadata like this:
A bundle may include a copy of a given package at a specific version but may work with a lower range.
This can make the bundle useful in a wider range of scenarios, because it can still work in an environment where an older version of the package must be used.
Figure 5.3 B and C can both export and import the Servlet package, which makes it possible for the framework to choose to substitute packages so all bundles use a single class definition.
Unlike importing exported packages, which gives the framework resolver more flexibility when resolving imports, implicit export attributes can be used to limit the framework’s options for resolving an import.
Generally speaking, OSGi regards the same package exported by multiple bundles as being completely equivalent if the package versions are the same.
This is beneficial when it comes to dependency resolution, because it’s possible for the framework to satisfy an import for a given package from any available matching exporter.
In certain situations, you may not wish to have your bundle’s imports satisfied by an arbitrary bundle; instead, you may want to import from a specific bundle.
For example, perhaps you patched a bug in a common open source library, and you don’t want to risk using a nonpatched version.
When to import your exports The question on your mind probably is, “With all these benefits, shouldn’t I make all my exports substitutable?” Not necessarily.
If the packages in question are somehow coupled to private (non-exported) packages, or all packages are exported, you should only export them.
Conversely, if other bundles can reasonably expect to get the packages from a different provider, you may want to import and export them.
For example, if you’re embedding and exporting common open source packages, you may want to import them too, because other bundles may reasonably expect to get them from other providers; this is especially necessary if your other exported packages have uses constraints on the common packages.
Importing and exporting a package is also useful when you’re using an interfacebased development approach.
In interface-based programming, which is the foundation of the OSGi service approach, you assume there are potentially multiple implementations of well-known interfaces.
You may want to package the interfaces into their implementations to keep them self-contained.
In this case, to ensure interoperability, the bundles should import and export the interface packages.
Because the whole point of OSGi services is to foster collaboration among bundles, the choice between importing only or exporting and importing interface packages is fairly common.
You do have an alternative approach: to always package your collaborative interface packages into a separate bundle.
By never packaging your interfaces in a bundle that provides implementations, you can be sure all implementations can be used together, because all implementations will import the shared packages.
If you follow this approach, none of your implementations will be self-contained, because they’ll all have external dependencies on the shared packages.
The trade-off is deciding whether you want more bundles with dependencies among them or fewer self-contained bundles with some content overlap.
Additionally, the framework implicitly attaches the bundle’s symbolic name and version to all packages exported by a bundle.
Therefore, the previous metadata conceptually looks like this (also shown in figure 5.4):
These attributes can only be specified by the framework; explicitly specifying them results in an installation exception.
With these implicit attributes, it’s possible for you limit the framework’s resolution of an imported package to specific bundles.
For example, an importing bundle may contain the following snippet of metadata:
In this case, the importer limits its dependency resolution to a specific bundle by specifying its symbolic name with a precise version range.
As you can imagine, this makes the dependency a lot more brittle, but under certain circumstances this may be desired.
You may be thinking that implicit export attributes aren’t completely necessary to control how import dependencies are resolved.
You can also use good old arbitrary attributes to achieve the same effect—just make sure your attribute name and/or value are sufficiently unique.
For example, you can modify your exporting manifest like this:
In this case, the importer needs to specify the corresponding attribute name and value on its Import-Package declaration.
There’s an advantage to using this approach if you’re in a situation where you must have brittle dependencies: it’s not as brittle as implicit attributes.
You’re able to refactor your exporting bundle without impacting importing bundles, because these attribute values aren’t tied to the containing bundle.
On the downside, arbitrary attributes are easier for other bundles to imitate, even though there are no guarantees either way.
In short, it’s best to avoid brittle dependencies, but at least now you understand how both implicit and arbitrary export attributes allow importing bundles to have a say in how their dependencies are resolved.
Thinking about the flip side, it may also occasionally be necessary for exporting bundles to have some control over how importing bundles are resolved.
As we discussed in the last subsection, in some situations this isn’t desired.
Up until now, the importing bundle appears to be completely in control of this situation, because it declares the matching constraints for dependency resolution.
For example, consider the following metadata snippet for importing a package:
Such an import declaration matches any provider of javax.servlet as long as it’s in the specified version range.
Now consider the following metadata snippet for exporting a package in another bundle:
Will the imported package match this exported package? Yes, it will, as shown in figure 5.5
The name of the attribute, private, may have tricked you into thinking otherwise, but it’s just an arbitrary attribute and has no meaning (if it did have meaning to the framework, it would likely be a directive, not an attribute)
When it comes to matching an import to an export, only the attributes mentioned on the import declaration are compared against the attributes on the export declaration.
In this case, the import mentions the package name and version range, which match the exported package’s name and version.
In some situations, you may wish to have a little more control in your exporting bundle.
For example, maybe you’re exposing a package containing a nonpublic API, or you’ve modified a common open source library in an incompatible way, and you don’t want unaware bundles to inadvertently match your exported packages.
Figure 5.5 Only attributes mentioned in the imported package declaration impact dependency resolution matching.
Any attributes mentioned only in the exported package declaration are ignored.
Managing your exports mandatory export attributes, which you declare using the mandatory export package directive of the Export-Package manifest header.
To see how mandatory attributes work, let’s modify the previous snippet to export its package, like this:
You add the mandatory directive to the exported package to declare the private attribute as mandatory.
Any export attribute declared as mandatory places a constraint on importers.
If the importers don’t specify a matching value for the attribute, then they don’t match.
The export attribute can’t be ignored, as shown in figure 5.6
The need for mandatory attributes doesn’t arise often; you’ll see some other use cases in the coming sections.
Until then, let’s look into another more fine-grained mechanism that bundles can use to control what is exposed from their exported packages.
In chapter 1, we discussed the limitations of Java’s rules for code visibility.
There’s no way to declare module visibility, so you must use public visibility for classes accessed across packages.
This isn’t necessarily problematic if you can keep your public and private APIs in separate packages, because bundles have the ability to hide packages by not exporting them.
Unfortunately, this isn’t always possible, and in some cases you end up with a public implementation class inside a package exported by the bundle.
To cope with this situation, OSGi provides include and exclude export filtering directives for the Export-Package manifest header to enable fine-grained control over the classes exposed from your bundle’s exported packages.
EXCLUDE/INCLUDE DIRECTIVES The exclude and include export package directives specify a comma-delimited list of class names to exclude or include from the exported package, respectively.
In this hypothetical example, the utility class is part of the private API.
It’s included in this package due to dependencies on the service implementation.
Figure 5.6 If an export attribute is declared as mandatory, importing bundles must declare the attribute and matching value; otherwise, it won’t match when the framework resolves dependencies.
In general, you should avoid such scenarios, but the following metadata snippet illustrates how you can do this with export filtering:
This exported package behaves like any normal exported package as far as dependency resolution is concerned; but at execution time, the framework filters the Util class from the package so importers can’t access it, as shown in figure 5.7
A bundle attempting to load the Util class receives a “class not found” exception.
The value of the directive specifies only class names; the package name must not be specified, nor should the .class portion of the class file name.
In some cases, it may be easier to specify which classes are allowed instead of which ones are disallowed.
The default value for the include directive is *, and the default value for the exclude directive is an empty string.
You can also specify both the include and exclude directive for a given exported package.
A class is visible only if it’s matched by an entry in the include directive and not matched by any entry in the exclude directive.
You should definitely strive to separate your public and private APIs into different packages so these mechanisms aren’t needed, but they’re here to get you out of a tough spot when you need them.
Next, we’ll move on to another mechanism to help you manage your APIs.
A given bundle can see only one version of a given class while it executes.
In view of this, it’s not surprising to learn that bundles aren’t allowed to import the same package more than once.
What you may find surprising is that OSGi allows a bundle to export the same package more than once.
For example, the following snippet of metadata is perfectly valid:
How is it possible, you ask? The trick is that the bundle doesn’t contain two separate sets of classes for the two exported packages.
The bundle is masquerading the same set of classes as different packages.
Why would it do this? Expounding on the previous snippet, perhaps you have unmodifiable third-party bundles with explicit dependencies on javax.servlet version 2.3.0 in your application.
In the end, all bundles requiring either version of javax.servlet can resolve, but they’ll all use the same set of classes at execution time, as shown in figure 5.8
As with export filtering, this is another mechanism to manage your APIs.
You can take this further and combine it with some of the other mechanisms you’ve learned about in this section for additional API management techniques.
For example, you generally don’t want to expose your bundle’s implementation details to everyone, but sometimes you want to expose implementation details to select bundles; this is similar to the friend concept in C++
A friend is allowed to see implementation details, but nonfriends aren’t.
To achieve something like this in OSGi, you need to combine mandatory export attributes, export filtering, and duplicate exports.
To illustrate, let’s return to the example from export filtering:
In this example, you excluded the Util class, because it was an implementation detail.
For friends, you need to export the package without filtering:
Now you have one export hiding the Util class and one exposing it.
How do you control who gets access to what? That’s right: mandatory export attributes.
The following complete export metadata gives you what you need:
Only bundles that explicitly import your package with the friend attribute and matching value see the Util class.
Clearly, this isn’t a strong sense of friendship, because any bundle can specify the friend attribute; but at least it requires an opt-in.
Figure 5.8 A bundle can export the same package multiple times, but this is only a form of masquerading.
Only one set of classes exists for the package in the bundle.
Best practice dictates avoiding the friendship concept, because it weakens modularity.
If an API is valuable enough to export, you should consider making it a public API.
In this section, we’ve covered additional capabilities that OSGi provides for exporting packages to help you deal with various uncommon use cases.
In the next section, you’ll learn how to make importing packages a little more flexible, which again can provide some wiggle room when you’re trying to get legacy Java applications to work in an OSGi environment.
Explicitly declared imports are great, because explicit requirements allow you to more easily reuse code and automate dependency resolution.
This gives you the benefit of being able to detect misconfigurations early rather than receiving various class-loading and class-cast exceptions at execution time.
On the other hand, explicitly declared imports are somewhat constraining, because the framework uses them to strictly control whether your code can be used; if an imported package can’t be satisfied, the framework doesn’t allow the associated bundle to transition into the RESOLVED state.
Additionally, to import a package, you must know the name of a package in advance, but this isn’t always possible.
What can you do in these situations? The OSGi framework provides two different mechanisms for dealing with such situations: optional and dynamic imports.
Let’s look into how each of these can help, as well as compare and contrast them.
Sometimes a given package imported by a bundle isn’t strictly necessary for it to function properly.
Consider an imported package for a nonfunctional purpose, like logging.
The code in a given bundle may have been programmed to function properly regardless of whether a logging facility is available.
To express this, OSGi provides the resolution directive for the Import-Package manifest header to mark imported packages as optional.
The dependency on the logging package is optional, as indicated by the use of the resolution directive with the optional value.
All imported packages have a resolution associated with them, but the default value is mandatory.
We’ll look at how this is used in practice in section 5.2.4, but for now let’s consider the other tool in the box: dynamic imports.
Certain Java programming practices make it difficult to know all the packages that a bundle may need at execution time.
Often the name of the class implementing the JDBC driver is a configuration property or is supplied by the user at execution time.
Because your bundle can only see classes in packages it imports, how can it import an unknown package? This sort of situation arises when you deal with service provider interface (SPI) approaches, where a common interface is known in advance, but not the name of the class implementing it.
You may have expected from the previous examples that dynamic imports would be handled using an import directive rather than their own manifest header, but they’re sufficiently different to warrant a separate header.
In the most general sense, a dynamic import is expressed in the bundle metadata like this:
This snippet dynamically imports any package needed by the bundle.
When you use the wildcard at the end of a package name (for example, org.foo.*), it matches all subpackages recursively but doesn’t match the specified root package.
Given the open-ended nature of dynamic imports, it’s important to understand precisely when in the class search order of a bundle they’re employed.
Requests for classes in an imported package are delegated to the exporting bundle; searching stops with either a success or failure (section 2.5.4)
The bundle class path is searched for the class; searching stops if found but continues to the next step with a failure (section 2.5.4)
If the package in question isn’t exported by the bundle, requests matching any dynamically imported package are delegated to an exporting bundle if one is found; searching stops with either a success or failure.
As you can see, dynamic imports are attempted only as a last resort.
But when a dynamically imported package is resolved and associated with the importing bundle, it behaves just like a statically imported package.
You can also use dynamically imported packages in a fashion more similar to optionally imported packages by specifying additional attributes like normal imported packages.
In the first case, all subpackages of javax.servlet of version 2.4.0 are dynamically imported, whereas in the second only the explicitly mentioned packages are dynamically imported.
More precise declarations such as these often make less sense when you’re using dynamic imports, because the general use case is for unknown package names.
We apparently have two different ways to loosen bundle imports.
There are intended use cases for both optional and dynamic imports, but the functionality they provide overlaps.
To better understand each, we’ll look into their similarities and differences.
Both are used to express dependencies on packages that may or may not be available at execution time.
Although this is the specific use case for optional imports, it’s a byproduct of dynamic imports.
Only normally imported packages (that is, mandatory imported packages) impact bundle dependency resolution.
If a mandatory imported package can’t be satisfied, the bundle can’t be resolved or used.
Avoid the Siren’s song Dynamic imports are alluring to new OSGi programmers because they provide behavior similar to that in standard Java programming, where everything available on the class path is visible to the bundle.
Unfortunately, this approach isn’t modular and doesn’t allow the OSGi framework to verify whether dependencies are satisfied in advance of using a bundle.
As a result, dynamically importing packages should be seen as bad practice, except for explicitly dealing with legacy SPI approaches.
Loosening your imports packages are required to be present when resolving dependencies.
For optional imports, this is the whole point: they’re optional.
For dynamic imports, they aren’t necessarily optional; but because they aren’t known in advance, it’s not possible for the framework to enforce that they exist.
This is typically the sort of issue the OSGi framework tries to help you avoid with explicit dependencies; we shouldn’t be dealing with classloading issues as developers.
By now, you must be wondering what the difference is between optional and dynamic imports.
It has to do with when the framework tries to resolve the dependencies.
The framework attempts to resolve an optionally imported package once when the associated bundle is resolved.
If the import is satisfied, the bundle has access to the package.
If not, the bundle doesn’t and will never have access to the package unless it’s re-resolved.
For a dynamically imported package, the framework attempts to resolve it at execution time when the bundle’s executing code tries to use a class from the package.
Further, the framework keeps trying to resolve the dynamically imported package each time the bundle’s executing code tries to use classes from it until it’s successfully resolved.
If a bundle providing the dynamically imported package is ever deployed into the executing framework, the framework eventually will be able to resolve it.
After the resolve is successful, the bundle is wired to the provider of the package; it behaves like a normal import from that point forward.
Let’s look at how you can use these mechanisms in a logging example, which is often an optional activity for bundles.
The OSGi specification defines a simple logging service that you may want to use in your bundles, but you can’t be certain it will always be available.
One way to deal with this uncertainty is to create a simple proxy logger that uses the logging service if available or prints to standard output if not.
The proxy logger has a constructor that takes the BundleContext object to track log services, an init() method to create a ServiceTracker for log services, a close() method to stop tracking log services, and a log() method for logging messages.
Looking more closely at the init() method, you try to use the logging package to create a ServiceTracker B.
Because you’re optionally importing the logging package, you surround it in a try-catch block.
If an exception is thrown, you set your tracker to null; otherwise, you end up with a valid tracker.
When a message is logged, you check if you have a valid tracker C.
If so, you try to log to a log service.
Even if you have a valid tracker, that doesn’t mean you have a log service, which is why you verify it D.
If you have a log service, you use it; otherwise, you log to standard output.
The important point is that you attempt to probe for the log package only once, with a single call to init() from the constructor, because an optional import will never be satisfied later if it’s not satisfied already.
When the bundle is started, you create an instance of your proxy logger that’s used throughout the bundle for logging.
Although not shown here, the bundle passes a reference or somehow provides access to the logger instance to any internal code needing a logger at execution time.
When the bundle is stopped, you invoke close() on the proxy logger, which stops its internal service tracker, if necessary.
How would this example change if you wanted to treat the logging package as a dynamic import? The impact to the Logger class is as follows.
You can no longer make your ServiceTracker member variable final, because you don’t know when it will be created.
To make your proxy logger thread safe and avoid creating more than one ServiceTracker instance, you need to synchronize your entry methods B.
Because the logging package can appear at any time during execution, you try to create the ServiceTracker instance each time you log a message C until successful.
As before, if all else fails, you log to standard output.
These two examples illustrate the differences between these two mechanisms.
As you can see, if you plan to take advantage of the full, dynamic nature of dynamically imported packages, there’s added complexity with respect to threading and concurrency.
There’s also potential overhead associated with dynamic imports, not only because of the synchronization, but also because it can be costly for the framework to try to find a matching package at execution time.
For logging, which happens frequently, this cost can be great.
Optional imports are optional We should point out that you can use dynamic imports in a fashion similar to optional imports.
In this sense, the use of the optional import package mechanism is itself optional.
For example, you can modify the metadata of the optional logger example to be a dynamic import instead, but keep the code exactly the same.
If you did this, the two solutions would behave equivalently.
If this is the case, then why choose one over the other? There’s no real reason or recommendation for doing so.
Even though optional imports overlapped dynamic imports, they were added for consistency with bundle dependencies, which were also added in R4 and can also be declared as optional.
We’ve finished covering the advanced capabilities for importing packages, but there’s still a related concept provided by OSGi for declaring dependencies.
In some situations, such as legacy situations where modules are tightly coupled or contain a given package split across modules, importing a specific package isn’t sufficient.
For these situations, OSGi allows you to declare dependencies on specific bundles.
In section 5.1.2, we discussed how implicit export attributes allow bundles to import packages from a specific bundle.
The OSGi specification also supports a module-level dependency concept called a required bundle that provides a similar capability.
In chapter 2, we discussed a host of reasons why package-level dependencies are preferred over module-level dependencies, such as them being more flexible and fine-grained.
But there is one particular use case where requiring bundles may be necessary in OSGi: if you must deal with split packages.
In OSGi terms, it’s a package split across multiple bundles.
In standard Java programming, packages are generally treated as split; the Java class path approach merges all packages from different JAR files on the class path into one big soup.
This is anathema to OSGi’s modularization model, where packages are treated as atomic (that is, they can’t be split)
When migrating to OSGi from a world where split packages are common, we’re often forced to confront ugly situations.
But even in the OSGi world, over time a package may grow too large and reach a point where you can logically divide it into disjoint functionality for different clients.
Unfortunately, if you break up the existing package and assign new disjoint package names, you break all existing clients.
Splitting the package allows its disjoint functionality to be used independently; but for existing clients, you still need an aggregated view of the package.
This gives you an idea of what a split package is, but how does this relate to requiring bundles? This will become clearer after we discuss what it means to require a bundle and introduce a use case for doing so.
The big difference between importing a package and requiring a bundle is the scope of the dependency.
Whereas an imported package defines a dependency from a bundle to a specific package, a required bundle defines a dependency from a bundle to every package exported by a specific bundle.
To require a bundle, you use the Require-Bundle manifest header in the requiring bundle’s manifest file.
REQUIRE-BUNDLE This header consists of a comma-separated list of target bundle symbolic names on which a bundle depends, indicating the need to access all packages exported by the specifically mentioned target bundles.
You use the Require-Bundle header to specify a bundle dependency in a manifest, like this:
The framework tries to satisfy each required bundle; if it’s unable to do so, the bundle can’t be used.
The framework resolves the dependency by searching the installed bundles for ones matching the specified symbolic name and version range.
To a large degree, requiring bundles is just a brittle way to import packages, because it specifies who instead of what.
The significant difference is how it fits into the overall class search order for the bundle, which is as follows:
Requests for classes in an imported package are delegated to the exporting bundle; searching stops with either a success or failure (section 2.5.4)
Requests for classes in a package from a required bundle are delegated to the exporting bundle; searching stops if found but continues with the next required bundle or the next step with a failure.
The bundle class path is searched for the class; searching stops if found but continues to the next step with a failure (section 2.5.4)
If the package in question isn’t exported or required, requests matching any dynamically import package are delegated to an exporting bundle if one is found; searching stops with either a success or failure (section 5.2.2)
Packages from required bundles are searched only if the class wasn’t found in an imported package, which means imported packages override packages from required bundles.
Did you notice another important difference between imported packages and packages from required bundles in the search order? If a class in a package from a required bundle can’t be found, the search continues to the next required bundle in declared order or the bundle’s local class path.
This is how Require-Bundle supports split packages, which we’ll discuss in more detail in the next subsection.
First, let’s look at the remaining details of requiring bundles.
As we briefly mentioned in section 5.2.4, it’s also possible to optionally require a bundle using the resolution directive:
Figure 5.9 Requiring a bundle is similar to explicitly importing every package exported by the target bundle.
Requiring bundles your bundle attempts to use potentially missing classes.
It’s also possible to control downstream visibility of packages from a required bundle using the visibility directive, which can be specified as private by default or as reexport.
If a bundle contains this, any bundle requiring it also sees the packages from bundle A (they’re re-exported)
This mechanism isn’t very modular, and using it results in brittle systems with high coupling.
Now let’s return our attention to how Require-Bundle supports aggregating split packages.
Avoiding split packages is the recommended approach in OSGi, but occasionally you may run into a situation where you need to split a package across bundles.
Because class searching doesn’t stop when a class isn’t found for required bundles, you can use Require-Bundle to search for a class across a split package by requiring multiple bundles containing its different parts.
For example, assume you have a package org.foo.bar that’s split across bundles A and B.
Both bundles claim to export org.foo.bar, even though they each offer only half of it.
Yes, this is problematic, but we’ll ignore that for now and come back to it shortly.
Figure 5.10 When bundle B requires bundle A with reexport visibility, any packages exported from A become visible to any bundles requiring B.
Now, if you have another bundle that wants to use the entire org.foo.bar package, it has to require both bundles.
When code from bundle C attempts to load a class from the org.foo.bar package, it follows these steps:
If the request succeeds, the class is returned; but if it fails, the code goes to the next step.
If the request succeeds, the class is returned; but if it fails, the code goes to the next step.
It tries to load the class from bundle C’s local class path.
The last step allows org.foo.bar to be split across the required bundles as well as the requiring bundle.
Because searching continues across all required bundles, bundle C is able to use the whole package.
What about a bundle wanting to use only one half of the package? Instead of requiring both bundles, it can require just the bundle containing the portion it needs.
Sounds reasonable; but does this mean that after you split a package, you’re stuck with using bundle-level dependencies and can no longer use package-level dependencies? No, it doesn’t, but it does require some best practice recommendations.
If another bundle wants to use Import-Package to access the portion of the package contained in bundle B, it can do something like this:
This is similar to using Require-Bundle for the specific bundle.
If you add an arbitrary attribute to each exported split package—called split, for example—you can use it to indicate a part name instead.
This has the benefit of being a little more flexible, because if you later change which bundle contains which portion of the split package, it won’t break existing clients.
What about existing clients that were using Import-Package to access the entire org.foo.bar package? Is it still possible? It’s likely that existing client bundles are doing the following:
Will they see the entire package if it’s now split across multiple bundles? No.
How can the framework resolve this dependency? The framework has no understanding of split packages as far as dependency resolution is concerned.
Requiring bundles installed and another bundle comes along with the above import declaration, the framework treats A and B as both being candidates to resolve dependency.
It chooses one following the normal rules of priority for multiple matching candidates.
Clearly, no matter which candidate it chooses, the resulting solution will be incorrect.
To avoid such situations, you need to ensure that your split package portions aren’t accidentally used by the framework to resolve an import for the entire package.
Likewise for bundle B, but with split equal to part2
Now for a bundle to import either part of the split package, they must explicitly mention the part they wish to use.
But what about an existing client bundle wanting to import the whole package? Because its import doesn’t specify the mandatory attribute, it can’t be resolved.
You need some way to reconstruct the whole package and make it available for importing; OSGi allows you to create a facade bundle for such a purpose.
To make bundle C a facade bundle, you change its metadata to be.
The only change is the last line where bundle C exports org.foo.bar, which is another form of re-exporting a package.
In this case, it aggregates the split package by requiring the bundles containing the different parts, and it re-exports the package without the mandatory attribute.
Now any client importing org.foo.bar will be able to resolve to bundle C and have access to the whole package.
Admittedly, this isn’t the most intuitive or straightforward way to deal with split packages.
This approach wasn’t intended to make them easy to use, because they’re best avoided; but it does make it possible in those situations where you have no choice.
Summarizing split package best practices In short, if you must use a split package, make sure you follow these steps:
Always export split packages with a mandatory attribute to avoid unsuspecting bundles from using them.
Use either Require-Bundle or Import-Package plus the mandatory attribute to access parts of the split packages.
To provide access to the whole package, create a facade bundle that requires the bundles containing the package parts and exports the package in question.
Despite these dire-sounding warnings, OSGi provides another way of dealing with split packages, called bundle fragments.
We’ll talk about those shortly, but first we’ll discuss some of the issues surrounding bundle dependencies and split packages.
Using Import-Package and Export-Package is the preferred way to share code because they couple the importer and exporter to a lesser degree.
Using RequireBundle entails much higher coupling and lower cohesion between the importer and the exporter and suffers from other issues, such as the following:
Mutable exports—Requiring bundles are impacted by changes to the exports of the required bundle, which introduce another form of breaking change to consider.
Such changes aren’t always easily apparent because the use of reexport visibility can result in chains of required bundles where removal of an export in upstream required bundles breaks all downstream requiring bundles.
Shadowing—Because class searching continues across all required bundles and the requiring bundle’s class path, it’s possible for content in some required bundles to shadow other required bundle content and the content of the requiring bundle itself.
The implications of this aren’t always obvious, especially if some bundles are optionally required.
Ordering—If a package is split across multiple bundles, but they contain overlapping classes, the declared order of the Require-Bundle header is significant.
All bundles requiring the bundles with overlapping content must declare them in the same order, or their view of the package will be different.
Completeness—Even though it’s possible to aggregate split packages using a facade bundle, the framework has no way to verify whether an aggregated package is complete.
Restricted access—An aggregated split package isn’t completely equivalent to the unsplit package.
Each portion of the split package is loaded by its containing bundle’s class loader.
In Java, classes loaded by different class loaders can’t access package-private members and types, even if they’re in the same package.
This is by no means an exhaustive list of issues, but it gives you some ideas of what to look out for when using Require-Bundle and (we hope) dissuades you from using it too much.
So far, we’ve introduced you to some of the more advanced features of managing OSGi dependencies, including importing exports, implicit export attributes, mandatory export attributes, export filtering, duplicate exports, optional and dynamic imports, and requiring bundles.
These tools allow you to solve some of the more complex edge cases found when migrating a classic Java application to an OSGi environment.
That must be it—we must have covered every possible mechanism of specifying dependencies, right? Not quite.
Dividing bundles into fragments curve ball to be thrown into the mix: bundle fragments.
Fragments are another way to deal with split packages by allowing the content of a bundle to be split across multiple, subordinate bundle JAR files.
Although splitting packages isn’t a good idea, occasionally it does make sense, such as with Java localization.
When a program wants to convert information into the user’s preferred locale, it uses a resource bundle to do so.
A ResourceBundle is created by loading a class or resource from a class loader using a base name, which ultimately defines the package containing the class or resource for the ResourceBundle.
This approach means you typically package many localizations for different locales into the same Java package.
If you have lots of localizations or lots of information to localize, packaging all your localizations into the same OSGi bundle can result in a large deployment unit.
Additionally, you can’t introduce new localizations or fix mistakes in existing ones without releasing a new version of the bundle.
It would be nice to keep localizations separate; but unlike the split package support of Require-Bundle, these split packages generally aren’t useful without the bundle to which they belong.
OSGi provides another approach to managing these sorts of dependencies through bundle fragments.
We’ll come back to localization shortly when we present a more in-depth example, but first we’ll discuss what fragments are and what you can do with them.
If you recall the modularity discussion in chapter 2, you know there’s a difference between logical modularity and physical modularity.
Normally, in OSGi, a logical module and a physical module are treated as the same thing; a bundle is a physical module as a JAR file, but it’s also the logical module at execution time forming an explicit visibility encapsulation boundary.
Through fragments, OSGi allows you to break a single logical module across multiple physical modules.
This means you can split a single logical bundle across multiple bundle JAR files.
Breaking a bundle into pieces doesn’t result in a handful of peer bundles; instead, you define one host bundle and one or more subordinate fragment bundles.
A host bundle is technically usable without fragments, but the fragments aren’t usable without a host.
Fragments are treated as optional host-bundle dependencies by the OSGi framework.
But the host bundle isn’t aware of its fragments, because it’s the fragments that declare a dependency on the host using the Fragment-Host manifest header.
FRAGMENT-HOST This header specifies the single symbolic name of the host bundle on which the fragment depends, along with an optional bundle version range.
A fragment bundle uses the Fragment-Host manifest header like this:
The Fragment-Host header is somewhat confusingly named, because it seems to be declaring the bundle as a host; it should be read as “require fragment host.” Although this header value follows the common OSGi syntax, you can’t specify multiple symbolic names.
A fragment is limited to belonging to one host bundle, although it may be applicable to a range of host versions.
Note that you don’t need to do anything special to define a bundle as a host; any bundle without a Fragment-Host header is a potential host bundle.
Likewise, any bundle with a Fragment-Host header is a fragment.
You now understand the relationship between a host and its fragments, but how do they work together? When the framework resolves a bundle, it searches the installed bundles to see if there are any fragments for the bundle being resolved.
If so, it merges the fragments into the host bundle.
Physically—The content and metadata from the fragments are conceptually merged with the host’s content and metadata.
Logically—Rather than giving each fragment its own class loader, the framework attaches the fragment content to the host’s class loader.
The first form of merging recombines the split physical pieces of the logical bundle, and the second form creates a single logical bundle because OSGi uses a single class loader per logical bundle to achieve encapsulation.
Returning to the discussion about resolving a bundle, if the bundle being resolved has fragments, the framework merges their metadata with the host’s and resolves the bundle as normal.
Figure 5.11 illustrates the before- and after effects of the merging process.
Fragments and package-private access As a technical side note, Java only allows package-private access to classes loaded by the same class loader.
Two classes in the same package, but loaded by two different class loaders, can’t access each others’ package-private members.
By loading fragment classes with the host’s class loader, the fragment classes are properly recombined to avoid this issue.
This isn’t always important, but the distinction between these two forms of support for split packages is worth understanding.
In addition to merging the exported and imported packages and required bundles, the bundle class paths are also merged.
This impacts the overall class search order for the bundle, like this:
Requests for classes in an imported package are delegated to the exporting bundle; searching stops (section 2.5.4)
Requests for classes in a package from a required bundle are delegated to the exporting bundle; searching continues with a failure (section 5.3.1)
The host bundle class path is searched for the class; searching continues with a failure (section 2.5.4)
Fragment bundle class paths are searched for the class in the order in which the fragments were installed.
Searching stops if found but continues through all fragment class paths and then to the next step with a failure.
If the package in question isn’t exported or required, requests matching any dynamically import package are delegated to an exporting bundle if one is found.
Searching stops with either a success or a failure (section 5.2.2)
This is the complete bundle class search order, so you may want to mark this page for future reference! This search order makes it clear how fragments support split packages, because the host and all fragment class paths are searched until the class is found.
Fragments and the Bundle-ClassPath The bundle class path search order seems fairly linear, but fragments do introduce one exception.
When specifying a bundle class path, you can specify embedded JAR files, such as.
If fragments aren’t involved, the framework ignores a non-existent JAR file on the bundle class path.
But if the bundle has fragments attached to it, the framework searches the fragments for the specified JAR files if they don’t exist in the host bundle.
In the example, imagine that the host contains default.jar but doesn’t contain specialized.jar, which is contained in an attached fragment.
The effect this has on the class search order is that the specified fragment content is searched before some of the host bundle content.
Sometimes this is useful if you want to provide default functionality in the host bundle but be able to override it on platforms where you have specialized classes (for example, using native code)
You can also use this approach to provide a means for issuing patches to bundles after the fact, but in general it’s better to update the whole bundle.
Some final issues regarding fragments: Fragments are allowed to have any metadata a normal bundle can have except Bundle-Activator.
This makes sense because fragments can’t be started or stopped and can only be used when combined with the host bundle.
Attaching a fragment to a host creates a dependency between the two, which is similar to the dependencies created between two bundles via Import-Package or Require-Bundle.
This means if either bundle is updated or uninstalled, the other bundle is impacted, and any refreshing of the one will likely lead to refreshing of the other.
We started this foray into fragments discussing localization, because it’s the main use case for them.
Next, we’ll look at an example of how to use fragments for this purpose.
The main application window is implemented by the PaintFrame class.
Recall its design: PaintFrame doesn’t have any direct dependencies on the OSGi API.
Instead, it uses a ShapeTracker class to track SimpleShape services in the OSGi service registry and inject them into the PaintFrame instance.
ShapeTracker injects services into the PaintFrame using its addShape() method, as shown in the following listing.
The addShape() method is invoked with the name, icon, and service object of the SimpleShape implementation.
The exact details aren’t important, but the shape is recorded in a data structure, a button is created for it, its name is set as the button’s tool tip, and, after a few other steps, the associated button is added to the toolbar.
The tool tip is textual information displayed to users when they hover the mouse over the shape’s toolbar icon.
It would be nice if this information could be localized.
You can take different approaches to localize the shape name.
One approach is to define a new service property that defines the ResourceBundle base name.
This way, shape implementations can define their localization base name, much as they use service properties to indicate the name and icon.
Listing 5.4 Method used to inject shapes into PaintFrame object.
Another approach is to focus on where the shape’s name is set in the first place: in the shape implementation’s bundle activator.
The following listing shows the activator of the circle implementation.
The hardcoded shape name is assigned to the service property dictionary, and the shape service is registered.
The first thing you need to do is change the hardcoded name into a lookup from a ResourceBundle.
You modify the activator to look up the shape name using the key constant defined at B from a ResourceBundle you create C, whose resulting value is assigned to the service properties.
Even though we won’t go into the complete details of using ResourceBundle objects, the important part in this example is when you define it.
Listing 5.6 Modified circle bundle activator with ResourceBundle name lookup.
If the example was more complicated, you’d have many more default mappings for other terms.
To provide other mappings to other languages, you need to include them in this same package, but in separate property files named after the locales’ country codes.
You do this for each locale you want to support.
Then, at execution time, when you create your ResourceBundle, the correct property file is automatically selected based on the locale of the user’s computer.
This all sounds nice; but if you have a lot of information to localize, you need to include all this information in your bundle, which can greatly increase its size.
Further, you have no way of adding support for new locales without releasing a new version of your bundle.
This is where fragments can help, because you can split the resource package into different fragments.
You keep the default localization in your circle implementation, but all other localizations are put into separate fragments.
You don’t need to change the metadata of your circle bundle, because it’s unaware of fragments, but the content of your circle bundle becomes.
For this example, you’ll create a German localization fragment bundle for the circle using the property file shown earlier.
The important part of this metadata is the last line, which declares it as a fragment of the circle bundle.
It only contains a resource file for the German translation, which you can see is a split package with the host bundle.
You can create any number of localization fragments following this same pattern for your other shapes (square and triangle)
Figure 5.12 shows the paint program with the German localization fragments installed.
With this approach, you only need to deploy the required localization fragments along with your shape implementations, and you can create new localizations or update existing ones without releasing new versions of the shape bundles.
We’ve now covered all major aspects of the OSGi module layer! As you can see, tools are available to help you deal with virtually any scenario the Java language can throw at you.
But we have one more trick up our sleeves: the OSGi specification does a pretty good job of dealing with native code that runs outside of the Java environment.
We’ll look at this and how to deal with general factors relating to the JVM environment in the next and final section of this chapter.
Although Java has been fairly successful at attaining its original vision of “write once, run everywhere,” there are still situations where it’s not entirely able to achieve this goal.
One such situation is the myriad of Java platforms, such as Java ME and the different versions of Java SE.
Another situation is if you need to natively integrate with the underlying operating system, as may be necessary if you must communicate directly with underlying hardware.
As you may expect, in both these situations the OSGi specification provides mechanisms to explicitly declare these scenarios in your bundles to allow an OSGi framework to do whatever is necessary at execution time.
In this section, we’ll cover both of these topics, starting with the former.
If you develop a bundle with a dependency on specific Java execution environments, what happens if this bundle executes in an unintended environment? Most likely, you’ll get various exceptions for missing classes or methods and/or faulty results.
If you have a bundle with specific execution environment requirements, you must explicitly declare these requirements to avoid people unknowingly trying to use your bundle in invalid environments.
The OSGi specification defines an execution environment concept for just this purpose.
Like all bundle metadata, you use a manifest header to define it.
The OSGi specification defines standard values for the common execution environments; table 5.1 lists them.
Bundles should list all known execution environments on which they can run, which may look something like this:
This specific example indicates that the bundle runs only on modern Java platforms.
The framework doesn’t verify this claim; it only ensures that the bundle isn’t resolvable on incompatible execution environments.
OSGi/Minimum-1.2 Minimal required set of Java API that allows an OSGi framework implementation.
A given framework implementation can claim to provide more than one execution environment, because in most cases the Java platform versions are backward compatible.
Now that you understand how to declare your bundles’ required execution environments, let’s look at how to handle bundles with native code.
Java provides a nice platform and language combination, but it’s not always possible to stay purely in Java.
In some situations, you need to create native code for the platform on which Java is running.
Java defined Java Native Interface (JNI) precisely for this purpose; JNI is how Java code calls code written in other programming languages (such as C or C++) for specific operating systems (such as Windows or Linux)
A complete discussion of how JNI works is outside the scope of this book, but the following list provides the highlights:
Native code is integrated into Java as a special type of method implementation.
A Java class can declare a method as having a native implementation using the native method modifier.
But after compilation, the javah command is used to generate C header and stub files, which are used to create the native method implementations.
The native code is compiled into a library in a platform-specific way for its target operating system.
The original Java class with the native method includes code to invoke System.
Other classes can invoke the native method as if it were a normal method, and the Java platform takes care of the native invocation details.
Although it’s fairly straightforward to use native code in Java, it’s best to avoid it if possible.
Native code doesn’t benefit from the garbage collector and suffers from the typical pointer issues associated with all native code.
Resolve-time, not install-time enforcement Pay special attention to the previous sentence.
It’s possible to install a bundle on a given execution environment even if it’s not compatible with it, but you won’t be able to resolve it unless its required execution environment matches the current one.
This is tied to the bundle’s resolved state because it’s possible for the execution environment to change over time.
For example, you may switch between different versions of Java on subsequent executions of the framework.
This way, any cached bundles not matching the current execution environment will essentially be ignored.
Still, in those cases where it’s absolutely necessary, it’s nice to know that OSGi supports it.
One of the downsides of native code is the fact that you end up with an additional artifact to deploy along with your classes.
To make matters worse, what you need to do with the native library differs among operating systems; for example, you typically need to put native libraries in specific locations in the file system so they can be found at execution time (for example, somewhere on the binary search path)
When you embed a native library into your bundle, you must tell the OSGi framework about it.
As with all other modularity aspects, you do so in the bundle metadata using the Bundle-NativeCode manifest header.
With this header, you can specify the set of contained native libraries for each platform your bundle supports.
For example, if you have a bundle with native libraries for Windows XP, you may have a native code declaration like this one:
In this case, you state the bundle has two native libraries for Windows XP on the x86 architecture.
If Bundle-NativeCode is specified, there must be a matching header for the platform on which the bundle is executing; otherwise, the framework won’t allow the bundle to resolve.
In other words, if a bundle with the previous native code header was installed on a Linux box, the framework won’t allow the bundle to be used.
If these same libraries also work on Vista, you can specify this as follows:
In cases where the parameter is repeated, the framework treats this like a logical OR when matching, so these native libraries match Windows XP or Windows Vista.
If your bundle also has native libraries for Linux, you can specify that as follows:
You separate different platforms using a comma instead of a semicolon.
Notice also that the native libraries don’t need to be parallel.
In this example, you have two native libraries for the Windows platform but only one for Linux.
This bundle is now usable on Windows XP, Windows Vista, and Linux on the x86 architecture, but on any other platform the framework won’t resolve it.
In some cases, you may have either optional native libraries or a non-optimized Java implementation for unsupported platforms.
You can denote this using the optional clause, like this:
The * at the end acts as a separate platform clause that can match any platform, so this bundle is usable on any platform.
Even though it isn’t often necessary, it’s fairly easy to use this mechanism to create bundles with native code.
That’s it! You’ve now been introduced to many of the specialized features of the OSGi module layer.
You learned that the OSGi module layer provides many additional mechanisms to deal with collaborative, dynamic, and legacy situations, such as the following:
A bundle may import a package it exports to make its export substitutable for purposes of broadening collaboration among bundles.
Exported packages have bundle symbolic name and version attributes implicitly attached to them, which can be useful if you need to import a package from a specific bundle.
Exported packages may have mandatory attributes associated with them, which must be specified by an importer for it to be wired to the exported package.
The framework ignores optionally imported packages if no exporters are present when the importing bundle is resolved.
The framework only attempts to resolve dynamically imported packages when the importing bundle tries to use a class in the dynamically imported package.
Repeated attempts to use a class in the dynamically imported package will result in repeated attempts to resolve the package until successful.
It’s possible to require a bundle, rather than importing specific packages, which wires you to everything the target bundle exports.
Bundle fragments support splitting a bundle into multiple optional JAR files, which is helpful in such situations as localization.
Bundles with dependencies on specific Java platforms can declare these dependencies with required execution environments.
Most of these mechanisms are intended for specific use cases and shouldn’t be overused to avoid less modular solutions.
We’ve now covered all the major functionality of OSGi specification, which closes out this part of the book.
In the next section, we’ll look into more practical matters that crop up while trying to use and build applications on top of OSGi technology.
In the first part of the book, we focused on the details and theory behind the OSGi specifications, which can be a little daunting when you’re first getting started.
In this second part, you’ll put your newfound OSGi knowledge into practice.
We’ll look at approaches for converting JAR files into bundles.
After that, we’ll explore how to test and debug bundles using tried-and-true techniques.
We’ll finish by explaining how to manage different aspects of bundles and OSGibased applications, such as versioning, configuring, and deploying them.
Upon completing this part of the book, you should have all the knowledge you need to successfully use OSGi technology in your own projects.
After that, we’ll examine different ways of migrating a complete application to OSGi and finish up with a short discussion of situations where you might decide not to bundle.
By the end of this chapter, you’ll know how to take your current application and all of its third-party libraries and turn them into bundles, step by step.
You’ll be able to move existing projects to OSGi, plan new projects with OSGi in mind, and understand when it may not be the right solution for you.
In other words, you should be able to explain in detail to your manager and co-workers how OSGi will affect your project.
But before we reach that stage, we first need to consider a simple question that often comes up on the OSGi mailing lists: how can you turn your JAR file into a bundle? Moving toward bundles.
The first part of this book introduced the three layers of OSGi: module, lifecycle, and service.
We’ll now take a more practical look at how you can migrate existing.
As you saw in chapter 2, a bundle is a JAR file with additional metadata.
So to turn a JAR file into a bundle, you need to add metadata giving it a unique identity and describing what it imports and exports.
Simple, right? For most business-domain JAR files, it is; but for others (such as third-party GUI or database libraries), you’ll need to think carefully about their design.
Where is the line between what’s public and what’s private, which imports are required and which are optional, and which versions are compatible with one another?
In this section, we’ll help you come up with this metadata by taking a series of common library JAR files and turning them into working bundles.
We’ll also consider some advanced bundling techniques, such as embedding dependencies inside the bundle, as well as how to manage external resources and background threads.
Before you can even load a bundle into an OSGi framework, it must have an identity.
This identity should be unique, at least among the set of bundles loaded into the framework.
But how should you choose such an identity? If you pick names at random, you may clash with other projects or other developers, as shown in figure 6.1
We’ll look into approaches for defining both of these now.
One of the first steps in turning a JAR file into a bundle is to decide what symbolic name to give it.
The OSGi specification doesn’t mandate a naming policy but recommends a reverse-domain naming convention.
Let’s look at a real-world example, the kXML parser (http://kxml.sourceforge.net/)
If this JAR was the only one expected to provide the org.xmlpull.v1 API, or if it only contained this package, it would be reasonable to use this as the symbolic name.
But this JAR file also provides a particular implementation of the XmlPull API, so it makes more sense to use the name of the implementation as the symbolic name because it captures the essence of what the bundle provides:
Alternatively, you can use the domain of the project that distributes the JAR file.
Or if Maven (http://maven.apache.org/) project metadata is available, you can use the Maven groupId + artifactId to identify the JAR file:
Sometimes, you may decide on a name that doesn’t correspond to a particular package or distribution.
For example, consider two implementations of the same service API provided by two different bundles.
OSGi lets you hide non-exported packages, so these bundles can have an identical package layout but at the same time provide different implementations.
You can still base the symbolic name on the main top-level package or the distribution domain, but you must add a suffix to ensure that each implementation has a unique identity.
If you’re wrapping a third-party library, you may want to prefix your own domain in front of the symbolic name.
This makes it clear that you’re responsible for the bundle metadata rather than the original third party.
Don’t worry too much about naming bundles—in the end, you need to give each bundle a unique enough name for your target deployment.
You’re free to rename your bundle later if you wish, because by default the framework wires import packages to export packages regardless of bundle symbolic names.
It’s only when someone uses Require-Bundle (see section 5.3) that consistent names become important.
That’s another reason why package dependencies are preferred over module dependencies: they don’t tie you to a particular symbolic name forever.
After you’ve decided on a symbolic name, the next step is to version your bundle.
Determining the Bundle-Version is more straightforward than choosing the symbolic name, because pretty much every JAR file distribution is already identified by some sort of build version or release tag.
Table 6.1 shows some actual project versions and attempts to map them to OSGi.
Not every version is easily converted to the OSGi format.
Look at the last example in the table; it starts with a number, but this is part of the date rather than the major version.
This is the problem with free-form version strings—there’s no standard way of comparing them or breaking them into component parts.
OSGi versions, on the other hand, have standardized structure and well-defined ordering.
Later, you’ll use a tool called bnd that makes a good attempt at automated mapping based on common-sense rules, but even bnd has its limits.
After you’ve uniquely identified your bundle by name and version, you can add more information: a human-friendly Bundle-Name, a more detailed BundleDescription, license details, vendor details, a link to online documentation, and so on.
Remember that new OSGi bundles should also have this header:
This tells the OSGi framework to process your bundle according to the latest specification.
Although this isn’t mandatory, it’s strongly recommended because it enables.
Turning JARs into bundles additional checks and support for advanced modularity features offered by OSGi R4 specifications and beyond.
After you’ve captured enough bundle details to satisfactorily describe your JAR file, the next thing to decide is which packages it should export to other bundles in the framework.
Most bundles export at least one package, but a bundle doesn’t have to export any.
Bundles providing service implementations via the service registry don’t have to export any packages if they import their service API from another bundle.
This is because their implementation is shared indirectly via the service registry and accessed using the shared API, as illustrated in figure 6.2
But what about the package containing the Bundle-Activator class? Doesn’t that need to be exported? No, you don’t need to export the package containing the bundle activator unless you want to share it with other bundles.
As long as the activator class has a public modifier, the framework can load it, even if it belongs to an internal, nonexported package.
The question remains: when is it necessary for you to export packages, and which packages in your JAR file do you need to export?
The classic, non-OSGi approach is to export everything and make the entire contents of the JAR file visible.
For API-only JAR files, this is fine; but for implementation JAR files, you don’t want to expose internal details.
Clients might then use and rely on these internal classes by mistake.
As you’ll see in a moment, exporting everything also increases the chance of conflicts among bundles containing the same package, particularly when they provide a different set of classes in those packages.
When you’re new to OSGi, exporting everything can look like a reasonable choice to begin with, especially if you don’t know precisely where the public API begins or ends.
On the contrary: you should try to trim down the list of exported packages as soon as you have a working bundle.
Let’s use a real-world example to demonstrate how to select your exports.
Here are some of the packages containing classes and resources inside the core BeanUtils 1.8.0 library from Apache Commons (http://commons.apache.org/beanutils/):
BeanUtils uses only a few of the Collections classes; and rather than have an executiontime dependency on the entire JAR file, the project embeds a copy of what it needs.
What happens when your application requires both the BeanUtils and Collections JAR files?
This typically isn’t a problem in a non-OSGi environment because the application class loader exhaustively searches the entire class path to find a class.
If both BeanUtils and Collections were on the same class path, they would be merged together, with classes in BeanUtils overriding those from Collections or vice versa depending on their ordering on the class path.
One important caveat is that this only works if the BeanUtils and Collections versions are compatible.
If you have incompatible versions on your class path, you’ll get runtime exceptions because the merged set of classes is inconsistent.
OSGi tries to avoid this incompatibility by isolating bundles and only exposing packages by matching imports with exports.
It doesn’t see the complete set of classes sitting in the Commons Collections bundle.
You can do this because the Collections package doesn’t belong to the main BeanUtils API.
Now, if it was purely an implementation detail that was never exposed to.
Figure 6.3 The classic application class loader merges JAR files into a single class space.
Turning JARs into bundles clients, your job would be complete.
But there’s a hitch: a class from the Collections package is indirectly exposed to BeanUtils clients via a return type on some deprecated methods.
What can you do? You need to find a way to guarantee that the BeanUtils bundle uses the same Commons Collections provider as its clients.
The simplest solution would be to make this dependency explicit by importing org.apache.
Now, if the full package is available, you’ll import it; but if it’s not available, you can still use your internal private copy.
Will this work? It’s better, but it still isn’t entirely accurate.
See the sidebar “Revisiting uses constraints” if you want more details as to why an optional import won’t work.
The idea was that your bundle would import it if an exporter was available, but would use its private copy if not.
This doesn’t work, but why not? It’s all about uses constraints, as discussed in section 2.7.2
As we mentioned, BeanUtils exposes a type from the Collections package in a return type of a method in its exported types; this is a uses constraint by definition.
To deal with this situation, you must express it somehow.
Let’s assume you follow the optional import case and try to model the uses constraint correctly, like this:
This may work in some situations; for example, it would work if you deployed your BeanUtils bundle, another bundle importing BeanUtils and Collections, and a bundle exporting the Collections package.
In this case, all the bundles would be wired up to each other, and everyone would be using the correct version of the Collections packages.
But what would happen if the BeanUtils bundle was installed and resolved by itself first? In that case, it wouldn’t import the Collections package (because there isn’t one) and would use its private partial copy instead.
Now, if the other bundles were installed and resolved, you’d end up with the wiring depicted here:
Some want to reuse code from another large library but don’t want to bloat their own JAR file.
Some prefer to ship a single self-contained JAR file that clients can add to their class path without worrying about conflicting dependencies.
Some libraries even use tools such as Jar Jar Links (http://code.google.com/p/jarjar/) to repackage internal dependencies under different namespaces to avoid potential conflicts.
This leads to multiple copies of the same class all over the place, because Java doesn’t provide modularity out of the box! Renamed packages also make debugging harder and confuse developers.
OSGi removes the need for renaming and helps you safely share packages while still allowing you to hide and embed implementation details.
At this point, you may decide it’s a good time to refactor the API to make it more modular.
Separating interfaces from their implementations can avoid the need for partial (or so-called split) packages.
This helps you reduce the set of packages you need to export and make your bundle more manageable.
Although this may not be an option for thirdparty libraries, it’s often worth taking time to contact the original developers to explain the situation.
You should also be careful to avoid accidentally leaking implementation types via method signatures.
As you saw with the BeanUtils example, the more internal details are exposed through your API, the harder it is to modularize your code.
After you have your list of exported packages, you should consider versioning them.
This means the BeanUtils bundle is using its own private copy of the Collections types, whereas the importing bundle is using its imported collections types, so it will receive a ClassCastException if it uses any methods from BeanUtils that expose Collections types.
In the end, there’s no way to have a private copy of a package if its types are exposed via exported packages.
As we’ve concluded already, you must refactor your bundle to export preferably the whole package or to import the package.
A uses constraint on an optionally imported package is ignored if the optionally imported package isn’t wired to an exporter.
Turning JARs into bundles which implies that the packages change at the same rate as the bundle, but some packages inevitably change faster than others.
You may also want to increment the bundle version because of an implementation fix while the exported API remains at the same level.
Although everything starts out aligned, you’ll probably find that you need a separate version for each package (or at least each group of tightly coupled packages)
We’ll take an in-depth look at managing versions in chapter 9, but a classic example is the OSGi framework itself, which provides service APIs that have changed at different rates over time:
Knowing which packages to export is only half of the puzzle of turning a JAR into a bundle—you also need to find out what should be imported.
This is often the hardest piece of metadata to define and causes the most problems when people migrate to OSGi.
Do you know what packages a given JAR file needs at execution time? Many developers have tacit or hidden knowledge of what JAR files to put on the class path.
This leads to situations where an abundance of JAR files is loaded at execution time, not because they’re all required, but because a developer feels they may be necessary based on past experience.
The following lines show an example class path for a Java EE client.
Can you tell how these JAR files relate to one another, what packages they provide and use, and their individual versions?
With OSGi, you explicitly define which packages your bundle needs, and this knowledge is then available to any developer who wants it.
This means any developer turning a JAR file into a bundle has a great responsibility in defining the correct set of imported packages.
Unfortunately, standard Java tools don’t provide an easy way to determine which packages a JAR file may use at execution time.
Manually skimming the source for package names is time consuming and unreliable.
Byte-code analysis is more reliable and repeatable, which is especially important for distributed teams, but it can miss classes that are dynamically loaded by name.
For instance, this could load a class from any package:
The ideal solution is to use a byte-code analysis tool like bnd (http://aqute.biz/ Code/Bnd) followed by a manual review of the generated metadata by project developers.
You can then decide whether to keep generating the list of imported packages for every build or generate the list once and save it to a version-controlled file somewhere so it can be pulled into later builds.
Most tools for generating OSGi manifests also let you supplement or override the generated list, in case the manual review finds missing or incorrect packages.
After you’re happy with the metadata, you should run integration tests on an OSGi framework to verify that the bundle has the necessary imported packages.
Let’s continue with the BeanUtils example and use bnd to discover what imports you need.
The bnd tool was developed by the OSGi director of technology, Peter Kriens, and provides a number of Ant tasks and command-line commands specifically designed for OSGi.
Bnd uses a pull approach to divide a single class path into separate bundles based on a set of instructions.
This means you have to tell bnd what packages you want to pull in and export, as well as those you want to pull in and keep private.
Bnd instructions use the same format as OSGi directives, which means you can mix normal manifest entries along with bnd instructions.
In addition to accepting OSGi manifest headers as instructions, bnd adds some of its own, such as Include-Resource and Private-Package, to give you more control over exactly what goes into the bundle.
These instructions aren’t used by the OSGi framework at execution time.
The following instructions select the exported and non-exported (or so-called private) packages that should be contained in your final BeanUtils bundle.
You start by exporting all of the BeanUtils API, as discussed in section 6.1.2
Remember that you also want to remove the partial Collections package from the internals and import it instead.
Finally, you let bnd decide what this bundle needs to import.
Let’s put these instructions in a file named beanutils.bnd, which you can find under chapter06/ BeanUtils-example/ in this book’s examples:
Notice that unlike the standard OSGi headers, bnd package instructions can contain wildcards and negative patterns.
Bnd expands these patterns at build time according to what it finds in the project byte code on the class path, saving you the hassle of typing everything in minute detail.
After you’ve chosen your exported and internal packages, you invoke the bnd build task by passing it the original BeanUtils JAR file along with your custom bnd instructions:
Bnd processes the given class path using your instructions and generates a new JAR alongside the instructions file, called beanutils.jar.
You can extract the OSGi-enhanced manifest from the newly created BeanUtils bundle like so:
As you can see, it contains the following generated list of imported packages:
There are a couple of interesting points about this list.
First, bnd has added imports for all the BeanUtils packages that you want to export.
As we discussed in section 5.1.1, this is usually good practice when exporting an API that has multiple implementations, because it means that if (for whatever reason) an existing bundle already exports these packages, you’ll share the same class space for the API.
Without these imports, your bundle would sit on its own little island, isolated from any bundles already wired to the previous package exporter.
But if you don’t expect alternative implementations of Commons Collections, you can always turn off this feature with a special bnd directive:
Bnd has also found byte code references to the Apache Collections and Logging packages, which aren’t contained in the BeanUtils bundle and must therefore be imported.
Just think: you can now tell what packages a JAR file needs at execution time by checking the imported package list in the manifest.
BeanUtils, it should also deploy Commons Collections and Commons Logging (or another bundle that provides the same logging package, like SLF4J)
Just as with exported packages, you should consider versioning your imports.
Chapter 2 explained how versioning helps ensure binary compatibility with other bundles.
You should try to use ranges rather than leave versions open-ended, because doing so protects you against potentially breaking API changes in the future.
One recommended practice is to use a range starting from the minimum acceptable version up to, but not including, the next major version.
This assumes a change in major version is used to indicated that the API isn’t binary compatible.
This ensures that only versions from the tested level to just before the next major release are used.
Not all projects follow this particular versioning scheme—you may need to tweak the range to narrow or widen the set of compatible versions.
The width of the import range also depends on how you’re using the package.
If you’re just calling the interface, this change doesn’t affect you.
If, on the other hand, you’re implementing the interface, this will definitely break, because you now need to implement a new method.
Adding the correct version ranges to imported packages takes time and patience, but this is often a one-time investment that pays off many times over during the life of a project.
Tools such as bnd can help by detecting existing version metadata from dependencies on the class path and by automatically applying version ranges according to a given policy.
While you’re reviewing the generated list of imported packages, you may notice a few that aren’t used at execution time.
Some code may only be executed in certain scenarios, such as an Ant build task that’s shipped with a library JAR file for convenience.
Other JAR files may dynamically test for available packages and adapt their behavior at execution time to match what’s installed.
In such cases, it’s useful to mark these imports as optional to tell the OSGi framework that the bundle can still work even when these packages aren’t available.
Table 6.2 shows some real-world packages that are often consid-ered optional.
As you saw back in section 5.2, OSGi provides two ways to mark a package as optional.
Turning JARs into bundles list them as dynamically imported packages.
For packages you never expect to be used at execution time, like the Ant packages, we suggest that you either use the optional attribute or remove them from the list of imported packages.
Use resolution:= optional when you know the bundle will always be used the same way after it’s installed.
If you want a more adaptive bundle that reacts to the latest set of available packages, you should list them as dynamic imports.
If you’re new to OSGi and unsure exactly what packages your JAR file uses, consider using.
This makes things similar to the classic model, where requests to load a new class always result in a query to the complete class path.
It also allows your bundle to successfully resolve regardless of what packages are available.
The downside is that you’re pushing the responsibility of finding the right set of bundles onto users, because you don’t provide any metadata defining what you need! This approach should only be considered as a stopgap measure to get you started.
You’ve now chosen the exports and imports for your new bundle.
Every nonoptional, nondynamic package you import (but don’t export) must be provided by another bundle.
Does this mean that for every JAR file you convert into a bundle, you also need to convert each of its dependencies into bundles? Not necessarily, because unlike standard JAR files, OSGi supports embedding JAR files inside bundles.
Sometimes a JAR file has a close dependency on another JAR file.
Maybe they only work together, or the dependency is an implementation detail you want to keep private, or you don’t want to share the static member fields in the JAR file with other bundles.
In these situations, it makes more sense to embed the dependencies inside the primary JAR file when you turn it into a bundle.
Embedding the JAR file is easier than converting both JAR files to bundles because you can ignore packages that would otherwise need to be exported and imported between them.
The downside of embedding is that it adds unnecessary weight for non-OSGi users, who can’t use the embedded JAR file unless the bundle is first unpacked.
Figure 6.4a shows how a CGLIB bundle might embed ASM, a small utility for processing byte code.
Table 6.2 Common optional imported packages found in third-party libraries.
Alternatively, you can consider creating a new bundle artifact that embeds all the related JAR files together instead of turning the primary JAR file into a bundle.
This aggregate bundle can then be provided separately to OSGi users without affecting users of the original JAR files.
Figure 6.4b shows how you can use this approach for the CGLIB library.
Although this means you have an extra deliverable to support, it also gives you an opportunity to override or add classes for better interoperability with OSGi.
You’ll see an example in a moment and also later on in section 6.2.1
This often happens when libraries use external connections or background threads, which ideally should be managed by the OSGi lifecycle layer.
You may not realize it when you use a third-party library, but a number of them have a form of state.
This state can take the form of a background thread, a file system cache, or a pool of database connections.
Libraries usually provide methods to manage this state, such as cleaning up resources and shutting down threads.
Often, you don’t bother calling these methods because the life of the library is the same as the life of your application.
In OSGi, this isn’t necessarily the case; your application could still be running after the library has been stopped, updated, and restarted many times.
On the other hand, the library could still be available in the framework long after your application has come and gone.
You need to tie the library state to its bundle lifecycle; and to do that, you need to add a bundle activator (see section 3.4.1)
These threads are started lazily so there’s no need to explicitly initialize the pool, but the library provides a method to shut down and clean everything up:
To wrap the HttpClient library JAR file up as a bundle, you can add an activator that shuts down the thread pool whenever the HttpClient bundle is stopped.
You have to tell OSGi about this activator by adding metadata to the manifest:
You can see this in action by building and running the following example:
You should see it start and attempt to connect to the internet (ignore log4j warnings):
Stop the HttpClient bundle, which should clean up the thread pool, and check again:
Unfortunately, this isn’t a complete solution, because if you stop and restart the test bundle, the thread pool manager reappears—even though the HttpClient bundle is still stopped! Restricting use of the HttpClient library to the bundle active state would require all calls to go through some sort of delegating proxy or, ideally, the OSGi service registry.
Thankfully, the 4.0 release of the HttpClient library makes it much easier to manage connection threads inside a container such as OSGi and removes the need for this single static shutdown method.
Bundle activators are mostly harmless because they don’t interfere with non-OSGi users of the JAR file.
They’re only referenced via the bundle metadata and aren’t considered part of the public API.
They sit there unnoticed and unused in classic Java applications until the bundle is loaded into an OSGi framework and started.
Whenever you have a JAR file with implicit state or background resources, consider adding an activator to help OSGi users.
We’ve now covered most aspects of turning a JAR file into a bundle: identity, exports, imports, embedding, and lifecycle management.
How many best practices can you remember? Wouldn’t it be great to have them summarized as a one-page cheat sheet?
Figure 6.5 presents a cheat sheet that gives you a handy summary of converting JAR files into bundles.
OK, you know how to take a single JAR file and turn it into a bundle, but what about a complete application? You could take your existing JAR, EAR, and WAR files and turn them all into bundles; or you could choose to wrap everything up as a single application bundle.
What techniques can you use to bundle up an application, and what are the pros and cons? For the answers to this and more, read on.
Most applications are made up of one or more JAR files.
One way to migrate an application to OSGi is to take these individual JAR files and convert each of them into a bundle using the techniques discussed in the previous section.
Converting lots of JAR files is time consuming (especially for beginners), so a simpler approach is to take your complete application and wrap it up as a single bundle.
In this section, we’ll show you how to start from such a single application bundle and suggest ways of dividing it further into multiple bundles.
Along the way, we’ll look at how you can introduce other OSGi features, such as services, to make your application more flexible.
Finally, we’ll suggest places where it doesn’t make sense to introduce a bundle.
Let’s start with the single application bundle or so-called mega bundle.
A mega bundle comprises a complete application along with its dependencies.
Anything the application needs on top of the standard JDK is embedded inside this bundle.
Splitting an application into bundles and made available to the application by extending the Bundle-ClassPath (2.5.3)
This is similar to how Java Enterprise applications are constructed.
In fact, you can take an existing web application archive (WAR file) and easily turn it into a bundle by adding an identity along with Bundle-ClassPath entries for the various classes and libraries contained within it, as shown in figure 6.6
The key benefit of a mega bundle is that it drastically reduces the number of packages you need to import, sometimes down to no packages at all.
Boot delegation can also avoid certain legacy problems (see section 8.2 for the gory details)
The downside is that it reduces modularity, because you can’t override boot-delegated packages in OSGi.
A mega bundle with boot delegation enabled is close to the classic Java application model; the only difference is that each application has its own class loader instead of sharing the single JDK application class loader.
The sample code for this book comes with a copy of the jEdit 4.2 source, which you can unpack like so:
The jEdit build uses Apache Ant (http://ant.apache.org/), which is good news because it means you can use bnd’s Ant tasks to generate OSGi manifests.
How exactly do you add bnd to the build? The following listing shows the main target from the original (non-OSGi) jEdit build.xml.
If you don’t want to change the build process but still want an OSGi-enabled manifest, you can take the jEdit binary, run it through an analyzer like bnd, and add the generated OSGi headers to this static manifest.
As we mentioned back in section 6.1.3, this approach is fine for existing releases or projects that don’t change much.
On the other hand, integrating a tool such as bnd with your build means you get feedback about the modularity of your application immediately rather than when you try to deploy it.
Let’s make things more dynamic and generate OSGi metadata during the build.
This is the recommended approach because you don’t have to remember to check and regenerate the metadata after significant changes to the project source.
This is especially useful in the early stages of a project, when responsibilities are still being allocated.
There are several ways to integrate bnd with a build:
If you need certain features of the jar task, such as indexing, you should use the first or second option.
If you’re post-processing classes or need to filter resources, choose either the second or third option.
Let’s go with the third option to demonstrate how easy it is to switch your build over to bnd.
It will also help you later, in section 6.2.2, when you start partitioning the application into separate bundles.
The first line above shows where to put the JAR file, and the second lists fixed manifest entries.
Here, you first give the location of the bnd JAR file to tell Ant where it can find the bnd task definition.
Then you specify a bnd task to create your bundle JAR file, giving it the project class path and the file containing your bnd instructions.
If you don’t tell bnd to pull a certain package into the bundle, don’t be surprised if the package isn’t there.
You’re building a single mega bundle, so you need only one instruction file: call it jedit-mega.bnd.
The first thing you must add is an instruction to tell bnd where to put the generated bundle:
The bnd task can also copy additional manifest headers into the final manifest, so let’s ask bnd to include the original jEdit manifest rather than duplicate its content in your new file:
You could have left the manifest file where it was, added your instructions to it, and passed that into bnd, but this would make it harder for people to separate out the new build process from the original.
It’s also better to have the bnd instructions at the project root where they’re more visible.
You can now try to build the project from inside the jEdit directory:
What went wrong? You forgot to tell bnd what packages to pull into your new bundle! Using the JAR-to-bundle cheat sheet from section 6.1.6, add the following bundle headers to jedit-mega.bnd along with a bnd-specific instruction to pull in all classes and resources from the build class path and keep them private:
Getting back to the task at hand, when you rebuild the jEdit project you now see this:
Take care with wildcards Remember that bnd supports wildcard package names, so you can use * to represent the entire project.
Although this is useful when creating mega bundles, you should be careful about using wildcards when separating a class path into multiple, separate bundles, or when already bundled dependencies appear on the class path.
Always check the content of your bundles to make sure you aren’t pulling in additional packages by mistake!
The resource files can now be found on the build class path.
Bingo! You should see the main jEdit window appear, as shown in figure 6.7
Your bundle works as a classic JAR file, but will it work as a bundle? Let’s review the manifest.
Your jEdit bundle doesn’t export any packages, but it does use packages from Swing.
These should come from the system bundle, which is typically set up to export JDK packages (although this can be overridden)
You may wonder if you should add version ranges to the packages imported from the JDK.
This isn’t required, because most system bundles don’t version their JDK packages.
You only need to version these imports if you want to use another implementation that’s different from the stock JDK version.
We should also mention that the final manifest contains some bnd-specific headers that aren’t used by the OSGi framework (such as Private-Package, Tool, and Bnd-LastModified)
They’re left as a useful record of how bnd built the bundle, but.
Splitting an application into bundles if you don’t want them, you can remove them by adding this bnd instruction to jedit-mega.bnd:
The new manifest looks correct, but the real test is yet to come.
You must now try to deploy and run your bundle on an actual OSGi framework.
Will it work the first time or fail with an obscure exception?
You can deploy your jEdit bundle by using the same simple launcher used to launch the earlier paint examples.
Remember, this launcher first installs any bundles found in the directory and then uses the first Main-Class header it finds to bootstrap the application.
Your manifest already has a Main-Class, so you need to point the launcher at the jEdit directory, like so:
The bundle installs and the application starts, but it hangs at the splash screen shown in figure 6.8, and the main jEdit window never appears.
If you look closely at the top of the stack trace, you see the following warning message:
Why did this work when the bundle was run as a classic application, but not when the bundle was installed in an OSGi framework? The answer lies in the URL Handlers Service we discussed briefly back in section 4.6.1
Your first thought may be to try to disable the URL Handlers Service so it doesn’t install this factory.
Turning off the global URL Handlers Service also has serious implications.
It means no bundle can contribute dynamic protocol handlers, which would break applications that rely on the URL Handlers Service.
The JDK’s URL Handler factory uses Class.forName() to search the application class path for valid handlers, but your jeditresource handler is hidden from view inside the jEdit bundle class loader.
But how can you add OSGi-specific code without affecting classic jEdit users? Cast your mind back to section 6.1.5, where we talked about using lifecycles to manage external resources.
This is exactly the sort of situation that requires a bundle activator, such as the one shown next.
This is a common cause of head-scratching for people new to OSGi, because the framework can’t tell when you accidentally forget a Bundle-Activator header.
When you’ve added an activator, but it’s having no effect, always check your manifest to make sure it’s been declared—it saves a lot of hair!
Your activator code uses OSGi constants and interfaces, so you must add the core OSGi API to the compilation class path in the jEdit build.xml.
This API is only required when compiling the source; it isn’t necessary at execution time unless the activator class is explicitly loaded.
One more build, and you now have a JAR file that can run as a classic Java application or an OSGi bundle! The following snippet shows the final set of bnd instructions for the jEdit mega bundle:
One last wrinkle: you have to tell jEdit where its installation directory is by using the jedit.home property.
Normally, jEdit can detect the installation directory containing its JAR file by peeking at the application class path, but this won’t work when running it as a bundle on OSGi because the JAR file is loaded via a different mechanism:
With this last piece of configuration in place, you should see jEdit start and the main window appear, as you saw earlier in figure 6.8
It should also still work as a classic Java application.
You’ve successfully created a mega bundle for jEdit with a small amount of effort.
What are the downsides of a mega bundle? Well, your application is still one single unit.
You can’t replace or upgrade sections of it without shutting down the complete application, and doing so may shut down the entire JVM process if the application calls System.exit()
Because nothing is being shared, you can end up with duplicate content between applications.
Effectively, you’re in the same situation as before moving to OSGi, but with a few additional improvements in isolation and management.
This doesn’t mean the mega bundle approach is useless—as a first step, it can be reassuring to be able to run your application on an OSGi framework with the minimum of fuss.
It also provides a solid foundation for further separating (or slicing) your application into bundles, which is the focus of the next section.
You now have a single mega bundle containing your entire application.
The next step toward a full-fledged flexible OSGi application is to start breaking it into bundles that can be upgraded independently of one another.
How and where should you draw the lines between bundles?
Bundles import and export packages in order to share them, so it makes sense to draw lines that minimize the number of imports and exports.
If you have a high-level design document showing the major components and their boundaries, you can take each major component and turn it into a bundle.
If you don’t have such a document, you should look for major areas of responsibility such as business logic, data access, and graphical components.
Each major area can be represented by a bundle, as depicted in figure 6.9
Another way to approach this is to review the benefits of modularity (described in section 2.2) and think about where they make the most sense in your application.
For example, do any areas need to be upgraded or fixed independently? Does the application have any optional parts? Are common utilities shared throughout the application?
Returning to the jEdit example, what areas suggest themselves as potential bundles? The obvious choice to begin with is to separate the jEdit code from third-party libraries and then try to extract the main top-level package.
But how do you go about dividing the project class path into different bundles?
Recall what we said about bnd back in section 6.1.3, that it uses a pull approach to assemble bundles from a project class path based on a list of instruction files.
All you need to do is provide your bnd task with different instruction files for each bundle.
The following example divides the class path into three bundles:
The first bundle contains all third-party classes—basically, any package from the build directory that doesn’t start with org.gjt.sp.
This copies all other packages into the bundle and keeps them private.
Using the earlier jedit-mega.bnd file as a template, you can flesh out the rest to get.
You also exclude the installer package because it isn’t required at execution time and doesn’t belong in the third-party library bundle.
The second bundle contains the top-level package containing the main jEdit class.
Notice that the only difference between this file and the mega bundle instructions shown earlier is the selection of private packages; everything else is exactly the same.
The main bundle also replaces the mega bundle as the executable JAR file.
The third and final bundle contains the rest of the jEdit packages, which we’ll call the engine for now.
Notice how the same packages listed in the main instructions are negated in the engine instructions.
Refactoring packages between bundles is as simple as moving entries from one instruction file to another.
You now have three bundles that together form the original class path, but none of them share any packages.
If you tried to launch the OSGi application at this point, it would fail because of unsatisfied imports between the three bundles.
Should you go ahead and export everything by switching all Private-Package instructions to Export-Package? You could, but what would you learn by doing that? Let’s try to export only what you absolutely need to share, keeping as much as possible private.
There are three ways you can find out which packages a bundle must export:
Gain an understanding of the code base and how the packages relate to one other.
Read the Import-Package headers from generated manifests to compile a list of packages that “someone” needs to export.
You can use the bnd print command to avoid having to unpack the manifest.
Repeatedly deploy the bundles into a live framework, and use any resulting error messages and/or diagnostic commands (such as the diag command on Equinox) to fine-tune the exported packages until all bundles resolve.
The first option requires patience, but the reward is a thorough understanding of the package structure.
It also helps you determine other potential areas that can be turned into bundles.
The third option can be quick if the framework gives you the complete list of missing packages on the first attempt, but sometimes it feels like an endless loop of “deploy, test, update.” The second option is a good compromise between the other two.
The bnd tool has already analyzed the code base to come up with the list of imports, and you already know that the framework will follow the import constraints listed in the manifest.
The structured manifest also means you can write a script to do the hard work for you.
The first group includes third-party packages, next is the main jEdit package, and the long group contains other jEdit packages.
It’s clear that the third-party library bundle needs to export only three packages and the main jEdit bundle just the top-level package.
Unfortunately, the jEdit engine bundle needs to export almost all of its packages, indicating a tight coupling between the engine and the top-level jEdit package.
This suggests that it would be better to merge these two bundles back together, unless you were going to refactor the code to reduce this coupling.
Let’s ignore this for now and press on, because this separation will eventually lead to an interesting class-loading issue that’s worth knowing about.
You should also add version ranges to your imports, as suggested back in section 6.1.3
Rather than endure the hassle of explicitly writing out all the ranges, you can take advantage of another bnd feature and compute them:
Following are the final bnd instructions for the jEdit third-party library bundle:
And here are the final bnd instructions for the jEdit engine bundle:
You still have one more (non-OSGi) tweak to make to the main jEdit bundle instructions.
Remember that you now create three JAR files in place of the original single JAR file.
Although you can rely on the OSGi framework to piece these together into a single application at execution time, this isn’t true of the standard Java launcher.
You need some way to tell it to include the two additional JAR files on the class path whenever someone executes:
Thankfully, there is a way: you need to add the standard Class-Path header to the main JAR file manifest.
The Class-Path header takes a space-separated list of JAR files, whose locations are relative to the main JAR file.
These final main-bundle instructions allow jEdit to work both as a bundle and an executable JAR:
As we hope this example demonstrates, after you have an application working in OSGi, it doesn’t take much effort to start slicing it up into smaller, more modularized bundles.
But is this all you can do with jEdit on OSGi—keep slicing it into smaller and smaller pieces?
So far, we’ve focused on using the first two layers of OSGi: module and lifecycle.
There’s another layer you haven’t yet used in this chapter: service.
The service layer is different from the first two layers in that it can be hard to tell when or where you should use it, especially when migrating an existing application to OSGi.
Often, people decide not to use services at all in new bundles, instead relying on sharing packages to find implementations.
But as you saw in chapter 4, services make your application more flexible and help reduce the coupling between bundles.
The good news is, you can decide to use services at any time; but how will you know when the time is right?
There are many ways to share different implementations inside a Java application.
You can construct instances directly, call a factory method, or perhaps apply some form of dependency injection.
When you first move an application to OSGi, you’ll probably decide to use the same tried-and-tested approach you did before, except that now some of the packages come from other bundles.
As you saw in chapter 4, these approaches have certain limitations compared to OSGi services.
Services in OSGi are extremely dynamic, support rich metadata, and promote loose coupling between the consumer and provider.
If you expect to continue to use your application outside of OSGi—for example, as a classic Java application—you may be worried about using the service layer in case it ties you to the OSGi runtime.
No problem! You can get the benefits of services without being tied to OSGi by using component-based dependency injection.
If you already use dependency injection, moving to these component models is straightforward; sometimes it’s only a matter of reconfiguring the dependency bindings in your original application.
But make sure you come back and read the intervening chapters; they’ll be an invaluable guide when it comes to managing, testing, and debugging your new OSGi application.
Where might you use services in jEdit? Well, jEdit has its own home-grown plugin framework for developers to contribute all sorts of gadgets, tools, and widgets to the GUI.
These static method calls are convenient, but they make it hard to mock out (or replace) dependencies for testing purposes.
Instead of relying on static methods, you can change jEdit to use dependency injection.
Plugins then have their dependencies injected, rather than call jEdit directly.
This also simplifies unit testing, because you can swap out the real bindings and put in stubbed or scripted test implementations.
With this in mind, is there a smaller task that can help bridge the gap between OSGi bundles and jEdit plugins and make it easier to use services?
You can consider replacing the jEdit plugin framework with OSGi, much as Eclipse replaced its original plugin framework.
To do this, you have to take the JARClassLoader and PluginJAR classes and extract a common API that you can then reimplement using OSGi, as shown in figure 6.10
You use the original jEdit plugin code when running in classic Java mode and the smaller OSGi mapping layer when running on an OSGi framework.
Extracting the common plugin API is left as an interesting exercise for you; one wrinkle is the fact that jEdit assumes plugins are located in the file system, whereas OSGi supports bundles installed from opaque input streams.
The new plugin API can have methods to iterate over and query JAR file entries to avoid having to know where the plugin is located.
These methods will map nicely to the resource-entry methods on the OSGi Bundle interface.
How about being able to register OSGi bundles as jEdit plugins? This is a stepping stone to using services, because you need a bundle context to access OSGi services.
The main jEdit class provides two static methods to add and remove plugin JAR files:
Following the extender pattern introduced in section 3.4, let’s use a bundle tracker to look for potential jEdit plugins.
The code in the following listing uses a tracker to add and remove jEdit plugin bundles as they come and go.
Listing 6.4 Using the extender pattern to install jEdit plugins.
The code identifies jEdit plugins by looking for a file called actions.xml in the bundle root B.
Because the jEdit API only accepts path-based plugins, it ignores bundles whose locations don’t map to a file E.
To remove a plugin bundle, it uses another jEdit method to map the location back to the installed PluginJAR instance C.
The last piece of the puzzle is to start the bundle tracker only when jEdit is ready to accept new plugins.
If you look at the jEdit startup code, you may notice that one of the last things it does in finishStartup() is send out the initial EditorStarted message on the EditBus (jEdit’s event-notification mechanism)
The code registers a one-shot component that listens for any message event, deregisters itself, and starts the bundle tracker D.
Look in the Plugins menu; no plugins should be available.
Selecting this item opens the window shown in figure 6.11
If you stop the calculator bundle, this window immediately disappears, and the Plugins menu once again shows no available plugins:
Cool—the extender successfully bridges the gap between OSGi bundles and jEdit plugins! You can now use existing OSGi management agents, such as the Apache Felix Web Console (http://felix.apache.org/site/apache-felix-web-console.html), to manage jEdit plugins.
This small example shows how standards like OSGi can make it much easier to reuse and assemble existing pieces into new applications.
Are you eager to start moving your application to OSGi? Wait, not so fast! We have one last topic to discuss before we close out this chapter, and it’s something you should keep asking yourself when you’re modularizing applications: is this bundle adding any value?
Sometimes, you should take a step back and think, do I need another bundle? The more bundles you create, the more work is required during build, test, and management in general.
Creating a bundle for every individual package is obviously overkill, whereas putting your entire application inside a single bundle means you’re missing out on modularity.
Some number of bundles in between is best, but where’s the sweet spot?
One way to tell is to measure the benefit introduced by each bundle.
If you find you’re always upgrading a set of bundles at the same time and you never install them individually, keeping them as separate bundles isn’t bringing much benefit.
You can also look at how your current choice affects developers.
If a bundle layout helps developers work in parallel or enforces separation between components, it’s worth keeping.
But if a bundle is getting in the way of development, perhaps for legacy class-loader reasons, you should consider removing it, either by merging it with an existing bundle or by making it available via boot delegation (we briefly discussed this option at the start of section 6.2.1)
Consider the jEdit example: have you reached the right balance of bundles?
Recall the Import-Package discussion back in the section “Stitching the pieces together.” We mentioned an interesting issue caused by placing the top-level package in its own bundle, separate from the rest of the jEdit engine.
You can see the problem for yourself by starting the OSGi version of jEdit and selecting File > Print.
A message box pops up (see figure 6.12), describing a failure in a BeanShell script.
Why did the script fail? The error message suggests a class-loading problem.
This is a utility class that manages BeanShell script execution for jEdit.
It’s part of the top-level jEdit package loaded by the main bundle class loader, and it configures the BeanShell engine to use a special instance of JARClassLoader (previously discussed in section 6.2.3) that.
Splitting an application into bundles delegates to each plugin class loader in turn.
This is so BeanShell scripts can access any class in the entire jEdit application.
If none of the plugin class loaders can see the class, this special class loader delegates to its parent class loader.
For a classic Java application, this is the application class loader, which can see all the jEdit classes on the class path.
One thing you know it can’t see is the BufferPrinter1_4 class.
You could check the manifest to make sure this package is being exported as expected; but if you’re using the instructions from the section “Stitching the pieces together,” then it is.
It’s being exported from the engine bundle, but is it being imported by the main bundle? Without an import, this package isn’t visible.
Let’s avoid cracking open the JAR file and instead use bnd to see the list of imports.
Aha! The main bundle manifest contains no mention of the org.gjt.sp.
A last question before you try to fix this issue: why didn’t bnd.
Listing 6.5 Using bnd to print imported and exported packages.
The only reference to this package was in a BeanShell script, which wasn’t analyzed by the bnd tool.
You now have all the answers as to why the script failed, but how should you solve the problem? Bnd supports adding custom analyzers to process additional content, so you could write your own BeanShell analyzer for bnd.
But what if writing such an analyzer is outside your expertise? Can you instead fix the class-loading problem at execution time? There are two approaches to solving this type of class-loading issue:
The first approach is only possible when the library provides some way of passing in the class loader or when it uses the Thread Context Class Loader (TCCL) to load classes.
The BeanShell library does provide a method to set the class loader, but jEdit is already using it to pass in the special class loader that provides access to all currently installed jEdit plugins.
Rather than mess around with jEdit’s internal JARClassLoader code and potentially break the jEdit plugin framework, you’ll take the second approach and add the missing imports to the main bundle.
This has the least impact on existing jEdit code—all you’re doing is updating the OSGi part of the manifest.
Although this testing could be automated to save time, let’s instead try the suggestion from the end of section 6.1.3 and allow the main jEdit bundle to import any package on demand:
Add this to the jedit-main.bnd instruction file, and rebuild one more time.
You can now open the print dialog box without getting the error message.
The application will also continue to work even if you use a more restrictive dynamic import, such as.
Why does this work? Well, rather than say up front what you import, you leave it open to whatever load requests come through the main bundle class loader.
As long as another bundle exports the package, and it matches the given wildcard, you’ll be able to see it.
But is this the right solution? Merging the main and engine bundles back together would solve the BeanShell problem without the need for dynamic imports.
You already know these bundles are tightly coupled; keeping them apart is causing you further trouble.
This is a good example of when introducing more bundles doesn’t make sense.
OSGi isn’t a golden hammer, and it won’t magically make code more modular.
In short, if you’re getting class-loading errors or are sharing lots of packages between bundles, that could be a sign that you should start merging them back together.
This sounds useful, but there’s a catch: if you use boot delegation, you won’t be able to use multiple versions or dynamically deploy them.
But if it avoids tangled classloading problems and helps keep your developers sane, you may decide this is a fair trade.
You can often achieve more by concentrating on modularizing your own code.
Leave complex third-party library JAR files on the application class path until you know how to turn them into bundles or until an OSGi-compatible release is available.
As we often say in this book, you can decide how much OSGi you want to use: it’s definitely not an all-or-nothing approach!
But what is involved in testing bundles? After you’ve split your application into many independent parts, how do you keep everything consistent, and how do you upgrade your application without bringing everything down? The next chapter will discuss this and more, as we look at testing OSGi applications.
But what can you do to make sure you’re on the right track to modularity and not turning your applications into tangled spaghetti? As is true for any piece of software, the best way to track quality is with regular testing.
Testing can confirm that your modularized code meets the same requirements as your original application.
Testing can verify that your code will continue to work when deployed inside the target container.
It can even help you practice different deployment scenarios in the safety of your friendly neighborhood test server.
Even a simple nonfunctional test, such as checking the number of shared packages between bundles, can avoid tangles forming early on in development.
Why wait until the end of a project to discover if your code works in the strict environment of an OSGi framework or how well your chosen bundles fit together? Migrate and modularize your tests along with your code! This chapter will help put this advice into practice by taking you through three different approaches:
The last section in particular takes a closer look at how unit and integration test concepts relate to modular applications and introduces the idea of management testing.
If you’re eager to learn more about testing modularity and you’re already familiar with in-container tests and object mocking, feel free to skip ahead to section 7.3
By the end of this chapter, you should be comfortable with testing OSGi applications, which will lead to better quality bundles for everyone.
Let’s start by continuing the theme from chapter 6 and get some existing tests running on an OSGi framework.
Imagine you have an application that you want to modularize and move to OSGi.
You almost certainly have existing tests that check requirements and expected behavior.
You can use these tests to verify and validate the modularization process, either by manually running them at key stages or by using an automated build system that runs tests on a regular schedule—say, whenever people check in code.
These tests give you confidence that your modularized application is to some extent equivalent to the original, at least when run with the test framework.
But what they don’t tell you is whether your code behaves the same inside an OSGi container.
To find out, you need to run your tests twice: inside the target container as well as outside.
Running these tests outside the container is a matter of using your favorite test framework, like JUnit (http://junit.sourceforge.net/) or TestNG (http:// testng.org/)
There are many good books on testing standard Java applications, so we assume you already know how to write unit tests and run them using Ant, Maven, or your IDE.
But what about testing inside an OSGi container; how does it work in practice, and is it worth the effort?
Would you develop and deploy a web application without ever testing it inside an application server? Would you ship a product without testing its installer? Of course not! It’s important to test code in the right environment.
If you expect to use a class with OSGi, you should test it inside an OSGi framework—how else will you discover potential class-loading or visibility issues? But before you can run your existing JUnit or TestNG tests inside the container, you first need to deploy them.
As you saw in chapter 6, whenever you want to deploy something into an OSGi framework, you must consider packaging and placement.
Does this mean you should bundle tests along with the application code? It depends on how you expect the code to be used in OSGi.
Internal classes can only be tested from inside the same bundle, but public-facing code can and should be tested from another bundle to mimic real-world conditions.
Testing code inside the same bundle typically means the caller and callee share the same class loader, but many OSGirelated issues only appear when different class loaders are involved.
We’ll concentrate on the last two options (intra-bundle and inter-bundle tests) because it’s much more realistic to have the test code running inside the container along with the code being tested.
Bundle testing means deploying tests in bundles just like any other piece of code, but how much effort is involved in getting tests up and running in an OSGi framework? Let’s find out right now by converting an existing open source library and its test suite into separate bundles.
The Apache Commons Pool project (http://commons.apache.org/pool/) provides a small library for managing object pools.
You’ll use the source distribution for Commons Pool 1.5.3, which contains the code for both the library and its test suite:
Begin the example by splitting the Commons Pool library and tests into two bundles.
The main subproject extracts the library source, compiles it, and creates a simple bundle that exports the main package, but hides the implementation (.impl) package.
The test subproject does exactly the same thing for the test source, but it appends -test to the bundle symbolic name to make sure the bundles are unique.
The Commons Pool tests are JUnit tests, so you also need access to the JUnit library.
Should it be deployed as a bundle or placed on the external class path? Exposing the packages from the external class path means you don’t have to turn JUnit into a bundle, but it also means JUnit can’t see test classes unless they’re on the same class path or explicitly passed in via a method call.
Migrating tests to OSGi scan bundles for tests and feed the class instances to JUnit, instead of relying on the standard test runner.
We’ll look at a tool that does this in section 7.3
You can use the bndwrap Ant task from the bnd tool (http://aqute.biz/Code/Bnd) to quickly wrap the JAR file.
The bndwrap task analyzes the JAR and creates a bundle that exports all packages contained inside it.
It also adds optional imports for any packages that are needed but not contained in the JAR file.
Unfortunately, this import list doesn’t contain your test packages, because JUnit doesn’t know about them yet.
This dynamic import means JUnit will be able to see any future test class, as long as some bundle exports them.
This tells your example launcher to start the JUnit test runner after deploying all the bundles.
Your launcher will automatically pass any arguments after the initial bundle directory setting on to the Main-Class.
Hmm...the JUnit bundle couldn’t see the TestAll class even though the test bundle clearly exports it.
If you look closely at the package involved and cast your mind back to the visibility discussion from section 2.5.3, you should understand why.
This is the same package that’s exported by the main Commons Pool bundle! Remember that packages can’t be split across bundles unless you use bundle dependencies (section 5.3), and you’re using package dependencies.
You could use Require-Bundle to merge the packages together and re-export them (see section 5.3.1 for more about re-exporting packages), but you’d then need to use mandatory attributes to make sure JUnit and other related test bundles were correctly wired to the merged package.
This would lead to a fragile test structure and cause problems with package-private members (to find out why, see the discussion near the start of section 5.4.1)
A better solution is to use fragments (section 5.4) to augment the original bundle with the extra test classes.
This is all you need to declare your test bundle as a fragment of the main library bundle.
With this change in place, you can rebuild and repeat the test.
You should see JUnit run through the complete Commons Pool test suite, which takes around two minutes:
You’re now running all your tests inside the combined library bundle (the intrabundle option from figure 7.1) because your test fragment contains both internal and public-facing tests.
You could go one step further and use a plain bundle for public tests and a fragment for internal tests, but you’d need some way to give JUnit access to your internal tests.
You don’t want to export any internal packages from the fragment because that would also expose internals from the main bundle, potentially affecting the test results.
The least disruptive solution is to keep a single public test class in the fragment that can be used to load the internal tests.
You can move the remaining public-facing tests to a new package that doesn’t conflict w them in a separate bundle.
The result is a combination of both inter-bundle and intra-bundle testing.
Figure 7.3 shows an example of such a structure for test ing Commons Pool.
You can also run the test example outside of the container by invoking JUnit with the various bundles on the standard Java class path.
In this case, you don’t need to start the OSGi framework.
To try this, use the test build target instead of test.osgi.
You’ve seen how easy it is to run tests both inside and outside of a container, but how do you know if you’re testing all possible scenarios and edge cases? Most projects use coverage to measure test effectiveness, although this doesn’t guarantee you have wellwritten tests! Given the importance of test coverage, let’s continue with the example and find out how you can record coverage statistics inside an OSGi container.
It’s always good to know how much of your code is being tested.
Like test results, coverage can vary depending on whether you’re testing inside or outside a container.
This makes in-container tests just as important as out-of-container tests when determining overall test coverage.
The first and third stages can be done outside of the OSGi container.
This leaves you with the second stage: testing the instrumented classes inside the chosen container.
You already know you can run the original tests in OSGi, so what difference does instrumentation make? It obviously introduces some sort of package dependency to the coverage library, but it also introduces a configuration dependency.
The instrumented code needs to know where to find the coverage database so it can record results.
When using boot delegation, you must make sure coverage packages are excluded from the generated Import-Package in the library bundle or at least made optional.
Not doing this would lead to a missing constraint during resolution, because no bundle exports these packages.
The simplest approach is to add the coverage JAR file and its dependencies to the launcher’s class path and update the system packages.
Next simplest is boot delegation: here you have the extra step of removing coverage packages from the ImportPackage of your instrumented bundle.
Let’s take the interesting route and turn the coverage JAR file into a bundle.
Our chosen coverage tool for this example is Cobertura 1.9.3 (http://cobertura.sourceforge.net/), but all the techniques mentioned should work for other tools as well.
The first step is to create a new JAR file which contains the original Cobertura JAR file and all of its execution-time dependencies.
You embed these dependencies because you want this to be a standalone bundle.
Remember, this bundle will only be used during testing, so you have more leeway than if you were creating a production-quality bundle.
You then use the bnd tool to wrap the JAR file in the same way you wrapped JUnit, making sure you set Bundle-ClassPath so the bundle can see its embedded dependencies.
All you need to do now is instrument the classes and run the tests:
You use the instrument property to enable the various instrumentation targets.
As soon as the tests complete, the build runs the Cobertura report task to process the results.
In this section, you saw how to take existing tests (and test tools) and run them inside an OSGi container.
Deciding how to bundle tests is no different than deciding how to bundle an application.
Visibility and modularity are just as important when it comes to testing.
But what about going the other way? Can you take OSGi-related code and test it outside the container?
When you first begin to modularize and migrate an application over to OSGi, you probably won’t have a direct dependency on the OSGi API.
This means your code can still be tested both inside and outside the container.
But at some point you’ll want to use the OSGi API.
It may start with one or two bundle activators, and then maybe use the bundle context to look up a service.
But what if you have code that uses the OSGi API? Such code can’t be tested outside the container—or can it?
Imagine if you could mimic the container without having to implement a complete OSGi framework.
There’s a technique for doing this, and it goes by the name of mocking.
OSGi is just a load of fancy class loaders! Oh, wait, we didn’t mean that sort of mocking.
Besides, we all know by now that there’s a lot more to OSGi than enhanced class loading.
We’re talking about using mock objects to test portions of code without requiring a complete system.
A mock object is basically a simulation, not a real implementation.
It provides the same API, but its methods are scripted and usually return expected values or additional mocked objects.
Object mocking is a powerful technique because it lets you test code right from the start of a project, even before your application is complete.
You can also use it to test situations that are hard to recreate with the real object, such as external hardware failures.
Let’s take this example and test it outside OSGi by mocking the API: verifying calls made to the API and scripting the desired responses.
We’ll show you how easy it is to script scenarios that may be hard to reproduce in a real container, look at mocking in a multithreaded environment, and wrap things up by reliably demonstrating the race condition mentioned in section 4.3.1
How might you use mocking to test an OSGi application? Let’s look at code from earlier in this book: the LogService lookup example from chapter 4 that contained a potential race condition.
It receives a context object in the activator start method, uses this context to get a service reference B, and uses this reference to get the actual instance C.
Each of these objects has a well-defined interface you can mock out, and the example code uses only a few methods from each API.
This is good news because when mocking objects you only need to simulate the methods that are used, not the complete API.
You already know that this code compiles against the OSGi API, and back in chapter 4 you even tried it on an actual framework.
But does it use the service API correctly? This is the sort of test that’s hard to write without mocking.
Sure, you can run tests on the container by invoking your code and checking the results as you did back in section 7.1, but this doesn’t tell you if the code is using the container the right way.
For example, the container won’t complain if you forget to unget a service after you’re done with it, but forgetting to do this skews service accounting and makes it look like your bundle is still using the service when it isn’t.
The container also doesn’t know if you use the result of getService() without checking for null.
Writing a test that’s guaranteed to expose this race condition on a live framework is hard, but trivial with mock objects.
How exactly does mocking help? Because mock objects are scripted, you can verify that the right methods are called in the appropriate order.
You can throw exceptions or return null values at any point in the sequence to see how the client handles it.
Typically, five steps are involved in mocking out an API:
You’ll use EasyMock (http://easymock.org/) in this example, but any mocking library will do.
You can also find a completed version of the unit test in the solution directory if you don’t feel like typing all this code.
Let’s go through each of the five steps in detail and mock out the OSGi API:
Create prototype objects for parts of the API that you want to mock out: BundleContext, ServiceReference, and LogService.
You can do this by adding the following lines to the empty test case:
You use a strict mock for the context, because you want to check the call sequence.
Script the expected behavior of the log client as it finds and calls the LogService:
Using your knowledge of the service API from chapter 4, you expect that the client will call your mock context to find a reference to the LogService, to which you respond by returning a mock service reference.
You expect the client to pass this reference back your mock context in order to get your mock LogService.
Finally, you expect the client to call your mock LogService with a valid log level and some sort of message string.
Use your mock objects, and pretend to be the OSGi container:
Consider the active lifecycle of an OSGi bundle: first it’s started, and some time later it’s stopped.
You don’t worry about mimicking the resolution stage in this.
You know the client will spawn some sort of thread to use the LogService, so you wait one second to give that thread time to make the call and pause.
Using sleep here isn’t ideal; later, you’ll see how you can replace it with proper handshaking.
Then, when the one second is up, you stop the client bundle.
The last step is to make sure you saw the expected behavior during the test:
This method throws an exception if the observed behavior doesn’t match.
At this point, you should have a complete test that compiles and runs successfully:
Excellent: you’ve confirmed that your client uses the OSGi API correctly when a LogService is available.
But what happens when a LogService isn’t available; does it handle that too?
As we mentioned back at the start of this section, mocking is a powerful testing technique because it lets you script situations that are hard to re-create inside a test environment.
Although it’s easy to arrange a test in an OSGi container without a LogService, it would be difficult to arrange for this service to appear and disappear at exactly the right time to trigger the race condition you know exists in your client code.
First, let’s test what happens when no LogService is available by adding the following expectation between your last expect and the call to replay:
This states that you expect the client to begin another call to look up the LogService, but this time you return a null reference to indicate no available service.
If you try to run the test now, it will fail because you don’t give the client enough time to make a second call before stopping the bundle.
Your log client pauses five seconds between each call, so you need to add five seconds to the existing sleep:
The client now gets enough time to begin a second log call, but the test still fails:
It appears that your client is using another method (getBundle()) on the BundleContext to find the owning bundle when no LogService is available.
If you look at the rest of the client code under chapter07, you’ll see that it uses this to get the bundle identifier when logging directly to the console.
You don’t mind how many times your client calls getBundle(), if at all, so let’s use a wildcard expectation:
You need to provide a new mock to represent your Bundle object.
This time, instead of simulating each method the client uses, you take a shortcut and use a nice mock on the first line.
Nice mocks automatically provide empty implementations and default return values.
You expect your log client to request this mock bundle from your mock bundle context after you return the null service reference, but it may ask for it zero or more times.
One last thing you must remember to do is add your mock bundle to the replay list.
With the new expectation in place and everything replayed, the test passes once more:
Every time you want to test additional log calls, you need to extend the sleep, which makes your tests run longer and longer.
You should try to replace it with some form of handshaking.
But even with handshaking, your log client will still pause for five seconds between each call.
If only you could replace the pause method while keeping the rest of the code intact.
You’re currently testing a simple log client that spawns a separate thread to make log calls.
Knowing how to test multithreaded bundles is useful, because people often use threads to limit the amount of work done in the activator’s start method.
As we mentioned at the end of the last section, the main difficulty is synchronizing the test thread with the threads being tested.
Up to now you relied on sleep, but this is a fragile solution.
Thankfully, the log client has an obvious place where you can add such a barrier: the protected pauseTestThread method, which currently puts the client thread to sleep for five seconds.
You could consider using aspect-orientated programming (AOP) to add a barrier to this method, but let’s avoid pulling in extra test dependencies and use an anonymous class to override it instead:
The anonymous class replaces the original pauseTestThread method with one that uses a countdown latch, initialized with the number of expected log calls.
Each time the client makes a log call, it calls pauseTestThread and counts down the latch.
When no more log calls are expected, the client thread suspends itself and waits for the rest of the test to shut down.
The test code only needs to wait for the latch to reach zero before it stops the client bundle:
The test includes a timeout in case the client thread aborts and can’t complete the countdown; but if everything goes as expected, the updated test finishes in under a second:
So far so good: all you have to do to test additional log calls is increment the latch count.
But what should you do if your client thread doesn’t contain a pause method or this method can’t be replaced or extended? Another solution is to add barriers to the mocked-out objects themselves by using so-called answer objects.
Answers let you perform basic AOP by intercepting method calls, which you can use to add synchronization points.
In this (incomplete) example, you script an answer that always returns a null service reference and use it to suspend the client thread whenever it makes this call.
This works as long as the client thread initiates the expected call at the right time and there are no problems with suspending the client in the middle of this call.
But it also leaves the client code untouched, which in this case means a five-second pause between log calls.
You’ll test another log call in the next section, so let’s stick with the original latch solution.
OSGi is dynamic: bundles and services may come and go at any time.
The key to developing a robust application is being able to cope with and react to these events.
You could deploy your bundles into a real framework and attempt to script events to cover all possibilities (we’ll look at this in more detail in section 7.3), but some scenarios require microsecond timing.
Remember the race condition we mentioned at the start of this section? This will be exposed only if you can arrange for the LogService to disappear between two method invocations—a narrow window.
Many factors can cause you to miss this window, such as unexpected garbage collection and differences in thread scheduling.
With mocking, you can easily script the exact sequence of events you want:
You begin by expecting another log call, so remember to bump the latch count up to three calls.
The LogService is still available at this point, so you return the mock reference.
The client is expected to dereference this by calling getService(), and at this point you pretend the LogService has vanished and return null.
You follow this by expecting another wildcard call to get the bundle, just as you did in section 7.2.3, because the log client may need it to do some alternative logging to the console.
You may want to compare it with the class in the solution subdirectory.
It covers normal and missing service conditions and the edge case where the service is there to begin with but quickly disappears.
Running it should expose the problem that you know is there but couldn’t re-create reliably on a real framework:
One tip: you’ll need to extend the test to expect calls to ungetService(), because the working example attempts to release the service reference after each successful call to getService()
In this section, you learned how to mock out the OSGi API and script different scenarios when testing bundle-specific code that uses OSGi.
Mocking helps you test situations that are next to impossible to recreate in a real container.
It also provides a counterpoint to the first section where you were running existing tests inside a real container on code that often had no dependency on OSGi at all.
The last section will attempt to harmonize both approaches, by explaining how to script modular tests and run them on a variety of frameworks.
In the previous section, you successfully mocked out the OSGi API and ran your tests without requiring a framework.
Of course, the less you depend directly on an API, the easier it is to mock.
Such components rarely need to use the OSGi API themselves, so testing becomes a matter of reconfiguring bindings to inject mocked-out dependencies in place of the original instances.
But as we discussed in section 7.1.1, eventually you’ll want to run tests on a real OSGi framework.
These container tests typically don’t increase your code coverage—any unit and mocked-out tests should have already tested the critical paths.
Instead, these tests verify that your code conforms to the container:
Advanced OSGi testing is it packaged correctly, does it follow the container programming model, does it use standard APIs?
You should run your tests on as many containers as possible to guard against container-specific behavior.
But keeping all these containers up to date and managing their different settings and configurations soon becomes tiresome.
The newly standardized OSGi embedding and launching API (discussed in chapter 13) helps, but it lacks features that would make testing on OSGi much easier: automatic test wrapping, dynamic bundle creation, and common deployment profiles.
Luckily, several recently released OSGi test tools provide all these features and more.
OSGi-enabled test tools bring other benefits because they embrace OSGi, such as improved test modularity and management.
You can use them to run a complete range of tests from basic unit tests, through various combinations of integration tests, all the way up to advanced management tests.
You’ll see a real-world example of this later that uses one of the more popular OSGi test tools called Pax Exam to test a service bundle in isolation, combined with client bundles, and finally with older versions of the same service to try out a proposed upgrade scenario.
Let’s begin with a brief review of the various OSGi test tools available today.
At the time of writing this book, three major test tools are available for OSGi:
All follow the same basic approach to building and deploying tests:
The Spring DM test support obviously works best with Spring-based applications.
Although you can also use it to test non-Spring applications, doing so requires several Spring dependencies that make it appear rather heavy.
Spring DM testing also only supports JUnit 3, which means no annotated tests.
DA-Testing, on the other hand, provides its own test API, optimized for testing service dynamics such as the race condition you saw in section 7.2.5
This makes it hard to move existing JUnit or TestNG tests over to DA-Testing, because developers have to learn another test API, but it does make dynamic testing much easier.
In this chapter, you’ll use Pax Exam from the OPS4J community, because we believe it’s a good general-purpose solution; but many of the techniques covered in this section can be adapted for use with the other tools.
One of Pax Exam’s strengths is its support for a wide range of different OSGi frameworks, which is important if you want to produce robust portable bundles.
Even with all this, there can be subtle differences between implementations.
Perhaps part of the specification is unclear or is open to interpretation.
On the other hand, maybe your code relies on behavior that isn’t part of the specification and is left open to framework implementers, such as the default Thread Context Class Loader (TCCL) setting.
The only way to make sure your code is truly portable is to run the same tests on different frameworks.
This is like the practice of running tests on different operating systems—even though the JDK is supposed to be portable and standardized, differences can exist, and it’s better to catch them during development than to fix problems in the field.
Unfortunately, many OSGi developers only test against a single framework.
This may be because they only expect to deploy their bundles on that particular implementation, but it’s more likely that they believe the cost of setting up and managing multiple frameworks far outweighs the perceived benefits.
This is where Pax Exam helpstesting on an extra OSGi framework is as simple as adding a single line of Java code.
You’ll continue to use Ant to run these tests, although Pax Exam is primarily Maven-based.
This means you need to explicitly list execution-time dependencies in build.xml, instead of letting Maven manage this for you.
Look at the fw subproject; it contains a simple test class that prints out various framework properties.
This tells JUnit to use the named test runner instead of the standard JUnit one.
The Pax Exam JUnit4TestRunner class is responsible for starting the relevant framework, deploying bundles, and running the tests.
The @Configuration annotation identifies the method that provides the Pax Exam configuration.
Right now, you ask it to deploy the standard OSGi compendium bundle B from Maven central in to the default framework.
It accepts a BundleContext argument that’s supplied by Pax Exam at execution time.
You use this bundle context to print out various properties, including the symbolic name of the test bundle C.
You should see something like the following, but with properties that match your system.
You may have noticed that the symbolic name of the test bundle is pax-exam-probe.
This bundle is generated at execution time by Pax Exam and contains your test classes.
The default container is Apache Felix, but you can easily ask Pax Exam to run the same test on other frameworks as well.
Pax Exam does the hard work of downloading the necessary JAR files and setting up any framework-specific configuration files.
You just need to sit back and rerun your test:
This time you should see three distinct sets of output, as shown here.
Listing 7.4 Using Pax Exam to run tests on multiple frameworks.
Notice how some of the properties vary slightly between frameworks, in particular the OS name.
This is a reminder of why it’s a good idea to test on a variety of frameworks: to make sure you aren’t depending on unspecified or undocumented behavior.
You just saw how easy it is to run a test on many different frameworks using Pax Exam.
But how well does it work with existing unit tests and existing test tools?
At the start of this section, we mentioned how OSGi test tools can help you modularize and manage tests.
Because Pax Exam integrates with JUnit as a custom runner, you can use it in any system that can run JUnit tests.
This means you can mix non-OSGi unit and integration tests with Pax Exam–based tests and have the results collected in one place.
The Configuration Admin Service is a compendium service that provides and persists configuration data for bundles.
This test directory contains mocked-out unit tests to test internal details along with Pax Exam integration tests to test the expected Configuration Admin Service behavior.
We’ve taken these tests and separated them into unit and integration tests so you can see the difference.
These are still considered unit tests because they don’t run inside an OSGi container.
You could bundle them into a fragment as you did in section 7.1 and deploy them using Pax Exam, in which case they would be called bundle tests.
Bundle tests are somewhere between unit and full-blown integration tests.
They test more than a single class or feature but don’t involve more than one bundle.
After you’ve tested your core functionality both inside and outside the OSGi container, you can move on to integration testing.
Integration tests are where you start to piece together your application and test interactions between individual components.
To test combinations of components, you need some way to compose them.
For standard Java applications, it can be tricky deciding which JAR files you need to load; but with OSGi applications, all the dependency information is available in the metadata.
Deployment becomes a simple matter of picking a set of bundles.
You can find the Apache Felix Configuration Admin Service integration tests under the it subproject.
You may wonder why there isn’t much output during the tests.
This is because you’ve set the local logging threshold to WARN.
To see more details about what Pax Exam is running, edit the local log4j.properties file, and change the threshold from WARN to INFO.
This integration test checks that the Configuration Admin Service implementation successfully records configuration data that is registered before the managed bundle starts.
One such method is used to set configuration data using the current Configuration Admin Service B.
The test creates and installs a managed bundle on the fly and waits for the configuration to be delivered to this managed bundle.
It makes sure the delivered configuration is correct C before removing the configuration.
The test waits for the managed bundle to be notified about this removal and verifies it was correctly notified D.
It almost looks like a unit test, except calls are being made between components instead of inside a single component or class.
The other tests under the it subproject follow the same basic pattern, which may be repeated several times:
Disrupt the state (by calling services, or adding or removing bundles)
Right now, the tests are only configured to run on Apache Felix, but let’s see if they also pass on other frameworks.
Pax Exam now runs each test three times—once per framework.
No failures or errors! The Apache Felix Configuration Admin Service implementation works the same on all three frameworks.
This shouldn’t be unexpected, because one of the goals driving OSGi is reusable modules.
When you find you need a particular compendium service, and your current framework doesn’t provide it, look around in case you can reuse a bundle from another site.
You can even use Pax Exam to try different combinations of compendium bundles.
Pax Exam makes integration testing as simple as unit testing, but like any good tool you have to be careful not to overuse it.
Each integration test has the overhead of starting and stopping an OSGi container, so the overall test time can build up as you add more and more tests.
People are looking into reusing containers during testing, but for some tests you need complete isolation.
Although work is being done to reduce the cost of each test, this cost will never be zero.
In practice, this means you should look carefully at your tests and try to get the most from each one.
Integration testing is normally considered the last phase of testing before starting system or acceptance tests.
You’ve tested each piece of functionality separately and tested that the pieces work together.
There’s nothing else to test before verifying that your application meets the customers’ requirements—or is there?
This book contains an entire chapter (chapter 10) about how to manage OSGi applications, so it’s clear that management is an important aspect.
You should reflect that by testing applications to make sure they can be successfully managed, upgraded, and restarted before releasing them into production.
Too often, we see bundles that work perfectly until they’re restarted or bundles that can’t be upgraded without causing ripples that affect the whole application.
What might management testing cover? Table 7.2 has some suggestions.
Install Installing new bundles (or features) alongside existing used implementations.
Uninstall Uninstalling old bundles (or features) that may or may not have replacements.
Upgrade Upgrading one or more bundles with new functionality or bug fixes.
Downgrade Downgrading one or more bundles because of an unexpected regression.
Graceful degradation Seeing how long the application functions as elements are stopped or uninstalled.
We’ll show you how OSGi and Pax Exam can help with management testing.
But what if you have an application that uses an earlier version? Can you upgrade to the latest version without losing any configuration data? Why not write a quick test to find out?
It’s based on the listConfiguration test from the existing Apache Felix integration test suite.
Listing 7.6 shows the custom configuration for the upgrade test.
You want to reuse the helper classes from the earlier tests, so you explicitly deploy the integration test bundle alongside your management test.
You also deploy the old Configuration Admin Service bundle and store the location of the new bundle in a system property so you can use it later to upgrade Configuration Admin Service during the management test.
You use a system property because the configuration and test methods are executed by different processes, and system properties are a cheap way to communicate between processes.
The rest of the test follows the same script as the original listConfiguration test with three key differences.
First, you make sure the installed Configuration Admin Service bundle is indeed the older 1.0.0 release, by checking the OSGi metadata:
Second, you do an in-place update of the Configuration Admin Service bundle to the new edition:
You perform an in-place update to preserve the existing configuration data in the bundle’s persistent data area (see section 3.3.4)
This works only when you’re upgrading bundles to a new version.
If you wanted to switch to a Configuration Admin Service implementation from another vendor, you’d need both bundles installed while you copied the configuration data between them.
Third, you make sure the Configuration Admin Service bundle was successfully updated to the new version before finally checking that the configuration data still exists:
You can run this management test with a single command from the chapter07/ testing- example/ directory:
You can even extend the upgrade test to make sure it works on other OSGi frameworks, as you did with the original Apache Felix Configuration Admin Service integration tests.
You’ll see that the test passes on all three frameworks, which is more proof that this service implementation is truly independent of the underlying OSGi framework.
Imagine building up a modular library of management actions (install, start, stop, upgrade, and downgrade) that you can quickly tie together to test a particular task.
Such management testing can help squash potential problems well in advance, minimizing real-world downtime.
Earlier in this chapter, we showed you how to test an application all the way up from individual classes to single bundles and combinations of bundles.
Just now, we looked at testing different management strategies, such as upgrading and downgrading components, to make sure the application as a whole (and not just this release) continues to behave over its lifetime.
At this point, you should be ready to move on to system and acceptance tests.
These tests don’t need special treatment regarding OSGi, because OSGi is just an implementation detail.
As long as the application can be launched, it can be tested.
This chapter covered three different approaches to testing OSGi applications:
In an ideal world, you’d use a combination of these three approaches to test all your code, both inside and outside one or more OSGi containers.
In the real world, projects have deadlines and developers need their sleep, so we suggest using tools such as Pax Exam to automate as much of the test-bundling and -deployment work as possible.
These tests should grow along with your application, giving you confidence that you do indeed have a robust, modular application.
You just learned how to test individual bundles and application deployments in OSGi, but what should you do when an integration test unexpectedly fails with a class-loading exception or a load test runs out of memory? If you were working on a classic Java application, you’d break out the debugger, start adding or enabling instrumentation, and capture various diagnostic dumps.
Well, an OSGi application is still a Java application, so you can continue to use many of your well-honed debugging techniques.
The key area to watch out for is usually related to class loading, but that’s not the only pitfall.
OSGi applications can have multiple versions of the same class running at the same time, requiring greater awareness of versioning; missing imports can lead to groups of classes that are incompatible with other groups; and dangling services can lead to unexpected memory leaks when updating bundles.
Debugging bundles you examples of how to debug all these problems and suggest best practices based on our collective experience of working with real-world OSGi applications in the field.
Say you have an application composed of many working bundles and one misbehaving bundle: how do you find the bad bundle and debug it?
Applications continue to grow over time—more features get built on top of existing functionality, and each code change can introduce errors, expose latent bugs, or break original assumptions.
In a properly modularized OSGi application, this should only lead to a few misbehaving bundles rather than a completely broken application.
If you can identify these bundles, you can decide whether to remove or replace them, potentially fixing the application without having to restart it.
But first, you need to find out which bundles are broken!
Take the paint example you’ve worked on in previous chapters.
Imagine that you get a request to allow users to pick the colors of shapes.
In order to compile against the new SimpleShape API, they need to implement this method; so from their perspective, this is a major change.
You should therefore increment the API version in the main paint example build.xml file to reflect this.
The last version you used was 5.0, so the new version is.
You now need to implement the setColor() method in each of the three shape bundles.
Listing 8.1 Implementing the setColor() method for the triangle shape.
This class lazily delegates to the real shape via the OSGi service registry, so it also needs to implement the new setColor() method.
You’ll use a broken implementation instead and assume you already have access to the shape instance:
This sort of mistake could be made by a new team member who doesn’t know about the lazy delegation approach and assumes that m_shape has been initialized elsewhere.
If the application happened to call draw() early on, this bug could go unnoticed for a long time, because m_shape would always be valid by the time the code reached setColor()
But one day, someone may reasonably change the application so it calls setColor() first, as follows from the ShapeComponent class, and the bug will bite.
This example may seem a little contrived, but it’s surprisingly hard to write bad code when you really want to.
You now have a broken OSGi application, which will throw an exception whenever you try to paint shapes.
Let’s see if you can debug it using the JDK provided debugger, jdb (http://java.sun.com/javase/6/docs/technotes/tools/solaris/jdb.html)
This means it lacks some of the polish and user-friendly features found in most other debuggers.
But jdb is still a useful tool, especially when you’re debugging on production servers that have limited installation environments.
Jdb starts up, but it won’t launch your application until you type run:
You should see the updated paint window appear, as shown in figure 8.1
All you had to do is use the jdb command instead of java and specify the class path and main class (the jdb command doesn’t support the -jar option)
You didn’t have to tell jdb anything about your bundles or the OSGi framework; from jdb’s perspective, this is just another Java application.
If you try to draw a shape in the paint window, jdb reports an uncaught exception in the AWT event thread:
Felix bundle cache If you happen to see several I/O exceptions mentioning the felix-cache, check that you haven’t got any leftover debugged Java processes running.
When you forcibly quit jdb using Ctrl-C, it can sometimes leave the debugged process running in the background, which in this case will stop new sessions from using the local felixcache directory.
You can ask it to stop the application when this sort of exception occurs again, like so:
Keep resuming the program until you see a long exception stack trace appear on the jdb console.
This isn’t a new exception: it’s the AWT thread printing out the original uncaught exception.
The top of the exception stack confirms that it was caused by your faulty code inside DefaultShape, which you know is contained inside the paint frame bundle.
Notice that jdb doesn’t give you a way to correlate the exception location with a particular JAR file.
What if you didn’t know which bundle contained this package? You could try to locate it using the console, but most framework consoles only let you see exported packages.
For internal packages, you would have to come up with a list of candidate bundles by manually checking the content of each bundle and comparing the exception location with the appropriate source.
As you’ll see in a moment, tracking a problem to a specific bundle is much easier when you use an OSGi-aware debugger, such as the Eclipse debugger.
Returning to the broken example, try to paint another shape.
Jdb now detects and reports the exception at the point at which it’s thrown inside setColor()
But because you haven’t attached any source files, it doesn’t show you the surrounding Java code:
When you print the current value of m_shape, you can finally see why it failed:
If you’re an experienced Java programmer this should be familiar; no special OSGi knowledge is required.
But take another look at the command where you attached your source directory:
This command has no knowledge of bundles or class versions; it merely provides a list of candidate source files for jdb to compare to debugged classes.
Jdb allows only one version of a given source file to be used at any one time, which makes life difficult when you’re debugging an OSGi application containing multiple bundle versions.
You have to know which particular collection of source directories to enable for each debug session.
If you use an IDE such as Eclipse, which knows that multiple versions of a class can coexist in the same JVM, you don’t have to worry about which source relates to which bundle version.
The IDE manages that association for you as you debug your application.
Now import these two directories into Eclipse as existing projects.
To debug these bundles in Equinox, the OSGi framework used by Eclipse, click the drop-down arrow next to the bug icon (circled at the top of figure 8.2), and select Debug Configurations.
Follow these instructions to configure a minimal Eclipse target platform for debugging the paint example:
When you’re happy with your selection, click the Debug button to launch the debugger.
Two different paint frames appear, as shown in figure 8.3
This is because you have two versions of the code running simultaneously in the same JVM.
Before you start to paint, let’s add a breakpoint so the debugger will stop when someone tries to use a null object reference.
You now have the two paint examples running in the Eclipse debugger.
If you try to paint with the original version, which has three shapes in its toolbar, everything works as expected.
But if you try to paint with the new version—the one with the paintbrush in its toolbar—the debugger stops (see figure 8.5)
The debugger has correctly identified that the affected source code is from chapter08 even though there are multiple versions of this class loaded in the Java runtime.
Again, the problem is caused by a null shape object.
Using the Eclipse IDE, you can trace the exception back to the specific bundle project.
You can also click different frames in the stack trace to see what other bundles (if any) were.
Compare this to jdb, where it was difficult to tell which bundles were involved in a given stack trace without a good understanding of the source distribution.
You’ve successfully debugged an OSGi application with existing tools, from the basic jdb debugger to the full-fledged Eclipse IDE.
But what do you do when you finally track down the bug? Do you stop your application, fix the code, and restart? What if your application takes a long time to initialize, or if it takes hours to get it into the state that triggered the bug in the first place? Surely there must be a better way!
Thankfully, there is an answer to the question we just asked.
HotSwap is a feature of the Java 5 debugging architecture that lets you change the definition of a class at execution time without having to restart the JVM.
The technical details behind HotSwap are outside of the scope of this book; what’s more interesting is whether it works correctly with OSGi.
To use HotSwap, you need to attach a native agent at startup to the low-level debugging hooks provided by the JVM.
One such agent is attached whenever you run an application under jdb.
Although jdb provides a basic redefine command to swap in.
Figure 8.5 (continued) Exception caused by a bad setColor() method.
Embedding source inside OSGi bundles OSGi defines a standard place for embedding documentation inside bundles: OSGIOPT.
Bundles containing source code under OSGI-OPT/src can be debugged in Eclipse even when you don’t have a project associated with them in your workspace.
Debugging bundles newly compiled classes, it won’t work for the previous example.
Jdb refuses to redefine classes that have multiple versions loaded, because it can’t determine which version should be redefined.
But what about Eclipse? Can it help you update the right version of DefaultShape?
In the previous section, you successfully used the Eclipse debugger to manage multiple versions of source code while debugging.
Will Eclipse come to the rescue again and let you fix the broken DefaultShape implementation while leaving earlier working versions intact? If you still have the Eclipse debugger instance running, you can skip to the next paragraph.
Trigger the exception again by attempting to paint a shape.
You should have the paint example suspended in the debugger at the point of failure, as you saw in figure 8.5
Unlike jdb, which has to be told which classes to redefine, the Eclipse debugger automatically attempts to redefine any class whose source changes in the IDE (provided you have automatic builds enabled)
For a quick solution, you can copy and paste the relevant code from the draw() method, as follows.
Copying code this way is fine for a quick debugging session, but it’s better to extract the initialization code into a common method for use by both the draw() and setColor() methods.
Reducing code duplication makes testing and debugging a lot easier.
For now, you’ll keep things simple: paste the code from listing 8.2 over the broken setColor() implementation.
What happened? Most, if not all, of you got an error message like the one in figure 8.6, saying the JVM couldn’t add a method to an existing class.
This happened because Eclipse tried to update both versions of the DefaultShape class.
Instead, the debugger attempted to add the setColor() method to the old class, but adding methods isn’t supported by the current Sun implementation of HotSwap.
If you debug the same example using IBM Java 6 as the target runtime (remembering, of course, to first revert the setColor() method back to the broken version), you can successfully fix the problem without restarting the process.
Figure 8.7 confirms that even after using HotSwap to squish the bug, both the old and new paint examples continue to work on the IBM JDK.
Although you eventually managed to use HotSwap to fix the problem in your bundle, this isn’t exactly what you want, because all versions of DefaultShape were updated.
By chance, this didn’t affect the old paint example because you were adding a completely new method.
It has no effect on the old application and sits there unused.
But what if you wanted to change a method that was tightly coupled to existing code? You could end up fixing one version only to find out you’d broken all the others by unintentionally upgrading them with the new logic.
This may not be a big deal during development, because you’ll probably be focusing on one version at a time; but can you do better when debugging OSGi applications in the field?
For those who don’t know, a JVM agent is a small native library that attaches to the process on startup and is granted low-level access to the Java runtime.
Whenever you recompile a class, JRebel automatically updates the appropriate version loaded in the JVM without affecting any other versions of the class.
This makes it easy to develop, debug, and compare different releases of an application at the same time.
What are the downsides? The main downside is reduced performance due to the extra tracking involved.
JRebel also needs to know how custom class loaders map their classes and resources to local files.
It currently supports the Equinox OSGi implementation, but there’s no guarantee it will work with other OSGi frameworks.
Finally, you need to add an option to the JVM command line to use it, which is problematic in production environments that lock down the JVM’s configuration.
Some places won’t let you use JVM agents at all because of the potential security issues involved.
Agents have access to the entire process and can redefine almost any class in your application.
Adding an agent to your JVM is like giving root access to a user in Linux.
For these reasons, JRebel is usually best suited to development environments.
But what if you’re working somewhere that forbids the use of debuggers or JVM agents? Is there any other way you can update the broken bundle without restarting the whole process?
Back in section 3.7, we discussed the update and refresh parts of the OSGi lifecycle.
Well, you can use them here to deploy your fix without having.
To see this in action, you first need to revert the setColor() method of the local DefaultShape class back once again to the broken implementation:
Also, add your command shell to the current set of bundles, so you can ask the framework to update the fixed bundle later:
First, confirm that you have the broken implementation installed by attempting to paint a shape (you should see an exception)
Then, in another operating system shell, fix the setColor() method of the DefaultShape class using the code from listing 8.2, and rebuild the paint frame bundle in a new window:
Go back to the OSGi console, and type the following:
Here, 6 is the ID of the paint frame bundle, as reported by the bundles command.
When you issue the update command, the framework updates the bundle content by reloading the bundle JAR file from its original install location.
It also stops and restarts the paint frame bundle, so you should see the paint frame window disappear and reappear.
The paint example is now using the fixed code, which means you can paint multicolored shapes as shown in figure 8.8
Notice that you didn’t need to follow the update with a refresh.
This is because the paint frame bundle doesn’t export any packages, so you know there are no other bundles hanging onto old revisions of the DefaultShape code.
Unlike JRebel, the OSGi update process doesn’t depend on a special JVM agent.
These reasons together mean you can use the OSGi update process in a production environment.
The downside is that you have to update and restart the entire bundle, potentially destroying the current state, rather than redefine a single class.
If you wanted to keep any previously drawn shapes, you would need to persist them somehow when stopping and restore them when restarting.
You’ve just seen how you can debug and fix problems in OSGi applications using everyday development tools such as jdb and Eclipse.
You looked at more advanced techniques, such as HotSwap and JRebel, and finally used the tried-and-tested OSGi update process to fix a broken bundle.
We hope these examples made you feel more comfortable about.
In the next section, we’ll take a closer look at a set of problems you’ll eventually encounter when using OSGi: class-loading issues.
OSGi encourages and enforces modularity, which, by its nature, can lead to class-loading issues.
Maybe you forgot to import a package or left something out when building a bundle.
Perhaps you have a private copy of a class you’re supposed to be sharing or forgot to make sure two tightly coupled packages are provided by the same bundle.
These are all situations that break modularity and can lead to various class-loading exceptions.
The right tools can help you avoid getting into these situations in the first place, but it’s still worthwhile knowing what can happen and what the resulting problem signatures look like.
The following sections take you through a number of common class-loading problems, what to look out for, what might be the cause, and how to solve them.
All the exceptions discussed in this section come from the same example application: a simple hub-and-spoke message system that uses the OSGi extender pattern (see section 3.4) to load spoke implementations at execution time.
The only thing that changes throughout this example is the.
By the end of this section, you should understand which class-loading issues can arise from simple changes in content and metadata and how you can diagnose and fix them when something goes wrong.
A subtle difference between these two types will help you understand why the exception occurred and how to fix it.
This could occur in a Java application for three main reasons:
You know all about public, protected, and private access; but how many of you know what package private means? Package-private classes are those without any access modifier before their class keyword.
Their visibility rules are unique: in addition to only being visible to classes from the same package, they’re also only visible to classes from the same class loader.
Most Java programs have a single application class loader, so this last rule hardly ever comes up.
OSGi applications contain multiple class loaders, but as long as each package is loaded by only one class loader, it’s effectively the same as before.
The real problem arises with split packages (see section 5.3), which span several class loaders.
Package-private classes from a split package in one bundle aren’t visible to fellow classes in other bundles.
Figure 8.10 shows three different package-private scenarios: one classic and two involving split packages.
This builds and deploys a spoke bundle with incorrect extender metadata concerning its implementation class: it lists the name as MySpokeImpl instead of SpokeImpl.
This is an easy mistake to make in applications configured with XML or property files because of the lack of type safety.
The resulting exception gives the name of the missing class:
You should use this information to check if the name is correct, the class is visible, and the package containing the class is either imported or contained inside the bundle.
The hardest problems involve third-party custom class loaders; you inevitably need access to the class loader’s source code to determine why it couldn’t see a particular class, as well as have the patience to unravel the exception stack.
First, this is an error rather than an exception, which means applications are discouraged from catching it.
Second, it means the initial class that started the current load cycle was found, but the class loader wasn’t able to finish loading it because a class it depends on was missing.
This can happen when a class is compiled against a dependent API, but the resulting bundle neither contains nor imports that package.
The runtime begins to load the spoke implementation but can’t find the named interface when defining the class:
Although the cause in this example is clear, developers often get side-tracked by assuming the initial class is at fault.
The real culprit may be hidden down at the bottom of the stack as the original.
Figure 8.11 summarizes the difference between the two missing-class exception types; together, they make up many of the class-loading issues you’ll encounter when using OSGi.
Unfortunately, these two exceptions don’t have a monopoly on confusing OSGi developers.
How many of you would expect a ClassCastException from the following code?
At first glance, it looks correct: you configure a service tracker to track services of type org.foo.Item and cast the discovered service, if any, to the same type.
Instead of calling the no-argument open() method as usual, you pass in a Boolean: true.
This tells the service tracker to track all services whose type name matches the org.foo.Item string, not just the ones that are classloader compatible with your bundle (we discussed a similar situation back in section 4.5.1)
If another bundle provides an Item service and happens to get the org.foo package from a different class space than you did, you’ll see a ClassCastException at the last line.
How can this be? Recall from chapter 2 that class loaders form part of a type’s identity at execution time, so the exact same class byte code loaded by two different class loaders is considered to be two distinct types.
This makes all the difference, because OSGi uses a class loader per bundle to support class-space isolation in the same Java runtime.
The spoke and hub end up using different class loaders for the same API class, which makes the spoke implementation incompatible with the hub:
OSGi frameworks sometimes label their class loaders with the bundle identifier, so calling getClassLoader()
You can also use the framework console to find out who’s exporting the affected package and who imports it from them.
Use this to build a map of the different class spaces.
The specific commands to use depend on the framework; at the time of writing this book, the OSGi Alliance is still standardizing a command shell.
On Felix, the inspect package command is the one to use.
On Equinox you would use the packages or bundle commands.
Once you understand the different class spaces, you can adjust the bundle metadata to make things consistent to avoid the ClassCastException.
Bundles must have a consistent class space to avoid running into classrelated problems, such as visibility or casting issues.
When you have two tightly coupled packages, it’s sometimes necessary to add uses constraints to make sure these packages come from the same class space.
Perhaps you don’t think you need all these uses constraints cluttering up your manifest—after all, what’s the worst that can happen if you remove them?
Let’s find out by running the fourth example in the class-loading series:
Yet again you get a class-loading exception, except this time it happens inside the spoke implementation.
The Java runtime notices that you attempted to load two different versions of the Message class in the same class loader—in other words, your class space is inconsistent:
How did this happen? Your new spoke bundle has an open version range for the hub API, which means it can import any version after 1.0
You may wonder what this package is doing in your spoke bundle—maybe you’re experimenting with a new design, or perhaps it was included by mistake.
The hub extender and test bundles still have the original, restricted version range:
They get Spoke and Message from the original API bundle, but your spoke bundle has.
This means it gets the original Spoke interface from the API bundle and the updated Message from itself.
Remember, the framework always tries to pick the newest version it can.
Thus the Spoke interface and your implementation see different versions of the Message interface, which causes the LinkageError in the JVM.
You must tell the framework that the various hub packages are related, so it can stop this mismatch from happening.
Solving class-loading issues you no longer see any exceptions or linkage errors:
You just saw how uses constraints can help you avoid inconsistent class spaces and odd linkage errors, but what happens if they can’t be satisfied? You can find out by tweaking the version range for org.foo.hub in the spoke bundle.
These two ranges are incompatible: there's no way you can find a solution that satisfies both.
Unfortunately, the framework exception doesn’t tell you which constraint failed or why.
Determining why a solution wasn’t found without help from the framework can be time consuming, because the search space of potential solutions can be large.
Fortunately, Equinox has a diag command to explain which constraints were left unsatisfied.
With Felix, you can add more details to the original exception by enabling debug logging.
For example, if you change the last line in the PICK_EXAMPLE script to.
The message tells you the unsatisfied constraint is related to the org.foo.hub package.
It also gives you the identifiers of the bundles involved.
This is another reason why it’s a good idea to use uses constraints.
Without them, you’d have to debug confusing class-loading problems with no support from the framework.
By using uses constraints, you can avoid linkage errors to begin with and help the framework explain why certain sets of bundles aren’t compatible.
But it can only do this if the constraints are valid and consistent, which is why we recommend you always use a tool to compute them, such as bnd.
So far, we’ve concentrated on what happens when your bundle metadata is wrong; but even a perfect manifest doesn’t always guarantee success.
Certain coding practices common to legacy code can cause problems in OSGi because they assume a flat, static class path.
One practice worth avoiding is the use of Class.forName() to dynamically load code.
Suppose you’re writing a module that needs to look up a class at execution time based on some incoming argument or configuration value.
Skimming through the Java platform API, you spot a method called Class.forName()
Give it a class name, and it returns the loaded class.
Perfect, right? Its ongoing popularity suggests many Java programmers agree; but before you sprinkle it throughout your code, you should know it has a flaw: it doesn’t work well in modular applications.
It assumes the caller’s class loader can see the named class, which you know isn’t always true when you enforce modularity.
How does this affect you as an OSGi developer? Any class you attempt to load using Class.forName() must either be contained, imported, or boot-delegated by the bundle making the call.
When you’re loading from a selection of known classes, this isn’t a big deal; but if you’re providing a general utility (such as an aspect-weaving service), there’s no way to know which classes you may need to load.
And even if you happen to know, you may decide to keep things flexible for the future.
If you remember our discussion on discovering imports from section 6.1.3, you may think this sounds like a job for dynamic imports:
But dynamic imports only work when the wanted packages are exported.
In addition, your bundle can get wired to many different packages in numerous client bundles.
If any one of these bundles is refreshed, your bundle will also end up refreshed, which in turn may affect the other bundles.
Finally, you can import only one version of a package at any one time.
If you want to work with non-exported classes or handle multiple versions of the same code concurrently, you need to find another way to access them.
Whenever you work with OSGi class loading, always remember that well-defined rules govern visibility.
Every loaded class must be visible to at least one class loader.
Your bundle may not be able to see the client class, but the client bundle certainly can.
If you can somehow get hold of the client class loader, you can use it to load the class instead of using your own class loader.
This job is much easier if the method arguments already include a type or instance of a type that you know belongs to the client.
Let’s see how easy it can be with the help of the sixth spoke implementation.
This spoke assumes each Message implementation has an accompanying Auditor class in the same package and uses reflection to access it and log receipt of the message.
The reason behind this design isn’t important; you can imagine that the team wants to support both audited and non-audited messages without breaking the simple message API.
What’s important is that by using Class.forName(), the spoke bundle assumes it can see the Auditor class.
But you don’t export your implementation packages, so when you run the example, we hope you aren’t too surprised to see an exception:
You know the Auditor sits alongside the Message implementation in the same package, so they share the same class loader (you don’t have any split packages)
You need to access the Message implementation class loader and ask it to load the class like so:
Remove the Class.forName() line from the spoke implementation in listing 8.3, and replace it with the previous line.
We don’t use it because there's a subtle but important difference between these statements:
In the last example, you found the client class loader by examining one of the arguments passed into your method and used that to look up the client’s Auditor class.
What if none of the method arguments relate to the client bundle? Perhaps you can use a feature specifically introduced for application frameworks in Java 2: the Thread Context Class Loader.
The Thread Context Class Loader (TCCL) is, as you may expect, a thread-specific class loader.
Each thread can have its own TCCL; and, by default, a thread inherits the TCCL of its parent.
You can access the TCCL with a single line of Java code:
The TCCL is useful when you’re writing code that needs dynamic access to classes or resources but must also run inside a number of different containers such as OSGi.
Instead of adding a class-loader parameter to each method call, you can instead use the previous code to access the current TCCL.
All the container needs to do is update the TCCL for each thread as it enters and leaves the container.
When done properly, this approach also supports nesting of containers, as shown in figure 8.13
You should use a try-catch-finally block to guarantee that the correct TCCL is restored even if an exception or error occurs somewhere inside the container code:
The initiating class loader is used to initiate the load request.
It may delegate through several class loaders before finding one that has already loaded the class or can load it.
The class loader that defines the class (by converting its byte code into an actual class) is called the defining class loader.
The result of the load request is cached in the defining class loader in case anyone else wants this class.
Although it behaves like loadClass() when looking for new classes, it caches the result in both the defining and initiating class loaders.
It also consults the initiating loader cache before delegating any load request.
With loadClass(), the resulting class can depend on your context, perhaps according to which module you’re currently running in.
But with forName(), you get the same result regardless of context.
Because this extra caching may lead to unexpected results in a dynamic environment such as OSGi, we strongly recommend you use loadClass() instead of forName()
Let’s see how the TCCL can help you solve a class-loading issue without affecting the API.
You should see an exception when the spoke attempts to load the Auditor class:
If you look at this spoke implementation, you’ll see that it uses the TCCL, as shown here.
As long as the TCCL is assigned properly by the container or the caller, this should work.
The OSGi standard doesn’t define what the default TCCL should be: it’s left up.
This example uses Apache Felix, which leaves the default TCCL unchanged; in other words, it’ll be set to the application class loader.
To avoid this exception, you need to update the TCCL in the test bundle before sending the message.
To be consistent, you should also record the original TCCL and reset it after the call completes.
This last step is important if you want to nest or share containers inside the same process, as you saw in figure 8.13
This listing saves the old TCCL, sets the new TCCL, and then restores the old TCCL.
With these three changes, you can rerun the test without any class-loading problems:
You used the same example code to see a wide range of different exceptions you may encounter when developing OSGi applications.
We hope this will provide you with a foundation for any future class-loading investigations.
If you can relate a particular exception with one of the examples here, the associated solution will also help fix your problem.
Unfortunately, class loading isn’t the only problem you’ll encounter when working with OSGi, but the next topic we’ll look at is indirectly related to class loading.
An OSGi application contains several class loaders, each one holding on to a set of resources.
Unused class loaders are freed as bundles are uninstalled and the framework is refreshed, but occasionally a rogue reference keeps a class loader and its associated resources alive.
Memory leaks occur in OSGi applications as in any other Java application.
All you need is something like a rogue thread or static field hanging on to one end of a spaghetti ball of references to stop the garbage collector from reclaiming the objects.
In a desktop Java application, you may not notice any memory leaks because you don’t leave the application running for a long time.
As soon as you restart the JVM, your old application with its ever-growing heap of objects is gone, and you get a brand-new empty heap to fill.
Server-side OSGi applications, on the other hand, can have longer lifetimes; an uptime of many months isn’t unreasonable.
One of the strengths of OSGi is that you’re able to install, update, and uninstall bundles without having to restart the JVM.
Although this is great for maximizing uptime, it means you have to be careful not to introduce memory leaks in your bundles.
You can’t always rely on the process being occasionally restarted.
Furthermore, updating a bundle introduces a new class loader to hold the updated classes.
If there’s anything holding on to objects or classes from the old class loader, it won’t be reclaimed, and your process will use more and more class loaders each time the bundle is updated or reinstalled.
Any leak is a cause for concern, but depending on your requirements, not all leaks warrant investigation.
You may not even notice certain leaks if they add only a few bytes to the heap every now and then.
What’s the best way to find leaks in an OSGi application?
As with debugging, you can continue to use your existing heap analysis skills to examine OSGi applications.
Sure, there are more class loaders than in a normal Java application; but standard Java EE containers also contain multiple class loaders, and that doesn’t stop developers from finding memory leaks inside web applications.
Let’s see what an OSGi application heap dump looks like.
It consists of a single bundle that creates and accesses a ThreadLocal variable every time the bundle starts and fails to remove it when the bundle stops.
Introducing the PermGen heap Class-loader leaks can be more problematic than simple object leaks because some Java runtimes, like Sun’s HotSpot JVM, place classes in a separate heap space called the Permanent Generation, or PermGen for short.
This class heap is much smaller than the main object heap, and its capacity is controlled by a different GC setting: -XX:MaxPermSize.
If every bundle update adds hundreds of new class revisions without unloading anything, you’ll probably exhaust the PermGen before you run out of object heap space.
Following recommended practice, the ThreadLocal is a static member of the class.
This is safe because the JVM guarantees to supply a distinct instance of the data object for each thread accessing the ThreadLocal.
But how does forgetting to remove the ThreadLocal cause a memory leak? If you read the ThreadLocal Javadoc, you may expect the JVM to clear up stale references (http://java.sun.com/javase/6/docs/api/java/lang/ThreadLocal.html):
Each thread holds an implicit reference to its copy of a thread-local variable as long as the thread is alive and the ThreadLocal instance is accessible; after a thread goes away, all of its copies of thread-local instances are subject to garbage collection (unless other references to these copies exist)
If the bundle has been updated and the framework refreshed, surely the stale data object is no longer accessible and should be removed, right? Unfortunately, the Java 5 ThreadLocal implementation has a subtle behavior that causes it to hang on to values longer than is strictly necessary.
As you’ll soon see, missing the remove() call in stop() means that the data object is kept alive indefinitely because you don’t use any other ThreadLocals in the example.
In the worst case, even this isn’t guaranteed to purge all stale thread-local map entries.
Here, 1 is the ID of the leaky bundle, as reported by the bundles command.
You should see the heap expand each time you call update:
If you continue to update the bundle, you’ll eventually get an OutOfMemoryError:
You should now have a heap-dump file in your current working directory.
Plenty of open-source tools work with heap dumps; in this case, you’ll use the Eclipse Memory Analyzer (MAT, http://eclipse.org/mat/)
This tool provides a graphical view of the heap and several useful reports to quickly identify potential leaks.
Figure 8.14 shows the leak suspect report for the captured heap dump.
Notice how it correctly identifies the ThreadLocal as the root of the leak.
But can it tell you what application code was responsible? To find out, click the Details link at the bottom.
Doing so opens a detailed page about the ThreadLocal, including the various thread stacks that created the instances (see figure 8.15)
It clearly shows the bundle activator start() method is the one responsible for creating all these instances.
With your knowledge of the OSGi lifecycle, you can infer that the problem is in the activator stop() method.
To solve this leak, all you need to do is go back to the bundle activator and add a call to remove() the ThreadLocal in the stop() method.
This forces the underlying data object to be cleared and means the bundle’s class loader can be collected on each update/refresh.
You should now be able to continually update the bundle without incurring an OutOfMemoryError.
This example shows that analyzing heap dumps from OSGi applications is similar to analyzing dumps from everyday Java applications.
You’ve also seen that misbehaving code can cause memory leaks in OSGi as with any other container.
In addition to the everyday leaks Java developers have to be careful of, the OSGi framework introduces a new form of memory leak to trap the unwary: dangling services.
In section 4.3.1, we discussed why it’s a bad idea to access a service instance once and store it in a field: you don’t know when this service is unregistered by the providing bundle.
Your bundle continues to keep a strong reference to the original service instance and its entire graph of references long after the providing bundle has been updated or uninstalled (see figure 8.16)
You’re also keeping alive the class loaders of any classes used by this instance.
As with many memory leaks, you can end up with a significant amount of space being kept alive by a single field.
Clearing this field frees everything and allows your application to continue running.
How do you find this one field in the metaphorical haystack that is your application?
In an ideal world, your application won’t resemble a haystack! Often, you’ll have some idea where the leak may be, because of the bundles involved.
For example, if bundle A leaks when it’s updated, and you know that it’s used only by bundles X and Y, you can.
This is another benefit of modularity: by enforcing module boundaries and interacting indirectly via the service registry, you reduce the contact points between modules.
You no longer have to read through or instrument the entire code base for potential references, because different concerns are kept separate from one another.
But regardless of how much code you have to look through, you can use a couple of techniques to narrow the search, ranging from high-level queries to low-level monitoring.
You can perform high-level monitoring by using facilities built into the OSGi framework to track service use.
The Bundle API has a method called getServicesInUse() to tell you which services the OSGi framework believes a given bundle is actively using at any given time.
Remember from chapter 4 that this is done by tracking calls to getService() and ungetService()
Unfortunately, many developers and even some service-based frameworks don’t call ungetService() when they’re done with a service, which can lead you to think there is a leak where there isn’t one.
This approach also doesn’t detect when a direct reference to the service escapes from the bundle into some long-lived field.
You can also use the getUsingBundles() method from the ServiceReference API to perform a reverse check and find out which bundles are using a given service, but this too doesn’t account for incorrectly cached instances.
There are open source agents that can analyze the heap to find leak candidates.
It should be possible to take these generic agents and develop them further to add knowledge about OSGi resources, so they can watch for references to OSGi service instances on the Java heap and determine which bundle is responsible for holding on to them.
A recent example of this is the OSGi inspector agent (http://wiki.github.com/mirkojahn/OSGi-Inspector/)
Just as you saw when debugging, it’s one thing to find out why something is happening; being able to do something about it (and, in this case, protect against it) is even more important.
The simplest way to protect against dangling services is to let a component framework such as Declarative Services manage services for you.
But even component frameworks may not be able to help against rogue clients that stubbornly refuse to relinquish references to your service.
You somehow need to give these bundles a reference that you can clear yourself, without requiring their cooperation.
One way to do this is by using a delegating service proxy.
A delegating service proxy is a thin wrapper that implements the same set of interfaces as the original service.
It contains a single reference to the real service implementation that can be set and cleared by methods only visible to your registering bundle.
By registering this delegating proxy with the service registry instead of the real service implementation, you stay in control.
Because client bundles are unaware of the internal indirection, they can’t accidentally keep a reference to the underlying service.
As figure 8.17 shows, you can decide to sever the link at any time.
Notice that there's still a small leak, because the rogue client maintains a strong reference to the service proxy.
But this should be much smaller than the graph of objects and classes referenced by the actual service implementation; otherwise, you don’t gain much by using a proxy.
You can see an example of a service proxy in the code examples:
It caches the log service instance in a field and repeatedly calls it every few seconds:
Try stopping the log service by going to the OSGi console and typing.
You should see an exception when the log client next calls the service:
In an ideal world, this would make the client take action and clean up after itself.
If it doesn’t, the only leak is the service proxy.
But what does the service proxy look like? The following listing shows the sample implementation.
You use JDK reflection to create the proxy, because this approach is less error-prone than creating a new implementation by hand and delegating each method individually.
The proxy is defined in the same space as the LogService class and provides the same API.
Active logger instances are tracked with an internal shared map.
You use reflection to delegate method calls to active loggers and throw exceptions for inactive loggers.
You could manually create delegating service proxies up front, but doing so would only make sense for small numbers of services.
For large systems, you want a generic service that accepts a set of interfaces at execution time and returns the appropriate delegating service proxy.
There’s some overhead involved in both memory and performance, so you may only want to consider using a delegating service proxy only when you don’t trust client bundles to do the right thing or your service uses so many resources that even a single leak could be dangerous.
We started this chapter with a practical guide to debugging OSGi applications using the console debugger (jdb) and an advanced IDE (Eclipse)
We then moved on to specific issues you may encounter while working with OSGi, including seven classloading problems:
This was followed by a couple of related resource discussions:
The next couple of chapters should be a welcome break from all this low-level debugging and testing.
Look out for fresh, high-level concepts as we discuss managing OSGi bundles and applications!
You know how to use modularity to improve the cohesiveness of your application code; how to use lifecycles to bring dynamic installations and updates to application environments; and how to use services to decouple your modules via interface-based programming techniques.
You’ve also learned approaches and techniques for creating, testing, and debugging bundles.
In this chapter and the next, we’ll move our focus away from coding bundles to issues of managing bundles and OSGi-based applications.
With the OSGi Service Platform, your deployed set of bundles becomes your application’s configuration.
As such, the task of managing bundles is one of the Managing bundles.
Versioning packages and bundles most important skills you’ll need to fully master OSGi.
In this chapter, we’ll explore different aspects of bundle management, including the following:
With these skills, you’ll be better equipped to deploy and manage bundles in various application configurations.
From what you’ve learned so far, you know that versioning is a core part of any OSGi application.
When the framework resolves bundle dependencies, it takes these versions into account.
In this section, we’ll discuss the recommended policy for versioning these artifacts and discuss advantages and disadvantages of different versioning strategies.
To get things started, let’s provide some motivation for OSGi’s approach to versioning.
OSGi, on the other hand, treats versioning as a first-class citizen, which makes it easier to handle versioning in a meaningful way.
This emphasis on versioning means that a proper versioning strategy is important for maintaining application consistency.
You must be thinking, “Hey! I already version my JAR files!” Tools like Maven and Ivy let you specify versions for JAR files and declare dependencies on those versions.
You know that module-level dependencies are brittle when it comes to expressing finegrained dependencies between units of code.
Likewise, applying versions only at the module level has some drawbacks.
Such a model is too simple and forces all packages in a JAR file to be versioned in lockstep with the other packages.
Let’s look at some of these issues in more detail.
Consider a case where you bundle related packages together and assign a version number to the resulting JAR file.
Later, you may need to alter some code in one of the contained packages; such a change may be the result of a bug fix or a change to the API contract.
This new JAR file needs a new version number associated with it.
With a single version number for all the packages, it’s left to upstream users of the JAR file to decide whether the change warrants their making the update.
Because the only information they have is the module-level version-number change, it’s often a stab in the dark as to whether the updated functionality is required for their application.
Upstream users don’t typically use all the functionality provided by a JAR file and depend on only a subset of it.
Depending on which subset they use, it’s possible that nothing of importance has changed for them.
A counterargument is that if the bundle is highly cohesive, it makes no sense to update a single package without its siblings.
Although this is true, it’s not uncommon for JAR files to be less than optimally cohesive.
These constraints ensure that the cohesiveness of packages is maintained by capturing internal package dependencies.
This means upstream users aren’t forced to depend on anything more than the API-level contract of the exported packages.
Luckily, in OSGi, you can version your packages either independently or in lockstep with the bundle, as shown in figure 9.1
The OSGi approach of package-level versioning and dependencies leads to less churn in the development lifecycle.
Less churn implies less risk, because existing modules are better understood than updated modules, which may introduce unexpected behavior into a complex system.
This concept is extremely powerful and removes a lot of the pain from assembling applications out of independent JAR files, because you can make better-informed decisions about when and what to update.
Package-level versioning is also helpful when it comes to running different versions side by side.
Java doesn’t explicitly support this by default, but OSGi does.
In many cases, this seemingly unimportant feature frees you from worrying about backward compatibility or changes to artifacts outside your control.
Your bundles can continue to use the versions of packages with which they’re compatible, because your entire application no longer has to agree on a single version to place on the class path.
Versioning must be done as a core task throughout the development process, not as an afterthought.
Versioning packages and maintaining a versioning policy is a lot of work.
One easy way to reduce the amount of work is to have less to version.
In OSGi, you have the option to not expose the implementation packages of a bundle (assuming that no other bundle needs them)
As a consequence, the simplest option you have is to not export packages to avoid the need to version them.
When you need to export packages, then you need to version them.
Let’s look more closely at how you can implement a versioning policy for packages in OSGi.
As we mentioned previously, the OSGi specification doesn’t define a versioning policy, which means you can use any scheme that makes sense to you.
But the OSGi specification does recommend the following policy behind version-number component changes:
Why? Versions are important for the consumer to specify what’s needed, and this policy makes it possible to easily express a floor and a ceiling version in between which all versions are acceptable.
As you saw in chapter 2, a version range is expressed as a pair of version numbers inside braces or parentheses.
This follows mathematical interval notation, where a square brace signifies an inclusive value and a parenthesis signifies an exclusive value.
As an example, consider a typical definition of a package import:
Being able to specify such ranges is useful because the import can be satisfied by a wider range of exports.
This scheme works only if producers and consumers operate with a shared understanding of the kind of compatibility being expressed by a given version number.
The recommended OSGi versioning policy sounds good, and it’s been used successfully by many projects.
But new users should still take care due to a subtlety regarding the use of Java interfaces, which is related to whether an interface is being used or implemented.
The difference seems trivial, but it becomes important in the context of versioning.
If you’re in control of all the bundles, you can define a policy to ensure that method addition always causes a change in the major version number, which allows all consumers of a package to use a [1.0,2.0) version range.
In reality, you’re unlikely to be in control of all the bundles.
Furthermore, such a drastic policy would limit the reusability of your bundles, because consumers only using the interfaces would have no way to express that they’re fine with an added method.
Figure 9.2 Impact on versioning between using and implementing the interface.
You don’t want to define your versioning policy on a bundle-by-bundle basis.
So, whether you follow the recommended approach or not, you should at least try to use the same policy globally.
This gives you a fairly good understanding of versioning policy for packages, but what about versioning bundles? We’ll explore bundle-versioning policies next.
Bundles and packages are related through containment: bundles contain packages.
Because both bundles and packages have version numbers, what is the relationship between them? You need to adopt a versioning policy to define this relationship.
In the simple case, a bundle may contain several related implementation packages, all with the same version number.
Here it’s advisable to make the bundle version mirror the version of the implementation packages.
When you’re dealing with a bundle containing packages of different versions, the most consistent versioning policy is to increment the bundle version based on the highest change of a package inside it.
For example, if any package has a major number increase, the major number of the bundle should increase as well; the same is true if the highest change was to a minor or micro portion of the version.
With this policy, it’s possible to judge the impact of an updated bundle based on its version number.
Unfortunately, this may not always make sense, especially if the versions of the individual packages represent a well-known product version.
For example, let’s assume you want to create a bundle for the core API of the OSGi framework.
In this case, you have several independently versioned packages, but the.
A refined approach The best strategy devised so far is to shift the burden to the consumer.
Now, the question is, what version should you assign to the org.osgi.core bundle? There’s no single answer.
You would then need to use the minor number to match the release number of the specification.
Because the OSGi specification has also had minor number releases (such as 4.1), you would then need to use the micro number for the minor number of the specification.
Unfortunately, this wouldn’t be exactly what you want either, because there have been updates in the minor numbers of the contained packages.
To make matters worse, if you ever needed to update the bundle for a different reason (like a packaging mistake), then you’d need to use the qualifier to express that the bundle had changed.
Although there’s no single policy you can prescribe for versioning bundles, at a minimum you should try to reflect incompatible changes at the package level in your bundle version number.
The management task to take away from this section is that versioning is important and shouldn’t be left as an afterthought.
If done correctly, the OSGi concept of versioning is extremely powerful and removes a lot of the pain from assembling applications.
To get it right, you need to define a versioning policy and enforce it on all your bundles and exported packages.
Figure 9.3 The platform implementation contains many subpackages that must evolve in step with the specification, but what’s the version of the implementation?
With versioning covered, let’s look into another important management task: configuring your bundles.
To make your bundles more reusable, it’s a good idea to introduce configuration properties to control their behavior.
Recall from chapter 3, when we introduced the shell example, that you used configuration properties to alter its behavior, such as the port on which it listened for client connections.
Configuring bundles is an important aspect of using them, so it would be beneficial if there was a standard way of managing this.
At a minimum, it would be nice to have the following:
Fortunately, the OSGi Alliance defines the following three compendium specifications to help you address these issues:
Even with these specifications to help you, adding bundle configurations to the mix creates more issues for you to worry about.
For example, you have to make sure to consider this data when you change the bundles in your systems, because configuration data generally isn’t compatible across bundles or even bundle versions.
The data is subject to deployment and provisioning just like bundles.
It allows you to set the configuration information of deployed bundles.
You use this service to set a bundle’s configuration data, and it ensures that the bundle receives the data when it becomes active.
Consider the scenario in figure 9.4, where a bundle needs an integer port number and a Boolean secure property.
In this case, you provide these values to the Configuration Admin Service, and it provides these values to the bundle when it’s activated.
Using this approach, bundles have a simple, standard way of obtaining configuration data.
How does this work? The Configuration Admin Service maintains a database of Configuration objects, each of which has an associated set of name-value pair properties.
If you have a bundle that needs configuration data, it must register one of these two services defined in the Configuration Admin specification.
When you’re registering one of these managed services, you need to attach a service.pid (service persistent identity) service property to it.
Each managed Configuration object also has a service.pid associated with it, which the Configuration Admin Service uses as a key to match configuration data to the bundle needing it.
You may have noticed you’re dealing with two conceptually different layers when using the Configuration Admin Service.
On the other layer, you have a bundle and the services it provides that you want to configure.
Figure 9.4 An administrator configures a bundle in the framework by interacting with the Configuration Admin Service.
This approach decouples the administrator from having to know the internal workings of the bundle using the configuration data.
What is a PID? In a nutshell, you can associate a persistent identity, or PID, with each registered service by specifying it in the service property dictionary when you register its managed service.
If you specify a service.pid property, it must be unique for each service.
Its purpose is to uniquely and persistently identify a given service, which allows the Configuration Admin Service to use it as a primary key for bundles needing configuration data.
As a convention, PIDs starting with a bundle identifier and a dot are reserved for the bundle associated with that identifier.
You’re free to use other schemes for your PIDs; just make sure they’re unique and persistent across bundle activations.
Configuring bundles connects these two layers together when it delivers the configuration data.
Of course, the reverse is also possible, and the Configuration Admin Service may tell a managed service that its configuration has gone away, which means it needs to stop performing its functionality because it no longer has a valid configuration.
This approach gives you a flexible system, where you can configure and control any kind of service or any number of service instances in a common way.
Let’s look into the details of implementing a managed service next.
Now that you understand the underlying basics of how the Configuration Admin Service works by associating configuration data to managed services, let’s explore an example.
The actual interface you need to implement looks like the following:
The argument to this method is a Dictionary containing the configuration properties.
In this example, a simple echo server listens on a port and sends back whatever it receives.
When you receive a new configuration, you first stop the existing server, if there is one.
Then, you check whether you received a null configuration, which indicates that the previous configuration was deleted and there is no new one.
If this is the case, there’s nothing else to do.
Otherwise, you get the port number from the dictionary and verify its existence.
If it exists, you parse it and create and start a new server for the given port.
A bundle can register any number of ManagedService services, but each must be identified with its own PID.
You should use a ManagedService when configuration is needed for a single entity in the bundle or where the service represents an external entity like a device.
Then, for each detected device, a ManagedService is published with a PID related to the identity of the device, such as the address or serial number.
Remember, with a ManagedService, there’s only one configuration: the configuration for the specific PID.
Configuration properties A configuration dictionary contains a set of properties in a Dictionary object.
The name or key of a property must always be a String object and isn’t case sensitive during lookup, but preserves the original case.
For arrays and collections, they must only contain values of the same type.
Configuring bundles the same factory can have any number of configurations.
Using this approach, you can instantiate a service for each configuration associated with your managed service factory, for example.
This way, by creating a new configuration for the managed service factory, you create new service instances.
A slightly different use case is related to services representing entities that can’t be identified directly, such as devices on a USB port that can’t provide information about their type.
This way, the Configuration Admin Service can differentiate between a managed service factory and a managed service.
For the managed service factory, it assigns a new and unique PID to each created configuration for the factory.
Because you’re going to manage a number of servers, you introduce a map to hold them.
The factory interface defines two new methods, deleted() and getName()
The latter is a descriptive name for the factory, and the former notifies your factory that a previously updated configuration has gone away, which results in you stopping the corresponding server.
Notice that the updated() method has a different signature from the ManagedService interface B.
It now accepts a PID, which is necessary because your managed service factory needs to know the PID for the supplied configuration; it correlates the PID with a specific echo server.
For each one, you need a PID and a configuration.
The rest is similar to what you did for a single server in the ManagedService example.
The only exception is that now you must add the resulting server instance to your list of servers C.
This covers the basics of what you need to do to make your bundles configurable.
Now we need to look into how you configure bundles by creating configurations.
It’s one thing to make your bundles configurable, but you need some way to specify and set the property values you want to use to configure them.
You need to learn how to create and manage configurations; you use the Configuration Admin Service for this.
It provides methods to maintain configuration data by means of Configuration objects associated with specific configuration targets that can be created, listed, modified, and deleted.
To illustrate how these all fit together, you continue to improve the shell example in the following listing by creating a new command to manage configurations.
In this example, you create a cm command that accepts five different subcommands: list, add-cfg, remove-cfg, add-factory-cfg, and remove-factory-cfg.
The code is largely responsible for delegating to private methods to perform the functionality of the subcommands.
The following code shows how cm list lists available configurations.
You can optionally specify an LDAP filter to limit which configurations are returned; specifying no filter results in all configurations.
In either case, an array of Configuration objects is returned, which are the holders of the actual configuration properties.
Then you print the configuration properties, using the getProperties() method of the Configuration object to retrieve them.
You can use the add-cfg subcommand to create new Configuration objects.
To create a Configuration object, you call getConfiguration() on ConfigurationAdmin.
This method creates the Configuration object on the first call and returns the same object on subsequent calls.
This sets the Configuration object’s bundle location to null, which means it isn’t currently associated with any bundle.
You finish initializing the new configuration by getting any existing properties, parsing the specified properties and merging them with existing properties, and finally updating the configuration.
Because you handle existing properties, you can use the add-cfg subcommand to create and modify configurations.
You can use the remove-cfg subcommand to remove Configuration objects.
The subcommand accepts a PID that you use to get the Configuration object from the ConfigurationAdmin service.
When you have the Configuration object, you call delete() on it.
The add-factory-cfg subcommand creates a Configuration object for a managed service factory.
You can obtain this location via the calling bundle’s getLocation() method.
Location binding is a security feature to ensure that only management bundles can modify configuration data, and other bundles can only modify their own configuration data.
If the bundle location of a configuration for a given PID is set to null (as in listing 9.5), the Configuration Admin Service binds the first bundle registering a managed service with the given PID to this configuration.
After the bundle location is set, then configurations for the given PID are only delivered to the bundle with that location.
When this dynamically bound bundle is subsequently uninstalled, the location is set to null again automatically so it can be bound again later.
The remove-factory-cfg subcommand allows you to remove a factory configuration.
When you have it, you call delete() on it as before.
Type ant to build the example and java -jar launcher.jar bundles to execute it.
This session creates configurations for your managed service and managed service factory.
As you should be aware now, the result of these two commands is subtly different.
The first directly configures the service associated with the PID, whereas the latter causes a service to be created from the managed service factory.
For the combined example, if you go to another operating system shell after performing the first two steps, you can telnet into your configured echo servers using the specified port numbers.
That finishes our quick tour of the Configuration Admin Service.
You should now be able to use Configuration Admin to create externally configurable bundles, instantiate services using configurations, and manage configurations.
But wait, how do you know what kind of data your configurable bundles accept? All we’ve said so far is that managed services are configured with simple name-value pairs.
Sometimes that may suffice, but often you may want to tell other bundles or entities, such as a user, about the structure of your bundle’s configuration data.
The Metatype Service, which we’ll introduce next, allows you to define your own metatypes and associate them with your bundles and services.
Assume for a moment that you’re deploying a new bundle for the first time into a framework that has your Configuration Admin shell command available.
If this new bundle provides some services that are configurable, you can use your shell command to configure it, right? Unfortunately, because this bundle is new to you, you have no idea which properties it accepts, nor which ones are required for it to operate.
In this kind of scenario, it would certainly be helpful if the bundle could convey to you what a valid configuration look likes.
It aggregates metatypes (descriptions of types) contributed by bundles and allows others to look up these definitions.
Using this service allows you to introspect what a managed service accepts as a valid configuration and also validate configurations against these schema, which are subject to the same update and versioning mechanisms as the bundles that provide them.
As you can see in figure 9.6, there are two ways to provide metatype information about your managed services:
A bundle can contain XML resources in its OSGI-INF/metatype directory, which are picked-up by the Metatype Service using the extender pattern.
A managed service can implement a second interface called MetaTypeProvider.
If for some reason a bundle does both, only the XML resources are considered, and the MetaTypeProvider service is ignored.
From a client perspective, the Metatype Service defines a dynamic typing system for properties.
This allows you, for example, to construct reasonable user interfaces dynamically.
The service itself provides unified access to the metatype information provided by deployed bundles.
An object class contains descriptive information and a set of namevalue pairs.
Here’s what this looks like for the example echo server:
You specify an attribute definition (AD) to describe the configuration properties the echo server needs.
Notice the Designate element: this is where you make the link between the type (the OCD) and the instance (the PID)
To use metatype information, you use the Metatype Service to look up metatype definitions.
Using the discovered metatype information, you can generate user interfaces or validate configurations, for example.
To demonstrate how to use the Metatype Service, let’s add a type command to the shell to display metatype information, as follows.
The type command accepts a bundle identifier as an argument.
You get the MetaTypeService and retrieve the Bundle object associated with the specified bundle identifier.
If there is metatype information, you get the PIDs; and for each PID, you get the object class definition.
You can now use this command to get a list of all known PIDs and their respective properties for any given bundle identifier.
If you haven’t already done so, type ant to build the example and java -jar launcher.jar bundles to execute it.
All you need to do is execute the type command with the bundle identifier of a bundle providing metadata, and you get a description of what kind of properties any associated PIDs can understand.
This makes it a little easier to properly configure arbitrary services.
Where are we now? You’ve learned how to configure bundles and provide metatype information about configuration properties.
This combination allows you to create externally and generically configurable applications.
What more do you need? Not all configuration information is intended to be externally managed; for example, most preference settings in an application fall in this category.
Where should a bundle store such configuration information? The OSGi Preferences Service can help you here; let’s look at how it works next.
In many cases, applications need to store preferences and settings persistently.
Of course, this chapter is about managing bundles, and, technically, dealing with preference settings isn’t really a management activity.
Still, we include it here because it’s related to configuration data in general, and this gives us an opportunity to present another standard OSGi Compendium service.
The OSGi Preferences Service gives bundles a mechanism to persistently store data.
You could use this mechanism to store preference settings, but the Preferences Service has several advantages:
The Preferences Service provides simple, lightweight access to stored data.
It doesn’t define a general database service but is optimized to deliver stored information when needed.
It will, for example, return defaults instead of throwing exceptions when the back-end store isn’t available.
The Preferences Service data model is a multirooted hierarchy of nodes: a system root node exists for system settings, and you can create any number of named user root nodes for user settings.
Each one of these root nodes is the root of a tree of Preferences objects.
A Preferences object has a name, a single parent node (except for a root node, which has no parent), and zero or more child nodes.
It’s possible to navigate a tree either by walking from one node to its parent or children or by addressing nodes directly via a relative or absolute path.
This is possible using the node names separated with the / character, much like file system paths.
Each Preferences object has a set of key/value pairs, called properties.
The key is a case-sensitive string that lives in a separate namespace from that of the child nodes, which means a node can have a property with the same key as one of its children.
Configuring bundles value must always be able to be stored and retrieved as a string.
Therefore, it must be possible to encode/decode all values into/from strings.
A number of methods are available to store and retrieve values as primitive types.
You can use the getUsers() method to enumerate all usernames that have stored preferences.
When you have a node, you can navigate the preference tree using the childrenNames(), parent(), and node() methods on the returned Preferences node.
For setting values, the Preferences interface offers some simple methods to store key/value pairs:
For each of these methods, a corresponding getter method exists.
Getter methods always accept two arguments: the first to specify the key of the property to retrieve, and the second to specify a default value in case the property doesn’t exist (or in case of errors)
Assuming you want to store the last time your bundle was started, you can do this using the system preferences:
Preferences are per bundle The preferences saved by one bundle are completely distinct from the preferences saved by another bundle.
The Preferences Service doesn’t provide a mechanism for one bundle to access another bundle’s preferences storage.
If this is needed, you must obtain a reference to the source bundle’s preferences in another way, such as directly passing a reference to the other bundle.
This stores the current time as a long in the system preferences start node.
As you can see, this is pretty simple stuff, but it’s convenient to have a standard service definition rather than having to invent it yourself.
The combination of the Configuration Admin, Metatype, and Preferences Services provides for flexible approaches when it comes to configuring your bundles, which can save you a lot of management effort.
So far in this chapter, we’ve talked about how to manage versions and spent a fair amount of time showing how to manage bundle configuration data.
Now we’ll switch to our final topic: managing when a given bundle is activated after it’s started.
From chapter 3, you know that starting a bundle involves invoking the Bundle.
If the bundle has a BundleActivator, and it’s resolvable, the framework creates an instance of the bundle’s activator and invokes start() on it, allowing the bundle to initialize itself.
The act of starting a bundle and of it being activated are two independent concepts, although typically they occur together.
Sometimes you may want to start a bundle but not necessarily activate it until some later time.
Your bundle’s exported packages aren’t able to function properly without a BundleContext (for example, perhaps they require a service from the registry)
Your bundle’s initialization is costly, and you want to defer it until it’s needed.
The OSGi specification allows bundles to declare a lazy activation policy, which indicates to management agents that something like one of the previous two issues applies to it.
Of course, you can use alternative approaches to deal with these situations.
For the first issue, you can program the bundle classes to always throw exceptions until activated.
For the second, you can minimize initialization in the bundle activator and use threads to do work in the background.
Sometimes these alternative approaches are feasible, but sometimes throwing exceptions isn’t so clean, nor is it possible to completely reduce all startup overhead, especially if you’re starting lots of bundles.
In these cases, you can use the lazy activation policy.
At the same time, the OSGi Preferences Service saves preferences for each bundle independently of other bundles, whereas Java Preferences saves preferences of one user of the system independently of other users.
Although the OSGi specification defines activation policies in an open-ended way, there’s currently only one activation policy: lazy.
The main gist of the lazy activation policy is this:
The lazy bundle’s activation is deferred until a class is loaded from it.
After a class is loaded from the lazy bundle, the framework completes its activation as normal.
This is fairly straightforward, but some small details lurk inside.
Let’s revisit the bundle lifecycle diagram in figure 9.8 to get a better understanding of the impact.
The bold arrows in figure 9.8 depict additional transitions in the bundle lifecycle state diagram.
When a bundle is started lazily, it transitions to the STARTING state, which is denoted by the framework by firing a BundleEvent of type LAZY_ACTIVATION, instead of the normal STARTING event.
The bundle stays in this state until it’s stopped or a class is loaded from it.
Stopping a lazy bundle in the STARTING state returns it to the RESOLVED state and results in STOPPING and STOPPED bundle events.
When a class is loaded from a lazy bundle in the STARTING state, this acts as a trigger for the framework to automatically activate the bundle, which completes the normal process of creating the bundle activator and calling its start() method, resulting in the normal STARTING and STARTED bundle events.
Figure 9.8 The lazy activation policy causes a bundle to defer activation and linger in the STARTING state until a class is loaded from it, at which point the framework completes its activation.
Because loading a class from one lazy bundle may require other classes to be loaded from other lazy bundles, the framework may end up activating chains of lazy bundles.
The framework doesn’t activate the lazy bundles as it loads classes from them, because this can lead to arcane class-loading errors.
Instead, the framework delays the activation of each lazy bundle it discovers in a class-loading chain until it finishes loading the instigating class.
At that point, the framework activates the detected lazy bundles in reverse order.
For example, assume ClassA is loaded from bundle A, which requires ClassB from bundle B, which in turn requires ClassC from bundle C.
If all of the bundles are lazy and in the STARTING state, the framework will activate bundle C, bundle B, and then bundle A before returning ClassA to the requester.
Now that you know how the lazy activation policy works, let’s look into the details of using it.
The process of using activation policies involves both the bundle wishing to be lazily activated and the management agent deciding whether to start a bundle lazily.
The default behavior is eager activation, although there is no explicit way to specify this value.
To use this in the bundle manifest, you do this:
Only bundles containing this manifest header can have their activation deferred.
The reasoning behind this goes back to one of the main use cases motivating deferred activation: a bundle that requires a BundleContext for its exported packages to function properly.
In this use case, only the bundle itself knows if this is the case; thus, only the bundle itself can declare the policy.
Attention! Be aware that loading resources from a bundle doesn’t trigger lazy activation, only classes.
Also, the specification doesn’t unambiguously define how the framework should treat the class-loading trigger, so the precise behavior may vary.
In particular, some frameworks may scope the trigger to the lifetime of the bundle’s class loader (it needs to be re-triggered only if the bundle is refreshed), whereas others may scope the trigger to the bundle’s ACTIVE lifecycle state (it needs to be re-triggered after the bundle is stopped and restarted)
This may sound a little odd, because deferring activation sounds like a good thing to do all the time.
Why pay the cost of activating a bundle before it’s needed? You could start all bundles lazily and they would only activate when another bundle used them, right? There’s a fly in the ointment with this approach.
If your bundle isn’t activated, it won’t ever get a chance to publish its service in its bundle activator.
Thus, no other bundles will be able to use it, and it will never be activated lazily.
So, even if it were possible to apply an activation policy to a bundle externally, it wouldn’t always end up working the way you intended.
One final detail for bundles declaring an activation policy: the specification offers fine-grained control over which precise packages trigger lazy activation.
The specification defines include and exclude directives, which declare a comma-separated list of included and excluded packages, respectively.
Assuming you have bundles with the declared lazy activation policy, the management agent has the final say as to whether their activation is deferred.
In chapter 3, you learned about using Bundle.start() to start and eagerly activate a bundle.
If you call Bundle.start() with no arguments on a bundle with lazy activation policy, it will be activated eagerly, as normal.
When you use this flag, you’re saying that you want to start the bundle using its declared activation policy.
A bundle with no activation policy will be started eagerly as usual, whereas one with the lazy policy will have its activation deferred as described in the previous section.
There’s no requirement to start bundles declared as “lazy” lazily.
Eagerly starting a bundle is always acceptable; it means you’re willing to pay for the startup cost immediately.
In most cases, eager activation is more than sufficient, so you won’t need to worry about activation policies.
But in those situations where it’s required, it can make your life simpler.
That’s it! We’ve covered a variety of bundle management topics.
In this chapter, we discussed how to manage bundles, including a range of issues:
You must carefully consider the versioning of both packages and bundles when working with OSGi.
It’s up to you to define and adhere to such as policy.
Related to configuration data, the Preferences Service provides a standard mechanism for bundles to manage system- and user-preference settings.
The lazy activation policy defers bundle activation until a class is loaded from the lazily started bundle, allowing management agents to defer the cost of bundle startup.
These topics have given you a fairly good foundation for managing your bundles.
Next, let’s look at how to build and manage OSGi-based applications.
Now, we’ll move beyond managing individual bundles to issues related to managing OSGi-based applications composed of many bundles.
As we’ve mentioned previously, in OSGi-based applications the deployed set of bundles is your application’s configuration.
This is a powerful aspect of the OSGi approach, so understanding this point and knowing how to manage sets of bundles is important to be able to fully take advantage of OSGi technology.
In this chapter, we’ll explore a couple of different aspects of application management:
Controlling bundle activation order using the StartLevel service Managing applications.
With these tools, you’ll be better equipped to build, deploy, and configure sophisticated OSGi-based applications.
When you’ve created some configurable bundles and versioned them according to a meaningful policy, you need to install them into an OSGi framework.
In chapter 3, we looked at the various details of the lifecycle layer API, which allows you to install, start, update, and uninstall bundles from a running framework.
Given the nature of modularity, it’s likely your applications will grow over time to include too many bundles for you to manage their deployment in an ad hoc fashion.
Manually installing and updating tens, hundreds, or even thousands of bundles becomes impractical.
What can you do? This is when it becomes important to think about how you (or your users) are going to discover and deploy bundles.
The solution, in OSGi lingo, is to create a specific type of bundle called a management agent.
Although we’ve shown how to programmatically manipulate the lifecycle of a bundle, it’s typically not a good idea for a bundle to change its own state or the state of other bundles.
Such a bundle is difficult to reuse in other compositions, because it’s tightly bound to the other bundles it expects to control.
The solution employed by most management agents is to externalize the information about which bundles to install or start.
For example, management information can refer to bundles using URIs and aggregate useful groups of bundles using some sort of composition language/mechanism.
A management agent can generically process such information, leaving it nicely decoupled from the bundles it’s managing.
Granted, it’s perhaps too simplistic because it only accepts and executes commands; but if such capabilities are sufficient for your application, it’s fine.
Even for your shell, you could easily extend it to handle command scripts for executing commands in batches.
You could then create a couple of scripts, one for each configuration you need.
Your shell assumes human interaction to either directly or indirectly make the correct decisions and issue commands to manage the bundles.
You could devise a system with rules to automate some of this by reacting to certain conditions autonomously.
Consider a home-automation system that’s able to detect a new device, automatically discover a driver for it in a remote repository, and subsequently install the driver along with its dependencies.
Or you may have an application that automatically adapts itself to the language of the current user by installing the necessary locale bundles.
OSGi supports you in developing such an agent by providing you with the means to monitor and.
One of the more critical aspects of managing the framework is determining which bundles should be deployed to it.
Various strategies are possible to manage complex sets of interdependent bundles.
The two most prominent at the moment are the OSGi Bundle Repository (OBR) and Deployment Admin.
The difference in focus between the two can be summarized as follows:
Deployment Admin focuses on the deployment of sets of bundles and associated resources.
In the following sections, we’ll explore these two technologies in more detail and show you how to use them to provision or deploy your applications and bundles.
The OSGi Bundle Repository (OBR) is officially not an OSGi standard specification; rather, it’s a proposal for a specification, internally referred to as RFC 112 in the OSGi Alliance.
Because OBR is only an RFC, its details may change in the future, but it’s still a useful tool as it is.
Alternative technologies A number of other technologies attempt to address deployment and provisioning for OSGi, including Apache Ace, Paremus Nimble, and Equinox p2:
Ace is a software distribution framework based on Deployment Admin.
It focuses on centrally managing target systems, and distributing software components, configuration data, and other artifacts to them.
The target systems are usually OSGi-based, but they don’t have to be.
Nimble is based on open source work from the Newton project and focuses on building an extensible resolver architecture that can deal with other types of dependencies outside of the OSGi modularity layer, such as service-level dependencies.
For example, if a bundle containing servlets is deployed and activated, a servlet container should be deployed and activated alongside it.
We won’t discuss the details of any of these in the remainder of this book.
If you’re interested in them, they’re just a Google search away.
Dependency deployment—Provide a simple mechanism to deploy a bundle and its transitive set of dependencies.
To achieve the first goal, OBR defines a simple bundle repository with an API for accessing it and a common XML interchange format for describing deployable resources.
An OBR repository can refer to other OBR repositories, defining a federation of repositories.
But it’s not necessary to define federations, so it’s possible to create independent repositories specifically for your own purposes and applications.
One of the main goals of OBR was simplicity, so it’s easy for anyone to provide a bundle repository.
One of the benefits of using an XML-based repository format is that no server-side process is needed (although server-side processes are possible)
Figure 10.1 shows the federated structure of an OBR repository.
The key concept of an OBR repository is a generic description of a resource and its dependencies.
A resource is an abstract entity used to represent any type of artifact such as a bundle, a certificate, or a configuration file.
The resource description allows an agent to discover applicable artifacts, typically bundles, and deploy them along with their transitive dependencies.
Resource requirements are satisfied by capabilities provided by other resources or the environment.
Figure 10.1 The OBR proposed specification provides a federated index that allows a management agent to resolve and install large numbers of bundles from a number of remote locations.
The OBR index files are aggregated by a RepositoryAdmin service that resolves bundle dependencies on behalf of a management agent.
Using this information, an OBR implementation is able to resolve a consistent set of bundles for deployment given an initial set of bundles to be deployed.
With this overview of OBR, let’s look at how you can create a repository for it.
To illustrate how to create an OBR repository, let’s use the bundles from the servicebased paint program example.
The repository is just an XML file containing the metadata of the bundles.
We’ll go through the entries in the XML file and explain the schema along the way.
Assume you have the bundles from the example in a directory called paint-bundles.
The directory contains the paint frame bundle, the API bundle, and the three shape bundles:
The framework’s resolution algorithm will never pull in additional resources; it only considers installed bundles.
Another gotcha is the fact that the current OBR RFC doesn’t currently mandate uses constraints when resolving dependencies.
This can lead to unexpected failures at execution time if a uses constraint prevents bundles from resolving.
You could create the repository XML file by hand, but you can use several different tools to create one instead.
This example uses BIndex (http://www.osgi.org/Repository/ BIndex), which is provided by the OSGi Alliance.
For Maven users, there’s also Maven support, which we’ll discuss in appendix A.
This creates a repository.xml file that contains the metadata of the bundles from the example.
The main XML element is a repository tag defining the repository:
The lastmodified attribute is used as a timestamp by the OBR service to determine whether something has changed.
The most interesting element is the <resource> tag: it describes a bundle you want to make available.
The created repository XML file contains one resource block per bundle.
The shape API bundle converted into OBR is as follows.
The capability elements B and C represent what the bundle provides.
In this case,B represents the bundle itself, because the bundle can be required (for example, Require-Bundle), whereas C represents the package exported by the bundle.
Bundle dependencies are represented as requirement elements, such as the one for an imported package D.
Both capabilities and requirements have a name, which is actually a namespace; it’s how capabilities are matched to requirements.
For example, capabilities representing exported packages and requirements representing imported packages both have the package namespace.
In general, a capability is a set of properties specified using a <p> element with the following attributes:
Looking more closely at the bundle capability B, you see it’s a fairly straightforward mapping from the bundle identification metadata:
Likewise, the package capability C is also a simple mapping from the bundle’s Export-Package header:
A requirement is an LDAP query over the properties of a capability.
So, to match a requirement to a capability, first the namespace must match.
If that matches, the requirements LDAP query must match the properties supplied by the capabilities.
Even with the LDAP query, the package requirement D is a fairly easy mapping from the Import-Package header:
One reason the filter D looks somewhat more complicated than necessary is that version ranges aren’t directly supported by the filter syntax and must be expressed as the lower and upper bound.
Even though the mappings are straightforward, it’s still nice to have a tool like BIndex doing this for you.
You can even integrate BIndex into in your build cycle so your repository is updated whenever your bundles change.
The repository XML is all well and good, but you’re probably wondering how you can use repositories in your management agent.
You don’t need to know anything about the XML format to use OBR.
All you need to do is grab the service implemented by OBR and use it.
The best way to familiarize you with how to use repositories is to give an example and explain what it does along the way.
Let’s use the shell example again and extend it with a new command to add/remove/list repositories and browse the bundles inside them.
The programmatic entry point to the OBR specification is the RepositoryAdmin service, which is represented by the following interface:
This RepositoryAdmin service provides centralized access to the federated repository.
An OBR implementation implements this interface as well as the other types referenced by it.
The code in the following listing shows the code for the new obr-repo command.
It uses RepositoryAdmin to add, remove, and list repositories as well as to discover resources.
An external repository client uses the RepositoryAdmin and Resolver interfaces to download and install bundles and their transitive dependencies.
The obr-repo command has the following subcommands: list-url, add-url, remove-url, and list.
A RepositoryAdmin provides access to a number of repositories referenced by URLs.
You implement the list-url subcommand B to list these repositories by retrieving the RepositoryAdmin service and calling its listRepositories() method, which gives you access to the associated Repository objects.
In this case, you loop through the repositories and print their names and URLs.
You can add or remove repository URLs with the add-url and remove-url subcommands, respectively.
As you can see at C and D, there’s a one-to-one mapping to the addRepository() and removeRepository() methods of the RepositoryAdmin service.
You loop through the discovered resources and print their presentation name, symbolic name, and version.
You can now use this command to configure repositories and discover bundles.
After you’ve discovered a bundle you want to use, you need to deploy it.
Discovering bundles is one half of the OBR story; the other half is deploying them and their dependencies into the framework.
If the desired resources resolve successfully, deploy them with Resolver.
The following listing implements an obr-resolver shell command to resolve and deploy resources.
If you find any resources, you add the first one to the Resolver and call resolve() to resolve its dependencies from the available repositories B.
If the resource is successfully resolved, you print out all of the dependencies of the resource you’re deploying.
If the resource couldn’t be resolved, you print out the missing requirements.
Type ant to build the example and java -jar launcher.jar bundles to execute it.
In this session, you first use the add-url subcommand to add your repository containing the paint program bundles.
Using the list subcommand, you browse the bundles contained in the repository.
Then, you use the obr-resolver command with an LDAP filter to select and deploy the paint-frame bundle, which also installs its dependencies.
That’s about all you need to know to start using OBR to discover and deploy your bundles.
Often, this is enough to manage the growing complexity of your applications.
But sometimes you’ll be faced with a slightly different scenario that doesn’t fit as well with what OBR provides.
Perhaps you want to package your application in a single deployment unit composed of several bundles.
What can you do in this case? Another OSGi Compendium specification targets such needs.
With OBR, you tend to think about deploying specific bundles and letting OBR automatically calculate and deploy any dependent bundles.
With Deployment Admin, your thinking changes to deploying entire applications or subsystems as a single unit.
The Deployment Admin specification standardizes some of the responsibilities of a management agent; specifically, it addresses lifecycle management of interlinked resources on an OSGi Service Platform.
Deployment Admin defines a way to package a number of resources in a deployment package.
A deployment package is a JAR file with a format similar to a bundle.
The DeploymentAdmin service can process bundle resources itself, but other types of resources in the deployment package are handled by passing them to a ResourceProcessor service for that specific type of resource.
The chosen ResourceProcessor service appropriately processes the given resource type.
The uninstallation and update of a deployment package works similarly: bundles are processed by the DeploymentAdmin service, and other types of resources are handed off to ResourceProcessors.
All ResourceProcessor services are notified about any resources that are uninstalled or updated.
If all resources have been processed, the changes are committed.
Most implementations tend to provide only a best effort rollback.
To get a better idea of how it works, we’ll present some of the details of deployment packages next.
After that, we’ll give an example of how you can use the Deployment Admin to install and manage deployment packages.
As an example, let’s think about how to provision your paint program.
To be able to show all of what deployment packages have to offer, let’s assume you want to provide a core version of the program containing the drawing frame and the shape API bundles.
This way, you’re able to deploy the actual shape implementations separately via an extension pack.
The extension pack contains the square, circle, and triangle bundles.
Let’s go with this approach and explore the different ways you can use deployment packages to make it work.
The general structure of a deployment package is shown in figure 10.4
This ordering is carefully designed to allow deployment packages to be streamed in such a way that the contents can be processed without needing to download the entire JAR file.
The deployment package design has a few other desirable characteristics.
First, the deployment package puts metadata in its manifest, similar to bundles, which allows you to turn it into a named and versioned set of resources.
Second, by taking advantage of the fact that JAR files can be signed, you can use signed JAR files to make your deployment packages tamperproof.
For this example, you can do either of the following (see figure 10.5):
Create a deployment package for the core bundles and one package for all shape bundles.
Create a deployment package for the core bundles and individual deployment packages for each shape bundle.
The difference is obviously that in the first case, you’ll deploy either all shapes or none; and in the second case, you can extend the core bundle piecemeal.
The important point to understand, though, is that you can’t use both approaches at the same time: you must choose one.
In terms of the example, you need to make a decision.
In this case, you’ll go with the first approach and create a single deployment bundle for all shapes.
But because deployment packages can be updated, you can gain some flexibility by starting with only one shape in the deployment package and then adding another one in an updated version and.
When you create an update that adds or removes resources from a previous version, you don’t even have to package the resources inside the update; instead, you can use fix packages.
It can only be installed if a previous version of that deployment package is already installed.
A fix package contains only the changed and new resources.
A fix package (called the source) therefore must specify the range of versions that the existing deployment package (called the target) must have installed.
You’ll see this shortly when we walk through the example.
Let’s assume that you want to be able to add new shapes to the application when they become available.
In this scenario, it makes sense to start with a core deployment package and create fix packages, adding new shapes as they become available.
Now that you’ve figured out your packaging approach, how do you proceed? You need to create a manifest for the target that contains the paint frame and shape API bundles; you’ll use this to provision the paint program core.
Then you need to create the manifest of the fix package that you’ll use to add the three shape bundles to the core.
When you have your manifests, you need to create two JAR files with the corresponding manifests and your bundles, you can optionally sign them, and you’re good to go.
Deployment packages are greedy These two different packaging strategies can’t be used simultaneously.
The specification only allows resources to belong to a single resource package.
Using both approaches at the same time or changing your approach after the fact would move ownership of the bundle resources to another deployment package and thus violate the specification.
A deployment package is defined as a set of resources that must be managed as a unit.
The resources in a deployment package are assumed to be tightly coupled, such as a bundle and its configuration data.
As a consequence, a resource can belong to only one deployment package; otherwise, for example, you could run into situations where you had two different, conflicting configurations for the same bundle.
You first specify the deployment package’s symbolic name and version.
Next, you specify the list of resources contained in the JAR file.
You specify the name of a resource, its symbolic name, and its version; you must do this for each resource.
To finish, you need to use the jar tool to create the JAR file with the appropriate content, and you’re finished with your first deployment package.
Now you need to create the manifest for your fix package containing the shape bundles.
Because the fix package is an update to your core package, the symbolic name stays the same, but the version is upgraded to 2.0.0
This version-numbering scheme expresses the assumption that only major version-number changes indicate added.
Signing deployment packages In this example, you don’t sign your deployment package, nor is it required for you to do so.
If you want to create a signed deployment package, you use the jarsigner tool from the standard Java SDK.
The signing process is no different than signing a normal JAR file; it results in the signatures being placed in the deployment package JAR file in the META-INF directory and after the MANIFEST.MF file.
Additionally, each entry section in the manifest contains a digest entry.
You don’t need to package the bundles already present in the core package, but you still need to mention them in the manifest.
Then you specify the shape bundles in the same fashion as before.
To use the deployment packages, you need to make each available via a URL.
Next, you can use the provided DeploymentAdmin service in your management agent to install, update, and uninstall deployment packages.
To demonstrate how a management agent can use Deployment Admin, you’ll again return to the shell and create a new dpa shell command to list, install, and uninstall deployment packages.
This command will use the DeploymentAdmin service, which is represented by the following interface:
Like the previous example commands, you more or less map the command onto the DeploymentAdmin service interface.
Type ant to build the example and java -jar launcher.jar bundles to execute it.
You then update it to include the fix package for the shapes.
You list the installed deployment packages and then uninstall the deployment package.
Note that the Apache Felix implementation of Deployment Admin doesn’t currently implement the uninstall functionality.
This highlights the difference between the OBR and Deployment Admin approaches, because you can manage your bundles as a single unit of deployment rather than individual bundles.
Before concluding our discussion on Deployment Admin, we’ll discuss resource processors.
Resource processors are an important part of the Deployment Admin specification, because they extend OSGi deployment beyond bundles.
Deployment Admin can process bundle resources in deployment packages by itself; but when it comes to other types of resources, it needs to enlist the help of ResourceProcessor services.
A ResourceProcessor is a service used to appropriately process arbitrary resource types; it implements the following interface:
Deployment Admin connects resource types to resource processors using the Resource-Processor header in the resource entry of the deployment-package manifest.
You use this header to specify the service PID of the needed resource processor.
These kinds of services are provided by customizer bundles delivered as part of the deployment package.
This allows Deployment Admin to start customizers first, so they can provide the necessary ResourceProcessor services to handle the deployment package content.
Resource processors may result in new file system artifacts but can perform other tasks like database initialization or data conversion, for example.
Each nonbundle resource should have a processor associated with it.
With the necessary resource processor specified, Deployment Admin is able to process all resource package content.
Before processing of the deployment package starts, Deployment Admin creates a session in which all actions needed to process the package will take place.
A session isn’t visible to clients of the DeploymentAdmin service; it’s used to join the required resource processors to the processing of the deployment package.
If an exception is raised during a session by any of the resource processors or the session is canceled, Deployment Admin rolls back the changes.
As we mentioned before, this may only be a besteffort rollback, but it’s normally sufficient to leave the framework in a consistent state.
If no exceptions are raised during a session, Deployment Admin commits the changes.
During a commit, DeploymentAdmin tells all joined ResourceProcessor services to prepare and subsequently commit their changes.
As you can see, this essentially provides a two-phase commit implementation.
This is why the Deployment Admin specification doesn’t mandate full transactional behavior.
In this section, we’ve looked at two different ways of deploying bundles.
Deployment Admin provisions sets of bundles and their required resources as complete units.
These provide solutions to many of the deployment and discovery tasks you’ll need for a management agent.
Of course, if necessary, you can always use the core OSGi API to create something for your specific needs.
Now that you know how to deploy bundles to the OSGi framework, we need to look at one final management-related task.
After deploying a set of bundles, sometimes you need to control their relative activation order.
In certain scenarios, you may need to control the relative order in which deployed bundles are activated and/or deactivated.
There are some good reasons to control such ordering, but there are many more bad ones.
Best practice dictates that you should create your bundles to be independent of activation and deactivation ordering.
OSGi allows bundles to listen for lifecycle events from other bundles because it eliminates the need to order dependencies and allows bundles to be aware of changes and react to them.
Ordering constraints are another form of coupling among bundles, which severely limits their ability to be reused and arbitrarily composed.
A bundle shouldn’t require that functionality from another bundle be available for it to be started itself; instead, it should wait for the functionality to become available and then continue with its own functionality.
Having said that, there are a few valid reasons why you may want to ensure that a given bundle is activated before another.
For example, you may want to implement a splash screen to display the progress of your application’s startup.
If your splash screen is developed as a bundle, you need a way to ensure that it’s activated first.
After all, what good would a splash screen showing the startup progress be if it came up last? You can generalize this kind of functionality as a high-priority feature, which in general requires ordering because it needs preferred treatment.
In addition to high-priority features, ordering may be needed in two other scenarios:
When a bundle violates the best practices mentioned earlier and relies on implicit activation ordering during startup.
In reality, you should consider fixing or replacing such a bundle; but if you can’t, then you must ensure that the bundles it depends on are started first.
Again, this is extremely bad practice, and you should feel a generous amount of shame until the bundle is fixed.
When bundles can be grouped into sets with certain desirable properties.
For example, you may define a set of bundles comprising a safe mode, where you deactivate all but a small set of trusted bundles and provide limited core functionality for safety or security reasons.
How can you influence and control relative activation and deactivation ordering among bundles? By using the standard Start Level Service provided by the OSGi framework.
The Start Level Service allows a management agent to control the relative activation/ deactivation order among bundles as well as when transitions should occur.
The idea is simple, and you may already be familiar with it from other contexts, such as in UNIX environments where system services are started or stopped based on the system’s current run level.
In OSGi, the framework has an active start level associated with it, which is a nonnegative integer indicating the start level in which it’s executing.
The framework starts with an active start level of zero and, by default, transitions to an active start level of one when it’s fully running.
Each bundle also has an integer start level associated with it, which indicates the required start level of the bundle.
Only bundles with a start level less than or equal to the framework’s active start level are allowed to be in the ACTIVE state.
The Start Level Service is represented by the following interface:
Modifying the active start level of the framework—You can change the framework’s active start level with setStartLevel()
Doing so results in all active bundles with a higher start level being stopped, and bundles with a lower or equal start level that are persistently marked as started being activated.
The framework activates the bundle if it’s persistently marked as started and the new start level is less than or equal to the active start level or stops the bundle if the new start level is greater than the active start level.
Querying relevant values—You can query the framework’s active start level, the start level of a bundle, and the initial bundle start level.
Additionally, you can query whether a given bundle is persistently marked as started.
What does all this mean in simple terms? The framework’s active start level and a bundle’s start level control whether a bundle can be started.
This means that if you explicitly start a bundle (invoke Bundle.start() on it), it won’t activate unless the bundle’s start level is less than or equal to the framework’s active start level.
In such a case, the only effect of invoking Bundle.start() is that the bundle is persistently marked as started.
If the framework’s active start level is eventually changed to a greater or equal value, the bundle will be automatically activated by the framework.
As you can imagine, changing the active start level of the framework can have a dramatic impact on the framework, because a lot of bundles may be started or stopped as a result.
When you use the Start Level Service to change the framework’s active start level, all active bundles with start levels greater than the target start level are stopped, whereas all bundles persistently marked as started with start levels less than or equal to the target start level are started.
The background thread effectively increments or decrements the current active start level one step at a time, depending on whether the new active start level is greater than or less than the current active start level, respectively.
At each step, the background thread starts or stops the bundles at that level until the new target level is reached.
To illustrate how you use the Start Level Service, you’ll add startlevel and bundlelevel commands to the shell.
These two commands, implemented in the following listing, perform the four functions mentioned earlier.
Executing the startlevel command without an argument prints the framework’s active start level B.
Next, the bundlelevel command allows you to set and get the start level of an individual bundle.
Here’s a simple session demonstrating what you can do with these commands.
In this example session, you first use the startlevel command to display the framework’s current active start level, which is 1 by default.
Subsequently, when you install and start the foo bundle, you can see from the following bundles command output that it’s not started yet.
You raise the framework’s active start level to 2, which ultimately causes the foo bundle to be started.
Using the bundlelevel command to set the foo bundle’s start level to 3 stops the bundle again.
You’ll not likely need this service often, because bundle activation ordering isn’t good practice, but it can come in handy in certain situations.
We’ve finished covering application management; let’s summarize what we’ve discussed.
In this chapter, we discussed how to manage your OSGi-based applications.
One of the key management tasks is deploying bundles to the OSGi framework.
You can use multiple techniques to do so, including rolling your own approach or using technologies like OBR and Deployment Admin.
You can use the Start Level Service to control the relative activation order of your deployed bundles, which may be needed in a few situations like creating splash screens and different execution modes.
These topics have given you a fairly good foundation for managing your bundles.
Now that you know how to build and manage your OSGi applications, we’ll move into more advanced topics, such as service-oriented component models.
In the first part of the book we looked into the core OSGi framework specification and explained its most important features and capabilities.
In the second part of the book, we turned to the pragmatic issues of developing OSGi-based applications.
In this third and final part of the book, we’ll explore a variety of advanced topics.
To help you simplify OSGi development, we’ll introduce OSGibased component frameworks.
These component frameworks should be interesting for all OSGi developers.
We’ll also look into launching and embedding the OSGi framework, enabling security in OSGi-based applications, and developing web and distributed applications in OSGi.
After completing this final part of the book, you should have a good idea of all the possibilities that OSGi technology provides.
So far in this book, we’ve shown you how to develop applications using the core OSGi framework layers: module, lifecycle, and service.
In chapter 2, we mentioned the similarities between module- and component-oriented programming.
In chapter 4, we mentioned how the OSGi service model can work alongside component models.
There’s obviously some degree of synergy between OSGi and component technologies.
This has led a variety of existing component technologies to integrate with OSGi as well as a variety of new component frameworks being built on top of it.
Component-oriented approaches have become incredibly popular in Java development over the past decade, and a vast number of approaches are available, Component models and frameworks.
The variety and variation among component-oriented approaches is staggering, but one thing is typically common: they ignore or only pay lip service to modularity issues related to deployment and executiontime verification and enforcement.
This means OSGi technology provides a perfect foundation for integrating existing component approaches or defining new ones.
In this chapter and the next, we’ll introduce you to component orientation in general and as it relates to OSGi technology.
This chapter will cover introductory aspects and present the first OSGi standard component framework, called Declarative Services, which is lightweight and fairly representative of how component frameworks are integrated with OSGi.
In the next chapter, we’ll introduce a couple more advanced component frameworks.
We’ll reuse the example paint program to illustrate how these component frameworks simplify OSGi-based development.
Although component-oriented programming has been around for a while, there’s no single definition for most of the concepts it embodies (which is similar to module orientation)
Therefore, you shouldn’t take the discussion in this section as the bible for all component-oriented approaches.
The main questions we intend to address for the scope of this chapter and the next are, what are components, and why do we want them? We’ll answer these questions in the following two subsections, respectively.
A key aspect of all component technologies is that they describe functional building blocks that are typically more coarse-grained than what we normally associate with objects (although object orientation isn’t required for component orientation)
These building blocks are typically business logic; they provide functionality via interfaces.
Conversely, components may consume functionality provided by other components via their interfaces.
Components for a given approach are usually programmed according to a particular pattern defined by a component model.
A component framework implements the runtime needed to support a component model and execute the components.
For example, the Common Object Model (COM) defines a component model that’s implemented by different component frameworks for different platforms.
Likewise, it’s also possible for a component framework to support multiple component models, such as the JBoss Microcontainer.
Generally speaking, components have some explicit way of declaring their provided interfaces.
This can be done through certain patterns, such as implementing an interface or extending a base class, or it can be done more explicitly at execution time by publishing provided interfaces, such as using an interface repository.
Likewise, components may have some explicit way of declaring their dependencies on the provided interfaces of other components, such as with declarative metadata, or they may be responsible for managing their own dependencies at execution time, such as querying an interface repository.
Often, components are packaged as independent deployment units, such as JAR files or DLLs, but this isn’t strictly necessary.
The general approach for creating an application from components is to compose it.
This means you grab the components implementing the functionality you need and compose them (match required interfaces to provided interfaces) to form an application.
Component compositions can be declarative, such as using some sort of composition language to describe the components and bindings among them; or implicit, where the.
This is common when vendors try to differentiate implementations of standard component models; think about how Java EE application servers try to differentiate themselves.
The reality is that no clear line separates a component model from a component framework.
The important differentiation to take away is that a component model describes what it means to be a component, and the framework provides the runtime to execute components adhering to a component model.
Couldn’t these two be considered one and the same or at least be combined? Yes, they could, but components and modules serve different purposes and are somewhat orthogonal (they’re not completely orthogonal, because components are made from code that can ultimately be packaged into modules)
Modules deal with code packaging and the dependencies among code.
Components deal with implementing higher-level functionality and the dependencies among components.
Components need their code dependencies managed, but they technically don’t need a module system to do it (often it’s us programmers doing it via the class path)
A good summary is that you can think of modules as dealing with static code and compile-time dependencies, whereas components deal with instances and executiontime dependencies.
For the application to execute, the application’s constituent components must somehow be loaded into the component framework and instantiated.
This description of component orientation is by no means complete.
Depending on the component model, components may have a variety of capabilities, such as explicit lifecycle control.
Some component models and frameworks differentiate between component types and instances (for example, there can be multiple component instances from a given type), whereas others treat them as being the same (only one instance per component)
You’ll see some of these differences rear their heads in our later discussions of specific OSGi-based component frameworks.
For now, it’s sufficient if your general understanding of component orientation is as a programming approach promoting coarse-grained, composable application building blocks.
The long-held promise of component orientation is that we’ll be able to create applications easily and quickly by snapping them together from readily available, reusable components.
The actual merits of this rosy view of component orientation are debatable, but there are benefits to be gained by adhering to a component model.
First and foremost, it promotes separation of concerns and encapsulation with its interfacebased approach.
This enhances the reusability of your code because it limits dependencies on implementation details.
Another worthwhile aspect of an interface-based approach is substitutability of providers.
Because component interaction occurs through well-defined interfaces, the semantics of these interfaces must themselves be well defined.
As such, it’s possible to create different implementations and easily substitute one provider with another.
You may have noticed that these benefits are pretty much the same as we described for OSGi services.
This is part of the reason why such a strong synergy exists between OSGi and component technologies; more on this shortly.
Because component models typically make the provided and required interfaces of components explicit (or at least explicitly focus on them), you end up with more reusable software that’s amenable to composition.
And because component models typically require a specific pattern or approach for development, your code ends up more uniform and easier to understand.
This uniformity also leads to another potential benefit: external management.
This last point isn’t necessarily obvious; but by creating components following a specific model, external entities can understand more about your code and potentially take some tasks off your hands.
For example, transactions and persistence are handled automatically for components in EJB.
Figure 11.1 Trivial component composition of two components: FooImpl and BarImpl.
OSGi and components where some component frameworks automatically make components remotely accessible.
And as you’ll see in this chapter, execution-time dependency management is also possible.
This all sounds useful, but are there any downsides to using components? Yes, there are always issues to be considered in any architectural decision.
Table 11.1 details some general issues you should consider when choosing whether to use components.
Overall, we feel the positives far outweigh the potential negatives.
Given this general motivation for component orientation, let’s move on to discussing how all this relates specifically to OSGi.
For those reading between the lines in the last section, it may not come as a complete surprise, but there’s a reason why the synergy between OSGi and component technologies is so strong.
The core OSGi specification defines a component model and the framework for executing the corresponding components.
The type of component model defined by OSGi is a special kind, called a service-oriented component model.
Let’s take a minute to look at the specification in this new light.
The high-level description of the OSGi component model can be understood by equating bundles with components and services with component interfaces.
We’ll put a little more meat on this description by breaking down how the OSGi core specification maps to the component-oriented concepts of the last section.
For a bundle to be a component, it implements a bundle activator.
Bloat Some component frameworks are relatively heavy, so they may not be appropriate for small applications.
How complex are your application dependencies? Is the extra functionality provided by a component framework required?
Diagnosis Debugging service-dependency problems requires a new set of tools to figure out what’s going on when your services aren’t published as expected.
Debugging dependency problems is often simplified by having generic tools that can be applied to common component models.
Build- or execution-time problems caused by component configuration files becoming stale with respect to Java source code can be frustrating to debug.
A number of projects are building in support of component models, and they will only increase over time.
The bundle JAR file is the independent unit of deployment for a component.
In OSGi, the logical bundle (the component) is equated with the physical bundle JAR file (the module)
Technically, this means there can be only one component per module and, further, only one component instance.
OSGi’s killer feature is in the richness of its dynamic module layer.
Some component models and frameworks deal with modularity (the code level), but few if any provide such rich features.
At the module level, as you’ve learned, bundles have a way of explicitly describing the code they provide and require.
At the component level, the core OSGi specification doesn’t define a way for bundles to explicitly declare the services they provide and require.
Instead, these issues are left to be handled manually by the bundle at execution time.
The distinction between component model and framework is definitely blurred in the OSGi specification, because it goes to great lengths to ensure that the component framework has standardized behavior.
This ultimately makes aspects of what might ordinarily be thought of as belonging to the component framework part of the component model.
The OSGi-based component frameworks described here and in the following chapter can be seen as extensions to the OSGi component model and framework.
This is sometimes confusing because the distinction of what is part of the model, the framework, or components themselves isn’t always obvious.
For example, the OSGi Configuration Admin Service defines how bundles can be configured.
But it isn’t part of the OSGi component model, nor the OSGi component framework; it’s an agreement among the Configuration Admin component and its client components.
The framework should be the execution environment for components and little else.
Just as there isn’t an explicit way for bundles to declare their provided and required services, the core OSGi specification doesn’t define an explicit way to compose an application.
In OSGi, the composition of a component-based application is the set of deployed bundles.
The interesting part is how the composition is constructed (matching provided services to required services), which is done at execution time via the service registry.
This service-oriented interaction pattern among components is what makes the OSGi approach a service-oriented component model.
Further, the use of an interfaced-based interaction via services enables substitutability of providers.
Combining late binding and provider substitutability results in a flexible.
OSGi and components component model where compositions are malleable, because they don’t specify explicit component implementations, nor precise bindings among them.
In the OSGi model, this also opens up the possibility of advanced scenarios based on executiontime dynamism.
The OSGi approach is flexible, but it’s also a little low-level.
For example, although OSGi uses an API-based approach, many modern component models use or are moving toward an API-less approach, such as using Plain Old Java Objects (POJOs) as components.
This has led to the creation of several OSGi-based component frameworks and/or extensions to the core OSGi approach.
In the next section, we’ll provide an overview of what these additional component frameworks are trying to achieve.
The main weakness of the OSGi component model is its reliance on components manually managing their own service-level dependencies, even though module-level dependencies are automatically managed.
All of the component frameworks we’ll discuss in this chapter and the next also address this issue.
Because the approaches have a lot of similarities, we’ll try to describe some of the issues in a general way here.
OSGi-based component frameworks adopt the bundle JAR file as the deployment unit for components.
In general, they break the “one component per JAR file” approach of the standard OSGi component framework and allow any number of components to be contained in it.
They all define additional, component-related metadata, which is packaged in the bundle to describe the components contained in the bundle JAR file.
They then employ the extender pattern to listen for bundles containing components to come and go so they can start managing the contained components, as shown in figure 11.2
A component’s description defines which services it provides and which it requires; we’ll go into a little more depth on this topic shortly.
Service-oriented component models Service-oriented component models rely on execution-time binding of provided services to required services using the service-oriented interaction pattern (publish-findbind)
Often, execution-time dynamism is also associated with service-oriented component models, but this isn’t technically a requirement to receive some of the benefits.
For example, COM follows a similar approach of execution-time binding to required components, but it doesn’t assume that these components will also come and go during application execution.
Still, following this approach allows you to treat the deployed set of components as the application configuration, which leads to flexibility in your application composition.
The lifecycles of components contained in a bundle are subservient to their containing bundle, meaning that components can only be active if their containing bundle is active.
Beyond that, the lifecycle of an individual component is based on its own dependencies and constraints.
Component frameworks typically define valid and invalid component lifecycle states based on whether a component’s dependencies are satisfied.
Because the lifecycle of the contained components is managed by the component framework, component bundles typically don’t have bundle activators.
A component framework listens for bundle lifecycle state changes in the component bundles to trigger management.
As a result, component frameworks introduce some other sort of callback mechanism (a lifecycle hook) for components wishing for such notification; you can think of this as a component activator.
Luckily, such lifecycle hooks are usually unnecessary, because services and service dependencies are managed automatically, which was the main purpose for having a bundle activator in the first place.
The most immediate benefit of having the component framework manage service dependencies is the simplification it brings.
It removes redundant boilerplate code for handling each service dependency, and it also eliminates some of the complex, error-prone aspects.
Consider a trivial example where a component FooImpl depends on service Bar and should only publish its service when Bar is available.
Figure 11.2 Component frameworks in OSGi are generally implemented as other bundles using the extender pattern.
If one is discovered, it checks whether this is the first Bar service it has found C.
If so, it calls the FooImpl.setBar() method prior to registering the Foo service of FooImpl.
If more than one Bar service is found, backups are stored B.
If BarTracker detects that the Bar service being used has been removed D, it replaces that service with one of the backups E.
If no backup is available, it unregisters the Foo service and calls the FooImpl.setBar() method with null.
You may be looking at this code and thinking that it looks complicated.
We agree that it’s reasonably so, particularly if you also consider that it covers only a single, oneto-one service dependency.
Things get more complex (and redundant) as you get more dependencies.
OSGi-based component frameworks allow you to describe these types of issues; then the frameworks worry about it for us.
If the component’s required services are satisfied, the component framework can instantiate the component and publish its provided services into the service registry.
The descriptions of provided services are normally straightforward (just mentioning the interfaces under which to publish the component), but the descriptions of required services can be rich.
Because dynamism adds complexity, some component frameworks allow components to control how much service dynamism a component sees.
If a component wants to treat a given dependency as having a static lifecycle, it won’t see new services arriving after instantiation and will be completely invalidated if a service being used goes away.
On the other hand, dependencies having a dynamic lifecycle can potentially see (and handle) service dynamism at execution time without being invalidated.
For example, you may want to create a component with an optional, dynamic dependency on a log service.
If the log service isn’t there, your component can function without it; but if one arrives, your component can start using it as soon as it’s available.
Don’t worry if these service dependency characteristics are a little fuzzy at this point; they’ll become clearer as we look into the various component frameworks in more detail.
Another area where component frameworks can help in removal of boilerplate code is in the management of component configuration.
In chapter 9, you saw how you can use the Configuration Admin Service to configure OSGi services using a simple portable model.
Still, the developer must provide some boilerplate code to interact with this service.
Most component frameworks provide ways to simplify component configuration and interaction with the Configuration Admin Service.
Finally, a number of component frameworks allow for custom extension points to allow third-party providers to provide advanced capabilities such as audit management, persistent state, and transaction management using declarative hooks (either in user code or via side files)
These sorts of capabilities turn component frameworks into rich programming environments, allowing you to strip away the layers and focus your code on the core of your business process without sacrificing portability.
Having introduced component models and how they relate to modularity in general and OSGi specifically, let’s now turn our attention to a practical demonstration of using component models and frameworks in OSGi.
In this chapter and the next, we’ll look at three different OSGi-based component frameworks: Declarative Services, Blueprint Container, and iPOJO.
You’ll re-create the example paint program using each of these component frameworks.
For each, you’ll have the components shown in figure 11.3, where each component is packaged in its own bundle (although this isn’t strictly necessary)
The PaintFrame component provides a Window service and has an aggregate dependency on SimpleShape services.
The shape components each export a single SimpleShape service, with no service dependencies.
The WindowListener component has a mandatory, one-to-one dependency on a Window service; it shuts down the framework when the window it’s bound to closes.
The WindowListener also has an optional, oneto-one dependency on a LogService to log a message when the window is closed.
All versions of the paint program function like the original, but how they achieve this varies quite a bit.
Let’s take this high-level of view of OSGi-based component frameworks and make it concrete by turning our attention to the first component framework on the list: Declarative Services.
The Declarative Services specification was defined by the OSGi Alliance as part of the R4 compendium specification.
The focus of the Declarative Services specification is to address three main areas of concern, outlined in table 11.2
The Declarative Services specification addresses these issues by managing service publication and dependencies for components.
Managing service publication on behalf of components allows Declarative Services to defer service creation and improve both startup performance and the memory footprint; and managing service.
We’ll look into precisely how Declarative Services does these things in the remainder of this chapter.
Consider the circle bundle converted to use Declarative Services (the square and triangle bundles follow the same pattern)
If you remember the previous version, the first thing you’ll notice is that it no longer contains a BundleActivator implementation.
If the bundle doesn’t have an activator, how does it provide and use services? The clue you need is located in the bundle’s manifest file, which has the following new entry:
It serves two purposes: its existence tells the Declarative Services framework that this bundle contains components and the referenced XML file contains metadata describing the contained components.
When a bundle is installed into the OSGi framework, the Declarative Services framework follows the extender pattern and probes for this manifest entry.
If it exists, the Declarative Services framework takes over management of the contained components according to the information in the referenced file.
Startup time Because many bundles have bundle activators, the initialization time of each bundle adds to the initialization time of the entire application.
Registering services often implies the creation of many classes and objects up front to support the services.
These classes and objects needlessly consume memory and resources even if these services are never used.
Complexity A large amount of boilerplate code is required to handle complex service-dependency scenarios.
Management of this code in small scenarios is at a minimum a chore, but in large environments this boilerplate code represents a real risk in terms of software maintenance.
Declarative Services naming convention, but the files can go anywhere within the bundle.
It’s possible to include multiple component files in a single bundle using a comma-delimited set, a * wildcard pattern, or a mixture of the two.
This explains the lack of a bundle activator for your component bundle, but how exactly does your SimpleShape service get published? Next, we’ll look more closely at the component description file to see how components declare their provided services.
The following code snippet shows the declaration used to tell the Declarative Services framework to publish your Circle class as a service in the OSGi service registry under the SimpleShape interface:
In this metadata, you define two properties for your component.
Properties may be used to configure a component (which we’ll look at shortly), but they also act as service properties and are automatically attached to services published by a component.
In this case, these properties are used by your paint frame to identify the shape.
You define the component’s implementation class, which must be reachable on the bundle class path where this component description is located.
Finally, you declare that the component provides the SimpleShape service.
Listing 11.2 Circle class used in the Declarative Services paint example.
Fragmented components It’s also possible to place component descriptions into bundle fragments (which we covered in chapter 5)
In this scenario, only the host bundle’s Service-Component manifest header is read, although the XML files may reside in the bundle fragments.
A possible use case for doing this is if you want to support several different component configuration options where you choose at deployment time which is instantiated.
We don’t classify this as a recommended use case, because fragments bring in all sorts of complex lifecycle issues and break a number of best practices with respect to modular boundaries, but we cover it here for the sake of completeness.
This Circle class is exactly the same as the prior version.
All you do is drop the associated bundle activator and add the component description instead.
Before moving on to consuming services, let’s discuss component properties a little further.
For the circle component, you use the component properties to specify service properties.
This follows a pattern of property propagation similar to what you saw in chapter 9 for Configuration Admin, where configuration properties are attached to the service.
In the paint example, you statically declare the properties in a component XML file using the <property> element.
This is the last place a Declarative Services framework looks for component properties.
The priority of these properties is as listed, with the first having the highest priority.
This means properties coming from a higher-priority source override the same property coming from a lower-priority source.
This precedence behavior allows a developer to assign default component properties, but another user or system administrator can change the configuration at execution time to suit their specific needs.
In the previous version of the paint program, you added the shape icon object directly as a service property using the following code:
In the vast majority of situations, this limitation isn’t a big deal, but it’s something to consider.
We’ll return to this topic briefly in the next section and in the following chapter when we discuss the Blueprint and iPOJO component frameworks.
Providing services is straightforward: a component declares which internal class implements which provided service interface, and the Declarative Services framework publishes the service at execution time after the bundle containing the component descriptions is activated.
What happens if the component has dependencies on other services?
To see how Declarative Services components can use services, let’s turn our attention to the paint program’s paint frame.
There’s quite a bit of information in this component description.
Looking at some of the aspects that are similar to the circle component, you see the component declaration, but this time you assign paint as its name.
You introduce a new element B, defining a “reference” (a dependency) to a service implementing the SimpleShape interface.
There are other servicedependency characteristics, but before we go into the details of those, let’s talk more about binding methods.
The Declarative Services specification defines the following method signatures for binding methods:
The first form injects the service’s associated ServiceReference into the component instead of the service object itself.
This allows the component to find out which services are available in the framework without retrieving them.
This method is typically used in conjunction with the ComponentContext, which we’ll discuss a little later, to implement extremely lightweight solutions where service objects are created only when absolutely necessary.
Using this binding method, the Declarative Services implementation retrieves the actual service object from the OSGi service registry and injects it into the component.
The component developer may choose to store a reference to the service object; but you must take care to dereference the service when the corresponding unbind method is called, to prevent memory leakage.
The third form behaves much like the second, except that the associated service properties are also injected into the component.
Because you need the service properties to retrieve the shape name and icon for the paint frame component, this is the form you’ll use.
The Declarative Services framework calls this addShape() method when any SimpleShape service is published in the OSGi service registry, passing in the service and the associated map of service properties.
You read the name property of the shape and load its ImageIcon representation.
As we mentioned earlier, the Declarative Services specification is only able to handle simple property types, so in this version of the paint frame component you have to explicitly load the resource via the shape object’s class loader.
Finally, you store a reference to the shape service object in an internal map for use later.
You use the binding method form that supplies the service attributes as a map.
You use the name property from the map to figure out which component has been removed and remove your reference to the service object from the internal map.
Now, let’s return our attention to how components describe their service dependencies.
So far, we’ve discussed that the component service dependency description includes a service interface and binding methods.
Many service dependencies fall into the category of being a hard dependency on a single service object.
But what if your component is different? Recall the following snippet from listing 11.3:
We haven’t discussed the cardinality and policy dependency characteristics yet, but they help you address more sophisticated service dependency situations.
The notion of cardinality plays two roles in the Declarative Services specification:
Binding method accessibility You may have noticed that the binding methods you’ve defined have package-private visibility.
The Declarative Services specification states the following with regard to method visibility:
As a matter of best practice, you should generally protect binding methods, because doing so prevents external code from injecting services out of band of the main service-registry lifecycle (assuming the Java security manager is enabled—we’ll look at security in chapter 14)
From the snippet, you can see that the paint frame has an optional, aggregate dependency on shape services; this means it wants to be bound to all available shape services, but doesn’t need any to function.
Cardinality is fairly straightforward, but the dependency policy is a little trickier to understand.
A component service dependency can be declared with either of two policy values: dynamic or static.
What does this mean? A dynamic policy means that the component is notified whenever the service comes or goes, whereas with a static policy, the service is injected once and not changed until the component is deactivated.
In essence, if you use a dynamic policy, your component needs to cope with the possible issues (such as threading and synchronization) resulting from service dynamism.
If you use a static policy, you don’t need to worry about issues related to service dynamism, but your component sees only one view of the services published in the OSGi registry while it’s active.
For example, the paint frame component specifies a dynamic policy.
Therefore, if a shape it’s using goes away, it sees the change immediately and dynamically adapts accordingly.
You’ve seen this in earlier examples, where you dynamically added and removed shapes.
If this dependency were specified as static, then if a shape service being used by the paint frame departed, the paint frame component instance would need to be thrown away, because a static policy means the component isn’t programmed such that it can handle service dynamism.
We’ll continue this discussion about component lifecycle in the next subsection.
To illustrate, let’s look at the WindowListener component of the modified paint program; its Declarative Services component description is as follows.
The WindowListener component has a static, singular, and mandatory dependency on a Window service B.
You specify a target LDAP filter to locate the specific Window service of interest C; recall that the filter references the property you associated with the PaintFrame component’s Window service in its component description in listing 11.3
Additionally, the WindowListener component has a dynamic, singular, and optional dependency on a log service.
Filtering services based on attributes is relatively easy and, at first glance, dealing with optional services appears equally easy.
But there are some subtle mechanics of which you need to be aware when using the dynamic dependency policy.
Target reference properties You saw earlier that component properties can be used to define the service properties associated with a component’s provided service.
Component properties can also be used to configure service-dependency target filters at execution time.
To do this, the property name must be equal to the name associated with the service reference appended with .target.
In this case, you could override the window target using a property of this form:
This binds the window listener to windows attributed with the name=other identifier.
Doing this directly in the static component description is of relatively low value.
But if you remember the discussion on component properties, these values can also be set at execution time via the Configuration Admin Service or using component factories, which opens up a set of interesting use cases.
You use an AtomicReference to protect yourself from threading issues related to the service being bound or unbound while your component is using it.
You also need to be aware of the fact that the LogService may in fact not be bound because it’s optional, so you check whether the service is bound and log a message if so.
The use of a wrapper method to achieve this is one possible mechanism; for a more advanced solution, you could use null objects to protect other areas of code from this execution-time issue.
So far, you’ve seen how to describe components that publish and consume services, but we’ve only indirectly discussed component lifecycle management.
Next, we’ll provide more details about the lifecycle of Declarative Services components.
Having described your components, the next issue to consider is their lifecycle.
When are components created? When are they destroyed? Are there any callbacks at these stages? How can you access the BundleContext if there is no BundleActivator? We’ll deal with each of these questions in this section.
In chapter 3, we introduced the bundle lifecycle: in essence, bundles are installed, then resolved, and then activated.
Declarative Services defines a similar lifecycle for components, where they’re enabled, then satisfied, and then activated.
The Declarative Services specification defines the following stages to a component lifecycle:
Enabled—A simple Boolean flag controls whether the component is eligible for management.
Satisfied—The component is enabled, its mandatory dependencies are satisfied, any provided services are published in the service registry, but the component itself isn’t yet instantiated.
Activated—The component is enabled, its mandatory dependencies are satisfied, any provided services are published in the service registry, and the component instance has been created as a result of a request to use its service.
Modified—The configuration associated with the component has changed, and the component instance should be notified.
Deactivated—Either the component has been disabled or its mandatory dependencies are no longer satisfied, so its provided services are no longer available and its component instance, if created, is dereferenced for garbage collection.
You can enable/disable a component declaratively using the enabled attribute of the <component> XML element and programmatically using the ComponentContext.
A simple use case for this is to reduce startup time by disabling all but a small number of components and then enabling additional components later as needed.
Similarly, neither the enabled nor satisfied stages result in instantiating the component class in an effort to avoid unnecessary work.
A component can become satisfied only if all of its mandatory dependencies are satisfied.
After it’s satisfied, it may become activated if its provided service is requested.
Each component description ultimately is reified as a single component instance that will be managed by the Declarative Services framework; by default, a one-to-one mapping exists between a component description and a component instance.
The component lifecycle is coupled to the lifecycle of its containing bundle.
Only components in activated bundles are eligible for lifecycle management.
If a bundle is stopped, the Declarative Services framework automatically deactivates all activated components contained in it.
Let’s dive into some code to see what this means in practice for the paint application.
You define a default, no-argument constructor for the component class; Declarative Services component classes must define such a constructor.
You define an activate() callback method to be invoked by the Declarative Services framework when the component is activated, along with a corresponding deactivate() callback method to be called when the component is deactivated.
You may be wondering about the Map passed into the activate() method, which you use to configure the size of the component.
The Declarative Services lifecycle callback methods accept a number of different argument types to give the component additional information, which we’ll describe next.
They may also accept zero or more of the following argument types (ordering isn’t important):
Declarative Services callback methods The names of callback methods (activate() and deactivate()) are defaults.
If you wish to use a different pattern or are migrating legacy code, you can define the names of these callback methods via attributes on the <component> XML element.
The activation and deactivation methods are optional, so if your component has no need to track its activation state, you can leave them out.
Also, if you use the default activate() and deactivate() method names, there’s no need to define these in the component declaration because the Declarative Services framework will discover them automatically.
Although it isn’t shown in the example, there’s also a callback method for the modified component lifecycle stage.
Unlike the activation and deactivation methods, this lifecycle callback has no default method name, so you must define the method name in the <component> element, as follows:
This indicates that the component is interested in being notified about configuration updates and specifies the name of the callback method.
The Declarative Services framework creates a unique ComponentContext object for each component it activates.
This object plays a role for components similar to the role the BundleContext object plays for bundles—it provides access to execution environment facilities.
The getProperties() method allows a component to access its configuration properties.
The methods for locating services provide an alternative approach to service injection for using services; this alternative strategy is discussed in the “Lookup strategy” sidebar.
The getBundleContext() method provides access to the containing bundle’s BundleContext object.
The getUsingBundle() method is related to component factories, which we’ll discuss later.
In the general case, whether or not a component is satisfied is dictated by whether or not its service dependencies are satisfied.
But this isn’t the only type of dependency considered by Declarative Services; another situation is where a component is dependent on its configuration properties.
We’ve mentioned that it’s possible to configure a Declarative Services component by specifying an entry in the Configuration Admin Service with PID corresponding to the name of the component.
These configuration properties override any specified in the XML description and provide a way to tweak the behavior of a component at execution time.
It’s also possible to define a policy for how dependencies on configuration properties should be handled.
Declarative Services defines the following configuration policies: optional, require, and ignore.
The default configuration policy is optional, indicating that Configuration Admin will be used if available.
Lookup strategy So far in this section, we’ve shown you how to inject services into components using binding methods.
This pattern is known as the Hollywood principle: “Don’t call us, we’ll call you.” In some circumstances, it’s useful to apply the Reverse Hollywood principle, “Do call us, we won’t call you.”
The Declarative Services specification supports both approaches; it refers to the injection approach as the event strategy and the alternative as the lookup strategy.
The event strategy provides instant notification about service changes, whereas the lookup strategy is able to defer service creation until the last minute.
The family of locateService() methods on the ComponentContext facilitate the lookup strategy.
These methods each take a String argument that specifies the name of the associated service reference in the component description XML file to retrieve.
For example, you could change the paint frame description to be the following:
In this case, you don’t have bind or unbind methods.
To access any bound services, you need to use the ComponentContext, like this:
You can use the event strategy for some service references and the lookup strategy for others.
This option provides a highly responsive but lightweight approach to service binding.
Declarative Services won’t be satisfied until there’s a configuration for it in Configuration Admin.
The require policy is useful in cases where no sensible default value can be given for a configuration property (such as a database URL)
The ignore policy indicates that only the declared component properties should be considered.
In this example, the component requires a corresponding configuration to be present in Configuration Admin, or else it can’t be satisfied.
Components can be satisfied by the availability of their service dependencies and configuration, but the Declarative Services framework still won’t activate (instantiate) a component until another bundle requests its provided service.
What about a component that doesn’t provide a service? How will it ever be activated?
Many components, such as the shape components in the paint example, exist solely to provide a function to other parts of an application.
If no other deployed bundles consume the services these bundles provide, there’s no need to expend resources activating the associated components.
Components that provide services are delayed by default in Declarative Services.
If you look again at the component declaration of the paint frame, you see that it specifies the immediate="true" attribute:
This turns off the delayed behavior and forces the Declarative Services implementation to construct the paint frame as soon as it’s satisfied.
Because the paint frame component provides a service, it would be delayed by default.
The first takes a single configuration and uses it to configure a service provided by a bundle; the second allows multiple configurations to be createdeach corresponding to a new service.
If the component name matches a registered PID for a nonfactory configuration, the Declarative Services framework creates a single instance of the component.
But if the component name matches a registered factory PID, a new component instance is created for each configuration associated with the factory PID.
This provides a lightweight way of constructing several different component instances from a common component definition.
For components that don’t provide a service, it’s an error to set immediate to false because they would never be instantiated; instead, they’re implicitly defined as immediate components.
The final tool in the Declarative Services toolbox is component factories.
These provide a programmatic mechanism to instantiate new instances of components.
In many ways, this is similar to the factory PID mechanism of creating components mentioned earlier; but instead of going via the ConfigurationAdmin interface, the Declarative Services specification provides a mechanism to declare a component as explicitly providing a factory API, which is then manipulated by a secondary service to construct actual instances of the components.
To see how this works, let’s consider a slight variation of the original paint example where a shape component factory is registered to provide shape components on demand.
This results in the Declarative Services framework publishing a ComponentFactory service into the service registry.
Component factory services can be used like any normal services; for example, client code wishing to be injected with this component factory can do the following:
Here the target attribute of the reference element is set to the name of the factory attribute of the declared component.
To create a new shape instance, you use the following code.
The component factory is registered with the ShapeManager component using callback methods B.
This concludes our introduction to component models in OSGi and review of the Declarative Services specification.
Type ant to build the example and java -jar launcher.jar bundles to run it.
In this chapter, we reviewed the general principles and motivation of componentoriented programming and looked at how components and modules intersect and interact in an OSGi context.
Components are application building blocks that adhere to a component model.
Components further support separation of concerns by separating interface from implementation.
Components support external management of concerns, allowing you to offload mundane and potentially error-prone tasks to component frameworks.
The OSGi framework is a component framework, where bundles are equivalent to components that interact via services.
Configuration Admin Service factories The component factory provides an alternative mechanism to the Configuration Admin managed service factory approach mentioned earlier.
Which approach you take is largely a matter of preference.
Note that the component factory approach and the managed service factory approach are mutually exclusive: it’s not possible to create a component factory that is instantiated by a factory PID.
Declarative Services is an OSGi standard component framework that manages service publication, service dependencies, and configuration dependencies on behalf of components.
Component orientation in general and Declarative Services in particular are worthwhile approaches when you’re working with OSGi.
In the next chapter, we’ll push even further by looking at two more advanced component frameworks, in case Declarative Services doesn’t address all your needs.
In the last chapter, we introduced you to component-oriented programming and how it relates to OSGi.
We also introduced a lightweight component framework defined by the OSGi Alliance, called Declarative Services, which you used to recreate your paint program.
Declarative Services is just one possible component framework for OSGi.
In this chapter, we’ll introduce you to two more: Blueprint and iPOJO.
These component frameworks provide more advanced capabilities than Declarative Services.
The numerous component frameworks for OSGi may at first seem daunting, but the good news is that you aren’t necessarily constrained to a single decision for all time.
One of the popular component frameworks in Java today is Spring Beans.
The Blueprint Container specification (Blueprint for short) from the OSGi R4.2 Enterprise specification is based heavily on the Spring/OSGi integration work done in Spring Dynamic Modules.
One benefit of standardizing this work is that it has resulted in several implementations of this specification from other vendors, including the Apache Aries and Eclipse Gemini projects.
Let’s look into the Blueprint architecture, after which we’ll discuss how you can build the paint program using Blueprint.
Blueprint defines a component in terms of a number of elements, each of which has an underlying manager in the Blueprint component container.
Each Blueprint component definition can contain zero or more of the managers listed in table 12.1
Bean Provides components with the same basic semantics as Spring beans:
Reference Gets a single service from the OSGi service registry for the component based on the service interface and an optional filter over the service properties.
One to rule them all? It may seem confusing that the OSGi Alliance has defined two “standard” component frameworks: Declarative Services and Blueprint.
Both specifications are interoperable at execution time (see section 12.3) via services, so either can be used to implement a given service interface without impacting clients.
Declarative Services focuses on building lightweight components with quick startup times.
We see this throughout computing: often there are numerous ways to accomplish similar, but not quite identical, tasks.
From the OSGi Alliance’s perspective, it makes sense to have different communities standardizing their approaches around OSGi technology, rather than trying to dictate a single approach for everyone.
Let’s now look at a concrete example of Blueprint in action.
In this section, we’ll explore how to use the Blueprint specification to build the example paint program.
As with Declarative Services from the previous chapter, we’ll start by looking at the converted circle bundle.
Again, this bundle no longer contains a bundle activator class, but it does contain the circle.xml file shown in the following listing.
If you’re familiar with Spring, some of this XML should look a little familiar.
For example, Blueprint uses the <bean> element to describe components, which you use here to define the circle component.
You also use it again to specify a value for a service property.
Spring users should also be familiar with the <entry> element, which is used by Spring and Blueprint to define the entries of map objects.
You use <entry> in this case to define the service properties for your service interface.
The <service> element, which you use to publish the bean associated with the ref identifier circle into the OSGi service registry with the declared set of nested service properties.
Reference list Gets one or more services from the OSGi service registry for the component based on the service interface and an optional filter over the service properties.
Environment Provides components access to the OSGi framework and the Blueprint container, including the bundle context of the component.
Let’s dig a little deeper into the details of precisely how you provide services using Blueprint.
At a glance, there appear to be only syntactic differences between the Declarative Services version of this component and the Blueprint one.
There’s one big functional difference: the ability to define complex attribute objects.
Blueprint introduces a factory bean concept, which you use in the example to create an ImageIcon service property.
This factory bean allows you to provide a class to perform the nontrivial actions required to create an ImageIcon from the XML model.
The factory-bean pattern also lets Blueprint create objects with nontrivial constructors and use them in the component as services, parameters, or service properties.
To provide the circle’s SimpleShape service, you directly specify the interface name as an attribute of the <service> element.
Blueprint supports a couple of other options for supplying the service interface: the <interfaces> element and the auto-export attribute.
Given this bean and class definition, you can describe the component’s provided Foo service in any of the following equivalent ways:
The first is the longhand form of a service definition, which allows a Blueprint component to export more than one interface for a given bean.
The second is the shorthand form for explicitly exporting a single service interface.
The last is an automatic form, where the service manager reflectively calculates the interfaces under which the bean should be registered.
For this last form, you must specify one of the following autoexport attribute values:
After you’ve described your component and the services it provides in your XML description, you need some way to tell the Blueprint implementation about it.
As with Declarative Services, Blueprint introduces a new manifest header to reference the component description file.
If you examine the circle bundle’s manifest, you see it has the following entry:
Following the same approach as Declarative Services, Blueprint employs the extender pattern and probes bundles to determine if they contain this manifest header.
If so, the Blueprint implementation manages the contained components; if not, the bundle is ignored.
A limitation for providing services Blueprint requires using Java interfaces for services, whereas basic OSGi and Declarative Services allow (but don’t recommend) you to use a Java class as a service.
You may wonder why Blueprint requires services to be interfaces.
The part before is the path of the directory in the bundle class path.
One interesting point to note is that a single logical component can be defined over many Blueprint XML files.
This idea is borrowed from Spring Beans, where it’s useful if you need to switch the behavior of a component in different environments—say, for local testing versus enterprise deployment.
We’ve covered the basics of providing services with Blueprint; next, let’s look at how you consume them.
Blueprint tries to simplify dealing with the dynamic nature of OSGi services by using a proxy approach, instead of binding components directly to an OSGi service.
The injected proxy is backed by actual service objects from the service registry, as shown in figure 12.1
Service proxies are injected into Blueprint components using either the reference or reference-list manager.
The easiest way to demonstrate service reference behavior is with a simple example of injecting a service A into a client B.
In this example, you have a class B that depends on a service A, which will be injected via a setService() method.
With fragments, it’s possible to define different component configurations by installing different fragments into the OSGi framework, where the host bundle is the base component bundle.
In real-world scenarios, you can use this approach to specify entirely new object graphs or service relationships.
This is potentially powerful but also very low level because it relies on intimate knowledge of the components being extended in this fashion.
Figure 12.1 Blueprint injects a proxy object, which masks changes in the OSGi service registry.
If the underlying service provider goes away and returns, the client code is insulated from this dynamism.
Given this declaration, the Blueprint container knows it must inject an instance of A from the service registry into the component by calling the setService() method.
It’s also possible for Blueprint to inject the service proxy via a constructor.
The following class C has a dependency on service A, which is injected via a constructor argument:
In this case, the Blueprint service-dependency description looks like this:
What if your client code doesn’t depend on a single service instance, but instead wants to aggregate several service instances? The following example shows a class D that aggregates many A services from the OSGi service registry:
In this case, class D is injected with a proxied list that aggregates the OSGi services.
Changes in the OSGi service registry are reflected in the list.
New services are appended to the end of the list, and old services are removed.
The XML to describe this sort of dependency in Blueprint is as follows:
This is similar to the previous dependency on a single service.
Proxies and service dynamism Blueprint uses proxies to mask OSGi service dynamism so you don’t have to worry about the concurrency issues in your code.
Now let’s look at how Blueprint lets components have a dynamic view of services using reference listeners.
Reference listeners allow a component to receive services via bind and unbind callbacks.
In this example, class E receives callbacks via the addService() and removeService() methods when an A service is registered or unregistered in the OSGi service registry, respectively.
The body of the someAction() method must guard against the fact that the service may be null.
You express this sort of dependency in Blueprint XML as follows:
A reference-listener callback can have any of the following signatures, which have the same semantics as their Declarative Services equivalents (from the last chapter):
One issue to keep in mind: the Blueprint specification mandates that binding methods have public method access.
Although the risk is probably minor in most scenarios, it does open Blueprint components exposed as services to the possibility that external code using reflection can inject dependencies even if a security manager is.
The Blueprint specification ensures that the hasNext() and getNext() operations are safe with respect to changes in the OSGi service registry.
These types of issues aren’t specific to Blueprint, but to how Blueprint handles OSGi’s service dynamism.
Concerned users can work around this using a nonservice helper delegate that manages the reference list—although this is a lot of work compared to marking the methods as nonpublic.
Let’s use your newfound Blueprint knowledge and continue to convert the paint program to use it.
Your paint frame component has a dependency on SimpleShape services, and it provides a Window service.
The following listing provides its definition in the Blueprint XML syntax.
You begin by defining the paint frame component and specifying lifecycle methods to be invoked by the Blueprint container after its properties have been injected; we’ll leave the details of these for the next section.
You use the <reference-list> element at B to ask the Blueprint container to listen for all SimpleShape services it finds in the OSGi service registry.
Notice that you use the availability attribute on the <reference-list> element.
As with Declarative Services, Blueprint includes the notion of mandatory and optional service dependencies, where mandatory dependencies impact the component lifecycle and optional ones don’t.
We’ll discuss this more when we cover the Blueprint component lifecycle.
Possible values for the availability attribute are optional and mandatory; the default is mandatory.
This tells the Blueprint container to inject the services into the addShape() and removeShape() methods of the paint frame component.
You choose this approach because your paint frame needs to react dynamically to the arrival and departure of shape services.
Finally, you use the <service> element to publish the paint frame bean as a service in the OSGi service registry.
Recall that you can’t register classes as services in Blueprint; you must use an interface.
The addShape() and removeShape() methods for the paint frame component look basically the same as the Declarative Services example, but with one minor difference.
You’re given the Icon object directly versus having to look it up from the shape’s class loader, as is required for Declarative Services.
You’ve now seen how to define a component, how to publish services, and how to consume services.
The next area of the Blueprint specification we’ll explore is the component lifecycle.
Similar to Declarative Services, Blueprint is responsible for managing the lifecycles of its components.
Blueprint also supports lifecycle callbacks, eager/lazy component activation, and access to the execution-time container environment.
Additionally, Blueprint introduces some new concepts, such as service damping and grace periods.
We’ll look into each of these topics in more detail, starting with general Blueprint component lifecycle management.
Similar to Declarative Services, a Blueprint component’s lifecycle is controlled overall by its containing bundle’s lifecycle.
For individual beans, their lifecycle is tied to the state of their service dependencies.
All mandatory service dependencies must be satisfied for a bean to become enabled.
When a bean is enabled, any service managers associated with it can register their associated service interfaces in the OSGi service registry.
For Declarative Services, you learned that if a mandatory service dependency is broken, the component instance immediately becomes invalid.
Blueprint handles this differently via the concepts of damping and grace periods.
As we previously mentioned, Blueprint injects proxies into components instead of the actual service objects.
These proxies are the mechanism Blueprint uses to provide service-dynamism damping; said a different way, it uses this approach to hide service dynamism from the component.
When a mandatory service is removed from the service registry, the Blueprint container first stops providing any services offered by the impacted component.
Additionally, any attempts by the component to use the proxy associated with the missing service block the calling thread.
Blueprint proxies block for a configurable amount of time (five minutes by default)
Client code should therefore be coded defensively to gracefully deal with execution-time exceptions, but this is true in general and not just in Blueprint.
The benefit of this approach is it eliminates service unpublish/publish waves rippling throughout the framework during bundle updates.
But if you’re unaware, it can lead to unexpected lockups in your application, which is particularly interesting in GUI scenarios.
To configure the timeout behavior, you specify the timeout attribute on a service reference.
This value specifies the number of milliseconds to wait until the service reappears.
The timeout value must be equal to or greater than zero, where a timeout of zero signifies an indefinite wait.
In the window-listener component, you can see this in action:
Damping isn’t the only mechanism Blueprint employs to hide service dynamism; another one is grace periods.
Normally, a Blueprint component container doesn’t enable its component until the component’s mandatory service dependencies are satisfied.
Reference listener != service dependency One important point to note is that for a Blueprint container to treat a bean as dependent on a service reference, the bean must be bound to the service via an injection mechanism—either a property or a constructor argument.
A reference-listener callback is not treated as an injection mechanism.
So, even though a bean receives a callback when the service appears and disappears, it won’t necessarily stop providing its services unless it has the property or argument injection relationship as well.
The grace period ends in success if all mandatory dependencies can be satisfied or in failure if its timeout is exceeded.
If the grace period was a success, the component is enabled and its services are registered.
If the grace period was a failure, the container is destroyed and the component isn’t created.
You can see how this affects the Blueprint component lifecycle in figure 12.2
You can configure the grace period timeout value using directives on the BundleSymbolicName manifest header in the Blueprint component bundle.
Here, you set the grace period timeout to be 10 seconds.
In this case, the Blueprint container won’t wait for any mandatory service references to be.
Blueprint Container satisfied and will create any Blueprint components contained in the bundle.
This results in components being injected with service proxies, which may or may not have backing services available.
For those components without backing services for mandatory dependencies, the Blueprint container won’t publish their provided services.
This is similar to the case where required services depart at execution time, which means that if any threads try to use them, those threads are blocked.
In listing 12.2, you saw that the Blueprint XML declaration allows you to define callback methods that are invoked by the Blueprint container to initialize and destroy beans when they’re enabled and disabled, respectively.
In the paint frame component, you use these callbacks to control when the component is visible, as shown next.
You still have an issue regarding precisely when your component—in this case, the paint frame—is created.
As with Declarative Services, Blueprint components are lazy by default, which means components aren’t created in advance to delay class loading until a published service is requested by another bundle.
If you need your component to be created eagerly, you can request this behavior from the container.
You declare Blueprint managers as eager or lazy using the activation attribute on the associated XML element with a Boolean argument.
The laziness of a component is also impacted by how it provides its services.
If the autoexport attribute from section 12.1.2 is used, the Blueprint container must activate the underlying component to perform class loading to calculate the service interfaces.
The final lifecycle-related issue we’ll discuss is how Blueprint components gain access to the underlying OSGi execution environment.
As you’ve seen so far, the Blueprint specification uses managers to control various aspects (such as services, references, and beans) of a component.
Each manager defines and controls the specific lifecycle characteristics of its associated aspect.
The same pattern is applied to entities outside of the component; these are called environment managers.
The WindowListener needs the bundle context so it can retrieve the system bundle to shut down the OSGi framework when the paint frame is closed.
In the following snippet’s WindowListener component XML description, you use the environment manager to inject the bundle context into the WindowListener class as a property:
The implementation code in the WindowListener class looks like this:
The bundle context ends up being injected using an ordinary setter method.
With this, we’ll conclude the discussion of the Blueprint version of the paint program.
Type ant to build the example and java -jar launcher.jar bundles/ to run it.
This example uses the Apache Aries (http://incubator.apache.org/aries/) implementation of the Blueprint specification.
In the final section on Blueprint, we’ll look at some other advanced features it provides.
The features of Blueprint we’ve described so far are largely comparable to those of Declarative Services.
But because Blueprint is an evolution of Spring Dynamic Modules, a lot of experience from that work was carried over into creating the Blueprint specification.
This heritage has resulted in some advanced concepts, such as manager values, scopes, type converters, and metadata, which we’ll discuss in this section.
If you’ve developed Spring applications in the past, you likely know it’s possible to define complex object graphs using the Spring XML model.
This gives software architects a number of options when composing applications, because the overall program structure doesn’t need to be hardcoded in Java.
This is particularly useful for scenarios such as desktop testing versus enterprise deployment; with just a few tweaks of XML, you can wire together a raft of new functions without recompiling code.
Blueprint has inherited this ability and supports several constructs in the XML declaration:
It’s also possible to use the various Blueprint managers we’ve been discussing inline in these constructs.
Every Blueprint manager has a particular value it provides, similar to macro expansion.
You’ve already seen this in action in the way reference managers give access to underlying service references, where the value of a service-reference manager is either a service object or a ServiceReference to a service object.
For completeness, table 12.2 lists the value objects associated with each Blueprint manager.
This capability makes it possible to define reasonably sophisticated constructions in the component XML descriptions.
In this example, you construct a Foo object into which you inject its services property with a Map.
The Bar service is provided by an inlined bean, constructed from the BarImpl class.
Here, the FooImpl class is injected with a property whose value is wholly constructed from the Blueprint XML model.
This is definitely a contrived example, but it shows the flexibility of the Blueprint model.
As with manager values, Blueprint has inherited the concept of scope from Spring Dynamic Modules.
A scope can be applied to bean and service managers; it defines the lifetime over which the specified manager is applicable.
Blueprint defines two scopes—singleton and prototype—but they imply subtly different behavior depending on whether they’re applied to a bean or a service manager, as shown in table 12.3
The Blueprint specification defines a rich inversion of control (IoC) framework for wiring objects together at execution time.
Often, you need to convert between types in order to perform an injection.
The most obvious example is converting from String values in XML to integer, boolean, or other basic Java types.
The Blueprint specification defines a default set of type converters that can convert a String value to a target typed value.
The Blueprint specification also allows you to extend the default set of type converters.
Here, you define a type converter using the class AtomicConverter that takes a reference to the top-level Blueprint converter as an argument in its constructor.
A type converter doesn’t need to implement any specific interface, although it must implement two methods:
The code for the atomic conversion class is shown in the following listing.
Bean One instance of the bean object is constructed when the bean is activated.
This pattern is usually applied to stateless services and core components.
All inlined beans (which you saw in the last subsection) are automatically prototype scope.
Service A single service object is shared by all clients of the service.
A new service object is returned to each bundle, which provides a similar result if your bean implements an OSGi ServiceFactory.
Listing 12.4 Converter class to coerce an Object to an AtomicReference.
The canConvert() method checks whether it can convert the supplied object to the given reified type.
The convert() method is called by the Blueprint container if the canConvert() method returns true.
The top-level Blueprint converter is injected into the constructor of the AtomicConverter class to allow it to convert generic arguments.
For example, AtomicConverter can use the top-level converter to convert a source object to an Integer and then create an AtomicReference to this converted object.
The Blueprint built-in type converter delegates to registered converters, so a call to the converter during construction may fail because a needed type converter may not have been registered yet.
Having defined this converter, any injection targeting an AtomicReference<T> value is automatically converted into an AtomicReference of the appropriate type using the example converter.
This pattern of conversion is useful if you have to adapt third-party code that you can’t change, but you nonetheless want to have a common model at execution time.
Next, we’ll discuss metadata, which is the last advanced Blueprint feature before we move on to the iPOJO component framework.
Metadata is a programmatic representation of the XML description of the Blueprint component.
In Spring, the main use case for accessing metadata is to dynamically modify the model (add new elements, modify elements, and so on) at execution time.
This forms a flexible component pipeline much like aspect weaving in Java code, but at the component level.
But in the current Blueprint specification, this model is largely for informational purposes.
Still, it’s useful because it can be used to build diagnostic tools to analyze the structure of Blueprint components, for example.
The Blueprint specification defines many classes to model the Blueprint components.
We won’t list every method and its meaning here, but figure 12.3 provides a view of the Blueprint metadata interface hierarchy.
To access the metadata model, the Blueprint specification provides the BlueprintContainer interface, which serves a purpose similar to the ComponentContext in Declarative Services.
This concludes our look at the OSGi Blueprint Container specification.
We’ll now turn our attention to the last component framework on our list: iPOJO, from the Apache Felix project.
Outside of the OSGi Alliance, a number of different component models have been built for or ported to the OSGi environment:
In this section, we’ll focus on iPOJO due to its novel features and because we (the authors) are all involved in the Apache Felix project.
One of the main goals of iPOJO is to simplify creating dynamic, service-oriented applications in OSGi.
The biggest difference between iPOJO and Declarative Services or Blueprint is its approach, which includes the following:
High level of extensibility—The component management features provided by the iPOJO component container are implemented by handlers from which you can pick and choose.
You can also create custom handlers for specific management tasks; see figure 12.4
In most of the remainder of this chapter, we’ll explore the features of iPOJO, but we won’t cover everything.
For starters, we’ll focus on using the annotation approach for describing components, because you’ve already seen enough XML and API.
But keep in mind that everything you do with annotations you can do with the XML- and API-based description approachesit depends on your preference.
This instrumentation inserts hooks into the component class file so it can be externally managed.
Although iPOJO also supports execution-time byte-code instrumentation, the simplest way to get it done is with a build-time step to process your components.
To achieve this, iPOJO integrates with Ant, Maven, and Eclipse.
As an example, here’s the Ant task for the circle bundle:
Upon completion of this build step, iPOJO has instrumented the byte code of any components contained in the referenced bundle.
The details of how iPOJO instruments the component byte code aren’t as important; but for the curious, iPOJO instruments all components in a single, generic way to enable intercepting member field and method accesses.
All functionality provided by the iPOJO component framework (providing services, requiring services, and so on) is provided by handlers using these interception hooks.
The hooks themselves don’t change the component behavior; the modified component classes behave the same as before and can still be use without iPOJO, although they now have some code dependencies on it.
Figure 12.4 iPOJO components are an aggregation of handlers attached to the component container at execution time.
In addition to instrumenting the component byte code, the iPOJO build step also converts the component description to a canonical form.
If you use XML to describe your components, an XML parser is needed only during build time and not at execution time.
If you use annotations to describe your components, they’re needed only at build time, and the resulting JAR file can still be used on an older JVM.
Parsing the resulting descriptions at execution time is more efficient, because it’s in a simplified and less verbose form.
As with the other component frameworks, you want to achieve three main tasks with iPOJO: publishing services, consuming services, and configuring components.
As we mentioned, iPOJO supports a variety of approaches for describing components.
As before, let’s start with the circle component; the iPOJO version of its source code is shown in the following listing.
One of the benefits of using annotations is that the metadata is closely associated with the source code it’s describing.
Listing 12.5 iPOJO declaration of circle component type using annotations.
What about that XML file? We said you’d use annotations, but the previous Ant task references a circle.xml file.
In Declarative Services and Blueprint, a component description is typically a component instance description, meaning it results in a component instance.
In iPOJO, component descriptions describe a component type; instances must be explicitly instantiated.
As you’ll soon see, the circle.xml file doesn’t describe the component: you use it to create an instance.
You use @Component to declare the Circle class as an iPOJO component; see the sidebar “Immediate components and service properties” for why you use the immediate flag.
With the @Provide annotation, you indicate that your component provides a service.
You leave iPOJO the task of determining the type of the service, which defaults to all implemented interfaces (only SimpleShape in this case)
Notice also that because you’re using annotations, which are part of the Java source code, you can use static constant fields for the attribute names, unlike in Declarative Services or Blueprint; this greatly reduces the risks of metadata rot due to changing attribute names.
You can also explicitly specify interfaces or classes to provide, as shown in the following snippet:
As with Declarative Services and Blueprint, the circle bundle no longer needs to have a bundle activator because iPOJO manages service publication.
As mentioned previously, you have to modify the component’s build process to include the iPOJO Ant task, but that’s all there is to it.
Immediate components and service properties Just as with Declarative Services and Blueprint, iPOJO delays class loading and component instance creation for as long as possible.
Sometimes this delay is inconvenient, and you want the component created immediately.
When using @ServiceProperty, iPOJO uses the member field value as a service-property value.
But if component creation is deferred (which is the default behavior), iPOJO can’t get the field value because the field doesn’t yet exist.
As a result, iPOJO first registers the service with no service properties.
When the service is requested by another component, then the component is instantiated, which causes the field to be initialized and added to the service.
To rectify this situation, @ServiceProperty supports a value attribute to set the default value of the service property; but this works only for simple types, not for complex types like this example’s icon.
To deal with complex types, you need to use the immediate attribute of @Component to tell iPOJO to instantiate the component immediately.
In most cases, the two can be used interchangeably or mixed and matched.
In either case, a component service dependency description can include (among others)
Given the similarities between method and field injection, the approach you choose often comes down to preference.
Still, there are some things you can do only with one or the other.
For example, if you want to be notified whenever a desired service becomes available, you need to use method injection to get a callback, which makes it possible to react immediately to dynamic changes.
Yet it’s possible to use field injection and method injection at the same time to get the best of both worlds.
We’ll first explore method injection, because it’s similar to the mechanisms you saw in Declarative Services and Blueprint.
These annotations can be applied to methods with any of the following signatures:
To proxy or not to proxy By default, iPOJO injects proxies instead of the actual service object.
This creates a managed reference to the service that can be passed around internally in the component.
For platforms like Android where dynamic byte-code generation isn’t supported, iPOJO reverts to Java’s dynamic proxies.
Note that iPOJO proxies aren’t like Blueprint proxies, in that they don’t do any sort of blocking of the calling thread if no backing service is available.
Instead, by default, they hide the fact that the service is missing by using the null-object pattern, which we’ll discuss shortly.
In the first two cases, you need to specify the type of the service dependency using the specification parameter in the annotation; in all the other cases, iPOJO infers the service type from the method signature.
The binding methods for the window-listener component are as follows:
You annotate the bind and unbind methods right in the Java code.
The particular window service in which your window listener is interested has a name service property with the value main, to differentiate it from other window services that may be in the service registry.
To make sure your window listener tracks the correct window, you use the filter attribute of the binding annotation to specify an LDAP filter matching the name service property.
This particular dependency is on a single service instance, which is the default in iPOJO.
How do you declare an aggregate dependency? You can see an example in the iPOJO version of the PaintFrame shown in the following listing.
Listing 12.6 Bind and unbind methods for the iPOJO PaintFrame.
Because the paint frame depends on all available shape services, you use the aggregate annotation attribute to inform the iPOJO framework.
At execution time, iPOJO injects all discovered shape services into the component.
The service properties of injected services are also needed to get the service name and icon, so you use the binding method signature that includes the service properties.
All the component frameworks we’ve covered provide mechanisms to simplify the task of accessing OSGi services; but accessing services is only one part of the challenge.
Another issue is dealing with the dynamic nature of services.
If services can come and go at any point, you must code a service consumer defensively.
Doing so involves one or more of the following patterns:
Bind/unbind method pairs Conceptually, a bind method is paired with an unbind method.
You aren’t technically required to have both, but if you do, iPOJO treats them as a logical pair.
This is because iPOJO creates a union of attributes out of paired bind/unbind methods, so it isn’t necessary to repeat the attributes.
If you do repeat attributes, they must have the same value, or iPOJO complains.
If the method name starts with bind or unbind, the remaining part of the method name is used as an identifier to match pairs.
For example, iPOJO determines that bindFoo() and unbindFoo() are a matched pair with an identifier of Foo.
Sometimes it isn’t possible to name your methods following the bind/unbind naming convention: for example, if you’re dealing with legacy or third-party components.
In these cases, you can use the id annotation attribute to explicitly specify the pair’s identifier.
Using timeouts if the service removal is only temporary (during a software upgrade)
Declaring dependencies as mandatory such that a component is shut down if its dependencies become unsatisfied during execution time.
As we mentioned previously, iPOJO performs byte-code instrumentation on components to enable field-access interception.
For the @Requires annotation, the iPOJO framework intercepts field access at execution time to provide components access to their required services.
At a very high level, this acts as if you’ve sprinkled your code with a liberal number of AtomicRefererences.
This ensures that the component always sees a consistent view of the services as they appear in the OSGi service registry at a given moment, without all the tedious boilerplate synchronization code in the source files.
The @Requires annotation also works with collections or arrays to aggregate multiple services from the OSGi service registry.
In addition, it can create default objects or null objects if an optional service isn’t available, which greatly simplifies your source code because you don’t need to perform null checks throughout.
Let’s look at how you can use these features in the paint program.
In Declarative Services and in Blueprint, you use an AtomicReference to ensure that you have a consistent view of the service in your component.
In iPOJO, you declare the log service dependency on a field, like so:
To access the log service, you use the field like this:
In Declarative Services and Blueprint, you have to use the AtomicReference to hold the log service and then check for null before using it.
In iPOJO, you can use the log service, because optional dependencies automatically receive a null object if no real.
A null object implements the target service interface but doesn’t do anything meaningful.
The @Requires annotation goes even further with respect to service dynamism.
The iPOJO runtime ensures that a given field access always returns the same service instance from the moment in time a given calling thread enters a component method and uses a service until it ultimately exits the original entry method.
This means that even if the calling thread somehow calls out to another component and reenters the original component, it always sees the same service instances until it exits the original component once and for all.
Essentially, iPOJO associates a shadow copy of a component’s field after.
Null objects and default implementations Unless you explicitly tell it not to do so, iPOJO injects a null object in place of missing optional service dependencies.
These null objects are created using a trivial mock object pattern where any method that returns void takes no action, and methods that return values return the default false, 0, or null, depending on which is appropriate.
If you’re using service proxies (which is the default), this means the service proxies are injected with null objects if a backing service isn’t available.
If you aren’t using proxies, then your component is injected with a null object directly.
This approach saves you from having to check for null in your component code.
If you don’t desire this behavior, you can disable null object creation like this:
If you disable null objects and you’re using proxies, your component code must be prepared to catch runtime exceptions when accessing a proxy object if the backing service is missing (similar to the unavailable service exceptions in Blueprint and indicative of OSGi service dynamism in general)
If you aren’t using proxies, you’ll need to check for null service values in your component code.
When using proxies, it’s recommended to keep the default behavior of null object creation, because the whole point of proxies is to try to insulate the component from dynamism, but the choice is yours.
In the case where you’re using null objects without proxies, it’s possible for your component to determine whether it has a null object using instanceof, because all null objects implement the Nullable interface.
As a final comment, because a null object is just a default service implementation that doesn’t do anything, iPOJO provides one more wrinkle.
You can supply your own default service implementation instead of the normal null object:
When you do this, iPOJO constructs an instance of the DefaultFoo class and injects it into the proxy or component whenever a real Foo service is unavailable.
Suppose you have a method that does something like this:
This allows iPOJO to simplify the task of dealing with stateful services in the dynamic service environment provided by OSGi.
This is cool, but it doesn’t mean you don’t have to worry about anything! Due to dynamism, accessing a service is similar to using remote services, which means they can throw exceptions for unknown reasons.
For example, if Foo represents some device that becomes physically disconnected, its service methods are likely to throw exceptions when you access them.
In short, you still need to code defensively, just as in distributed computing.
You now know how to describe your components’ provided and required services.
Like the other component frameworks you’ve seen, your components’ lifecycles are controlled and managed according to these component characteristics.
We’ll look more deeply at the iPOJO component lifecycle next.
As with the other component frameworks, iPOJO component instances are either valid or invalid depending on whether their mandatory service dependencies are satisfied.
When a component is valid, iPOJO can publish its provided services in the OSGi service registry.
When a component is invalid, iPOJO must remove its provided services from the service registry and release the associated component instance if one was created.
At execution time, the iPOJO runtime watches for bundles containing components to be installed into the running OSGi framework.
After these bundles are activated, iPOJO takes over their management.
In addition to treating all service dependencies as either mandatory or optional, iPOJO treats them as either static or dynamic; this is called a binding policy.
This concept is also present in Declarative Services and has the same meaning here.
The best way to understand the difference between a static and dynamic service dependency is to consider a specific service dependency, such as an aggregate dependency on the SimpleShape service.
For a component with a dynamic, aggregate dependency, iPOJO adds services to and removes them from the component at execution time as the associated services appear and disappear in the service registry without invalidating the component instance (in other words, the component instance lifetime spans service dynamism)
For a component with a static, aggregate dependency, iPOJO injects the component with a snapshot of the available services when the component instance was created.
The main benefit of using static service dependencies is that your component code is simpler because it never has to worry about dealing with dynamism; but, by default, iPOJO assumes service dependencies are dynamic.
You can explicitly choose which binding policy iPOJO uses for a given service dependency.
The binding policy should be determined on a case-by-case basis for each of your component’s service dependencies.
This gives you pretty rich control over your component’s service dependencies, but sometimes this still isn’t sufficient—for dependencies that are potentially very short-lived, for example.
Service dependencies are generally mandatory (they must be satisfied to instantiate the component) or optional (they aren’t needed to instantiate the component)
But some types of service dependencies don’t fit neatly into these two categories.
For example, perhaps your component needs a specific service during startup but then never needs it again.
Such a dependency can’t be declared optional, because you need it at startup.
At the same time, if you declare it mandatory, and it goes away later, your component instance will be invalidated even though it didn’t need the service anymore.
In this scenario, the component only has a dependency on the service at a particular point in time.
For this reason, iPOJO supports temporal service dependencies, which don’t impact the overall component lifecycle like optional dependencies, but.
How does iPOJO ensure this? It blocks the calling thread if a matching service isn’t available.
Declaring a temporal dependency is similar to a normal service dependency.
Although the name is the same, this isn’t the same @Requires annotation.
By using it, whenever a thread accesses m_log, it either gets a log service or blocks until one is available.
You can use the timeout annotation attribute to specify a timeout value, which when expired results in a service exception.
If you’d rather not receive an exception, you can use the onTimeout annotation attribute to indicate that you’d rather receive a null value, a null object, or a default implementation.
The @Validate annotation is applied to component methods to be called when all mandatory service dependencies are satisfied.
For example, the paint frame component uses this mechanism to make its frame visible:
The @Invalidate annotation is applied to component methods to be called when any of the mandatory service references become unsatisfied and iPOJO is going to release the component instance.
The paint frame component likewise uses this mechanism to close and dispose of its frame:
Damping, anyone? The behavior of iPOJO’s temporal dependencies is similar to the damping concept used by Blueprint.
Technically, if you used temporal dependencies liberally, you’d end up with a similar effect of having all your dependencies damped.
Although this is possible, it isn’t the intended use case for temporal dependencies, and we advise against it.
Generally speaking, most service dependencies are either mandatory or optional.
The use of damped dependencies may result in systems that exhibit odd behavior when faced with service dynamism.
Callback methods such as these are nice if you want your components hooked into their own lifecycle.
But what if you want them to actively participate in it?
Both of these annotations can be associated with a boolean member field in the component.
This tells the iPOJO runtime to monitor this field to control the lifecycle of the component.
If the component sets isValid to false, iPOJO invalidates the component instance and throws it away.
You can use this approach to model exceptional conditions, such as an invalid configuration with no reasonable defaults.
ServiceController is a little more dynamic and allows the component to control when its provided services are published:
In this case, if the component sets isProvided to false, the iPOJO runtime removes the instance’s service from the service registry.
If isProvided is set to true again, iPOJO publishes the service into the service registry again.
You can specify the precise service interface using the specification annotation attribute, if the component provides more than one service.
As with the other component frameworks, you can access the underlying OSGi BundleContext object associated with the bundle containing the components.
So how do you instantiate your components in iPOJO? You’ll find out next.
At this point, you’ve seen how to define an iPOJO component using Java annotations, and we’ve looked into component lifecycle issues; but, surprisingly, nothing you’ve learned so far creates any component instances.
Unlike Declarative Services and Blueprint, where component definitions are typically treated as configured component instances, iPOJO always treats a component definition as a type definition.
The distinction is the same as between a class (type) and an object (instance)
An iPOJO component description defines a template for creating component instances; but creating an instance requires an extra step, much like using new in Java to create an object.
How do you accomplish this in iPOJO? There are four possibilities:
We’ll look into each of these options in this section.
Recall earlier that when we discussed setting up the build process for an iPOJO component, you saw the following Ant task referencing a circle.xml file:
This instructs iPOJO to create an instance of the circle component.
Although the circle.xml file is contained in the same bundle as the circle component, this needn’t be the case.
The beauty of iPOJO’s strict separation between component type and instance is that you can package all your component types into bundles, deploy which.
Apache Felix iPOJO types you need, and then separately deploy a bundle containing an application configuration describing which instances of which components to create and how to configure them.
This component prints a message telling you its name, where its name is injected into the member field name.
Here’s how to create and configure four different instances of the component:
You declare four different component instances and uniquely configure each.
When the bundle containing this component configuration is activated, the iPOJO runtime finds the component type associated with the name hello and instantiates it four times, injecting the appropriate configuration into the corresponding instance.
In addition to simple name-value properties, iPOJO also supports lists, maps, arrays, sets, and dictionaries as configuration properties.
And remember, the XML is only parsed at build time—no XML parsing goes on at execution time.
Regardless, some people wish to avoid XML, which brings us to the next approach.
It provides a way to create a component instance without XML and is largely equivalent to declaring a static singleton in Java code.
The @Instantiate annotation results in iPOJO creating a component instance at execution time when the containing bundle is activated and the component becomes valid.
The main downside of this approach is that it hinders component reusability, because it presupposes that the number and configuration of your component instances are the same for every scenario in which they’re used.
Although the XML and annotation approaches likely satisfy the majority of use cases for most people, they don’t cover all possibilities.
For example, what if you need to dynamically create component instances? iPOJO provides two different ways to accomplish this.
We’ve told you that iPOJO maintains a strict separation between type and instance, but we didn’t tell you how iPOJO does this.
The Factory interface is fairly straightforward and largely defines methods for creating configured component instances.
Internally, iPOJO uses these factory services to create the component instances you declare using XML or @Instantiate.
To differentiate one component factory service from another, iPOJO registers them with unique factory.name service properties, which is the name of the component class by default but can be any name you choose.
How does this allow you to dynamically create component instances? Because these are just OSGi services, you can look them up in the service registry and use them like any normal service.
In this example, you define a component with a dependency B on a component factory service for the previous trivial Hello component implementation.
You specify the desired factory using the filter attribute of @Requires; in this case, you previously named the component type hello.
Like any normal service dependency, the Creator component becomes valid only if a matching factory service is available.
In the create() method, you prepare a new Hello instance configuration by setting the name property to the passed-in value and then use the factory to create the instance C.
In the rename() method D, you use the ComponentInstance object returned from the factory service to configure previously created instances.
When you’re finished with the instance you dispose of it in dispose()
This approach is well-suited to pooling, allowing you to programmatically create and release component instances.
If you swapped your Hello implementation for a database connection pool or a thread pool, for example, instances could be programmatically created as other components in the framework noticed degradation in application performance.
Although this mechanism lets you dynamically create instances at execution time, it ties components to the iPOJO API.
But this effect can be minimized: iPOJO provides another approach to eliminate this coupling.
This approach is fairly similar to the iPOJO factory service, except that it uses the standard OSGi interface rather than an iPOJO-specific one.
To illustrate, the next listing shows the previous Creator component refactored to use the ConfigurationAdmin service instead.
You then set the configuration property with the passed-in name and update the configuration.
To update the component, in rename() you find the Configuration object associated with the passed-in name C.
If it’s found, you update its name property with the specified value.
Finally, in dispose() you again find the Configuration object associated with the passed-in name and delete it D, which disposes of the instance.
Although this approach is somewhat less direct than using iPOJO factory services, the component now only depends on standard OSGi APIs.
We haven’t touched on all of iPOJO’s features (such as composite service description, which goes beyond what we can cover in this section), but we’ve discussed most of what you’ll need to get started.
Type ant to build the example and java -jar launcher.jar bundles/ to run it.
In this and the preceding chapter, we’ve shown you three OSGi-based component frameworks.
You have to pick based on your requirements, but table 12.4 provides a summary of some of the features of each to make this task a little easier.
Before closing out this chapter, we’ll let you in on a little secret about OSGi component frameworks: you don’t have to choose just one.
They can all work together via the OSGi service layer.
To a large degree, you can use any combination of these component frameworks in your application.
To show this in action, let’s convert the paint application to use the following components:
To achieve this goal, you need to make a handful of minor changes to your components so they’ll play well together.
We hear you asking, “Wait! Why do we need to change the components? I thought you said they can work together.” Technically, they can; but there are some issues due to disparate feature sets.
You need to smooth over one or two discontinuities among the various component models; table 12.5 summarizes these issues.
In the SimpleShape interface, add an Icon getIcon() method and remove the ICON_PROPERTY constant that’s no longer used.
Doing so bridges the gap between Declarative Services capabilities and Blueprint capabilities with respect to service attributes.
As a consequence, each SimpleShape implementation now loads its own ImageIcon.
Also, the DefaultShape class delegates the getIcon() call to the SimpleShape implementation where possible and handles the loading of the underconstruction icon when the service is no longer available.
The PaintFrame class uses the getIcon() method on SimpleShape to load the icon versus handling this itself.
In Declarative Service components, you use simple string service properties with class loading to load icons.
All components need to agree on the interfaces they’ll expose.
Type ant to build the example and java -jar launcher.jar bundles to execute it.
All the components from the different frameworks integrate nicely into a single application.
Component frameworks can simplify the task of creating OSGi-based applications and add useful capabilities, including lazy initialization, complex service-dependency management, and configuration externalization.
Often, you’ll end up having to do a lot of this work yourself, so using a component framework can free you from the drudgery.
The following list summarizes the component frameworks we’ve investigated in the past two chapters:
Declarative Services is an OSGi specification and is the simplest framework, offering management of service dependencies and component configuration.
Blueprint is also an OSGi specification and provides features similar to Declarative Services, but with a richer configuration model.
It’s familiar to developers who come from a Spring background.
With any of these component frameworks, you can build rich, dynamic, OSGibased applications, with the added bonus that they can all integrate and collaborate via the OSGi service registry.
Now we’ll switch focus from dealing with the internal structure of your applications to external concerns.
Until now, we’ve assumed that applications are a set of bundles running inside an OSGi framework, but sometimes they’re more complicated.
For example, you may need to be in control of how your application is launched, or you may not be able to package an entire application as bundles.
What do you do then? In the next chapter, we’ll look at how to launch and/or embed an OSGi framework.
We’ve spent a lot of time talking about creating, deploying, and managing bundles and services.
Interestingly, you can’t do anything with these unless you have a running OSGi framework.
For such an important and necessary topic, we’ve spent very little time discussing how precisely to achieve it.
Not only is it necessary, but by learning to launch the framework, you’ll have the ability to create custom launchers tailored to your application’s needs.
It even opens up new use cases, where you can use an instance of an OSGi framework inside an existing application or even embedded inside a bundle.
Embedding the OSGi framework into an existing application Launching and embedding an.
In this chapter, you’ll learn everything you need to know about launching the OSGi framework.
To help you reach this goal, we’ll dissect the generic bundle launcher you’ve been using to run the book’s examples.
You’ll also refactor the paint program to see how to embed a framework instance inside an existing application.
As we mentioned back in chapter 3, you face a dilemma when you want to use a bundle you’ve created.
You need a BundleContext object to install your bundle into the framework, but the framework only gives a BundleContext object to an installed and started bundle.
So you’re in a chicken-and-egg situation where you need an installed and started bundle to install and start your bundle.
This typically involved some combination of auto-deploy configuration properties for each framework implementations’ custom launchers and/or shells with textual or graphical interfaces.
These mechanisms worked reasonably well but weren’t portable across framework implementations.
With the release of the OSGi R4.2 specification, the OSGi Alliance defined a standard framework launching and embedding API.
Although this isn’t a major advance in and of itself, it does help you create applications that are truly portable across framework implementations.
You may wonder if this is really necessary or all that common.
There are two main reasons why you may want to create your own framework instance:
Your application has custom startup requirements that aren’t met by your framework’s default launcher.
For legacy reasons, you can’t convert your entire application into a set of bundles that run inside an OSGi framework.
Previously, if either of these applied to your project, you had to couple your project to a specific framework implementation by using its custom API to launch it.
Now, R4.2compliant frameworks share a common API for creating, configuring, and starting the framework.
As we previously mentioned, at execution time the OSGi framework is internally represented as a special bundle, called the system bundle, with bundle identifier zero.
This means active bundles are able to interact with the framework using the standard Bundle interface, which we reiterate in the following listing.
Although this provides an internal framework API for other bundles, it doesn’t help externally when you want to create and start framework instances.
When the R4.2 specification looked to address this situation, the logical place to start was with the Bundle interface.
This was a good starting point, but it wasn’t completely sufficient.
To address the missing pieces, the R4.2 specification defines a new Bundle subtype, called Framework, which is captured in the following snippet:
Because it extends Bundle, this means framework implementations now look like a bundle externally as well as internally via the system bundle.
Whether this is or isn’t the case depends on the framework implementation.
As you can see, the Framework interface is a simple extension, so you don’t have too much new API to learn.
In the following subsections, we’ll fully explore how to use this API to configure, create, and control framework implementations in a standard way.
It’s great to have a standard interface for framework implementations, but you can’t instantiate an interface; you need a way to get a concrete implementation class.
It isn’t possible for the OSGi specification to define a standard class name, so it adopts the standard Java approach of specifying service-provider implementations in JAR files: META-INF/services.
In this case, META-INF/services refers to a directory entry in a JAR file.
More specifically, it contains metadata about the service providers contained in a JAR file.
Here the term service isn’t referring to an OSGi service, but to well-known interfaces and/or abstract classes in general.
All in all, the concept is similar to the OSGi service concept.
The META-INF/services directory in a JAR file contains service-provider configuration files, which refer to a concrete implementation class for a given service.
Concrete service implementations are connected to their abstract service type via the name of the file entry in the directory, which is named after the fully qualified service it implements.
At execution time, when a service provider is required, the code needing it queries the service-provider configuration file.
When a concrete type is obtained from the content of the file, the code needing the service can load and instantiate the associated class.
The OSGi specification uses this mechanism to provide a standard way to get the concrete framework implementation class.
But rather than directly retrieve a framework implementation class, OSGi defines a framework factory service as follows:
This interface provides a simple way to create new framework instances and pass a configuration map into them.
As a concrete example, the Apache Felix framework implementation has the following entry in its JAR file declaring its service implementation:
The content of this JAR file entry is the name of the concrete class implementing the factory service:
Of course, these details are only for illustrative purposes, because you only need to know how to get a framework factory service instance.
You obtain a ServiceLoader instance for a framework factory like this:
Using the ServiceLoader instance referenced by factoryLoader, you can iterate over all available OSGi framework factory services like this:
In most cases, you only care if there’s a single provider of the factory service; you can invoke it.next() to get the first available factory and use FrameworkFactory.
If it finds one, it reads the content of the file.
Within the loop, it searches for the first line not starting with # (the comment character) and assumes that the line contains the name of the concrete class it should instantiate at C.
The method throws an exception if an error occurs during this process or if a factory provider couldn’t be found.
Next, we’ll look into how you use the factory service to configure a framework instance.
When you have a framework factory service, you can create an instance of Framework.
Typically, you don’t use a default framework instance; instead, you often want to configure it in some way, such as setting the directory where the framework should store cached bundles.
Prior OSGi specifications defined a few standard configuration properties; but until the framework factory API, there was no standard way to set them.
As part of the R4.2 specification process, several new standard configuration properties were also introduced.
No configuration required You don’t have to pass in configuration properties when creating a framework; null is an acceptable configuration.
The OSGi specification says framework implementations must use reasonable defaults, but it doesn’t explicitly define all of them.
Be aware that you won’t necessarily get the same behavior unless you explicitly configure it.
If this property isn’t set, a reasonable default is used.
If no value is specified, the framework storage area isn’t cleaned.
Currently, the only possible value is onFirstInit, which causes the framework instance to clean the storage area the first time it’s used.
If not set, the framework must provide a reasonable default for the current VM.
Possible values are boot for the boot class loader, app for the application class loader, ext for the extension class loader, and framework for the framework’s class loader.
We won’t go into the precise details of all the standard configuration properties, so consult the R4.2 specification if you want details not covered here.
With this knowledge, you know how to configure and instantiate a framework instance; let’s look at how to start it.
When you have a Framework instance from FrameworkFactory, starting it is easy: invoke the start() method inherited from the Bundle interface.
The start() method implicitly initializes the framework by invoking the Framework.init() method, unless you explicitly initialize it beforehand.
If the init() method wasn’t invoked prior to calling start(), then it’s invoked by start()
You can relate these methods to the framework lifecycle transitions, similar to the normal bundle lifecycle:
The init() method gets the framework ready but doesn’t start executing any bundle code yet.
All cached bundles are reloaded, and their state is set to Bundle.INSTALLED.
The start() method starts the framework instance and performs the following additional steps:
If the framework isn’t in the Bundle.STARTING state, the init() method is invoked.
The framework sets its beginning start level to the configured value, which causes all reloaded bundles to be started in accordance with their activation policy and start level.
You may wonder why the init() method is necessary and why all the steps aren’t performed in the start() method.
In some cases, you may want to interact with the framework instance before restarting cached bundles, but some interactions can only happen via the framework’s BundleContext object.
But if you want to perform some actions before all the cached bundles restart, call init() first to do what you need to do followed by a call to start()
When the framework is active, subsequent calls to init() and start() have no effect.
Next, we’ll look at how you shut down a running framework.
As you may guess, stopping an active framework involves invoking the stop() method inherited from the Bundle interface.
This method asynchronously stops the framework on another thread, so the method returns immediately to the caller.
If you want to know when the framework has finished shutting down, call Framework.
The following steps are performed when you stop a framework:
If you want to stop the framework, you must call stop() on it first.
The waitForStop() method takes a timeout value in milliseconds and returns a FrameworkEvent object whose type indicates why the framework stopped:
When the framework has successfully stopped, it can be safely discarded or reused.
The normal startup process will commence, except the bundle cache won’t be deleted again if the storagecleaning policy is onFirstInit, because that applies only the first time the framework is initialized.
Otherwise, you can stop and restart the framework as much as you like.
That’s all there is to creating and launching frameworks with the standard framework launching and embedding API from the R4.2 specification.
Let’s explore your newfound knowledge by examining the generic bundle launcher.
What is the difference between the two? The conceptual difference is that launching refers to creating and starting a framework instance in isolation, whereas embedding refers to creating and starting a framework instance within (embedded in) another application.
Technically, there’s very little difference between the two, because creating, configuring, and starting a framework instance with the API is the same in either case.
When you launch a framework, all functionality is typically provided by installed bundles, and there’s no concern about the outside world.
But when you embed a framework, you often have functionality on the outside that you want to expose somehow on the inside or vice versa.
Embedding a framework instance has some additional constraints and complications that we’ll discuss later in this chapter.
These are the same basic steps the generic bundle launcher uses, as we’ll introduce in the following subsections by breaking the example into short code snippets.
The complete source code for the generic launcher is in the launcher/ directory of the book’s companion code.
As you’ve seen throughout the book, the generic bundle launcher installs and starts all bundles contained in a directory specified as a command line argument.
The launcher is composed of a single class, called Main, which is declared in the following code snippet.
The static member variable holds the framework instance you’re going to create.
You verify that a directory was specified as a command line argument.
If a directory was specified, you get the files contained in it and save all files ending with .jar into a list to be processed later.
You can’t always guarantee that the launcher process will exit normally, so it’s a good idea to try to ensure your framework instance cleanly shuts down.
Depending on the framework implementation, you can end up with a corrupted bundle cache if you don’t shut down cleanly.
The following listing adds a shutdown hook to the JVM process to cleanly shut down your framework instance.
Listing 13.4 Using a shutdown hook to cleanly stop the framework.
The JVM shutdown hook mechanism requires a Thread object to perform necessary actions during process exit; you supply a thread to cleanly stop the framework.
When the shutdown thread executes, you verify that a framework instance was created B and, if so, you stop it.
Because shutting down the framework happens asynchronously, the call to fwk.stop() returns immediately.
It’s necessary to have your thread wait; otherwise, there’s a race condition between the JVM process exiting and your framework stopping.
The process is in an awkward state during shutdown, and not all JVM services are guaranteed to be available.
There’s also the potential for deadlock and hanging the process.
In short, it’s a good idea to try to cleanly shut down the framework, but be aware of the potential pitfalls and do as little work as possible in the shutdown hook.
In section 13.2.1, you determined which bundles you want to install; all you need now is a framework instance.
You begin by creating a variable to hold a reference to your main bundle, which is a bundle with a Main-Class entry in its manifest file; we’ll come back to this concept in a couple of sections.
After that, you create a list to hold the bundles you successfully install.
In the setup for the framework instance, you create a configuration map for it.
For the generic launcher, you copy the system properties in the configuration map as a convenience and only set one configuration, which cleans the bundle cache on first initialization.
In most cases, you likely won’t want to do this; but for the purposes of.
Next, you get the framework factory service and use it to create a framework instance using the configuration map.
Because you configured the framework to clean its bundle cache on first initialization, you know your framework has no bundles installed in it.
The following snippet shows how to install the bundles contained in the directory specified on the command line:
You first get the BundleContext object associated with the system bundle; this is possible because the Framework object extends Bundle and represents the system bundle.
You loop through the JAR files discovered in the specified directory and install them using the system bundle context; any exceptions cause the launcher to fail.
After you install a bundle, you add it to the list of installed bundles and probe to see if its manifest contains a Main-Class header, which you’ll use later.
If there’s more than one bundle with a Main-Class header, you use the last one you discover.
You’ve installed all of the bundles, but they aren’t doing anything yet.
You can accomplish this in a simple loop over all installed bundles, invoking start() on each one:
You may wonder why you don’t start each installed bundle right after installing it.
It’s better to install and start bundles in two passes: one pass for installing and one pass for starting.
This approach helps alleviate ordering issues when it comes to dependency resolution.
If you install a bundle and start it immediately, it may fail to resolve.
Launching the framework because it may depend on some bundle that’s not yet installed.
By installing all the bundles first, you stand a better chance of successfully resolving the bundles when you activate them.
Notice also that you don’t call start() on all bundles B; instead, you only call start() on bundles that aren’t fragment bundles.
Fragments can’t be started and will throw an exception if you try to start them, which is why you avoid doing so.
How do you know a bundle is a fragment? This simple approach works:
You check to see if the bundle’s manifest headers contain the Fragment-Host header.
If so, it must be a fragment, and you don’t want to start it.
You’ve installed and started all the bundles contained in the specified directory.
But for the examples in this book, you need one more step.
In chapter 2, we showed how you can use the module layer all by itself to modularize the paint program.
In that example, none of the bundles contained a BundleActivator, because activators are part of the lifecycle layer.
In such an scenario, you need a way to start your application: you can use the standard Java MainClass JAR file manifest header as a way to define a main bundle from which you can load the main class and execute its static void main() method.
We defined this approach for this book to show that it’s possible to use the OSGi modularity layer to modularize OSGi-unaware applications.
You could also consider introducing a custom manifest header for this purpose to avoid confusion with the standard Main-Class header.
The next listing shows how to load the main class and invoke its main() method.
Listing 13.5 Invoking the main class from the main bundle.
If you have a main bundle, you need to invoke its main class’s main() method; you won’t necessarily have a main bundle if the bundles have activators.
First, you get the name of the class from the Main-Class manifest header.
Using this name, you load the class from the main bundle.
Then, you use reflection to get the Method object associated with the main class’s main() method.
You make an array to contain any additional command line arguments passed into the launcher after the specified directory.
Finally, you use reflection to invoke the main() method B, passing in any command line arguments.
At this point, your launcher should have your bundled application up and running.
What’s left to do? Not much; just sit around and wait for it to finish, like this:
Why do you do this? Why not let the calling thread run off the end of your main method, similar to what you do with Swing applications? Unlike Swing applications, which result in a non-daemon thread starting for Swing event delivery, you don’t have any guarantee that the OSGi framework will create any non-daemon threads.
If you aren’t familiar with the concept of daemon threads, it’s a fancy way of saying background threads.
For the Java VM, if only daemon threads are present, the VM process terminates.
You need to explicitly wait for the framework to stop, because you know the main thread is non-daemon and will keep the VM process alive.
For similar issues, you call System.exit() to end the VM process.
If you didn’t call exit() here, and a bundle started a non-daemon thread that wasn’t properly stopped, then the VM process wouldn’t exit after stopping the framework.
This is similar to Swing applications, which require an explicit call to exit() because the Swing event thread is non-daemon.
You’ve successfully created a completely generic launcher that will work with any OSGi R4.2 framework implementation.
To use this launcher with an arbitrary framework implementation, put it on the class path with the launcher, and you’re good to go.
But what about situations where you can’t convert your entire application into bundles? In that case, you may want to embed a framework instance inside your application.
In some situations, it isn’t possible to convert your entire application into bundles, where everything runs inside the OSGi framework.
This can happen in legacy situations where conversion into bundles is prohibitively expensive, or in situations where there’s resistance or uncertainty about converting the entire application.
Even in these sorts of situations, you can use OSGi technology for specific needs.
For example, it’s not uncommon for Java-based applications to provide a plugin mechanism for extensibility purposes.
If your application has a plugin mechanism or you’re thinking about adding one, an embedded OSGi framework can do the trick (in chapter 6, you saw how to convert jEdit’s plugin mechanism to use OSGi)
You may be thinking, “Wouldn’t I be better off creating my own simple plugin mechanism in this case?” Typically, the answer is, no.
The dynamic class-loading aspects of plugin mechanisms are difficult to get right.
Over time, you’ll likely need to add more advanced features, such as support for library sharing, side-by-side versions, or native libraries, at which point you’ll start to enter complicated territory and have to reinvent the wheel.
By using OSGi, all this is taken care of for you, so you can concentrate on implementing your application’s core functionality.
If you’re concerned about the size of OSGi frameworks, remember that they’re intended to run on embedded devices, and most implementations aren’t too hefty.
In addition, you get the benefit of having a known standard, which makes it easier for your plugin developers and provides the opportunity to reuse existing bundles.
Embedding an OSGi framework instance into an application may sound pretty exotic; but thanks to the standard framework launching and embedding API, it’s largely the same as launching the framework.
You do need to understand some differences and a few issues; in the remainder of this section, we’ll discuss these issues as well as present an example of embedding a framework instance into an application.
The main issue around embedding a framework instance into an application is the distinction between being on the inside of the framework versus being on the outside of the framework.
The bundles deployed into the embedded framework live in a nice insulated world and know nothing about the outside.
It’s possible to traverse the isolation boundary provided by the framework, but the inside/outside distinction places some constraints on how the application can interact with installed bundles and vice versa.
If you decide to embed a framework instance, what are some of the things you’ll likely want to do with it? You’ll probably want to.
Let’s look at what you need to do in each of these cases.
You already know how to interact with an embedded framework instance: through the standard launching and embedding API.
When you create an instance of an R4.2compatible framework implementation, you get an object that implements the Framework interface.
As you saw previously, this interface gives you access to all the API necessary to control and inspect the framework instance.
The framework instance represents the system bundle and provides you a passage from the outside to the inside of the framework, as depicted in figure 13.3
From the system bundle, you can start and stop the framework as well as deploy, manage, and interact with bundles.
If you’re using an embedded framework instance as a plugin mechanism in your application, you use this API to deploy plugin bundles by loading them from a directory or providing a GUI for user access, for example.
It’s also through this API that you can provide services to bundles and use services from bundles.
Figure 13.2 The embedded framework instance forms an isolation boundary between the bundles on the inside and the application objects on the outside.
Avoid being on the outside The best approach for dealing with the inside/outside divide is to eliminate it by converting your entire application to bundles.
If you’re on the fence about this issue, you can start with an embedding approach and later convert the rest of your application to bundles.
But if you have a choice up front, start with all bundles.
Luckily, there’s no new API to learn when it comes to providing application services to embedded bundles or using services from them.
You learned about providing and using services in chapter 4, and that knowledge applies here.
The only real difference is that you use the system bundle to do everything, because the application has no bundle associated with it.
Because you need a BundleContext to register or find services, you use the BundleContext associated with the system bundle.
You can get access to it by calling getBundleContext() on the framework instance.
From there, registering and using services is pretty much the same as if the application were a bundle.
Simple, right? As you may expect, there is one main constraint.
By default, the application on the outside and the bundles on the inside only share core JVM packages, so it would be possible for the application and bundles to interact using objects from classes defined in core JVM packages.
This works out fairly well if everything you need is in a core JVM package, but this isn’t typically the case.
Luckily, there’s a rudimentary way to share packages from the application to the contained bundles via framework configuration.
The launching and embedding API defines two previously mentioned configuration properties for this purpose:
Figure 13.3 A framework instance represents the system bundle and provides the means to manage the framework instance as well as interact with deployed bundles.
Typically, you’ll only use the latter property, because the specification requires the framework to set a reasonable default for the former.
For an example, suppose you’re going to create a version of the paint program that used an embedded framework instance.
In that case, you likely want to put the SimpleShape interface on the class path so you can share a common definition between the application and the bundles.
The syntax to use when specifying the property is exactly the same as for the ExportPackage manifest header, which means you can specify additional packages by separating them with commas; you can also include version information and attributes.
The need to perform this configuration is an extra step for the application, but from the bundle’s perspective it’s business as usual.
Bundles need to specify the package on their Import-Package manifest header, as normal, and the framework gives them access to the package following normal OSGi rules.
What about the situation where you don’t have a common class available from the class path? Because the application can’t import packages from bundles, there isn’t much you can do here.
The main option is to resort to reflection, which is possible because OSGi service lookup can be performed by the class name.
This gives you access to the ServiceReference that you can use to get access to the service object so you can invoke methods on it using reflection.
If you have different definitions of the service class on the outside and inside, you can try to get fancy and use dynamic proxies to bridge the two types in a generic way.
But this is beyond the scope of this chapter and can easily be avoided by converting your entire application to bundles.
Necessary, but not sufficient It’s necessary to specify this configuration property to share class path packages with bundles, but it isn’t sufficient to only do this.
You must also ensure that the specified packages are available on the class path when you start your application.
You do so the standard way (by specifying them on the JVM class path)
If you’re going to pursue the embedded framework route, you may run into a few other issues related to who’s expecting to be in control.
Generally speaking, the OSGi framework assumes it’s in control of the JVM on which it’s running.
If you’re embedding a framework, you probably don’t want it to be in control or at least want it to share control with the application in which you’re embedding it.
It’s not uncommon to run into issues related to JVM singleton mechanisms, such as URL and content handler factories or security.
Singleton mechanisms like these are only intended to be set once at execution time.
OSGi framework implementations need to be responsible for initializing these mechanisms to properly implement specification functionality.
When a framework is embedded in another application, often the application assumes it’s in control of these singletons.
The OSGi specification doesn’t specifically address these aspects of framework embedding, so how implementations deal with it is undefined.
Some frameworks, like Apache Felix, go to lengths to try to do the right thing, but the right thing often depends on the specific use case.
If you run into issues in these areas, you’ll have to consult the documentation or support forums for your specific framework implementation.
Another area where issues arise is in the use of the Thread Context Class Loader (TCCL)
If you’re not familiar with this concept, each thread in Java has a class loader associated with it, which is its context class loader.
The TCCL provides a backdoor mechanism to subvert Java’s normal, strict hierarchical class loading.
Application servers and various frameworks use this mechanism to deal with class-loading dependencies that can’t be shoehorned into hierarchical class loading.
Unfortunately, this crude attempt at dealing with class-loading dependencies doesn’t mesh well with OSGi modularity.
Thread Context Class Loader travails The TCCL can be both a blessing and a curse.
Used correctly, it can enable access to classes in places where it wouldn’t otherwise be possible; but it can have unexpected side effects in cases where modularity is enforced.
Embedding an OSGi framework is a typical example of where things may go wrong.
This can happen if the outside application or container sets the context class loader.
In this case, it’s leaking classes into the class space of bundles being accessed from the outside.
Typical examples of situations in which problems can occur include the following:
Consider a situation where you’re embedding an OSGi framework inside a container using log4j for logging.
The container will obviously have log4j on its class path.
Now, if the container happens to set the TCCL to its own class loader and then calls into the framework, a bundle using log4j may end up with unexpected problems because classes from the container can be found that shouldn’t, or vice versa.
For a simple illustration of framework embedding, you’ll convert the service-based paint program from chapter 4 into a standalone application with an embedded framework instance.
Because the service-based paint program is completely composed of bundles, you need to transform it into a Java application.
The new standalone paint program uses an embedded framework instance as a plugin mechanism by which it can deploy custom shape implementations.
Because the OSGi specifications don’t address this issue, you can’t be sure it’s handled the same way by different frameworks.
For example, Apache Felix doesn’t do anything in regard to the TCCL, whereas other frameworks try to automagically set it to the “correct” bundle class loader.
One piece of useful advice to keep in mind is that the TCCL is inherited by threads.
So if you set the TCCL of a given thread, and it in turn creates a new thread, it’ll inherit the same TCCL.
Of course, this can be a good or a bad thing, depending on your situation.
The important part is to think about what the TCCL will be for any threads created by the framework and/or bundles; it will be implicitly inherited if you don’t explicitly set it.
For the standalone paint program, you don’t need to change the shape bundles.
What does need to be changed? The original service-based paint program didn’t need a launcher, because the bundle activator in the paint bundle served this purpose.
For the standalone paint program, you need a launcher that creates the paint frame and the framework instances and wires everything together.
Additionally, because the paint program needs a common class definition to interact with bundles implementing shapes, you must move the shape API into the standalone application so the application and bundles can use the same SimpleShape service-interface definition.
Note that figure 13.4 depicts the application as a quasi bundle with an exported package and service dependencies.
This is just for illustrative purposes: the application is a normal JAR file.
The structure of the modified paint program source code is as follows:
What’s the design of the standalone paint program? Recall the original design of the paint program: the main paint frame was designed in such a way as to be injected with shape implementations.
This approach had the benefit of allowing you to limit dependencies on OSGi API and to concentrate your OSGi-aware code in the shape tracker.
In keeping with these design principles, you’ll do most of the work in the launcher Main class, which creates the embedded framework instance, deploys the shape bundles, creates the paint frame, and binds the paint frame to the embedded shape services.
Perhaps at this point you’re thinking that this sounds similar to the generic framework launcher you created in the previous section.
Using the framework in an embedded way isn’t all that different, other than the issues we outlined previously.
As a result, the launcher code for the standalone paint program will bear a striking resemblance to the generic launcher.
This last aspect doesn’t have an analogue in the original service-based paint program, but we include it to demonstrate that it’s possible to provide services from the outside.
As before, we’ll break the launcher into small snippets and describe each one.
Because the paint program is no longer a bundle, you replace its bundle activator with a Main class.
The primary tasks this class performs are easy to discern from the main() method.
The performed functionality is a combination of the generic launcher and the old bundle activator: adding a shutdown hook, creating a framework instance, and creating a paint frame.
The only new task is publishing an external trapezoid shape service, which you’ll see is pretty much the same as publishing a normal service.
Because adding a shutdown hook is basically identical to what you did for the generic launcher, we’ll skip that step and go directly to creating the framework instance.
The createFramework() method follows fairly closely to the launcher, so we’ll go over the details quickly.
The method starts, like the launcher, with discovering which bundles it should install into the framework instance:
Here you get the contents of the bundles directory in the current directory and add all contained JAR files to a list.
This is rather simplistic, but it’s sufficient for this example.
Now you can create the framework instance and deploy the discovered bundles.
Listing 13.7 Creating the framework instance and deploying discovered bundles.
As with the generic launcher, you configure the framework to clean its bundle cache on first initialization.
For performance reasons, you probably wouldn’t want to do this if you were using the framework as a plugin mechanism, because it’s slower to repopulate the cache every time.
You do it in this case to make sure you’re starting from a clean slate.
An important difference from the launcher, which we alluded to previously, is at B.
Here you configure the framework to export the org.foo.shape package from the class path via the system bundle.
This allows bundles to import the package from the application, thus ensuring that they’re both using the same interface definition for shape implementations.
You also need to ensure that this package is on the class path; but because you’re going to package it in the application JAR file, it should definitely be available.
Next, you create the framework with the defined configuration and start it.
You get the system bundle’s bundle context, which you use to install the discovered bundles.
Now let’s look at how you publish an external service into the framework instance.
This code is basically the same as what you saw back in chapter 4 for publishing services.
The only difference is that you use the system bundle’s bundle context to register the service, because the application doesn’t have its own bundle context.
Of course, what makes this possible is the fact that you’re using the same org.foo.shape package on the inside and the outside, which means your trapezoid shape works just like the shapes provided by any of the shape bundles.
Now you’re ready to bind everything together to complete the functioning paint program.
You create the paint frame itself and then add a window listener to cleanly stop the embedded framework instance and exit the JVM process when the frame is closed.
Then you display the paint frame; but at this point it isn’t hooked into the embedded framework instance.
You get the system bundle’s bundle context and use it to create a shape tracker for the paint frame B; this is what binds everything together.
Due to the original design, you don’t need to spread OSGi API usage throughout the application.
Type ant to build the program and java -jar paint.jar to run it.
Listing 13.8 Creating the paint frame and binding it to the framework instance.
You didn’t need to do anything to make this happen, other than ensure that the application and the bundles used the same service interface.
That’s all there is to the OSGi R4.2 standard launching and embedding API.
The OSGi specification doesn’t define a standard launcher with a standard way to configure and launch a framework.
The standard launching and embedding API introduced in OSGi R4.2 is the next best thing to a standard launcher, because it allows you to create a single launcher that works across framework implementations.
The OSGi R4.2 specification introduced the Framework interface to represent a framework instance.
The Framework interface extends the existing Bundle interface, which extends your existing knowledge of managing bundles to framework instances.
The Framework instance represents the system bundle, which provides access to the system bundle’s bundle context for performing normal bundle tasks (such as installing bundles and registering services)
The META-INF/services approach finds a FrameworkFactory provider, which enables framework creation without knowing a concrete framework implementation class name.
Although using completely bundled applications is the preferred approach, the launching and embedding API also simplifies embedding framework instances into existing applications.
When you embed a framework instance into an application, the main constraint involves dealing with the difference between being on the outside versus the inside.
If direct interaction with bundles is required, you often need to share common class definitions from the class path.
Other than some additional constraints, embedding a framework instance is nearly identical to launching a framework instance.
In the next chapter, we’ll look into configuring framework instances to deal with security.
In such applications, bundles can come and go at any time, and it’s easy to allow third parties to extend your application in a well-defined way.
But as with most things in life, there’s a downside to this flexibility: you open yourself (or your users) to security vulnerabilities because third-party bundles can’t be completely trusted.
Luckily, the Java platform has built-in support for secure sandboxes, and the OSGi framework is designed to take advantage of them.
Creating custom permission conditions for advanced use cases Securing your applications.
To secure or not to secure sandboxes and their restrictions are difficult to get right and often hard to manage.
This is especially true in an environment as dynamic as the OSGi framework.
To help with this situation, OSGi defines an extensive and powerful security model to ease security management.
In this chapter, we’ll familiarize you with the Java security model and how OSGi uses it to securely deploy and manage applications.
You’ll learn how to create secure applications and well-behaved bundles for security-enabled OSGi frameworks.
Before we start with that, let’s look at some general issues you’ll need to consider when trying to secure your applications.
Modern applications and software solutions increasingly center around loosely coupled, extensible architectures.
Component and service orientation are applied to almost all areas of application development including distributed systems, ubiquitous computing, embedded systems, and client-side applications.
One of the main drawbacks of loosely coupled, extensible applications is the potential security issues around executing untrusted code.
Often, users are left to make security policy decisions, and they’re typically unable to assess the impact of granting a given permission.
Further, because the user is typically using an application to perform some task, security is largely viewed as an obstacle because it doesn’t contribute to the task at hand.
It’s inherently tricky to establish meaningful identity of third-party software providers.
It’s often necessary to differentiate between providers or types of providers to properly grant or deny permissions.
Often the origin of the software artifact (where it came from) is used for this purpose, but techniques like digital signatures are also needed to ensure that the software hasn’t been tampered with.
Digital signatures introduce the complicated process of creating and maintaining certificates and trust between certificates, which can be onerous for both users and developers.
This raises perhaps the biggest issue with securing code: it adds another burden to development.
Even if you don’t plan to run with security enabled, your code has to be aware of security if you want it to be possible for other people to use the code when security is enabled.
Then, to make matters worse, if you decide to enable security, the fine-grained security checks impose an execution-time performance penalty.
Despite these issues, security isn’t something that can or should be ignored, because plenty of people are willing to take advantage of software vulnerabilities.
What do you need to do? Providing meaningful security management involves three key aspects:
As we mentioned previously, identity can be established by the location or origin of the software artifact or by cryptographic measures using digital certificates.
Especially for the latter approach, the software provider generally needs to make the code available in such a way that you can establish the needed credentials.
When you have identity established, you need to define the permissions that code should have.
For OSGi, this is the responsibility of whoever is managing the framework, which can be a gateway operator, a server administrator, or even an end user.
As a consequence, permission management should be kept as simple as possible.
Last but not least, security must be built into the code itself.
You have to think about internal security checks to prevent external code from performing undesired operations and also how to limit privileges so external code can perform potentially sensitive operations in a safe way.
Assuming you’re able to develop your code with all the security checks in the right place, define a reasonably policy to manage permissions, and sign it using a trusted certificate for establishing identity, is all the work worth it?
In some cases, it’s not within the scope of your application.
Either the performance impact is too great or the development costs are too high.
Often, these issues serve as the determining factor for creating security-enabled applications.
This is compounded by the fact that if code isn’t designed to be usable in security-enabled environments, it’s unlikely to happen by accident.
This results in a catch-22 type of situation, where the difficulty associated with creating secure code results in security being ignored, which makes it next to impossible to use such code with security enabled, thus further raising the barriers for deciding to develop with security in mind in the first place.
In the remainder of this chapter, we’ll show you that taking advantage of the security capabilities of the OSGi framework needn’t be too difficult.
In the next section, we’ll start by taking a high-level view of Java and OSGi security.
Great—but where do you start? Let’s begin at the beginning and look at the Java security architecture and its permission model, on which the OSGi security model is based.
It’s important to understand the Java security architecture; but to keep this chapter tightly scoped, we’ll introduce only the parts needed to understand the remainder of the chapter.
Specific permissions allow access to specific, sensitive operations, such as file system or network socket access.
How do you grant Permission objects to code? The Java security architecture is based on the two fundamental concepts of domain- and role-based security:
Role-based security revolves around authenticating users or processes and granting them permissions based on who they are.
The OSGi framework security model relies on Java’s domain-based approach; the rolebased approach is possible, but only as a layer on top.
In standard Java, role-based security is provided by the Java Authentication and Authorization Service (JAAS) framework, but OSGi also provides its own API in the form of the User Admin Service.
We won’t deal with role-based security in this chapter; for more information on the User Admin Service, refer to the OSGi compendium specification.
The Permission class is a base class from which more specific permissions can be derived via subclassing.
You grant Permission objects to code to give it the ability to perform sensitive operations.
Additionally, the Permission class has a method called implies() that accepts another Permission.
This method checks to see if the supplied permission is implied by the target permission (similar to being a subset)
Thus, Permission objects are used to both grant and check permissions.
Permissions are granted to protection domains, and all classes belong to a single protection domain.
Sound complicated? Actually, in OSGi it’s pretty simple, because a domain is mapped one-to-one with a bundle; you can think of it as a bundle protection domain.
All classes originating from a given bundle are members of the same bundle protection domain.
All classes loaded from a bundle are associated with the bundle’s protection domain, thus granting them the permissions granted to the bundle.
To understand how protection domains enable permission checking, consider code that performs a sensitive operation, such as creating a file.
The code in the JRE for file access performs security checks internally to make sure the invoking code has permission to perform the operation.
Internally, the code associated with performing file system operations triggers a specific permission check by using the security-checking methods of SecurityManager or AccessController.
When triggered, the JVM collects the ProtectionDomains of all classes on the call stack leading to the invocation of the sensitive operation.
It checks that each protection domain on the call stack has at least one permission implying (granting) the specific permission being checked by the method.
This checks whether at the point of the permission check, all protection domains on the call stack have permission p.
Looking at figure 14.1, the JVM performs a stack walk from the class performing the security check and determines that classes A, B, C, and D are involved.
Subsequently, it determines that classes A and C originate from the protection domain of Bundle B, and classes B and D originate from the protection domain of Bundle A.
With this information, the JVM checks whether all protection domains have some permission implying the checked permission.
This provides a good foundation for understanding the Java security architecture, but there’s one final piece to the puzzle: privileged calls.
You now know that checking a specific permission triggers a complete stack walk to collect all involved protection domains and verify that they all imply that permission.
For example, assume you have a service with a method for appending a message to a log file.
Because disk operations trigger file system–related permission checks, all code on the call stack must have permission to write to the file.
This may be fine if only trusted code is involved; but in an extensible and collaborative environment like OSGi, you generally want to allow arbitrary bundles to share code and services, so it’s likely that some code on the call stack won’t be trusted.
In such cases, if a permission check always walks up the entire call stack, you either have to disallow all nontrusted code or grant code-sensitive permissions to untrusted code.
Neither choice is palatable, which is why Java supports privileged calls.
A privileged call is a mechanism to short-circuit the stack walk when performing a permission check.
In practice, this allows trusted code to perform sensitive operations on behalf of code with insufficient permissions.
When the doPrivileged() method is invoked, it invokes the run() method of the passed-in PrivilegedAction.
Any subsequent permission checks triggered by the PrivilegedAction stop walking the call stack at the last privileged call.
Thus, only the protection domains from the privileged action onward are considered by subsequent permission checks.
Call stack Figure 14.1 The JVM checks permissions by collecting all protection domains associated with classes on the call stack and seeing if each involved protection domain has the specified permission granted to it.
Returning to the example of a service for appending a message to a log file, you trust the bundle containing the service implementation, but you don’t want to give direct file system access to anyone else.
To do this, your service must encapsulate its file system operations inside a PrivilegedAction and use doPrivileged() like this:
Any triggered permission checks stop walking the call stack at the run() method, which means nontrusted code further up the stack won’t have its protection domain checked for the triggered permissions.
Pushing this example further, you may decide to limit which code can call the append() method.
To so this, you can create your own Permission subclass, which you can grant to code.
For the append method, if you create an AppendPermission, it can check the permission before performing the privileged call:
Here your service asks the SecurityManager to check whether the code on the call stack has been granted the custom AppendPermission.
If so, it can continue to perform the file-append operation; otherwise, a security exception is thrown.
You do it this way because you want to perform security checks only if security is enabled, to avoid performance penalties when it’s not enabled.
That pretty much sums up the important pieces of the Java security architecture.
A potential downside is that managing all these permissions can be complex.
Luckily, the OSGi specification lessens some of this complexity by defining services to help you perform.
We’ll look at these services shortly; first, let’s examine OSGispecific permissions defined by the OSGi specification.
Certain methods in the OSGi framework API perform sensitive operations or provide access to sensitive information.
To control which code can access these sensitive methods, the OSGi specification defines a few custom permissions, as you learned about in the last section.
You can group these permissions by the layers of the OSGi framework, as shown in table 14.1
We’ll introduce these OSGi permissions in the following subsections, and you’ll subsequently use them when we discuss permission management.
PackagePermission is a module-layer permission giving you the ability to limit which packages a bundle can import or export.
You may not want any arbitrary code using the packages containing these privileged operations.
In that case, you can use PackagePermission to limit which bundles can import the packages containing the associated classes.
Likewise, you can use PackagePermission to control which bundles can export a given package, because you may only want trusted bundles providing some packages.
Names and actions Standard Java permissions typically have constructors that accept two parameters: a name string and an actions string.
The meaning of these parameters is determined by the specific permission.
The combination of name and actions allows you to express everything the permission allows you to control.
All of the OSGi-specific permissions follow this pattern, as you’ll see.
To grant a specific PackagePermission, you need to supply the name and actions parameters for its constructor; these parameters are described in table 14.2
For the actions, import gives a bundle permission to import the named packages, export gives a bundle permission to export and import the package, and exportonly does as its name implies.
You may wonder why export also gives permission to import the named packages.
It’s to support bundles’ ability to import packages they export (that is, substitutable exports), as described in section 5.1.1
To get an idea of how PackagePermission works, let’s take a conceptual look at how the framework uses it.
Assume you have a bundle with the following imports and exports:
When the framework resolves this bundle, it checks to see whether the bundle has the following permissions granted to it:
For these checks to succeed, you’d have to grant the necessary permissions, such as.
Notice that you don’t need to grant the bundle permission to import org.bar, because it’s implied by the export action.
That’s the basics for PackagePermission; let’s move on to the next OSGi permission.
Similar to PackagePermission, BundlePermission is a module-layer permission for controlling bundle and fragment dependencies.
To grant a BundlePermission, you need to construct it with the parameters shown in table 14.3
To control bundle dependencies, the provide action gives a bundle permission to be required by bundles matching the supplied symbolic name, whereas require gives it permission to require matching bundles.
String name Name of the package or packages to which this permission applies.
Comma-delimited list of the actions granted by the permission (export, import, or exportonly)
Fragment dependencies are controlled by the host and fragment actions, which give a bundle the ability to be a host bundle for matching fragments or be a fragment for matching hosts, respectively.
Another similarity to PackagePermission is that the provide action implies the require action.
Using BundlePermission isn’t sufficiently different from using PackagePermission, so we won’t look into it any further.
AdminPermission is a lifecycle-layer permission to control access to sensitive framework operations and information.
The operations and information protected by AdminPermission are diverse, which makes it somewhat complex but fairly powerful.
Table 14.4 shows the parameters needed to create such a permission.
When you grant AdminPermission to a bundle, that bundle is allowed to perform the specified actions on the bundles matching the filter.
The filter uses the same LDAP filter syntax used by the OSGi service registry, but only the following attributes are defined for matching:
First we’ll briefly describe what the granted actions allow on the matching bundles:
String symbolicName Symbolic name of the bundle to which this permission applies.
String actions Comma-delimited list of the actions granted by the permission (provide, require, host, or fragment)
As you can see, AdminPermission gives you fine-grained control over which bundles can do what on which bundles.
For example, assume a bundle wants to install another bundle using code like this:
This triggers the framework to check whether all code on the call stack has the following:
This is relatively straightforward, although granting the permission can be a little confusing.
The thing to remember about AdminPermission is that you use it to grant a bundle the right to perform specific operations on other bundles.
The filter constructor parameter is how you specify the bundles that can be controlled.
For a more complicated example, you can grant an AdminPermission like this:
Granted, this is completely contrived, but it illustrates the possibilities.
You’ve now seen the module- and lifecycle-layer permissions, which means you have one framework layer to go—services.
ServicePermission is a service-layer permission for controlling which services a bundle can provide or use.
As with the other permissions, the actual permission granted is controlled by its constructor parameters, as shown in table 14.5
The get action grants the ability to use the specified services, whereas the register action grants the ability to provide the specified services.
For the get action, you can also use an LDAP filter for name, which matches.
To get a better understanding of how this permission is used, consider the following snippet of code a bundle can use to find a service and to register a service:
This triggers the framework to check whether all code on the call stack has the following:
For the associated bundle to perform these tasks, you can grant it these permissions:
In the first permission, you use a wildcard to allow it to access all services in the org.foo package.
That completes the OSGi-specific permissions you can grant to bundles.
Before we move on to discussing permission management, let’s briefly discuss file permissions, because they behave slightly differently in an OSGi environment.
In a standard Java environment, a file permission created with a relative path is interpreted as being relative to the directory from which the Java process was started (the current working directory)
Instead, it’s treated as relative to the root of the private data area of the associated bundle.
Typically, this doesn’t have much of an effect, especially because the framework automatically grants bundles permission to read, write, and delete files in their own private area.
The main thing this enables is the ability to grant a bundle additional permissions for files in its private data area, such as the execute permission.
Next, we’ll discuss how you manage them with the Conditional Permission Admin Service.
String actions Comma-delimited list of the actions granted by the permission (get or register)
Until now, we’ve talked mostly about the details of permissions (what they look like and what they mean) and otherwise glossed over how you grant permissions to bundles.
The useful part is being able to grant and manage permissions for groups of bundles in accordance with your desired security policies.
To help you achieve this goal, the OSGi specification defines the Conditional Permission Admin Service.
Whereas standard Java offers a file-based policy approach for permission management, OSGi only defines an API-based approach, because it fits better with the inherently dynamic nature of the OSGi environment.
The Conditional Permission Admin Service is the one place to go to define and maintain your security policy.
Further, it introduces a new way of performing permission management by defining the concept of conditional permission management, which is how it got its name.
If you’re at all familiar with standard Java permission management, you know that the basic approach is to grant permissions to code using a policy file.
A standard Java policy file may look something like this:
All other classes are only granted read/write access to the /tmp/ directory.
Although this example assigns only a single permission for each case, you can assign any number of permissions in a single group.
When the security manager walks up the call stack to check permissions at execution time, the permissions for a given class are determined by effectively using either its signer or code base as a key to look up the associated protection domain to see which permissions have been granted to it.
If the protection domain has the permission, the call can proceed; if not, the call fails.
In particular, it allows you to grant permissions only based on one of two conditions:
The Conditional Permission Admin Service improves on this by introducing an abstract condition concept, which allows you to grant permissions based on arbitrary conditions.
A condition acts as a Boolean guard that determines whether a permission.
Because permissions are granted to bundles in OSGi, conditions are evaluated against the bundles on the call stack to determine which permissions have been granted to a bundle.
If multiple conditions are associated with a permission group, all conditions must be satisfied for the permissions to apply (a logical AND)
Not only does it allow you to introduce your own arbitrary conditions for granting permissions, but these conditions can also be much more dynamic and fine-grained.
For example, you can create a condition to only grant permissions based on license status via remote server communication or even the time of day.
We’ll get into creating custom conditions later; for now, we’ll continue to explore what’s provided by the Conditional Permission Admin Service.
What about performance? If you know anything about Java security, you probably know it can have a significant impact on execution-time performance.
Evaluating all conditions for all bundles on the call stack on every permission check can get expensive.
Luckily, the Conditional Permission Admin Service provides a way to mitigate this cost in a lot of cases by differentiating between mutable and immutable conditions.
This means the Boolean results for immutable conditions only need to be calculated once per bundle protection domain.
The set of ConditionInfo objects encodes the conditions that must be true for the permissions to apply, and the set of PermissionInfo objects encodes the permissions to be granted.
You may wonder why you need ConditionInfo and PermissionInfo objects to encode the conditions and permissions, respectively, rather than directly creating instances of conditions and permissions.
This is because the bundle assigning permissions may not have access to the associated classes, because you’re in a modular environment.
Both of these info objects encode a target class name and its constructor arguments.
More specifically, a ConditionInfo encodes two arguments: the class name of the condition and an array of String objects for any constructor arguments for the condition class.
The PermissionInfo object, on the other hand, encodes three arguments: the class name of the permission and the standard name and actions arguments of the permission class constructor.
As a simple example, you can construct a PermissionInfo object like this:
To see a ConditionInfo example, you’ll need a concrete condition to play with, so we’ll introduce one next.
We’ve talked abstractly about conditions, but we haven’t yet discussed any concrete condition types.
Intuitively, you can probably guess that these conditions correspond to the two types of conditions that exist in standard Java policy files.
You’ll learn about the former right now and the latter when we discuss bundle signing a little later.
In other words, this condition matches bundles with the same location string, which for all intents and purposes is equivalent to the bundle’s origin or code base.
The condition location string may contain * as a wildcard to match multiple locations.
This particular example matches all bundles coming from the foo.
Set permissions for other bundles to implement a security policy.
Nothing too surprising here, but these steps do assume you’re starting from a clean slate.
When an OSGi framework is started for the first time with security enabled, all bundles have AllPermission.
If your bundle isn’t the first, it may not be able to get the service or may get a security exception when it tries to change the security policy, because AllPermission is required to change permissions.
For now, let’s assume your bundle is first or at a minimum has AllPermission.
Although it may not technically be necessary, you clear the list to make sure there aren’t any other random permissions in your security policy.
What does this particular permission entry do? The name you set is a unique key to identify the entry and has no inherent meaning; if you specify null for the name, a unique name will be generated for you.
The single ConditionInfo and PermissionInfo objects in their respective arrays match your bundle and grant it AllPermission.
We’ll expand on the last argument, the access-decision flag, in the next section.
Assuming this completes successfully, you’ve successfully modified the security policy.
To set permissions for other bundles, you follow a similar set of steps: get an update object, add or remove any desired permissions, and then call commit()
Just make sure you don’t delete the entry giving your own bundle AllPermission!
Until this point, we’ve talked about granting permissions to allow code to perform some operation.
This is the standard way to think about permissions in Java.
The OSGi R4.2 specification introduced a new wrinkle: deny-access decisions.
Instead of only using permissions to say what code is allowed to do, you can also use them to say what code isn’t allowed to do.
Being able to allow/deny permissions makes it possible to use a white list/black list approach for handling security.
Deny-access decisions can significantly simplify some security policies because they let you easily handle an exception to a general rule.
Consider a case where you want to allow a bundle to import all packages except those.
This highlights the most important difference between this older (and deprecated) approach and the newer update approach: changes happen immediately and don’t require any sort of commit operation.
Managing permissions with Conditional Permission Admin with names starting with org.foo.secure.
How can you implement such a policy with only allow-access decisions? You’d have to exhaustively grant permissions to import every package except the ones you want to exclude.
During a permission check, if the associated conditions match and the permission being checked is implied by the associated permissions, the bundle on the stack will be denied permission to perform the operation.
To complete the hypothetical example, you can grant a bundle the following permission:
Of course, to give it permission to import everything else, you also have to grant it the following permission:
This also raises another important point when mixing allow and deny decisions into a single security policy: ordering.
When a permission check is triggered, the entries in the policy table are traversed in ascending index order until the first one is found where the conditions are satisfied and the required permission is present.
If the associated access policy is DENY, the check fails.
If it’s ALLOW, the checking continues with the next bundle protection domain on the stack.
Thus, in the example, to implement the policy correctly the denied permission must be added before the allowed permission.
To provide a slightly more familiar approach to defining a security policy for seasoned Java developers, you’ll now use this API to create a policy-file reader.
A lot of what you need for doing this is provided by the OSGi specification already, so it’s pretty easy to accomplish.
As luck would have it, the Conditional Permission Admin Service specification standardizes such an encoding.
In a similar fashion, the encoded format of a PermissionInfo is.
As with conditions, type is the fully qualified class name of the permission, and the remaining are the quoted name and actions for its constructor.
A more concrete example looks like this (we’ve added line breaks for readability):
All you’ll need to do to set and/or change your security policy is to edit your policy file and then start this bundle.
More precisely, its start() method looks like the following listing.
You get the policy file, which defaults to a file called security.policy but can be configured.
The previous step is necessary to make sure the policy-reader bundle has sufficient permission to make future changes to the security policy.
You clear the existing policy to make sure you’re starting with a clean slate, and then loop through the encoded permissions to decode them and add them to your list of objects.
The only thing left to do is commit the update.
Because the update may fail if the permissions were changed concurrently, you throw an exception in this case.
This bundle is generic and can be used in any security-enabled framework to put a policy file into effect.
You’ll see it in action a little later when we show a complete example with digitally signed bundles and a custom condition.
Defining a security policy by assigning permissions to bundles is a workable approach, but being able to step up a level can simplify things.
For example, you may trust a particular provider, so it’s nice to be able to assign permissions based on the provider rather than individual bundles.
Doing so simplifies defining a security policy, because it raises the level of abstraction.
Digitally signed bundles can help you achieve this; specifically, they help you do two things:
The former provides the ultimate goal, but without the latter, the former would be meaningless.
You’ll learn about both as we discuss digital signing and certificates.
We’ll show you how to create certificates and use them to digitally sign your bundles.
Providing a complete and detailed description is beyond the scope of the book, so we’ll focus on describing just enough to have it make sense.
With that in mind, table 14.6 introduces some relevant terms we’ll use throughout the remainder of the chapter.
You don’t need a complete understanding of digital cryptography to use the technology effectively.
You’ll be using digital signing based on public key cryptography, which involves a public key and a private key.
The public key is shared with the world in the form of a certificate.
The private key is kept secret and used to sign data by performing a computation over it.
The resulting value can be verified by performing another calculation over the data using the public key.
This verifies that the signer has access to the private key and that the data hasn’t been modified.
Digital signing A mathematical approach for verifying the authenticity of digital data.
Specifically, used to verify the identity of the provider and that the data hasn’t been modified.
Signature A unique value calculated when data is digitally signed.
A form of digital signing using two mathematically related keys: a public key and a private key.
The private key is a guarded secret used to sign data.
The public key is shared with others in the form of a certificate, which they can use to verify that a signature was generated with the private key.
This allows you to infer the identity of the provider and determine whether someone has tampered with the data.
Certificate A form of metadata about a public key, binding it to the identity of the private key holder.
This binding is achieved by having a well-known (trusted) third party sign the public key/ identity pair.
The identification portion of a certificate; specifically, as defined by the X.509 ITU-IT standard.
A certificate has a reference to the certificate of its third-party signer, which includes a reference to the certificate of its signer, and so on, until the root.
The root of the certificate chain is a self-signed certificate.
In OSGi, the signer of a bundle is associated with it.
With this association, you can grant permissions to a bundle based on its signers.
For example, you can assign permissions to all bundles from a particular company, if the company signs its bundles.
You can also grant permissions to perform operations on bundles signed by specific principles.
These approaches provide a simple yet powerful way to control who can do what inside your application.
An administrator can grant a restricted set of permissions to a signer, after which the signer can create bundles that use those permissions or some subset, without any intervention or communication with the administrator for each particular bundle.
To understand how this all fits together, consider the following scenario.
Assume you have a system that features a set of core bundles (which we’ll call the core domain) and an arbitrary number of third-party plugin bundles (which we’ll call the third-party domain)
This means you expect fully trusted bundles and not completely trusted bundles to exist in your system, but you want to provide a level of isolation between them.
Your goal is to create a simple security policy that lets you manage core and third-party bundle domains without knowing the precise bundles in each set.
To implement the desired security policy for this scenario, you need to create two root certificates for the core and third-party domains.
These root certificates will be used by the framework to establish a chain of trust for the two domains.
With these two certificates, you can then sign certificates of core and third-party providers with the appropriate certificate.
When they use their individual certificates to sign bundles they’ve created, the framework can use the root certificates to establish a chain of trust and determine to which domain a bundle belongs.
The details of all this are based on Java 2 JAR file signing, which means the same tools you use to sign JAR files can be used for OSGi.
To create the needed certificates and their associated public and private keys, you’ll use the keytool command provided by the JDK.
It can create and manage certificates and keys inside a keystore, which is an encrypted file defined by Java for this purpose.
For this scenario, you use keytool to create two certificates and their associated public/private keys for the core and third-party domains like this:
This creates a keystore called keys.ks containing two new key pairs and a certificate for each pair with aliases of core and third-party.
The keystore is protected by the password foobar, and the keys themselves have the password barbaz.
The -dname switch allows you to specify the distinguished name you use to identify yourself, which in this case is the baz organization in Germany (de)
For our purposes, it’s sufficient to recognize a distinguished name (DN) as a set of comma-delimited attributes, such as in the example: CN=core,O=baz,C=de.
These attributes specify the common name, organization, and country, respectively.
The hierarchical aspect of this namespace is that it goes from the least significant (but most specific) attribute to the most significant.
The root of the tree for these attributes is the country, which is then divided into organization, and further divided into common names within an organization.
Two DNs with the same attributes but different order are different DNs.
The next thing to do is sign your key pair certificates with themselves.
It may sound a little strange, but this is how you make them root certificates.
It’s a common thing to do, as you can see by the fact that the keytool command has support for it:
The only difference from the previous command is that you use -selfcert instead of -genkey.
Now you have key pairs that you can use to sign other certificates or bundles to make them part of your trusted certificate chain.
To allow other people to verify your signatures, you need to extract the certificates from the keys.ks keystore and import them into a new keystore called certificates.ks.
Why? Because the keys.ks keystore contains your private keys; you need another keystore that contains only your public keys to share with the outside world.
Currently, your certificates are saved as key entries (a public/private key pair and its certificate) in the keystore.
You need to export them and re-import them as certificate-only entries, which you do like this:
You can verify the contents of your keystores like this:
You have everything in place now, which means we can look into signing bundles to make them members of one of your domains.
A bundle JAR file can be signed by multiple signers; the signing follows normal Java JAR signing rules.
The only additional constraint for a bundle is that all entries inside the bundle must be included in the signature, but entries below the META-INF/ directory aren’t included.
Normal Java JAR file signing allows for partially signed JAR files, but OSGi doesn’t.
It’s lucky that signing all entries except those below META-INF/ is the default in JAR signing, so you can use the jarsigner tool included in the JDK.
The following will sign a bundle with your core private key:
Signing another bundle with your third-party private key looks very similar.
This command should output jar verified if you’ve correctly signed the bundles.
Assuming you have, you now have one bundle in the core domain and one in the third-party domain.
This makes it easy for you to grant permissions to either, based on the signer of a bundle, as you’ll see next.
To assign permissions to bundles based on who signed them, you need a condition.
If you specify the exclamation mark, it negates the result of the DN matching expression.
This matches a bundle on the call stack if it’s signed by the core certificate of the example, which means any permissions associated with this entry will be granted to.
On the other hand, the following won’t match a bundle if it was signed by the core certificate:
The DN matching expression in these two examples illustrates how simple DN matching can be.
It can also be sophisticated, because it supports various flavors of wildcard matching.
You saw that a DN is composed of multiple attributes, like country, organization, and common name.
When performing DN matching, you’re matching against these attributes using a comma-delimited list, such as what you saw earlier with CN=core, O=baz,C=de to match the core certificate.
Additionally, because certificates can be signed by other certificates, you can match against the other certificates in the chain: you delimit different certificates with a semicolon.
To match certificates in a chain, use DN matching expressions against the DN associated with each certificate you’re trying to match.
This matches a bundle that was signed by the bar organization from France using its extensions certificate, which was signed by your core certificate.
You need to understand two important points about chain matching:
Certificates further up the chain that aren’t mentioned are ignored.
Order is important, because reversing it indicates the opposite signing relationship.
When you match certificate chains, you’re specifying an interest from the most specific certificate of the chain onward.
Both attribute matching and certificate chain matching support wildcards, but the rules for comparison are more complicated than string-based wildcard matching.
If a wildcard is used as part of the right-hand argument of an attribute, such as.
CN=*,O=baz,C=de this matches either of the two certificates (core and third-party)
You can also use a wildcard for more than one attribute:
This matches any certificate from the baz organization from any country.
The rules for certificate matching are also relevant to AdminPermission, discussed in section 14.3.3
If you recall, AdminPermission accepts an LDAP filter over a limited number of attributes to describe target bundles.
The value for the signer attribute of the LDAP filter is a DN matching expression.
Certificates are trusted when they’re known by the OSGi framework.
Each file path must point to a JKS keystore, which can’t have a password.
The framework uses the keystores as trust repositories to authenticate certificates of trusted signers.
The stores must be used only as read-only trust repositories to access public keys.
You should now understand how to use certificates to sign your bundles and grant permissions based on the bundle signer.
With that out of the way, let’s look at how you can use local permissions to know which permissions a bundle needs.
For the example, it’s another way to match both of your certificates.
This kind of wildcard can also be combined with the previous:
O=baz,C=* This also matches all certificates from the baz organization from any country.
The attribute wildcard can be used in a certificate chain and behaves as described earlier, but when used standalone it matches at most one certificate.
CN=core,O=baz,C=de This matches either a bundle signed by another certificate that was signed by your core certificate or a bundle signed directly by your core certificate.
CN=core,O=baz,C=de This matches any bundle signed by your core certificate anywhere in the certificate chain.
Bundle signing provides a powerful yet fairly simple mechanism for creating desired security policies.
But it doesn’t help address one nagging issue: how do you know which permissions to grant a bundle? Even if you’ve verified a bundle’s signature and know that the bundle comes from a trusted provider, you still need to answer this question.
Even if you fully trust a provider, it’s better to limit a bundle’s permissions to a precise set of required permissions to further prevent intended or unintended security breaches.
The standard Java security architecture doesn’t help you here; instead, you must rely on prior knowledge about the code’s requirements or trial and error.
OSGi specifically addresses this issue with a concept called local permissions.
Local permissions are defined by a resource contained in the bundle, which describes the maximum set of permissions required by the bundle.
This set of permission is enforced by the OSGi framework.
A bundle can be granted fewer permissions than its local permissions, but it never gets more permissions.
At first blush, it may seem a little odd to have a bundle define its own permissions, but the purpose is more for the deployer to audit and analyze a bundle.
Bundles aren’t guaranteed to receive the permissions they request and therefore should be programmed to degrade gracefully when they receive less.
As a deployer, though, local permissions simplify your life because you can easily determine what permissions are required and which you’re willing to give.
For example, if the local permissions request the use of network sockets, it’s clear that the bundle has the potential to access the wider internet.
If you decide this is acceptable, you can trust this audit because it’s enforced by the framework at execution time.
What do local permissions look like in practice? The bundle-permission resource is a file in the bundle’s OSGI-INF/ directory called permissions.perm.
It contains a listing of all of the bundle’s required permissions.
As a simple example, let’s assume you provide a bundle that only wants to export a single package, org.foo.
All other non-empty lines describe required permissions as encoded PermissionInfo objects.
This is simple but effective when it comes to auditing the security impact of a given bundle.
You’ve now learned about some powerful tools for defining a security policy; but in the infamous words of many infomercials, “Wait! There’s still more!” In the next section, we’ll cover the most advanced tool available: the ability to create custom conditions for your security policy.
We’ll explore why you may want to do this and show you how to do it by implementing two custom conditions.
Although these are often sufficient to implement reasonable security policies, in some cases you may want or need more.
In this section, we’ll show you how to harness this power by creating two custom conditions: a date-based condition and a user-input condition.
As you may imagine, providing custom conditions is a security-sensitive process.
You certainly don’t want a malicious bundle to shadow an actual condition with a faulty one.
For this reason, providing conditions isn’t possible via normal bundles.
Custom conditions are valid only if they’re made available from the framework’s class path (that is, they’re provided by the system bundle)
When you use ConditionInfo to construct a new condition instance, the framework loads the specified condition class from the class path and tries to call a static method on it that looks like this:
This is a factory method, although it need not return a new instance for each call.
If such a method isn’t available, the framework falls back to trying to find a constructor to invoke with the following signature:
Assuming it finds one or the other, it uses the condition as part of the permission check.
The custom condition must implement the Condition interface, which is defined as follows:
The static TRUE and FALSE objects are used for conditions that are always true or false, respectively.
Its getCondition() method can determine immediately whether the supplied bundle’s location matches; it only needs to return TRUE for matches and FALSE for nonmatches, because these values will never change.
Other than that, the interface is reasonably simple, but the best way to explain the remaining methods is by way of some examples.
Assume you want to restrict certain permission sets to be available before a given point in time, but not after.
Imagine that you want to associate permissions with a period of validity, where the ability to perform certain operations expires after some time.
The following listing shows a condition you can use to make this possible.
When the framework evaluates this condition, it uses the static getCondition() method to create an instance for the target bundle.
The condition’s constructor B converts its argument to a long, which sets the date.
The framework then checks whether the condition is postponed by calling the isPostponed() method.
This tells the framework whether the condition should be evaluated immediately or deferred; this condition is immediate, but you’ll see an opposite example later.
Because this condition isn’t postponed, the framework invokes the isSatisfied() method immediately to test the condition.
This method checks whether the current time in milliseconds is still lower than the ending date supplied in the constructor argument.
Note that the second isSatisfied() method is only used for postponed conditions and is ignored here.
The isMutable() method is purely used by the framework to optimize condition evaluation.
If a condition is immutable, the framework only needs to call its isSatisfied() method one time and can cache the result.
For mutable conditions, the framework needs to evaluate the condition on every check.
For this particular condition, you have an interesting case because it’s mutable until the ending date is reached, after which it becomes immutable.
You can now use this custom condition to define your security policy like the standard conditions.
For example, in the policy file you can do something like this:
As we mentioned previously, you need to put this condition on the class path of the framework to use it.
You can achieve this by adding it directly to your application class path or by using a special kind of bundle called an extension bundle.
To package this custom condition inside an extension bundle, you create a bundle with the following manifest:
Now you need to install this bundle into your framework, after which you can use the condition in your security policy.
That was fairly easy, so let’s move on to a more sophisticated example.
Often, the only means to determine whether some code is allowed to perform an operation is to ask the user.
We see this regularly when running Java applications from a web browser or on a mobile phone.
Extension bundles Extension bundles can deliver optional parts of the framework implementation or provide functionality that must reside on the boot class path.
An extension bundle is treated as a fragment of the system bundle.
For example, the following example uses the FragmentHost header to specify an extension bundle for the Felix framework implementation:
Because extension bundles are special, there are certain restrictions on what you can do with them.
Typically, they’re used to add stuff to the class path and possibly to export additional packages from the system bundle.
In particular, permission checks tend to be fine-grained, and in this case executing the condition is costly (and potentially annoying to the user)
Luckily, the Conditional Permission Admin Service specification defines mechanisms to deal with such situations.
Certain conditions can be costly to evaluate, such as asking the user for permission.
In such situations, you should evaluate the conditions as postponed conditions.
Doing so causes the framework to delay their verification until the end of the permission check.
A condition must always return the same value for the isPostponed() method, so that the Conditional Permission Admin Service can cache the value.
If the method returns false, the no-argument version of the isSatisfied() method checks the permission, which is intended to be used for quick evaluations.
On the other hand, if the method returns true, the version of isSatisfied() that takes arguments is used; it’s intended for more costly evaluations.
For example, a condition can verify whether a mobile phone is roaming.
This information is readily available in memory, and therefore this condition need not be postponed.
Alternatively, a condition obtaining authorization over the network should be postponed to avoid the delay caused by network latency if not necessary.
Looking more closely at the parameters of the isSatisfied() method used to evaluate postponed conditions, you see that it takes an array of Condition objects and a Dictionary.
The array always contains a single element: a reference to the receiving condition.
This behavior was introduced in the R4.2 specification, because prior specification versions could verify multiple conditions at the same time.
As a result, this change makes the array relatively worthless.
The method was reused to avoid creating a breaking change for existing custom conditions.
The Dictionary parameter is a context object for the condition implementation, which it can use to maintain state between invocations.
The same Dictionary object is passed into all postponed conditions of the same type during a single permission check.
Let’s implement an example that asks the user to authorize permissions.
Impact on creating security policies From the point of view of the security-policy creator, it doesn’t matter whether a condition is postponed.
The impact is in how the framework processes the conditions and/or how the condition implementation optimizes evaluation.
The important result is that the framework evaluates postponed conditions only when no immediate condition entry implies the required permission, which allows the framework to avoid costly condition evaluation if possible.
When you’re creating a security policy, you can ignore this aspect of conditions.
You’ll implement the ask-the-user condition by splitting it into two parts:
An AskTheUser class that presents the user with a Swing dialog box asking to authorize permission requests.
This is a good example for postponed conditions, because you don’t want to bother the user with questions if the check fails for other reasons and user interaction is slow.
The constructor accepts the question to ask B, which you display in the ask() method C.
In the ask() method, you use a JOptionPane confirmation dialog box to query the user.
You return true or false depending on whether the user confirms or rejects the request, respectively.
The next listing shows how you implement the condition itself.
Because the condition is postponed, you can stub out the other isSatisfied() method C.
Now let’s look at how to implement the postponed isSatisfied() method, which is shown in the following listing.
Here you check whether you’ve already asked the user by looking at the m_alreadyAsked flag, which is only necessary to avoid a race condition if multiple threads are trying to set the initial value; after that, the framework will cache the result because the condition is immutable.
If the user hasn’t already been asked, you create a new AskTheUser object with your question C and call its ask() method.
When you get the result, you set the alreadyAsked flag to true D to make sure that the user is asked only one time for the given bundle.
Finally, you return the result or invert the result if “!” was specified in the ConditionInfo.
This is because the use of Swing will result in a lot of additional permission checks, so you must limit the protection domains involved to the protection domain of the condition itself.
Because the condition must be on the class path, it’ll have the protection domain of the framework, which needs to have AllPermission.
If, for whatever reason, you get an exception, you return false.
If you package it as an extension bundle or add it to the framework class path, you can use it to let the user make security decisions by including it in your policy file like this:
We’ve covered a lot of ground in this chapter, so you should be commended for making it this far.
To wrap up the discussion on security, we’ll look at an example that pulls everything together.
What’s left is to show you how to start a framework with security enabled.
You need to make sure a security manager is installed in the system and tell the framework where it can find the trusted root certificates.
You can either set a custom security manager or have the framework install its own security manager.
If the property isn’t specified, the framework doesn’t set the security manager; security will still work if a security manager is already set, but not all features of OSGi security may work.
In particular, if the existing security manager uses AccessController, postponed conditions won’t work.
Even though some aspects of enabling security are standardized, not all aspects are.
As a result, enabling security is handled a little differently by different framework implementations.
We’ll use the Apache Felix framework to show a concrete example.
The Felix framework is special because it provides its Conditional Permission Admin Service implementation as an extension bundle.
This means that in addition to setting the previous properties, you also need to deploy the security provider bundle.
Luckily, this is easy to do with the bundle launcher; add it to the directory containing the bundles you want to launch.
This gets you a framework with security enabled and an initial security policy.
In this case, the initial security policy file contains the following:
This sets up your framework with a keystore and the password necessary to access it.
To illustrate what you can do with all of this, let’s add to the paint program a security policy that uses the security features you’ve learned about.
The security policy will allow core providers to provide shapes automatically; all others will require explicit approval from the user.
Other than provide shape services, bundles are allowed to do anything.
Start by creating a policy file with an entry to grant AllPermission to bundles signed by the core certificate:
Next, create an entry to grant all other bundles permission to register a shape service based on the condition that the user approves it.
If a bundle that isn’t signed by your core certificate tries to register a shape service, the user is asked to grant or deny that request.
Pretty simple, right? This entry only deals with asking the user to approve shape services from non-core bundles; you still need to create an entry to grant these bundles the ability to do everything else.
In that case, you need to grant them AllPermission except for registering shape services.
If you follow the rule ordering in the policy file, it looks like it will always prompt the user if a bundle isn’t signed by the core.
But this isn’t the case, because AskUserCondition is a postponed condition.
That means it’s evaluated only if no other rule with an immediate condition implies the permission.
If a bundle is signed by the core certificate, it immediately matches the first rule, which allows it to do anything.
If a non-core signed bundle performs any secure operation other than registering a shape service, the first rule doesn’t apply, the second rule is postponed, the third rule doesn’t apply, and ultimately the fourth rule is matched that allows the bundle to do anything.
If a non-core signed bundle tries to register a shape service, the first rule doesn’t apply, the second rule is postponed, the third rule applies because the permission is implied, and this rule fails due to the DENY access decision.
This causes the framework to evaluate the postponed second rule because it logically came before the failed entry, which prompts the user and grants the permission based on the user’s reply.
As you can see, you only ask the user at the end if all other rules don’t provide the needed permission.
The DENY access decision of the third rule provides a way to shortcircuit the rule evaluation.
To see this security policy in action, go into the chapter14/ combined-example/ directory of the book’s companion code; type ant to build it and the following to run it:
This starts your shell in a security-enabled framework running your security policy.
You first need to install the bundles of the paint program:
These bundles were signed by the core certificate and have AllPermission, so you should see an empty paint program (one with no shapes) after starting the paint bundle.
Doing this causes the user to be prompted to grant the permission, as shown in figure 14.3
If you grant the bundle permission, you get a circle shape in the paint program, as shown in figure 14.4
Figure 14.3 Secured paint program prompting the user to grant the unsigned circle bundle permission to provide a shape service.
Figure 14.4 Secured paint program after the user has granted permission to the unsigned circle bundle.
Finally, to show that core-signed bundles can provide shapes without prompting the user, install and start the square bundle:
You should now have a paint program with circle and square shapes, as shown in figure 14.5
Congratulations! If you’ve made it this far, you know just about everything there is to know about securing your OSGi-based applications.
In this chapter, we introduced you to the Java security model and showed how OSGi uses it to provide the infrastructure to deploy and manage applications that must run in secure environments.
It’s important to have security in mind when you’re writing bundles, because otherwise they probably won’t be good citizens in a security-enabled environment.
The Conditional Permission Admin Service introduces a new way of managing security by means of conditions that must be satisfied in order for certain permissions to be applicable.
Figure 14.5 Secured paint program with an unsigned circle bundle and a signed square bundle.
Specifying local permission inside of bundles provides a convenient and simple way to audit and enforce the permissions needed by a bundle.
It’s easy to implement and provide custom conditions for use in your security policies.
Postponed conditions let you defer expensive condition evaluation until the end, which allows you to fine-tune your security policies for efficiency.
With this knowledge under your belt, you can secure your framework according to your specific security policies and develop bundles that can work in security-enabled frameworks.
In the next chapter, we’ll look into how you can use and provide web services in OSGi as well as how to build web applications on top of OSGi.
We hope that throughout the course of this book, we’ve been able to convince you that OSGi technology is fairly easy to use and extremely powerful.
This final chapter touches on an area that we haven’t covered yet but that is hugely important to many modern developers: web applications and web services.
We’ll show you how to build and deploy web applications using OSGi, and the benefits this technique can bring to traditional web-development frameworks.
You’ll reuse a lot of knowledge from earlier in the book to build a dynamic, distributed OSGi application.
Almost all organizations and many individuals have some form of web presence, whether via social networking sites, static HTML pages, simple one-tier web applications, medium-sized n-tiered architectures, Web applications and web services.
Developers of these types of systems are familiar with a number of key technologies, including web services for back-end communication between business tiers and web applications for user interaction via a browser.
If you’re reasonably familiar with Java, you know that a plethora of tools and technologies are available to help you build such applications.
In fact, there are so many that it’s impossible for us to cover all the possibilities in a single chapter.
Instead, we’ll pick a few of the more popular Java toolkits and show you how OSGi can improve on their design and usage.
From here, you should be able to extend the general principles we cover to integrate OSGi with other toolkits of your choice.
To illustrate our points, we’ll look at a number of simple examples before explaining how you can extend an existing stock-watcher web application from the Google Web Toolkit (GWT) tutorial to use OSGi.
For the purposes of brevity, we’ll focus on the aspects of these technologies that directly relate to OSGi and skip over (or even ignore) some of the more complex aspects of web development and distributed computing in general.
Our goal is to show you how OSGi can work in a web context, not how to build and manage all aspects of web applications or services.
We’ll start our foray into web technologies by looking at web applications, before moving on to web services.
Unless you’ve been living on the moon for the last decade, you must’ve had some exposure to web applications, whether as a user or as a developer.
Web applications are a class of applications that present their user interface in a standard web browser such as Internet Explorer, Firefox, or Safari.
They range from consumer shopping carts to online banking, from travel booking to social networking, from games to employment to government—the list is pretty much endless.
In this section, we’ll look at using OSGi with the following web-application technologies:
Figure 15.1 provides a simple diagram of the components you’ll build in this chapter.
What benefits can OSGi bring to web-application development to cause you to break from the status quo? The major benefits are related to OSGi’s different layers:
The module layer provides an improved physical and logical structure, so web applications are easier to maintain and deploy.
The lifecycle layer enables managing web-application installation and activation, to control what is available and when.
The services layer supports a more loosely coupled application development approach, making it easy to swap in different implementations or even move those pieces to other machines to improve performance without changing a single line of client code.
There are two main routes into the OSGi framework for web applications: the OSGi HTTP Service specification and the Web Applications specification.
The Web Applications specification is one of the R4.2 Enterprise specifications and defines a web application bundle (WAB)
A WAB is a special web archive (WAR) file that supplies OSGi metadata and relies on the OSGi framework’s lifecycle layer to control when its resources are made available.
You find the HTTP Service like any other OSGi service, by looking in the service registry using the BundleContext:
Having found the HTTP Service, what can you do with it? The HttpService interface provides methods to register and unregister static resources (for example, images or HTML pages) and Java servlets.
Figure 15.1 In this chapter you’ll build a simple web application hosted on a single OSGi framework that calls out to a number of backend OSGi frameworks using webservices protocols.
Let’s look at how you use this interface; you’ll start with registering static resources and then move on to servlets.
Let’s dive into a web application by creating a bundle to register a set of static resources.
You’ll reuse your knowledge of components from chapter 12 to build a simple iPOJO component that registers resources with the HTTP Service.
Listing 15.1 shows the complete source code for this component.
You may wonder why you’re using an iPOJO component instead of a simple BundleActivator.
The reason is the complex startup-ordering issues associated with using multiple services, because your component uses the HTTP Service and the Log Service.
You could do this without a component framework, but using one makes life simpler.
The real work, for this example, is done when you register content from the /html directory within your bundle to the root context of the HTTP Service C.
In other words, the file /html/index.html from within your bundle is served as /index.html from the HTTP Service.
You unregister it when the service is removed or the component is deactivated D.
What does the end result look like? Figure 15.2 shows the service- and bundle-level dependencies of the ResourceBinder component.
Figure 15.2 The ResourceBinder has a mandatory dependency on the HTTP Service for providing content and an optional dependency on the Log Service for logging errors.
In the previous example, you passed in null, but what does this parameter do? HttpContext provides a way to inject the HTTP Service with resourcelookup and -access policies.
Let’s first look at the API, followed by an example, to show what this allows you to do.
The handleSecurity() method provides a callback to allow the HTTP Service to verify whether a request should be allowed for a given resource.
The getResource() method provides a mechanism to define how a particular resource is mapped to a URL, which makes it possible to host contents from any scheme accessible from URLs.
Finally, the getMimeType() provides a mechanism to control the MIME type headers returned with the stream of a particular resource.
Table 15.1 describes the behavior of the default HttpContext as defined by the OSGi specification.
To demonstrate how to use the HttpContext, let’s create a ResourceTracker to track bundles and automatically register their resources with the HTTP Service.
Listing 15.2 shows the body of the addBundle() method of the BundleTracker subclass.
The client has no control over the port or URL on which the service is running.
That’s the job of the administrator of the OSGi framework.
The HTTP Service specification defines framework properties to configure the service ports:
In this case, the launcher passes system properties through to the framework properties.
As with ServiceTracker, which we introduced in chapter 4, BundleTracker supports a filter pattern based on bundle states and a customizer object to fine-tune which bundles are tracked and/or to create a customized object to track with the bundle.
Compared to the simple BundleTracker you created in chapter 3, the OSGi BundleTracker performs the same task, but does so in a more sophisticated way.
In particular, it handles concurrency issues better and allows you to track bundles based on desired states, instead of just the ACTIVE state as the simple implementation did.
In this example, you define an HTTP-Resources manifest header that bundles can use to specify resources they wish to register with the HTTP Service.
You check whether a bundle specifies any resources in the HTTP-Resources header B.
The format of this header is a comma-delimited list of directories that may optionally be aliased (you’ll see this working in a second)
If any resources are found, you create a ProxyHttpContext (shown in the following listing) and register the resources with the HttpService.
If you used the default HttpContext, the HTTP Service would try to find the requested resources in your ResourceTracker bundle, which clearly isn’t correct.
ProxyHttpContext attempts to find the resources in the bundle you’re tracking.
The key line of code in this class passes the getResource() call through to the tracked bundle B.
If you deploy this bundle into an OSGi framework along with an HTTP Service and your ResourceTracker, then its resources are registered; you can browse them at http://localhost:8080/resource/index.html.
This is just one trivial usage of the HttpContext object.
Now that you’re familiar with registering static resources with the HTTP Service, let’s look at how it allows you to use servlets in an OSGi environment.
Java servlets are the building block on which a vast number of web applications have been built.
Some of the key advantages of the servlet specification are the relative simplicity of the API and the huge number of tools and frameworks available to help you develop web applications with it.
Creating web applications provides a mechanism to dynamically register servlets using the following HttpService method:
Can you do the same for servlets? Yes, you can.
But because the registerServlet() method expects an instance of a servlet, instead of using the BundleTracker, you’ll find servlets in the OSGi service registry and automatically register them with the HTTP Service.
The following listing shows a snippet from an iPOJO component that maps servlets registered in the service registry with a Web-ContextPath service property to any available HttpServices.
In this example, you declare an optional service dependency on the LogService to allow you to inform the outside world of any exceptions.
You synchronize on the current servlets to create a snapshot of them and to add the HttpService to the set of known services.
Then you iterate over the current servlets and register them with the recently discovered HttpService B.
Listing 15.4 Binding servlets in the OSGi service registry using iPOJO.
Hence you need similar logic in the bindServlet() method, shown in the next code snippet.
In this method, you read the Web-ContextPath from the service headers B.
If this isn’t null, you then snapshot the HttpServices and store the servlet using the same object lock as in listing 15.4—ensuring that you don’t miss any services C.
Finally, you iterate over the available HttpServices and register the new servlet D.
The final piece of the puzzle is the actual registration of a servlet.
Here you create the trivial HelloServlet shown in the following listing, which prints a message in the web browser.
You register this component using the Servlet interface and add the Web-ContextPath service property with iPOJO annotations.
Listing 15.5 Binding servlets in the OSGi service registry using iPOJO (continued)
Listing 15.6 Binding servlets in the OSGi service registry using iPOJO (continued)
Before leaving this section on the HTTP Service, we should also point out the support provided by the Pax Web project (http://wiki.ops4j.org/display/paxweb/Pax+Web)
This project defines a WebContainer service interface that extends the standard OSGi HttpService interface.
This new interface provides a number of extra methods to register other servlet-related services, including JSP, servlet filters, and servlet event listeners.
We won’t cover Pax Web in depth, but we’ll show you how to run a shopping cart example from another Manning publication, Web Development with Java Server Pages, Second Edition (Fields, Kolb, and Bayern, 2001), in an OSGi context.
The following listing shows a Declarative Services component for registering JSPs when the WebContainer service is published in the OSGi service registry.
This component registers all JSPs in the bundle under a shared HttpContext and unregisters the JSPs.
The relationship between the servlet and HTTP contexts The HTTP Service specification specifies that only servlets registered with the same HttpContext object are part of the same ServletContext.
The HTTP Service implementation creates a ServletContext for each unique HttpContext object that is registered.
You specify the component implementation class and declare the component as immediate, so an instance is created as soon as the component’s dependencies are satisfied.
Then you specify a one-to-one dependency on a WebContainer service, which you want injected into your component using the specified binding methods.
We’ll leave it as an exercise for you, but you can trivially extend this to use the BundleTracker approach from listing 15.2 to track JSP bundles centrally, rather than duplicating binding logic in different bundles.
To see this example running, go into the chapter15/pax-web/ directory of the book’s companion code.
Type ant to build the example and java -jar launcher.jar bundles to execute it.
When you do, you should see a simple shopping cart page.
Add a couple of items to the cart to verify that it works, as shown in figure 15.4
We’ve shown you how to deploy a range of web-application technologies from static resources to servlets to JSPs using the HTTP Service or its extensions.
This may leave you wondering, “What about my WAR files?” Good question.
In the next section, we’ll look at the standard way to deal with WAR files in OSGi.
Despite the widespread use of WAR files, until recently there was no standard way to use WAR files in an OSGi framework.
Due to the increasing use of OSGi technology in the enterprise domain, member companies in the OSGi Alliance are now producing enterprise-related specifications.
The OSGi R4.2 Enterprise specification is the result of this effort.
The Enterprise specification defines another set of compendium specifications specifically targeting enterprise technologies.
One of these specifications is the Web Applications specification, which provides a standard way for servlet and JSP application components to execute within an OSGi framework by defining a web application bundle (WAB)
Figure 15.4 JSP shopping cart application running in an OSGi environment.
To demonstrate the process of creating a WAB, you’ll take the stock-watcher application from the GWT tutorial and convert it to run in an OSGi context.
You can use bnd to convert the WAR file generated by the GWT build into a bundle using the following Ant target:
Bnd takes its configuration properties from the build.properties file in the same directory, which contains the following:
Most of these headers look similar to those introduced in chapter 2; if you aren’t familiar with bnd syntax, refer to appendix A.
Briefly, you first specify the bundle symbolic name for your WAB.
Next, you set up the bundle class path to include the gwtservlet.jar file, which is embedded in the WAR file, and the WEB-INF/classes directory, which contains the classes of your application.
You embed the various resources used by this application, including JavaScript files and images.
Then you specify two optional package imports that are only used in testing scenarios.
The header is used by the web container extender bundle.
This bundle is defined in the Web Application specification; it uses the extender pattern, which we discussed in chapter 3, to track bundles with the Web-ContextPath header and register the servlet resources specified in these WABs as a web application, similar to the previous examples in this chapter.
The value of this header specifies the context root that the web container uses to register the web application.
All web-accessible resources in the bundle are served up relative to this path.
Before we delve any further into the inner workings of WAB files, let’s launch the GWT application to show it in action.
Go into the chapter15/gwtapp/ directory of the book’s companion code.
Type ant to build the example and java -jar launcher.jar bundles to execute it.
The Web Applications specification allows you to take advantage of OSGi’s module layer to share classes installed elsewhere in the OSGi framework, ensure that you have the correct dependencies installed, and enforce defined module boundaries.
You can also use the lifecycle layer to allow dynamic installation, update, and removal of your web application.
What about services? Yep, the example uses services too! The following listing shows how.
Figure 15.5 The Google stock-watcher application running in an OSGi context.
Modularity improves memory consumption In this trivial example, sharing classes offers relatively little value, because the stock watcher has few external dependencies.
But consider the benefits of being able to share classes in a large web application environment.
In standard WAR development, each application must embed its own set of dependencies in its WAR file under the WEB-INF/lib directory.
For utility classes, such as collections libraries, XML parsers, and logging frameworks, this can mean a lot of duplicate classes get loaded into the VM for each WAR file installed in your application server.
In an OSGi environment, you can move dependencies into separate bundles that are then shared among installed applications, reducing the overall memory footprint.
Although the details of GWT aren’t important for this example, note that you override the init() and destroy() methods of javax.servlet.
Having cached a reference to the BundleContext, you can use it to discover other services in the framework.
To demonstrate how you can use services in a WAB context, you can make a minor change to the sample GWT application to discover a trivial StockProvider service from the OSGi registry using the following interface:
This service returns a Map of stock prices for the given symbols.
You see whether a StockProvider service is registered using a ServiceTracker.
If one is available, you use it to read prices for the specified symbols or throw a checked exception to indicate that an error message should be displayed to the user.
You’ve now taken an existing servlet application and deployed it to an OSGi environment using the WAB format.
You’ve also extended this application to discover services from the OSGi registry.
As it stands, this is a trivial example; but you’ll see in the next section how to extend the example further by using the service abstraction to allow your application to be divided into a multiprocess application.
First, we’ll briefly cover one remaining area of interest: how to support standard WAR files in OSGi.
As a convenience for users who wish to migrate web applications to OSGi but don’t wish to undertake the effort of converting a WAR file to a WAB, the Web Applications specification provides a utility mechanism to convert a WAR file to a WAB at execution time: the Web URL Handler.
To use the Web URL Handler, all you need to do is prefix any existing URL pointing to a WAR file with the webbundle protocol when installing the WAR file into the framework.
For example, you could use your shell’s install command like this:
The Web URL Handler converts the referenced WAR file into a WAB on the fly prior to the OSGi framework installing.
The Web URL Handler makes a best-effort attempt to convert a WAR to a WAB, but in certain circumstances you may have to give it extra hints to help the process go smoothly.
In this example above, you specify a BundleSymbolicName as a parameter in the query portion of the URL.
The Web URL Handler also supports a number of other parameters that affect the outcome of the conversion; these parameters are listed in table 15.2
OSGi and JNDI Retrieving the OSGi bundle context from the ServletContext is the most direct way to interact with the OSGi environment.
Many existing servlets use JNDI to discover Java EE services—wouldn’t it be great if a bridge existed between these two worlds? Such a bridge does exist in the R4.2 Enterprise specification, so rest assured that you can use this mechanism to access services.
Unfortunately, these are singletons, so they can’t be shared and they aren’t dynamic.
When the URL Handlers service receives a request for a specific protocol or content type, it delegates the request to the appropriate underlying service to perform the processing.
In the first half of this chapter, we’ve looked at a range of web-application technologies and shown how they can be integrated with OSGi.
In the second half of this chapter, we’ll turn our attention to making OSGi services available across process boundaries—that is, how to build distributed OSGi applications.
Until this point in the book, all your applications have resided in a single JVM process; but this is rarely the case for web-based applications.
The entire ethos of internet-based development is predicated on distributed processes communicating over network protocols.
You saw how to do this at a low level in chapter 3, where you built a simple telnet implementation.
But this is the early twenty-first century, and the zeitgeist for distributed computing today is web services.
In this section, we’ll investigate OSGi-based technologies for communicating between JVM processes using web-service protocols.
Obviously, we’ll only be able to scratch the surface of distributed computing, because the topic is too large and complex to cover in a single section of a chapter.
Instead of going into a lot of detail about specific web-service protocols or technologies, we’ll introduce you to some of the key features of the Remote Services specification, which is another specification in the OSGi R4.2 Enterprise specification.
The Remote Services specification and its sibling specifications, Remote Services Admin and SCA Configuration Type, provide a comprehensive model for building distributed computer systems in OSGi.
The value of this parameter must follow OSGi versioning syntax.
Import-Package List of packages on which the WAR file depends.
Web-ContextPath Context path from which the servlet container should serve content from the resulting WAB.
If the input JAR is already a WAB, this parameter is optional but may be used to override the context path.
To see how this works in practice, let’s look at the stock-watcher application you built in the last section.
It has a three-tier architecture, consisting of a web browser connected to a back-end servlet engine that talks to an in-process StockProvider service.
A logical step in this section of the book is to split the StockProvider service into a separate JVM process and communicate with it using an over-the-wire protocol, such as SOAP.
Let’s look into how you can realize this design using the Remote Services specification.
The first step in making a distributed OSGi application is to create the remote implementation of the StockProvider service.
To do this, create the BundleActivator shown in the following listing.
Figure 15.6 The Google stock-watcher application running in an OSGi context.
As you can see, this is a fairly typical BundleActivator.
You’re basically registering a service with a set of properties.
You may be asking yourself, “Where is the remote communication?” That’s the cool thing about the Remote Services specification: it shields you from those messy details.
The specification defines a set of service properties you can attach to your services to indicate that they should be made available remotely.
The actual remote communication is handled by another bundle or set of bundles; these types of bundles are classified as distribution provider bundles.
The value * indicates that all interfaces specified when registering the service should be exported remotely.
You can also change this to a String array to specify a specific set of interfaces.
For example, consider the whiteboard pattern for servlets that we provided earlier, in section 15.1.1
It makes little sense to register a Java5 servlet interface remotely, because it’s entirely an in-memory API.
The rest of the attributes specify either intents or configuration for the distribution provider, which it uses to decide how to publish the remote service.
We’ll look at intents and configuration in more detail a little later; for now, you can probably intuitively guess that you’re requesting that your service be exposed using a SOAP interface from the specified URL.
To create a remote service, you need to select a distribution provider.
For this example, we’ve chosen to use the Apache CXF Distributed OSGi implementation (http://cxf.apache.org/distributed-osgi.html), which is a Remote Services distribution provider built on top of Apache CXF.
Type ant to build the example and java -jar launcher.jar bundles/ to run it.
You can test your intuition by visiting http://localhost:9090/stockprovider?wsdl in a web browser.
That’s all there is to it! By deploying the StockProvider bundle into an OSGi framework along with a distribution provider, you’re able to make it available remotely.
Before we move on to the client side of the example, let’s look a little more at intents and configuration.
To understand intents and configuration it’s useful to consider the actual mechanics of how OSGi distribution providers publish a service remotely.
This is the cue for it to make the corresponding service available remotely.
Given no other information, a distribution provider can pick any number of ways to make the service available remotely.
It can use various protocols (SOAP, REST, RMI, and so on)
It can use a range of different security or authentication mechanisms (such as SSL, DSA, Blowfish, LDAP, or Kerberos)
There are even many different transport technologies (HTTP, JMS, Unicast, Multicast, and P2P)
There’s no single best choice for any of these options.
When you’re building distributed applications, as with most applications, one size doesn’t fit all.
Having said that, it doesn’t make sense for business-level services to specify the minute details of how they should be made available remotely.
Coming back full circle to the theme from chapter 2, this is another area where separation of concerns is applicable.
Intents and configurations provide a layer of indirection between the service provider and the distribution provider.
They allow the service provider to specify just enough information to ensure that the service behaves as expected, yet still allow the distribution provider to optimize communications for the environment in which they’re deployed.
Now that you understand what intents and configuration are in the abstract, let’s look at them in concrete terms.
Intents are a pattern borrowed from the Service Component Architecture (SCA) specification.
An intent is a string value with agreed-on distribution-level semantics.
To make this concept less abstract, let’s look at an example of an intent you might attach to a registered service:
In this case, you’re communicating two different intents to the distribution provider.
The second, authentication, says the client application should be authenticated prior to using the service.
Figure 15.7 When making remote services available, the number of options is bewildering: protocols, transports, authentication schemes, and encryption algorithms all play their part.
The precise details of how these intents are accomplished is left up to the distribution provider.
Because the meaning of the intents is well-known, a distribution provider can make its best attempt at how to achieve them in its underlying implementation.
This aids in decoupling distributed applications, because you can specify the qualities of the desired remote communication without tying yourself to a particular distribution technology.
If you move your application to a different environment, a different distribution provider may make equally valid but potentially different choices.
The SCA specification defines many intent values, but the precise details are beyond the scope of this book—for more information on SCA, visit the OSOA consortium website (www.osoa.org)
A service provider specifies intents, and the distribution provider realizes them.
The distribution provider must honor the requirements of the service provider, but it’s free to add any behaviors it feels are appropriate.
These may include default communication protocols, authentication schemes, and buffering strategies, as shown in figure 15.8
In summary, intents provide a distribution provider with some flexibility when it comes to deciding how to distribute a service.
Sometimes, though, you know exactly how you want your service to be made available remotely.
In these situations, you need a mechanism to give specific instructions to the distribution provider.
This is expressed by delimiting the intent value with the.
Figure 15.8 Service providers and distribution providers can each define intents that are applied to a service endpoint.
Configuration properties provide a mechanism for the service provider to communicate explicit settings to the distribution provider.
Given the range of possible configuration schemes, the Remote Services specification defines a mechanism for how the configuration is encoded.
In the earlier example, you added the following property to the service:
This specifies that the configuration properties follow the CXF web-services configuration scheme or configuration type.
Note that this doesn’t mean that only CXF can be used to distribute the service; it means the semantics of the configuration properties are defined by CXF.
The Remote Services specification suggests a naming convention for configuration properties, which is the configuration type followed by.
It’s possible to use a number of different configuration types, in which case you may see something like this:
The idea behind using configuration properties from multiple configuration types is to make your service’s configuration more broadly applicable.
Some other distribution providers may understand it too, but not all of them will.
By using additional configuration types, you make your service’s configuration understandable to a wider range of distribution providers.
They provide an extensible and flexible mechanism for service providers to specify to distribution providers.
This forms a recommended approach for different distribution providers to share configuration data in a vendor-neutral format.
Providing and consuming web services how services should behave in a distributed environment.
Let’s now turn our attention to the other side of the equation: client-side distributed services.
Returning to the stock-watcher example, what do you need to do it to make it use the remote StockProvider service? Currently, it looks for the StockProvider service in the OSGi service registry; what needs to change? With respect to your application, nothing at all.
Because your client bundle runs in a separate JVM, all you need to do is install a distribution provider into the client-side OSGi framework and configure it to discover the distributed StockProvider service.
The distribution provider will automatically create a proxy of the remote service and inject it into the local service registry, which the stock-watcher application will discover and use as normal.
If you’re familiar with technologies such as Zeroconf, SSDP, UDDI, and Jini, you’re acquainted with the concept of discovery.
Even if you aren’t familiar with these technologies, it should be relatively intuitive that discovery is a pattern used in distributed computing to allow a service provider to announce the presence of a service and for a consumer to find it.
Often, this is achieved using a central registry or peer-to-peer mechanism, such as multicast TCP or multicast DNS.
With such approaches, services are discovered as needed by client applications.
The Remote Services specification provides an extensible pattern for implementing service discovery, which we’ll cover in more depth in the next section.
For now, we’ll look at what you need to do to configure your distribution provider to discover the remote StockProvider service.
Figure 15.9 The distribution provider bundle creates a remote endpoint for the service provider.
It may also announce the location and type of this endpoint for other distribution provider bundles to find.
The client-side distribution provider discovers remote endpoints and creates proxies to these services, which it injects into the local OSGi service registry.
The Remote Services specification doesn’t explicitly define how discovery is implemented, only how it should behave.
With this approach, the discovery process is directed by XML files contained in bundles installed in the framework.
The following listing shows the XML that describes the StockProvider service, which is nearly identical to the configuration for publishing the service.
You define the interface that the discovered service will provide B.
Then you provide the configuration entries needed by the distribution provider to bind to the remote service into the OSGi service registry C.
In this example, this new bundle is purely for configuring the discovery process, so it only contains this XML file.
The service is automatically published into the OSGi service registry; you can look it up and invoke methods on it, which results in remote method invocations being sent using SOAP to the server proxy created in the remote JVM.
You now know how to provide and consume remote services.
Let’s wrap up this example by seeing how to use this service in the stock-watcher application.
It’s time to see the updated stock-watcher application in action.
Type ant to build the application and java -jar launcher.jar bundles/ to start it.
Providing and consuming web services http://localhost:8080/stockwatcher/stockPrices/, and enter the stock name foo.
You should see results appear in the browser and in the output of the first console, as follows:
This output shows that the method invocation of the local StockProvider service is being sent across the wire from the stock-watcher JVM using SOAP to the stock-provider JVM.
As you can see, it’s fairly straightforward to configure a distribution provider to import a remote service for use locally.
In the example, the client isn’t particularly picky about which StockProvider service it uses: it takes whichever one is available in the service registry.
The Remote Services specification allows the consumer to be more selective; we’ll conclude this section by looking into how it does so.
Earlier, we covered how the service provider uses intents and configuration to have control over how its service is exposed remotely.
In a symmetric fashion, clients often need to use services with specific characteristics.
For example, a medical insurance web application may require encrypted communications to ensure patient confidentiality, or a financial trading application may require a certain protocol to communicate between services for performance or regulatory reasons.
Using the OSGi service registry’s query mechanism, clients can select services using filters over the intents and configurations specified on published services.
Let’s consider the simplest case of differentiating between local and remote services.
In this case, the Remote Services specification requires distribution providers to automatically add a service.imported service property to imported remote services.
If you explicitly want to bind to only a remote service, you can use a filter like the following:
Alternatively, if you explicitly want to bind to only a local service, you use a filter like the following:
Dealing with failure One thing that should be obvious to experienced developers of distributed software is that remote services are unreliable.
Regardless, you should expect these types of exceptions to occur when dealing with a remote service.
Now, let’s consider the more complex case of matching remote-service qualities.
We also mentioned that a distribution provider can augment this set.
The Remote Services specification requires distribution providers to automatically add a service.intents service property to imported remote services, which contains the union of the service provider and distribution provider intents.
Therefore, if you want a service that propagates transactions and uses encryption, you can use a filter like the following:
We’ve looked at how the Remote Services distribution provider makes it easy to publish and consume remote services within an OSGi-based environment.
What if you’re coming at this from the other side? What if you’re a distributed software developer and want to import/export services from/to the OSGi service registry using your own distribution technology of choice? In that case, you’ll need to implement your own distribution provider.
The goal isn’t to create something particularly useful, but to show the underlying mechanics at play.
For the purposes of this example, you’ll create a simple RemoteRegistry interface to abstract away the details of dealing with remote services.
You’ll first see how you can export local OSGi services into your remote registry; then you’ll see how to import remote services into the local OSGi service registry.
Figure 15.10 provides a view of the classes involved in this example.
These two intents should match because the client doesn’t care how the confidentiality is achieved, but a pure LDAP filter match would fail.
To work around such issues, the Remote Services specification requires distribution providers to expand all implied qualified intents on services so LDAP queries function intuitively.
But before we get there, let’s look briefly at the other classes in this diagram.
The RemoteRegistry interface provides a simple lookup and listener scheme similar to those of the OSGi service registry.
Figure 15.10 Simple registry scheme that abstracts mechanism of service discovery.
Remote Services Admin If you’re interested in building this sort of technology, we advise you to look at the Remote Services Admin chapter of the OSGi R4.2 Enterprise specification.
It provides a more complete model for building pluggable discovery and transport schemes, but it goes beyond the scope of this book.
Hash map? You may think we’re cheating a little by using a HashMap in this example—and we are.
But this HashMap-based approach demonstrates all the key functionality of implementing a Remote Services distribution provider, which involves dealing with an externally managed service registry.
By necessity, we must ignore the complex issues in the area of distributed computing, such as network-discovery protocols, remote procedure calls, and object marshaling.
These are all important topics, but they’re beyond the scope of this book.
We leave you as architects or developers with the task of choosing your favorite distributed technologies if you wish to implement a real remote registry.
As the name implies, it tracks any services that have been marked for export.
Then you store the intents and configurations supported by your remote registry.
You’ll see a little later how these are derived; for now, let’s look at the overridden addingService() method in the next listing.
This method is called by the ServiceTracker super-class whenever a service published in the OSGi service registry matches the filter specified in listing 15.13
In this method, you first get a reference to the matching service by calling the addingService()
Providing and consuming web services method of the ServiceTracker super class.
You check whether the service’s intents match the supported intents and configurations passed into the constructor.
If so, you determine the set of interfaces by which the service should be exported.
In a real-world scenario, it might be more appropriate to register the service once with multiple interfaces.
The approach used in this book is for conceptual simplicity only.
The next listing shows how to check whether a matching service is supported by the registry.
This code reads the intent and configuration values from the matching service’s service properties.
You then use String.equals() via List.removeAll() to verify that the service doesn’t export any intents and configurations your remote registry doesn’t support, respectively.
You now need to find out the interfaces that matching services wish to export remotely.
Listing 15.15 Checking if a service matches supported intents and configurations.
You first look for the appropriate service property indicating whether the service is to be exported.
If it is, you use a utility class to return the interfaces.
Then you check to see whether the name of the exported interface is *
If it is, you get the interfaces from the standard OSGi objectClass service property, which lists all the registered service interfaces of the service object.
You also need to override the ServiceTracker methods for handling when matching services are modified or removed, but we’ll skip describing these in detail because they’re fairly similar to adding services.
Let’s turn our attention away from exporting local services to a remote registry and toward importing remote services into the local OSGi service registry.
To facilitate this, the OSGi R4.2 core specification introduced a way to hook into the OSGi service registry using two new service interfaces:
To save ourselves from repeating boilerplate code in the following examples, you define a RegistryWatcher helper class to handle the lookup of services from the.
Framework service registry hooks The OSGi R4.2 core specification allows third-party code to inject various hooks into the framework service registry.
These hooks let you monitor or even mask service lookup, service discovery, and service registrations.
Services implementing these interfaces are registered in the OSGi service registry, just like any other service, but they’re picked up by the framework implementation.
These interfaces can provide some extremely powerful patterns, but you should be highly wary because they have the capacity to create complex situations that are difficult to debug.
That being said, they’re the only practical way to build distributed service models on top of the OSGi service registry, so here we are.
Providing and consuming web services remote registry and injection into the OSGi service registry.
To give context for the example, the following listing shows the implementation of the addWatch() method of RegistryWatcher.
You begin by checking whether this a new Watch—a unique class and filter request.
If it is, you find the existing services that match your watch criteria from the RemoteRegistry.
For each service, you check whether you’ve already imported it for a different watch.
If this is in fact a new service, you create a new Registration callable object.
Here, the Registration callable object is submitted to a background thread executor to avoid deadlock scenarios that can occur if you execute external code while holding the object lock on the m_watches object.
The next listing shows the code for the Registration inner class.
This class passes through the service properties of the remote service and registers the service object in the OSGi service registry.
The final area to look at is what happens if a new remote service is discovered by the watcher.
This method is called as a result of a RegistryListener event indicating that a new remote-service reference has been added to your RemoteRegistry.
You check whether this is a new service reference and whether any existing watch has been created for this service.
If so, you create another background registration and store the future OSGi service registration.
In summary, using your helper class and the service-hook interfaces, you can find out when a remote service is needed and inject it into the local OSGi service registry on demand.
You keep track of which types of services other bundles are interested in so you know which types of remote services you should import.
This may seem a little odd, given that it happens in the added() method, but it protects your listener against race conditions due to asynchronous event delivery.
You then inspect the body of the LDAP expression by using a utility class to walk your way through the filter expression to find references to the objectClass service property, indicating the service interfaces of interest.
Finally, you add a watch in your remote registry for the discovered service interfaces specified.
Now, when another bundle registers a service listener, your listener hook will find any matching remote services in the remote registry and add them to the local OSGi service registry.
This lets you handle asynchronous service lookup; but how do you handle direct service queries? We’ll look at this next.
This implementation is trivial because it asks the registry watcher to find any matching services in the remote registry, which then adds the services to the local OSGi service registry.
We’ll skip over the implementation of the DummyRegistry, because it’s indeed trivial (the curious can look in the companion code)
You can complete the example by creating a test bundle that exports a Foo service using the service.exported.
In a second bundle, add a ServiceTracker that finds the “remote” service in listing 15.21
Because all of this example is happening in the same OSGi framework (it isn’t distributed), you explicitly look for the service.imported service property to ensure that you find the “remote” version of your service versus the local service, both of which are published in the local framework’s service registry.
Type ant to build the example and java -jar launcher.jar bundles to run it.
Although this Remote Services distribution provider is simplistic, it demonstrates the general outline and underlying mechanics for getting remote services to work seamlessly with existing OSGi applications.
In this chapter, we’ve shown you how to build web applications and web services that take advantage of the OSGi framework.
We built on the advanced features of the OSGi framework and demonstrated the extensibility of the OSGi framework, including the following topics:
Using the HTTP Service to provide static resources and simple servlet-based applications in an OSGi framework.
Converting a more complex WAR-style application based on the stock-watcher application for the Google Web Toolkit into a web application bundle.
Examining at a high level the mechanics of implementing an OSGi Remote Services distribution provider.
Let’s quickly review what we’ve covered during the course of this book.
We started by introducing you to the core concepts of OSGi development provided by its module, service, and lifecycle layers.
In the middle of the book, we moved on to practical considerations of developing OSGi, including migrating, testing, debugging, and managing OSGi-based applications.
Finally, in this last part of the book, we covered a number of advanced topics including component development, launching and embedded use cases, how to manage security, and building web applications.
We’ve covered a lot of ground, and you deserve congratulations for making it all the way through.
We think you’ll agree that OSGi is both flexible and powerful—and now you have the skills and knowledge required to build your own dynamic modular applications using OSGi.
Throughout this book, you’ve been building OSGi bundles with the bnd tool, using Ant to manage the builds.
If you’re a fan of Maven, you may be feeling a bit left out at this point, but don’t worry.
To build a bundle, all you really need is the ability to customize the JAR manifest; you don’t need to change to an OSGi-specific build system.
On the other hand, the more a build system understands about what it’s building, the more it can assist you—so which build systems work particularly well with OSGi?
We start by revisiting bnd and Ant, but this time explaining the various bnd instructions used in the book along with some advanced ones with which you may not be familiar.
Finally, we’ll round things off with a brief overview of more OSGi-specific build systems, including Eclipse Plug-in Development Environment (PDE) and Maven Tycho.
But first, let’s return to where we left off: building bundles with Ant.
A.1 Building with Ant Apache Ant (http://ant.apache.org) is a build system for Java that uses XML to describe a tree of targets, where each target describes a sequence of tasks.
Ant is extended by writing new tasks in Java, such as the bnd tool’s bnd task that can generate one or more bundles from a given class path.
Usually, when you create a JAR, you take a directory and archive its contents.
OSGi bundles—there’s no easy way to tell if the OSGi manifest matches the contents or to quickly slice a large project class path into a consistent set of bundles.
Bnd is different because it takes a pull approach to assembling bundles: it doesn’t just archive everything it’s given.
Developers write instructions using OSGi-style headers, which can either be written as properties in the build or stored in separate property files.
The bnd tool uses these instructions to pick the classes and resources that should go into each bundle.
Because the tool knows each class and resource pulled into a bundle, it can make sure they form a consistent set and generate a valid OSGi manifest that represents the contents.
It can also glean information from other bundles on the class path, such as version information, and use that to automatically version imports.
Bnd headers follow the same pattern of comma-separated clauses defined in the OSGi specification, so you don’t have to learn yet another syntax to write instructions.
The bnd tool accepts standard OSGi headers such as Export-Package and Import-Package, as well as its own headers like Private-Package and Include-Resource that let you define additional bundle contents that are neither imported nor exported.
To make life easier, you can use wildcard patterns and negation in package headers; you don’t have to be explicit and list every single package in detail.
Bnd expands wildcards and normalizes versions, so the final bundle always has valid OSGi headers.
The following are some of the bnd headers you’ve used so far in this book.
EXPORT-PACKAGE This header is a comma-separated list of packages that should be contained and exported from this bundle.
By default, this is *, which pulls the complete class path into the bundle.
This is fine when you’re creating a mega bundle, but you’ll usually want to limit the packages pulled into the bundle and give them an explicit version:
If you want to export all packages except any internal or implementation packages, you can use a negated pattern.
But remember that the negated pattern must appear before the normal pattern, because the bnd tool processes patterns from left to right:
As discussed in chapter 5, it’s often a good idea to import your own exported API packages.
Older versions of bnd automatically added imports for all exports, whereas the latest versions try to make better guesses about which exported packages should be imported.
If you don’t want to import an exported package, you can add an attribute to each clause as follows:
IMPORT-PACKAGE This is a comma-separated list of packages that should be imported into this bundle.
By default, this is also *, which the bnd tool expands into any packages that are referenced from but not contained inside the bundle.
Ideally, you should use this header to tweak the results of the generated list, rather than explicitly list the packages you want imported.
The most important thing to remember is to leave the * at the end of the header; otherwise, you limit the ability of bnd to manage the set of imported packages for you:
PRIVATE-PACKAGE This is a comma-separated list of packages that should be included in this bundle but not exported.
This isn’t a standard OSGi header, so it won’t have any effect at execution time; it’s only used when assembling the bundle.
If a package matches both Export-Package and Private-Package, it will be exported.
INCLUDE-RESOURCE This is a comma-separated list of resource paths to include in this bundle.
This isn’t a standard OSGi header, so it won’t have any effect at execution time; it’s only used when assembling the bundle.
The simplest form of this header doesn’t accept any regular expressions and strips away directories when including resources.
The following example includes two resources, both at the root of the bundle:
To place resources in a subdirectory, you must use the assignment form.
All you need to do is put whatever location you want (followed by =) before the resource:
Embedding a JAR inside your bundle is as simple as naming it.
You don’t need to know its exact location because bnd automatically scans the project class path for it.
This feature is useful when you’re using bnd with repository-based build systems like Maven, where a dependency JAR can come from anywhere:
If you want to unpack the full JAR instead of embedding it, put an @ at the front:
And if you only want certain parts, you can select them using Ant-style path expressions:
The bnd tool goes a step further and also accepts inline short descriptions of.
You can even use bnd annotations to mark up component classes and setter methods; list the component class names in the header, and bnd will do the rest.
The official bnd site has a complete description of the Service-Component header as well as detailed examples.
If headers describe the what of bundles, then directives describe the how.
They let you fine-tune the packaging process so you get exactly the bundle you want.
Useful in multimodule projects when you want to provide some general export instructions, but you don’t want to affect the bundle contents.
Properties from included files override existing properties unless the name is prefixed with ~
A missing filename causes a build error unless the name is prefixed with -
This can be useful if you expect to deploy the bundle on Equinox or Eclipse, as the additional constraints can slow down the bundle-resolution process on certain frameworks.
Bnd includes optional plugins that can process Spring XML or generate bundles on demand in the style of Make.
Useful if you don’t want bnd-specific headers like Include-Resource ending up in your final bundle.
The source code must either exist in a directory or JAR on the class path, or be listed under the -sourcepath directive.
OSGi-aware IDEs like Eclipse automatically look for embedded sources and use them when debugging, so adding them can help developers.
The default policy is to only include the major and minor segments of the version, with no upper limit.
We’ll cover version policies in more detail in section A.1.5 and suggest some common policies.
The simplest macro is the property placeholder consisting of just a variable name:
All bnd macros start with a keyword, followed by one or more parameters separated by semicolons:
You can use this to select all public classes that implement a certain interface:
Or those that use certain annotations somewhere in the class:
This macro is useful for scanning component details at build time and recording the results in the manifest, so you can avoid having to repeatedly scan the bundle class path at execution time.
You can find the full query syntax on the bnd site.
You can provide an optional replacement string for the resource name, which can also contain the usual back references to groups in the matched pattern.
The following example macro evaluates to the names of all class resources, but with .java replacing .class:
As with findname, the replacement string can contain references to matched segments.
The mask contains one to four characters, each representing a segment of the version: major, minor, micro, and qualifier.
An = character means leave the segment unchanged, whereas + or - means increment or decrement the segment value.
As you’ll see in the next section, you can use this macro to automatically create ranges for any given version.
As we mentioned at the end of section A.1.3, the default policy for generated import versions is to keep the major and minor segments but drop the rest.
Using the resolution rules from chapter 2, this means the bundle won’t resolve against previous incompatible releases, but it will continue to resolve against any future release.
If the default version policy isn’t for you, you can define your own with the help of the version macro from the last section, with $(@) representing the detected import version.
The default policy gives you maximum flexibility in the future while stopping the bundle from accidentally resolving against older, incompatible releases.
To strengthen the lower bound further and only accept versions strictly older than the ones you built and tested against, use the entire version and don’t drop any segments:
What if you want to guard against future breaking changes and exclude future versions that aren’t binary compatible? You can do this by adding an upper bound to the range:
If you find all these similar brackets confusing, remember that bnd lets you mix bracket types, so you can always rewrite this last policy more clearly:
As you saw back in chapter 6, when you’re modularizing legacy applications, you may encounter the thorny issue of split packages, where two different JARs contain the same package but with different contents.
Bnd warns you when it detects split packages, but it still creates the bundle.
These warnings can be irritating if you already know about (and don’t mind) the split package.
Indeed, you may be creating this bundle in order to merge the different parts together.
To remove these warnings, add the following package attribute to any known split packages:
This tells bnd to silently merge the contents of the org.foo.bean package together but not overwrite existing entries if there’s any overlap.
You now know the various headers, directives, and macros available to us when building bundles with bnd in Ant, but what if you’re using another system, such as Maven? Do you have to learn yet another syntax, or is there a way to reuse your existing knowledge of bnd?
But what if you use Maven? Maven (http://maven.apache.org) is another popular build tool from Apache that also uses XML to describe builds, but this time the description is declarative rather than procedural.
Instead of listing the steps required to build a JAR (also known as an artifact), you list the packaging as jar in the project XML.
Maven favors convention over configuration; follow the Maven way, and you can keep your XML relatively lean and uncomplicated.
The problem is when you need to do something special outside of the normal Maven build process.
You can end up with pages of XML detailing each step of your customized build.
How well does OSGi fit with Maven? Will you need pages of configuration to assemble your bundle?
The Maven build process is extended by adding plugins to projects.
Maven plugins contain one or more Maven plain old Java objects (mojos), each of which represents a specific goal, such as assembling a JAR from classes and resources.
It adds a new packaging type called bundle that tells Maven how to package, install, and deploy OSGi bundles.
Add this plugin to your project XML (or parent project) and change the packaging to bundle, and it should create a bundle instead of a plain old JAR.
This creates a new Java project in the mybundle directory with example source and tests.
You should see Maven compile, test, package, and install a JAR called mybundle.jar.
If you look at the manifest inside the final JAR, it only contains a few entries about who built it:
Open the project file (pom.xml), and add the additional lines shown here:
Setting extensions to true tells Maven that this plugin contains a new packaging type, in this case bundle packaging.
To use this to build your project, you need to change the packaging listed in the project’s pom.xml from.
Notice how you didn’t need to add anything else to the project—the maven-bundleplugin uses existing information (project metadata, resources, and source code) to generate reasonable defaults for the OSGi manifest.
But what are these defaults, and how can you customize the bundle?
You may have noticed a couple of bnd headers in the last manifest.
In other words, the main task of the maven-bundleplugin is to translate Maven metadata into bnd instructions, so developers don’t have to repeat themselves over and over again.
Although these defaults are usually enough for most Maven projects, you’ll occasionally want to tweak or add additional instructions to fine-tune the bundling process.
Include-Resource All project resources in the current project (with property substitution)
Most of this customization is tweaking the Maven-produced manifest to match the one generated by the existing Ant build.
You can avoid this by extracting the common bnd instructions to a shared file and using the -include directive to pull it into both Ant and Maven builds.
Using a separate file for bnd instructions also avoids two formatting issues that can plague Maven bundle developers:
Other issues to watch out for when you start bundling Maven projects include the following:
The complete project class path is passed to bnd; so Export-Package: * will embed the entire class path, dependencies and all—which may or may not be what you want.
It doesn’t zip up target/classes; so, if any classes are missing or you see any unexpected additional classes, check your bnd instructions.
But watch out: this output includes debug from all plugins used in the build, so you probably want to save it somewhere so you can search for bundle details at your leisure.
Just as you did with Ant, you can use bnd instructions to tweak the manifest and select the bundle content.
It also keeps the Bundle-ClassPath header in sync, so any embedded JARs are automatically available on the bundle’s class path.
You won’t be surprised to learn that the main header is called Embed-Dependency.
It accepts a comma-separated list of patterns that are matched against the project’s Maven dependency tree.
Matching dependencies can either be embedded or selectively unpacked inside the bundle.
The full syntax of the Embed-Dependency header is as follows:
Pretty complicated, no? You may have noticed a resemblance between this syntax and the bnd tool syntax for selecting packages.
Now, you may be thinking that Embed-Dependency coupled with Embed-Transitive is a quick way to create a mega bundle containing everything you need for your application (see section 6.2.1)
Often this is true, but occasionally you end up pulling in a vast list of optional dependencies that aren’t needed at execution time.
Forget about downloading the internet—you can end up embedding it!
You should also be careful when mixing the Export-Package header (which pulls in classes and resources) with Embed-Dependency.
You can easily end up with duplicated content: one pulled in, the other embedded.
Instead, try to use the -exportcontents directive when you want to export packages contained in embedded dependencies.
You can use this file to select and deploy your project bundles onto OSGi frameworks:
You can even use it to remove stale bundle entries from your local OBR:
But how about non-local OBRs, like the one listing official Apache Felix bundle releases at http://felix.apache.org/obr/releases.xml? You can configure the mavenbundle-plugin to automatically update a remote OBR whenever it deploys a bundle to a remote Maven repository, but this isn’t enabled by default.
For more details about enabling and configuring remote OBR updates, see the plugin documentation on the Apache Felix website.
Next, add an plugin execution to generate the manifest as part of the build lifecycle:
Finally, you need to get Maven to include the generated manifest in the final artifact.
Exactly how this happens depends on the packaging you’re using, but most archives support some way to use an existing manifest file.
As you saw back in chapter 7, Eclipse includes support for developing bundles courtesy of its Plug-in Development Environment (PDE)
Whatever your workflow, you should be able to find something that works for you.
Let’s take a quick look at the alternatives currently available, starting with Eclipse PDE.
Unlike bnd, which generates a manifest based on a small recipe of instructions, Eclipse PDE provides dialog boxes and wizards for working directly with the manifest.
What you see is what will appear in the bundle.
Although this often leads to simpler manifests, it also means you have more responsibility for keeping the manifest up to date.
Sigil (http://felix.apache.org/site/apache-felix-sigil.html) is a tool that applies the OSGi modularity concepts to the build environment.
It extends build-time resolution technologies such as Maven and Ivy to resolve project dependencies primarily using the same Import-Package semantics encouraged by OSGi best practices.
This leads to a greater degree of decoupling and has been shown in real-world scenarios to reduce extraneous dependencies by up to a factor of 10 compared to module-level dependencies.
The Sigil project structure encourages delegation to avoid duplicated configuration and supports a flexible repository management framework.
Bndtools (http://njbartlett.name/bndtools.html) is an alternative to Eclipse PDE based on the bnd tool.
It provides dialog boxes and wizards for managing bnd instructions in Eclipse as well as menu options to run, test, and debug OSGi applications.
Developers already used to bnd should have no problem picking up bndtools; others will find its forms, syntax highlighting, and auto-completion a useful introduction to building bundles from recipes.
Osmorc (http://www.osmorc.org) is a plugin for IntelliJ IDEA that lets you choose whether to use an existing manifest for your bundle or have the IDE generate it.
Osmorc can generate a manifest based on either a simple form or a set of bnd instructions.
You can also use it to run and debug OSGi applications on all the major frameworks.
Netisgo (http://netbeans.org/features/java/osgi.html) is a plugin for NetBeans that forms a bridge between the NetBeans module system and OSGi.
It allows native modules and OSGi bundles to interoperate by mapping their metadata at runtime.
NetBeans also provides templates for simple Ant- or Maven-based OSGi builds.
Tycho (http://tycho.sonatype.org) is a collection of plugins specifically developed for building Eclipse plugins, applications, and update sites in Maven.
Its primary goal is to make the command line build match the Eclipse IDE build by replacing the standard Maven dependency resolution by a P2-based resolver.
Management of target platforms is also much easier with Tycho: add your required dependencies, and it will compute and download the appropriate target platform for you.
Bundlor (http://www.springsource.org/bundlor) is an alternative to bnd that works with Ant, Maven, and Eclipse.
Like bnd, it generates the bundle manifest from a recipe and uses instructions based on OSGi headers, but it uses a different syntax to control the results.
Bundlor also scans non-Java dependencies like Spring or Blueprint configuration files to find references that don’t appear in Java code but still need to be imported into the bundle.
This appendix lists the Core, Compendium, and Enterprise services defined in release 4.2 of the OSGi specification.
All services are optional: some are available from framework vendors, others are available from third-party vendors.
All of them are optional singleton services provided by the framework, except for the Service Hooks which may have zero or more instances registered by other bundles.
Provides the ability to control and reflect over bundle- and packagelevel wiring.
Start Level Controls the relative order of bundle startup by assigning start levels to bundles.
Multiplexes URL and content-handler factories to allow bundles to provide custom handlers.
Allows bundles to monitor and limit service registry events and access.
These services are provided by optional bundles installed on the base OSGi framework.
There can be zero or more versions of a service registered at the same time.
Preferences Provides storage and access to preferences and settings data.
Defines a packaging format and delivery approach to deploy an initial set of bundles.
Defines a packaging format and delivery approach for deploying bundlebased applications.
These services are provided by optional bundles installed on the base OSGi framework.
There can be zero or more versions of a service registered at the same time.
Defines an abstract application manager concept for managing arbitrary application types.
Defines how bundles can publish status variables and how administrative bundles can discover, read, and set status variables.
Enables foreign applications to participate in the OSGi service-oriented architecture.
Addresses how remote services can be provided inside an OSGi framework and discovered in a network.
OSGi lets you install, start, stop, update, or uninstall modules at execution time.
It’s the backbone of the Eclipse plugin system, as well as many Java EE containers, such as GlassFish, Geronimo, and WebSphere.
OSGi in Action provides a clear introduction to OSGi concepts with examples that are relevant both for architects and developers.
You’ll start with the central ideas of OSGi: bundles, module lifecycles, and interaction among application components.
With the core concepts well in hand, you’ll explore numerous application scenarios and techniques.
You’ll learn how to migrate legacy systems to OSGi and how to test, debug, and manage applications.
Th is book assumes readers with a working knowledge of Java, but requires no previous exposure to OSGi.
OSGi in Action brief contents contents foreword preface acknowledgments about this book Roadmap Code Author Online About the title About the cover illustration.
