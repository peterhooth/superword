Java Development with Ant is essential for anyone serious about actually shipping Java applications.
Erik and Steve give you the answers to questions you didn’t even know you have.
The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
To my wife, Bina, and our little deployment project, Alexander.
You’ve both been very tolerant of the time I’ve spent.
Gosh, is it time for a new edition already? That’s one of the odd aspects of writing about open source projects: the rapid release cycles and open development process mean that things date fast—and visibly.
In a closed source project, changes are invisible until the next release ships; in open source, there’s a gradual divergence between the code at the head of the repository and that covered in a book.
Both the build tool and the book were very successful.
Ant became the main way people built and tested Java projects, and our book showed how to use Ant in big projects and how to solve specific problems.
Ant 1.6 came along, and people started asking how some of the scalability improvements changed the build, and we would say “it makes it easier” without having any specifics to point to.
At the same time, other interesting technologies came along to help, such as Ivy for dependency management, and other tools for deployment and testing.
Java development processes had improved—and it was time to document the changes.
Erik, having just finished Lucene in Action, took a break from the Ant book series, leaving me the sole author of the second edition.
I was blessed with a good start: all the text from the first edition.
This text was a starting place for what turned out to be a major rewrite.
Along with the changes to Ant, I had to deal with the changes in Enterprise Java, in XML schema languages, as well as in deployment and testing tools and methodologies.
This made for some hard choices: whether to stay with JUnit and Java EE or whether to switch to Spring, OSGi, and TestNG as the way to package, deliver, and test applications.
I chose to stay with the conventional ecosystem, because people working in Java EE need as much help as they can get, and because the tooling around JUnit 3 is excellent.
If and when we do a third edition, things may well change yet again.
This book is now completely updated to show how to build, test, and deploy modern Java applications using Ant 1.7
Chapter 16, deployment, is a favorite of mine, because deployment is where I’m doing my research.
If your application is designed right, you could even roll out the application to a grid of 500 servers hosting the application on their spare CPU cycles!
That’s why building and testing Java applications is so exciting.
It may seem like housekeeping, something that an IDE can handle for you, but the projects that are the most interesting and fun, are the ones where you attempt to do things that nobody has done before.
If you are going to be innovative, if you want to be leading edge, you will need tools that deliver both power and flexibility.
Ant does both and is perfect for developing big Java applications.
I’ve enjoyed writing this book, and hope you will enjoy reading it!
Ant started its life on a plane ride, as a quick little hack.
It joined Apache as a minor adjunct—almost an afterthought, really—to the codebase contributed by Sun that later became the foundation of the Tomcat 3.0 series.
The reason it was invented was simple: it was needed to build Tomcat.
Despite these rather inauspicious beginnings, Ant found a good home in Apache, and in a few short years it has become the de facto standard not only for open source Java projects, but also as part of a large number of commercial products.
In my mind four factors are key to Ant’s success: its extensible architecture, performance, community, and backward compatibility.
The first two—extensibility and performance—derive directly from James’s original efforts.
The dynamic XML binding approach described in this book was controversial at the time, but as Stefano Mazzocchi later said, it has proven to be a “viral design pattern”: Ant’s XML binding made it very simple to define new tasks and, therefore, many tasks were written.
I played a minor role in this as I (along with Costin Manolache) introduced the notion of nested elements discussed in section 17.6
As each task ran in the same JVM and allowed batch requests, tasks that often took several minutes using Make could complete in seconds using Ant.
Ant’s biggest strength is its active development community, originally fostered by Stefano and myself.
Stefano acted as a Johnny Appleseed, creating build.xml files for numerous Apache projects.
Many projects, both Apache and non-Apache, base their Ant build definitions on this early work.
My own focus was on applying fixes from any source I could find, and recruiting new developers.
Nearly three dozen developers have become Ant “committers,” with just over a dozen being active at any point in time.
Much of the early work was experimental, and the rate of change initially affected the user community.
Efforts like Gump sprang up to track the changes and have resulted in a project that now has quite stable interfaces.
The combination of these four factors has made Ant the success that it is today.
Most people have learned Ant by reading build definitions that had evolved over time.
You have the opportunity to learn Ant from two of the people who know it best and who teach it the way it should be taught—by starting with a simple build definition and then showing you how to add in just those functions that are required by your project.
And if you find things that you feel need improving, then I encourage you to join Erik, Steve, and the rest of us and get involved!
In early 2000, Steve took a sabbatical from HP Laboratories, taking a break from research into such areas as adaptive, context-aware laptops to build web services, a concept that was very much in its infancy at the time.
He soon discovered that he had entered a world of chaos.
Business plans, organizations, underlying technologies—all could be changed at a moment’s notice.
One technology that remained consistent from that year was Ant.
In the Spring of 2000, it was being whispered that a “makefile killer” was being quietly built under the auspices of the Apache project: a new way to build Java code.
Ant was already in use outside the Apache Tomcat group, its users finding that what was being whispered was true: it was a new way to develop with Java.
Steve started exploring how to use it in web service projects, starting small and slowly expanding as his experience grew and as the tool itself added more functionality.
Nothing he wrote that year ever got past the prototype stage; probably the sole successful deliverable of that period was the “Ant in Anger” paper included with Ant distributions.
In 2001, Steve and his colleagues did finally go into production.
Their projectto aggressive deadlines—was to build an image-processing web service using both Java and VB/ASP.
From the outset, all the lessons of the previous year were applied, not just in architecture and implementation of the service, but in how to use Ant to manage the build process.
As the project continued, the problems expanded to cover deployment to remote servers, load testing, and many other challenges related to realizing the web service concept.
It turned out that with planning and effort, Ant could rise to the challenges.
Meanwhile, Erik was working at eBlox, a Tucson, Arizona, consulting company specializing in promotional item industry e-business.
By early 2001, Erik had come to Ant to get control over a build process that involved a set of Perl scripts crafted by the sysadmin wizard.
Erik was looking for a way that did not require sysadmin effort xxiii.
Ant solved this problem very well, and in the area of building customized releases for each of eBlox’s clients from a common codebase.
One of the first documents Erik encountered on Ant was the infamous “Ant in Anger” paper written by Steve; this document was used as the guideline for crafting a new build process using Ant at eBlox.
While working on JUnit and Ant integration, Erik dug under the covers of Ant to see what made it tick.
To get JUnit reports emailed automatically from an Ant build, Erik pulled together pieces of a MIME mail task submitted to the antdev team.
After many dumb-question emails to the Ant developers asking such things as “How do I build Ant myself?” and with the help of Steve and other Ant developers, his first contributions to Ant were accepted and shipped with the Ant 1.4 release.
In the middle of 2001, Erik proposed the addition of an Ant Forum and FAQ to jGuru, an elegant and top-quality Java-related search engine.
From this point, Erik’s Ant knowledge accelerated rapidly, primarily as a consequence of having to field tough Ant questions.
Soon after that, Erik watched his peers at eBlox develop the well-received Java Tools for Extreme Programming book.
Erik began tossing around the idea of penning his own book on Ant, when Dan Barthel, formerly of Manning, contacted him.
Erik announced his book idea to the Ant community email lists and received very positive feedback, including from Steve who had been contacted about writing a book for Manning.
They discussed it, and decided that neither of them could reasonably do it alone and would instead tackle it together.
Not to make matters any easier on himself, Erik accepted a new job, and relocated his family across the country while putting together the book proposal.
The new job gave Erik more opportunities to explore how to use Ant in advanced J2EE projects, learning lessons in how to use Ant with Struts and EJB that readers of this book can pick up without enduring the same experience.
In December of 2001, after having already written a third of this book, Erik was honored to be voted in as an Ant committer, a position of great responsibility, as changes made to Ant affect the majority of Java developers around the world.
Steve, meanwhile, already an Ant committer, was getting more widely known as a web service developer, publishing papers and giving talks on the subject, while exploring how to embed web services into devices and use them in a LAN-wide, campuswide, or Internet-wide environment.
His beliefs that deployment and integration are some of the key issues with the web service development process, and that Ant can help address them, are prevalent in his professional work and in the chapters of this book that touch on such areas.
Steve is now also a committer on Axis, the Apache project’s leading-edge SOAP implementation, so we can expect to see better integration between Axis and Ant in the future.
Together, in their “copious free time,” Erik and Steve coauthored this book on how to use Ant in Java software projects.
They combined their past experience with research into side areas, worked with Ant 1.5 as it took shape—and indeed helped shape this version of Ant while considering it for this book.
They hope that you will find Ant 1.5 to be useful—and that Java Development with Ant will provide the solution to your build, test, and deployment problems, whatever they may be.
Writing a book about software is similar to a software project.
There’s much more emphasis on documentation, but it’s still essential to have an application that works.
Writing a second edition of a book is a form of software maintenance.
And how the world has changed! Since the last edition, what people write has evolved: weblogs, REST services, XMPP-based communications, and other technologies are now on the feature lists of many projects, while deadlines remain as optimistic as ever.
The Java building, testing, and deployment ecosystem has evolved to match.
I’ve had to go back over every page in the first edition and rework it to deal with these changes, which took quite a lot of effort.
The result, however, is a book that should remain current for the next three-to-five years.
There are also the reviewers and the members of the Manning Early Access Program, who found and filed bug reports against early drafts of the book.
The Ant team deserves to be thanked for the ongoing evolution of Ant, especially when adding features and bug fixes in line with the book’s needs.
Discussions on Ant’s developer and user mailing lists also provided lots of insight—all participants on both mailing lists deserve gratitude.
Alongside Ant come other tools and products, those covered in the book and those used to create it.
There’s a lot of really good software out there, from operating systems to IDEs and networking tools: Linux and the CVS and Subversion tools deserve special mention.
I’d also like to thank my HP colleagues working on SmartFrog for their tolerance xxvi  ACKNOWLEDGMENTS.
The best way to test some aspects of big-project Ant is on a big project, and yours was the one I had at hand.
This book should provide the documentation of what the build is currently doing.
Finally, I’d like to thank my friends and family for their support.
Writing a book in your spare time is pretty time-consuming.
Now that it is finished, I get to rest and spend time with my wife, my son, our friends, and my mountain bike, while the readers get to enjoy their own development projects, with their own deadlines.
This book is about Ant, the award-winning Java build tool.
From its beginnings as a helper application to compile Tomcat, Apache’s Java web server, it has grown to be a stand-alone tool adopted across the Java community, and in doing so has changed people’s expectations of their development tools.
If you have never before used Ant, this book will introduce you to it, taking you systematically through the core stages of most Java projects: compilation, testing, execution, packaging, and delivery.
If you’re an experienced Ant user, we’ll show you how to “push the envelope” in using Ant.
We place an emphasis on how to use Ant as part of a large project, drawing out best practices from our own experiences.
Whatever your experience with Ant, we believe that you will learn a lot from this book and that your software projects will benefit from using Ant as the way to build, test, and release your application.
We assume no prior experience of Ant, although even experienced Ant users should find much to interest them in the later chapters.
We do expect our readers to have basic knowledge of Java, although the novice Java developer will benefit from learning Ant in conjunction with Java.
Some of the more advanced Ant projects, such as building Enterprise Java applications and web services, are going to be of interest primarily to the people working in those areas.
We’ll introduce these technology areas, but we’ll defer to other books to cover them fully.
Part 1 introduces the fundamentals of Ant and shows how to use it to build, test, package, and deliver a Java library.
Part 3 is a short but detailed guide on how to extend Ant in scripting languages and Java code, enabling power users to adapt Ant to their specific needs, or even embed it in their own programs.
Chapter 2 digs into Ant’s syntax and mechanics, starting with a simple project to compile a single Java file and evolving it into an Ant build process, which compiles, packages, and executes a Java application.
To go further with Ant beyond the basic project shown in chapter 2, Ant’s abstraction mechanisms need defining.
Chapter 3 introduces Ant’s properties and datatypes, which let build-file writers share data across parts of the build.
This is a key chapter for understanding what makes Ant shine.
Ant and test-centric development go hand in hand, so chapter 4 introduces our showcase application alongside JUnit, the tool that tests the application itself.
From this chapter onwards, expect to see testing a recurrent theme of the book.
Chapter 7 takes what we’ve packaged and distributes it by email and FTP and SCP uploads.
It’s often difficult to envision the full picture when looking at fragments of code in a book.
In chapter 8, we show a single build file that merges all the stages of the previous chapters.
Chapter 8 also discusses the issues involved in migrating to Ant and adopting a sensible directory structure, along with other general topics related to managing a project with Ant.
Part 2 The second part of the book extends the core build process in different ways, solving problems that different projects may encounter.
Chapter 9 starts by showing how to extend Ant with optional and third-party tasks to perform new activities, such as checking out files from revision control, auditing code, and adding iteration and error-handling to a build file.
Chapter 10 looks at big-project Ant—how to build a big project from multiple subsidiary projects.
This chapter is complemented by Chapter 11, which uses the Ivy libraries to address the problem of library management.
Having a tool to manage your library dependencies and to glue together the output of different projects keeps Java projects under control, especially large ones.
Web development is where many Java developers spend their time these days.
Chapter 12 shows how to package, deploy, and then test a web application.
You can test a web application only after deploying it, so the development process gets a bit convoluted.
Chapter 13 discusses a topic that touches almost all Java developers: XML.
Whether you’re using XML simply for deployment descriptors or for transforming documentation files into presentation format during a build process, this chapter covers it.
Chapter 14 is for developers working with Enterprise Java; it looks at how to make an application persistent, how to deploy it on the JBoss application server, and how to test it with Apache Cactus.
The final two chapters of Part 2 look at how to improve your development processes.
Chapter 15 introduces continuous integration, the concept of having a server automatically building and testing an application whenever code is checked in.
This is a topic that many developers neglect for one reason or another, but it typically ends up coming back to haunt us.
Automating thiswhich is possible—finishes the transformation of how a Java project is built, tested, and deployed.
Part 3 The final part of our book is about extending Ant beyond its built-in capabilities.
Ant is designed to be extensible in a number of ways.
Chapter 17 provides all the information needed to write sophisticated custom Ant tasks, with many examples.
Beyond custom tasks, Ant is extensible by scripting languages, and it supports many other extension points, including Resources, Conditions, FilterReaders, and Selectors.
At the back Last but not least are three appendices.
Appendix A is for new Ant users; it explains how to install Ant and covers common installation problems and solutions.
Because Ant uses XML files to describe build processes, appendix B is an introduction to XML for those unfamiliar with it.
All modern Java integrated development environments (IDEs) now tie in to Ant.
Using an Ant-enabled IDE allows you to have the best of both worlds.
Appendix C details the integration available in several of the popular IDEs.
What we do not have in this edition is a quick reference to the Ant tasks.
When you install Ant, you get an up-to-date copy of the documentation, which includes a reference of all Ant’s tasks and types.
Bookmark this documentation in your browser, as it is invaluable.
You’ll find links to the source and the author forum plus some extra content that isn’t in the book, including a couple of chapters from the previous edition and a bibliography with links.
This antbook.org web site links to all the source code and Ant build files in the xxx  ABOUT THIS BOOK.
They are hosted on the SourceForge open source repository at http://sourceforge.net/projects/antbook.
Ant and its online documentation can be found here, while the Ant user and developer mailing lists will let you meet other users and ask for help.
Bold Courier typeface is used in some code listings to highlight important or changed sections.
This page provides information on how to get on the forum once you are registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialog between individual readers and between readers and the authors can take place.
It is not a commitment to any specific amount of participation on the part of the authors, whose contribution to the AO remains voluntary (and unpaid)
We suggest you try asking the authors some challenging questions, lest their interest stray!
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s web site as long as the book is in print.
His involvement in Ant started in 2000, when he was working on early web services in Corvallis, Oregon; he is a long-standing committer on Ant projects and a member of the Apache Software Foundation.
He holds a degree in Computer Science from Edinburgh University, and lives in Bristol with his wife Bina and son Alexander.
In the absence of any local skiing, he makes the most of the offroad and on-road cycling in the area.
He currently works for the Applied Research in Patacriticism group at the University of Virginia, and consults on Lucene and Solr through eHatcher Solutions, Inc.
Thanks to the success of the first edition, Erik has been honored to speak at conferences and to groups around the world, including JavaOne, ApacheCon, OSCON, and the No Fluff, Just Stuff symposium circuit.
Erik congratulates Steve, his ghost writer, for single-handedly tackling this second edition.
The colorful diversity of the illustrations in the collection speaks vividly of the uniqueness and individuality of the world’s towns and regions just 200 years ago.
This was a time when the dress codes of two regions separated by a few dozen miles identified people uniquely as belonging to one or the other.
The collection brings to life a sense of isolation and distance of that period—and of every other historic period except our own hyperkinetic present.
Dress codes have changed since then and the diversity by region, so rich at the time, has faded away.
It is now often hard to tell the inhabitant of one continent from another.
Perhaps, trying to view it optimistically, we have traded a cultural and visual diversity for a more varied personal life.
Or a more varied and interesting intellectual and technical life.
We at Manning celebrate the inventiveness, the initiative, and the fun of the computer business with book covers based on the rich diversity of regional life of two centuries ago‚ brought back to life by the pictures from this collection.
Steve returned to HP Laboratories, in the UK, getting into the problem of grid-scale deployment.
In the meantime, Ant 1.6 shipped, not breaking anything in the first edition, but looking slightly clunky.
There were easier ways to do some of the things we described, especially in the area of big projects.
We finally sat down and began an update while Ant 1.7 was under development.
Starting the update brought it home to us how much had changed while we weren’t paying attention.
Nearly every popular task has had some tweak to it, from a bit of minor tuning to something more fundamental.
Along with Ant’s evolution, many of the technologies that we covered evolved while we weren’t looking—even the Java language itself has changed.
We had to carefully choose which technologies to cover with this book.
We’ve put the effort into coverage of state-of-the-art build techniques, including library management, continuous integration, and automated deployment.
Without the wonderful response to the first edition, we would never have written it.
And we can say that without the wonderful tools at our disposal—Ant, JUnit, IntelliJ IDEA, jEdit, and Eclipse—we wouldn’t have been able to write it so well.
If you’re one of those people, remind us of this fact if you.
It will store appointments and allow all events on a given day/range to be retrieved.
It will not be very useful, but we can use it to explore many features of a real application and the build process to go with it: persistence, server-side operation, RSS feeds, and whatever else we see fit.
We’re writing this Extreme Programming-style, adding features on demand and writing the tests as we do so.
We’re also going to code in an order that matches the book’s chapters.
That’s the nice thing about XP: you can put off features until you need them, or, more importantly, until you know exactly what you need.
All the examples in the book are hosted on SourceForge in the project antbook and are available for download from http://antbook.org/
Everything is Apache licensed; do with it what you want.
What’s changed since the first edition? The first edition of this book, Java Development with Ant, was written against the version of Ant then in development, Ant 1.5
This version, Ant in Action, was written against Ant 1.7
If you have an older version, upgrade now, as the build files in this book are valid only in Ant 1.7 or later.
The spawn attribute of the <java> task lets you start a process that will outlive the Ant run, letting you use Ant as a launcher of applications.
If you’ve been using Ant already, all your existing build files should still work.
Ant is developed by a rigorous process and a wide beta test program.
That’s one of the virtues of a software build tool as an open source project: it’s well engineered by its end users, and it’s tested in the field long before a product ships.
Learning Ant Welcome to Ant in Action, an in-depth guide to the ubiquitous Java build tool.
In this book, we’re going to explore the tool thoroughly, using it to build everything from a simple little Java library to a complete server-side application.
Ant’s reusable datatypes and properties play an important role in writing maintainable and extensible build files.
After reading this section, you’ll be ready to use Ant in your own projects.
Ant is written in Java and is designed to be cross-platform, easy to use, extensible, and scalable.
It can be used in a small personal Welcome to the future of your build process.
It’s more than just a reference book for Ant syntax, it’
The origin of Ant is a fascinating story; it’s an example of where a spin-off from a project can be more successful than the main project.
Ant was written by James Duncan Davidson, then a Sun employee, to make it easier for people to compile.
The tool he wrote did that, and, with help from other.
Soon it spread to other open source projects, and trickled out into helping Java developers in general.
In that year and for the following couple of years, using Ant was still somewhat unusual.
Nowadays, it’s pretty much expected that any Java project you’ll encounter will have an Ant build file at its base, along with the project’s code and—hopefully—its tests.
All Java IDEs come with Ant support, and it has been so successful that there are versions for the .NET framework (NAnt) and for PHP (Phing)
Perhaps the greatest measure of Ant’s success is the following: a core feature of Microsoft’s .NET 2.0 development toolchain is its implementation of a verson: MSBuild.
That an XML-based build tool, built in their spare time by a few developers, is deemed worthy of having a “strategic” competitor in the .NET framework is truly a measure of Ant’s success.
In the Java world, it’s the primary build tool for large and multiperson projectsthings bigger than a single person can do under an IDE.
Why? Well, we’ll get to that in section 1.2—the main thing is that it’s written in Java and focuses on building and testing Java projects.
Ant has an XML syntax, which is good for developers already familiar with XML.
For developers unfamiliar with XML, well, it’s one place to learn the language.
These days, all Java developers need to be familiar with XML.
In a software project experiencing constant change, an automated build can provide a foundation of stability.
Even as requirements change and developers struggle to catch up, having a build process that needs little maintenance and remembers to test everything can take a lot of housekeeping off developers’ shoulders.
Ant can be the means of controlling the building and deployment of Java software projects that would otherwise overwhelm a team.
We have just told you why Ant is great, but now we are going to show you what makes it great: its ingredients, the core concepts.
The first is the design goal: Ant was designed to be an extensible tool to automate the build process of a Java development project.
A software build process is a means of going from your source—code and documents—to the product you actually deliver.
If you have a software project, you have a build process, whether or not you know it.
It may just be “hit the compile button on the IDE,” or it may be “drag and drop some files by hand.” Neither of these are very good because they aren’t automated and they’re often limited in scope.
With Ant, you can delegate the work to the machine and add new stages to your build process.
Or the creation of XML configuration files from your Java source.
Once you have an automated build, you can let anyone build the system.
Then you can find a spare computer and give it the job of rebuilding the project continuously.
This is why automation is so powerful: it starts to give you control of your project.
Ant is Java-based and tries to hide all the platform details it can.
This makes it easy to extend Ant through Java code, using all the functionality of the Java platform and third-party libraries.
It also makes the build very fast, as you can run Java programs from inside the same Java virtual machine as Ant itself.
Putting Ant extensions aside until much later, here are the core concepts of Ant as seen by a user of the tool.
Build Files Ant uses XML files called build files to describe how to build a project.
In the build file developers list the high-level various goals of the build—the targets—and actions to take to achieve each goal—the tasks.
A build file contains one project Each build file describes how to build one project.
Very large projects may be composed of multiple smaller projects, each with its own build file.
A higher-level build file can coordinate the builds of the subprojects.
Each project contains multiple targets Within the build file’s single project, you declare different targets.
These targets may represent actual outputs of the build, such as a redistributable file, or activities, such as compiling the source or running the tests.
Targets can depend on other targets When declaring a target, you can declare which targets have to be built first.
This can ensure that the source gets compiled before the tests are run and built, and that the application is not uploaded until the tests have passed.
When Ant builds a project, it executes targets in the order implied by their dependencies.
Targets contain tasks Inside targets, you declare what work is needed to complete that stage of the build process.
You do this by listing the tasks that constitute each stage.
When Ant executes a target, it executes the tasks inside, one after the other.
Tasks do the work Ant tasks are XML elements, elements that the Ant runtime turns into actions.
Behind each task is a Java class that performs the work described by the task’s attributes and nested data.
These tasks are expected to be smart—to handle much of their own argument validation, dependency checking, and error reporting.
The fact that it’s easy to extend Ant with new classes is one of its core strengths.
Often, someone will have encountered the same build step that you have and will have written the task to perform it, so you can just use their work.
If not, you can extend it in Java, producing another reusable Ant task or datatype.
To summarize, Ant reads in a build file containing a project.
In the project are targets that describe different things the project can do.
Inside the targets are the tasks, tasks that do the individual steps of the build.
Ant executes targets in the order implied by their declared dependencies, and the tasks inside them, thereby building the application.
Figure 1.1 shows the Ant build file as a graph of targets, each target containing tasks.
When the project is built, Ant determines which targets need to be executed, and in what order.
This lets simple rules such as “deploy after compiling” be described, as well as more complex ones such as “deploy only after the unit tests have succeeded.”
Inside each target are task declarations, which are statements of the actions Ant must take to build that target.
Targets can state their dependencies on other targets, producing a graph of dependencies.
When executing a target, all its dependents must execute first.
Listing 1.1 shows the build file for this typical build process.
Listing 1.1 A typical scenario: compile, document, package, and deploy.
Upload all files in the dist directory to the ftp server.
Why did we invoke Ant with -propertyfile ftp.properties? We have a file called ftp.properties containing the three properties server.name, ftp.
The property handling mechanism allows parameterization and reusability of our build file.
This particular example, while certainly demonstrative, is minimal and gives only a hint of things to follow.
In this build, we tell Ant to place the generated documentation alongside the compiled classes, which isn’t a typical distribution layout but allows this example to be abbreviated.
Using the -propertyfile command-line option is also atypical and is used in situations where forced override control is desired, such as forcing a build to upload to a different server.
This example shows Ant’s basics well: target dependencies, use of properties, compiling, documenting, packaging, and, finally, distribution.
Because Ant tasks are Java classes, the overhead of invoking each task is quite small.
For each task, Ant creates a Java object, configures it, then calls its execute() method.
A simple task such as <mkdir> would call a Java library method to create a directory.
A more complex task such as <ftp> would invoke a third-party FTP library to talk to the remote server, and, optionally, perform dependency checking to upload only files that were newer than those at the destination.
A very complex task such as <javac> not only uses dependency checking to decide which files to compile, it supports multiple compiler back ends, calling Sun’s Java compiler in the same Java Virtual Machine (JVM), or executing a different compiler as an external executable.
Ant decides which compiler to use and what its command line is are issues that you rarely need to worry about.
Specify the build file correctly, and Ant will work out target dependencies and call the targets in the right order.
The targets run through their tasks in order, and the tasks themselves deal with file dependencies and the actual execution of the appropriate Java package calls or external commands needed to perform the work.
Because each task is usually declared at a high level, one or two lines of XML is often enough to describe what you want a task to do.
Ten lines might be needed for something as complex as creating a database table.
With only a few lines needed per task, you can keep each build target small, and keep the build file itself under control.
That is why Ant is popular, but that’s not the only reason.
Why is that? What are its unique attributes that helped it grow from a utility in a single project to the primary build system of Java projects?
Ant is free and Open Source Ant costs nothing to download.
It comes with online documentation that covers each task in detail, and has a great online community on the Ant developer and user mail lists.
If any part of Ant doesn’t work for you, you can fix it.
All the Ant developers got into the project by fixing bugs that mattered to them or adding features that they needed.
The result is an active project where the end users are the developers.
Ant makes it easy to bring developers into a project One of the benefits of using Ant comes when a new developer joins a team.
With a nicely crafted build process, the new developer can be shown how to get code from the source code repository, including the build file and library dependencies.
Even Ant itself could be stored in the repository for a truly repeatable build process.
It is well-known and widely supported Ant is the primary build tool for Java projects.
Lots of people know how to use it, and there is a broad ecosystem of tools around it.
It integrates testing into the build processes The biggest change in software development in the last few years has been the adoption of test-centric processes.
The agile processes, including Extreme Programming and Test-Driven Development, make writing tests as important as writing the.
These test-first processes say that developers should write the tests.
Ant doesn’t dictate how you write your software—that’s your choice.
An Ant build file can mandate that the unit tests must all pass before the web application is deployed, and that after deploying it, the functional tests must be run.
If the tests fail, Ant can produce a nice HTML report that highlights the problems.
Adopting a test-centric development process is probably the most important and profound change a software project can make.
It enables continuous integration With tests and an automated build that runs those tests, it becomes possible to have a machine rebuild and retest the application on a regular basis.
How regularly? Nightly? How about every time someone checks something into the code repository?
This is what continuous integration tools can do: they can monitor the repository and rerun the build when something changes.
If the build and tests work, they update a status page on their web site.
If something fails, developers get email notifying them of the problem.
This catches errors within minutes of the code being checked in, stopping bugs from hiding unnoticed in the source.
It runs inside Integrated Development Environments Integrated Development Environments (IDEs) are great for editing, compiling, and debugging code, and they’re easy to use.
It’s hard to convince users of a good IDE that they should abandon it for a build process based on a text file and a command line prompt.
Ant integrates with all mainstream IDEs, so users do not need to abandon their existing development tools to use Ant.
Ant doesn’t replace an IDE; a good editor with debugging and even refactoring facilities is an invaluable tool to have and use.
Ant just takes control of compilation, packaging, testing, and deployment stages of the build process in a way that’s portable, scalable, and often reusable.
This means that developers can choose whatever IDE they like, and yet everyone can share the same automated build process.
When do you need Ant? When is an automated build tool important? The approximate answer is “whenever you have any project that needs to compile or test Java code.” At the start of the project, if only one person is coding, then an IDE is a good starting point.
As soon as more people work on the code, its deliverables get more complex, or the test suite starts to be written, then its time to turn to Ant.
This is also a great time to set up the continuous integration server, or to add the project to a running one.
While Ant was never designed with this reuse in mind, it can be used this way.
Although Ant is a great build tool, there are some places where it isn’t appropriate.
Ant is not the right tool to use outside of the build process.
Its command line and error messages are targeted at developers who understand English and Java programming.
You should not use Ant as the only way end-users can launch an application.
Some people do this: they provide a build file to set up the classpath and run a Java program, or they use Ant to glue a series of programs together.
This works until there’s a problem and Ant halts with an error message that only makes sense to a developer.
Nor is Ant a general-purpose workflow engine; it lacks the persistence or failure handling that such a system needs.
Its sole options for handling failure are “halt” or “ignore,” and while it may be able to run for days at a time, this is something that’s never tested.
The fact that people do try to use Ant for workflow shows that there’s demand for a portable, extensible, XML-based workflow engine.
Ant is not that; Ant is a tool for making development easier, not solving every problem you can imagine.
If you’re just starting out writing some code, it’s easier to stay in the IDEs, using the IDE to set up your classpath, to build, and to run tests.
You can certainly start off a project that way, but as soon as you want HTML test reports, packaging, and distribution, you’ll need Ant.
It’s good to start work on the build process early, rather than try to live in the IDE forever.
How does it fare in comparison to its competition and predecessors? We’ll compare Ant to its most widely used comptetitorsIDEs Make, and Maven.
IDEs are the main way people code: Eclipse, NetBeans, and IntelliJ IDEA are all great for Java development.
Their limitations become apparent as a project proceeds and grows.
It’s very hard to add complex operations, such as XSL stylesheet operations, Java source generation from IDL/WSDL processing, and other advanced tricks.
It can be near-impossible to transfer one person’s IDE settings to another user.
IDE-based build processes rarely scale to integrate many different subprojects with complex dependencies.
Producing replicable builds is an important part of most projects, and it’s risky to use manual IDE builds to do so.
All modern IDEs have Ant support, and the IDE teams all help test Ant under their.
One IDE, NetBeans, uses Ant as its sole way of building projects, eliminating any difference between the IDE and Ant.
The others integrate Ant within their own build process, so you can call Ant builds at the press of button.
The Unix Make tool is the original build tool; it’s the underpinnings of Unix and Linux.
In Make, you list targets, their dependencies, and the actions to bring each target up-to-date.
Each target in a makefile is either the name of a file to bring up-to-date or what, in Make terminology, is called a phony target.
Phony targets have names like clean or all and can have no dependencies (that is, they always execute their commands) or can be dependent upon real targets.
One of the best parts of Make is that it supports pattern rules to determine how to build targets from the available inputs, so that it can infer that to create a .class file, you compile a .java file of the same name.
All the actions that Make invokes are actually external programs, so the rule to go from .java files to .class files would invoke the javac program to compile the source, which doesn’t know or care that it has been invoked by Make.
Here’s an example of a very simple GNU makefile to compile two Java classes and archive them into a JAR file:
The makefile has a phony target, all, which, by virtue of being first in the file, is the default target.
It depends upon project.jar, which depends on two compiled Java files, packaging them with the JAR program.
The final rule states how to build class (.class) files from Java (.java) files.
In Make, you list the file dependencies, and the tool determines which rules to apply and in what sequence, while the developer is left tracking down bugs related to the need for invisible tab characters rather than spaces at the start of each action.
When someone says that they use Make, it usually means they use Make-on-Unix, or Make-on-Windows.
It’s very hard to build across both, and doing so usually requires a set of Unix-compatible applications, such as the Cygwin suite.
Because Make handles the dependencies, it’s limited to that which can be declared in the file: either timestamped local files or phony targets.
This adds work for task authors, but benefits task users.
Ant versus Make Ant and Make have the same role: they automate a build process by taking a specification file and using that and source files to create the desired artifacts.
However, Ant and Make do have some fundamentally different views of how the build process should work.
With Ant, you list sequences of operations and the dependencies between them, and you let file dependencies sort themselves out through the tasks.
The only targets that Ant supports are similar to Make’s phony targets: targets that are not files and exist only in the build file.
You omit file dependencies, along with any file conversion rules.
Instead, the Ant build file states the stages used in the process.
While you may name the input or output files, often you can use a wildcard or even a default wildcard to specify the source files.
For example, here the <javac> task automatically includes all Java files in all subdirectories below the source directory:
Ant will call each task in turn, and the tasks can choose whether or not to do work.
The advantage of this approach is that the tasks can contain more domain-specific knowledge than the build tool, such as performing directory hierarchy-aware dependency checking, or even addressing dependency issues across a network.
The other subtlety of using wildcards to describe source files, JAR files on the classpath, and the like is that you can add new files without having to edit the build file.
This is nice when projects start to grow because it keeps build file maintenance to a minimum.
Ant works best with programs that are wrapped by Java code into a task.
The task implements the dependency logic, configures the application, executes the program, and interprets the results.
Ant does let you execute native and Java programs directly, but adding the dependency logic is harder than it is for Make.
Also, with its Java focus, there’s still a lot to be said for using Make for C and C++ development, at least on Linux systems, where the GNU implementation is very good, and where the development tools are installed on most end users’ systems.
Maven is a competing build tool from Apache, hosted at http://maven.apache.org.
The standard archetype is for a Java library, but others exist and more can be written.
Like Ant, Maven uses an XML file to describe the project.
Ant’s file explicitly lists the stages needed for each step of the build process, but neglects other aspects of a project such as its dependencies, where the source code is kept under revision control, and other things.
Maven’s Project Object Model (POM) file declares all this information, information that Maven plugins use to manage all parts of the build process, from retrieving dependent libraries to running tests and generating reports.
Central to Maven is the idea that the tools should encode a set of best practices as to how projects should be laid out and how they should test and release code.
Ant, in comparison, has no formal knowledge of best practices; Ant leaves that to the developers to decide on so they can implement their own policy.
Ant versus Maven There is some rivalry between the two Apache projects, though it is mostly goodnatured.
The developer teams are friends, sharing infrastructure bits and, sometimes, even code.
Ant views itself as the more flexible of the two tools, while Maven considers itself the more advanced of the pair.
There are some appealing aspects to Maven, which can generate a JAR and a web page with test results from only a minimal POM file.
It pulls this off if the project is set up to follow the Maven rules, and every library, plugin, and archetype that it depends upon is in the central Maven artifact repository.
Once a project starts to diverge from the templates the Maven team have provided, however, you end up looking behind the curtains and having to fix the underpinnings.
That transition from “Maven user” to “plugin maintainer” can be pretty painful, by all accounts.
Still, Maven does have some long-term potential and it’s worth keeping an eye on, but in our experience it has a hard time building Java projects with complex stages in the build process.
To be fair, building very large, very complex Java projects is hard with any tool.
As an Apache project, it’s controlled by their bylaws, which cover decision-making and write-access to the source tree.
Those with write-access to Ant’s source code repository are called committers, because they’re allowed to commit code changes directly.
All Ant users are encouraged to make changes to the code, to extend Ant to meet their needs, and to return those changes to the Ant community.
As table 1.1 shows, the team releases a new version of Ant on a regular basis.
When they come out,  public releases are stable and usable for a long period.
New releases come out every 12–24 months; point releases, mostly bug fixes, come out about every quarter.
The team strives to avoid breaking existing builds when adding new features and bug fixes.
Nothing in this book is likely to break over time, although there may be easier ways of doing things and the tool will offer more features.
Users of older versions should upgrade before continuing, while anyone without a copy of Ant should download and install the latest version.
If needed, Appendix A contains instructions on how to do so.
This chapter has introduced Ant,  a Java tool that can build, test, and deploy Java projects ranging in size from the very small to the very, very large.
Ant uses XML build files to describe what to build.
Each file covers one Ant project; a project is divided into targets; targets contain tasks.
These tasks are the Java classes that actually perform the construction work.
Ant orders the execution so targets execute in the correct order.
Ant is a free, open source project with broad support in the Java community.
It also integrates well with modern test-centric development processes, bringing testing into the build process.
Major revisions come out every one to two years; minor revisions release every three to six months.
Ant is written in Java, is cross platform, integrates with all the main Java IDEs, and has a command-line interface.
Using Ant itself does not guarantee a successful Java project; it just helps.
It is a tool and, like any tool, provides greatest benefit when used properly.
We’re going to explore how to do that by looking at the tasks and types of Ant, using it to compile, test, package, execute, and then redistribute a Java project.
Let’s start with a simple Java project, and a simple build file.
The first chapter described how Ant views a project: a project contains targets, each of which is a set of actions—tasks—that perform part of the build.
Targets can depend on other targets, all of which are declared in an XML file, called a build file.
This chapter will show you how to use Ant to compile and run a Java program, introducing Ant along the way.
Compiling and running a Java program under Ant will introduce the basic concepts of Ant—its command line, the structure of a build file, and some of Ant’s tasks.
Table 2.1 shows the steps we will walk though to build and run a program under Ant.
The program will not be very useful, but it will introduce the basic Ant concepts.
In normal projects, the build file will be a lot smaller than the source, and not the other way around.
Table 2.1 The initial steps to building and running a program.
Everything will go under here: source files, created output files, and the build file itself.
The fact that this program does nothing but print the argument list is unimportant; it’s still Java code that we need to build, package, and execute—work we will delegate to Ant.
Appendix A describes how to set up an Ant development system on both Unix and Windows.
To compile Java programs, developers also need a properly installed Java Development Kit.
To test that Ant is installed, at a command prompt type.
A good response would be something listing a recent version of Ant, version 1.7 or later:
Step one: verifying the tools are in place Section 2.3
Step two: writing your first Ant build file Section 2.4
Any such message indicates you have not installed or configured Ant yet, so turn to Appendix A: Installation, and follow the instructions there on setting up Ant.
We are going to get Ant to compile the program.
Ant is controlled by providing a text file that tells how to perform all the stages of building, testing, and deploying a project.
These files are build files, and every project that uses Ant must have at least one.
The most minimal build file useful in Java development is one that builds all Java source in and below the current directory:
This is a piece of XML text, which we save to a file called build.xml.
We would not recommend you use it in a real project, for reasons revealed later in this chapter, but it does compile the code.
It’s almost impossible for a Java developer to be unaware of XML, but editing it may be a new experience.
While XML may seem a bit hard to read at first, and it can be an unforgiving language, it isn’t very complex.
Readers new to XML files should look at our brief overview in Appendix B.
Let’s look at that first build file from the perspective of XML format rules.
The <project> element is always the root element in Ant build files, in this case containing two attributes, name and default.
This file could be represented as a tree, which is how XML parsers represent XML content when a program asks the parser for a Document Object Model (DOM) of the file.
The graphical view of the XML tree makes it easier to look at a build file, and so the structure of the build file should become a bit clearer.
At the top of the tree is a <project> element, which has two attributes, name and  default.
All Ant build files must contain a single <project> element as the root element.
It tells Ant the name of the project and, optionally, the default target.
A target represents a single stage in the build process.
It can be called from the command line or it can be used internally.
A build file can have many targets, each of which must have a unique name.
The names of these elements should hint as to their function: one calls the javac compiler to compile Java source; the other echoes a message to the screen.
These are the tasks that do the work of this build.
The compilation task has one attribute, srcdir, which is set to “.” and which tells the task to look for source files in and under the current directory.
The second task, <echo>, has a text child node that will be printed when the build reaches it.
In this build file, we have configured the <javac> task with attributes of the task: we have told it to compile files in the current directory.
Attributes on an element describe options and settings that can only.
Figure 2.1 The XML Representation of a build file is a tree: the project at the root contains one target, which contains two tasks.
This matches the Ant conceptual model: projects contain targets; targets contain tasks.
The attributes and elements of every built-in task are listed in Ant’s online documentation.
Bookmark your local copy of this documentation, as you will use it regularly when creating Ant build files.
In the documentation, all parameters are XML attributes, and all parameters specified as nested elements are exactly that: nested XML elements that configure the task.
Now, let’s get our hands dirty by running the build.
We’ve just covered the basic theory of Ant: an XML build file can describe targets to build and the tasks used to build them.
You’ve just created your first build file, so let’s try it out.
With the Java source and build file in the same directory, Ant should be ready to build the project.
If the build file has been typed correctly, then you should see the following response:
Ant has compiled the single Java source file in the current directory and printed a success message afterwards.
This is the core build step of all Ant projects that work with Java source.
It may seem strange at first to have an XML file telling a tool how to compile a Java file, but soon it will become familiar.
Note that we did not have to name the source files; Ant just worked it out somehow.
We will spend time in chapter 3 covering how Ant decides which files to work on.
For now, you just need to know that the <javac> task will compile all Java files in the current directory and any subdirectories.
If that’s all you need to do, then this build file is adequate for your project.
You can just add more files and Ant will find them and compile them.
Of course, a modern project has to do much more than just compile files, which is where the rest of Ant’s capabilities, and the rest of this book, come in to play.
When you’re learning any new computer language, it’s easy to overlook mistakes that cause the compiler or interpreter to generate error messages that don’t make much sense.
Imagine if somehow the XML was mistyped so that the <javac> task was misspelled, as in.
With this task in the target, the output would look something like.
Whenever Ant fails to build, the BUILD FAILED message appears.
Usually it’s associated with Java source errors or unit test failures, but build file syntax problems result in the same failure message.
If your editor has good XML support, the editor itself will point out any XML language errors, leaving the command line to find only Ant-specific errors.
Editors that are Ant-aware will also catch many Ant-specific syntax errors.
An XML editor would also catch the omission of an ending tag from an XML element, such as forgetting to terminate the target element:
Well-laid-out build files, formatted for readability, help to make such errors visible, while XML-aware editors keep you out of trouble in the first place.
One error we still encounter regularly comes from having an attribute that isn’t valid for that task.
Spelling the srcdir attribute as sourcedir is an example of this:
This message indicates that the task description contained an invalid attribute.
Usually this means whoever created the build file typed something wrong, but it also could mean that the file’s author wrote it for a later version of Ant, one with newer attributes or tasks than the version doing the build.
That can be hard to fix without upgrading; sometimes a workaround isn’t always possible.
It’s rare that an upgrade would be incompatible or detrimental to your existing build file; the Ant team strives for near-perfect backwards compatibility.
The error you’re likely to see most often in Ant is the build halting after the compiler failed to compile your code.
If, for example, someone forgot the semicolon after the println call, the compiler error message would appear, followed by the build failure:
The build failed on the same line as the error in the previous example, line 4, but this time it did the correct action.
The compiler found something wrong and printed its messages, and Ant stopped the build.
The error includes the name of the Java file and the location within it, along with the compiler error itself.
The key point to note is that failure of a task will usually result in the build itself failing.
This is essential for a successful build process: there’s no point packaging or delivering a project if it didn’t compile.
If the build does actually succeed, then the only evidence of this is the message that compilation was successful.
Let’s run the task again, this time in verbose mode, to see what happens.
Ant produces a verbose log when invoked with the -verbose parameter.
This is a very useful feature when figuring out what a build file does.
For this build, the most interesting lines are those generated by the <javac> task.
First, the task did not compile Main.java, because it felt that the destination class was up-to-date.
The task not only compiles all source files in a directory tree, but it also uses simple timestamp checking to decide which files are up-to-date.
The second finding is that the task explicitly skipped the files build.xml and Main.class.
What is the log in verbose mode if Ant compiled the source file? Delete Main.class then run Ant again to see.
The core part of the output provides detail on the compilation process:
This time the <javac> task does compile the source file, a fact it prints to the log.
It still skips the build.xml file, printing this fact out before it actually compiles any Java source.
This provides a bit more insight into the workings of the task: it builds a list of files to compile, which it passes to the compiler along with Ant’s own classpath.
The Java-based compiler that came with the Java Development Kit (JDK) is used by default, running inside Ant’s own JVM.
The log also shows that we’re now running on a Unix system, while we started on a Windows PC.
Ant doesn’t care what platform you’re using, as long as it’s one of the many it supports.
A well-written build file can compile, package, test, and deliver the same source files on whatever platform it’s executed on, which helps unify a development team where multiple system types are used for development and deployment.
Before actually running it, we need to get the compilation process under control by imposing some structure on the build.
The build file is now compiling Java files, but the build process is messy.
Source files, output files, and the build file: they’re all in the same directory.
If this project gets any bigger, things will get out of hand.
The structure we’re going to impose is quite common with Ant and is driven by the three changes we want to make to the project.
To minimize that risk, you should always separate source and generated files into different directories.
We want to place the Java source file into a Java package.
We want to create a JAR file containing the compiled code.
This should be placed somewhere that also can be cleaned up by Ant.
To add packaging and clean-build support to the build, we have to isolate the source, intermediate, and final files.
Once source and generated files are separated, it’s safe to clean the latter by deleting the output directory, making clean builds easy.
These are more reliable than are incremental builds as there is no chance of content sneaking into the output.
It’s good to get into the habit of doing clean builds.
The first step, then, is to sort out the source tree.
We like to have a standard directory structure for laying out projects.
Ant doesn’t mandate this, but it helps if everyone uses a similar layout.
Table 2.2 shows what we use, which is fairly similar to that of Ant’s own source tree.
The others contain files that are created during the build.
To clean up these directories, the entire directory trees can be deleted.
The build file also needs to create the directories if they aren’t already present, so that tasks such as <javac> have a directory to place their output.
We want to move the Java source into the src directory and extend the build file to create and use the other directories.
Before moving the Java file, it needs a package name, as with all Java classes in a big project.
We add this name at the top of the source file in a package declaration:
When the Java compiler compiles the files, it always places the output files in a directory tree that matches the package declaration.
The next time the <javac> task runs, its dependencychecking code looks at the tree of generated class files and compares it to the source files.
It doesn’t look inside the source files to find their package declarations; it relies on the source tree being laid out to match the destination tree.
Table 2.2 An Ant project should split source files, compiled classes files, and distribution packages into separate directories.
This makes them much easier to manage during the build process.
If Ant keeps recompiling your Java files every time you do a build, it’s probably because you haven’t placed them correctly in the package hierarchy.
It may seem inconvenient having to rearrange your files to suit the build tool, but the benefits become clear over time.
On a large project, such a layout is critical to separating and organizing classes.
If you start with it from the outset, even on a small project, you can grow more gently from a small project to a larger one.
Modern IDEs also prefer this layout structure, as does the underlying Java compiler.
Be aware that dependency checking of <javac> is simply limited to comparing the dates on the source and destination files.
A regular clean build is a good practicedo so once a day or after refactoring classes and packages.
With the source tree set up, the output directories follow.
Separate from the source directories are the build and distribution directories.
We’ll configure Ant to put all intermediate files—those files generated by any step in the build process that aren’t directly deployed—in or under the build directory.
We want to be able to clean up all the generated files simply by deleting the appropriate directory trees.
Keeping the directories separate and out of the control of any Software Configuration Management (SCM) tool makes cleanup easy but means that we need to tell Ant to create these directories on demand.
Our project will put the compiled files into a subdirectory of build, a directory called “classes”
Different intermediate output types can have their own directories alongside this one.
As we mentioned in section 2.5.2, the Java compiler lays out packaged files into a directory tree that matches the package declarations in the source files.
The compiler will create the appropriate subdirectories on demand, so we don’t need to create them by hand.
We do need to create the top-level build directory and the classes subdirectory.
We do this with the Ant task <mkdir>, which, like the shell command of the same name, creates a directory.
This call is all that’s needed to create the two levels of intermediate output.
To actually place the output of Ant tasks into the build directory, we need to use each task’s attribute to identify a destination directory.
For the <javac> task, as with many other Ant tasks, the relevant attribute is destdir.
A common stage in a build process is to package files, placing the packaged file into the dist directory.
There may be different types of packaging—JAR, Zip, tar, and WAR, for exampleand so a subdirectory is needed to keep all of these files in a place where they can be.
To create the JAR file, we’re going to use an Ant task called, appropriately, <jar>
We’ve dedicated chapter 5 to this and the other tasks used in the packaging process.
For this introductory tour of Ant, we use the task in its simplest form, when it can be configured to make a named JAR file out of a directory tree:
Doing so shows the advantage of placing intermediate code into the build directory: you can build a JAR file from it without having to list what files are included.
This is because all files in the directory tree should go in the JAR file, which, conveniently, is the default behavior of the <jar> task.
With the destination directories defined, we’ve now completed the directory structure of the project, which looks like the illustration in figure 2.2
Figure 2.2 The directory layout for our projectkeeping source separate from generated files.
The shaded directories and files are created during the build.
This is going to be the basic structure of all our projects: source under src/, generated files under build/, with the compiled classes going under build/ classes.
Future projects will have a lot more files created than just .class files, and it’s important to leave space for them.
With this structured layout, we can have a new build file that creates and uses the new directories.
Now that we have the files in the right places and we know what we want to do, the build file needs to be rewritten.
Rather than glue all the tasks together in one long list of actions, we’ve broken the separate stages—directory creation, compilation, packaging, and cleanup—into four separate targets inside the build file.
This build file adds an init target to do initialization work, which means creating directories.
We’ve also added two other new targets, clean and archive.
The archive target uses the <jar> task to create the JAR file containing all files in and below the build/classes directory, which in this case means all .class files created by the compile target.
The clean target cleans up the output directories by deleting them.
We’ve also changed the default target to archive, so this will be the target that Ant executes when you run it.
As well as adding more targets, this build file adds another form of complexity.
In our current project, for the archive to be up-to-date, all the source files must be compiled, which means the archive target must come after the compile target.
Likewise, compile needs the directories created in init, so Ant must execute compile after the init task.
Ant needs to know in what order it should execute targets.
These are dependencies that we need to communicate to Ant.
We do so by listing the direct dependencies in the depends attributes of the targets:
In our project, the archive task depends upon both init and compile, but we don’t bother to state the dependency upon init because the compile target already depends upon it.
If Ant must execute init before compile and archive depends upon compile, then Ant must run init before archive.
What isn’t important is the order of targets inside the build file.
Ant reads the whole file before it builds the dependency tree and executes targets.
There’s no need to worry about forward references to targets.
If you look at the dependency tree of targets in the current example, it looks like figure 2.3
Before Ant executes any target, it executes all its predecessor targets.
If these predecessors depend on targets themselves, Ant considers those and produces an order that satisfies all dependencies.
If two targets in this execution order share a common dependency, then that predecessor will execute only once.
Usually in Make, you name the source files that a target depends on, and the build tool itself works out what to do to create the target file from the source files.
In Ant, you name stages of work as targets, and the tasks inside each target determine for themselves what their dependencies are.
Figure 2.3 Once you add dependencies, the graph of targets gets more complex.
Here clean depends upon init; archive depends on compile, and, indirectly, init.
All of a target’s dependencies will be executed ahead of the target itself.
Interlude: circular dependencies What happens if a target directly or indirectly depends on itself? Does Ant loop? Let’s see with a target that depends upon itself:
When Ant parses the build file, it builds up the graph of targets.
If there is a cycle anywhere in the graph, Ant halts with the error we’ve just seen.
Any tasks placed in the build files outside of any target will be executed before the target graph is created and analyzed.
In our experiment, we had an <echo> command outside a target.
Ant executes all tasks outside of any target in the order they appear in the build file, before any target processing begins.
With a loop-free build file written, Ant is ready to run it.
Now that there are multiple targets in the build file, we need a way of specifying which to run.
You can simply list one or more targets on the command line, so all of the following are valid:
Calling Ant with no target is the same as calling the target named in the default attribute of the <project>
This example demonstrates that Ant has determined the execution order of the targets.
As both the compile and archive targets depend upon the init target, Ant calls init before it executes either of those targets.
It orders the targets so that first the directories get created, then the source is compiled, and finally the JAR archive is built.
What happens when the build is run a second time?
Let’s look at the log of the build if it’s rerun immediately after the previous run:
Ant goes through all the targets, but none of the tasks say that they are doing any work.
Here’s why: all of these tasks in the build file check their dependencies, and do nothing if they do not see a need.
No files have been compiled, and the JAR is untouched.
If you add the -verbose flag to the command line, you’ll get more detail on what did or, in this case, did not take place.
The verbose run provides a lot of information, much of which may seem distracting.
When a build is working well, you don’t need it, but it’s invaluable while developing that file.
Here the build lists the order of target evaluation, which we’ve boldfaced, and it shows that the <jar> task is also dependency-aware: the JAR file was not modified since every file inside it was up-to-date.
That shows a powerful feature of Ant: many tasks are dependency-aware, with special logic to handle problems such as timestamps inside Zip/JAR files or to remote FTP sites.
Now that the build file has multiple targets, another question arises.
Can we ask for more than one target on the command line?
Developers can run multiple targets in a single build, by listing the targets one after the other on the command line.
But what happens when you type ant compile archive at the command line? Many people would expect Ant to pick an order that executes each target and its dependencies once only: [init, compile, archive]
Unix Make would certainly do that, but Ant does not.
Instead, it executes each target and dependents in turn, so the actual sequence is init, compile, then init, compile, archive:
This behavior is a historical accident that nobody dares change.
However, if you look closely, the second time Ant executes the compile target it does no work; the tasks get executed but their dependency checking prevents existing outputs from being rebuilt.
A simple left-to-right execution order would run the compile target before it was initialized.
Ant would try to execute the targets in this order, but because the compile target depends upon init, Ant will call init first.
If you try to control the execution order by listing targets in order, you may not get the results you expect since explicit dependencies always take priority.
Being able to run multiple targets on the command line lets developers type a sequence of operations such as ant clean execute to clean the output directory, rebuild everything, and run the program.
Of course, before they can do that, Ant has to be able to run the program.
We now have a structured build process that creates the JAR file from the Java source.
At this point the next steps could be to run tests on the code, distribute it, or deploy it.
Calling Java programs from the command line isn’t hard, just fiddly.
If we run our program from the build file, we get some immediate benefits:
A target to run the program can depend upon the compilation target, so we know we’re always running the latest version of the code.
You can halt a build if the return code of the program isn’t zero.
Integrating compiling with running a program lets you use Ant to build an application on demand, passing parameters down, including information extracted from other programs run in earlier targets.
To run the program, we add a new target, execute, which depends upon compile.
The first two arguments use the value attribute of the <arg> tag, which passes the value straight down to the program.
The final argument uses the file attribute, which tells Ant to resolve that attribute to an absolute file location before calling the program.
Interlude: what can the name of a target be? All languages have rules about the naming of things.
In Java, classes and methods cannot begin with a number.
Ant targets can be called almost anything you want—their names are just strings.
However, for the sake of IDEs and Ant itself, here are some rules to follow:
Don’t call targets "" or "," because you won’t be able to use them.
Targets beginning with a minus sign cannot be called from the command line.
This means a target name "-hidden" could be invoked only by other tasks, not directly by users.
We would advise against using dots in names, such as "build.install", for reasons we won’t get into until the second section of the book entitled, “Applying Ant.”
With the execute target written, we can compile and run our program under Ant.
What does the output of the run look like? First, let’s run it on Windows:
The compile task didn’t need to do any recompilation, and the execute task called our program.
Ant has prefixed every line of output with the name of the task currently running, showing here that this is the output of an invoked Java application.
The first two arguments went straight to our application, while the third argument was resolved to the current directory; Ant turned “.” into an absolute file reference.
This shows another benefit of starting programs from Ant rather than from any batch file or shell script: a single build file can start the same program on multiple platforms, transforming filenames and file paths into the appropriate values for the target platform.
This is a very brief demonstration of how and why to call programs from inside Ant, enough to round off this little project.
Chapter 6 will focus on the topic of calling Java and native programs from Ant during a build process.
We’ve nearly finished our quick introduction to Ant, but we have one more topic to cover: how to start Ant.
We’ve already shown that Ant is a command-line program and that you can specify multiple targets as parameters.
We’ve also introduced the -verbose option, which allows you to get more information on a build.
We want to do some more to run our program.
First, we want to remove the [java] prefixes, and then we want to run the build without any output unless something goes wrong.
Ant can take a number of options, which it lists if you ask for them with ant -help.
The current set of options is listed in table 2.3
This list can expand with every version of Ant, though some of the options aren’t available or relevant in IDE-hosted versions of the program.
Note also that some of the launcher scripts, particularly the Unix shell script, provide extra features, features that the ant -help command will list.
Some options require more explanation of Ant before they make sense.
This option lets you control which build file Ant uses, allowing you to divide the targets of a project into multiple files and select the appropriate build file depending on your actions.
To invoke our existing project, we just name it immediately after the -f or -buildfile argument:
This is exactly equivalent to calling ant compile with no file specified.
It’s easier to use the -find option, which must be followed by the name of a build file.
This variant does something very special: it searches the directory tree to find the first build file in a parent directory of that name, and invokes it.
With this option, when you are deep into the source tree editing files, you can easily invoke the project build with the simple command:
Most other command-line options are less risky, such as those that control the log level of the program.
We stated that we want to reduce the amount of information provided when we invoke Ant.
Getting rid of the [java] prefix is easy: we run the build file with the -emacs option.
The option is called -emacs because the output is now in the emacs format for invoked tools, which enables that and other editors to locate the lines on which errors occurred.
For our exercise, we only want to change the presentation from the command line, which is simple enough:
This leaves the next half of the problem—hiding all the output.
Three of the Ant options control how much information is output when Ant runs.
Two of these (-verbose and -debug) progressively increase the amount.
The -verbose option is useful when you’re curious about how Ant works or why a build isn’t behaving.
The -debug option includes all the normal and verbose output and much more low-level information, primarily only of interest to Ant developers.
To see nothing but errors or a final build failed/success message, use -quiet:
One of the attributes of <echo> is the level attribute, which takes five values: error, warning, info, verbose, and debug control the amount of information that appears.
The default value info ensures that messages appear in normal builds and in  -verbose and -debug runs.
By inserting an <echo> statement into our execute target with the level set to warning, we ensure that the message appears even when the build is running as -quiet:
To eliminate the [echo] prefix, we add the -emacs option again, calling >ant -q -emacs.
Asking for -quiet builds is good when things are working; asking for -verbose is good when they are not.
Using <echo> to log things at level="verbose" can provide extra trace information when things start going wrong.
The other way to handle failure is to use the -keep-going option.
The -keep-going option tells Ant to try to recover from a failure.
If you supply more than one target on the command line, Ant normally stops the moment any of these targets—or any they depend upon—fail.
The -keep-going option instructs Ant to continue running any target on the command line that doesn’t depend upon the target that fails.
This lets you run a reporting target even if the main build didn’t complete.
It lists the main targets in a project and is invaluable whenever you need to know what targets a build file provides.
Ant lists only those targets containing the optional description attribute, as these are the targets intended for public consumption.
This isn’t very informative, which is our fault for not documenting the file.
To see both main and sub targets in a project, you must call Ant with the options -projecthelp and -verbose.
The more complex a project is, the more useful the -projecthelp feature becomes.
We strongly recommend providing description strings for every target intended to act as an entry point to external callers, and a line or two at the top of each build file describing what it does.
Having looked at the options, especially the value of the -projecthelp command, let’s return to the build file and add some descriptions.
Listing 2.1 shows the complete listing of the final build file.
In addition to adding the description tags, we decided to make the default target run the program.
We’ve marked the major changes in bold, to show where this build file differs from the build files and build file fragments shown earlier.
Listing 2.1 Our first complete build file, including packaging and executing a Java program.
That’s forty-plus lines of Ant XML to compile ten lines of Java, but think of what those lines of XML do: they compile the program, package it, run it, and can even clean up afterwards.
More importantly, if we added a second Java file to the program, how many lines of code need to change in the build file? Zero.
As long as the build process doesn’t change, you can now add Java classes and packages to the source tree to build a larger JAR file and perform more useful work on the execution parameters, yet you don’t have to make any changes to the build file itself.
That is one of the nice features of Ant: you don’t need to modify your build files whenever a new source file is added to the build process.
Others, including Eclipse and IntelliJ IDEA, let you add build files to a project and run them from within the GUI.
To show that you can run this Ant under an IDE, figure 2.4 shows a small picture of the "execute" target running under Eclipse.
All the examples in this book were run from the command line for better readability.
However, most of the build files were written in IDEs and often were tested there first.
Don’t think that adopting Ant means abandoning IDE tools; instead you get a build that works everywhere.
Ant is told what to build by an XML file, a build file.
This file describes all the actions to build an application, such as creating directories, compiling the source, and determining what to do afterwards; the actions include making a JAR file and running the program.
The build file is in XML, with the root <project> element representing a Ant project.
This project contains targets, each of which represents a stage of the project.
A target can depend on other targets, which is stated by listing the dependencies in the depends attributes of the target.
Ant uses this information to determine which targets to execute, and in what order.
The actual work of the build is performed by Ant tasks.
These tasks implement their own dependency checking, so they only do work if it is needed.
Consult Appendix C for the steps needed to do this.
Running Ant is called building; a build either succeeds or fails.
Builds fail when there’s an error in the build file, or when a task fails by throwing an exception.
In either case, Ant lists the line of the build file where the error occurred.
Ant can build from the command line, or from within Java IDEs.
The command line has many options to control the build and what output gets displayed.
Rerunning a build with the -verbose option provides more detail as to what is happening.
The most important argument to the command line is the name of the targets to run—Ant executes each of these targets and all its dependencies.
After this quick introduction, you’re ready to start using Ant in simple projects.
If you want to do this or if you have deadlines that insist on it, go right ahead.
The next two chapters will show you how to configure and control Ant with its properties and datatypes, and how to run unit tests under it.
If your project needs these features, then please put off coding a bit longer, and keep reading.
In the last chapter, we used Ant to build, archive, and run a Java program.
Now we’re going to look at how to control that process through Ant’s datatypes and properties.
In programming language terms, Ant’s tasks represent the functionality offered by the runtime libraries.
The tasks are useful only with data, the information that they need to know what to do.
Java is an object-oriented language where data and functions are mixed into classes.
Ant also has the approximate equivalent of variables in its properties.
To pass data to tasks, you need to be able to construct and refer to datatypes and properties in a build file.
As with tasks, datatypes are just pieces of XML, pieces that list files or other resources that a task can use.
It does go into some depth, so don’t be afraid to skip bits and return to them later.
Just as Java has classes and variables, Ant has datatypes and properties, which are at the core of its capabilities.
All build files make use of them in one way or another, and all Ant users need to understand them.
Datatypes store complex pieces of information used in the build—for example, a list of files to compile or a set of directories to delete.
These are the kinds of things Ant has to manage, so build files need a way to describe them.
You can declare them inside a task or define them outside, give them a name, and then pass that name to a task.
This lets you share a datatype across more than one task.
A typical Ant build has to handle files and paths, especially the notorious classpath.
The fileset and path datatypes crop up throughout Ant build files.
The fileset datatype can enumerate which files to compile, package, copy, delete, or test.
Defining a fileset of all Java files, for example, is straightforward:
This reference name can be used later wherever a fileset is expected.
For example, copying our source code to another directory using the same source.fileset is.
This will work only if the fileset was defined previously in the build, such as in a predecessor target.
Otherwise, Ant will fail with an error about an undefined reference.
That’s a quick introduction to datatypes, which we’ll be using throughout the book.
Ant properties are essential not just to share information around the build, but to control build files from the outside.
For example, changing a build to use a different version of a third-party library, perhaps for testing purposes, can be made as trivial as this:
In either case, the Ant property host is now bound to the value localhost.
Unlike Java variables, Ant properties are immutable: you cannot change them.
The first task, project, or person to set a property fixes it for that build.
This rule is the opposite of most languages, but it’s a rule that lets you control build files from the outside, from tools such as IDEs, or from automated build systems.
It’s also the key to letting different users customize a build file to work on their system, without editing the build file itself.
Simply by defining the appropriate property on the command line, you can change the behavior of your own or someone else’s build file.
Inside the build file, properties let you define a piece of information once and share it across many tasks.
Now that we’ve defined datatypes and properties, let’s use them to get Ant to compile a program.
This task provides a front end to many Java compilers, including the normal JDK compiler and alternatives such as Jikes and gjc.
Most of the differences in invocation and command-line parameters are handled by the task, so users can use whichever compiler they prefer without the build file author having to know or care.
To compile Java source on the command line, you have to specify the source files and usually a destination directory.
Other common options control whether debugging information is included, and what the classpath for the compiler is.
Here’s how we would go about compiling some Java source on the command line:
We declare the destination directory and add it to our classpath, say that our source package hierarchy begins in the src directory, and that we want full debugging information.
Finally, we declare that we want all Java files in a single directory compiled.
Sun’s javac program is helpful, in that it automatically compiles source files you didn’t tell it to compile, if import statements imply that they’re needed.
Other compilers, such as that from the Kaffe JVM, aren’t so greedy, and we would need to specify every .java file.
You need a Java compiler such as the JDK javac program.
See appendix A for installation and configuration information in order to use <javac>
Also note Ant’s way of using domain-specific terminology for concepts such as classpath.
Most of these attributes are Boolean in nature—debug, optimize, nowarn, verbose, and deprecation.
Ant allows flexibility in how Booleans can be specified with on, true, and yes all representing true and any other value mapping to false.
Using this information, we can write the following piece of a build file:
This <javac> example is more than a simple translation: we’ve started to adapt it to Ant’s way of working.
To explain this example, we’ll have to introduce some new concepts.
The srcdir attribute f implicitly defines a fileset containing all files in the specified directory tree.
This single fragment of a build file contains most of the core concepts of Ant.
A path, sometimes called a path-like structure in Ant’s documentation, is an ordered list of elements, where each element can be a file or a directory.
It describes paths such as the Java CLASSPATH, or the PATH environment variable of Unix and Windows.
This definition contains one element, whose location attribute can specify a single file or directory.
This path attribute separates its parameters into individual elements, using either a semicolon (;) or colon (:) to split them.
There’s some special handling for a Windowsstyle c:\winnt; this will be treated as a single directory, and not c and winnt.
Directories can be separated by either a forward-slash (/) or  a back-slash (\), regardless of operating system; a build file shouldn’t have to care what system it runs on.
If a path structure consists of only a single path or location, it can be specified using a shortcut form as in.
This set of files creates a path containing all JAR files in the lib directory.
This is a path built from a fileset, which we’re about to introduce.
Each element in a path is ordered from the top and down so that all files within a fileset would be grouped together in a path.
The result in this example is that the path would contain all the JAR files, but the order cannot be predicted.
A standalone path declaration can be given a name via its id attribute.
This name has to be unique across all Ant datatypes given ID values; this is a separate namespace from property and target names.
The name can be referenced whenever a path is needed:
The refid attribute references the defined path; if no such path has been defined at that point in the build, Ant will fail with an error.
The other way to use a path is inline, in any task that takes a nested element of the path type.
They may have other names, though the word path is usually in there.
The latter path element shows that not all path elements end in the word path—this is a special case for compatibility with the command-line version.
It also shows that the online manual (or an Ant-aware text editor) is important to have when writing build files.
When using the task, we could declare two <src> tags to compile two separate directory trees of source code into a single output directory:
There are lots of permutations of all the ways in which these fileset and path capabilities can work together to accomplish precisely choosing the files desired.
We’ll expose you to some of these variations throughout this book.
While a path represents a list of files or directories, a fileset represents a general-purpose collection of files, such as all the Java files in a source tree.
It’s the main way to pass a collection of files to an Ant task.
Most build processes operate on sets of files, either to compile, copy, delete, or manipulate in any number of other ways.
These processes are so central that Ant provides the fileset as a built-in datatype, one of the more generic sets of resource types.
A fileset is a set of files rooted from a single directory.
By default, a fileset specified with only a root directory will include all the files in that entire directory tree, including files in all subdirectories recursively.
For a concrete running example that will demonstrate fileset features as we discuss them, let’s copy files from one directory to another:
In its current form, all files from the web directory are copied to the newweb directory.
This example will evolve into copying only specific files, altering them during the copy with token replacement, and flattening the directory hierarchy in the newweb directory.
Selecting all files in a directory is a bit of a blunt instrument.
Many filesets restrict their selection to a subset of the files by using a patternset.
A fileset can contain multiple patternsets, which restrict the files in the fileset to those that match or don’t match a specified pattern.
We can use one to include all JSP files under a web directory:
Had we specified just *.jsp, only the JSP files in the web directory would have been copied, but the files in any subdirectories wouldn’t have been copied.
The patternset itself doesn’t refer to any actual files until it’s nested in a fileset and, therefore, rooted at a specific directory.
The patterns it supports are simple regular expressions on a directory path.
The rules for pattern matching in the strings are as follows:
The directory separators "/" and "\" are converted into the correct separator.
As well as being embedded inside filesets, patternsets can be specified independently as standalone datatypes.
Exclusion patterns take precedence, so that if a file matches both an include and exclude pattern, the file is excluded.
Each of these elements has if/unless attributes, which are covered in the conditional patternset section later in this chapter.
This patternset includes all JSP pages in a single directory.
Including and excluding patterns allows filesets to be defined precisely to encompass only the files desired.
You can specify more than one include file by using nested includesfile elements.
You can specify more than one exclude file by using nested excludesfile elements.
This one includes all JSP pages in a directory tree, except any in the directory test and.
As you can see, explicit exclusion is a powerful tool.
One thing that’s important to know is that some file types are excluded by default, the default excludes patterns.
In many builds, special or temporary files end up in your source tree from IDEs and Software Configuration Management (SCM) systems such as CVS and Subversion.
To avoid having to always explicitly exclude these, exclude patterns are enabled by default for some common patterns.
Many users have been bitten by the confusion caused when a fileset omits files because they match one of these default exclude patterns.
The <fileset> element has a defaultexcludes attribute for turning off this behavior.
If needed, you can change the set of defaultexcludes files using the <defaultexcludes> task.
And you can even print the list of current patterns:
Table 3.3 Default exclude patterns, which are used in filesets to match files that aren’t used, copied or deleted by default.
If you get the list wrong, you can end up excluding all files!
Filesets in use Having covered filesets and patternsets, we can apply the information to the <javac> task.
This task is one of the many implicit fileset tasks.
Rather than requiring you to add a fileset of source files as a nested element, the task itself supports many of the attributes and elements of a fileset:
This task has <javac> acting as a fileset, including some files and excluding some others.
Note: you can’t reliably use excludes patterns to tell <javac> which files not to compile.
If a Java file you include needs another file, Sun’s javac compiler will search the source tree for it, even if it’s been excluded from the fileset.
This is a feature of the compiler, and not Ant.
Another thing to know is that filesets resolve their files when the declaration is first evaluated.
This may not be when it’s declared, but when it’s first used.
This is important to know when referring to a previously defined fileset, as new files and directories matching the patterns may have appeared between the resolution and reference; these new files do not appear in the fileset.
Filesets and paths are some of the most common of Ant’s datatypes and are often passed down to tasks within nested elements.
Other tasks name their parameters from the role of the data.
Most tasks with implicit filesets can be recognized by their dir, includes, and.
A lot of the core Ant tasks take arguments this way, though it’s no longer encouraged in new tasks because nested datatypes are preferred.
Regardless of how they’re passed down, datatypes are the main way of configuring Ant tasks, and the fileset and path are ubiquitous.
The fileset is the most common datatype that build file authors will write.
One of its strengths is that it can select which files to work on by much more than just the name of the file.
Filenames are a common way of selecting files for operations, but not always enough.
Sometimes you want to delete out-of-date files or upload only changed files to a remote site.
What if you want to delete files, leaving directories in their place? You can do all these things by refining the fileset with selectors.
Each selector is a test that’s applied to each file in the fileset (or other selector container)
It narrows down the selection of files in the fileset.
These selectors can be combined inside selector containers to provide grouping and logic.
Containers may be nested inside containers, enabling complex selection logic.
Any fileset can be restricted by these selectors to choose only those files that match the specific tests.
This <copy> task will copy only the files from the web directory that don’t exist in the currentfiles directory.
Using the <contains> selector, we choose only those files containing a certain string:
Only the files containing the text “System” in the web directory are copied to the currentfiles directory.
By default <contains> is case-sensitive, but it can be changed using casesensitive="no"
All rules must be satisfied before a file is considered part of a fileset, so when using selectors in conjunction with patternsets, the file must match the include patterns, must not match any exclude patterns, and the selector rules must test positively.
If you don’t find the current selectors adequate, you can write a custom one in Java.
There are a few other file and directory datatypes that crop up in special cases, which we’ll explore next.
We’ve covered the Ant datatypes that are frequently used by Ant tasks, but there are several other datatypes that are used by a smaller number of tasks.
Here’s a brief overview of the most important ones: filelist, dirset, and filterset.
These are ordered lists of files and directories that may or may not exist.
They’re useful when you need to order a set of files.
The latter file shows that filename expansion doesn’t take place in filesets.
The other big difference is that while a fileset finds all existing files that match a pattern, a filelist can contain filenames that don’t yet exist.
Dirset The fileset datatype incorporates both files and directories, but some tasks prefer to only operate on directories.
The path datatype also supports a nested <dirset>, which allows for easier construction of classpath elements for multiple directories.
Filterset During the build process, you sometimes need to dynamically fill in values of a file, often with timestamps and version information; sometimes with tuning code for a specific project.
Putting the current date or version information into files bundled with a build, such as documentation.
A token is defined as text surrounded by beginning and ending token delimiters.
That concludes our overview of the main datatypes of Ant.
Filesets and paths are the most common and are the ones that will soon become familiar.
Although we’ll return to datatypes later in the chapter, looking at datatype references and Ant’s resources model, we now have enough to get started.
Before doing that, it’s time to look at properties, because together with filesets and paths, they form the core of Ant’s configuration data.
One of  the most important concepts in Ant is that of properties.
Ant properties contain string values that can be used wherever a string is needed in a build file.
They can be set in tasks or on the command line, and can both control the build process and configure it.
Ant provides various built-in properties, properties that the runtime sets for you.
This example was run with the -file command-line option to specify a different build file name, as shown in ant.file.
The basedir property defaults to the path of the current build file—it can be changed by adding a basedir attribute on the <project> element.
Build files can rely on these being set, although sometimes IDE-hosted Ant runs can find that this isn’t always the case.
All JVM system properties are provided as Ant properties, letting your build files.
Here are sample results from running this code on a Windows machine:
If you’re curious about the set of properties at any point in a build file, the <echoproperties> task can list the current set to the console or to a file.
On its own, this task will list all properties in the order they’re stored in the hashtable:
The XML output is sorted, which makes it easy to browse.
You could also save the list to a file to compare against a previous version.
Listing properties or saving them to a file is useful while learning about properties and for diagnostics.
We often have a diagnostics target that lists the current set of properties.
Being able to read properties is only half the problem.
The <property> task is the normal way to set properties.
It provides many ways to assign properties, the most popular being.
A common action in a build file is selecting one of two choices, such as whether to compile debug information into the .class files.
We can define a property named build.debug and set its value to true.
However, it isn’t the best way to set filenames or paths, where the location attribute is preferable.
Setting a property to a filename The location attribute of the <property> task converts a relative path into an absolute location and converts directory separators to that of the target platform.
Build file authors can choose between forward slashes (/) or backslashes (\) based on personal preference:
On a Unix system, the path could be something like /home/erik/release.
This resolved path can be passed down to native programs or Java code, as it’s now absolute and platform-specific.
The directory against which relative paths are resolved is the base directory of the project, which is typically the directory where build.xml resides.
You can often get away with using the value attribute instead; many build files do.
Consider it a best practice, rather than an absolute requirement.
In addition to setting properties in the build file, Ant can be configured by properties in external files.
Loading properties from a properties file Loading properties from a Java properties file is a common way of customizing builds.
We can create a file named build.properties in the root directory of our project, alongside the build file.
Ant’s  <property> task will load the file in the file attribute:
Property values in the properties file are expanded during the load.
To turn them into absolute values, the build file would have to reassign them.
There are two other issues with property file loading that developers need to.
One is that to use a backslash in the file, it must be repeated twice:
Indeed, it doesn’t even warn of a problem, except when you run Ant in verbose (-v) mode.
It lets you offer the option to control the build, without making it mandatory.
To understand how to do that, you need to understand Ant’s unusual property assignment model.
Why overriding a property doesn’t work First, a little test—examine the following code lines and guess their output given the properties file just defined, the one that sets build.debug to false:
We wouldn’t have asked this question had it been completely straightforward.
A property’s value doesn’t change once set: properties are immutable.
Whoever sets a property first, sets it for the duration of the build.
Understanding this rule and how to take advantage of it is essential for using Ant.
Why does Ant have such a rule, one at odds with most other languages? It could.
Why? Because you can manipulate the build file from the outside.
To do a single build with debugging turned off, just set it on the command line.
In that build, the property build.debug is frozen to false, regardless of what assignments are made.
This is the direct opposite of variables in a program, where the last assignment becomes its value.
Ant’s property override rule turns every property into a place in which someone can deliberately override the build file to meet their needs.
Choosing to compile against different JAR files in a build.
All these things can be enabled through a judicious use of Ant properties.
We’ll define many build file settings via properties in order to give users a way of controlling the build and to encourage Ant users to do the same.
Loading environment variables The <property> task can import environment variables into Ant properties.
In order to avoid colliding with existing Ant properties, environment variables are loaded with a name prefix.
All environment variables are loaded into Ant’s internal properties with the prefix env.
This gives us properties like env.PATH, or, on Windows NT platforms, env.Path.
Tasks in the build can use these settings and change them before executing native and Java programs.
Extracting environment variables is one of the early setup actions most build files do.
We’ve been busy setting Ant properties with the <property> task.
Many tasks set properties in the course of their work; it’s one of the main ways of passing data between tasks.
The <available> task can check for the existence of a Java class or resource in a classpath or for the existence of a file or directory in the file system.
One use of <available> is to check for a particular class in a classpath.
This could let Ant skip targets when a prerequisite is missing.
The task can look for a class in Ant’s own classpath or another supplied path:
If it is absent, the property isn’t touched and remains undefined.
The <available> task can also look for files or directories:
The file attribute specifies the file or directory to locate.
The optional type attribute can require the file to be of a specific type—either a file or dir for file and directory, respectively.
The final availability check is for a Java resource, which is any file that can be found on the classpath.
You can even look for a class without loading it, by giving the path to the implementation class:
This is equivalent to the classname probe we’ve already seen, except the .class file itself must be requested, and package separators replaced with forward slashes.
The <available> task is essentially one of Ant’s many conditions.
It’s an Ant task, but it also can be nested inside the <condition> task.
Most other conditions only work inside an Ant task that supports them.
There are lots of other conditions, which can be used to set properties or control other aspects of a build.
Most of Ant’s tests are grouped under the <condition> task, which will set a named property if a nested condition holds true.
Here’s a test that sets the property os to the value "windows" if the underlying OS is either of the two Windows platforms, or "other" if it is anything else:
The conditions that Ant’s conditional tasks support are listed in table 3.6, along with the version of Ant they appeared in.
The list of tests began in Ant 1.4 and has grown over time to let you test everything from Ant’s version to the availability of remote computers.
The list of tests began in Ant 1.4 and has grown over time to let you test everything from Ant’s version to the availability of remote computers.
The test tells Ant to halt the build unless there’s something listening for inbound TCP connections on port 8080 on the local machine.
We use the property server.port to hold the port to test; anyone who runs the server on a different port can override this property and the test will adapt.
Many other tasks do this, of which a simple one is <tstamp>, which creates build timestamps for naming files or including in messages.
It sets three properties automatically based on the current date/time.
The <format> element also supports locale and offset attributes to change the time zone or the output format—refer to the task reference for these specifics.
We like to set timestamps in the ISO 8601 time representation, which are common in XML documents described in the XML Schema language.
We can save it in a properties file inside the application, then embed it into generated files.
If we set the file attribute of <echo>, we can print this to a file, where it can be included in the application’s distribution package.
The <tstamp> task is a useful example of how many Ant tasks set properties when they run.
Many other tasks do this; they’ll be introduced as we automate other aspects of the build process.
Another way of setting properties is on the command line.
Ant users can set Ant properties on the command line before Ant does any work.
For example, you may want to test against a different version of a library, or supply a password to an FTP upload.
Ant has two command-line options to set properties: -D and -propertyfile.
Nothing can override a property set from the command line.
Ant has two classes of properties, user properties and standard properties.
User properties consist of system properties and command-line defined properties, as well as properties overridden using <ant>
Properties defined on the command line get set as user properties and are truly immutable, ignoring even the immutability exceptions noted earlier.
You can also name files of properties to load, using the -propertyfile option and following it with a filename:
This will load the property file before the build file itself is executed.
If the property file is missing, a warning is printed but the build continues.
The fact that property expansion doesn’t take place can cause surprises.
Ant properties, whether they’re set on the command line or by a task, are the main way to control Ant.
There are three main ways in which properties can help control builds.
In all the techniques, the value of a property is usually unimportant.
Most of Ant’s conditional execution assumes that a property being set equals true and unset equals false.
Now, let’s go through the three main ways Ant can be controlled by properties.
Here’s how to use the if attribute to conditionally include source code in a JAR file:
Each target’s conditions are evaluated just prior to the target’s execution.
This allows dependent targets to control their successors through properties.
In this little demonstration, the copysource target could be enabled by setting copy.source.
Alternatively, the copy.source property could be defined by using one of the many variants of <property>
The next is that a condition is met if a property is defined; its.
Finally, some people expect that if a target’s condition isn’t met, then its dependencies should not execute.
All proceeding targets get processed before the test is looked at.
Alternatively, a property value can be used to stop the build.
Ant also has if and unless attributes to block the build when a property is defined or undefined.
The if/unless attributes used to be the only way to make failure conditional other than placing it in a conditional target.
You may encounter it in existing projects, although a nested <condition> is usually easier to use.
We can also use properties to control which files get pulled into filesets, which lets you choose which files to work on based on the state of the system.
This is a useful feature for including or excluding files from compilation depending on the existence of libraries.
This example takes advantage of <javac> acting as an implicit fileset, but the if/ unless technique can configure the files that go into any fileset, and hence into any task that supports them.
Overall, conditional patternsets, targets, and tasks are among the main ways that properties can configure a build, skipping files or tasks when not needed.
Another way that properties configure tasks is through inline string expansion, which can be used in attributes or elements.
Together, they make properties the main way to share information across tasks.
The other way to share data across tasks is by using shared datatypes, passing references to the types to multiple tasks.
Every Ant datatype declaration can have a unique identifier, which can be used as a reference.
The path can also be referenced in other datatype declarations, such as when the test classpath is set up:
Anywhere a datatype is declared, it can have an id associated with it, which can then be used later.
Normally referenced datatypes are defined outside any task, in a target that gets executed before the tasks that refer to the data.
While properties and datatypes are independent from one another for most practical purposes, there are a couple of interesting intersections between them.
The ubiquitous <property> task can convert any reference to its string representation by calling the toString() operation on the datatype.
As an example, let’s turn a path into a string.
The <path> datatype resolves all relative items to their absolute paths and converts.
This technique works for any Ant datatype that has a useful toString() value.
We can also use references across patternsets, and use the same string conversion.
Using references for nested patternsets Patternsets provide a nice abstraction for file and directory name matching for use inside of filesets.
Defining a patternset only once with an id allows it to be reused in any number of filesets.
The binary.files patternset excludes both .txt and .xml files, and the files included or excluded by the image.files patternset.
The string representation of a patternset is useful for debugging purposes, so defining a property using the patternset refid yields these results:
If you don’t know why an Ant build doesn’t do what you expect, print it in <echo>
If you echo at level="verbose", then the message doesn’t appear except on a.
Most Java projects depend on other Java libraries and JAR files.
Some may only be needed at build time, others during testing or when executing the final program.
The Ant build file needs to set up the classpath for the tasks that perform all these operations; otherwise the build will fail.
The simplest (and most common) way to manage libraries is to have a directory (by convention, the directory lib), into which you put all dependent libraries.
To add a new JAR to the project, drop it into the directory.
To include these files on our compilation classpath, we declare a path that includes every JAR file in the directory:
With this technique, it’s easy to add new JAR files, and you can see what libraries a project uses.
If you check the JARs in the lib directory into the source code repository, all developers will get the files they need—and any updates.
Dropping JAR files into a single directory does have its limitations, and in big projects it’s too simple a solution.
One technique to help you prepare for that is this:
Give every library that you put in the lib/ directory a version number in the filename, if it doesn’t have one already.
This lets you see at a glance which versions of the libraries you’re using.
We’re going to stick with this technique of JAR file management throughout the first section of the book.
In the second section, Applying Ant, we’ll return to the problem of library management.
Having introduced Ant’s main datatypes and its property mechanism, we’ve introduced enough of Ant’s type system to be able to build complex applications.
These are just a set of Ant datatypes with standard Java interfaces, interfaces that enable tasks to use them as sources of data.
It’s an attempt at fitting a unified data/ file access model to Ant, including the existing datatypes.
Ant works with files—files in the file system, files in JAR, Zip, or tar archives, and files joined together to make a path.
Why doesn’t Ant have a simple, consistent model of data sources?
The answer is simple: nobody was thinking that far ahead.
Solving one problem at a time, Ant has slowly acquired all the different types without any unified model of them.
Everything you can use as a data source is a resource, and everything that groups resources together is a resource collection.
A resource collection is something that holds resources? What does that mean? Well, it means that there are some common things that can be grouped together and fed into tasks.
A task that’s “resource-enabled” can accept one or more resourcesand work with the content the resource refers to—without worrying about what kind of resource it’s dealing with.
If it had been in Ant from the outset, we’d have a simpler conceptual model to explain.
We’re going to stick to the first view for now, the classic model of how Ant works.
While the vision of resources is appealing—you can feed any data source or destination to a task—the reality is that most things work best with filesets and filelists, because the code behind most tasks expects to read and write files, using the files’ timestamps to provide dependency information.
We’ve probably scared readers off the Ant datatypes, but we’ve tried to give a brief yet complete introduction to them.
Here are some things that we recommend in order to make effective use of them:
Matt Benson—he managed to do a major rewrite of Ant’s internals with almost nothing breaking!
Ant properties are the key to making builds customizable and controllable.
Use the value variant for other string values, including filename fragments if needed.
You should have to declare a path or fileset only once.
Using <filterset> to perform simple text substitutions during a build can accomplish powerful things like inserting dates or other dynamic build-time information.
Be careful not to use it on binary files, however.
Use conditions and conditional targets and tasks to adapt to the environment.
Build files can be made more robust, or fail with better diagnostics messages.
Carefully consider the directory structure of your project, including how properties will map to top-level or subordinate directories.
By planning this well, a parent build can easily control where it receives the output of the child build.
A simple, yet effective, way to manage JAR files in a single project is to place them all in a subdirectory, usually called lib, and include all these JAR files in the classpath used to build and run programs.
This chapter has introduced the foundational Ant concepts of paths, filesets, patternsets, filtersets, properties, and references.
Our compilation step utilizes all of these facilities, either directly or indirectly:
We use a property, build.debug, to control whether compilation is performed with debug on or off.
Typically, the includeAntRuntime value should be set to no, but our compilation is building a custom Ant task and requires ant.jar.
All files in the src tree are considered for compilation because no excludes or explicit includes were specified.
From this chapter, several important facts about Ant should stick with you throughout this book and on into your build file writing:
Ant uses datatypes to provide rich, reusable parameters to tasks.
Filesets represent a collection of files rooted from a specified directory.
Tasks that operate on sets of files often use Ant’s fileset datatype.
Patternsets can be defined and applied to any number of filesets.
The actual element names used for datatypes within a task may vary, and a task may have several different elements all using the same datatype.
Ant’s documentation clearly defines the types each attribute and element represents and is the best reference for such details.
They provide a mechanism to store variables and load them from external resources including the environment.
Several additional datatypes have been introduced, but we haven’t provided a lot of detail yet.
We’ll cover them as it’s time to use them; look to chapter 5 in particular.
You already should have a general knowledge of Ant’s abstractions, which will enable you to define your build process at a high level.
Over the next few chapters, we’ll show you how to do just that.
With Ant’s datatypes introduced and the <javac> task thoroughly explored, we know how to build a simple program.
It’s time to start the main project of the book, which is what the next chapter will do—a diary application and web site.
More importantly, the next chapter will cover writing and running the tests for the main application, as we’re going to write our code test first!
You can write code, but unless you’re going to write tests or formal proofs of correctness, you have no way of knowing if it works.
You can pretend it does, but your end users will discover the truth.
Unless you like fielding support calls, you need to be testing your code.
JUnit provides the test framework to write your tests in Java, and Ant runs the tests.
At this point we’ve learned how Ant can build and run an application.
This chapter is going to look at testing, introduce JUnit, and show you how and.
It will introduce the program that will be developed through the book.
Using this program—a diary—it will show how Ant can integrate this testing into the build process so that every time you type ant, the source is compiled and tested.
This makes it easy to run the tests and makes it impossible to ship code that fails those tests.
Testing is running a program or library with valid and invalid inputs to see that it behaves as expected in all situations.
Many people run their application with valid inputs, but that’s just demonstrating that it can be made to work in controlled circumstances.
Testing aims to break the application with bad data and to show that it’s broken.
Automated testing is the idea that tests should be executed automatically, and the results should be evaluated by machines.
Modern software development processes all embrace testing as early as possible in the development lifecycle.
To show that code works With a test, you can pass in parameters that are designed to break the program, to stress it at the corners, and then you can see what happens.
A well-written test often reveals bugs in your code that  you can fix.
To replicate bugs If you can write a unit test that replicates the bug, you are one step closer to fixing it.
You have a way of verifying the problem, and you have some code you can step into to find out what’s going wrong.
The best part? The test remains, stopping the bug from creeping back.
The complexity of a modern system implies that you need to have studied something like the pi-calculus to stand a chance.
Even if you have the skills, are you really going to prove everything still works after every change?
To test on different platforms If you’ve automated tests, you can run them on all target platforms.
That includes Java versions, application/web server releases, and operating systems.
People always criticize Java as “write once, test everywhere,” but once testing becomes easy, testing everywhere becomes possible.
At that point, you really do have code that runs everywhere.
To enable regression testing A new Java release comes out every six to twelve months.
Libraries in a big project may be updated every month or two.
How do you know that your program still works whenever a piece is upgraded? Regression testing, that’s how.
Regression tests verify that an application still works the way it used to, that there have been no regressions.
All bug-replication tests become regression tests the moment the bug is fixed.
Refactoring is now a well-known concept: the practice of rearranging your code to keep it clean and to help it adapt to change.
As defined by Martin Fowler, refactoring is “the restructuring of software by applying a series of internal changes that do not affect its observable behavior” (Fowler 1999)
That is, it changes the internals of a program without changing what it does.
If you’re going to refactor your program, large portions of your source can change as they’re moved around, restructured, or otherwise refactored—often at the click of a button in the IDE.
After you make those changes, how can you be sure that everything is still working as before? The only way to know is through those tests that you wrote before you began refactoring, the tests that used to work.
Instead of writing code and hoping that it works, or playing with a program by hand to reassure yourself that all is well, testing can show how much of a program really is working.
Ant goes hand in hand with this concept, because it integrates testing with the build process.
If it’s easy to write tests and easy to run them, there’s no longer any reason to avoid testing.
This is the first place in our book where we delve into the application that we built to accompany this text.
We’re going to use this application through most of the remaining chapters.
Why is the “testing” chapter the right place to introduce our application? Because the tests were written alongside our application: the application didn’t exist until this chapter.
We’re going to write a diary application that will store appointments somewhere and print them out.
Later on, the diary will save data to a database and generate RSS feeds and HTML pages from that database.
We’ll add these features as we go along, extending the application, the tests, and the build file in the process.
Using an agile process doesn’t mean we can skip the design phase.
We just avoid overdesigning before implementing anything we don’t immediately need.
Accordingly, the first step for the application is to sketch out the architecture in a UML tool.
The core of our application will be the Events class, which will store Event instances.
Every Event must have a non-null id, a date, and a name; extra text is optional.
The operation Event.equals() compares only the ID values; the hashCode() value is also derived from that.
The Event.compareTo operator is required to have the same semantics as the equals operator, so it too works only on the ID value.
To sort events by time, we must have a special Comparator implementation, the DateOrder class.
Oh, and the ID class itself is the java.util.UUID GUID class.
We aggregate events in our Events class, which provides the manipulation options that we currently want.
Provided all elements in the collection are serializable, all the serialization logic is handled for us.
Two methods, Events.load() and Events.save() aid serialization by saving to/from a file.
In keeping with XP philosophy, we avoid writing features until they’re needed; if they’re not needed, we omit them.
This is also why the class exports very few of the operations supported by its internal map; initially it exports only size() and iterator()
That’s a sketch of the initial design of our program.
We just have to think about how to test the program before we write a line of code, as that’s the core philosophy of test-first development.
Figure 4.1 UML diagram of the core of our diary.
Interfaces and classes in grey are those of the Java libraries.
We’re going to assume they work and not test them ourselves.
Test-first development means writing the tests before the code, wherever possible.
Why? Because it makes developers think about testing from the outset, and so they write programs that can be tested.
If you put off testing until afterwards, you’ll neglect it.
When someone does eventually sit down to write a test or two, they’ll discover that the application and its classes may be written in such a way that its almost impossible to test.
The classic way to show that a program works is to write special main methods on different classes, methods that create an instance of the class and check that it works as expected.
For example, we could define a main method to create and print some files:
It will print something we can look at to validate:
You don’t just want to test an individual class once.
You want to test all your classes, every time you make a change, and then have the output of the tests analyzed and presented in a summary form.
Manually trying to validate output is a waste of time.
You should also want to have your build stop when the tests fail, making it impossible to ship broken programs.
The assistant is JUnit, a test framework that should become more important to your project than Ant itself.
The two tools have a longstanding relationship: JUnit made automated testing easy, while Ant made running those tests part of every build.
Unit tests test a piece of a program, such as a class, a module, or a single method.
They can identify problems in a small part of the application, and often you can run them without deploying the application.
System tests verify that a system as a whole works.
A server-side application would be deployed first; the tests would be run against that deployed system, and may simulate client behavior.
Acceptance tests verify that the entire system/application meets the customers’ acceptance criteria.
Performance, memory consumption, and other criteria may be included above the simple “does it work” assessment.
These are also sometimes called functional tests, just to cause extra confusion.
Regression testing means testing a program to see that a change has not broken anything that used to work.
JUnit is a unit-test framework; you write tests in Java to verify that Java components work as expected.
It can be used for regression testing, by rerunning a large test suite after every change.
It can also be used for some system and acceptance testing, with the help of extra libraries and tools.
JUnit is one of the most profound tools to arrive on the Java scene.
It single-handedly took testing mainstream and is the framework that most Java projects use to implement their test suites.
If you consider yourself a Java developer and you don’t yet know JUnit, now is the time to learn.
If you wish to explore JUnit in much more detail, we recommend JUnit in Action by Vincent Massol.
Ant integrates JUnit into a build, so that you don’t neglect to run the tests and to give you nice HTML reports showing which tests failed.
JUnit is a member of the xUnit testing framework family and is now the de facto standard testing framework for Java development.
JUnit, originally created by Kent Beck and Erich Gamma, is an API that makes it easy to write Java test cases, test cases whose execution can be fully automated.
The archive contains junit.jar—which is the JAR file you need—the JavaDocs, and the source (in src.jar)
JUnit 3.x has been stable for so long that many tools.
However the generated reports aren’t perfect, as Ant is still running and reporting the tests as if they were JUnit 3.x tests.
A new task for JUnit 4 is needed, one that will probably be hosted in the JUnit codebase itself.
The abstract TestCase class is of most interest to us.
The Assert class provides a set of assertions that methods in a test case can make, assertions that verify that the program is doing what we expect.
Test case classes are what developers write to test their applications.
Figure 4.2 JUnit UML diagram depicting the composite pattern utilized by TestCase and TestSuite.
A TestSuite contains a collection of tests, which could be either more TestSuites or TestCases, or even classes simply implementing the test interface.
The Assert class provides a set of static assertions you can make about your program.
The first thing we must do with JUnit is write a test case, a class that contains test methods.
Provide a constructor, accepting a single String name parameter, which calls super(name)
Write some public no-argument void methods prefixed by the word test.
Here is one such test case, the first one for our application:
We have a single test, testCreation, in which we try to create an event.
Until that class is written, the test case won’t compile.
If the Event constructor throws a RuntimeException, the test won’t work.
Merely by trying to instantiate an object inside a test case, we’re testing parts of the application.
With the test case written, it’s time to run it.
Test cases are run by way of JUnit’s TestRunner classes.
From the Windows command-line, we could run the text TestRunner like this:
The ‘.’ character indicates a test case is running; in this example only one exists, testCreation.
The Swing TestRunner displays success as green and failure as red.
For this same test case, its display appears in figure 4.3
Ant uses its own TestRunner, which runs the tests during the build, so the GUI isn’t needed.
Ant can do something the GUIs can’t do: bulk-test hundreds of tests and generate HTML reports from the results.
That is our goal: to build and test our program in one go.
A test method within a JUnit test case succeeds if it completes without throwing an exception.
A test terminates with an error if the method throws any other kind of exception.
Anything other than success means that something went wrong, but failures and errors are reported differently.
These aren’t Java assert statements, but inherited methods that you place in tests.
Most of the assertion methods compare an actual value with an expected one, or examine other simple states of Object references.
There are variants of the assert methods for the primitive datatypes and the Object class itself.
Diagnosing why a test failed is easier if you provide meaningful messages with your tests.
Figure 4.3 JUnit’s Swing GUI has successfully run our test case.
If  there was a red bar, we would have a problem.
Table 4.1 Assertions that you can make in a JUnit test case.
Every test case should use these assertions liberally, checking every aspect of the application.
Using the assertions To use the assertions, we have to write a test method that creates an event with sample data, then validates the event:
The complementary operation asserts that a reference is not null.
There are overloaded versions of this method for all Java’s primitive types except floating point numbers.
The values are deemed equal if the difference is less than the delta supplied.
If an infinite value is expected, the delta is ignored.
This is a stricter condition than simple equality, as it compares the object identities using expected == actual.
Asserts that the two objects have different identities, that is, expected != actual.
Unconditional failure, used to block off a branch of the test.
It’s important to keep test methods as simple as you can, with many separate test methods.
This makes analysis easier: there should be only one reason for any test to fail.
For thorough testing, you need lots and lots of tests.
In a well-tested project, the amount of test code may well be bigger than the main source itself.
Everyone on the team needs to think it’s important—all the developers, all the management.
If anyone thinks that writing tests is a waste of time, you won’t get the support or coverage you need.
You’ll need to convince them otherwise by showing how testing gets applications working faster than shipping broken code and waiting for support calls.
The only way to do that is to write those tests, then run them.
The lifecycle of a TestCase JUnit runs every test method in the same way.
It enumerates all test methods in a test class (here, our LessSimpleTest class) and creates an instance of that class for each test method, passing the method name to the constructor.
Then, for every test method, it runs the following routine:
That is, it calls the method public void setUp(), runs the test method through some introspection magic, and then calls public void tearDown()
The results are forwarded to any classes that are listening for results.
You can add any number of test methods to a TestCase, all beginning with the prefix test.
You can use this trick to turn off tests or to write helper methods to simplify testing.
Test that Event.equals() works for comparing an object to itself.
Test that the date we supplied in the constructor was used.
Evaluate event.toString() and verify that the name of the event is returned.
Test methods can throw any exception they want: there’s no need to catch exceptions and turn them into assertions or failures.
What you do have to make sure of is that the method signature matches what’s expected: no parameters and a void return type.
If you accidentally add a return type or an argument, the method is no longer a test.
For example, with IntelliJ IDEA you can map something like “test” to:
This creates a stub test method and prompts us to fill in the name.
Declaring that it throws an Exception lets our test throw anything.
To summarize: the setUp and tearDown methods are called before and after every test method and should be the only place where you prepare for a test and clean up afterwards.
Once the tests are written, it’s time to run the tests.
Ant has a task to run the JUnit tests called, not surprisingly, <junit>
Core tasks are built into ant.jar and are always available.
Optional tasks are tasks that are supplied by the Ant team, but are either viewed.
Older versions of Ant required junit.jar to be in Ant’s classpath by placing it.
Ant 1.7 has changed this, so that now a copy of JUnit in the classpath that you set up for compiling and running the test code is all that’s needed.
Unfortunately, a lot of existing build files assume that the junit.jar is always on Ant’s classpath, so they don’t bother to add it.
Given how important JUnit is, you may as well copy it and place it where Ant can load it.
This is a good time to introduce how Ant adds libraries to its classpath.
When you type ant at the command line, it runs Ant’s launcher code in antlauncher.jar.
This sets up the classpath for the rest of the run by.
Adding every JAR listed in the CLASSPATH environment variable, unless the -noclasspath option is set.
Adding every JAR in every directory listed on the command line with the -lib option.
All we need to do then is download junit.jar, stick it in the correct place and have Ant’s <junit> task pick it up.
This is something that can be done by hand, or it can be done by asking Ant to do the work itself.
Interlude: how to automatically fetch JAR files for use with Ant.
If you’re online and a proxy server is not blocking outbound HTTP connections, change to Ant’s home directory, then type:
This code runs a special build file that fetches all the libraries that Ant needs and saves them in ANT_HOME/lib.
If you need to save the JAR files in your personal .ant/ lib directory, add the -Ddest=user clause.
If you work in a security-sensitive organization, you shouldn’t download and install files without first authenticating them.
You might even want to consider downloading the source and building the files yourself.
This tells Ant to look at its internal state and print out useful information.
It also prints out system properties, including Ant’s Java classpath.
If there is no junit.jar listed there, the <junit> task will work only if it’s explicitly added to the test classpath.
Now let’s write the first bit of our Event class.
Once we implement this much of our class, the tests compile.
Let’s get Ant building and running the tests to see if our implementation is adequate.
The <junit> task is an “optional” task, one that is so important you must have it and junit.jar in your Ant distribution.
The task runs one or more JUnit tests, then collects and displays the results.
It can also halt the build when a test fails.
To execute the test case that we’ve just written via Ant, we can declare the task with the name of the test and its classpath, like this:
First, our code is broken and second, we need <junit> to tell us what failed and stop the build afterwards.
Before fixing these problems, we need to get our directory structure and Ant build file set up to accommodate testing.
Once you start writing tests, you have two sets of source files for every project, the main source and the test source.
It’s essential to control how everything is laid out in the file system to avoid contaminating the release software with test code.
Test code must be kept separate from production code to keep the test code out of the production binary distributions and to let you compile the tests and source separately.
Tests should also use a Java package hierarchy, as with normal source.
One strategy is to place tests in their own source tree but in the same package as the codes they test.
This gives the tests package-level access privileges to the code being tested so the tests can test classes and methods that aren’t public.
The other strategy is to place tests in their own source tree, in different packages.
This forces the tests to use the public API of the classes.
It also keeps the tests running when the main files are archived into signed JAR files.
Tests are put into test packages under the packages they test.
It’s essential to keep project source and test source separate and to be consistent across projects.
Ant can use this proposed layout to build and package the source and test classes separately and to keep all generated files, including test results, away from the original content.
With this new layout, we need to add a few additional targets to initialize the testing directory structure, compile the test code, and then execute the tests and generate the reports.
Figure 4.4 illustrates the target dependency graph of the build file.
We use Ant properties and datatypes to make writing our test targets cleaner, to avoid hard-coded paths, and to allow flexible control of the testing process.
First, we assign properties to the various directories used by our test targets:
We need a different classpath for our tests than for the main source.
We need JUnit’s JAR file for compilation and execution, and the test/classes directory for execution.
How do we do this? We  rely on junit.jar being on Ant’s classpath.
As long as we include Ant’s classpath in our tasks, we get JUnit for free.
This is cheating and only works provided all projects can use the same version of JUnit.
Tests can be compiled only after the main source is compiled; the test run depends on the tests being compiled.
To compile the tests, we need to add in the compiled classes:
To run the tests, we also need the compiled tests on the classpath:
If we add a dependency to the core project, the classpaths to compile and run the tests pick it up immediately.
Before compiling tests, we need to create the relevant directories:
Most unusually, it deletes two of them before recreating them.
This is a brute-force purge of the directories’ contents to make sure the results of previous test runs aren’t mixed in with the latest run.
The alternative is to add junit.jar to the projects lib directory.
Having set up the directories and the various classpaths, we’re ready to set up.
If we care about testing, the build must stop when the tests fail.
We don’t want to package or release a program that doesn’t work.
By default, <junit> doesn’t halt the build when tests fail.
There’s a reason for this: you may want to format the results before halting.
For now, we can set the haltonfailure attribute to true to stop the build immediately.
The build halted because the test case failed, exactly as it should.
The summary output provides slightly more details: how many tests were run, and how many didn’t pass.
We need more information than this, which <junit> will gladly provide.
To analyze why tests fail, we need to see the results in detail, including the names of the failing tests, stack traces of exceptions and anything printed to the output and error streams.
To get detailed console output, we change the task slightly:
We turn off the printsummary option because it duplicates and interferes with the console output.
Looking at the source of this test, we can find the line that caused the problem:
It is the test of toString() that is failing, because we forgot to include the event name in the string.
We can fix that now, and we’ll know when it’s fixed, as the tests will pass and the <junit> task will not fail.
Detailed test results are the best way of determining where problems lie.
The other thing that can aid diagnosing the problem is the application’s output, which we can also pick up from a test run.
Lots of Java code prints information to the console, either directly to System.out and System.err or via logging libraries.
With no formatters specified and printsummary either on or off, the <junit> task swallows all console output.
If printsummary is set to "withOutAndErr", <junit> will forward everything through Ant’s console.
Printed output is great for diagnostics after a test has failed.
The next thing we need to do is run all the tests in the project in a single <junit> task.
Developers should not have to edit the build file when adding new test cases.
Likewise, you can write TestSuite classes, but again, who wants to edit Java source unless they need to? Why not delegate all the work to the machine?
You can nest filesets within <batchtest> to include all your test cases.
The normal naming-convention scheme calls for test cases to end with the word “Test”
Here, SimpleTest and LessSimpleTest are our test cases, and CoreTestCase is the abstract base class.
We use TestCase as the suffix for our abstract test cases and declare the classes as abstract so that IDEs do not try to run them either.
The <junit> task fails if it’s told to run a class that’s abstract or that isn’t a test case.
Setting up a batch test pattern makes adding new test cases to a project trivial.
Developers can now add new test cases without editing the build file.
The easier it is to add and run test cases, the more likely they are to be written.
Making it easy to add new test cases has one little side effect.
Before long, there will be many test cases, which results in an increase in the time to run the tests and an explosion in the size of the test output.
Developers cannot be expected to sit staring at the console watching results scroll by, so we need a better way of presenting the results—such as HTML pages.
Being able to read the text output is useful, as you encounter these results quite often, including in emails from other people.
This formatter creates an XML file for every test class.
It applies some XSL transformations to the XML files generated by the XML <formatter>, creating HTML files summarizing the test run.
You can browse tests, see which ones failed, and view their output.
You can also serve the HTML files up on a web site.
The <report> element instructs the transformation to use either frames or noframes Javadoc-like formatting, with the results written to the todir directory.
Figure 4.5 shows the framed report of our test run.
Navigating to a specific test case displays results such as those shown in figure 4.6
You get a summary of all tests and can zoom in on a test case of interest.
Note the timestamp field; it can warn you of old results.
The hostname is there in case you’re collating results from different machines.
Firstly, <junit> doesn’t have any dependency logic; it always runs all tests.
The main page summarizes the test statistics and hyperlinks to test case details.
We’ve had to turn off haltonfailure in order to run <junitreport>
How can we generate reports and stop the build if the tests failed?
To halt the build after creating the HTML pages, we make the <junit> task set an Ant property when a test fails, using the failureProperty and errorProperty attributes.
A test failure means an assertion was thrown, while an error means that some other exception was raised in the test case.
Some teams like to differentiate between the two, but we don’t.
We just want to halt the built if a test failed for any reason, which we can ask for by naming the same property on both attributes.
Using the properties set by <junit>, we can generate the reports before we fail the build.
Listing 4.1 shows the complete target needed to run the tests, create the reports, and halt the build on failure.
Figure 4.6 Test case results showing the assertion that failed, and the stack trace.
Keep an eye on the Time Stamp to make sure you‘re not viewing old test results.
Running this target will run the tests, create the report, and halt the build if any test raised an error or failed.
It even prints the name of the directory into which the reports went, for pasting into a web browser.
The HTML reports are the nicest way to view test results.
Be aware that the XSL transformation can take some time when there are a lot of tests to process.
Sometimes we split the testing and report creation in two in order to let us run the tests without creating the HTML reports.
With HTML output, the core of Ant’s JUnit coverage is complete.
Projects can use what we’ve covered—batch execution of test cases and HTML output—for most of their needs.
There are a few more advanced topics that may be of interest.
Once you have testing up and running in a project, come back to this section and think about using them to improve the test process.
Before closing off our JUnit introduction, there are a few more tricks to know about running JUnit under Ant.
These are all optional extras, described in no particular order, but useful to have on hand when needed.
To run a single test case, we just define the name of the test on the command line by setting the testcase property:
This is a good technique for a big project—even Ant’s own build file does it!
Running JUnit in its own JVM The <junit> task, by default, runs within Ant’s JVM.
It’s a lot more robust to run tests in a new JVM, which we can do with the attribute fork="true"
This term, forking, comes from Unix, where a process can fork into two identical processes.
Java doesn’t implement Unix’s fork() operation, but Ant comes close and uses the term in some of its tasks.
Use a different JVM than the one used to run Ant (jvm attribute)
Test different instantiations of a singleton or other situations where an object.
Set up a path for loading native libraries and test Java Native Interface (JNI) code.
We like to fork our code because it makes things more robust; the test run cannot break Ant, and Ant’s state doesn’t affect the test code.
Running tests in a new process does cause some problems, because the classes needed by the formatters and the test cases themselves must be in the classpath.
One way to do this is to adjust the classpath for running tests:
If you want to be more selective about classpaths, include ant-junit.jar and junit.jar.
You also need the Apache Xerces XML parser or an equivalent.
Since this is now built into the Java runtime, there’s no need to explicitly ask for it.
What happens when <junit> forks? Unless you say otherwise, every test case class is run in its own JVM.
This can be slow, especially when there’s a lot of static startup code in the application being tested.
You can control this behavior with the forkMode attribute, which takes any of the values listed in table 4.3
The once option starts a new JVM for every set of tests which have matching JVM configuration options, so it may need to start a few if the <junit> declaration is complex.
It does run tests faster and should be played with.
Just don’t rely on a single JVM lasting for all tests.
Passing information to test cases One common problem in a test run is passing configuration information down to the test.
How can test cases be configured? It may be OK to hard code values into the test source, but not things that vary on a per-system basis, such as the path to a file on the local drive or the port that a server is listening on.
To configure tests dynamically, we pass the information down as Java system properties, using the <sysproperty> element.
This is the equivalent of a -D argument to a Java command-line program.
It is often useful to have a helper method that asserts that the property actually exists:
If you use this method, any absent property is detected immediately and raised as a test failure.
Filename properties can be handled with an extension of this technique that looks for a property naming a file that must exist:
Checks like these are valuable when you run JUnit from IDEs, as they’ll have their own way of configuring properties, and a bit of rigor here stops problems from sneaking in.
We can also use Java language assertions in JUnit tests.
There’s no benefit in using Java assert statements inside JUnit tests, but libraries may contain them.
Enabling Java Assertions You can enable Java assertions in a forking <junit> task; the request is ignored when fork="false"
If your libraries have assertions, turn them on during testing!
If you reach the limit of what XML+XSLT can do in terms of report generation, there’s one more option: writing your own result formatter.
Creating your own test result formatter The <formatter> element has an optional classname attribute, which you can specify instead of type.
Examine the code of the existing formatters to learn how to develop your own, if you ever have the urge.
It’s easy to learn, and once you start using it you cannot help wondering why you never used it before.
Ant makes unit testing simple by running JUnit, capturing the results, and halting the  build if a test fails.
You can design programs to make them easier to test; you can also design programs to make testing nearly impossible.
Give them each their own unique directory tree with an appropriate package-naming structure.
If all your tests pass the first time, you’re probably not testing vigorously enough.
Add a new test case for every bug you find.
When a test case fails, track down the problem by writing more tests before going to the debugger.
It’s better to know that testFileLoad failed, rather than test17 failed.
From an Ant perspective, the most important thing to do is write build files that run tests every build.
Don’t make running the tests something only one person in the team does, and then only once a week.
If you need to retrofit tests to an existing application, don’t panic.
Before adding new code, write tests to validate the current behavior and verify that the new code doesn’t break this behavior.
When a bug is found, write a test case to identify it clearly, then fix the bug and watch the test pass.
Keep at it and you’ll slowly build up test coverage of the application.
Now, there’s one more little topic to cover  before we finish this chapter: JUnit versions.
The JUnit 3 branch of JUnit is the version of the framework most broadly used and is widely supported in Ant, IDEs, and other tools.
It lets developers use Java 5 annotations as a way of marking up test classes and methods.
Why hasn’t this book adopted the latest version of JUnit?
The main reason for sticking with the JUnit 3.8 branch is because it’s been so successful that it has become integrated with much of the Java development infrastructure.
There are other extensions which have yet to migrate, such as HttpUnit, XmlUnit, and DbUnit.
If you need these extensions, you need to stick with the JUnit 3.8.x branch.
When will these extensions move to JUnit 4? We don’t know.
It also runs under Java 1.4, using Javadoc annotations for test markup.
TestNG has its own task, which is built into the test framework.
The XML reports that the task generates feed into Ant’s existing <junitreport> task, so TestNG integrates with Ant as well as JUnit does.
However, it has some interesting features, and anyone thinking of moving to JUnit 4 should also look closely at TestNG before making a decision.
In this chapter, we started writing a diary application by writing the JUnit tests for these classes at the same time as the application itself and integrating the tests into the build process so that Ant compiles both source trees and runs the tests, generating HTML report pages.
We’ll extend this application through the book, packaging and running it, redistributing it, then using it in a web application with the Enterprise Java APIs adding persistence.
Throughout all these changes, we’ll know the moment any change breaks the basic functionality of the system, because the test cases written in this chapter will highlight problems the moment any change breaks the core.
This is why testing is better than debugging manual “experiments” with an application: tests pay for themselves many times over.
Test-centric development transforms how you write code, usually for the better.
JUnit is Java’s main testing framework; it integrates tightly with Ant.
Tests are most useful if you run them after every change, which is where Ant joins in.
The <sysproperty> element can pass information from Ant to the test cases.
JUnit makes it so easy! The benefits of tests are so tangible! Yet again and again, we join some project, ask for the tests and hear, “Oh, we haven't written them yet.” Writing tests takes time, but so does debugging.
Moving to a test-centric process is a key way to improve product quality.
For that reason, if there’s one message we want readers to remember from this entire book, it is not “We should build with Ant”; it is “We should build and test with Ant.”
We want to take the compiled classes and create a JAR file that can itself be bundled into some source and binary redistribution packages—such as Zip and tar files—for different platforms.
We will then be able to execute the JAR file and upload the Zip and tar files to servers.
We are effectively releasing our diary as a library, on the basis that having passed its tests, it’s ready for use.
What else does a project need to do before releasing a Java program?
It can be used inside our application, or i can be redistributed for other people to use.
Write any installer configuration files, such as Java Web Start files.
Build the application and package it into a JAR file.
Bundle the JAR, the documentation, and any other files into redistributable.
It can take the source files and create .class files and JavaDoc documentation, then package everything up as redistributables.
Along the way it can copy, move, and delete files and directories.
The build file can already compile the source and run the tests; in this chapter we’ll look at the rest of the packaging problem.
We’ll start with file system operations to get everything into the right place.
The basis of the packaging and deployment process is copying and moving files around.
Ant has a set of tasks to do this, most of which operate on filesets.
We can use them to prepare directories and files for the packaging steps in the build.
Creating Directories Before we can distribute, we need a destination for our distributable files.
Let’s create a subdirectory dist with another doc for documentation under it.
As usual, we declare these locations through properties to provide override points.
Figure 5.1 The packaging process: a JAR library consists of getting the source and data files into the JAR and the documentation into a directory, then creating the Zip and tar packages for distribution.
The <mkdir> task creates all parent directories in its dir attribute, so when the task is executed with dir= "dist/doc", the whole directory tree would be created on demand.
This same recursive creation applies to deletion, where the entire distribution directory can be deleted all at once.
It can just as easily delete an entire directory with.
This task is dangerous, as it can silently delete everything in the specified directory and those below it.
If someone accidentally sets the dist.dir property to the current directory, then the entire project will be destroyed.
For more selective operations, <delete> takes a fileset as a nested element, so you can specify a pattern, such as all backup files in the source directories:
Usually, filesets ignore the editor- and SCM-generated backup files that often get created, but when trying to delete such files you need to turn off this filtering.
Three attributes on <delete> handle failures: quiet, failonerror, and deleteonexit.
The task cannot delete files if another program has a lock on the file, so deletion failures are not unheard of, especially on Windows.
When the failonerror flag is true, as it is by default, Ant halts the build with an error.
If the flag is false, then Ant reports the error before it continues to delete the remaining files.
You can see that something went wrong, but the build continues:
The quiet option is nearly the exact opposite of failonerror.
When quiet= "true", errors aren’t reported and the build continues.
Setting this flag implies you don’t care whether the deletion worked or not.
You can’t rely on this cleanup being called, but you could maybe do some tricks here, such as marking a file that you know is in use for delayed deletion.
Things may not work as expected on different platforms or when Ant is run from an IDE.
Its role in packaging is to clean up destination directories where files can go before adding the directory contents to JAR, Zip, or tar archives.
At its simplest, you can copy files from one place to another.
You can specify the destination directory; the task creates it and any parent directories if needed:
You can also give it the complete destination filename, which renames the file during the copy:
To do a bulk copy, declare a fileset inside the copy task; all files will end up in the destination directory named with the todir attribute:
By default, <copy> is timestamp-aware; it copies only the files that are newer than those of the destination.
At build time this is what you want, but if you’re using the task to install something over a newer version, set overwrite="true"
Doing so can stop other tasks from thinking that files have changed.
If you want to change the names of files when copying or moving them, or change the directory layout as you do so, you can specify a <mapper> as a nested element of the task.
The <chmod> task can be used to set permissions after a copy—a task that is a no-op on Windows—so it can be inserted where it’s needed.
Similarly, Ant cannot read permissions when creating a tar archive file, a problem we’ll solve in a different way.
It first tries to rename the file or directory; if this fails, then it copies the file and deletes the originals.
An unwanted side effect is that if <move> has to copy, Unix file permissions will get lost.
As with <copy>, this task uses timestamps to avoid overwriting newer files unless overwrite="true"
The <move> task is surprisingly rare in build files, as copying and deleting files are much more common activities.
Its main role is renaming generated or copied files, but since <copy> can rename files during the copy process and even choose a different destination directory, there’s little need for the task.
We’ve shown how filesets can select files to copy or move, but what if you want to rename them as they’re moved? What if you want to flatten a directory so that all JAR files are copied into one single directory? These are common operations in preparing files for packaging.
Ant’s mappers generate a new set of filenames from source files.
Any time you need to move sets of files into a new directory hierarchy, or change parts of the filename itself, such as an extension, look for an appropriate mapper.
Mappers implement file-renaming algorithms, telling tasks like <copy> how files should be renamed during the operation.
Mappers implement file-renaming algorithms, telling tasks like <copy> how files should be renamed during the operation.
Mappers are powerful, and it’s worthwhile looking at them in detail.
If a project has any need to rename files and directories or move files into a different directory tree, a mapper will probably be able to do it.
It’s used when a task needs a mapper, but you don’t need to do any filename transformations:
Because it’s the default mapper of <copy>, the following declarations are equivalent:
It’s fairly rare to see the identity mapper because you get it for free.
The next mapper, the flatten mapper, is used when collecting files together in a.
The flatten mapper strips all directory information from the source filename to map to the target filename.
This is one of the most useful mapping operations, because it collects files from different places and places them into a single directory.
If we wanted to copy and flatten all JAR files from a library directory hierarchy into a single directory ready for packaging, we would do this:
If multiple files have the same name in the source fileset, only one of them will be mapped to the destination directory—and you cannot predict which one.
Although it copies everything to a single directory, the flatten mapper doesn’t rename files.
To do that, use either the glob or regexp mapper.
Glob mapper The very useful glob mapper can do simple file renaming, such as changing a file extension.
It has two attributes, to and from, each of which takes a string with a single asterisk (*) somewhere inside.
The text matched by the pattern in the from attribute is substituted into the to pattern:
The glob mapper is useful for making backup copies of files by copying them to new names, as shown in the following example.
This task declaration will copy all JSP pages from the web directory to the new_web directory with each source .jsp file given the .jsp.bak extension.
If you have more complex file-renaming problems, it’s time to reach for the big brother of the glob mapper, the regexp mapper, which can handle arbitrary regular expressions.
Regexp mapper The regexp mapper takes a regular expression in its from attribute.
Source files matching this pattern get mapped to the target file.
Here’s a simple example of a way to map all .java files to .java.bak files.
The <copy> example for the glob mapper can be rewritten this way:
Quite sophisticated mappings can be done with this mapper, such as removing a middle piece of a directory hierarchy and other wacky tricks.
One conversion is so common it has its own mapper: the package mapper.
Package mapper The package mapper transforms the * pattern in its from attribute into a dotted package string in the to pattern.
The result is a flattening of the directory hierarchy where Java files need to be matched against data files that have the fully qualified class name embedded in the filename.
This mapper was written for use with the data files generated by the <junit> task’s XML formatter.
The package mapper lets you map from Java classnames to these files:
Another use would be to create a flat directory tree of all the source code:
This mapper has an opposite, the unpackagemapper, which goes from dotted filenames to directory separators.
All the mappers covered so far focus on renaming individual files in a copy.
Mappers can do more than this, as they can provide any mapping from source filenames to destination names.
One mapper, the merge mapper, maps every source file to the same destination mapper.
The merge mapper maps all source files to the same destination file, which limits its value in a <copy> operation.
This is a task that compares a fileset of source files to a mapped set of destination files and sets a property if the destination files are as new as the source files.
With the merge mapper, <uptodate> lets us test if an archive file contains all the latest source files:
The property will not be set if the Zip file is out of date, a fact that can be used to trigger the execution of a conditional target that will create the Zip file only on demand.
Mappers can also go the other way, generating multiple names from a single source, which lets you map a source file to multiple destinations.
Composite mapper The composite mapper takes multiple mappers inside it and returns the result of mapping the source file to every mapper.
The more source files you have, the more mapped filenames you end up with: it’s the “or” operation of mapping.
There’s one other mapper that takes nested mappers, the chained mapper.
Chained Mapper The chained mapper lets you chain together a list of other mappers to create the final set of filenames.
There are a few more mappers listed in the documentation but rarely used.
The script mapper lets you describe the mapping logic in any scripting language that Java supports.
If you have a complex renaming problem that the regexp mapper can’t handle, script mapper offers you a way to solve it without writing a new mapper in Java.
We can even have the tasks change the contents of the files by filtering them.
It’s good to customize the text files that go with a library, inserting the current date and version into them.
These are the files that people read, and people like to know they have the current release.
We can customize text files in Ant by patching the files on the fly.
When filtering, the tasks replace tokens in the file with absolute values.
In our diary project, we have a text file called doc/readme.html with some release notes.
When creating the distribution packages, we want to insert a timestamp into this file.
The <tstamp> task sets the DSTAMP and TSTAMP properties to the current date and time.
The other trick in the task declaration is that it disables dependency checking by.
This is because we want the filtered copy to always overwrite the destination file, even if it exists.
Applying this filtered copy on the HTML template produces the following:
Adding information to a generated file is good for support because you can tell when something was built.
If a project keeps version information in a properties file, the build version can also be included in the filter.
Keeping this data out of text files lets you update the version in a single place and have it propagate to the documentation automatically.
Replacing text in a file can be tricky, which is why Ant’s filters search for the filter tokens within a pair of delimiters.
If you want, you can supply a new prefix and suffix in the filterset declaration.
One thing you must never do is try to filter a binary file; it may get corrupted.
This essentially means do not copy binary files with a filter set.
In the Ant manual, there’s a <filter> task that lets you specify a default filter for every move and copy that follows.
Since this filter applies even to binary files, DO NOT USE IT! Once a global <filter> is set, you cannot copy a binary file without bad things happening.
It’s time to use the tasks to prepare our code for packaging.
When preparing to distribute code, always do a clean build first.
Our clean target should clean up all the output directories:
We’ve opted not to set failonerror= "false" on the deletions, because we’ve chosen to find out if the delete fails.
When run, the clean target guarantees that there’s nothing in the directories that Ant will build into.
The <javac> task will build everything again, so the .class files in build/classes will be current.
We need to accompany those files with any resources in the source tree; we do so with a <copy> action.
Alongside the generated .class files, developers often need to include data files in the Java package hierarchy and, in so doing, copy them into the JAR.
The normal way to work with these files is to keep them all in the source directory and copy them into the compiled classes directory after compiling the Java code.
First, we define a property containing the pattern to include:
We declare this as a property for ease of reuse.
This target copies the data files into the build/classes directory, and so onto the classpath or into the JAR we’re about to make.
Make sure the pattern includes all the files you want to distribute and none that you do not.
If we include resources with our test classes, we need to do the same action, albeit with a different source and destination:
We normally do this copy immediately after compiling the source, right after the <javac> task.
There’s never any situation in which we would not want to copy the resources over, so having a unified target is good.
After compiling the code we need to generate the JavaDoc HTML files that both the source and binary distribution packages can redistribute.
Then we declare a new target to create the documentation.
We aren’t going to cover how to use the <javadoc> task because it would take far too long.
It has 50-odd parameters and over a dozen nested elements that show how complex creating the documentation can be.
The underlying javadoc program has about 25 arguments; the complexity in the task is mainly to provide detailed control as to what that program does.
The task needs only three things—the source directories b, the destination directory c, and a list of files to document.
It also likes the classpath used by <javac> to compile the application d.
The <source> nested element is where you can list the Java files to document, but specifying packages is usually much easier, especially when you can give a wildcard to import an entire tree.
You can specify a package in three ways, as listed in table 5.2
For any complex project, the standard tactic is to list the packages to.
The final option, packagelist, is not usually used; it exists to make it easier to migrate from Ant.
For <javadoc> to work, it needs access to all the libraries used in the application.
If the task cannot resolve references to classes, it prints out warnings and the documentation ends up incomplete.
To minimize this, we pass the task the compile classpath via the classpathref attribute.
By placing the <javadoc> task in a target called javadocs, with a dependency declaration of depends="compile", the task is only called when the classpath is set up and the source compiled.
One big choice to make is what level of methods and fields to document—internal versus external.
We’ve set access="private" to expose everything; the other options being package, protected, and public.
Figure 5.2 shows the result: HTML pages that cover our library.
This documentation can be included in the redistributables, or served up on the.
Existing documents can be packaged as is, or get copied through a filterset to.
There’s one other change for some files: fixing the line endings.
Shell scripts and plain text documentation have one extra need: their lines must end with the appropriate line endings for the target platform.
If that option is not set, the task defaults to setting the line ending of the local system.
For our project, we currently have only a Python script, but we may want to add some more.
These patterns are used in our "scripts" target, which does three things.
It copies the files into the distribution directory, patches the name of the JAR file we are creating, and then fixes the line endings before finally making the Unix files executable:
The <fixcrlf> task overwrites the destination files if the existing line endings don’t match those in the eol attribute.
Unless you specify a directory in the destdir attribute, this will mean it overwrites the original files.
Now, what if we want to create text files and scripts for both Windows and Unix?
Creating multiple versions for multiple targets If you create distributions with scripts and text files that are targeted at multiple operating systems, you may need to create multiple copies of the relevant files, each with the correct line endings in the target.
Listing 5.1 shows a target that takes a readme template and creates the Unix file README and a Windows version, README.TXT, each with the appropriate line.
Tab conversion lets you avoid layout surprises when the recipient views a file in an editor with different tab spacing parameters than normal.
Tab conversion inside Java source files receives special treatment by <fixcrlf>
If you need to do it, look at the javafiles attribute in the documentation.
We prefer to leave redistributed source untouched, as it makes it much easier to incorporate submitted patches.
Creating Javadoc files and patching text files are pretty much the limit of Ant’s built-in document processing facilities.
In chapter 13, we’ll use its XML tasks to create HTML and Java source files from XML source documents, which is another way to create documentation.
There’s another Apache project, Apache Forrest, that provides a tool to create documentation as PDF files and web pages from content written in various XML formats, such as the DocBook and OpenOffice.org formats.
The tool uses Ant to start everything off, and it includes its own version of Ant for that purpose.
Forrest is good for any project that has complex documentation requirements, because you can handwrite documents in the OpenOffice.org word processor and design graphics using an SVG editing tool, such as Inkscape.
This content can be integrated with machine-generated content, all of which can be converted to HTML and PDF and published at build time or served dynamically.
Because Forrest is based on Ant, it can be integrated with an Ant-based build process.
If you have a project with lots of documentation, investigate it.
Listing 5.1 Example target to generate Unix and Windows Readme files from the same original.
It has derivatives, the most common of which are the WAR and EAR files for web applications and Enterprise applications, respectively.
Zip files/archives are the ubiquitous archive files of DOS and, more recently, Windows.
They can store lots of files inside by using a choice of compression algorithms, including “uncompressed.”
The JAR, WAR, and EAR archives are all variations of the basic Zip file, with a text manifest to describe the file and potentially add signing information.
The WAR and EAR files add standardized subdirectories to the JAR file to store libraries, classes, and XML configuration files.
This can make the WAR and EAR files self-contained redistributables.
The full set of packaging tasks have the implementation class hierarchy of figure 5.4
A JAR file stores classes in a simple tree resembling a package hierarchy, with any metadata added to the META-INF directory.
This directory contains a manifest file MANIFEST.MF, which describes the JAR file to the classloader.
At its simplest, it archives an entire tree of files, usually the output of the build.
The task creates a manifest file unless you explicitly provide one.
By default compress= "true", but an uncompressed archive may be faster to load.
Figure 5.4 The implementation hierarchy of Ant’s packaging classes and tasks.
The <jar> task is dependency-aware; if any source file is newer than the JAR file, the JAR is rebuilt.
Deleting source files doesn’t constitute a change that merits a rebuild; a clean build is needed to purge those from the JAR file.
There is an update attribute that looks at dependencies between source files and files stored inside the archive.
It can be used for incremental JAR file updates, in which only changed files are updated.
Normally, we don’t bother with things like this; we just rebuild the entire JAR when a source file changes.
The duplicate attribute tells Ant what to do when multiple filesets want to copy a file to the same path in the archive.
The default option, add, is dangerous because it silently corrupts JAR files.
Ant itself will ignore the duplicate entry, and so will the JDK jar program.
If you don’t want to have users of your application or library making support calls, or want to waste mornings trying to track down these problems in other people’s libraries, change the default value! The most rigorous option is fail, which warns of a duplication; preserve is good for producing good files without making a fuss.
Once created, we need to check that the JAR file contains everything it needs.
Graphical tools may be easier to use, but they have a habit of changing the case of directories for usability, which can cause confusion.
WinZip is notorious for doing this, making any all-upper-case directory lower-case and leading to regular bug reports in Ant, bug reports that are always filed as “INVALID”.1
There is an online bug reporting system for Ant, but all the developers are working in their spare time, for free.
If your bug report gets closed this way, don’t take it personally.
The <unjar> task takes a source file, specified by src, and a destination directory, dest, and unzips the file into the directory, preserving the hierarchy.
It’s dependencyaware; newer files are not overwritten, and the timestamp of the files in the archive is propagated to the unzipped files.
You can selectively unzip parts of the archive, which may save time when the file is large.
To use the task to validate the build process after the archive has been unzipped, you should check for the existence of needed files or, perhaps, even their values:
Here we expand classes in the archive and then verify that a file in the expanded directory tree matches that in the tree of compiled classes.
Binary file comparison is a highly rigorous form of validation, but it can be slow for large files.
To be honest, we rarely bother with these verification stages.
Instead, we include the JAR file on the classpath when we run our unit tests.
If we left something out of the JAR, the unit tests will let us know.
We’ll modify our test run to do this in section 5.6, once the JAR file has the manifest and a signature.
It will contain the manifest version and the version of Ant used to build the file:
Sometimes this isn’t enough, such as when you want to specify the default entry point.
You also need to provide a manifest if you want to add extension libraries, following the even more complex Java extension specification Extension Mechanism Architecture.
Adding a manifest to the JAR file is trivial; point the manifest attribute of the task at a predefined manifest file:
The Sealed: true entry in the manifest marks non-empty packages as sealed b.
The classloader won’t allow any other JAR files to contain classes in the d1.core package, not even our own test classes.
We’ve set our default entry point to a diagnostics class c.
When Ant runs the <jar> task, it will parse and potentially correct the manifest before inserting it into the JAR file.
If the manifest is invalid, you’ll find out now, rather than when you ship.
This process has one weakness: someone has to create the manifest first.
Why not create it during the build process, enabling us to use Ant properties inside the manifest? This is where the <manifest> task comes in.
The outcome of this task will be something like the following manifest, although the exact details depend on who created the file, when they created it, and the version of Ant and Java used:
The task can also be used as an element inside the <jar> task, avoiding the need to save the manifest to a temporary file.
We prefer the standalone action, as it’s easier to examine the generated content.
Another recurrent bug report raised against Ant is that “<jar> wraps long manifest entries,” usually related to the classpath attribute.
The task follows the specification to the letter, especially the rule “No line may be longer than 72 bytes.” If you encounter a problem with something failing to parse a manifest, and the cause is wrapped lines, then it is usually the third-party application that’s at fault.
This is exactly what you want, except in the special case where the system reading the manifest doesn’t comply with the specification.
Some mobile phones with J2ME runtimes suffer from this problem.
The MANIFEST.MF file is the main piece of metadata JAR files use.
Sometimes you may need to add extra content to the META-INF directory, alongside the manifest, which the <jar> task can cope with.
There’s a nested fileset element, <metainf>, which lets you specify the metadata files to add to the JAR.
We still declared the manifest file, even though the <metainf> element appeared to.
There are four things to consider for better <jar> tasks:
Copy all the files you want to include in the JAR into one place before building.
This makes it easier to see what will be included.
Create your own manifest, either in a file or in Ant, and explicitly ask for it with the manifest attribute.
If you leave it to the <jar> task, you get a minimal manifest.
It keeps duplicate entries out of a file and avoids possible problems later on.
Finally, and arguably most importantly, give your libraries a version number at the end.
When we get into repository-based downloading, in chapter 11, you’ll see how versionlabeled JAR files can be used.
It pays off in your own project the moment you start trying to track down bugs related to versions of libraries.
If every JAR file has a version, including your own, it’s much easier to replicate a problem.
Now that we’ve built the JAR, we can sign it.
Signing is useful when you redistribute stuff through Java Web Start or by other means.
It doesn’t take much effort to add signing support, but it’s important to start doing it early on.
Signed JAR files are loaded slightly differently, with the classloader preventing other JAR files from declaring classes in the same packages.
It’s important, therefore, to start signing early on—even if the key is made up just for the purpose.
It avoids our creating fundamental design errors that only show up later on in the project.
To sign JAR files, we need a public/private key pair in a password-protected keystore, and Ant will need that password.
One thing you don’t want to do is put that in the build file itself—anyone with access to the source repository will see it.
Instead, you need the <input> task to prompt the user.
This task pauses the build with a prompt; the user then has to enter a string.
First, it doesn’t work properly in older IDEs or in automated builds, but we can work around that.
More seriously, the string you type in gets echoed to the console.
We cannot avoid this, not until Ant adds a Java 6-specific input handler.
The <chmod> task is encountered very rarely in Ant projects.
Because Java 5 and earlier has no API to read or write Unix file permissions, all of Ant’s file operations drop.
If you want to set permissions on a local file, <chmod> must be the last Ant task to manipulate the file.
With the password in a property, we’re nearly ready to sign the JAR.
Generating a signing key To authenticate JARs in a Java runtime, you have to buy a certificate from one of the approved vendors.
For testing purposes or for private use, you can generate a selfsigned certificate using Sun’s keytool tool, which Ant wraps up into the <genkey> task.
This task adds a key into a keystore, creating the store if needed:
This task creates a new alias in the keystore, with a certificate that’s valid for 366 days.
Although these keys are cryptographically sound, tools such as the Java Web Start don’t trust them.
If you’re verifying JAR files in your own application, you’re free to use self-generated keys, and within an organization or community you may be able to convince end users to add your certificate (or private certification authority) to the trusted list.
What we can do with an untrusted key is sign the JAR and verify that our application works with signed JAR files and the classloader complications that follow.
Our manifest now contains digest signatures of the classes inside the JAR, and there are new files in the META-INF directory, including the public certificate of the generated pair.
The <signjar> task can bulk sign a set of JAR files, using a nested fileset element.
It also performs basic dependency checking, by not attempting to sign any files that are already signed by the user.
It doesn’t check to see if the file has changed since the last signing.
This means that you should not mix JAR signing with incremental JAR creation: the update flag in the <jar> task must remain at false.
Java behaves differently with signed JARs, and some applications can break.
To be sure that this has not happened, we must take the signed JAR file of the diary classes and run our existing tests against it.
Running JUnit against a signed JAR file, rather than the raw classes, lets us test more things.
It lets us test that the classes were added to the JAR file, that we’ve remembered to add any resources the application needs, and that the signing process has not broken anything.
It also lets us state that the tests were run against the redistributables, which is something to be proud of.
It is very easy to test against the JAR file.
Recall that in chapter 4, we set up our classpath for compiling and running tests like this:
We need to change one line to run against the generated JAR file:
We also have to declare that the test targets depend upon the JAR file being created:
As all the test targets, including test-compile, depend upon this test-init target, we are now set up to compile and run the tests against the JAR file.
As clean builds are usually fast, don’t be afraid to run them regularly.
Since the tests still appear to be working, we can say that we’ve finished our process of generating the JAR file.
Next comes redistributing the JAR file inside Zip and tar packages.
It lets you add the contents of one Zip file to another, expanding it in the directory tree where you choose, and it lets you place files imported from the file system into chosen places in the Zip file.
This eliminates the need to create a complete directory tree on the local disk before creating the archive.
The last two attributes let you declare the Unix file permissions.
The values are interpreted by the Info-ZIP implementations of the zip and unzip programs that are common on Unix and Linux systems.
In theory, you could set the permissions for executables and files, and when unzipped on the command line, they would be set.
We tend to use the tar file format when setting permissions, but that could just be a historical quirk of ours.
If you create a Zip file with permissions, you may not need to make a tar file at all.
To create the Zip file in our build, the first step is to define the names of the new output files.
We use the plural, as we plan to create two files for distribution: a binary redistributable and a source edition.
We do so by adding properties to the start of the project, declaring a full path to each Zip file.
First, we realize that the combination of project-name and version is going to be used so often it needs factoring into its own property.
Then we create properties naming both the Zip files we’re about to create.
These files will be fed into the tasks to create the Zip files and follow-on tasks that will expand the same files for testing.
There are two Zip files to create—binary and source—with two targets.
Our binary distribution contains the project’s JAR file, all the documentation, and any scripts in the bin directory.
We want the signed JAR and the documents that were patched with DOS file endings.
We’ve given every zipfileset the prefix of the project name and version.
This is the Unix tradition: the redistributable should expand into a directory containing the name and title of the project.
This is very useful for having versions of programs side by side, which is why it’s a common technique.
To verify that this task works, we create a target to unzip the file:
If you use a GPL- or LPGL-licensed library, this is where you discover whether its license applies to your own code.
Hand in hand with the binary distribution goes the source distribution.
In the open-source world, there’s often little difference between source and binary distributions.
In closed-source software there is a difference, but the source is still regularly distributed, just not as broadly.
We’re going to include the JAR file; then the components for our source build file become clear.
They are: the source, test, and documentation directory trees; the build file; and the binary Zip file itself:
The result is a file that runs out of the box but which contains the entire source and, of course, the build file.
There’s one little extra trick we can do to validate a source distribution: we can run Ant in it and verify that it works.
At the risk of a forward reference, here’s the target to run the unzipped copy of our own build file:
There’s some magic there to deal with passing down the password and other signing information, which is where the <ant> task gets complicated and why we won’t return to it for five more chapters.
Here are some tips to make creating Zip files easier:
Copy all files you want to include in the JAR into one place before building.
This makes it easier to test that the needed files have been copied.
Don’t distribute JAR files with a .zip extension—it causes confusion.
Use the <zipfileset> element for more control over the entries in the.
Create a subdirectory for everything, with the name and version of the project.
Include the Unix documents and scripts alongside the Windows ones.
Everything that runs Java can expand the files by using the jar tool if need be, and they’re supported on Windows, Unix, and other platforms.
Even so, there’s some value in producing Unix-specific source and binary distribution packages, which Ant can do as easily as it can create a Zip file.
The preferred archive format for Unix systems is the tar file, while many Linux distributions can install RPM or .deb packages.
Projects may want to produce these files alongside .zip distributions.
Doing so makes the files more acceptable to Unix users.
It also can give you some statistics that measure Unix and Linux package downloads alongside Windows ones if you’re distributing via a web site that collects download statistics.
The archive includes not only the folder hierarchy, but also the file permissions, including the files that are executable.
A version of the tar program can be found on every Unix platform, and it’s even.
Well, after creating the Zip distribution, we unzipped it, to verify it was all there.
What if we were to create a tar file from that directory tree, adding file permissions as we go? That might seem like cheating, but from the extreme programming perspective, it’s exactly the kind of lazy coding developers should be doing.
Listing 5.3 shows the target that we use to create the archive of the binary distribution, giving shell scripts execute permissions as we do so.
The file permission is in the base-8 format used in Unix API calls.
The default permission is 644 (read/write to the owner, read to everyone else), and the default identity is simply the empty string.
Listing 5.3 Creating a tar file from our expanded Zip file.
We normally ignore these options, worrying only about file permissions.
First, we include everything but the executable files with default permissions, then we include the executable files with the relevant mask.
We must not include the executables in the first fileset, because if we did, the files would be included twice with unpredictable results.
Problems with tar files The original tar file format and program doesn’t handle very long path names.
There’s a 100-character limit, which is easily exceeded in any Java source tree.
The GNU tar program supports longer filenames, unlike the original Unix implementation.
If you choose to use the GNU format, add a warning note on the download page about using GNU tar to expand the library.
Also, tell whomever deals with support calls about the issue, because not enough people read the documentation.
The usual sign is a bug report about missing source files, primarily on Solaris, AIX, or HP/UX.
The problem never surfaces on Linux, as GNU tar is the only tar tool there.
Tar files are also a weak format for sharing, because they’re uncompressed, and so can be overweight compared to Zip files.
Although optional, setting this attribute shows that you have chosen an explicit policy.
Of the options, fail, gnu, and warn make the most sense.
They take a single source file in the src attribute and an output file in either the destfile or zipfile attribute.
When executed, these tasks create a suitably compressed file whenever the destination file is absent or older than the source.
We do, of course, have to check that the compressed tar file is usable.
You can use these tasks to verify that the redistributable files are in good condition.
For example, here’s the .tar.gz file expanded into a directory tree:
Because Ant doesn’t set permissions on untarred files, these tasks aren’t quite as useful as Unix and GNU gunzip and tar commands.
Projects have to be ruthless and ask if creating tar files is worth the effort, or if using a Zip file is adequate.
Simple projects should find that Zip files are an adequate cross-platform redistributable, especially if you’re targeting other Java users.
Creating tar files is an act of political correctness: Unix users may expect it, and the JPackage group (see below) likes to use it as a starting place for its RPM files.
Not only are they the only files that work on Windows, every JDK comes with the jar tool.
Therefore, wherever there’s a JDK, there’s a way to expand the file.
Tar files, on the other hand, are an optional extra.
The other important file format is the Linux RPM package.
Closing the subject of packaging for Linux, we note that there’s a task, <rpm>, that runs the RedHat Package manager, generating an RPM file.
These are files that can be installed by using the application management tools of RedHat, SuSE, and other RPM-based Linux distributions.
The authors never use <rpm>, because we don’t create RPM packages.
The RPM format is great for managing native programs on Linux.
But only system administrators can install RPM files, and they affect the whole system.
In Java, it’s a lot easier to have personal installations with personal copies of all needed JAR files, even private JREs for separate projects.
You don’t often need system-wide installation on a self-managed box.
The JPackage project team (http://jpackage.org/) has a different opinion on the matter.
If you want to start creating RPM files, look at what they do and join their mailing list.
You should also be aware that recent Linux distributions often include the JPackage artifacts, which can be a help or a hindrance depending on how you adapt to them.
Consult the JPackage team for its advice on how to redistribute JPackage-friendly code, especially if you want to integrate with Linux distributions or management tools.
Together, the JAR, Zip, and tar files form the core of Ant’s packages.
There’s one more thing we can do with all these archives, and that is read the data back.
What if we just need to get one file from inside a JAR and feed it into another task? Do we really have to create a temporary directory, expand the JAR, pass the file reference, and remember to clean up afterwards? Isn’t there an easier way?
Now it’s time to explore the topic, since our project has things like JAR, Zip, and tar files.
We’re going to peek under the covers of Ant for a moment and describe what a resource is in terms of Java classes and interfaces.
Some resources are touchable, meaning they implement the Touchable interface with its method touch() to set the time of the resource.
Files, URLs, and files inside Zip and tar archives are some of Ant’s built-in resource types.
A resource collection is anything that acts as a container for resources.
Formally, it is any datatype that implements the ResourceCollection interface, with its methods size(), iterator(), and isFileSystemOnly()
The Resource class itself claims to be a ResourceCollection with one resource—itself.
A resource-enabled task is a task that’s been written or updated to support resources and/or resource collections.
Many such tasks accept resource collections that refer to only those resources that are in the local file system—that is, whose isFileSystemOnly() test returns true.
These collections can be used in resource-enabled tasks that no longer have to care where or how the data gets into the task.
You can feed the contents of a property to a task as easily as pointing it at a file.
From a packaging perspective, there are built-in resources to access the individual entries in a JAR, Zip, or tar archive.
These resources enable build files to access files inside the archives without expanding them.
These resources can be passed directly to any resource-enabled task, or grouped into a resource collection.
Every resource is something that can act as a source of data.
Some can act as output streams; those that are touchable can be used in the <touch> task to set their timestamps.
The <url> resource lets you use any URL that the Java runtime supports as a source of data.
To actually use any of these resources, you need to declare them inside a resource collection, which is an Ant datatype that supports nested resources.
Resources can be collected into resource collections, and this is where it gets interesting.
These resource collections are essentially different ways to describe a set of resources to a task.
The best bit comes when you look at the ability to restrict, combine, and sort resources.
To summarize where we are so far: resources represent sources (and sometimes destinations) of data; they can be aggregated in resource collections.
The built-in resources can refer to files, strings, properties, or files stored inside packaged tar or Zip files.
We can use the resource collections to access these resources.
Making use of resources To use resource collections, you declare them inside a resource-enabled task.
In an ideal world, you wouldn’t need to differentiate between filesets, paths, filelists, and other collection types—you’d just pass them to a task.
A unified model of resources is new to Ant, and it’s only gradually being added to Ant’s tasks.
This filecentric view of the world is not just restricted to Ant: tools like Sun’s Java compiler work only with files in the file system, not remote URLs or source in a Zip file.
The only way to move such programs to support arbitrary resources would be to provide a complete new virtual file system underneath.
The <copy> task is fully resource-enabled; it will copy content from any resource to a file or directory.
You can use <copy> to get anything onto the local file system for other tasks to work with.
This is the primary way that non-file resources are used in Ant today.
When we write new tasks in chapter 17, they’ll be resource-enabled from the outset.
Can we treat everything as a resource and use <copy> instead? Yes.
Here’s a target that uses one <copy> to extract the .tar file from inside the .gz file, then extracts all the entries inside the tar file:
We’re going to use resources in the future when they solve specific problems.
The whole resource concept will take time to propagate through tasks, especially thirdparty ones.
Tasks that accept resource collections rather than just, say, filesets, are much more flexible as they can accept data from so many more sources.
Build file authors will be able to do tricks, such as pull source files straight from a JAR or Zip file and feed them straight into another task.
Resources can be used in another interesting way: developers can provide new resources or resource collections to represent new sources of data.
For now, the primary use of the concept is in <copy>
It can extract resources from any collection and save them to a file.
Most other tasks, especially those that wrap existing code, can only handle in-file-system resources.
Packaging your Java classes into JAR files is the first step to using or distributing your code.
Once the JAR is made, you can use it in other programs or execute it.
What we’ve covered in this chapter is the full spread of distribution packaging, from Zip to tar.
Zip is the most uniformly supported and the one to start with; tar is also popular in Unix.
Building a redistributable package can go wrong—usually due to errors in the build file.
There are three ways of addressing this problem: manual tests, automated tests, or shipping the product and waiting for complaints.
We like the automated test system, and we’ve shown some ways to verify that the process works.
Finally, we’ve introduced the resource and resource collection model of Ant.
These form the conceptual model behind the fileset and other collections of data sources and destinations in Ant.
Resource-enabled tasks, such as <copy>, are able to use arbitrary resources as data sources, which lets build files work directly with files straight out of JAR, Zip, or tar archives.
Although new to Ant, resources will become more broadly supported over time and will lead to a better conceptual model of data sources and destinations.
With our diary JAR file created and the source and binary distributions put together, our build is in a position to do two things.
The JAR can be executed, and the distribution packages redistributed.
Ant is one of the best tools for starting programs in Java; it wraps all the platform-specific details into a few tasks.
It’s so useful that it’s often used behind the scenes in many applications.
Let’s start with an overview of the problem, get into <java>, and then look at native code.
Ant is packaging it into JAR files and then into Zip and tar packages for distribution.
Ant does a lot of work in its own libraries, in the Java code behind tasks.
You don’t need to explicitly run external programs on every line of a build file just to get the application compiled, packaged, and tested the way you’d have to do with the Make tool.
The most common program to run from inside Ant is the one you’re actually building, such as our diary application.
Ant can delegate work, such as tools like javac in the JDK.
When you need to run programs from inside Ant, there are two solutions.
It’s no harder than writing any other Java class, but it does involve programming, testing, and documentation.
It’s the most powerful and flexible means of integrating with Ant, and the effort is sometimes worthwhile.
It’s a great way of making software easy for other developers to use.
Java programs can even run inside Ant’s own JVM for higher performance.
Figure 6.1 illustrates the basic conceptual model of this execution.
Many Ant tasks work by calling native programs or Java programs.
As a case in point, the <javac> task can run Sun’s javac compiler in Ant’s JVM, or IBM’s Jikes compiler as a native application.
When running a program, Ant normally halts the build until it finishes.
Console output from the program goes to the Ant logger, which forwards it to the screen, while input data goes the other way.
Users can specify input sources and output destinations, whether they’re files, Ant properties, or filter chains, which are Ant components to process incoming data.
Before we can run our diary, we need an entry point—a method that the Java runtime can invoke to start the program running.
Listing 6.1 shows this entry point in its class DiaryMain.
Figure 6.1 Ant can spawn native applications, while Java programs can run inside or outside Ant's JVM.
Listing 6.1 An entry point to add events to our diary.
This method will create or update the calendar file calendar.d1 with the appointment passed in as arguments.
If no arguments are passed down, it lists the current diary; otherwise it tries to create one from the time, name, and text arguments passed in.
This entry point turns the JAR file into a program, one that can be run from the command line.
It also can be run from Ant, which lets us integrate it with the build and which allows us to have Ant provide arguments from Ant properties and datatypes.
The central task for running Java applications is called, not surprisingly, <java>
In this chapter, we’re going to run our diary entry point with it, an entry point that gets compiled into the JAR file created in chapter 5:
The task takes the name of the entry point class via its classname attribute.
What happens when we run this target? First Ant runs the targets that build the JAR file.
Oops! We left out the classpath and get to see Ant’s error message when this happens.
If you’re new to Java, this will be a new pain, but to experienced Java developers, it should be a very familiar sensation.
To run our program, we have to set up the classpath for <java>
The basic <java> task runs only with Ant’s classpath, which is as follows:
All directories or JAR files named with -lib options on the command line.
Any libraries an IDE hosting Ant saw fit to include on the classpath.
If you’re going to use the same classpath in more than one place, declare the path and then refer to it using the classpathref attribute.
One common practice is to extend the compile-time classpath with a second classpath that includes the newly built classes, either in archive form or as a directory tree of .class files.
We do this by declaring two classpaths, one for compilation and the other for execution:
The compile classpath will include any libraries we depend upon to build; then the run classpath extends this with the JAR just built.
This approach simplifies maintenance; any library used at compile time automatically propagates to the runtime classpath.
With the new classpath defined, we can modify the <java> task and run our program:
The successful output of this task delivers the results we want:
Setting up the classpath and the program entry point are the two activities that must be done for every <java> run.
To pass data to a running program, the task needs to be set up with the arguments for the main() method and any JVM system properties that the program needs.
The argument list is the main way of passing data to a running application.
You can name arguments by a single value, a line of text, a filename, or a path.
Ant resolves the arguments and passes them on in the order they’re declared.
The only tricky spot is handling those symbols that XML does not like.
The second argument used the file attribute to pass down a filename—the current directory:
Ant will resolve it to an absolute location before passing it to the program.
It generates a single argument from the comma- or colon-separated file path elements passed in.
As with other paths in Ant, relative locations are resolved, and both Unix or MS-DOS directory and path separators can be used.
The invoked program receives a path as a single argument containing resolved filenames with the directory and path separators appropriate to the platform.
Ant will retrieve the path and add it as another argument.
Returning to our Ant target, and running it on Windows, this is what we see:
All the files and paths are resolved relative to the project directory and converted to the platform-specific representation before any invoked program sees them.
We also have encountered one little quirk of Ant path mapping: what does "/" resolve to? In.
But in Windows, it resolves to the root directory of the current disk.
This can cause a problem if you want to create a single fileset spanning content in two Windows disks, such as C: and D: because it cannot be done.
There is one other way to pass arguments down, using the line attribute.
This takes a single line, which is split into separate arguments wherever there’s a space between values:
Is this equivalent? Not where the home directory has a space in it:
They were expecting Ant to pass a series of all arguments down to the program, but Ant passed them down as a single argument.
Learn the differences between the different argument options, and use the right one for each problem.
The only reason for using it is to support an unlimited number of arguments—perhaps from a property file or prompted input.
Anyone who does this had better hope that spaces aren’t expected in individual arguments.
The other way to pass data down is through JVM system properties.
Java system properties are set on the Java command line as -Dproperty=value arguments.
This is useful in configuring the JVM itself, such as controlling how long.
There are two alternate options instead of the value parameter: file and path.
Just as with <arg> elements, the file attribute lets you name a file.
Ant will pass down an absolute filename in the platform-specific format.
The path attribute is similar, except that you can list multiple files, and it will convert the path separator to whatever is appropriate for the operating system:
Propertysets A propertyset is an Ant datatype that represents a set of properties.
This declaration passes down to the program all Ant properties beginning with "proxy" and all properties on Ant’s own command line.
The <propertyset> datatype has a few more features that make it useful.
Being a datatype, you can declare it in one location with an id attribute and reuse it via idref attributes.
You can also use a mapper to rename the properties in the set.
For these to be picked up reliably, we need to run our code in a new Java process, a new JVM instance.
The <java> task runs inside Ant’s JVM unless the fork attribute is set to true.
Nonforked code can be faster and shares more of the JVM’s state.
You get perfect isolation from Ant’s own classloader and code.
Forked Java programs can run in a new working directory, with different JVM options and environment.
Forked Java programs can run in a different version of Java than Ant itself.
You cannot run a JAR ("-jar something.jar") in the Ant’s JVM.
Memory-hungry programs can run in their own JVM and not use Ant’s memory.
Forked programs can be killed if they don’t finish in a specified time period.
In informal experiments on a single machine, the time to build and run the program usually appears to be two seconds, rather than the one second that the original version took.
That could be a doubling of execution time, but it’s only an extra second, as measured with Ant’s single-second clock.
Does a second or so of delay matter that much?
We prefer to always fork our Java programs, preferring isolation over possible performance gains.
This reduces the time spent debugging obscure problems related to classloaders and JVM isolation.
Once we fork <java>, we can change the JVM under which it runs and the options in the JVM.
You can actually choose a Java runtime that’s different from the one hosting Ant by setting the jvm attribute to the command that starts the JVM.
This lets you run a program under a different JVM.
In addition to choosing the JVM, you can configure it.
The most commonly used option is the amount of memory to use, which is so common that it has its own attribute, the maxmemory attribute.
The memory option, as per the java command, takes a string that lists the number of bytes, kilobytes (k), or megabytes (m) to use.
A call to java -X will list the ones on your local machine.
Because these options can vary from system to system, they should be set via properties so that different developers can override them as needed.
Here, for example, we set the default arguments for memory and the server JVM with incremental garbage collection, using properties to provide an override point for different users:
The exact syntax of these arguments is the same as for the <arg> elements.
We use the line attribute, despite the negative things we said about it earlier, as it permits a single property to contain a list of arguments.
This would transform their garbage collection policy, without affecting anyone else.
We’ve also changed the starting directory of the program with the dir attribute.
This lets you use relative file references in your code and have them resolved correctly when running.
Only the new JVM picks up the directory change, not Ant itself.
None of these JVM options have any effect when fork="false"; only a warning message is printed.
If they don’t seem to work, look closely at the task declaration and see if forking needs to be turned on.
Regardless of whether the program runs in Ant’s own JVM or a new process, we can still capture the return code of the program, which can be used to pass information back to the caller or signal an error.
What’s going to happen if we pass a bad parameter to our program? Will the build break? First, we try to create an event on a day that doesn’t exist:
Running this, we get a stack trace, but the build still thinks it succeeded.
What happened? Many Ant tasks have an attribute, failonerror, which controls whether the failure of the task breaks the build.
Most such tasks have a default of failonerror= "true", meaning any failure of the task breaks the build, resulting in the familiar BUILD FAILED message.
The <java> task is one such task, halting the build if failonerror="true" and the return value of the Java program is non-zero.
That happens if System.exit() is called with a non-zero argument or if the JVM failed, as it does when the entry point throws an exception, or if the command line arguments to the JVM are invalid.
To get this behavior, developers must explicitly set the attribute, as shown here:
Calling this target, we get an error message and a halted build:
This is normally what you want, so set failonerror="true" on all uses of <java>
Alternatively, we could capture the return value and act on it in follow-up tasks.
Capturing the return code Sometimes, you want to know if the program returned a status code or what the value was without halting the build.
Knowing this lets you do conditional actions on the return code or run programs that pass information back to Ant.
We could use a <condition> to test the return value and act on it, or pass the property to another task.
For this to work, the manifest must be set up correctly.
When we run this new target, we get the same output as before.
If our JAR file depended upon other libraries, we must declare these dependencies in the manifest by providing a list of relative URLs in the Class-Path attribute of the Main section in the JAR manifest.
There’s no other way to set the classpath for JAR files, as java itself ignores any command-line classpath when running JAR files via -jar.
We normally use the classname attribute and set up the classpath in Ant.
Now it’s time to introduce <exec>, which can run native executables.
Both tasks have a lot in common, so many of the concepts will be familiar.
We’ll return to <java> when we look at features common to both tasks.
A native program is any program compiled for the local system, or, in Unix, any shell script marked as executable.
There are many other programs that can be useful, from a command to mount a shared drive, to a native installer program.
Ant can call these programs with the parameters you desire.
To run an external program in Ant, use the <exec> task.
The name of the program and arguments to pass in.
Other Ant components to act as input sources or destinations.
A string that should be in the name of the OS.
One use of the task is to create a symbolic link to a file for which there is no intrinsic Java command:
You don’t need to supply the full path to the executable if it’s on the current path.
We can even use the task to start Java programs.
If a Java program has a shell script to launch the program, <exec> can start it.
We can use it to start our diary application via a Python script.
This script will launch the diary JAR via a java -jar command, passing down all arguments.
To run this script from Ant (Unix only), we invoke it after it’s been prepared by the "scripts" target, which uses <chmod> to make the file executable:
We need to give the full path to the script or program to run.
If we didn’t, <exec> would fail with some OS-specific error code if the executable couldn’t be found.
The executable attribute doesn’t convert its value to a file or location but executes it as is.
This is inconvenient at times, but it does let us run anything on the path.
What happens if a program cannot be run, because it’s not there? This is a separate failure mode from the return code of a program that actually starts, which is what failonerror cares about.
It’s true by default, which is where most people should leave it.
One common use of <exec> is issuing shell commands, which is unexpectedly tricky.
We’ve reached the point where we can use <exec> to start a program.
Can we also use it to run shell commands? Yes, with caveats.
Many shell commands aren’t native programs; rather, they’re instructions to the shell itself.
For example, one might naively try to list the running Java processes and save them to a file by building a shell command, and use this shell command in <exec> as a single command, via the (deprecated) command attribute:
In addition to getting a warning for using the command attribute, the whole line needs to be interpreted by a shell.
You’ll probably see a usage error from the first program on the line—in this case the jps command: exec-ps: [exec] The command attribute is deprecated.
The trick is to make the shell the command and to pass in a string containing the command you want the shell to execute.
The Unix sh program lets you do this with its -c command, but it wants the commands to follow in a quoted string.
A command that uses both single and double quotes needs to use the &quot; notation instead of the double quote.
A lot of people encounter problems trying to run shell commands on Ant; it keeps the user mailing list busy on a regular basis.
All you have to do is remember that shell commands aren’t native programs.
It’s the shell itself that Ant must start with the <exec> task.
Running shell scripts shows another problem: the program we want to run varies with the underlying platform.
Can you run such programs and keep your build file portable? The answer is “Maybe, if you try hard enough.”
The <exec> task can keep build files portable by letting you list which operating systems a program runs on.
If Ant isn’t running on a supported OS, the program isn’t started.
Before the task is executed, Ant examines the value of the.
A valid os attribute would have to be something such as "Linux AIX Unix", based on your expectations of which OS versions would be used.
This method is very brittle and usually breaks when a new OS ships.
The newer osfamily attribute lets you declare which particular family of operating systems to use, according to the families listed in table 6.2
This method is much more robust, as an osfamily="winnt" test will work for all current and future NT-based Windows operating systems.
We can use this attribute to restrict a program to a specific OS family.
Doing so allows us to have different versions for different platforms side by side.
Here, for example, is a target that runs the Windows or Unix time commands:
By having the two tasks in the same target, each guarded by its own osfamily attribute, the relevant target will be run.
The result will be the time according to the OS, something like date: [exec] 15:27
If you have a lot of <exec> code in your build and you want to make it multiplatform, use this osfamily attribute to make it somewhat portable.
Ant tasks such as <chmod> use it internally to avoid breaking under Windows.
Incidentally, the same operation system test can be used in an Ant condition.
The combination of a new Java version and a new OS can sometimes break all this.
There’s no specific test for a platform being Windows- or Unix-based in Java, so Ant makes guesses based on OS names and path and directory separators.
When a new version of Java gives an OS a new name, Ant can guess wrong.
We can also use conditions to set up <exec> itself, either by checking for the OS or by looking for the program.
Sometimes, you want to run a program, but you are prepared to continue the build if it isn’t there.
If you know where the program must be, then an <available> test can look for it.
But what if the only requirement is that it must be on the path? The <available> task or condition can search a whole file path for a named file, so probing for a program’s existence is a simple matter of searching for its name in the environment variable PATH.
Of course, in a cross-platform manner, nothing is ever simple; Windows and Unix name executables and even the PATH variable differently.
Taking this into account, looking for a program becomes a multiple-condition test.
The test needs to look for the executable with and without the .exe extension, across two different environment variables converted to Ant properties:
If we run this, it tells us if the python program is ready to run:
You can then write dependent targets that skip some work if the program is absent:
We do this in our build files to probe for programs, if they really are optional.
If we have to have them present, we just let <exec> fail with an error message, as it saves work.
We’ve also looked at how to use <exec> to run a program, to run a shell script, and to skip the execution if the operating system is unsupported or the program is missing.
You can tell Ant to kill the programs if they.
Programs don’t run in isolation: they operate in an environment that can change their behavior.
They have a stream giving them text, and two output streams—one for normal text, one for errors.
Developers may need to link those inputs and outputs to files or other things that can produce or consume the data.
Sometimes, programs hang, so the build needs to kill the programs if they take too long.
Sometimes, the program may be expected to outlast the build itself.
These are needs developers have, so they are problems that Ant has to address.
All programs run in an environment, one in which the PATH variable controls the search path for executables, and JAVA_HOME defines the location of the JDK.
Ant normally passes its own environment down to new processes that it starts.
We can change that, passing a new environment down to child processes, which is useful when running some programs.
What we cannot do is change Ant’s own environment—this is fixed when Ant is run.
The syntax of this element is identical to that of the <sysproperty> element:
Setting the path on a forked program—and it must be forked—lets us load new JNI libraries in the program that’s run by <java>
You also can choose whether the program inherits Ant’s current environment through the newenvironment attribute.
Usually, you pass down all current environment settings, such as PATH and TEMP, but sometimes you may want absolute control over the parameters:
Setting environment variables lets us control how some programs run.
We also like to control how programs stop, killing them if they take too long.
How do you deal with programs that hang? You can stop a running build by hand, but unattended builds need to take care of themselves.
When a program is executed with the timeout attribute set, Ant starts a watchdog thread that sleeps for the duration of the timeout.
When it wakes up, this thread will kill the program if it has not yet finished.
If failonerror is set to true, this will break the build.
Here’s a somewhat contrived example, using the Unix sleep command to sleep for fifteen seconds, with a timeout of two seconds1:
Running this target produces an error when the timeout engages:
If you really need to insert a pause into a build, the <sleep> task works across all platforms.
It’s useful to have timeouts for all programs that run in automated builds, especially those programs you write yourself—since they may be less stable.
We often do timeouts for <junit> runs too, for a similar reason.
Nobody likes to find the background build has hung for the entire weekend because a test went into an infinite loop.
Long-lived programs don’t have to block Ant if we run them in the background.
During testing or if you want to use Ant as a launcher of Java programs, you may want Ant to run the program in the background, perhaps even for it to outlive the Ant run itself.
Spawning can be accomplished by setting the spawn attribute to true.
After starting the program, Ant detaches the program from its own process completely so that when Ant exits, the spawned program will keep going.
Spawning has a price, which is complete isolation from all further interaction between Ant and the spawned program.
There’s no way to kill a spawned process from Ant.
Here we’re lucky: our program terminates of its own accord.
If it did not, we would have to find the process using the appropriate OS tools and kill it.
As an example, the jconsole program that comes with Java is something that developers may want to run against the JVM of a process under development:
The way that Ant completely forgets about the program is a bit brutal, as it’s hard to.
If you’re not careful, your builds can leak spawned applications, so use the spawn option with care.
The other advanced features of the tasks are all related to integrating the program with Ant.
Ant can pass data to the program and capture the results, letting us pull those results into files and Ant properties.
What if we want Ant itself to capture the output, perhaps to feed it into another task? That and generating input for the program are both possible under Ant.
You can feed the program input data from a file or property, and then save the output to different files or properties.
The attributes to configure all this are listed in table 6.3
To provide input to a program, we can identify a file with the input attribute, or a string with inputstring.
Setting inputstring="" is a way to tell Ant not to pass any data down.
The core attributes for capturing output are output and outputProperty.
If they alone are set, then all output from an application, both the “normal” and “error” streams (System.out and System.err), are redirected.
If you still want to see error output on the log, set the logerror attribute to true and Ant will still log it.
We can also remove the error log from the normal output by setting either the error filename attribute or the errorproperty property name.
The named destination will get the error stream, and the output destination will get the System.out messages.
Let’s add some of these attributes to the <exec> of the Python script that runs our diary program:
This target provides the empty string as input, and then logs output to a property that’s echoed afterwards.
Setting logerror="true" ensures that any error text still goes to Ant’s output stream.
This property will pick up the exit value of the application, which we could feed into another task.
If you want to chain programs and tasks together, save the output to a file or property then pass it into another.
Normally, the files and properties are sufficient for managing program I/O.
The I/O Redirector is the power tool of I/O for those special emergencies.
We’re not going to cover it in detail; instead we’ll refer the reader to Ant’s own documentation.
It supports all the I/Orelated attributes of the tasks listed in table 6.3 and adds an extra input-related attribute, loginputstring, which stops the input string from being logged by Ant on its way to the program, for extra security.
There are also three extra attributes, inputencoding, outputencoding and errorencoding which manage the encoding of the source and destination files.
The real power of the task comes in its nested mapper and filter elements.
Alongside these are three optional FilterChain elements, elements that let you attach Ant filters to the input and output streams.
Ant’s FilterChain mechanism has the same underlying ability; yet, as a relatively recent addition, it isn’t broadly used.
Accordingly, it doesn’t have a broad set of filters to.
This isn’t too problematic, as one of the filters executes scripts.
Here, for example, we can use JavaScript to convert the program’s output.
Run Ant with any needed script libraries, and suddenly you can add inline pre- or post-processing to Java or native code.
You don’t need to write JavaScript to post-process the output if Ant has built-in support for the transformation.
There are nearly twenty operations that you can use in a FilterChain—that is, twenty FilterReaders—that can take the output of a program and transform it.
These operations can be used for changing the output of an execution or the contents of a file into the form that a follow-on task can use.
Many common operations perform against files, such as stripping out all comments, searching for lines with a specific value, or expanding Ant properties.
These are operations that Ant can perform against files while they’re being moved or loaded, and against the input and output streams of the executed program.
We used one in the previous section to run some JavaScript code against the output of our application, but there are other actions that are available in a FilterChain.
A FilterChain is an Ant datatype that contains a list of FilterReaders.
Each FilterReader is something that can add, remove, or modify text as it’s fed through the chain.
The result is that the chain can transform all text that’s passed through it.
After this operation, the copied Java source files will not contain any comments.
These FilterChains can be used to set up data for a program or to clean up the output.
We’ve looked at running them, passing arguments, handling failure, and now handling I/O.
In Ant, we’d want to use its built-in methods to represent groups of files or other resources, such as filesets and other resource collections.
This task takes a resource collection and either invokes the application once for every entry in the collection, or passes the entire collection to the application as its arguments.
We could use this task to create a .tar file by using the Unix tar program instead of the <tar> task, and so—on Unix only—pick up file permissions from the file system:
Apart from the nested fileset listing the parameters, there’s a <srcfile/> element in the argument list to show the task where to put the expanded files in the list of arguments.
The parallel attribute declares that we want all files supplied at once, instead of executing the tar.
This would package up the files with the wrong paths.
How did we know which attributes to apply? Trial and error, in this case.
To see the task at work, we run Ant with the -verbose option:
You can specify a mapper to provide a map from source files to target files.
These files will be inserted into the command’s argument list at the position indicated with the <targetfile/> attribute.
The presence of a mapper element also turns on the task’s limited dependency logic: a file is included in the list if the destination file is missing or out of date.
Here’s the declaration to run the javap bytecode disassembler over our source:
Ant says this because too many people submitted bug reports about the quotes.
The quotes are printed to stop confusion about its mishandling of spaces/arguments.
By declaring a mapper from the suffixed source to some imaginary non-suffixed target files, we can use the <targetfile/> attribute to feed the command with the mapped classname.
We also declared that the program should be run once per file, with parallel="false"
The result is that every class file will be disassembled to Java bytecodes, with the output printed to Ant’s log.
There’s no equivalent of <apply> for Java programs, although it’s always been discussed.
Many of these tasks have made their way back into Ant or became third-party tasks for Ant and, together, have given Ant its power.
It may seem complex, but it’s probably Java’s most widely used and most debugged library at executing programs.
We can run native or Java code; redirect input to and from properties, files, and filters; and apply commands in bulk.
Now, for the curious, we’re going to look under the hood at the implementation classes.
Having a good idea of how they work is very important when troubleshooting Ant.
A security manager is set up to intercept System.exit() calls; you can use the <permission> element to change the policies of this SecurityManager.
Ant then loads the entry point class and calls its static main(String args[]) method with the arguments from the build file.
If a timeout was specified, a watchdog thread is started first to terminate the entry point thread if it times out.
This is why terminating a non-forked <java> is dangerous: it can leave Ant’s own JVM in a bad state.
They share the same code underneath to do the heavy lifting.
It’s the <exec> task that owns the problem of running applications.
The <exec> task is the main way that programs are executed in Ant; all tasks that run external programs will use its classes.
There are subclasses of CommandLauncher for different Java versions and various platforms, including Windows NT, Mac OS, OS/2, and others.
They use one of Java’s execution commands, usually Runtime.exec(), and sometimes start a shell script or batch file to handle all the complexity of execution.
Separate threads will pump data to and from the running process, while a static ProcessDestroyer instance tracks which processes are currently running.
When Ant’s JVM terminates, the ProcessDestroyer tries to shut down all the processes for a clean exit, though it can not be guaranteed.
When a program is spawned, Ant forgets about it completely.
This stops Ant’s termination affecting the spawned process, but it also prevents Ant from halting the process or capturing any output.
The big difference is the extra logic that’s required for setting up the command line—potentially breaking the operation into multiple executions—and handling the output.
An example is the <chmod> task, which applies the native chmod program to all the file resources supplied to it.
While it’s simple to call other programs from Ant, it soon gets complicated as you try to produce a robust, portable means of executing external applications as part of the build process.
Java programs are easy to work with, as the classpath specification and JVM options make controlling the execution straightforward.
In-JVM execution has a faster startup, but external execution is more trouble-free, which makes it a good choice.
For Java programs to be callable from Ant, they should be well documented.
Ideally, they should have a library API as well as a public entry point.
The API enables Java programs to use the external program as a set of classes, rather than just as something to run once.
The programs should let you set the base directory for reading in relative information, or have parameters setting the full paths of any input and output files used.
Explicitly state the classpath, rather than rely on Ant’s own classpath.
Set failonerror="true" unless you want to ignore failures or capture the result.
Using <exec> to call external applications or glue together commands in the local shell is a more complex undertaking, as you’re vulnerable to all the behavior of the underlying operating system.
It’s very hard to write a portable build file that uses native programs.
Our recommendations for native programs are very similar to those of the Java recommendations:
Test on more than one platform to see what breaks.
Calling external programs and processing the results through chained input and output files is not its strength.
Ant expects tasks to do their own dependency checking and to hide all the low-level details of program invocation from the user.
This task is an essential tool in executing newly written software and in integrating existing code with Ant.
By default, Java programs run inside the current JVM, which is faster, but the forked version is more controllable and robust.
If ever anything doesn’t work under Ant, set fork="true" to see if this fixes the problem.
It gives Ant the ability to integrate with existing code and with existing development tools, though the moment you do so, you sacrifice a lot of portability.
It can be useful for bulk operations, despite its limitations.
All of these programs have common features, such as the ability for a failing program to halt the build.
They also share attributes and elements to pass files and properties, or to dynamically generate data to a program and collect the results the same way.
This means you can integrate Ant with external Java and native programs to make them part of the build, giving the build file access to everything the Java tools and native commands offer, as well as providing a way to run the programs you build yourself.
Having completed packaging, testing, and running our diary program, the library—limited as it is—is ready for use.
Ant is now compiling, testing, and running our diary library.
We can now do two things with the library: distribute it or deploy it.
What does that mean? For us, distribution is the act of delivering a project’s artifact to one or more recipients.
This delivery may be direct, via email or a shared file system, or it may be through a web site.
Deployment is the problem of bringing up a functional application on a working computer, and is what often happens after distribution.
Distribution is just handing off the packaged code to someone else to get working.
Ant is good at distribution, since it has tasks to handle the common activities.
To explore these tasks, we’ll use Ant to distribute our application according to the following distribution activities.
These distribution files are uploaded to a remote FTP server, such as SourceForge.
Email-based distribution of a packaged application—The application is emailed to multiple recipients.
Recipients will receive the source distribution in Zip or gzip format.
The recipient list must be kept in a separate file.
Secure distribution with SSH and SCP—The build performs a secure upload of our program to a Unix server using the Secure Shell protocol, SSH.
Distribution over multiple channels—The artifacts will be distributed using all of the previous methods.
These contain the signed JAR file, the source, and the documentation.
The first step is to get everything ready for distribution.
Most of the preparation for distribution has been done in chapter 5: Ant can create .zip and .tar.gz packages containing source and binary redistributables.
We need to do a few more things before the real work begins.
Ant can address our distribution needs through a set of tasks that we haven’t yet encountered.
All the tasks but <get> have dependencies on external libraries; that is, JAR files that must be in Ant’s own classpath.
The online Ant documentation contains live links to the most up-todate versions and locations of these files.
The ant- JAR files should already be on your system; only the support libraries are likely to be needed.
Remember also that if you’re using an IDE-hosted Ant, you need to get these JAR files into its classpath somehow.
Examine the “Tasks availability” section—none of the ftp, telnet, mail, ssh or sshexec tasks should be listed as unavailable.
Unless -diagnostics thinks the tasks are present, we cannot distribute our application with Ant.
Once the tasks are available under Ant, we’re almost ready to distribute.
There’s one last step: securing the redistributables so that recipients can trust them.
Nobody wants to download malicious applications to their computers, yet this is what can happen whenever you download an application or library from the network.
We need to secure our distribution packages so that everyone can know that they come from trusted sources.
We can start by having Ant create checksums for the .zip and .tar.gz files.
We can upload them with the files, for people to compare against.
If that mail itself is signed, such as with PGP, then people who trust us can be sure the binaries haven’t been altered.
We can download the distributed files ourselves and verify that the checksums are still valid.
This ensures that nobody has tampered with the released files.
This task takes the name of a file or a nested fileset and the name of the algorithm, the default being md5
People downloading the file can now verify the checksum with the GNU tools by typing.
As well as saving the checksums to files, we save them to Ant properties simply by naming a property with the property attribute.
This will let us include the checksum in our email.
While the name is case-insensitive when matching the algorithm, the <checksum> task uses the algorithm attribute as the extension without changing the case.
If you want secure distribution, these tasks provide the best way to upload content.
With the packages created and checksum files and properties generated from them, the development computer is ready for distribution.
We’re going to distribute our application using FTP, email, and SSH, which means that we need an FTP server, an email server, and a machine running an SSH daemon.
That machine must also be running a web server so we can download the uploaded files from it.
It’s always good to check that you can connect to the server before trying to get it to work in Ant, because that will catch connection and account problems early, without Ant adding more confusion.
With the relevant servers in hand, we can start the upload process.
To do this, we start by declaring a named fileset containing all the redistributable files, for multiple tasks to use:
For distribution, we’re only concerned with connecting to a server and uploading changed files.
The exact process is slightly different for Unix and Windows machines, so we’ll have to treat each slightly differently.
Our first activity is uploading the fileset to a Unix system, specifically a Linux box running the vsftpd  FTP program.
The hostname, username, and other parameters all need to be passed to the build file; the parameters must be kept secure.
We also want to be able to switch between different properties files for different targets.
To keep the account details more secure, we store them in a properties file, one with the name of the server.
We can have multiple servers in different files, each with its own settings: username, password, system type, and upload directory.
The directory should have its access restricted to the developer, and not be put under revision control.
To load this properties file, we add a new target, ftp-init.
It uses the value of the Ant property server to select the file to load:
The <loadproperties> task always fails the build in such a situation, so it can detect when the value of server is wrong.
The first creates a destination directory; the second copies in the files.
We create the directory b then push the predefined fileset to the remote server c.
Without our test in the ftpinit target, the build would still have broken in the <ftp> task, but without a helpful error message.
When you return to a build file many months after writing it, you’ll appreciate the value of such diagnostics checks.
Let’s try again with the server selected on the command line:
For more detail, we could run Ant in -verbose mode, or set the verbose attribute of the <ftp> task to true.
With all configuration details kept in property files, we can upload to a Windows server that is running the FTP service:
Notice how we had to escape backslashes in both the directory and username, and how we had to include the domain name in the latter.
This shows that we can distribute our files to remote FTP servers, be they Windows or Unix.
These could be the machines where our application is to run, or they could be a site that publishes the files for others to download.
That’s exactly the service that SourceForge provides to all open source projects it hosts, which is a common target for distribution from Ant.
Open Source projects hosted on the SourceForge site (http://sourceforge.net) have to use FTP if they want to release files to the SourceForge download service.
Developers must upload their packages using anonymous FTP, then go to the project web page where a logged-in developer can release a “package.” That’s done in the project administration section, under “edit/release packages.”
To do a SourceForge upload, all we need is another properties file:
Ant can perform the upload, leaving the web page work to the developers:
We can upload to SourceForge as easy as to a machine next to our desk.
This shows the advantage of controlling the build with external properties files.
In the build file, the ftp-upload target doesn’t depend upon the targets that build the packages.
A packaged release can be made, tested, and uploaded to multiple sites without ever being rebuilt.
We don’t want the distribution targets to unwittingly trigger a rebuild.
The dependency checking also works across the network: it’s possible to set the FTP tasks to only upload files that have changed.
This can save bandwidth and make builds faster, but it’s risky.
To understand the dangers, you need to understand how the task determines if a remote file is out of date.
When you upload or download files over FTP, you can ask for the local and remote file times to be checked, so the upload or download only happens when needed.
This is a bit troublesome, as the task has to parse the output of the directory listings to determine timestamps.
Distribution across time zones can be extra hard, which is why recent versions of the task add a timediffauto attribute, telling the task to work out the time difference at the far end by creating a file there.
Even then, there’s the problem that the directory listings can be internationalized, with a different ordering of days, months, and years, and even month names in different countries.
Everything works with both our local targets, Linux and Windows:
The SourceForge target fails, because that server doesn’t allow any directory listing so it cannot check timestamps.
This is another reason why we don’t use FTP dependency checking and, instead, stick with depends="false"
If you do want to use timestamp dependencies to manage uploads, consult the <ftp> task’s documentation for all the details.
It’s possible to specify the approximate accuracy of the clocks, the remote time zone and language of the server, and, for unsupported languages, the complete list of month names needed to parse dates.
It may not have any dependency logic at all, but it’s more secure.
Before we get to that, let’s try distributing the program by email.
We’ll use the <mail> task for this, and send mail via Google’s gmail mail hub to avoid setting up a mail server of our own.
Ant’s <mail> task can send emails—either plaintext or “MIME”—with attachments and HTML text.
To send MIME messages, we need the JavaMail libraries (mail.jar and activation.jar) on Ant’s classpath.
If they’re missing, the task falls back to supporting plain text and uuencode-encoded data only.
As authenticated/ encrypted SMTP also uses these libraries, you need the JavaMail JARs if you want to.
Use SSL/TLS to make a secure connection to the mail server.
We’re going to send binary attachments via the gmail mail hub, for which we need these extra libraries.
Build files should always set the mailhost attribute from a property, even if the default is simply localhost, so that users can override it.
Our email target does this along with all other connection options:
We then list two filesets, pulling in the Zip file and its .sha1 checksum c.
In the text message, we also stick in the checksum, though without signed messages the distribution mechanism is still vulnerable to spoofing.
We’ll put the following server and account information into our build.properties file:
Using public mail servers is cheap and easy: just create a new account.
To check that it worked, we send an email to ourselves only.
A full check would involve downloading the files and verifying that the checksum matched.
Once we’re happy with the target, we can change the recipient list to deliver the message to its intended audience.
The tolist, cclist, and bcclist attributes all take a list of comma-separated email addresses, so we could email more people by extending our recipients list property:
There’s a task called <loadfile> that can load an entire file into a single property, which could be a better way of storing the addresses.
We could keep the entire recipient list in a single file, and keep that file under SCM.
To send a prettier message, we need an HTML message body.
Our original message said “This is paypal security, please run this program to secure your account,” but the spam filters kept deleting it.
We have to escape the HTML elements by using a CDATA section.
Alternatively, we could keep the HTML message in a file, and point <mail> at that file with the messagefile attribute.
This would make it trickier to insert Ant properties into the message.
Sending email from a build is easy, especially when free email services provide the infrastructure.
All you need is the right JAR files and a network connection.
As with FTP, it isn’t a very secure way of distributing things.
The files are sent in the clear, and the recipient has to trust that senders are who they say they are.
The best way to upload files is to use SSH.
The next distribution activity is to upload our application using SSH.
This protocol encrypts all communications, authenticates the server to the client via a public key, and the client to the server via a public/private key pair or via a password.
It’s a secure way to connect to remote servers or upload applications, and is widely used.
It does require an SSH server on the remote host, which is common on Unix.
Commercial SSH servers are available for Windows, and there’s an excellent free client implementation in PuTTY.
It does take a bit of work setting up the trust between the local and remote system.
You should do this outside of Ant to reduce the sources of confusion.
Turn to the Ant tasks only after you have command-line SSH clients talking to the target server.
If you don’t already have these keys, use the appropriate tool (e.g., ssh-keygen) to create an SSH keypair, and set a passphrase on the private key for extra security.
Next, on the command line SSH client, connect to the server using your password authentication.
You must use exactly the same hostname as you intend to use for the.
The Ant tasks will fail if the remote destination isn’t a known host.
You should be asked for the passphrase of the identity—not the password.
If you get a password prompt, it means that the identity isn’t in the server’s authorized key list.
Once SSH is working, disconnect and use SCP to copy something over.
Only after the command line tools are working should you turn to the Ant tasks that use SSH.
We’re going to upload files to the server with <scp>, which transfers files over an SSH connection.
Both tasks have a dependency on the JSch library, from JCraft, at http://www.jcraft.com/jsch/
The JAR file needs to be on Ant’s classpath for <scp> to work.
Setting up the build file to upload the redistributable files is similar to the FTP upload of section 7.2
We can use the same fileset of files to upload, and use serverspecific property files to customize the task for different targets.
An initialization target, ssh-init, loads the file defined by the property server:
We have to declare the target server on the command line with the argument -Dserver=k2
If we want a default server, we could put in our personal build .properties file.
It’s easy to add an override there, and then forget about it.
IDEs that let you debug a build file are very useful to track down such problems.
This task has a set of attributes that closely match that of the scp program.
It can copy a single file to another filename, or copy an entire fileset to a specified directory.
The hard part is constructing the destination string b, which has the same syntax as in the scp command:
It can take some effort to get these strings right, which is why learning to use the command-line tool is good.
You can experiment there and use the results in your build files.
For setting up our destination path in Ant, we derive it from properties:
We don’t specify the password, because we’re using a private key to log in.
To do this, we set the <scp> task’s keyfile attribute to the location of the private key, and we set the password to unlock the key file in the passphrase attribute.
If the key file is unprotected, we can omit that attribute.
The result is a task to perform key-based authentication and upload of our files:
It’s good to see that everything really worked—setting up the trust between the two machines can be quite tricky.
If we run the task again, we get exactly the same output: there’s no dependency logic in this task.
We have Ant securely uploading our artifacts to a server, with both the server and client authenticating by using private/public keys.
Now we can retrieve the files from the remote site and check their checksums so we make sure that the upload really worked.
First, we have to set up the names of the source and destination files:
The <basename> task extracts the last item in a directory path and sets a property to it.
Here we use it to get the filename of the zip file from the full path.
We then construct a download string referring to the remote file, such as.
The <tempfile> task sets another property to the name of a temporary file.
The destination property is then set to the name of a file that doesn’t exist at the time the task is executed.
Unlike the createTempFile() method, the temporary file isn’t itself created.
With our remote and local filenames, we can copy the file from the server:
Compared to the declaration in the scp-upload target of section 7.4.1, the task has lost its nested fileset and the remoteToDir attribute, in exchange for the remoteFile and localToFile attributes.
After the download, we can verify that the checksum is the same as that of the original files:
Here <checksum> verifies that the file’s checksum matches the contents of the property attribute.
If there’s a mismatch, the condition fails and the build halts.
In this way, we can verify that a file we’ve pulled down has not been tampered with:
Imagine a build file that pulls down copies of your program from all the public mirrors and checks that.
We can do this in our build file with <sshexec>:
The information supplied to this target matches much of the <scp> command, though separate attributes are used for the username and host.
The command is executed at the far end in the shell and environment of the remote user, so wildcards are allowed; they are interpreted by the shell—not by Ant.
Creating the upload directories The <scp> task cannot upload to nonexistent directories; instead it will fail with the message  “no such file or directory.” To create the destination, we have to issue a mkdir command on that remote machine:
This mkdir -p command creates all the directories in one go and doesn’t fail if the directory already exists.
Making our scp-upload target depend upon this new target ensures that the destination directories are present before the upload.
There’s one more bit of SSH coverage left, and that is diagnosing failures.
Here are the error messages we’ve encountered, along with their meanings:
It could be that your public key isn’t in the host’s authorized_keys file.
If the command-line connection works, look at the keys, the password, and the passphrase and username properties.
The key troubleshooting step is running ssh on the command line first.
Because SSH is so secure, it’s the best way to access remote web sites.
If the remote system is running a web server, and <scp> is configured to upload into one of the directories in the web site, the artifacts can be downloaded.
As this is the main way that applications get downloaded by other people, having the build file check the download works; it prevents all those support calls that come when it doesn't.
To distribute via a Web server, we have to start Apache HTTPD or a similar application and upload the files into a published directory on the server.
By splitting up different stages in the build process, with targets that check the state of the previous operations, you can diagnose problems more quickly.
We now know that the long-haul upload worked properly, leaving only the new HTTP stage, for which we’ll use the <get> task.
Before doing that, we actually want to see if the web server is running on the remote server.
Before trying to download the files, we want to see if the web server is running.
This is useful during distribution, and it will become invaluable when we get to testing web applications.
The <http> condition looks for a remote page on a local or remote web server.
With the condition, we can test for local or remote web servers:
Fetching a JSP page will force the server to compile the page, if it hasn’t already been converted into Java code.
The server will return the HTTP error code of 500 when the page won’t compile, breaking the build.
A sibling test, <socket>, probes to see if a local or remote TCP socket is reachable.
Using these tests in a <condition> statement lets you control actions that could otherwise fail.
For example, you could send email if the local mail server is running, or deploy to a server if it was accessible, but you can skip that part of the build process if the mail server was not reachable.
You can use these network probes before network operations, skipping them if a server is absent.
If you use this test to set a property such as offline for tasks to use as a condition, then make the probe task conditional on this property not being already set.
This enables a notebook or home computer to run the build with the property set from the command line, disabling all network connection attempts.
This takes a hostname in the host attribute or a URL in the url attribute and tries to reach the remote host in the URL.
It does a low-level ping of a server, which is very reliable on a LAN, but rarely gets beyond a firewall.
It’s good for probing for local systems, though <socket> works better for checking that a host is actually listening on a known TCP port.
For retrieving files from a web server, the <http> condition is best.
It and the <get> task can fetch HTTP pages from remote sites, that being our next activity.
Any URL scheme that Java supports is valid in the url attribute, although the task is biased towards HTTP.
The dest attribute declares the filename to save the download to.
When working with http: and https: URLs, you can apply version-based checking to this download by using the usetimestamp attribute.
This tells the task to send the If-Modified-Since header to the web server, using the file’s.
Listing 7.2 The offline probe from Ant’s own build file.
The usetimestamp attribute for dependencybased downloads is valid only with HTTP.
If the server replies that the URL is unmodified, the task will not download the file again.
There’s an extended HTTP client under development, currently in the sandbox of not-yet-released extension “Antlib” libraries.
Go to Ant’s web site or SVN repository for details on this if you have complex HTTP requirements.
To download our redistributable file, we need to build a URL to the remote copy.
We add a new property to each of the properties files used for SSH uploads.
To ensure that this property is set, our download target must depend on the sshinit target to load the server-specific property file, and the checksum target that creates the validation checksums.
Because we set the verbose flag on the <get> task, the output includes a progress marker and source and destination information:
This output shows that the public web server is serving up the Zip file we uploaded earlier.
The SCP upload worked, and the web server is working.
This is a fully functional remote distribution, with automated testing alongside the upload operation.
If the upload and download targets succeed, we’ll know that everything works, without having to do any manual checks.
That completes our four redistribution activities: FTP, SSH/SCP, email, and HTTP.
All that’s left is to set up the build file so we can run all operations in one single build.
If we can do that, we have a completely hands-free distribution process.
We have created a set of targets to distribute the files using FTP, email, and SCP.
How do we run the same targets more than once with different values of the server property?
We know we can do this from the command line, with a series of repeated ant runs:
How do we do this inside Ant itself? With the task <antcall>, a task that runs any target and its dependencies, potentially with different properties.
It builds a big graph of all the targets, then it executes them in an order that guarantees that no target will be executed before its dependencies.
To upload our files to multiple hosts, we need to run the same target, multiple times, with different properties.
It takes the name of a target and runs that target and all its dependencies.
You can specify new properties and whether to pass down existing property definitions and datatype references.
The <antcall> task can call any target in the build file, with any property settings you choose.
This makes it equivalent to a subroutine call, except that instead of passing parameters as arguments, you have to define “well known properties.” Furthermore, any properties that the called target sets will not be remembered when the call completes.
A good way to view the behavior of <antcall> is as if you’re actually starting a new version of Ant, setting the target and some properties on the command line.
When you use this as a model of the task’s behavior, it makes more sense that when you call a target, its dependent targets are called also.
Now add a new target, which invokes the target via <antcall>:
This target defines some properties and then calls the do-echo target with one of the parameters overridden.
The first point to notice is that the init target has been called twice, once because call-echo depended upon it, and a second time inside the new <antcall> context because do-echo depended upon it.
The final observation is that the final trace message in the call-echo target appears only after the echo call has finished.
Ant has executed the entire dependency graph of the do-echo target as a subsidiary build within the new context.
This notion of Ant contexts is very similar to that of an environment in LISP or Scheme.
In those languages, an environment represents the complete set of definitions in which a function is evaluated.
Ant’s contexts are not so well isolated: Ant runs in a shared JVM; type definitions are global, and only properties and datatype references.
Some parts of the project are new, but the JVM is still shared.
It’s important to remember that all properties set in an <antcall> are local to that call.
Changes to the properties or references of the child project don’t propagate back up the calling build.
Information from the parent project can be passed down, if done carefully.
The inheritall flag can prevent the task from passing all existing properties down to the invoke target, that being the default behavior.
If the attribute is false, only new properties defined in the task declaration are passed down.
When you execute this target, the log showed that do-echo didn’t know the definition of arg2, as it was not passed down:
Regardless of the inheritance flag setting, Ant always passes down any properties set.
This means that anything manually set on the command line stays set, regardless of how you invoke a target.
Any properties defined on the command line always override anything set in the build file, no matter how hard the build file tries to avoid it.
This is actually very useful when you do want to control a complex build process from the command line, as you don’t need to care about how the build file is implemented internally.
You can also pass references down to the invoked target.
Creating new references is useful if the invoked target needs to use some path or patternset as one of its customizable parameters.
For the distribution problem, <antcall> will let us run the distribution targets against different servers by calling the targets multiple times, with the server property set to a different server on each call.
We just need a single target to issue the calls.
Having the minimal dependencies on each <antcall> target keeps the build fast and guarantees that the same artifacts are distributed to every server.
Once you start using <antcall>, the normal dependency rules of Ant are thrown out the window.
We’re not going to show the entire trace of the build, because it’s both verbose and repetitive.
As you can see, the ftp-upload target and its two predecessors, init and ftpinit, have run twice within a single build.
All told, it took less than two minutes to publish the packages to two local and two remote sites, and to email out the news.
This is what distribution should be: fully automated and available at the push of a button.
The build also shows how to use <antcall>, namely when you want to invoke the same target(s) more than once, perhaps with different properties.
Now, there’s one more aspect of <antcall> to look at: determining when its use is inappropriate.
If you see it a lot, something has gone wrong.
The common mistake is to use it to order all stages of a build:
As Ant creates a new project on every <antcall>, the build will be slow and memory hungry, with common targets being called repeatedly.
Except for targets that you want to call more than once, especially with different parameters, let Ant handle the order of targets by listing them in the dependencies attributes of other tasks.
Distribution is a common activity in a build process, which means that it  should be automated.
Nobody should be running programs from sites or people that they don’t trust.
This task lets you re-enter your build file, calling a named target with any properties you choose.
The <antcall> task is powerful and useful for some operations.
However, it does make a build slower and more complex.
Use <antcall> sparingly, remember that a target’s dependencies are also invoked, and don’t expect properties to be passed back to the caller.
With distribution out of the way, we’ve covered the entire process of using Ant to build, test, package, run, and distribute a Java program.
What we haven’t done is shown a single build file that does all of these activities.
It’s time for a quick review of all that we need Ant to do, with a single build file to do everything.
This review also will let us discuss how to write usable build files and how to migrate to Ant.
What we haven’t shown you is a single build file that incorporates all these things.
This chapter provides a higher-level view of our sample application’s build process, reviews the techniques that we’ve already presented, and introduces some new concepts.
The first concept is the most important: the art of writing good build files.
We’ve introduced the basic concepts, tasks, and types of Ant.
We’ve shown how i reads build files containing targets and tasks, showing Ant what needs to be done to build a project.
It should not become a maintenance project all of its own.
If you spend more time maintaining the build file than writing code, tests, or documentation, something has gone wrong.
There are several key ideas that we want to convey with our build file examples.
Start with that goal and work backwards as you write your targets.
The goal of our build file is to build a distributable JAR library that we can use in other build files and publish via SSH and email.
That gives us a dist target to create the distributables and a publish target to publish the file.
The dist target will need code that we compile, test, and package, giving us more targets and the dependencies between them.
And, of course, we need a clean target to clean up.
You can work backwards from the final goals into the stages of the build, each stage becoming a target.
Into the targets you can place the tasks needed to reach the current goal.
Integrate tests with the build We cannot overemphasize the importance of automated testing.
By putting testing into your build processes early, developers can write and execute tests without having to worry about the mechanics of how to run them.
The easier you make testing, the more tests get written, and the more tests get run.
This will directly improve the quality of the code and, hopefully, result in something you can ship sooner rather than later.
Keep it portable Ant runs on many platforms, hiding many details such as what the operating system uses as a file separator or how different platforms report errors differently.
Enable customization Ant properties allow for user, project, and per-build customizations.
Individual developers can override options in the build file by editing their build.properties files or by setting options with -D arguments on the command line.
You can also adapt to different machines by reading environment variables and Java system properties.
These are all practices that our example build files follow.
We’re writing the core classes of a diary application, classes to represent a calendar with events.
This library can be used in other applications once it’s packaged, tested, and distributed.
This chapter’s build file does all of that, integrating everything covered in the previous chapters.
It will compile and sign the JAR, run the tests, package everything into Zip files, then upload these to a remote site.
Projects begin with a name, a description, and, optionally, a default target.
Always give build files a unique name to avoid confusion when you work across projects.
Some text in the <description> element is useful, as it’s printed when Ant is passed the -p or the -projecthelp parameter.
You can have any name for the default target; we often use default because it avoids having to remember what the default target is when you ask for it on the command line:
This command asks for the clean target, then the default one.
Ant will first run clean and its dependencies, then default with its dependencies.
After the description come the public entry point targets and the initialization targets.
The entry point targets are those targets we expect people to call on the command line or our IDE.
This ensures that they’re always listed in the project help listings, and Ant-aware IDEs often highlight them.
Listing 8.1 shows the main entry points for the project: default, dist, publish, test, and clean.
This target uses <antcall> to upload the files to remote SSH and FTP Servers.
The final target, clean, uses properties that haven’t been declared yet, and doesn’t depend upon any other target.
All the properties it uses are to be declared outside of any target, later on in the build file.
Ant executes all out-of-target tasks before running any target, regardless of the relative location of targets and out-of-target tasks in the file.
This means that we can reorder targets for a most readable file, placing entry points ahead of the rest of the build file.
The next portion of the build file is dedicated to setting up the build by defining the main properties, paths and other datatypes of the project.
More datatypes may be defined inside targets; there’s no general rule for when to declare properties and types.
Having everything at the start makes it easier to locate declarations, but in-target declarations define properties closer to where they’re used, and allow you to incorporate the output of previous tasks and targets.
We like control of our build files, even if the original author felt they knew the right answers to everything.
How do we manage this? By having the build file load an optional properties file, build.properties.
Developers may not have a build.properties file, so the build file must be able to work without it being present.
There must be <property> tasks for every property the build file needs, with build.properties entirely optional.
Next we define the common properties of the project, the directories into which things go, and any other options we want to define in one place.
Listing 8.2 Setting Ant properties to the locations and values of the build file.
This listing shows the central definition of most properties used in the build.
They are properties that developers may want to override in their build.properties file.
For example, the version of the program is defined in this list b.
The one thing we cannot reliably do in a properties file is set relative file locations.
Whenever Ant encounters a <property> task that uses the location attribute, such as location="..", it resolves the location to an absolute path.
What would happen if we set the build.properties file to a different relative path?
After Ant loads the file, the value of parent would be "../.."
For a reliable build, you have to declare the full path:
This declaration ensures that the path is absolute, however it’s used.
After the property definitions come the datatypes, the paths, and filesets.
Declaring the datatypes In listing 8.3, we set the compile and test classpaths by chaining each path together for reduced maintenance.
Listing 8.3 Declaring the datatypes for the project: the paths and filesets that will be referred to by refid references in tasks.
We have to make sure that all paths, patternsets, filesets, and other datatypes have different id attributes.
It’s OK to clash with property names, but not with the ID of another datatype.
We also define a timestamp here for inserting into generated text files.
Any activity that does, such as creating a directory or compiling source, has been moved into a target.
Why? Because whenever Ant runs any build file, even just to get the -p target listing, it executes all tasks outside of a target.
It’s a bit unexpected if something as minor as running ant -p triggers some action such as creating all the output directories.
We tuck all such actions safely away in our init target:
It doesn’t bother to create all the directories, such as those for test results and reports.
We’ve chosen to leave those to the targets that run the tests.
It does create the build/ compile directory for compiled classes, because compiling the Java source is the next action in the build.
Compiling and testing are the core features of most Ant builds, just as they are the core of most Java projects.
Listing 8.4 Compiling the source and creating a JAR file.
The compile target also copies over all resources from the source directory into build/classes; the jar target creates the JAR from this directory.
The JAR is ready to be tested, which is the role of the targets of listing 8.5
Listing 8.5 Running the unit tests and creating the reports.
Create the test classes and data directories, deleting the data directories first to clean out old results.
Only when all tests are working can we move on to the next stage: creating a distribution.
The distribution targets create the documentation and the redistributable files.
We’ve chosen to only distribute Zip files to keep the build file leaner.
Listing 8.6 shows the first bit of work: getting the JavaDocs and other documentation into shape, converting the line endings on text files scripts into the right format for the target systems, and copying the patched files into a directory where they can be added to the Zip files.
Run all test cases called *Test except those known to fail.
After running these targets, the files are all ready for packaging, with both Windows.
We could avoid most of the fixup if all of our documentation was in HTML files, which are cross-platform.
Alongside the text files and JavaDocs, the distribution takes the source and the JAR file, a file that we want to sign.
Signing the JAR files Signing the JAR file authenticates it and enables uses such as Java Web Start deployment.
The build file uses the <input> task to prompt for a password, rather than hard code the password into the build file.
This is where developer customization in the build.properties file really kicks in.
However, it’s hard to retro-fit security into a Java project later on, because signed JARs are loaded differently.
If there’s any possibility of needing signed JARs, it’s better to start signing sooner rather than later.
With the JAR signed, the Zip files can be created.
Creating the Zip files The final packaging activity uses the <zip> task to create the binary and source Zip files.
This is the work of the targets in listing 8.8
We have two targets, unzip-src-zipfile and unzip-bin-zipfile, that will expand the Zip files and let you see what you created.
Once you start building Zip, JAR, or tar files using filesets from many places, it’s easy to pull too many or too few files into the artifacts.
Having a look at what’s produced is always wise, especially before you cut a release.
Nobody wants to upload or email Zip files that accidentally have 16MB of unneeded JAR files.
The diary project distributes its Zip files by emailing them out to known recipients and publishing them to a server using <scp>, the secure copy task.
These targets don’t depend on the packaging targets—they rely on the artifacts already existing.
By removing the dependencies, we can use <antcall> to invoke the targets with different properties, which lets us distribute the same Zip files to multiple destinations.
What we do need is a target that creates the checksums for the most recent Zip files.
Because the <checksum> task fails if the file attribute names a nonexistent file, this target implicitly checks that the Zip files are present.
This check stops us from uploading files that haven’t yet been created.
Some organizations, including Apache, sign their redistributable packages and emails with GNU Privacy Guard, to authenticate the checksums and provide an audit trail.
After the email comes the work to upload the files to a remote site.
After the upload, we copy it back to see that the checksums match.
Listing 8.10 Targets to upload the files to a remote server.
Projects that do publish this way should add the http-download target from that section to round off the upload.
For this build file, the scp targets are the last targets in the project.
Closing the file At the end of the file, the root element of the XML document must be closed, to make the document “well-formed XML.”
We cannot add any tasks, targets, or even comments after this closing tag.
That’s it! We’ve just walked through a complete build file, one that encodes the.
It shows how we’ve used the tool in one of our projects.
Now, how are you going to use it in your project?
When you use Ant, you get the opportunity to write a build file that describes how your project is built, tested, distributed, and deployed.
It may be a brand new application, or it may be a project that already builds under an IDE or by some other means.
You need to bring up the XML editor, and, starting with an empty <project> element, write the targets and tasks to automate your build process.
When you start with a new build file, you have complete control as to what it will do.
Where should you begin? Look at what the project has to deliver, and think about how Ant can help you do that.
Usually the application or library is the main deliverable, perhaps packaged in a Zip or tar file.
If you’re planning to host your application on a server, your deliverable needs to be a WAR or EAR file, which you then have to deploy.
The type of application you’re writing determines what the deliverables are and how you deploy or deliver these outputs.
Table 8.1 shows the outputs and distribution routes for common project types.
It has to create the deliverables and the artifacts, and then distribute or deploy them.
As an example, let’s imagine a calendar application that uses the diary library.
It will have a Swing GUI with supporting code and some HTML documentation.
We’ll package it as a JAR, then a Zip file, and distribute it by an SSH upload.
Each of these activities becomes a stage in the build.
Table 8.1 Common application types, their deliverables, and deployment routes.
Ant can handle all of this, with help from other tools.
Determine the build stages Once you have deliverables, you can list the stages needed to make them and the dependencies between them.
Start with the common states a project can be in, such as compiled, tested, and deployed, and think of the steps needed to achieve these goals.
Each major step in the build should have its own target for individual testing and use.
For our example client application, the entry-point targets would be all, test, dist, upload, and clean.
We would have internal targets: compile, archive, doc, and init, with more to come when needed.
The compile and test targets are central to our development process, as we want to run the tests whenever we build the application.
Plan the tests It’s never too early to start thinking about testing.
In Java, JUnit and sometimes TestNG are the foundations for testing, adding extension libraries to test specific technologies.
For our hypothetical client application, we have to test a Swing GUI.
A good model-view split lets us test the model with normal unit tests, leaving only the view as a problem.
An early action in the project will be to browse to explore the current options for testing Swing applications.
With luck, we should be able to perform the core GUI tests from a <junit> call.
The test will form its own source tree, alongside the application source.
This forces us to think about what makes a good directory structure for the program sources and for the generated files.
Lay out the source We need to think about the source layout right from the outset.
In particular, we should think about the Java package structure.
Doing so prevents contamination of the model by the view.
Web application WAR, code+JSP, SQL data Copy to web server; set up database.
Enterprise application EAR file with JAR and WAR files, SQL data Copy to application server; set up database.
If you want to access package-scoped classes and methods, you need to place tests into the same package as the classes they test.
As we’ve explained before, we like to put our tests into sub-packages because it forces us to use the public APIs and it lets us test even when our JARs are signed.
In our client application, we would have the layout illustrated in figure 8.1
Figure 8.1 How to lay out classes in a large project.
The files and directories in white are the source; those in grey are created in the build.
Source and test source code trees are split up and compiled into different directories.
Again, there are separate trees for the source and test files, so we.
We also need to save space for the XML test logs and the resulting HTML test reports, all of which can find a place under build/test.
With the directory layout and basic design of the build in place, we can now create the build file for the project.
Creating the core build file We can start a project by taking a team’s standard build file and customizing it.
If no such file exists, we start by coding the basic set of targets needed to get everyone building and testing code.
We don’t need to wait for the code before we start on the build file.
With no source for compiling or testing, Ant will still create the output directories and build an empty JAR file.
At this point, we have the foundation for our project: now it’s time to start coding.
Evolving the build file Nobody in the team should be afraid of editing the build file.
As they do so, they should try to keep the build file concise yet readable by using only a few pages to tell Ant how to build the project.
A build file for a project gets big and complex only if the build process itself is complicated.
When that happens, there are ways to manage the complexity, ways we shall explore in later chapters.
One problem with editing a build file is knowing which tasks to use.
Start by looking in the Ant documentation: there are so many tasks, you may find what you want.
There are also many tasks written by other people, tasks you can add to a build.
We’ll start exploring these tasks in chapter 9 once we’ve finished looking at how best to adopt Ant.
The next problem is how to move an existing project to the tool.
Migrating an existing project to Ant is possibly harder than starting with a new project.
Existing projects already have deliverables such as JAR and Zip files, test reports, and deployment processes that Ant needs to automate.
You aren’t in a position to make radical changes to the application design or directory layout; you rarely have much time for the migration; and if you break something, the team will give you a hard time.
This makes people reluctant to change an existing process, even if it’s hard work to use and extend.
In fact, the uglier and more complex the build process is, the more scared people are of fixing it.
This fear is unfounded: the uglier and more complex the build process is, the more it needs Ant.
We’ve found that it usually doesn’t take that long to move an existing project to Ant—that is, for a build file to compile, run, and archive an application.
Extending that build file with automated tests and deployment does take effort, but that will be ongoing effort.
Most likely, any project would benefit from more tests, automated testing, and automated deployment, so suggest Ant as the means of controlling these tasks.
Migrating to Ant is mostly a matter of following a fairly simple and straightforward process.
The ten steps of migration are listed in table 8.2
The key thing is to automate the basic bits of the build—creating the existing artifacts—without adding new things like testing until the basics are working and everyone is happy.
Try to move to Ant simply by adding a new build.xml file at the base of the project.
After that, it’s time to go live and start evolving the build file.
During the life of the project, you should rarely need to edit the build file to include new source files, documents, or tests; they should all be accommodated automatically.
The only reasons for build file maintenance should be new deliverables, new processing steps, and refactorings to clean up the process, such as moving all hard-coded paths and filenames into properties for easier overriding.
Table 8.2 Steps to migrate an existing project to Ant.
Check in Check everything in and tag it with a BEFORE_ANT label.
There should be no generated files in the project at this point.
From examining your existing build tool, make a list of your project outputs and the stages in creating them; build a list of Ant targets and dependencies from this.
Define directories Define your directory structure and the property names used to refer to these different directories.
Design the build file Make an initial design of your build file or reuse an existing one.
Arrange the source If you need to place the source into new directories, do so now.
Implement the build file Create the build file that you’ve defined or customize one you’re reusing.
Run a verbose build Run the build and verify that it’s working with the -verbose flag.
Add some tests Start writing tests if there were none already.
Evolve the build file Add more targets as you need them.
Testing after every little change is the key to a successful build file refactoring.
This chapter walked through a complete build file, one that contains.
An initial setup of the build, with properties and datatypes to configure Ant.
A compile stage, which compiles files, copies resources, and JARs the file up.
Targets to package the JAR and documentation into source and binary Zip files.
The information covered in this chapter shows what Ant can do: it can cover the gamut of build activities, from compilation to redistribution.
Readers should have the knowledge and tools necessary to build sophisticated, production-quality build files.
While there certainly are more tools and techniques available, they all rely upon the fundamentals we’ve already covered.
In the next section of this book, we’ll apply Ant and the techniques we’ve covered to a number of common development situations, such as code generation, Web applications, XML manipulation, Enterprise Java, and much more.
Applying Ant In the first part of this book, we introduced Ant, using it to compile, package and redistribute a library.
To build and test such applications, we need to automate many more activities, from managing builds that span more than one build file, to setting up the database and application server before we can deploy our application and run our tests.
Before then we’re going to look at more of Ant’s tasks, exploring the different.
The tasks we’re going to cover can all improve our application’s build in little ways: automating other steps in the process such as patching property and text files, or auditing the source.
Together they can automate even more steps of the process of getting software out the door.
They also will give the reader a clearer understanding of the nature of the tasks that constitute Ant’s functionality.
At this point in the book, we have a build file that can build, test, package, run, and distribute a Java library.
Therefore, even if you aren’t interested in the specific tasks we cover here, please skim the chapter and become familiar with the terminology.
You can accomplish a great deal with an out-of-the-box Ant installation.
However, eventually you’ll need more than it offers through its built-in tasks.
Ant’s optional tasks provide a set of extra features, many of which need extra libraries installed in Ant’s library directory.
There also are a growing number of Ant tasks—third-party tasks—which are written by other people.
These are often part of Java projects and make their applications more usable under Ant.
The broad variety of third-party tasks gives Ant its real power.
Altogether, there are four types of Ant tasks and datatypes:
Core—Tasks that work out of the box and are immediately available for use.
Optional—Tasks that ship with Ant but typically require libraries or external programs that do not ship with Ant.
Third-party—Tasks that were developed by others and which can be dropped into an Ant installation.
We also cover a few technical hitches that can occur when using optional and third-party tasks.
Optional Tasks In early versions of Ant, “optional” tasks were distributed as an add-on library (optional.jar) that users had to download separately.
Currently, Ant ships with a complete set of core and optional tasks.
Even so, there are still distinctions between the two task types.
Ant’s optional tasks come in different JAR files and the online documentation splits the tasks into two lists, core and optional.
With current distributions, this distinction may seem odd or unnecessary, but there are some remaining differences.
Most optional tasks need an extra library or program to work, and they come in JAR files that group them by their dependencies.
Optional tasks that don’t depend on third-party JAR files are all packaged into ant-optional.jar.
Normally this is invisible to users, unless they spend time browsing around Ant’s directories.
Those tasks that depend on extra libraries usually need them in Ant’s own classpath.
That means they must be in one of the directories from which Ant loads JAR files, a directory named on the command line with the -lib option, or listed in the CLASSPATH environment variable.
Ant’s documentation lists dependencies, which can change from release to release.
As Ant is built with the latest released versions of the external libraries, it’s good to upgrade these libraries when upgrading Ant.
The final difference between core and optional tasks is that many optional tasks do not get used or tested as much as the core set.
New tasks are added into the optional group either when they have an external dependency or when they’re viewed as less important than the core set.
In both cases, the number of users may be less than for other tasks, which means the support for and documentation of those tasks may be behind that of the main Ant tasks.
May does not mean that this is always the case.
You just need to be aware that if you use an optional task, you might have to do more of the maintenance yourself.
Third-party tasks The Ant team maintains the Ant runtime and the many core and optional tasks, but it lacks the time and skills to maintain all the other tasks that projects may need.
Third-party tasks are a common addition to many Ant-based projects.
This gives Ant users tasks that are tightly bound to the product and are maintained by the same development team, which results in high-quality tasks.
As a consequence, many of the most interesting things you can now do with Ant require third-party tasks.
We’ll explore some of these tasks throughout this section of the book.
Some people fear third-party tasks, preferring to use only the tasks that ship with.
They do this because they believe that third-party tasks are lower in quality than those that Ant bundles, and that Ant builds should use only those tasks that Ant bundles.
Many open source and commercial products provide Ant tasks to work with their products, and these are often well-written tasks that integrate seamlessly with the products themselves.
The quality, integration, and documentation for these tasks can be equal to or better than those that ship with Ant.
Third-party tasks, like optional tasks, are distributed in JAR files.
Since Ant 1.6, a new task-loading mechanism, Antlib, makes this process much simpler.
An Antlib is a JAR file whose type and task declarations are picked up by Ant when you declare the appropriate package URL in an XML namespace declaration.
The mechanism also lets library developers declare more than just the tasks; they can declare datatypes and define tasks in scripting language or XML, rather than just Java.
The Ant project itself is starting to distribute extension libraries as Antlibs, blurring the distinction between optional and third-party tasks.
After looking at some of Ant’s optional tasks, we’ll explore some third-party tasks.
First, we’ll look at the problem of getting an optional task installed.
To use most optional tasks, you must download and install extra libraries or programs.
The JAR containing the task itself isn’t always enough: any extra libraries that an optional task depends on must be on the classpath of Ant or you’ll see an error message.
If the optional task is present but a library that it depends on is missing, Ant will inform you of this fact.
This is not a bug; it is a configuration problem.
Ant’s documentation to see what’s needed and where it can be retrieved.
If the ant-jsch.jar file itself was missing, the class implementing the <scp>
This is not a bug; it is a configuration problem.
Ant assumes that any task implemented in its own packages is one of its own tasks, so Ant prints this special message.
It has not recognized that the task will need jsch.jar; it will only discover that fact once ant-jsch.jar is present.
Once you have the main set of optional JAR files for your projects, these error messages should be rare.
The only time they’ll surface is if you’re running Ant under an IDE and the relevant JAR files aren’t on the classpath.
In this situation, you’ll need to add them to the classpath through the appropriate IDE settings dialogs.
You can explicitly probe for a task by using the <typefound> condition.
This condition verifies that there’s a task matching that of the name attribute and that Ant can instantiate it, and it lets the build file fail with a helpful error message:
This example checks for the presence of the <scp> task and shows where the task and JAR come from.
If you run the target with verbose output (ant -v check-scp) you’ll see the detailed diagnostics that Ant normally prints when a task cannot be created.
If you encounter problems, the cause is likely to be one of a common set.
Here are the special problems you may encounter with optional tasks.
Missing mandatory helper library This is the most common problem.
When working with optional tasks, check the online documentation for information on which libraries are needed.
Missing optional helper library Some tasks will load but not work if a library is absent.
For example, the <mail> task needs mail.jar and activation.jar to send MIME messages, and it can send only plain text with attachments as UUENCODE-encoded files.
Tasks usually degrade gracefully or fail with task-specific error messages in these situations.
Out-of-date helper library Ant is built and tested against the latest release and development versions of all other open source Java libraries.
The latest versions of those libraries will have more fixed, new features and sometimes a changed API.
Missing task implementation Ant packages optional tasks by the names of their dependencies into files such as ant-junit.jar.
If one of these libraries is missing, none of the Ant tasks in it will be found.
Even if the JAR is there, it may not contain all the implementation classes.
This happens when someone makes their own build of Ant.
Library version conflicts If there are different versions of library classes on the classpath, Ant may end up running with an older version of the library.
This may happen if a JAR file—such as jython.jar—ships with some library classes (in this case the Jakarta-ORO regular expression classes)
The Ant documentation warns whenever a JAR file has this problem.
The only fix is to strip the particular library files, usually in a Zip file editor.
Missing executables Tasks that depend on an executable being on the path will fail with a platform-specific error if the program is absent or if the user’s path isn’t set up correctly.
On Windows, a message with “Error code 2” is the usual cue for this problem.
The power tool for tracking down many of these problems is ant -diagnostics.
If users of your build file are encountering problems, ask them to save the output of a diagnostics run to a file so that you can examine their configuration at your leisure.
There is also a <diagnostics> task, which runs the diagnostics code inside Ant itself:
This task is invaluable for troubleshooting Ant under an IDE, as it shows how the IDE has configured the tool.
Now that we’ve covered how to troubleshoot optional tasks, let’s use some.
Here we’ll explore a few more that often come in handy in a project:
Most of these tasks illustrate the optional nature of the tasks and require additional components to be installed in order to function properly.
For each task, we discuss the specific requirements it has and how to configure your system to run it.
One of the easiest and most common methods of configuring Ant and Java are Java property files.
The <propertyfile> task lets you create and manipulate property files, even incrementing numbers and dates.
We can use <propertyfile> to save the build date and time, machine name, user, and operating system into a properties file.
The date type is interesting, because it dynamically evaluates and formats the date; with value="now" it sets it to the current date or time.
When we get a support call related to this JAR, we can find out who built it, and when.
We could even have some Java code in the application to load the properties from the classpath, and print the information in a diagnostics screen.
While updating the file, the task doesn’t set any Ant properties.
It’s nice to have a properties file in a JAR with audit information—but how about setting some advanced properties, such as an auto-incrementing build number? Whenever the application is rebuilt, the number would increment.
We might also want to include an expiration date that our software could use to restrict the life of a demo version, or act as a trigger to probe for an updated release.
The <propertyfile> task can do both of these things, as it has the ability to increment numbers and dates.
Ant also includes a <buildnumber> task to increment build numbers more concisely.
In listing 9.2, we use both tasks to create/update a properties file at build-time, which not only stores the build number, but also sets an expiration date in the file.
Date types also support a unit attribute to define the value of the addition.
By setting the expiration.date entry to be the operation of “now+1 month”, we’ve set it to a date in the future.
Existing properties are untouched unless modified explicitly with an <entry> item.
All comments are lost, however; they are stripped out when the original is read in, and so they cannot be recreated.
The <javac> task passes .java files to the compiler if the corresponding .class file is older or absent.
It doesn’t rebuild classes—such as a parent class or an imported class—when the files that they depend upon change.
Rather, it looks at the generated .class files, extracts the references to other classes from them, and then deletes the class files if they’re out of date compared to their references.
In situations where there’s a large number of Java source files and when clean builds take a long time, the <depend> task is a great benefit to ensure incremental builds compile the correct files.
Adding the dependency check to the build process is fairly simple; we just paste it into the compile target above the <javac> call, as shown here:
Creates an expiry date by adding one month to today’s date.
The <depend> task requires two attributes, srcdir, which points to the Java source, and destdir, which points to the classes.
The cache attribute names a directory that is used to cache dependency information between runs; this directory is created on demand.
The <depend> task analyzes the .class files to determine which classes they depend on, and as this information doesn’t change when the source is unchanged, it can be safely cached from run to run to speed up the process.
Because it does speed up the process, however, we highly recommend that you always specify a cache directory.
The final attribute we’re using is closure, which tells the task whether to delete .class files if an indirect dependency has changed.
The merits of this attribute are unclear: it may be safer to set closure=true, but faster to leave it unset.
This isn’t mandatory; <depend> isn’t compiling the source and it doesn’t need to know where all the packages the source depends upon are stored.
Instead, the task uses any supplied classpath as a list of classes that may also have changed.
It looks inside JAR files to see the timestamps of classes, deleting local .class files if imports from the JAR have changed.
For faster dependency checking, list those JAR files that change regularly.
Running the target adds one more line to the compilation target’s output, here stating that seven files were deleted:
Therefore, a regular clean build is always a good idea.
These two tasks give a bit of the flavor of Ant’s optional tasks.
One set of useful optional tasks is the Software Configuration Management task family, which can be used to work with version control repositories to let a build run, check out, check in, or tag files.
There are a multitude of tasks that enable developers to make calls to the repositories from inside Ant.
These tasks can check in and check out code, and sometimes even add labels.
The exact set of services available depends on the particular SCM tool in use: each tool has a unique set of corresponding Ant tasks, most of which are implemented as optional tasks.
Each has its own tasks and its own set of operations.
Table 9.2 lists the corresponding Ant tasks showing which SCM systems support tasks for the comment actions of update, check out, check in, and label.
We’re not going to cover all the tasks, because there are many of them and some of the products are pretty obscure.
We’ll just look at CVS and mention its forthcoming successor, Subversion.
All the SCM tasks need external programs or libraries to run.
Most rely on native executables on the path, such as cvs, p4, svn, or cleartool.
During the development of this book, we used CVS servers as the repositories for.
We added a task to check out the code from the repository into a temporary directory, which makes it easy for developers to create a read-only copy of the source.
This method also enables you to check that the contents of the repository build properly.
Chapter 15 will automate this checkout and build as part of the continuous integration process, but a manual checkout is still useful to isolate SCM problems from those of the continuous integration server.
Here is the Ant script to check out the application using the <cvs> task:
When run, the <cvs> task will check out our project using the normal Windows/Unix CVS program, or fail with an error.
The <cvs> command also lets us label the repository and check in files.
Adventurous Ant users could have builds that automatically label the repository after a build, or check files in after updating them.
The <cvschangelog> task generates an XML file containing all the changes that have occurred within a specified date range on CVS modules.
The <cvstagdiff> task generates an XML file containing the differences between two CVS tags.
Ant ships with two XSL files, changelog.xsl and tagdiff.xsl, both in ANT_HOME/ etc, which turn these XML files into attractive hypertext markup language (HTML) reports.
You’ll need to refer to Ant’s documentation for more details on these tasks, but we leave you with the following example of how to generate a report from a CVS change log:
If CVS isn’t on the path, the <cvs> tasks—as with the other SCM tasks—fail with.
In Windows, “Create Process” and “code=2” usually appear somewhere in the message.
The standard way of debugging any of these SCM tasks is to.
Type cvs, svn, or the equivalent on the command line to verify the program is found.
Run the Ant target in -verbose mode to see the full command.
Run this full command on the command line to see why it doesn’t work.
If the executable is present, the usual cause of failure is either misconfiguration of the Ant task, or, for those tools that need some kind of password, incomplete authentication of the user.
Don’t try to use any of the SCM tasks until you have it working on the command line.
You can also give up on the tasks and just call the programs using <exec>:
We generally delegate SCM work (especially updates and labeling) to the continuous integration server.
Even so, it’s nice to have a task to use in some parts of the process, such as when labeling the repository before we make a release.
Although the book’s source was developed using CVS, at some point it will be migrated to Subversion.
Subversion, also known as svn, is the ultimate successor to CVS for open source and in-house projects.
Ant does not and will not ship with Subversion support out of the box.
Instead, a separate Antlib provides tasks that act as a thin wrapper around the svn command-line tool.
This library can be found on the Ant web site.
However, you can also use third-party tasks, which are any tasks that don’t come with Ant itself.
We mentioned them earlier in the chapter, and now it’s time to look at some in more detail.
There is an incredibly broad set of third-party Ant tasks.
In fact, there are more third-party tasks than there are core and optional tasks.
They cover almost everything you need to do in a project, from auditing your source code style to deploying web applications.
In this chapter we’ll look at logic and iteration tasks, then at code auditing.
In later chapters we’ll explore many other third-party task suites.
Three steps are all it takes to make third-party tasks usable in a build file:
The final two stages can be merged into one if you keep the JAR files in a well-known place, such as a directory in your SCM repository, and declare this classpath when you tell Ant about the tasks.
If you download a development tool or other product you want to use, look for the Ant tasks in the download or see if they’re mentioned on the web site.
The online documentation of Ant lists some tasks, but, as with any web site, the information listed at http://ant.apache.org/external.
If you can’t find what you’re looking for online, you can turn to search engines and the Ant user mailing list.
The Ant tasks should come in a JAR file, perhaps with some other library dependencies, and they need to be on the classpath.
Do this only if the library is to be used by all users of a machine on all projects.
Keep the files in a private directory and add the directory’s contents to the path via Ant’s -lib option.
You can include this directory in the ANT_ARGS environment variable for automatic inclusion.
These three options automatically add the task libraries to the classpath, so all you need to do in your build file is declare the mapping from task name to implementation class.
However, there’s one more trick you can use, though it requires a little extra coding in the build file: you can explicitly load the tasks using a classpath you set up in the build file.
This is the best tactic for a team project, because the JAR files can live in the SCM repository, and the build file declares them.
There’s no need for any work by the individual developers.
Adding the files to Ant’s classpath doesn’t make the tasks automatically available inside the build file.
Every build file that uses a third-party task must tell Ant to load the tasks, providing the names and/or XML namespaces to use.
This is called defining a task, which can be done in several ways.
To use a new third-party task in a build file, you need to tell Ant about it.
To define a single task, you must specify the task’s name and the class behind it.
The task name can be arbitrary, but it must be unique within the build file.
We’ll be exploring them in detail, but first let’s cover the ways of loading them.
Our build file now has the capability to use the <if> task in the same manner any other task is used.
To use the other tasks in the library, we could add other <taskdef> declarations, but this is needless work.
It’s better if the task library lists all its tasks in a properties file, which can be loaded in bulk.
It lists the tasks in the JAR and the names by which they should be used, including the <if> task we declared earlier.
The <taskdef> command can do the bulk of this entire file, defining all the tasks in one go:
Some projects also define datatypes in a separate file, which are declared with a <typedef> statement:
This statement will declare the Ant datatypes, but do this in a new classloader, not the one used for the tasks, even though the same classpath was used in both declarations.
Having the classes loaded in separate classloaders can stop the tasks from using the types properly.
It doesn’t matter what the string is, only that it’s a unique name for the shared classloader.
Bulk loading of tasks through a properties file is efficient, and most third-party libraries come with a properties file for this purpose.
This may seem unlikely, but as Ant continually adds new tasks and types, over time it’s almost inevitable—unless we do something to avoid the problem.
We need a way of telling Ant about third-party tasks yet keeping them separate from Ant’s own tasks.
What happens if you try to define a task or type if something of the same name has already been defined?
If the new definition is identical to the original, then the redefinition is silently ignored.
If the new definition is identical except for a different classpath, a warning message “Trying to override old definition” appears, and the new definition is accepted.
The result is that you can redefine new tasks with the same name as old ones.
This guarantees that a new version of Ant will not break your old build files, even if a thirdparty task used a name that is now claimed by Ant itself.
You won’t be able to use the new tasks, and tools and users of the build file may not realize that a third-party task is being used.
One solution is to give tasks very unique names, often a prefix such as ac—for the Ant-contrib tasks—a prefix that has to be used inside the properties file that defines the tasks.
Since Ant 1.6, it’s been possible to declare tasks into their own namespace.
Once that’s done, its “local” name can be the same as any task in any other namespace, yet there will be no name collision.
To recap the basic rules of XML namespaces that are covered in Appendix B: XML elements and attributes can be declared in a namespace, by first defining a prefix mapping to a URI, then qualifying the element or attribute declaration with the prefix.
For example, when Ant-contrib’s <switch> task is declared in the default Ant namespace, its appears in the file as just the task’s name:
If the task were declared in a namespace, the task would need to be accessed via a prefixed name:
Elements and attributes inside the task do not need to be prefixed, unless you want to include a type or task from a different namespace, or to make things clearer to readers.
Furthermore, the name of the prefix doesn’t matter, only the URI to which it’s bound.
You can reuse the same namespace prefix in different parts of the build file, and in different build files.
One complication with XML namespaces is that the namespace needs to be declared, either in the element using the namespace, or in an XML element that contains it.
In the example above we’ve declared it in the target, but usually we declare them in the <project> declaration.
This lets you use the task anywhere in the project without having to re-declare the namespace.
In this book, we declare most third-party tasks into new namespaces.
It may seem complicated at first, but there are long-term benefits with guaranteed unique names.
The real benefit comes with Antlib libraries, which we are about to explore.
Prior to Ant 1.6, the main way for declaring third-party tasks was via property files.
This has now been supplemented with Antlibs, which are JARs containing Ant tasks and types—tasks and types that are listed in an XML file.
We’ll cover the latter two tasks in the next chapter.
Everything in the library is loaded in the same classloader.
Ant can automatically load Antlib declarations from special antlib: URIs.
Automatic library loading can dramatically simplify how you use third-party tasks.
From then on, you can declare tasks or types in that namespace.
When Ant first encounters an Antlib namespace declaration, it tries to load the library but silently ignores all failures (it’s exactly equivalent to setting onerror="ignore" inside <typedef>)
The moment Ant actually tries to instantiate a task or type in the namespace, it will try again, this time failing with an error message if something went wrong.
This will be the usual "failed to create task or type" error message, with an extra Antlib-specific paragraph:
Antlib libraries can still be manually installed using the <typedef> task.
This allows skilled developers to do advanced things such as set up an explicit path if the library isn’t already in Ant’s classpath, or to use the library in the same build file in which the tasks are compiled and packaged.
Let’s explore this in a real set of tasks, the Ant-contrib suite, which provides useful functionality to build files.
One of the most popular suites of third-party tasks is the Ant-contrib Antlib, from http://ant-contrib.sourceforge.net.
These are invaluable in a big project because they add extra logic and decisionmaking operations to Ant, including iteration and exception handling.
Ant-contrib is a long-standing project, with stable tasks that are widely used.
It contains some tasks that aren’t in the Ant core.
The reason is that they’re viewed as “too procedural,” which is in opposition to the declarative goal of Ant.
Another reason is that they violate Ant’s “rules,” such as the one about properties being immutable.
They also cover things that Ant doesn’t, such as C++ compilation and mathematical operations.
As a result, any complex project is likely to find something of use in these third-party tasks.
The core of the Ant-contrib suite is its logic tasks.
Table 9.3 lists the tasks that extend Ant’s logic and execution abilities.
Alongside logic come the property operations of table 9.4, tasks that offer simple mathematics and dynamic property evaluation.
The <var> task does something that’s completely against all of Ant’s immutability rules.
There are some extra conditions in the library, all listed in table 9.5
Finally, there are a few other tasks, which are not so easy to classify.
We’re not going to cover any of them, but they’re available if ever you have a need for them.
There’s also a listener that records how long tasks take to execute.
At the time of writing, the version of their library was 1.0b2
Some of the tasks also require extra libraries and dependencies that are listed in the documentation, which you should pick up alongside the ant-contrib.jar file.
Table 9.5 Extra  Ant-contrib conditions to use in build files.
We may also need to tell the IDE about it too, adding it to its Ant classpath.
To explicitly import the tasks from a different location, we would need to set a path to the location of the JAR, then use a <typedef> declaration:
The first one does nothing if the tasks cannot be defined for any reason.
The report option prints errors but continues the build, while the final two will halt the build if trouble is detected.
It will raise an exception if the task or type does not load but do nothing if the resource itself isn’t found.
The strictest option, failall, raises an immediate error if the identified resource/URI isn’t resolvable, and is the best choice for immediately finding a problem.
The other options are relevant only for backwards compatibility or for skipping a premature <typedef> in projects in which custom tasks are actually compiled.
Once the tasks are loaded, they can be used alongside core and optional tasks.
We’ll now have a quick look at some of the most useful Ant-contrib tasks.
This only scratches the surface of the suite, so don’t be afraid to look around the documentation whenever you get a chance.
If there’s something you want to do in Ant that it doesn’t appear to do out of the box, the Ant-contrib library should be the next place to look.
For now, we’ll look at how to copy properties, some of the extra logic operations, and exception handling to give readers an idea of what the Antlib can do.
Copying properties One thing Ant lacks is pointers (or array indexing), there’s no way to refer to a property by way of another property.
That’s the kind of weakness that the Ant-contrib team view as a challenge.
Their <propertycopy> task lets you use property names to indirectly reference a property, which is a crude way of having pointers or arrays:
Using if/then/else logic Another recurrent problem in Ant is performing if/then/else tests and switching logic.
The logic tasks from Antcontrib make it possible, so we could fetch a web page only if the server is present and serving up pages without an error code:
You can accomplish the same thing with conditional targets with their if and unless attributes set to the name of a condition evaluated in a predecessor.
The Ant-contrib approach eliminates the temporary property and the need for multiple targets at the expense of hiding the conditional nature of the execution from Antaware tools.
Unlike Java, there’s no fall-through from one case to another, so there’s no need for a <break/> element.
Catching task exceptions A failing Ant task normally immediately stops the build with a BUILD FAILED banner.
If you want the build to continue when a task fails, turn to Ant-contrib’s <trycatch> task.
Returning to the example of a web page download, we could use it to clean up if a <get> request fails:
Executing this target produces this output when there’s no server:
After printing out the error, we threw the exception again using the <throw> task, which also comes from Antcontrib.
This is an extension of <fail> that takes a reference to an existing exception to throw.
The <trycatch> task is invaluable where you need to clean up when something goes wrong, such as stopping a program or removing temporary files after tests fail.
Iterative execution Sometimes, you may find yourself wishing there was a way to perform a set of Ant tasks for every file in a fileset, or iterating over a list of values.
Both of these tasks will take anything iterable: a list, a list of paths, or any Java datatype that has a method Iterator iterator()
This is needlessly inefficient, hence its replacement, the <foreach> task.
This takes a sequence of tasks inline, running them once for every item.
Properties set in the sequence are still immutable; any property set in the first iteration will not change the next time through the loop.
The task also takes a path, running the sequence for every file in the path.
Here’s a target that will print out every source file:
Developers could use this task to distribute files to a list of FTP sites or to pass a set of files to a <java> program, one after the other.
If the option parallel="true" is set, the iterations are run in parallel.
Most Ant tasks are never designed to be threadsafe, so be very careful with this.
Handling out-of-date artifacts The final Ant-contrib task that we’ll cover is the <outofdate> task.
This task runs a nested sequence of tasks if any target files are out of date compared to a set of source files.
Ant’s <uptodate> task compares a set of source files against a single destination file, setting a property if the destination file is up-to-date compared to all the source files.
A successor target can be made conditional on this scenario.
Instead of declaring any destination files, we list a mapper from our Java files to the generated HTML pages.
The result? javadoc runs only when one of the HTML pages is older than the Java file it documents or when it’s actually absent.
This is a simple use of the task; it offers many more options.
Note that <outofdate> is not only a task—it’s a condition.
Overall, Ant-contrib represents a way to boost Ant with a broadly usable set of tasks.
This quick overview of the library has only explored some of the main tasks it offers.
If you’re writing build files for complex projects, there is likely to be something of immediate relevance in there.
Alongside such general-purpose libraries come many that are designed for one specific purpose.
We’ll close this chapter with one such library, Checkstyle, to audit Java source code, as it’s a useful addition to any project.
Imagine, whenever you wrote some Java source, the “code police” would review your code and see if it broke any rules set by the team.
They could enforce code style policies and highlight possible design errors.
This could either be very irritating or very useful, especially on a large project.
With consistent code it’s easier for people to work on different parts of the application, and no bits of the application would degrade to low-quality code while the rest of the team was distracted.
It works as a command-line tool, an Ant task, and an IDE-plugin.
It has the capability to check the following, and more:
Code policies, such as final parameters, whether inline ?: conditions that are.
Checkstyle is configured by an XML file that declares which of its built-in or plug-in modules should be run over your code and what options each module has.
It ships with a default policy that implements Sun’s coding conventions (http://java.sun.com/ docs/codeconv/) and the style rules of Effective Java (2001)
The <checkstyle> task takes multiple source filesets; we omitted the test fileset for simplicity and because we may want some different rules there.
Passing down the classpath of the compiled code is optional, but it helps with some design checks, particularly one that examines thrown exceptions to see if they’re derived from RuntimeException and hence don’t need to be declared.
Like <junitreport>, the task has formatters write its output to the console or log file, as well as to XML format for integrated reporting.
The XSL file to do this is one of those that comes with Checkstyle.
We’ve taken both the XSL sheets and the sample Checkstyle policy file and put them in a directory under SCM.
We then commented out most of the checks, especially the whitespace ones.
When you start adding this to an existing project, you get a lot of whitespace errors, most of which are irrelevant.
Run checkstyle over our source, printing out to the console and to an XML file.
It’s up to every project to choose the rules and the options they want.
Individuals cannot override these, which is exactly what you want.
Project-wide code rules, by their very nature, should not be something that developers can customize.
In an open source project, having machineenforced rules is one way of making it easier for outsiders to read the source.
It can also ensure that any patches submitted to the project follow the rules.
You can obtain the latest Checkstyle release version from http://checkstyle.sourceforge .net; we used version 4.0beta5
The tool comes with a lot of extra JAR files; we were.
When we run this, we get a trace of errors, some generated reports and finally a halted build.
Adopting it can be painful, as it can throw many errors at the beginning.
Start by commenting out most of the rules and focusing on the serious problems.
Ant itself uses a far stricter policy set than that of listing 9.4, but it doesn’t require the task to succeed before releasing a distribution.
A very strict project may want to do just that.
One thing to remember about Checkstyle is that it’s a style checker.
It doesn’t check that the code works, only that the source looks right.
However, Checkstyle does look at the code and, as figure 9.1 demonstrates, it can find many defects.
Indeed, the biggest problem in retrofitting it to an existing project is just how many problems it finds.
It takes only a few more lines in a build file to add code-auditing from Checkstyle to a project, and it can find bugs that haven’t been caught in testing.
It’s a fantastic example of a new feature a team can add to a build once Ant is in charge of the compile and test process.
It also shows how third-party tasks can radically improve what the build file does for the developer team.
Figure 9.1 Stop! It’s the code police! The tool also has caught the fact that we’ve forgotten to make a constant final.
The case rule of constants (all capitals) did not match that of a variable.
Ant provides core tasks compiled into ant.jar and also ships with optional tasks that typically require additional components in order to function properly.
Many third-party tasks also exist, adding extra functionality to your build process.
These two tasks are how you can tell Ant about new task libraries.
With the Antlib feature, you can even have task libraries loaded just by stating the right URL in an XML namespace declaration.
In this chapter, we looked at the various SCM tasks as a sample of Ant’s optional tasks, then we looked at two useful third-party libraries: Ant-contrib, and Checkstyle.
Ant-contrib provides many extra features to Ant—features big and complex projects can use—while Checkstyle automates the “code police.” Both are great.
Ant’s web site documentation and Ant’s web site itself list many third-party tasks.
If Ant doesn’t provide what you need, check with the Ant web site or with the vendor of the product you’re automating around.
If all else fails, check with the Ant user email list.
Best practices with third-party and optional tasks We routinely use Ant’s optional tasks, as well as third-party and custom tasks.
Don’t be put off by tasks that require you to download additional dependencies.
Third-party tasks can also be added to this directory, or added to the classpath with a -lib argument.
Explicitly declaring the classpath in a <taskdef> declaration adds some work, but doing so can help the build file being used by other developers.
In this book, we declare all third-party tasks into their own namespaces.
It also enables the Antlib library loading mechanism, whereby the tasks and types of a library are automatically loaded just by identifying the package in an antlib: URL.
More and more third-party tasks provide antlib.xml descriptors to enable this feature.
If you can, keep task libraries and their dependencies under source code control.
Building your system should be as easy as checking out the repository and typing ant or maybe ant -lib with an SCM-managed directory.
When this book introduced Ant, it started off with a small project—a diary library.
As the chapters added more features, the project itself grew.
It’s now becoming a big project, as we’re planning to use it in a web application.
We now have the problem of scale to deal with, which is where this chapter comes to our aid.
It looks at how to use Ant with big projects.
That means projects that use multiple Ant build files to build different parts of a big system.
Somehow, we want Ant to integrate those separate builds into one big application.
Scalability is one of the classic challenges of software engineering.
How do you build very large projects? How do you manage library dependencies? How do you manage the many build files you end up with? These are the problems that big projects encounter.
Ant itself is continually evolving to address these problems, as developers use it for larger and larger projects.
Don’t wait until your project gets big before adopting features in Ant that are needed in big projects; small projects can benefit too.
Indeed, the earlier you start to use some of the techniques, the better.
The end is your project’s deliverables, and as a project gets bigger, the structure of the application itself has to change.
You may break the code into different modules, each comprising its own JAR/WAR/EAR file.
You may split the project into subprojects, each with its own source tree.
You may even have different release schedules for the subprojects.
But no matter what, you will still need to test it all.
Ant should not dictate how you organize your project; instead it has to adapt to you.
In this chapter we’ll cover the other problems, starting with that of delegating work across build files.
There’s more to do, there are more people on the team, and the integration issues are worse.
A small project could have one artifact, such as a JAR file, and its documentation.
A large project could have client-side and server-side components, native library add-ins, and a database somewhere.
These all need to be built, tested, and deployed together.
If the build process is inadequate, the effort of managing the build can spiral out of control.
Can Ant manage the build for a big project? Yes.
It may be great for small to medium projects, but it also scales up to work with large ones.
Like any software scaling exercise, scaling up doesn’t come automatically: you need to plan.
You also need the other foundational tools of a large project—source control management, defect tracking, and some planning—that we’ll assume you have in place.
It has a core library, and we’re now about to write a web-based front end and, perhaps, data persistence.
We still want to be able to run a single build file to bring it all up-to-date.
The standard solution to size in any system is to break it into smaller, more manageable modules.
In Ant, that means dividing the application into child projects, each with its own set of deliverables.
For our example application, penciling in some future subprojects gives us a number of child projects, as shown in table 10.1
These projects need to build in the right order, with the generated artifacts passed down to those projects that depend upon them.
We’ll do this with a master build file that can call the subprojects in the order that the file’s authors specify, with significant control over these invoked builds.
It can invoke a target and all its dependencies with a new set of properties.
This enables you to divide your build file into subprojects, one for each of the child projects of the actual software project, and call them from other build files.
The basic use of the <ant> task is simple: you use it to call any target in any other build file, passing in properties and references, if you desire.
When you call a target with it, you implicitly invoke any other target in the build file that the invoked target depends on.
That means if we use <ant> to call the dist target of another build file, that build file will run every target required to create the distribution.
To use <ant>, we have to know the name of a target to call, which means every project must have a standard set of targets.
The default target is going to be the default for each project, and will usually depend on dist to create a distribution.
The noop target is a special target added to test the whole build process.
In code, the stub targets look like those in listing 10.1
We now add similar targets for the other projects, resulting in a set of entry points whose meaning is consistent across the projects.
With all the projects laid out under a root directory, diary, we can create a basic master build file that calls the targets.
Listing 10.2 shows a master build file that builds the five subprojects.
This build file contains one target that invokes the subprojects.
We ordered the <ant> calls to ensure that all predecessor projects are built before those that depend on them.
Although the default target here is all, we use a property so that it can be overridden on the command line.
To run, say, the noop target, we would run Ant, explicitly selecting the noop target in our subsidiary build files by way of the target property:
This produces a trace of all the targets that are run:
Ant doesn’t show which subprojects are being built, unless you ask for it with a -verbose option.
A good trick is for every build file to print out its filename/directory with an <echo> statement outside any target.
This shows how Ant, a master build file, can delegate down to a set of child build files.
It’s the secret for breaking things up into modules, yet coordinating work between them.
To manage that work, we need to keep the master build file under control.
We have a master build file that can delegate to child projects, but the ordering of those projects is done by hand.
Why not use Ant’s target structure to model dependencies between subprojects? If there’s a target defined in the master build file for each subproject, invoking it with <ant>, then we can use their depends attributes to state how they depend on each other.
Ant will then control the order in which targets are run and, hence, subprojects are built.
The first step is to determine the direct dependencies between projects, as shown in figure 10.1
Figure 10.1 A graph of the direct dependencies between our modules.
Just as before, a call to ant delegate -Dtarget=clean will run the target "clean" on all delegate systems, only now Ant handles the correct order of child project invocation.
We can then write entry points to the build file, each using <antcall> to invoke the delegate target, setting the target parameter for us:
At this point, we can use the master build file to coordinate the overall build.
Calling a target such as all will delegate down to the child projects, calling the same target in each of them, in the order that the projects depend on each other.
This means that Ant can handle the problems of ordering the child projects—which becomes more important the more projects you have.
The master build file can do one other thing: it can control those child builds by passing data down to them.
We’ve just shown how to subdivide a project into a number of standalone child projects—each with its own build file—with one master build file to integrate them all and execute them in the right order.
If there’s a problem in this design, it’s that we don’t want to have to declare the same properties and tasks in all the different child projects.
There are ways to do this, which we shall now explore.
Master build files can control their child projects through properties.
Because of Ant’s property immutability rule, a child project cannot override any property set by a master build file.
This lets you write master build files that control complex details of the child project, even child projects that were never written to be called from a master build file.
As an example, figure 10.2 shows a build file that sets the dist.dir property for two child projects.
The outcome of this operation will be that the two child projects will place all their final distribution files into a single directory, rather than into their own directories.
In all our uses of the <ant> task so far, we’ve carefully declared inheritall= "false" without explaining why.
Although the two tasks share the same implementation code, when creating a master build file you use them slightly differently.
The <ant> task can invoke a completely separate build file.
Any of the properties and references that the <ant> task sets for the invoked project are immutable.
You can control the settings of a child project by predefining any property or path before its own initialization code tries to define it.
If the <ant> call defines a dest.dir property and all child projects use that property to name the directory for their redistributables, then that location becomes the destination directory for all distribution files—even if the child build files try to redefine it.
To use this feature, you need to know the rules by which properties are passed down:
Properties set on the command line are always passed down and can never be overridden.
If inheritAll is true, all properties set in the master build file are passed to the child projects.
Any properties defined inside <ant> override those set in the master build, but not the command line.
If inheritAll is false, only those properties defined inside the <ant> declaration and the command line are passed down.
Figure 10.2 A master build can set the properties for the child projects, even if those projects try to override them.
If the master build had accidentally used value instead of location, the directory location would have still been resolved in the client build files relative to their own directory, which would be a bug.
The command-line rule means that you can configure the master build from the.
Of course, this works only if projects are designed to be overridden.
Designing a project for easy overriding Controlling where the projects place their distribution packages is one common control option for a master build; others are which tests to run and which servers to deploy against.
For a child project to be controllable, it needs to make extensive use of properties.
A good build file should already be using properties to define any string, attribute, or file that’s used in multiple places.
For easy integration into a larger project, any option that could sensibly be overridden should first be defined with a property and then referred to, giving the master build an option to change the value.
To make this work, all child project build files should use the same names for the same controllable options.
It’s no use if core/build.xml used dest.dir for the destination directory, and webapp/build.xml used destination.dir.
In a single build file, using the value attribute to define a file location works, because when these properties are resolved to file locations, it will be in the same build file.
When you’re passing properties around to other build files, using the location attribute ensures that relative paths are resolved in the build file declaring the property, not in the build file using the property.
Sometimes, overridden projects get corrupted by some of the properties of the parent project.
This happens if the parent accidentally sets a property that’s used in the internal workings of the project.
Imagine if the child used a property tests .failed to log whether the unit tests failed.
If the parent project set the same property, the child project would think that the tests had failed.
The way to avoid this is to have standard names for properties that parent build files are expected to configure in children, and for the parent build files to not pass down any other properties.
This is dangerous as it increases the risk of an accidental property overwrite.
This is why setting inheritall="false" is so important: the best way to pass information down is inside the <ant> call.
Loading properties from a file is powerful, because a single file can then control which properties are set.
For example, we could modify our targets to load a common file, the values of which would be set in all the child projects:
This would force all projects to stick their distributables into the same target directory.
We cannot place relative file references in the file, as all properties are treated as simple values.
The <propertyset> element can be used to pass down a whole set of properties.
It lets you specify a prefix or regular expression for properties, which is useful for bulk propagation of values.
The last of these examples passes down a single property, but only if it’s defined.
Ant’s own documentation covers a few more features and examples; consult them if you have any complicated property propagation needs.
The other way to configure child projects is to have each one pull in a configuration file from the parent directory.
This ensures that every child project shares common settings, even when not invoked from a parent build file.
Next we set a property root.dir to the parent directory c, unless it was set to something else in our build.properties file.
If you move a project, you can point to the original root.dir location in your local build.properties file.
This is a highly effective way of eliminating repetition across projects.
Overall, <ant> forms the basis for sharing work across build files.
There’s nothing to stop a project taking an existing project from a third party, such as an open source project, and rebuilding it with customized settings.
There are some other tasks, built-in and third-party, that extend <ant> with more complex delegations.
Ant developers should know of them in case they have a need to use them.
The <subant> task exists to make delegation to an entire set of build files easier.
It looks very similar to <ant>, with various enhancements and with some corrections of default values that are, with hindsight, wrong:
You can run the same “generic” build file in all the specified directories.
Setting target="" will invoke the default target of the project; <ant> would.
Want to build the default target of all projects under the extensions directory, ignoring failures? It takes only three lines:
Applying it to our build file, we can use <subant> to delegate all our work.
In chapter 11 we’ll introduce an advanced way of ordering child builds, using the Ivy library management Antlib to create a list of projects sorted correctly for their internal dependencies.
The <subant> task can take this list and invoke the children in the right order.
Accordingly, we’ll return to this task in the next chapter.
Here’s an example of using return to get some checksum values back from our core project:
The two tasks are used much less frequently than the other delegation tasks, but enable something that isn’t built into Ant—the ability to use build files as function calls.
In either case, we try not to delegate too deeply: the master build file manages dependencies between the build files, and the individual build files assume that they are being called in the right order.
Any team that tries too hard to chain work together across too many build files is going to end up confused about what’s going on, and end up with a build that doesn’t behave.
What we do strive for across all our build files is code reuse by sharing and extending template build files.
Rather than copy and paste a working build file across ten projects, we try to have one build file that acts a bit like “base class” for the child projects.
Once you have more than one build file, you have a problem keeping them synchronized.
If you deal with this problem by copying and pasting values, your project is doomed to become a build file maintenance nightmare, just as when you use copyand-paste coding throughout your Java source.
The solution, in build files as well as Java, is this: “Don’t Repeat Yourself!” This is a phrase from The Pragmatic Programmer, and it’s an important rule to.
Even with a fully automated build, it’s hard to stop repetition from creeping into large projects.
Once a project has more than one build file, you tend to want the same targets in each.
Unless you can share these targets, you’ll end up with duplicate targets in different files.
Repeated options, such as the names of files or directories.
Repeated declarations of tasks, with the same or slightly different parameters.
Repeated options is the easiest of these to deal with: use properties to avoid declaring the same constant text more than once, which is what we’ve been doing in the book up till now.
More advanced features of Ant are needed to address the others.
The first of these will deal with the problem of repeated fragments of XML in different build files.
The first step to doing this is to know that in pure XML, the only way to share data between files is to use XML entities.
First we must declare the entity at the beginning of the file, after the <?xml?> header and before the XML data itself:
This doesn’t insert the file yet; it merely makes it known to the XML parser using the name properties.
The path to the file must be an absolute or relative URL and cannot contain Ant properties.
To insert the XML text inside the build file, we must declare the entity name.
When parsing the file, the XML parser will replace all entity references with the text behind them.
Ant sees everything from the included file as if it were pasted directly into the main XML file.
This technique is very limited, primarily because the XML parser does all the work.
Only incomplete fragments of an XML document can be included.
You can’t use Ant properties in setting the path to the file you want to include, and there’s no way of overriding included targets with new targets.
This mechanism is now obsolete, at least within Ant, which has its own solutionthe <import> task.
To import a build file into the current build file, you just use an <import> task outside any target:
When Ant processes a build file, it runs through all declarations outside the targets before executing any requested target.
Whenever Ant encounters an <import> declaration, it imports the referenced file by.
Handling collisions between target names by renaming the targets of the imported file.
On the surface, the task looks very similar to that of XML entities.
At first glance, <import> doesn’t appear significantly better than XML entities, but it is, because Ant is in control of the process.
It happens as the containing build file is interpreted, so property expansion can be used to locate the file.
Ant interprets the targets declared in the file specially, and provides what is effectively a simple inheritance model of build files.
One of the big differences is that Ant lets you override the targets defined in a file that’s pulled in via <import>
When we import a build file, it can add targets to our own build file.
Ant allows us to override those imported targets with new ones.
That gives build files a feature very similar to subclassing in Java itself.
A base build file can define the default behavior for a project by using standard targets, and build files that import the project can override this behavior by redefining the targets.
Let’s explore this behavior with a base.xml file containing the targets init, all, and dist:
Now import base.xml into a new build file, which will import this project and define a new dist target:
Declare at top of document Declare anywhere outside a target.
Takes absolute or relative URL references Takes absolute or relative filenames.
Must resolve to a file optional="true" imports will skip missing files.
Must resolve to a fragment of an XML document Must resolve to a complete Ant build file.
It’s an error to declare targets with the same name.
Are interpreted in the directory of the file into which they’re imported.
Has access to the original name and directory of the imported file through properties.
What’s going to happen? If we run ant -f app.xml all, what is printed?
The imported all target has run, as has its direct dependency, dist.
But instead of the implementation from the same build file running, the one from the outer build file has been executed.
And, because the new dist target did not depend upon the init target, that target wasn’t called at all.
The targets in the imported file are available on the command line.
Targets in the imported file can depend upon targets in the main file, and vice versa.
If a target is already defined in the build file, then it overrides any in the imported file.
When such an override takes place, any dependencies of the overridden target are lost.
These are the main aspects of the behavior of <import>
These inheritances and overwritten rules let you selectively override targets of a base build file, creating extended build files.
What if you don’t want Ant to override a target? You cannot prevent that, but you can explicitly invoke the original target by prefixing the project name to the target.
With our overriding dist target declared as depending on the basefile.dist target of the imported file, our Ant build now invokes the imported target and all its dependencies:
You can’t import two build files with the same project name; the build will fail if you try this.
You can always be sure that a fully qualified target is the one you want, not one from another build file.
From Ant 1.7, every imported target can always be referred to by its fully qualified name.
In Ant 1.6, only targets that were overridden got this treatment.
The advantage of the new rule is that it allows build files to explicitly call specific targets, whether they are overridden or not.
You can use this behavior to stop targets from being accidentally overridden:
If you don’t want target dependencies in an imported project to be overridden, always use the full target name.
There’s one other aspect to the <import> process that can be useful: the properties it defines in the process.
Any imported build file can find out its original filename.
When a file is imported, a new property is created that records the name of the file.
First, you need to know that ant.file is a property that’s always set to the full path of the build file that’s executed.
This is one of those things that Ant has always done.
The output of this build file is the path of the build file twice:
This output shows that the imported file’s top-level declarations run before any targets.
This property is visible across the merged build files from the <import> declaration onwards.
We can use this in the <dirname> task to determine the base directory of an imported project.
Once you have the base directory, you can find files and directories relative to that location, instead of just relative to the outer build file.
There’s one little caveat here: for all this to work, you cannot <import> projects with the same name into another project.
Ant will halt the build if you try to do so.
You must set the name of every <project> to something different.
Given it is so radical, we need to think about the best way to use it in real projects.
How can we use <import> to make maintaining a big project easier?
There are three different ways of using <import> that regularly crop up in build files:
One use of the <import> task is to completely import an existing build file, just to stick a few more tasks on the end.
Doing so guarantees that you don’t break the existing build file and lets you add new targets to a build file that may not even be yours; it may be from another project that you just need to build.
Anyone who looked at the sample files and saw the “ignore this for now” comments has now reached the point where we explain this situation.
Yet we can still make targets in our new build file depend on those in the original, such as where our ftp-init target depends on the base init target.
Isolating some parts of the build process, here, distribution, can help with some aspects of build-file maintenance.
We can be sure that the side effects of changes to the distribution build file will be limited to that file and any that import it or call it with <ant>
What we cannot do is isolate the file from changes in the files it importsany error in the chapter 6 build or changes to its init target could break our distribution build.
The second use of the <import> task is to create a reusable template file that can be extended by all the child build files in a large project.
All our child projects will import this target, allowing it to override any of the stub declarations with its actual implementations.
Here, for example, is the file persist/ build.xml, which currently does nothing other than import the targets.
The initial property settings will read the per-project build.properties file before anything else, then set up the location of the base build file to import.
This allows developers to move the entire child project somewhere else on their hard disk, yet still refer back to the original file.
Once a project starts depending on XML files in parent or sibling directories, it’s no longer selfcontained.
This prevents projects from releasing self-contained source distributions of child projects.
The whole project tree needs to be included for a build to work.
You can use the optional attribute of <import> to say that the build should continue if the imported file is missing, but that doesn’t help, because everything contained in the file is now missing from the build.
Source distributions need to include the whole source tree, including the common build files.
The other problem is that even with standardized target names, it becomes hard to manage the dependencies of targets across the base and extended build files.
Just as a deep hierarchy of Java classes can be hard to visualize, so can a deep tree of <import> declarations and overridden targets.
One technique that appears to make this more manageable is to have targets that represent states you want the project to be in.
Once you start overriding things, however, it’s not so clear what the targets do.
Our solution to this situation is to declare what we call milestone targets.
These targets represent the state of a build, such as ready-to-compile and packaged.
Instead they just declare what targets need to run to reach the milestone.
On a simple project, ready-to-compile is just a matter of setting up the classpath for the libraries.
On a more complex project, that state may be reached only if we generate Java source from XML files:
If the base build file declares the core milestones and basic sequence of actions, build files that <import> this file can define new steps that have to be taken to reach the goal, and can still inherit all the dependencies of the original milestone:
This is why it’s so important for these milestone targets to have no tasks inside them.
If it did any work, such as a <javac> compile, it would run before the new javacc target and perhaps not work.
To use milestone targets, you have to define a set of states that are common across all projects.
The idea is that at the command line, you tell Ant what state you want the build to be in, such as.
The tool will do all the work needed to get the application into that state, be it the common steps of the base build file or custom steps of derivative build files.
There’s one final way to use <import> that takes this notion of milestone targets and uses it to mix multiple template build files into a single project.
The first two ways of using <import> are both quite similar: you take a build file and extend it, just as if you were extending a Java class.
There’s another way, one that bears more of a resemblance to C++ programming—mixin build files.
Because C++ lets you inherit from multiple classes, you could mix in many different parent classes, to make your class as an aggregation of all its parents.
Similarly, the <import> task lets you define mixin build files.
These are build files that perform a specific function, such as creating an archive or deploying a web application to a local application server.
They don’t have to be self-contained; they can import other build files, which can mix in others.
Each build file will be imported only once, even if there are multiple <import> statements doing it.
We aren’t going to show in detail how to do it or explain best practices, because nobody knows how to do this properly.
This effectively makes new tasks available to the outer build files, which can glue them together in whatever order you choose.
You can use the command to implement simple inheritance, so you now have a better way of managing projects with multiple subprojects.
Ideally, each subproject should be nothing but a simple delegation to the base component.
Some things to keep in mind when using <import> follow:
The risk of clashes and unexpected overrides increases as you do so.
Give every project a unique name attribute in the <project> declaration.
It’s easy to miss this when using an existing file as a template, and it can be very confusing.
Provide clear, structured override points if you want to offer default targets that may need to be overridden.
Be very, very, careful when maintaining a common build file that’s imported into multiple projects.
If you accidentally name a target the same as one in any of the projects that import it, your build will not work, and you’ll have a hard time figuring out why.
Just as object-oriented design has flaws, inheritance and overriding of targets brings new dangers.
Yet <import> also delivers the ability to scale projects better than Ant has been able to do so before.
It’s one of the key ways of keeping build files easy to maintain, which was listed as one of the problems of large-scale Ant projects.
It can provide reusable targets, targets you can override when needed.
The next way to keep maintenance down is to define template tasks, templates that can standardize the options of commonly used tasks—macros.
Next came the <import> task, to share and reuse targets across build files.
That’s the final challenge of scale this chapter addresses, through Ant’s macro facilities.
Anyone who wrote C or C++ code will remember the #define preprocessor instruction.
This was a tool of power—and, in the wrong hands, terror.
The macro expansion took place before the language itself was parsed, so you could almost generate a new language if you got carried away.
It lets you do two things, both of which take place after parsing.
First, you can define new tasks with different defaults from existing tasks, using <presetdef>
Secondly, you can define a macro task, which is a sequence of other tasks.
In this way, you can build a new composite task from those that exist already.
We’ll start with the <presetdef> task, which is the simpler of the two tasks.
It lets you declare a new task based on an existing task.
This new task can be used anywhere you would use a normal task and when it’s executed, the task it wraps is invoked.
Whenever we wanted to execute a program, we would declare it with <robustExec> and the failonerror attribute would be set for us.
All other attributes and nested elements would still be the same as for the original task.
We like the <presetdef> task because it lets you lay down rules for what the default arguments to tasks should be, across all projects.
It also allows us to change those defaults in one place—the build file that defines the <presetdef> tasksand have those changes propagate.
It crops up in Ant’s own build and test files a lot, too.
How do you actually use it? Well, let’s fix the <exec> task.
This task is useful, but it defaults to ignoring any failure code returned by the program.
To fix that, we define a new task, with a different failure policy:
We can use it wherever we would normally use <exec>, and know that the failure-handling policy will be consistently set to what we want:
It can even be used inside another <presetdef> declaration: here defining a version that runs code only on Windows and which also checks the return codes:
This looks a bit like subclassing in object-oriented languages, but it isn’t.
We’re just predefining values, values callers can still override and extend.
Defining new default values is invaluable when you want to lay down the rules for performing common operations, such as compiling code.
A <presetdef> declaration of a new task can declare all the project policies.
By setting the uri attribute, we’ve placed the new task into a namespace.
This allows our task to coexist with the original name.
When we use it, we have to use the task in our new namespace:
The <presetdef> task is invaluable in a large project, because it can keep options consistent across all projects.
It also makes it easy to add a new default value to all build files, wherever a target is used.
That doesn’t mean that it should be used everywhere; there are hazards to be aware of.
When a new task is declared with <presetdef>, the default attributes of a task can be redefined.
The options that you set aren’t absolute rules; they are merely hints.
This is inadequate if you want to enforce rules across your projects.
Now imagine a new version of Ant ships, with a task called <archive>
When <archive> is used, which one is going to be run? The answer: whichever got declared last.
Ant warns if you redefine things, whether the definition is the same or different.
We prefer to define new tasks in private namespaces, to make it clear that they are not Ant’s own tasks, and to isolate ourselves from Ant’s own set of tasks.
Here are some effective ways to work with the <presetdef> task:
Don’t run out and declare <presetdef> wrappers for every task.
Do use <presetdef> when you want to lay down new rules for common tasks.
Declare the tasks in a new namespace, to isolate them from the built-in tasks.
It just needs a bit of care to be used effectively.
The other macro tool, <macrodef>, is equally powerful and needs to be treated with similar caution.
It doesn’t let you create a completely new task from a sequence of existing tasks, nor does it hide any of the details of the underlying task.
If you want to do more, you need a different task: <macrodef>
This task lets you define a macro task, with optional and mandatory attributes and nested elements.
When the new task is used in a build file, Ant passes the task’s parameters to the inner sequence of tasks, and your work is performed.
What does this do? The <macrodef> begins the declaration; we then list four attributes.
Then follows a sequence of tasks, wrapped in the <sequential> container.
One of the tasks is a simple <echo> to list the parameters when you run Ant in verbose.
This task copies all the files matching the macro’s pattern from the source tree to the.
If we don’t specify a pattern, we get the default set:
This will completely replace the default value, which isn’t available.
Nested text inside can be placed into a parameter named with the <text> element.
This text can be made optional or not, and leading and trailing whitespace can be stripped with the trim attribute:
We could use this new task to add log messages to our build files, messages that will appear only in a -verbose run.
Alongside text and attributes, the <macrodef> task supports nested elements.
We can then insert all XML under this element anywhere inside our sequence of targets, just by declaring the name of the element as an element inside the sequence:
Here’s a simple demonstration that echoes out the XML passed in:
We can use this with any XML inside our <files/> element.
The result of this run is what Ant’s <echoxml> task does: it prints out the nested XML to the screen or a file:
You can declare one element parameter as implicit, meaning that it takes all the XML nested inside the macro that doesn’t match any other element:
This states that the parameter <commands/> should be bound to all the nested XML.
This macro executes the tasks supplied as parameters twice in a row, so we can use it like this:
This shows that you can use the macro elements anywhere inside the macro, not just inside an Ant task or datatype:
Think of all the fun you can have with that.
With this and the Ant-contrib tasks of chapter 10, Ant is almost a real programming language.
It doesn’t have local variables, but it does have something close to it.
Anyone who writes a complex macro will end up using properties to pass information between the tasks in the macro.
The second time the macro runs, the old properties will still be set, because Ant’s properties are immutable.
The <var> task from Ant-contrib creates a property that can be a real variable, meaning it can be changed.
We can then unset a global property before calling any task that wants to use it.
Take a macro to set a property to the full path of a file that has been “reparented” to a different directory.
We can use this task to determine the name that some files will have, after a copy operation.
To make sure that our properties are acting as variables, we run the macro twice:
When we run this macro, we get the appropriate results for each build file, showing that the macro is working:
Remember that the macro is still using global properties for its work, and that the <var> task will reset any properties’ values.
Always use obscure property names for properties used inside macros.
One convention suggested by a reviewer2 was to use tmp.
Don’t write macros when you don’t have any repetition in your build file.
Use the Ant-contrib tasks for conditional logic, property resets with <var>, and other complex operations inside macros.
One obvious problem arises in a big project: where to declare the macros? This is where the mixin build files, mentioned in section 10.5.3, come into play.
It begins by declaring the project and the namespaces of the macros and the Ant-contrib:
Any build file can import this file to get all the tasks:
This gives all build files in the project access to the predefined tasks, providing us with a single point of control over how the core tasks of Ant are used in all build files across the projects.
Ant can scale to meet the needs of big projects, if used carefully.
Once you have multiple child projects, you have another problem—avoiding duplication of work.
It lets you import a shared build file into multiple build files, providing a base set of targets that can be (optionally) overridden.
To extend an existing build file with new follow-on functionality—such as deployment or distribution targets.
This isolates the deployment and distribution from the other work.
To override a base build file with specific actions at various stages in the application’s build.
This is similar to inheritance and overriding in object-oriented languages and can bring in similar complexity.
The use of a common state model, with milestone targets, can alleviate some of the problems.
As a way of writing mixin build files that define targets or tasks that can be used from any build file, without any dependencies on the state of the application at the time that they’re invoked.
Together they strive to keep the build files you write simple and consistent.
That little problem is going to take up the whole of the next chapter.
The first step is to define the problems we have.
How to set up the build with the JAR files that are needed to compile, test, and run the application.
How to pass a JAR file created in one project into other projects built on the same machine, or even to other developers in a team.
One topic that this book has been avoiding is this: how to manage library dependen cies in Ant.
We’ve covered the basi mechanics of setting up a classpath, but not where those JAR files should come from and how to look after them.
A single product addresses these issues, but it isn’t built into Ant.
Before we introduce it, we have to look at the underlying problem: managing JAR files.
The simplest way to manage libraries is to have a directory containing all the JAR files.
This is the main way of managing libraries in Ant.
If the directory is kept under revision control, then all developers who check out the project get the JARs.
They can roll back the library versions alongside the code, so old versions of an application stay synchronized with the libraries.
However, it doesn’t work for projects with multiple child projects, or when the classpaths for different parts of the build (compile, test, deploy, embedded-use, or standalone) are all radically different.
Now imagine if a server kept all the released versions of popular JAR files.
You could have a task that took the name and version of a file and retrieved it.
It could cache these files on the local disk, so laptops could still build when they were off the network.
Finally, if this repository was writeable, team members could use it to share artifacts across projects.
Such a repository exists, as do the Ant tasks to work with them.
The repository came from the Apache Maven project, whose developers wrote a build system for fixing what they saw as defects in Ant.
One idea—the Maven repository—called for a centralized repository of open-source artifacts.
Every artifact in the repository is accompanied by metadata in an XML file—the POM file.
This file describes the artifact and even declares which versions of other libraries the JAR depends on.
The result is that the build tool pulls down both the direct and the indirect dependencies of an application.
That’s what Apache Maven does, and it’s one of the selling points of Maven against Ant.
Yet we aren’t going to advocate a switch to Maven.
They not only dictate how to lay out your source, but they have a strict notion of your development lifecycle.
If your workflow is more complex, with activities such as creating Java source from XML files, making signed JARs of the test files, or even creating two JAR files from a single project, you end up fighting Maven’s rules all the way.
We shouldn’t have to switch to a different way of building, testing, and deploying applications just to get the classpaths in Ant set up.
It manages dependencies and nothing else—and it does this job very well!
Ivy is a tool to manage libraries in Ant and to coordinate Ant builds across related projects.
In October 2006, it moved into the Apache Incubator as a way of joining the Apache organization.
Once it has finished its incubation period, it should become a fully-fledged Apache project, possibly under the http://ant.apache.org site.
The main concept behind Ivy is that Ivy files describe the dependencies of a Java library or application, something they term a module.
Every module or child project in a big application has its own ivy.xml file.
This file lists what configurations the module has, such as compile, test, standalone, or embedded.
Each configuration lists the artifacts it depends on and the artifacts it generates.
The dependencies can be used to set up the classpath for part of the project or to compile files for redistribution.
The generated artifacts can be published to a repository, including the local file system.
Other projects can declare a dependency on the published artifacts, listing the name of the project, the module, and the version required.
They can also declare the configurations of a module on which they want to depend.
A module not only exports its own artifacts, but it can also export those artifacts on which it depends.
The first action is to create the ivy.xml file for diary/core, our core diary library.
Listing 11.1 shows this file, which goes into the diary/core directory alongside the build.xml file.
It states that the library has two dependencies and five configurations.
By adding a conf attribute to the <dependency> element, we can control which configurations add the JARs and their dependencies to the classpath.
This means that our compile configuration wants the default configuration of Log4J, as does the runtime configuration.
That configuration also needs Log4J on the classpath, but we haven’t asked for it.
This tells Ivy that the test configuration wants to inherit every dependency of compile.
Similarly, the default configuration extends both master and runtime c.
The configuration will contain all artifacts in both inherited configurations.
This configuration contains nothing but the module’s own artifact d, which tells Ivy that we publish the diary-core JAR file in this configuration.
We don’t need to bother stating the name of the artifact, as it defaults to that of the module, diary-core, from the organization called org.antbook.
Note that Ivy usually spells “organisation” with an “s” [European style] in its XML files.
That’s a lot of information in a few short lines.
It’s enough for Ivy to pull down the JARs needed for compiling and testing the library, to know what artifacts to publish.
When someone else declares a dependency on the diary-core JAR from the antbook.org organization, they can get the JAR file itself, they can get everything it needs at runtime, or they can get both.
How Ivy resolves dependencies The metadata in the ivy.xml file can be used by Ivy to determine what libraries each configuration of a module needs.
It can then look across repositories to find and fetch those files.
Before it downloads the artifacts—usually JAR files—it looks for metadata on the repository about the file.
It can read ivy.xml files and most Maven POM files.
Both file types describe the configurations and dependencies of the artifacts that are being retrieved.
Ivy looks at the metadata, determines what new dependencies there are, and follows them, building up a complete graph of the dependencies.
It detects when more than one version of the same artifact is on the graph, which constitutes a conflict, and hands off the problem to a conflict manager, which is an Ivy component that decides what to do.
Conflict managers decide which version of a file to use, such as the latest or the one asked for explicitly.
After all dependencies are resolved and all conflicts are addressed, Ivy can perform a number of activities.
The key ones are generating an HTML report showing the dependencies of every configuration and downloading the needed artifacts.
The HTML report is good for developers, and it will be the first thing we’ll work on.
The second action, copying down the JAR files, is exactly what we need in order to set up the classpath.
Ivy can copy over all the dependencies of a project, and then Ant can add them to its classpath through simple <fileset> declarations.
Ivy does the dependency management, while Ant does the build.
Let’s get Ant to set up the classpath for our diary-core module.
In early 2007, Ivy was moving into Apache, and its source was moving into the Apache Subversion server.
This means that the location of the Ivy downloads is changing, and the final URL is still unknown.
Start off at http://www.jayasoft.org/ivy, from where a link to the latest version will always be found.
This chapter was written against Ivy 1.4.1, which was the last pre-Apache release.
The JAR file contains the Ivy code and an antlib.xml file which declares the tasks.
To use Ivy in an Ant build file, all we need to do is declare the XML namespace, currently:
Once this is at the top of the file, we can use any of the Ivy tasks in table 11.1
We’ll focus on the core tasks to manage our dependencies and leave the rest for the online documentation.
The first task, the one that must be called in every build, is called <ivy:configure>
Before it can download artifacts, Ivy needs to know where to find them and where to store them locally.
Every module/child project has its own ivy.xml file, but ivyconf.xml should be shared between all projects of a big application.
The full details of this file are beyond the scope of this book.
What’s important is that it defines the different resolvers for retrieving and publishing artifacts.
They are as follows: A <filesystem> resolver team, which contains shared artifacts belonging to the team.
This repository is under SCM; all developers get a copy.
This resolver is created by Ivy itself and defines the standard layout and location for downloaded artifacts.
The default resolver is a chain resolver, a sequence of the local, team, and maven2 resolvers.
Files are located by looking in the cache, the team repository, and, finally, in the Maven2 repository.
Another chain, internal, defines how to search the local repositories.
The <modules> listing adds one extra feature: it declares that artifacts belonging to.
This avoids searching the network for the files that we create ourselves.
There’s an Apache Ivy-user mailing list where you can find help.
The ivyconf.xml file configures Ivy, but it needs to be passed to Ant.
It has to be called once per build, before any other Ivy task:
This ivy-init target defines a directory for retrieved artifacts, and then calls <ivy:configure> against the shared ivyconf.xml file.
Every build file that uses Ivy must have a target like this, and all targets that use other Ivy tasks must depend on it, so it gets called at startup:
If there’s an error in the configuration, the task will print a message and the build will fail.
If everything is successful, all the other tasks are available, of which the most fundamental is <ivy:resolve>
This resolves all of the dependencies of a single module.
When the dependencies of a module are resolved, that means that Ivy has determined the complete set of dependencies for all configurations of the module.
It has managed to locate all the artifacts, locally or remotely, and any associated metadata.
Resolution, then, is the single most important action Ivy can do for a project, and something on which most of the Ivy tasks depend.
For the diary application, that means that the dependencies get analyzed and the relevant versions of the JUnit and Log4J libraries retrieved—along with any of their dependencies.
The output is a brief summary of actions: what configurations there are, how many artifacts are depended upon, and how many files were downloaded.
In this run everything was in the cache; if an artifact was not there, here’s where it would be downloaded.
Once an ivy.xml file has been resolved, tasks that act on the resolved files and metadata can be used.
One of these tasks is invaluable when setting up a build: <report>, which creates a report on all the dependencies.
When setting up a project’s dependency and configuration information in the ivy.xml file, you need to know what’s happening.
You need to know which files are in which configuration and whether there were any conflicts.
Rather than analyze the build file logs or look at retrieved artifacts, the tool to use is Ivy’s <report> task.
This task creates an HTML report of the resolved system.
There’s also the <artifactreport> task, which outputs pure XML for further processing.
For most developers, the HTML report is the most useful.
This target generates an HTML page for every configuration in the chosen output directory.
Only after the report matches your expectations should you move on to retrieving artifacts.
It copies all of the libraries of the selected configurations into a directory tree:
Figure 11.1 Ivy reports the dependencies for one of the configurations.
This copies the files from the cache into separate subdirectories, one for each configuration:
If different configurations use the same artifacts, they get their own private copy from the local cache.
When the task is run again, it won’t copy the files if they already exist:
We also can look into a directory to see what’s there, such as the test configuration:
Incidentally, the sync="true" attribute of the <retrieve> task hands over complete control of the directory to Ivy.
Ivy will delete from the destination directories any files that aren’t currently part of the configuration’s dependencies.
This action lets us avoid having to clean up the directories when changing the dependencies of configurations; Ivy does it for us.
Once the artifacts are retrieved into directories, Ant can use the files.
To compile using the retrieved libraries, we can use Ant’s <path> datatype declaration to set up classpaths from the configuration directories:
To test that everything is set up, we run ant clean test to run all unit tests against.
As they pass, we know that the build is working, which means that the classpaths are set up right for the compile and test targets.
That’s it! We’ve used Ivy to set up the classpaths for the different activities in our application: compiling and testing.
Ivy resolves the dependencies for each configuration, including inheriting the dependencies of other configurations, downloads the files from repositories, adds the dependencies of those files to the configurations, and then creates status reports or copies the artifacts over for Ant to use.
We can use it to glue together projects, feeding the artifacts of one project into another.
Ivy can pull down artifacts from the local cache, a team repository, or a remote server; it can be used to pass metadata and artifacts from one Ant project to another, each of which is, in Ivy terminology, a separate module with its own ivy.xml file.
Each project’s build needs to publish the JAR files and other artifacts it creates into a shared repository.
The ivy.xml files of the other projects declare a dependency on the created artifacts, and Ivy will locate the files during resolution.
The <publish> task will copy the artifacts of a project to the repository identified by a named resolver, here the “local” resolver:
The task requires a version number for the files, which is set with the pubrevision attribute.
In our builds, we’ve been numbering all artifacts, but Ivy doesn’t require this.
You can create artifacts with simple names and have their revision number tacked on when the file is published.
This is useful when retrofitting Ivy to an existing project.
Ivy’s <ivy:buildnumber> task can even determine a build number from a repository, by determining the version number of the latest artifact in the repository and setting an Ant property to that value plus one.
With our build files already using version numbers, we avoid that step.
We do have to tell Ivy how to locate the files, which is where the artifactspattern attribute comes in.
It contains a directory path and a pattern for finding artifacts.
What we don’t do is list the actual files that are created.
This seems surprising, but it’s because the ivy.xml file declared the artifact already in its <publications> element:
This element declares that an artifact is published in the “master” configuration only.
The artifact’s name defaults to that of the module, and it also has the default extension/type of JAR.
In chapter 14, for example, we’ll create the EAR file diary.ear:
Again, this is invaluable when adding Ivy to existing code.
There’s no need to change the name of existing artifacts; you only need to add extra metadata and extend the build process with the <ivy:publish> task, a task which must be run as part of the local installation activities:
This trace of the publish process shows the actions Ivy took:
Ivy placed a copy of the ivy.xml file into the same directory in which the artifacts were created.
All Ivy variables in this file are expanded, so all version numbers and other late-binding values are expanded.
Sometimes Ivy doesn’t seem to update the metadata before publishing the file.
The <ivy:publish> operation placed the file in a user-specific repository.
There’s nothing to stop the task from publishing the file to a team repository, such as one on a shared file store.
This would work well if it takes a long time to build some of the project’s artifacts, and there’s no need for every developer to rebuild every part.
The team repository would share the files for everyone to use.
Published artifacts can be pulled into other projects simply by listing them in the <dependencies> section of the other project’s ivy.xml files.
This is a way of asking for the latest version published on the repository.
Ivy will look at the repositories and find the latest version.
As the ivyconf.xml file declares that org.antbook artifacts come from the local and team repositories only, Ivy doesn’t poll any remote server.
As well as asking for the latest version, we declare that the artifact and its metadata are changing c.
Normally Ivy doesn’t update metadata files once they’re in the cache; instead it saves the complete dependency graph of every configuration to its own XML file nearby.
For static dependencies, caching all the dependency information makes resolution much faster.
For changing files and metadata, that cached data can stop changes being picked up.
We need the most current copy from the repository, along with its metadata.
Unless we used <ivy:buildnumber> or the current timestamp to create a new build number on every build—which is an option—we need to tell Ivy to poll for metadata and artifact changes on every build.
Declaring the configuration to depend on The final detail is that in every configuration, we ask for the “master” configuration of the diary-core module only d.
When Ivy publishes artifacts to the repository, it adds the module’s dependency data.
Their goal is to simplify your life by giving you all the files you need.
There’s a price: sometimes you get more than you want.
As we don’t need Log4J to compile the web application and we don’t want it at runtime, we must ask for a configuration that contains the diarycore artifact but not its dependencies.
We normally start off with the Maven team’s configurations to be consistent with artifacts coming from the Maven repository.
The master configuration has the artifacts with no dependencies, runtime has the dependencies with no artifact, and default has both.
We use these standard configurations to make it possible to publish artifacts to the central repository, something that’s beyond the scope of this book.
We can still add private configurations for internal use (such as setting up a <taskdef> classpath) or to create new public configurations with different dependencies, such as embedded, redist, or webapp.
Having declared the dependencies on the master configuration only, we can run the new build.
Resolving the new project The complete ivy.xml file for the diary web application is much more complex than the diary-core module.
Running its ivy-resolve target creates a long list of dependencies:
This is the artifact published earlier with the <ivy:publish> task; it can now go on the classpath of the web application.
The older dependencies came from HttpUnit, which is something we’ll be using for testing the web application.
It was built against older versions of the two libraries, versions picked up by the Ivy resolver.
As newer versions were in the web application’s ivy.xml, the older versions were evicted and the newer versions retained on the dependency graph.
Eviction information is included in the HTML pages generated by <ivy:report>, so team members can see what versions are being used.
It highlights that the diary-core artifact was found on the local repository, and checked for changes (“searched”), while two artifacts were evicted.
Now that we are feeding the output of one project into the next, we can chain builds together just by running them in the right order.
This <delegate> presetdef can delegate targets down to the child components, building and perhaps publishing the core component before the web application.
Ordering the builds by hand works if the dependencies are simple and known by the author of the master build file, but doing so doesn’t scale.
Once you have more than a few projects, projects with different team members maintaining the build .xml and ivy.xml files, soon you have dependencies where you don’t expect.
If the list of build files to delegate to isn’t kept synchronized, projects start building with outdated artifacts and developers start wondering why changes don’t propagate, or worse: you ship old code.
If ordering builds by hand is unreliable, the solution is to have the machine work out the correct sequence.
If something were to look through all these files, it could see what modules were being built and which ones depended on others.
It could then sort the list of modules in the order needed to ensure that all a module’s predecessors were built before the module itself was built.
The <buildlist> task does this, creating a filelist of all the build.xml files ordered so that projects are built in the right sequence:
Figure 11.2 In this configuration, two obsolete modules are evicted.
We can use this task to create a <presetdef> task to delegate work to the build files:
The path can also be printed out, to show the execution order:
Running this target shows the build order of the projects:
This run shows that Ivy can not only manage the problem of dependencies between projects, it also can use that same information to order builds.
There are a couple of aspects of the task to be aware of: looping and indirection.
Loops in project dependencies If there’s a loop—that is, if two or more modules create a cycle in the dependencythe order of those looped projects is undefined.
Modules before the loop will be built in the right order, and modules after the loop will be ordered, but there’s no way to predict the order Ivy will choose for those in the loop.
This means that it’s acceptable to have loops in a project, but Ivy will not order the dependencies correctly.
Loops are not unusual in Java library projects, especially if one configuration of a project depends upon the output of another, which itself depends on the first.
Indirect dependencies Ivy doesn’t order builds correctly if two projects have an indirect dependency via a module that isn’t in the <ivy:buildlist> fileset.
If the diary-core-persist project was excluded from the <ivy:buildlist> fileset, the task wouldn’t care that persist-webapp depended indirectly on diary-core, and not build the components in the right order.
To create a correct ordering of the build files, the fileset inside the task must contain all related projects.
This doesn’t mean that you have to build them all, because the root attribute can restrict the list to only those build files that matter for a chosen model:
This task passes in all build files with matching ivy.xml files, but only selects those projects that the webapp module depends on.
The task can be used to build subsets of a system, using Ivy to manage the selection and ordering of the subset.
The <ivy:buildlist> task is the core Ivy task for complex projects.
There’s a reporting task <ivy:repreport>, which can create a report about all artifacts in a repository.
If a project publishes all generated artifacts to a private repository, this reporting task will list everything that gets produced and show the dependencies between artifacts.
There are some other details about Ivy worth knowing about, which we’ll take a quick look at.
There’s a lot more about Ivy that’s covered in its documentation, and there’s no substitute for experience.
Here are some techniques that are worth pulling out because they’re so important.
Ivy variables make configuration easier, especially as ivyconf.xml files can import other ivyconf.xml files, which may define properties as well as other parts of the system configuration.
Where Ivy variables are invaluable is in setting properties inside an Ivy file from Ant, because all Ant properties are turned into Ivy variables.
Whenever an Ivy task is invoked, it has access to all of Ant’s properties.
This behavior lets us control library versions through Ant properties.
The build file can load a properties file of all library versions, and Ivy files resolved later will pick up these values.
At the beginning of the build, we load in three files—one an optional file for user-specific customizations, one a child project-specific set of libraries, and the final one a declaration of versions for all projects/modules in the diary application:
During resolution, Ivy expands the properties, so the resolved Ivy files and generated reports show exactly which version was used.
It’s essential to manage dependency versions this way, to provide a single place to upgrade all versions of a library, across all child projects.
Doing so also lets developers experiment with new versions, such as a later release of JUnit:
As it’s now trivial to change from one version of an artifact to another, the hard problem is finding out what the name of an artifact is, and what versions of it are available.
There are web sites and search tools to help with this.
The best way to locate artifacts on the central Maven repository is to use one of the search engines, such as mvnrepository, http://mvnrepository.com/, and Maven Repo Search at http://maven.ozacc.com/
These repository search engines will take a name of an artifact and show the full organization and artifact name, as well as a list of public versions.
You also can point a web browser at http://ibiblio.org/maven2 and browse around.
When new artifacts do come out, the metadata with them can be a bit unstable; it can take time before the dependency information is correct.
If one machine is building projects with different artifacts from the others, delete the machine’s Ivy cache and rebuild everything; this will pull down the latest versions.
Neither version is needed, as Java 1.5 ships with Xerces built in.
Rather than switch to HttpUnit’s master configuration and explicitly ask for all its other dependencies, we can drop the XML parsers by excluding artifacts coming from xerces.
An <ivy:report> run after this change shows that the files have been dropped.
Artifacts to exclude can be specified by organization, module name, artifact name, or artifact extension, all of which can be given a regular expression containing wildcards for easier matching.
The more dependencies a project has, the more likely it is that unwanted files will end up on its classpath.
Being able to selectively exclude artifacts addresses the problem without requiring a private repository with private metadata files, which is the other solution.
There’s the cost and bandwidth of connecting to the servers and the risk that they can be unavailable.
It can contain all approved artifacts, with dependency metadata managed by the team, instead of the team having to rely on stability and availability of an external repository for successful builds.
The private repository promises better security, as only audited artifacts could be placed on the repository, and it provides a place where private artifacts can be stored, including Sun artifacts that cannot be served on the public servers.
Most big Ivy projects end up creating a team repository because of the control it offers.
One task, <ivy:install>, simplifies the process of setting up the repository, because it can copy artifacts and metadata from one repository (usually the remote one) to another repository.
It can populate the repository with the base files and metadata, metadata that can then be hand-tuned to meet a team’s needs.
How hard is it to do so? It takes a few days of careful work.
Here’s a workflow for the migration that can be used as a starting point:
Start after a release or in a period of calm and relative code stability.
Nobody should be editing the build files or changing dependencies.
Create a single ivyconf.xml file by copying a working example.
Define a common set of configurations for all projects, such as those of table 11.2
A common set of configurations can be shared across ivy.xml files with the <include> element:
Including configurations from a shared file keeps maintenance effort down and makes it easy to add new configurations to all applications.
To summarize: migration to Ivy is manageable, but you should practice on something small first.
At the beginning of this chapter, we listed four problems we wanted to address.
How to set up the build with the JAR files that it needs.
How to pass a JAR file created in one project into other projects.
How to be able to switch library versions on demand.
Together, Ant and Ivy can do all of these things.
If projects list their dependencies in an ivy.xml file, Ivy can retrieve them.
Different configurations can be set up for the different paths and filesets a project needs.
The <buildlist> task determines the correct order in which to build subsidiary projects.
If library versions are defined in a central properties file, a developer can switch library versions just by setting the appropriate version property to a different value.
They are all available online, and we’ll mention some aspects of Ivy configuration when it encounters a new feature or problem.
Now that we can set up libraries and dependencies between modules, it’s time to use the techniques by writing a web application.
This web application will use Ivy to pull in external dependencies and to add the diary-core JAR to the web application itself.
This chapter looks at what you have to do to build and test one.
Web applications are more complex to package than the simple JAR files we’ve done so far, and before they can be tested, they must be deployed.
This makes the build process much harder than what we’ve done so far.
Ant will have a more complex packaging problem to deal with, and it will have to deploy our code onto a web site before testing it.
Solving problems like these are where Ant shows what it can really do!
What is a web application? We now have a library to create entries in a diary, a build process that can handle mul tiple projects, and a way of sharing artifacts between them.
What next? How abou building a web site that uses the diary library and can publish events as an Atom feed?
A Java web application is a web site packaged into a JAR file.
The JAR file contains libraries, code, and resources—the pages, images, and other content that together form the web site.
The resources provide the static parts of the web site, while the Java classes, libraries, and JSP/JSPX content form the dynamic part of the application.
Web applications are not standalone; a servlet container hosts them.
This container, be it an embeddable servlet engine such as Jetty or a full-blown J2EE application server like JBoss, needs to know how to execute the web application.
Everything in it, other than content under the WEB-INF and META-INF directories, is served up by the web server.
The WEB-INF directory should contain a web.xml file describing the application to the server, while Java code goes under WEB-INF/classes and libraries into WEB-INF/lib.
Table 12.1 lists some of the content commonly added to WAR files.
Building and deploying WAR files is the core of a Web application’s build process.
Packaging the file is primarily a matter of getting all the stuff into the appropriate place, and getting the web.xml file right.
Figure 12.1 shows how to construct a web application, by hand or by Ant.
Ant needs to compile the Java code with <javac>, then package the generated .class files into a WAR file.
This WAR file must include any web content, any needed JAR libraries, and usually a web.xml manifest file.
After the WAR file is built, it can be deployed to a server.
This is done with functional testing, by making requests of the site.
You can do that by hand, and that is certainly the best way to see that things look right.
You can also delegate much of the testing to the computer, to save time and also to compile all the JSP pages.
This is a complex process—which is precisely why we need Ant to automate it.
Static content Images, HTML pages, and similar files; content that never changes while the server is running.
Saved in the WAR file outside the WEB-INF and META-INF directories.
Servlets Java classes that serve up parts of the site, such as a directory tree or all files with a specific extension.
Designing the web application The first step is to design the application.
It will do one thing: publish the diary’s events as an Atom feed so that subscribers can track up-and-coming events.
This library can generate various XML feed streams that we can publish with a servlet we write ourselves.
The library depends on JDOM, which is one of the many Java XML APIs.
Before we begin coding, we start with the housekeeping: setting up the source and destination directories.
The web application goes into its own directory, diary/webapp/, alongside the core/ and xml/ directories.
The master build file from chapter 10 will build the compound application.
Figure 12.1 The basic workflow for constructing a web application.
Creating the build file The build file for this project takes full advantage of the Ant concepts of the previous two chapters.
Next comes the declaration of the web-specific directories and files:
With these files imported, our common entry points and milestone targets, such as compiled, dist, and test, are defined.
We just need to add any new code, set up the compile classpath, and build our application as normal.
We want our web application to serve up the diary’s events as an Atom XML feed.
We can do this with the Rome syndication library, which can publish data in all the popular syndication/blog formats.
Listing 12.1 shows a servlet we’ve written, which  provides an Atom feed, the content generated by an EventSyndicator class.
This is a class that creates an Atom entry for every event in the calendar.
When deployed in the web application, the servlet will let anyone listen for forthcoming events.
Listing 12.1 A servlet to publish events as an Atom feed.
Web applications can include libraries inside the WAR file, in the WEB-INF/lib directory.
For a self-contained application, this directory should include every JAR the application needs beyond those provided by the container.
Web servers usually offer the servlet API, the JSP engine, and perhaps the whole of the Java EE API, so those must stay out of the WAR.
However, they’re needed at compile time if the application imports their classes.
The solution here is to create a new Ivy configuration, war, containing only those libraries we want in the WAR file.
The EventSyndicator takes events from the diary-core collection and can syndicate them.
We have to set up the <dependencies> list in the ivy.xml file for the servlet API in the compile configuration, and Rome and the locally created diary-core library in the compile, runtime, and war configurations:
As JDOM is a declared dependency of Rome, there’s no need to ask for it.
Running the ivy-resolve target of chapter 11 will copy the dependencies of each configuration into separate directories, one of which is used to set up the compile classpath.
The new war configuration is a private configuration that lists all libraries Ant will add to the WAR file.
Redistributing the libraries The WAR file must include all the libraries that aren’t provided by the application server.
An <ivy:retrieve> operation can place them into a directory that’s ready for adding to the WAR file.
This time we don’t want the version numbers on the files, so use a different pattern when retrieving them, asking only for the artifact and extension:
This target copies three files into the build/warlib directory: rome.jar, diary-core.jar, and jdom.jar.
They go under the root directory of the WAR file, and, in this project, in the web/ subdirectory of the SCM source tree.
These are HTML or XML pages mixed with JSP directives and Java code.
The latter is an XML version of JSP pages and has the extension .jspx.
Most aspects of the original JSP syntax have equivalents in the JSPX files.
We use the term “JSP File” to refer to both JSP pages and JSP documents.
All JSP files are processed by the “JSP engine,” usually the Jasper runtime from Apache Tomcat.
The first time the servlet fields a request for a JSP file, it creates a Java file from the JSP source, compiles it into a .class file, and finally runs it.
After that first time, processing a JSP file is as fast as other Java code, but that first fetch can be slow.
It will also be the place where errors show up.
Although Ant has a <jspc> task to compile web pages, and applications servers often have similar tasks, the only mechanism that has proven reliable is just to pull down the pages from the deployed application.
The ideal way to code in a JSP file is via new HTML/XML elements, JSP tag libraries.
The worst way to do it, from a maintenance perspective, is to embed the Java source straight into the JSP file.
But that’s so very easy, which is why the happy.jsp page in listing 12.2 does it.
If the named class cannot be loaded, the method throws a RuntimeException, which is caught and turned into an HTML error page by the servlet engine.
By locating classes from different libraries, this page verifies that the JARs the application needs are present.
If a needed class is missing, the server will return an error instead of the page.
We will use this page, the “happy page,” when we deploy the application.
If the happy page can be fetched, then the application is not only deployed, it considers itself to be healthy.
This JSP page goes into the web directory, along with any static content such as graphics files.
When building the WAR file, everything under that directory must be copied into the WAR file, starting at the root.
Before creating that WAR file, there’s one more thing to do: create the web.xml file.
The final part of the WAR File is an XML file to describe the web application, WEBINF/web.xml.
Although it used to be mandatory, since the servlet 2.5 API, it’s been optional.
Add JavaDoc annotations to the source and use a tool called XDoclet to create a web.xml file from introspecting the .class files and parsing the source.
The XDoclet tool can create XML files from JavaDoc-annotated source.
Prior to Java 1.5, it was the only way to generate the XML files that Web and Enterprise Java applications needed.
With annotations added to the Java language, there’s now a new way to mark up code.
Different annotations are being standardized to assist in integrating a web or Java EE application with its hosting application server.
Until then, XDoclet is something to consider if you have many servlets or if you’re writing JSP tag libraries.
Consult the book XDoclet in Action (Manning Publications) and the XDoclet web sites (http://xdoclet.codehaus.org/ and http://xdoclet.sourceforge .net/) for details.
For a simple web.xml file with a few servlets and mime types, XDoclet is overkill.
With the file written, the application is ready to be turned into a WAR file.
A WAR file is just a JAR file, so you can build it with the <jar> task.
An extension of this task, <war>, makes the process slightly easier.
It has special handling for the content that goes under WEB-INF, including classes, libraries, and the web.xml file.
There’s an attribute, webxml, that takes the name of a file to turn into the WEBINF/web.xml configuration file.
The attribute and corresponding file are mandatory, unless the needxmlfile attribute is set to false.
To create a WAR file, then, we can call the <war> task, filling in the different parts of the archive with the various data sources:
The task contains three filesets: the web pages, the classes, and the libraries.
Creating the WAR To create the WAR file, we run the dist target:
We have a WAR file containing static content and JSP pages.
Behind that we have a servlet, other classes, and bundled JARs.
The one thing we don’t yet have is a working application.
How do we know that? Because we haven’t tested it yet! We can test a web application only after it’s deployed.
Before you can test the application, you need to deploy it.
The good news is the WAR file is ready to be deployed.
It’s the application that Ant was written for, and it lets you deploy just by copying the WAR file into the right place.
Download a current version of the “core” package in a form suitable for your platform; it comes as .zip, .tar.gz, and as a windows .exe installer.
On Windows, the .exe installation installs Tomcat as a service, and it doesn’t provide batch files to start or stop the program (you get a native Windows .exe)
For development you need a version you put in a place of your choice, with the .bat files that the IDE needs for debugging.
You can start Tomcat by running startup.sh or startup.bat, as appropriate.
You can deploy onto Tomcat by copying the entire WAR file to the web application directory of the server.
If Tomcat isn’t running, the WAR will be loaded when Tomcat starts up.
If it’s running, the changes will be picked up within a few seconds.
Listing 12.4 shows a build file that deploys this way.
It extends the main build file for this chapter by importing it b, then declaring a new deploy target that adds a dependency on the deploy-by-copy target.
The target, in turn, copies the WAR file into the application server’s directory, possibly renaming it in the process c.
Because the deployment options may vary with every user and application server, all the configuration options are stored in a directory deploy/, with different property files for different computers.
Listing 12.4 The simplest way to deploy to Tomcat is by copying the WAR file to the web application directory of the sever.
If we delete a WAR file from the Tomcat or JBoss deployment directories, the application server  undeploys it by shutting down the web application.
Deleting the WAR file is an easy target to write:
You don’t actually need to undeploy an existing application before deploying an update; deploying the new WAR should suffice.
What can we do with it? It takes a lot of work to get a web application off the ground, so go look at its start page to see that, yes, it really is there.
Now let’s return to Ant and the tools it offers.
Because it can take some seconds for the WAR file to deploy, Ant should delay running tests until the application is up.
This is done by running both actions one after the other.
These conditions can be used to block the tests until the application is running.
This task evaluates a test repeatedly until it succeeds or the time limit is reached, sleeping between each test.
You can specify the maximum wait and sleep times in units ranging from milliseconds to weeks.
It will support a nested condition, which it will test repeatedly.
Here is a test that probes the local machine for a program listening on port 8080:
If not, the property named in timeoutproperty is set to true; Ant can <fail> on this.
The maximum wait time often needs tuning for the particular use.
We need to wait a few seconds, before starting the <waitfor> loop.
The task will halt the thread executing the task/build file for the time set by the four attributes hours, minutes, seconds, and milliseconds:
The alternative tactic is to always call the undeploy target before deploy and have another target that waits for the application to be completely undeployed:
If our deploy target depends on this new target, it will wait until the application server has stopped the running application before we upgrade it.
Either way, if the deploy target succeeds, the application server has deployed, so it’s ready for testing.
A web site is like any other piece of code: you don’t know it works unless you test it.
For web applications, we have to do system tests, tests that show how the entire system works.
Unit tests are good for qualifying the parts used to build the system and to.
They can find problems earlier, identify the area of trouble, and.
System tests can fetch web pages, simulate a remote user, and show that the application works on our chosen server.
JUnit can test the web application with the aid of an extension called HttpUnit, from http://www.httpunit.org/
HttpUnit allows test cases to interact with a web server as if it were a web browser.
It can start a session with a server, fetch pages, fill in forms, and navigate around the site.
Along the way, it can validate web pages, looking at elements in the page such as the title, forms, and text.
You can write code to follow links, letting you validate further pages off your starting page.
If you really know what you’re doing, it will give you the actual XML Document Object Model (DOM) of the HTTP response.
Ant integrates testing with HttpUnit into the build process, by building and deploying the web application, running the JUnit/HttpUnit tests, and presenting the results.
The effect is to make system testing of the web application no harder than unit testing the diary-core library.
To use HttpUnit, the first step is to download the latest version from http://httpunit.org and unzip it somewhere.
It contains the documentation and the two files you need to run the tests httpunit.jar and jtidy.jar.
They use the HttpUnit classes to talk to a web server, fetching and testing HTML pages on the remote sitethen validating the results against expectations.
As we’re likely to write more than one test case, we start with an abstract base test class, which we name WebTestBase.
It will contain the set-up logic to bind to a URL supplied to JUnit:
The class extends the JUnit TestCase class; the HttpUnit classes don’t replace any existing aspects of the JUnit framework.
Indeed, the classes work perfectly well outside the JUnit framework.
Our WebTestBase class adds two fields to work with HttpUnit:
Finally, we add some utility methods to use throughout our tests:
The key method is getPage(), which fetches a page under the web application.
If the JSP page doesn’t compile or if the page is unhappy, the server returns an HTTP 500 error code, which HttpUnit converts to an exception.
The second test fetches the root page of the application.
The test method fetches this page and then follows the links.
By giving each link an id attribute, the test can ask for the links by ID:
The test checks the title of the page b and then asks for the link with a given ID c.
If the link is missing this will return null, which an assertion picks up d.
Following the link e causes HttpUnit to fetch the referenced page, or fail if it cannot be retrieved.
By following links this way, JUnit can validate links on a page.
We could have looked up links by the text inside them, using a call such as home.getLinkWith ("happy test")
Doing so would enable us to avoid needing to add ID tags to links, but could break the test whenever the web page text changes.
Another test verifies that the unhappy.jsp page is working correctly, that it returns an error code when fetched:
Together these tests can verify that our web application deployed and that our index page is as we intended it to be.
Because happy.jsp checks the health of the application, the tests effectively verify that the application considers itself healthy.
This configuration doesn’t depend on the web application because there’s no code shared between them.
The targets in listing 12.5 use the classpath Ivy creates from this configuration to compile and package the tests.
Listing 12.5 Targets to compile the tests into a JAR file.
The compile target b should be familiar; after compiling the tests, Ant creates a test JAR file c which is then added to the classpath for running the tests d.
We can now run those tests and create the reports.
Running the tests is almost like running normal JUnit tests, with two differences.
The application must be deployed before the tests, so the test target depends on the deployed target b.
The other critical detail is that the URL to the server must be passed down to the unit tests, through a <sysproperty> element c:
Apart from these details, the fact that the target is now testing the deployed code on a web server is almost invisible to the build process.
HttpUnit can now check our servlet and our JSP pages every time we rebuild, and so make sure that the application keeps working.
That’s it! We have a complete build process for our web application, including deployment and testing.
The application may not be ready to go live, but with a deploy and test process in place, we can start working on it.
This chapter has covered web applications, and how to build, deploy and test them.
The task is really just <jar> with some extra knowledge about how to lay out a WAR file.
Web and application servers that support deploy-by-copy deployment, specifically Tomcat and JBoss, make deployment trivial.
To deploy the WAR file, copy it to the right directory.
Ant’s <get> task can fetch web pages and halt the build if an error is returned by the server.
The <waitfor> task can pause the build until a condition is met.
The <sleep> task can also insert delays, which is sometimes useful.
HttpUnit is the foundation for functional testing of web sites.
There’s an incredible amount of foundation work to get any web application off the ground.
There’s the coding and the whole WAR-file layout and creation process and the need to fetch JSP pages to force-compile them.
The way to stay on top of this is to automate as much as you can, having Ant build, package, deploy, and test the application, which is what we’ve covered in this chapter.
What next? Well, we could go straight into making the application persistent, creating an Enterprise application.
By doing so we can use Ant to validate the XML output of our web application and make more uses of XML in our build process.
We’ll see how to use Ant to validate the Atom feed of events coming off our diary web application, and to generate Java source files from XML templates.
We’ll also look at the basic ways that Ant can work with XML.
Validate an XML document In this chapter we’re going to take a partial break from the web site and do some low level XML work.
Many Java projects need some XML processing at build time.
It ma be the need to create configuration files on the fly, or it could be the desire to generat Java source files from XML.
Transform XML content into other XML documents, HTML files, or plain text.
This will give a taste of Ant’s abilities and show its limits too.
It will demonstrate where a bit of XML can benefit a Java application at build time, and how Ant can help.
Before beginning on XML processing, here’s a quick recap of the current state of XML parsing in Java.
This can explain why XML parsing and processing don’t always work.
Java supports multiple XML parsers, Apache Xerces being the one that Java 5 and Ant ship with, but various vendors provide others.
Java IDEs may run Ant under a different XML parser, which can cause problems; a target with an inline <diagnostics/> task can be used to list the in-IDE state and so identify the parser.
Apache Xalan is the standard XSLT engine used by Ant tasks; other implementations of TRaX may work, but they aren’t so widely used.
Java currently ships with a version of Xalan called XSLTC; this compiles XSL into Java.
It’s more memory-hungry than classic Xalan, which we believe works best with Ant.
If a build runs out of memory when doing XSL work, increase Ant’s memory in the ANT_OPTS environment variable or add Xalan to Ant’s classpath.
With that little detail out of the way, let’s start the XML work by creating an XML file.
The easiest way to create XML files in Ant is by using the <echoxml> task.
You just declare the name of the file and embed the XML content inside the task:
It’s easy to use and, because the XML is inline in the XML build file, guarantees that the generated file is well-formed.
The document is encoded in UTF-8, and any characters that need to escape are correctly escaped.
Unfortunately, the task is very limited; it was built from existing Ant code, and it has some problems.
You cannot insert other declarations, such as the DOCTYPE declaration used to bind a file to a DTD.
The order in which text elements and other nested child elements are included can vary.
This is a mess and, without the namespace information, an incorrect mess.
For such complex XML, we’re better off using the <echo> task.
That task will print anything to a file, including XML data.
No XML elements will be processed by the parser, but Ant properties will still be expanded inline:
You must not insert any spaces into the document ahead of the <?xml?> declaration.
The biggest problem is that it’s easy to create invalid documents, but that’s something the Ant build file can check for us.
That means that the XML and elements in the document are correct according to the XML specification.
There’s an even stricter state of an XML document, valid XML, in which the XML has been validated according to some specification.
Programmers can use this information to validate XML documents they work with, and Ant can use it to check XML documents it is handed.
It can let you check that XML configuration files are valid before deployment or that test data stored in XML files is.
In our project, we will do this, validating an XML representation of forthcoming events in the calendar.
Table 13.1 lists the main ways of describing the validity of an XML document.
It’s a set of rules that can be used to test a document for being valid; rules can be used instead of a schema language or alongside one.
Because of the way Schematron works, it doesn’t have a direct validation task.
To check that the XML files are well formed, use the built-in task <xmlvalidate>
By asking for lenient processing, we ask the task to not validate the XML against any schema or DTD:
The output is confirmation that every file we have so far created is well-formed XML:
This test is important when using <echo> to create the files, as it’s easy to get something wrong.
If you do have a schema, a specification of what XML documents you allow, then Ant can go one step farther and validate the document.
Ant supports two specification mechanisms out of the box, DTDs and XML Schema, with separate tasks to check each one.
The classic language to describe a valid XML document is a Data Type Definition (DTD) file.
We can use it to validate XML documents created with <echo>
We first need to declare the document type at the beginning of the file.
Files that have a DTD must declare this in their <!DOCTYPE> declaration b, which states the name of the root element and whether it’s a PUBLIC or a SYSTEM DTD.
Public DTDs need a public ID to uniquely identify the document.
For system DTDs all that’s needed is a relative or absolute URL.
In our example, we declare a public ID and a system reference, but the latter is invalid: there’s no file of that name in our local directory.
When we run this cdata-dtd target, we get an XML file with a reference to our simple DTD:
The <dtd> element lets you declare a public ID and a URL or file to the DTD.
It will then resolve DTD IDs and other references/URIs in the file against a nested list of declarations and against any loaded XML catalog files.
This makes it easier to manage large sets of DTDs.
First, what happens when this element is missing? Here <xmlvalidate> tries to validate the document by resolving the system reference and retrieving the file:
The task doesn’t give any reason for the failure, though in -verbose mode, the long stack trace of the underlying cause explains the problem:
First, <xmlvalidate> really is trying to validate the document against the DTD, and, second, if the DTD isn’t found, validation fails.
We need to tell the parser where to find the DTD:
By declaring a mapping from the publicId of the schema to the location where a local copy of the schema lives, the task can now verify that the document is valid.
Historically, DTD validation was all you needed to check all deployment descriptors and other documents before actually deploying programs to application servers.
More recently, XML Schema has become the language for describing things such as.
You can describe nearly anything in it, but it’s nearly impossible to read.
For example, an invalid facet error means that you have defined a pattern for a valid element or attribute (such as the regular expression of a telephone number), and the number in the document doesn’t match the pattern.
The other interesting error is a complaint about ambiguous particles.
This means that there are two ways to parse a document, and the parser doesn’t know what to do.
Unfortunately, it’s out there, it goes hand in hand with Web Services, and Java developers eventually encounter it.
No-namespace validation with XML schema To create an XML schema for our sample XML files, we used the ubiquitous XML Spy tool to reverse-engineer a schema.
Listing 13.1 An XML schema for a file with no namespace.
This can validate any XML document that doesn’t declare itself to be in any namespace.
When we run this target, if it succeeds nothing is printed at all.
Validating documents with namespaces To validate XML in a namespace we need to create a new XML schema by copying the one from listing 13.1 into a new file, namespace.xsd, and binding it to a namespace.
Listing 13.2 shows the file, with the extra line marked in bold.
Listing 13.2 A modified XSD file to declare the schema for a single namespace.
Instead of declaring the schema to use in the noNamespaceFile attribute, we’ve declared the namespace and file as a nested <schema> element b.
This shows where XML Schema can work with namespaces: we can mix elements and attributes from multiple namespaces in the same document, validating it by declaring the namespace and schema file for every namespace.
A good way to do this is to use <presetdef> to define the template for validation, which lists all schemas to use, and use this definition wherever files need to be validated.
Listing 13.3 shows an example of this complex build process that validates a SOAP API defined in WSDL and XSD files.
Issues with XSD not withstanding, it’s a powerful language, being namespace-aware and capable of validating documents without any <!DOCTYPE> declaration.
Validating deployment descriptors and other XML manifests can be a chore that Ant can automate with it.
That leaves one remaining problem: those documents described by the RelaxNG schema language, which is exactly the problem we need to address for the diary web application.
The diary web application generates a RelaxNG document in need of validation.
The Atom event feed servlet, the one that provides events to listening applications, generates an Atom 1.0 XML feed.
If we could retrieve that feed and validate it against the specification, then we would know if the third-party syndication library we’re using, Rome, is generating valid Atom data.
Ant has no built-in support for RelaxNG validation; it’s handled by the Jing.
This library contains a task called <jing> to validate XML files against RelaxNG schemas.
Declaring the Jing task Because the needed JAR, jing.jar, is in the ibiblio artifact repository, Ivy can retrieve the file.
This can verify the XML data generated by the web application once that data has been captured to a local file.
Fetching the XML feed Fetching the XML feed is no harder than fetching any other web page with the <get> task.
The only change is that the saved page is handed on to the validation target:
That leaves two more actions: getting a local copy of the relevant schema file and validating feed.xml against it.
The IETF Atom specification contains the RelaxNG specification as an appendix.
RelaxNG has two syntaxes, a pure-XML one and a compact syntax that’s easier for people to read.
Why put it there? Because it turns out to be very handy to have all schemas and DTDs in the classpath when running unit tests since it lets developers validate XML data within the Java tests.
Putting the schemas in the directory makes this possible, though we’ll have to make sure **/*.rng is copied into the test JAR.
With the feed downloaded and the .rng specification in a local file, we can now check our diary’s XML feed against the Atom standard:
This target uses the <jing> task, pointing it at the atom.rng file and stating that this is in the compact syntax and not XML.
The nested fileset is then set to the single file we wish to validate.
The target has succeeded, so we can be confident that the feed is a valid Atom 1.0 data.
Any hand-written or machine-generated XML can be validated against its schema definitions.
Ant has the limited ability to read XML files into properties, which can be used to extract information from XML documents.
If you have XML data files that contain values needed in your build process, the <xmlproperty> task may be able to help.
It can create properties from the elements and attributes in the XML document, setting the property values to the values of the XML nodes.
It has some limitations though: it doesn’t perform local DTD resolution, and it merges elements if there are duplicate names.
Running this target shows what Ant can extract from the XML document:
The two duplicate <element> declarations have been merged with a comma between them.
In older versions of Ant, only the first declaration would have been retained.
The task isn’t namespace-aware; you can refer to elements loaded in a namespaceenabled XML document, but you need to know the prefix and use that prefix when.
After loading, we use the <echoproperties> task to print out all properties with the prefix x:, that is, everything with that namespace alias:
The <echoproperties> task escapes everything, just as a Java properties file requires, so every colon has a backslash that’s just an artifact of the echoing:
We’ve now covered writing, reading, and validating XML from Ant.
There’s one more way that Ant can work with XML, and it is possibly the most powerful.
It can transform the XML into other forms through XML Stylesheets (XSL)
These XML files contain rules for turning one XML file into another document.
Ant can apply these to XML files as part of the build.
It’s the ideal format for documentation because it lets you transform it into display or print formats, such as HTML and PDF.
This can be done at runtime, perhaps with a framework such as Cocoon.
It can also be done at build time, to convert XML into readable text or HTML.
Indeed, that’s exactly how <junitreport> works, turning test logs into HTML pages.
Transforming an entire fileset of XML files with a single XSL stylesheet is easy.
Figure 13.1 shows the basic workflow: incoming XML files are transformed by an XSL stylesheet into new XML, HTML, or text files.
Those text files can be documentation, or they can be something more sophisticated, such as Java source files.
The problem we’re going to explore is exactly that: taking an XML source file, constants.xml, and creating Java source files and HTML documentation from it.
Define the structure and, perhaps, the DTD/Schema of the XML file.
Create XSL style sheets to generate Java and HTML files.
Define in the build file the locations of the XML and XSL files and the destination directories.
Here we’ll define the XML namespaces of the Atom drafts, the standards, and some string values for XML elements in their feeds.
For every URI in the constants.xml file, we’ll define a static final URI, turning any exception from the constructor into a runtime exception.
Our XML file will have constants, either URIs or strings.
Each will have a name, a value, and perhaps a description, the latter being multi-line.
From this, an approximate DTD falls out, as shown in listing 13.4
The constants.xml file goes into webapp/xml, the same directory as the file format.dtd.
This allows us to use a local system reference for the DTD.
We’ll be able to use these constants in the web application or its test code.
To do that, we need the constants in our Java source.
We could do that by hand, but that would just create maintenance problems.
Every time we added a new constant, we would have to remember to edit the Java source and the HTML documentation, and then rebuild everything.
What we want is something better: to create the Java source from the XML.
We need XSL style sheets for both Java and HTML files.
Listing 13.6 shows the XSL file to create the Java source.
It defines three URIs and the names of three elements in some proposed XML.
Listing 13.6 The style sheet, toJava.xsl, which generates Java from.
The XSL engine matches and applies the templates, outputting XML, text, or HTML.
In the stylesheet, the /constants XPath matches the root element of the document, <constants>, so that template is applied first c.
Inside that template goes the header of the Java document, the package declaration d, any imports e, and then the class declaration, a private constructor, and a static helper method f.
Finally, just before the closing bracket of the class and the end of the template h, we apply all templates that match uri or string against the children of /constants g.
That will apply the relevant templates for the child elements.
The <xsl:value-of> declarations in all these templates extract the text values of the XPath supplied in the select attribute.
Creating an XSL file that generates syntactically correct Java is no easy task.
Trying to produce code that lays out well is even harder.
It may be possible, but it would probably make the XSL file itself very hard to read.
The style sheet in listing 13.7 to generate HTML is much simpler.
By using the XPath "uri|string", this template will be used for either element c.
With both XSL files written, Ant can generate the Java and HTML files from the constants file.
To create the XML, we need a new target to define all the various source and destination files, and we need to create the destination directories.
This means that Ant is ready to create the files.
Generating the source with <xslt> After the complexity of the XSL files, the targets to run them come as a relief.
For both the source and the documentation, we declare an <xslt> task, listing the input XML file, the output file, and the XSL stylesheet:
The <xslt> task is dependency-aware, which means that it will run if the destination file is absent or if it is older than either the source file or the stylesheet.
This looks like it worked, but we should check to make sure.
A web browser can check the HTML page, as shown in figure 13.2
We can look at it; it appears valid, but it contains various constants laid out in a bit of a mess:
The best way to check is, of course, to compile the source.
We can use this in developer documentation, or add it to the web site content.
Adding generated code to the compile stage complicates the process.
We need to make sure that the source is generated before <javac> gets called.
We also need to run <javac> over both source trees, the main tree and the generated source, at the same time.
We cannot do one before or after the other if we want to have them interdependent.
Next comes a complete rewrite of the compile target, in order to change the srcdir property.
Running the new target is a bit of an anti-climax, as all that happens is that Ant compiles one more file:
A Java file, whether handwritten or generated from an XML source via an XSL transformation, is just a Java file.
Generating Java source from XML documents may seem like a lot of work.
Once the process is set up, however, it becomes trivial to extend it to generate more complex types.
Using XSL transformations to generate Java source is only one example of what XML+XSL can do in an Ant project.
It just happens to be the one that’s useful for many Java projects.
The task can just as easily be used to generate HTML pages from XML documents or even CSV-formatted text files from the XML results of a JUnit run.
Whenever your build needs to take XML files and create some kind of text, HTML, or XML file from it, turn to the <xslt> task!
Although not an XML workflow engine, Ant can produce, consume, and transform XML during a build.
This lets it handle the XML-related steps of the build process, from reading and writing XML files, to generating new content from XML source data.
The <xmlproperty> task lets you read XML back in to Ant properties, extracting information from the values of elements and attributes.
These can all verify that XML files match the specification, or, alternatively, that the specification is adequate.
If you’re creating XML files in Ant, validating them lets you know immediately when the files are invalid, helping you to get the build working.
This lets you store content in XML form and then dynamically generate text, HTML, or XML files.
We’ve shown how to use it to create both HTML content and Java classes from the same XML source.
Finally, remember this: Ant isn’t a tool for general purpose XML processing.
Ant’s XML processing is limited to a bit of XML work during the build process.
If you find yourself reaching the limits of what Ant can do with XML, then take the time to look at tools that focus on XML processing, and see if you can delegate those parts of the build to them.
Now, where does the web application stand? This chapter added validation of the Atom feed, and we’ve got some constants we need being generated from XML.
With the Atom feed working, that means that the web application can serve up data to any RSS client that supports the Atom protocol, including current versions of Mozilla FireFox and Internet Explorer.
If we want it to save and retrieve events to and from a database, we need to work on the back end of the application.
What that means is that the Java EE API provides all the APIs needed to make Java classes persistent to a database, and for the outside world to talk to the application over HTTP, CORBA, or the Web Service protocols.
It doesn’t provide implementations; Java EE applications work properly only inside an application server.
There’s been a bit of a backlash against Java EE, leading to the lightweight opensource alternatives of Hibernate and the Spring Application Framework.
All server-side frameworks have the same problem: you need to deploy and test them on an application server.
We’re going to make the diary Enterprise-ready by binding it to a database.
This requires that we rework the web application to make the diary and its events persistent.
Write a calendar API to provide access to the persistent data.
Ant, as we’ll see in the later parts of the project, can help.
But before we get to building, packaging, and testing the code, we need to install the SDK and write the application.
Installing the Java EE SDK Enterprise Java comes with a whole new set of Java classes, mostly in the javax packages.
The API is implemented in the file javaee.jar, which comes with the Java EE SDK.
The first step is to download the Java EE SDK from http://java.sun.com/javaee.
The JAR file provides most of the APIs for Enterprise Java and some Java annotation declarations to accompany them.
What it doesn’t contain is any of the implementation classes.
The Enterprise Java applications work only inside an Application Server; which is a program hosting the deployed Enterprise applications, creating Enterprise Java Bean (EJB) instances on demand, and implementing the various server-side APIs.
Selecting an application server To use all of the Java EE services, we need an application server.
It runs on most platforms, is thoroughly documented, and is the most popular open source application server.
A good alternative would have been Sun’s Glassfish application server.
It has a very nice web-based control panel as well as good support for current Java EE technologies.
Here are the core steps to follow to install JBoss:
In the page “Isolation and Call by Value Semantics,” you must check the “Enable deployment isolation/call by value” option.
Things will not work properly unless that option is selected.
If the JBoss web page is visible, the server is installed and ready for deployment.
For more information, you may want to check out the book JBoss at Work (Marrs and Davis, 2005)
Databases We also need a database, which complicates deployment and testing.
For deployment, you need to set up the database with the right tables and accounts.
For testing, we need to add data setup and then clean up on teardown.
This is a pure-Java database that can run standalone or as a lightweight in-process datastore.
It also has a nonpersistent mode in which the data is discarded after the program ends.
The product’s home page is http://hsqldb.org/; the version used was 1.8.0.7
The single JAR is all you need; it implements the Java Database Connectivity (JDBC) API for database access.
With all our tools set up, we’re ready to turn the diary into an Enterprise application.
To make our database persistent, we’ll have to turn some classes into Enterprise Java Beans (EJB)
There are three types of Enterprise beans, as listed in table 14.1
In previous versions of J2EE, Enterprise beans were complex classes backed up by complex XML files.
We can turn our Event class into an entity bean and design a session bean for access to this data.
Entity Bean A class that can be saved to and loaded from a database; fields in the class map to columns in tables.
Session Bean A class that provides access to the data through a local or remote interface.
It can contain “business logic,” meaning server-side code that works across beans or other parts of the system.
Message-Driven Bean A class that is invoked when incoming messages are received.
The entity manager handles persistence; the application server hosts everything and connects the web application with the session bean.
To make the diary’s Event class persistent, we mark it up with annotations from the Java Persistence API (JPA)
The bare minima of markup needed to make a class persistent are the annotations to bind the class to a table and a mapping for every persistent field.
A @Table declaration names the database table to use; the table we use here is called EVENTS:
The proxy class is created for us, and bridges from the web application to the session bean.
We then define a mapping of the persistent fields to database columns, producing table 14.2
Every persistent attribute in the class gets a @Column annotation on its getter method, the  method in the class offering access to the data.
The annotation should state the database column to use for the attribute and, sometimes, details about the mapping:
The trickiest field is the ID field, which is stored in the database as a string instead of as a serialized  java.util.UUID class.
This is done by providing a getter and setter for a string attribute called key, marking that as the primary key with the @Id attribute.
The method just converts the existing id field to a string.
Table 14.2 Class fields and types and their database equivalent.
The result is to turn the key field into the primary key, which can be used in query strings.
This brings Ant into the picture to build and package the class.
The task prints out a list of missing artifacts and fails the build:
One is to bypass Ivy and add it to the classpath by hand.
The other is to install the file into a team repository.
Chapter 11 raised the notion of a team repository as a way of providing secure artifacts or to offer better metadata.
It also can serve up files that are not in any public repository.
The diary application has a repository set up under the CVS tree; everyone who checks out a file gets a copy.
The log of the build shows where Ivy looked for the file, a search that included the team repository:
There’s no need to add any ivy.xml file, unless we want to add dependencies and multiple configurations.
It’s assumed to have no dependencies and one configuration, “default.” This is how the diary-core-persist component declares a dependency on its own ivy.xml file:
We also can use it as a persistent object, bound to a database through an EntityManager.
This is a class that binds events to a database, with methods to find, delete, and update beans.
The web application can work with one through a session bean.
Session beans are another part of the Enterprise Java Bean architecture.
They are where local or remote callers can interact with the system.
For the diary, a session bean can provide the public API, offering operations to add, remove, and find events.
Client applications will use the interface, with the Java EE container bridging between the interface and the implementation through a proxy class it creates—a proxy class that also implements the interface.
Session beans also can be exported as web services, allowing remote callers access to the same services.
These annotations declare the class to be a stateless session bean with one local interface.
That means that it doesn’t preserve any state between method calls, which makes for easy reuse.
The @Local attribute names the interface that we use for local invocation, that being all that we support.
The class itself extends the CalendarSession interface c, which we defined in listing 14.1
Users of the session bean work through this interface, an interface which the session bean will implement.
The most interesting declaration is where we declare an EntityManager d.
We declare the manager in a field, but we never set it to a value.
The application server will configure it for us, a technique known as dependency injection.
Before every transaction begins, the container sets the bean’s manager field to the current entity manager.
The class uses this injected entity manager to work with the database:
Like the entity bean, this class compiles without any special steps, provided the Java EE JAR is on the classpath.
What we cannot do with this class is run it outside an application server, which will do the dependency injection.
This forces us to put off testing until the classes are packaged and integrated with the web application.
This also provides incentive to get on with the packaging.
Writing a persistence.xml file By marking up our classes with annotations, we can almost completely avoid writing new XML configuration files for our beans.
We do still need one, META-INF/ persistence.xml, to define the persistence unit of the library.
The persistence unit describes how to bind the entity beans to a database, including server-specific customizations:
This file lets developers change the data binding without recompiling the application, although you still have to repackage it.
A single persistence.xml file also can contain multiple <persistence-unit> elements, each one targeting a different runtime or application server, or exporting different classes into different persistence units.
Packaging the beans With the code compiled and the XML file written, we can package everything with <jar>
This is what the web application now needs to import and work with.
With the entity and session beans coded, the web application can become persistent.
Add the persistent event and Java EE JARs to the compilation classpath.
Build the new WAR file, with its enhanced web.xml file and appropriate class libraries.
We’re going to omit most of the coding changes made to the app, as they don’t change the build process.
However, the web application does need to change its dependencies, swapping the servlet API dependency for that of javaee-1.5.jar and the beans in the diary-core-persist library.
Table 14.3 lists the dependencies that Ivy resolves to compile the web application.
Updating the web application code itself is harder, especially in the area of event management.
The CalendarSession interface provides access to the persistent data store; the web application needs access to an implementation.
In a fully compliant Java EE 5 container, this can be done with dependency injection, just as the session bean is bound to the entity manager:
To get the session bean, we need to look up the class by using the Java Naming and Directory Interface (JNDI) API:
We access this via a JBoss4Calendar class that acts as a session factory.
This JNDI-based binding mechanism appears so brittle that it caused lots of problems during development.
Consult the latest JBoss documentation before copying this code, and use session injection as soon as it’s working.
Assuming the lookup does work, we can now call getSession() when we need it, such as when creating an atom feed of all events:
This code gets a calendar session b, to enumerate all events c.
Similar changes to the code have to take place wherever the event list is used, and then the application is ready for compiling and packaging.
Packaging the new web application The web application of chapter 12 included every JAR that it needed in the WAR file’s WEB-INF/lib directory.
This web application will work only when packaged inside an Enterprise application archive (EAR) containing its dependencies.
They are JAR files with an .ear suffix, files that can contain the following artifacts:
Resource adaptors for binding to non-Java systems, as .rar files.
The EAR file contains the entire application, becoming a single thing to distribute, deploy, and undeploy.
Not only do you avoid duplicating libraries, by using a common classloader the application code can pass objects between different parts of the program without serializing them.
Collect all the JAR and WAR files needed for the application.
We need to know how to add JARs in the EAR onto the classpath of the beans and the web application.
Here are what appear to be the rules for declaring artifacts in the application.xml file, and how the classloaders work:
Everything in their WEB-INF/lib directory is added to its own classpath.
If an <ejb-module> file declares a classpath in its JAR manifest, the JARs in.
JBoss, in the “unified classloader” mode, builds up a classpath from all JARs.
JBoss in the proper classloader mode adds all client <java> modules to the classpath.
The default library directory of a Java EE 5 EAR file is /lib.
Application servers export their own classes; these may have priority over.
If you need to include “endorsed” JAR files, you need to know about Extension-List entries in manifests, or use an endorsed directory on the server for the files.
There are a lot of rules about setting up dependencies.
Chapter 8 of the Java EE Specification covers these in detail, though it ignores server-specific details, one of which is critical:
Every application server has a different set of bundled libraries on its classpath.
Duplicating these JARs in the EAR or WAR file can cause problems.
Furthermore, some artifacts, such as JDBC drivers, cannot be deployed in the EAR.
This means that if you want to deploy a JDBC database driver, it must be on the classpath in a place where the JDBC API can find it.
This whole problem of classpath setup is why the notion of application server portability is a dangerous myth.
It’s a myth because you’ll need to build different EAR files for different machines.
It’s dangerous because some people believe it, and it gets them into trouble.
This leads us to our definition of an EAR file:
An EAR file is a JAR file that contains an enterprise application.
The EAR file contains the WAR files, JAR files, and XML configuration data for a specific installation of a system.
That means we shouldn’t think of an EAR file as a broadly reusable library, like a normal JAR file.
It’s a way of packaging the files for a target deployment.
Collecting the artifacts The first step to building an EAR file is identifying which libraries we want and collecting them in one place.
We want our own diary-core-persist and diarycore libraries, without Log4J or the Java EE artifacts.
We want Rome with the JDOM XML library it needs.
All of these tasks are handled by a new configuration in the ivy.xml file:
The artifacts that we want to put in the EAR file get declared as being part of this configuration, resulting in the following ivy.xml dependencies:
This code declares that the EAR has all the same dependencies as compiling the web application, except for the unneeded Java EE JAR.
We explicitly ask for the “master” configuration of the diary-core and diary-core-persist artifacts so that Log4J doesn’t sneak in.
This task will copy all JAR files into the directory build/ear/lib, except for the WAR file itself.
To make the classes visible, these files need to be listed in the application.xml file.
The application.xml file is the place to list web applications and EJB packages.
In Java EE 5 runtimes, we also can point to a directory containing extra libraries, so we could trivially refer to all of the JARs by adding them to the /lib directory of the EAR.
As JBoss 4.0.5 doesn’t support that, we have to enumerate every artifact we want loaded.
By dropping the revision counter from the retrieved artifacts, we avoid this step and can use the XML file directly in the <ear> task, that being the final step of creating the EAR file.
The EAR file is built, with all the libraries it needs.
The EAR file can be deployed using the deploy-by-copy process from chapter 12, selecting the destination directory from property files in the persist-webapp/ deploy/ directory.
In use, the deploy target will build and deploy the EAR file:
If JBoss is running and the destination directory is correct, the server will now load the application.
The code is compiling, the EAR is packaged with all libraries, and it has been copied to the server.
Setting up the Java projects and build files for the persistent classes, the web application, and the EAR file itself is hard work.
Once they’re in place, the build files should stay stable while classes are added and the project evolves.
We can play with the deployed application, and we can finally get down to testing the application.
The existing web front end is there, generating an RSS feed from events, and behind the scenes the calendar is now being saved to a database.
How can we be sure this happening? By testing, of course!
The HttpUnit tests of chapter 12 act as a regression test of the application.
As Web and Java EE applications run only on an application server, we cannot directly run such unit tests under <junit>
Something devious is needed here, which is where Apache Cactus comes into play.
Cactus tests server-side code by running JUnit unit tests inside the application server.
Cactus operates in a client-server design, meaning a client-side JUnit run manages the test run on a remote server.
All the tests run remotely, with the results fed back to the client where they are presented as the output of a normal JUnit test run.
There’s some profound work going on behind the scenes to achieve this testing.
The test classes need to be present at both ends of the system, as the JUnit test runner still has to instantiate them.
Requests to run each test method are then relayed to the server over HTTP and delivered to a Cactus servlet.
The servlet creates the real test cases, runs the test methods, and feeds the results back.
It’s as if you’re running the <junit> task inside the server.
If you’re doing anything complex server-side, download Cactus from http://jakarta.apache.org/cactus/ and start writing server-side tests.
Writing a Cactus test case is like writing any other JUnit test.
The big differences are that the class must extend ServletTestCase, and enough server-side libraries must be available for  <junit> to create the class in its own JVM.
The client program relays test requests to the server over HTTP; results come back the same way.
This test uses the CalendarSession interface, creating an event then removing it, testing the state of the collection after each action.
The setUp() method runs on the server, so that’s where all setup operations must go.
It’s absolutely critical that this method is used for configuring the test, and not the constructor, because that is called in the <junit> JVM as well as the server.
With this test and some others coded, it’s time to return to the build file, which must compile the classes and run them on the server.
We need to add the test classes to the EAR and extra server-only libraries to the client for <junit> to run.
That is because JUnit creates instances of the test classes.
Every class the test case links to needs to be present, or else we’ll see a runtime linkage error.
We need to add Cactus to the list of dependencies for the test classpath, alongside junit.jar, httpunit.jar, jtidy.jar, and of course the compiled application itself.
Cactus is one more Ivy dependency, albeit without its dependencies on Xerces and Cargo:
Listing 14.3 A Cactus test case to run on the server.
With Cactus added, the existing test-compile target will build the Cactus tests.
Setting up the classpath is also a problem on the server side, because Cactus needs.
The compiled test classes need to get into the EAR file along with the Cactus and JUnit JAR files.
Cactus ships with five Ant tasks, which are listed in table 14.4
Three tasks configure WAR and EAR files for testing, while the remaining two tasks can run the test suites themselves.
To use Cactus, we must declare the tasks using the classpath created for compiling the tests, a classpath which contains everything we need.
First, the build file declares the name of the WAR file to create and the URL by which it will be known;
All the tasks are now ready for use, the first use being patching up the EAR file.
This task takes the name of the source EAR file b and that of the output file c.
It will then add a new WAR file to the archive by way of the nested <cactuswar> element d.
This takes the name of the source file, the context under which the web pages will be deployed e, and the list of test classes to include f.
When the task runs, it creates a new WAR file inside the EAR file.
This web application includes all the test classes and whatever Cactus class files are needed to run the tests, as well as a patched web.xml file to relay Cactus messages.
The task then adds the new web application to the EAR file’s application.xml entry:
With the test code and servlets, it shouldn’t be deployed to a public server for security reasons.
This task extends the normal <junit> task with integrated deployment to an application server.
We don’t need this, as we have our own deployment process already set up.
We need to copy the new EAR file to the deployment directory, then wait for it to be deployed—after which the tests can run.
This is exactly what the existing HttpUnit tests of the web application did, and the build file is almost identical.
It is the URL to the base of the Cactus-enabled web application, here "http://localhost:8080/cactus"
Listing 14.4 Targets to deploy the EAR file and run the Cactus test suite.
The command line to run the tests is ant cactus-run, with the tail of the results as follows:
This test run not only runs the new Cactus tests in the server, it runs the existing HttpUnit tests.
These run in the local <junit> JVM, not in the server.
Cactus tests and classic JUnit tests can live side by side.
If tests fail, the reports will even include remote stack traces from the server.
Together, the Cactus and HttpUnit tools extend JUnit to test the internal state and external behavior of a server-side application.
Until Cactus or an equivalent works with the newer test frameworks, the test suites for Java EE applications need to stay with JUnit 3.8.x and Cactus.
However, they’ll probably fail the moment we deploy to a different application server, where things are always subtly different.
We need to be able to debug what’s going on, even remotely.
One early problem we had was getting JNDI directory lookups right, meaning it was difficult to track down the fact that a “full” installation of JBoss didn’t include EJB3.0
Accordingly, we added a new static method to our FindClass class, one that can check that a JNDI entry resolves.
Here it is in our updated happy.jsp page, looking for the data source and the session bean:
The best bit: anyone can browse to the JSP page and see the diagnostics.
The deployment problems encountered in development have been turned into health checks for the operations team.
This chapter has entered the realm of Enterprise Java, looking at how to make entity bean and session bean classes by annotating the source.
The web application is part of a bigger program, and the WAR file must be deployed inside an Enterprise application archive, an EAR file.
In theory, this is a portable distribution package, which can be deployed onto any application server.
In practice, EAR files often need to be targeted for specific application servers, adding needed JAR files and removing those that cause problems.
The various XML files in the packages, persistence.xml, and application.xml may also need customization for different machines, which can complicate the build further.
Ivy comes into its own here, pulling in files at compile time, and again when setting up the libraries that the <ear> task can package into an EAR file.
Alongside the functional tests of HttpUnit comes Apache Cactus, which can run  JUnit tests on the server.
Cactus is a great way to test server-side Java code.
The key thing that readers should probably remember is this: Apache Cactus runs unit tests in the server.
We’ll return to the problem of deployment in chapter 16, when we show you how to take an application into production.
Before then, let’s see how continuous integration can transform a development process.
There’s one more thing that Ant enables that can transform how your project works.
Instead of running the builds by hand, why not delegate all the work to a machine? A machine that can rerun the builds and tests nightly, hourly, or whenever you feel the need? Running builds repeatedly is useful, but far too boring to do by hand.
That’s the kind of dull, repetitive work that machines are for, and where Ant can help.
This chapter is going to cover setting up a machine to run builds for you, not just to save developer time but also to transform your entire development process.
This chapter will use Luntbuild, which is a web application that can be set up to run Ant builds on demand.
But before we can get down to the application itself, we have to understand the concept of continuous integration.
Continuous integration is the practice of setting up a system to do your build for you all the time.
This is one of the most profound changes you can make to your development process.
If you have a machine always building and testing your project, then you can see at a glance whether all is well or not.
But you don’t need to check: if things break, the machine can email everyone a warning of the problem.
It may even be able to identify what change broke the build and who checked it in.
Nightly builds have long been a feature of software projects: it’s a famous aspect of the development of Windows NT and its successors.
But a nightly build has to handle everything checked in during the day.
You don’t find out until the morning that something broke, and then nobody can work with the latest code—it’s broken.
The whole team ends up wasting half the day finding out what went wrong and fixing it, only having half a day to write new code.
And then the whole process repeats itself the next day.
With continuous integration, you find out minutes after any change gets checked in whether it breaks the build or the tests; this lets you fix it before anyone else notices.
Ant builds can be scheduled and automated by using operating system jobscheduling services, such as the Unix cron tool, but that’s still not enough.
Dedicated tools can deliver much more, such as the following:
The core of the tool is the build loop, a thread that polls the source code repository every few minutes looking for changes.
After running a build, the results are published, along with notifications to developers and other interested parties.
In CruiseControl, the build loop was configured by a single configuration file, config.xml.
The web application was a separate process that does little, other than.
A servlet could act as a front end to the Java management extensions (JMX) beans in the build loop process so it could trigger builds, but all configuration was through that single XML file.
However, there are many challengers for the crown, some of which are listed in table 15.1
It’s a competitive area, with products competing on features, ease of use, and integration with other processes.
CruiseControl is hard to get up and running; it often needs a bit of nurturing to restart it or fix some problem.
This is the main reason why we’re not covering it, looking at Luntbuild instead.
The build loop polled for changes, ran the builds, and published reports; the web front end displayed the results.
A dedicated server You need a dedicated machine for running the builds.
Don’t try to reuse a developer’s own machine, as the developer can break the build by running tests at the same time, or by changing the system configuration in some incompatible way.
Find an old but reliable box, add more memory, install a clean OS release, tighten up security, and then install Java and Ant.
If you don’t have a spare machine, try running a Linux system under VMWare, Xen or something similar.
Bamboo http://atlassian.com/ Released in early 2007; integrates with the JIRA bug-tracking system.
Create a special user account with an email address, SCM login, and an IM account.
This user will be used for sending out notifications and for accessing and labeling the SCM repository.
A build that runs to completion You need a build file that runs.
The one thing to avoid is taking a build that doesn’t work and trying to host it on the server.
If you have this problem, set up the server to build a target that succeeds, skipping failing parts of the build such as the tests.
There’s a lot of work alongside the build, including polling the repository for changes, assigning blame, and generating the status web pages that Ant doesn’t do.
People have written tools to do all of this, so use them.
The more ambitious you get about running functional tests on the server or using different reporting mechanisms, the longer things will take.
Of all these requirements for continuous integration, time is the most precious, especially on a late project.
More subtly, just as writing tests from the outset changes how people code, having a machine continually building the application and running the tests changes how people work.
It’s better if this new lifestyle is lived from the birth of a project, rather than forced onto the team later on when they already have a defined style of working.
The rarity of free time in a project is also why we prefer Luntbuild over older tools such as Cruise Control.
Luntbuild is an open source project, whose homepage is http://luntbuild.javaforge .com/
It’s very easy to install: all configuration is done through web pages.
It also has a strong notion of dependencies across projects.
It has a self-contained mode, using a private database and servlet engine, or you can.
That means that it could be used in a managed production environment, with the application running under a central server, sharing a central database.
You could even have extra reporting programs running through the build information in the database.
Luntbuild uses the following terms, which we’ll explain in a moment: users, projects, VCS adaptors, builders, and schedules.
User A Luntbuild user is someone with password-protected access to the site.
Each user can have email, instant messaging, and weblog contact information.
Version Control System (VCS) adaptors Luntbuild uses the term version control system, with the acronym VCS, to refer to a software configuration management (SCM) repository.
A VCS adaptor is a binding to a specific SCM repository, including login information.
Every project also maps accounts in the SCM/VCS repository to Luntbuild user accounts, so it knows who to contact when things break.
Builds can be clean or incremental; incremental ones preserve the results of the previous build, and are faster, though they can be contaminated by previous builds.
Schedules can be triggered by the clock, by changes in the repository, or by manual requests.
Projects can have multiple builds and schedules, so a clean build can run every night, incremental builds every fifteen minutes, and tests run every hour.
Luntbuild comes with an installer JAR, which installs the server, asking a few questions on the way.
The simplest installation is a self-contained server using a private database.
Install the program on a disk with plenty of space.
Increase the session timeout from 30 minutes to something longer.
For the database, select HSQLDB and leave the option fields alone.
Hosting the tool under Tomcat or JBoss would ensure that the program restarts after it reboots, while a binding to MySQL may scale better.
Tomcat and JBoss also increase the installation effort, which is why they’re left alone.
To start Luntbuild, run the application giving your hostname and a socket port that you aren’t using.
If you’re using Ivy for library management or checking source out from a remote Subversion repository, you may need to add proxy settings to the command line:
Make sure the output shows that the program is listening on the server’s external network address:
If you aren’t careful, Luntbuild may just listen on the local loopback address such as 127.0.0.1:
This means the web page is only accessible to users on the same machine, not for anyone else on the network.
Start Luntbuild with the raw IP address if you cannot prevent the system from mapping from a hostname to a loopback address:
You’ll be prompted for username and password; use luntbuild for both.
This should then bring you to the home page of the program, from where you can configure it.
Luntbuild claims you can be up and running in half an hour.
Creating users The first step is to create users, both for login and for notification.
You can be notified by email, IM, or blog postings.
The password is set on the same page as the other options.
This forces you to reenter the password whenever updating any other setting, which means that whoever manages the users needs to know all the passwords.
Once the users have been entered, you should have a list such as that of figure 15.3
Different users can work on different projects and can have different rights.
We tend to run a relaxed house on a private network, but you may want to lock down the system.
Creating a project After creating the users, you can create a project.
Create a new project by clicking on the new document icon on the top-right corner of the page.
As anonymous users still appear to have read access to all projects and builds, you cannot keep builds private from anyone with access to the web server.
Binding to an SCM repository The “VCS Adaptors” tab of a project is where you set up the repositories of a project.
Every project needs at least one VCS adaptor, with one module underneath.
As well as declaring a repository, you also must define a module.
This represents a subsection of the repository and may be a specific branch or label.
Being able to mix branches and labels of different parts of the repository is an interesting feature.
It may let a team build and test multiple branches of a project.
Figure 15.3 A server populated with a full set of users.
We haven’t tried that; instead, we just create different projects for each repository.
Creating a builder The builder is where we have to set up Ant builds.
The configuration to build the whole diary is shown in figure 15.6
This is an important form, as it’s where the tool bridges to Ant.
Figure 15.4 A Luntbuild project is bound to a source code repository and can have different builders to build parts of the project on defined schedules.
The build properties field comes pre-populated with some Luntbuild options to be passed down to Ant, which we extended with three more:
These properties will place the distributable files in a place where Luntbuild users can browse and download them, with test reports available under the “reports” tab of the build results.
The no.sign.jar property tells our build file to skip targets that prompt for the keystore password and to sign the JAR files.
Command to run Ant The path to Ant; ant will suffice if it’s on the path.
Build script path The path to the build file from the base of the repository.
Environment variables Any environment variables to set before Ant runs.
To set up Ant’s JVM, set up the ANT_OPTS variable here.
Build success condition How to determine if a build was successful.
To create a new builder, click the “new document” icon on the top-right corner.
ANT_OPTS to select a different logger, such as Ant’s time-stamped logger.
The moment your text output changes, the string comparison fails, and Luntbuild thinks the build was unsuccessful.
This is why the build machine should be separate from any developer’s box.
It’s too easy for developers to change their system in a way that breaks the scheduled builds; a tweak to ANT_OPTS is enough to confuse Luntbuild.
Creating a schedule With the builder configured, Luntbuild knows how to build a project.
For that we need a schedule, which is a set of rules about what triggers a build.
A project may have different schedules, such as a clean nightly build and full test and a more frequent incremental build with partial testing.
A schedule has a trigger, which can be manual, simple (every few minutes), or cron, which can be used to set an absolute date or time for a schedule to start.
Figure 15.7 shows the schedule for the incremental build of this book’s examples.
Figure 15.7 The schedule contains the settings to run an incremental build of our project.
It declares dependencies on other schedules, the builder to run, and the actions to take on success.
The schedule configuration is quite complex, especially once you turn to the.
Any schedule can be marked as depending on any other schedule.
The schedules can then be set to trigger builds of everything they depend on, or everything that depends on them.
We haven’t found that feature to be too useful, because it ends up making builds slower and slower.
If every component builds on its own schedule, that should be all you need.
The exception is configuring one big “clean build of everything” target to run sporadically, such as every night.
That’s good for builds triggered by check-ins just to notify the individual team members by email or IM, so they can fix the problem before anyone else is disturbed.
Nightly builds can be set up to notify the team mailing list, so the problem becomes more obvious.
One useful feature of a schedule is a post-build strategy.
This is a list of builders to invoke after a project builds.
For example, a builder could be set up to deploy the application after a successful build or to create a distribution package.
Luntbuild would run these dependent builds only if the main build succeeded.
We could use this build to have a scaled-up set of tests, with the long-lived performance tests starting only if the functional tests of the system pass.
The final piece of project configuration is the mapping between user names in the SCM repository and Luntbuild users.
This is so that Luntbuild knows who to blame when things break.
The default mapping uses the repository username as the email address of the recipient, and sends notification messages over email as the notification.
Once a schedule has been defined, it can be run by pressing the green arrow icon on the schedule’s page or on the big list of all builds.
If the schedule requires predecessor builds to be built first, all the predecessors.
Figure 15.8 Mappings from usernames in the repository to Luntbuild users.
Similarly, if dependent schedules are triggered after a build, then they will.
While getting Luntbuild to work, it’s handy to have a schedule you can run whenever you feel like it.
To do so, set the trigger condition to always to ensure that it runs even if the repository files haven’t changed.
With everything configured, Luntbuild should run continually, polling the SCM repositories for change and executing builders—which run the Ant builds and targets of the project—on the predefined schedules.
Figure 15.9 shows our example server, which is running a set of schedules from three different projects.
Every build publishes its artifacts to the local Ivy repository, to be picked up by dependent builds.
The (currently failing) test run, Test, is decoupled from the run Incremental that builds the artifacts.
This decoupling enables the builds of all the dependent projects to continue even if a test fails upstream.
The failing test run will not be ignored, but fixing it is something that can be dealt with in a relaxed and methodical manner.
The clean build of the Clean Release schedule of the smartfrog-core is active, so its status line is animated; a button lets a logged-on user halt this run.
Despite the high number of scheduled builds, the daytime load on this system is light.
The tool may poll for changes quite frequently, but builds are triggered only when source or dependencies change.
The book is building; some of the other projects are failing.
To determine what’s working and what isn’t, you need to subdivide a project into modules that can be built and tested independently.
It can take time to do so, but the benefit is that it’s immediately obvious which parts of a project have problems.
In use, the biggest problem we’ve found is that nothing kills builds that take too long.
This may be what you want, but it if isn’t, then other builds get delayed while one build has hung.
We also found it a bit tricky to track down failures, especially when learning to use the tool.
There’s a link in the top of the page to the “System Log,” which shows all the log output from the tool related to problems checking out files from the repository.
If a schedule fails to run and there’s no build log, turn to the system log page to see what went wrong.
Where they’ve really been innovative is when managing dependencies between projects, and when integrating scheduled builds with triggered releases.
Scheduled builds let you run a big overnight “soak test” alongside commit-triggered builds, which can stop at the unit tests.
The goal of the soak test is not to finish as quickly as possible, but to act much like a real application, even with realistic pauses if that’s how end users are expected to interact with the server.
Such tests find problems with memory leakage, concurrency, state management, and other aspects of a system that a normal test suite can miss.
By their very nature, they are slow, so you run them overnight.
If you use MySQL as the repository, other programs can access the data.
It’s got a good installer; the default options are the best way to get up and running.
The dependency chain logic can demand-rebuild a project if something needs it.
The post-build actions let you run deployment and redistribution build file targets, which can be triggered after the main build or test run completes.
You can give multiple people the right to create new projects, which lets developers add new builds without unrestricted access to the host.
The more builds and schedules you add, the more the configuration GUI becomes a limit to scalability.
It doesn’t automatically kill builds that run for a long time.
We wouldn’t trust the user accounts system to keep the build status private.
It’s easy to install, so it’s worthwhile investing an hour or two in getting it running.
Perhaps a newly created file was omitted, or perhaps there was an error in the source.
Compilation failures will be caught before the developer has finished the next cup of coffee.
Indeed, if the server can page their phone, they get summoned back to their keyboard before the coffee has cooled enough to drink.
Test failures can take longer to show up if a project has a thorough enough test suite.
Even so, you should expect to see errors from the unit and functional tests within half an hour of a defect being checked in.
If the team cares about the error messages, then the problem gets fixed.
The idea is that the best time to fix a problem is immediately after adding the problematic code.
If the fix takes, then the application is working again.
It may not be complete, but the source will always compile, the build files will always work, and the tests that the server is set to run will pass.
There’s no more uncertainty about the state of the program: anyone can look at the server status page and check that all is well, reviewing the tests to see what the build tests.
Furthermore, the entire history of builds can be preserved, so anyone can go back and look at what changed.
This capability helps with blame assignment—finding out who or what broke the system.
It’s also a good way of keeping management happy without them bothering you for status reports: the server lets everyone know what’s going on.
For this reason, continuous integration is more than just setting up a server somewhere to run builds; it’s changing how you write code.
They also deploy the system and run all the single-host functional tests, which take about 30 minutes to complete.
If the servers are happy, the program is probably in a shippable condition.
Sometimes the test fails, with problems that can take some days to fix—particularly if the problem is fundamental, or a new test is added that shows a new fault in the system.
In that situation, someone has to sit down and fix the problem; it’s a team problem, not that of any particular individual.
Everyone has the right to fix any part of the program in order to get the build passing again.
It isn’t easy adopting such a process, particularly if the cause is not “your part of the program.” However, it’s critical for everyone to care about the overall status of the build and for management to care enough about it to let developers spend time fixing the build, as opposed to adding new features.
With Xen/VMWare-hosted servers, having one background build per developer is possible.
Developers If warning emails from the server are ignored, there’s no point in having an automated build service.
A common reason for ignoring a message is the belief that someone else will fix the problem, because it’s in their part of the project or because they broke the build.
As more changes are added, soon the build will be broken in many places, making the effort to fix the build that much harder.
Sometimes tests fail for no apparent reason: if everyone assumes it’s some else’s problem, the build may not get fixed for a very long time.
At the very least, the initial failure report should trigger some discussion about what broke and why.
Any developer should be able to get their hands dirty and fix the build.
If it’s in some part of the project that they don’t understand yet, a broken test is an ideal excuse to learn their way around that bit of the system.
Management Developers will fix a broken build only if they care about it.
This should happen once they have adopted the process and recognize the benefits.
But how do you convince developers that time spent fixing the build is more important than other things? That’s where management has to help.
Whoever is in charge of a project needs to encourage the developers to care about the notification emails and the build’s status.
In an open source project, the team leaders need to set an example by caring about the build status.
When failure notifications come in to the mailing list, team leaders.
The leaders also can credit people who put in the effort to fix the build.
Private projects have a management chain that the developers have to listen to, so.
That is, “I spent yesterday fixing the build” has to be accepted as a legitimate reason for a feature being delayed.
If managers think that testing matters, they have to support anyone who gets a broken build working again.
One subtle way of getting management support is to use the status page as a way of tracking changes and watching the status of the project.
If the developers know that their manager uses the status page as a way of measuring a project’s health, then they’ll try to keep that page “green.”
The less-subtle way to do this is to force a halt to all other work until the build is fixed.
It certainly makes priorities clear, but it’s an option that should come out only in emergencies, at least after the first couple of times.
Developers themselves need to care about the build; the problem for management is working out how to make developers care.
Even developers who are too lazy to run the tests can start to rely on the server to test for them.
It takes time, and the gradual addition of more and more work to the background builds.
The key to success is support from the team leads—management, key developers, and architects.
If the senior members of a project embrace continuous integration and change how they work, the rest of the team will follow.
Continuous integration is the concept of having a dedicated program that continually builds and tests an application.
This lets you build up a background build process, with clean nightly builds and incremental ones running throughout the day.
It hands off the repetitive work of building and testing to machines that are happy to do it all day long, leaving developers to write the code and the tests.
More subtly, by integrating everything all the time, different parts of a big system are able to work together.
Looking back at past projects, the biggest mistakes we’ve done with continuous integration are.
Not starting off with it at the beginning of the project.
We used to delay using it, primarily because CruiseControl was so painful to set up.
Tools like Luntbuild make it so easy to get up and running that there’s no such excuse anymore.
A single server can build many projects, whether they depend on each other or not.
Every project should adopt continuous integration, ideally as soon as the project is started.
Since all application servers now support this, isn’t deployment done? Hardly.
The real problem is not getting the application to the remote machine, or even getting the application loaded by the application server.
A web site cannot be described as successfully deployed until the database is up and running, the web site can talk to that database, and its pages are externally visible.
This is what deploy-by-copy forgets about: copying a file to the application server is a tiny piece of the problem.
It’s now time to look a deployment in detail—all the way to production.
Before we start, we need to revisi the question: What is deployment?
Just as we’ve automated the building and testing of our applications, so can we automate deployment.
It’s going to be hard, because deployment itself is complex.
But it is possible, and it can transform your development if you can manage it.
In this chapter, we’re going to explore the final piece of Ant’s built-in deployment support—database setup—then look at how Ant can delegate advanced deployment to another tool, SmartFrog.
The chapter will show that development processes need to evolve to embrace deployment, instead of fearing it.
Production systems are far more intricate than a single developer’s box.
There’s a database server, multiple application servers, and a router.
Every difference between the development systems and the production systems can break the application.
Developers write the code, but operations have to keep everything working.
Since server-side deployment is viewed as easy, everyone neglects the problem until the application is due to go live, at which point it’s too late to automate, leaving nothing but chaos and delays.
There’s a very simple test to see if the deployment process is working.
If you have to go into the air-conditioned server room on a weekend, deployment is broken.
If you’re scared of the phone ringing, deployment is broken.
If you’re a week away from going live and you haven’t started bringing up the server, then deployment is broken—you just don’t know it yet.
It’s possible to survive deployment, just as you can stay on top of the rest of the software development process—by having a process to keep it under control.
The idea of integrating deployment into the development process is very leadingedge.
It’s something that some companies appear to do, though they don’t talk about it much.
Our proposed process is based on applying test-centric development to the deployment process.
Table 16.1 Keep deployment under control by integrating it with the development process.
Deployment is too important to leave until the last minute.
You cannot just throw the application over the wall to the operations team and expect it to work.
Even during development, target the application server, database, and OS of the production system.
With a fully-automated deployment process, you can eliminate errors in the deployment process and deploy to more machines.
Your project needs to recognize that deployment will be a problem from the outset, something that needs to be part of the development process.
Work with operations Developers have to build a system that meets the needs of the operations team—a system that is secure, manageable, and doesn’t generate support calls late at night.
The key is to treat the needs and problems of operations as just another part of the software development process, with use cases, tests, and defect tracking.
The management tasks on the system—creating, backing up, and restoring the database; tracking down why a user cannot log in; blocking the IP address of a malicious client—are all use cases that the system needs to support, one way or another.
Work with operations to find out what they want to do, and support it.
Then find out what causes them trouble, and avoid it.
Target the production system Differences between production and development servers cause problems, so reduce them with development systems that match the production one.
That’s very expensive to do with dedicated hardware, but you don’t need dedicated hardware if you make use of virtualization.
A virtual machine (VM) is a complete image of a computer hosted under the real operating system (OS)
The virtual machine can have its own virtual disk, with its own OS, and its own virtual display and network card.
Most programs and operating systems run perfectly on such virtual machines.
The latest x86 CPUs can run a VM at almost the same speed as real hardware.
Every developer can have a whole set of virtual hosts to deploy and test onto.
These machine images can be set up to resemble the production systems, with separate hosts for the application server and database server.
Virtualization also allows a large database to be pre-populated and saved to a disk image that’s rolled back at the end of every test run, giving tests a realistic database with almost no set-up time.
It cannot be used for performance testing, and all host systems need more memory and hard disk than normal.
Most critically, every OS image stored to a virtual disk drive is another OS to maintain.
The operations team needs to be in charge of the virtual hosts.
Automate deployment The entire act of bringing up a new physical or virtual server should be fully automated and, ideally, hands-free.
If this can be achieved, then you can reliably create servers that work the first time.
That includes the OS, with Preboot Execution Environments (PXE), which is something for the operations team to worry about.
The stuff above the OS is something to work on together.
Everything above the OS—the application server, the database, or any other application—all need to be automatically installed.
Sometimes the products can create their own repeatable installation script, which can be used to replicate an install.
One tactic is to use the package management features of the underlying OS in order to get the OS management tools to push out software.
The weakness of these tools is that only system administrators can install the packages.
There should be no need to have super user/ administrator rights to install or run Java applications, unless they need access to locked-down parts of the machine.
The database needs to be installed, then populated with a realistic dataset.
Once the database is populated, it can be backed up and reused in test runs.
Either the relevant database directories can be backed up and restored before each run, or the complete virtual machine can be copied.
That snapshot of the database server can be brought up whenever a test run needs a server with a database full of test data.
The happy.jsp page and its tests for various classes are the foundation for a testable deployment.
As more problems arise, new tests can be added to show the problems and to verify that they aren’t present in deployed systems.
Having health checks in a web page makes it easy for humans to check that a system is happy; they just point their browser at it.
The real secret is that machines can do the testing too.
In the production system, monitoring tools can check the page every few seconds and restart the server if one system is unhappy.
This monitoring only works if the system has a thorough set of health tests, which will come by writing them during development.
Track deployment defects Every deployment problem is a defect, one that will come back later to haunt you.
Treat them as such, with entries in a defect-tracking system.
For the defect database to be useful, its symptoms must be noted with a statement such as “JSP pages do not compile,” the cause, such as “javac not found,” and the fix, i.e., “added JDK/bin to the path.” This is all obvious stuff.
What’s essential is that the defect tracking should begin the moment you start developing the application on your local server.
The developers gain more experience in the issues than anyone else.
This knowledge needs to be passed on to the rest of the team.
This whole problem of deployment is beyond anything a single tool can address.
Ant can help with these tasks by preparing artifacts for deployment and by integrating deployment with the build process.
Ant was the first build tool with built-in support for distribution and deployment.
It knows how to compile, test, and package Java programs.
It also knows how to redistribute the created artifacts by SSH, email, and FTP.
It doesn’t talk over the network to other copies of Ant.
It is unlikely to be able to run for months, and its workflow is very simplistic.
It runs a build to completion or it stops in an indeterminate state.
Sometimes you can tell tasks to ignore failures, but that isn’t real fault handling.
Ant is used all the way through to deployment, cropping up as a startup script or launcher application.
This happens because it’s the closest thing Java has to a crossplatform scripting language and because there are third-party tasks to simplify the process.
And that’s the secret to getting Ant to deploy properly: delegate deployment to other tools.
Ant is just part of the tool chain, something that’s best at building and testing.
It’s the program you can start from the IDE, the command line, or the continuous integration tool, so it’s a great way to start the deployment.
If Ant cannot deploy or undeploy the program, you cannot integrate deployment into the build and test cycle.
However, Ant by itself just isn’t up to the job of performing complex deployments and undeployments, especially remote ones.
It needs help from tools such as Cargo and SmartFrog.
Its specialty is deploying WAR and EAR files to application servers, and starting and stopping those same servers.
It can even install middleware by fetching and unzipping an.
It can also be used inside Apache Cactus, as a way of deploying the WAR or EAR file containing the tests.
Lack of space prevents us from covering Cargo, but the http://antbook.org/ web site has some details on it, along with the build file to deploy the application.
The reason we’re skipping over Cargo is that we’ve chosen to look at a more ambitious tool—SmartFrog.
But before we do that, there’s one last thing we want from Ant: a database.
Now, SmartFrog can do the database setup itself, but one reason to do it in Ant is that it’s easier to mix SQL operations into Ant targets, with the clean target also deleting tables from the database or having a target to create a user that sets the password from an Ant property.
Back in chapter 14, Ant used Cactus to deploy the diary application onto an application server and test it.
However, we used HSQLDB as the database, which is a pure-Java database that can run in the process creating the JDBC connection.
For our production system, we want a real database, such as MySQL.
Following our “target the production system” rule, that means we should be testing against MySQL, which will need to be installed and configured with a diary database and user.
Following the “automate deployment” rule, we have to automate the installation and setup of MySQL.
At the very least we can set up the database before we run the tests, cleaning out the old data and creating the user account.
Before Ant can talk to the database, we need to install MySQL.
An automated installation of MySQL is possible, but it’s also beyond the scope of this book.
For Linux it comes in various formats and in many distributions.
Set mysqld to start as a Windows service or a Linux daemon.
Type mysql to get the MySQL command line, then type status to see the database status.
Read the license: the driver is GPL, with special terms for open-source projects.
Understand it before redistributing MySQL or applications that depend upon it or its JDBC driver.
Ant’s <sql> task can issue SQL commands to a database over a JDBC link.
To use this with MySQL, we need to set it up with the MySQL JDBC driver on the classpath and the password of the root account.
These changes set Ivy up to retrieve both the MySQL and HSQLDB JDBC drivers, drivers that can be passed to the <sql> task through its classpathref attribute.
We can then use <presetdef> to define two new tasks:
Define a new task to issue SQL command to the local MySQL database.
Define a task to issue commands to the diary DB as the ‘diary’ user.
If you don’t know about JDBC, this task isn’t really the place to learn, although it does let you improve your SQL knowledge through experimentation.
Our <mysql-admin> preset task can issue commands to the database.
Here are a pair of targets to drop the database (if present), to create a new database with full rights to the diary account, and to set the password of that account.
The mysql-create-db target depends on the mysql-drop-db target to destroy the database before it’s rebuilt.
To avoid an error if the database is missing, that <sql> call has onerror="continue" set.
When we call the mysql-createdb target, it creates the new database.
We can then use our newly defined <mysql> task to issue commands to the newly created diary database, such as setting it up in advance of a run:
This target will create the EVENTS table, which is ready for the web application:
Although the EJB runtime can create a database schema on demand, in a production system the database administrator (DBA) owns the problem.
The DBA should give the developers the SQL statements they need to set up the database and give the operations team the data binding files the application server needs to bind the web application to it.
If they’re in a separate file, the src attribute can be used to load, parse, and run the file.
There’s one thing to be careful of when working with the <sql> task.
The task will execute a sequence of SQL statements inline, from a file specified with the src attribute, or in nested <transaction> elements.
The task has to parse the text, breaking it into separate statements, before calling the database via the specified JDBC driver.
It tries to recognize comments that start with -- or //, but the code that does this is very temperamental.
Avoid comments in the file, or any non-standard SQL that the JDBC driver will reject.
To summarize: if you’re using a database in an application, then database creation and configuration is one of the tasks you need to automate.
What it cannot do is install and configure the database itself, or bind the application server to the database.
What if you need to install and configure the application server? What if you need to add the JDBC drivers to its classpath? What if the database needs to be set up on a different machine? What if the LDAP server on a remote system needs to come up before the database or the application server does?
Those are the kinds of problems that real applications encounter.
Deployment is a complex problem, with configuration of different parts of the system and choreography across parts of the system a key issue.
Ant cannot handle this; it’s beyond the scope of a build tool.
What can? SmartFrog, an open source application from HP Laboratories, can do all of this from its Ant tasks.
Before we proceed, know that Steve works on SmartFrog as his day job, addressing the challenges of large-scale deployment.
SmartFrog is a Java-based framework for deploying and configuring applications across one or more machines.
It is to deployment what Ant is to building things: a language, a runtime, and a set of components for configuring and deploying applications.
View it as the machine-readable version of the fifty-page file telling the operations team how to install and configure the database and application servers, then deploy the application on top.
We want to run an Ant target that deploys our application across our multiple test servers, then later onto the production systems, as shown in figure 16.1
Figure 16.1 The goal for our distribution: Ant telling SmartFrog to deploy our entire system, including setting up the application server and the database.
Lots of tools can configure classes, such as Spring, Plexus, and HiveMind.
The goal of these tools is to make it easier to build an application by connecting together smaller components.
SmartFrog is more ambitious than these, because it tries to build a complex distributed application by connecting together code, classes, and programs across machines.
Configure intimate details of deployed applications, such as the handlers of a SOAP stack, the mime types of a web application, or its Log4J policies.
Run JUnit tests on multiple host machines, aggregating the results.
Start VMWare or Xen images, then deploy other things onto these images.
How can it do all this? Primarily because it’s nothing but a large distributed system itself, one that can configure, start, and stop components on the machines.
Everything else—the things that get started, the sequence of how they are started and stopped—is all implemented in SmartFrog components.
The fundamental concept behind SmartFrog The core concept behind SmartFrog is fairly profound: deployment is configuration.
The hard part of deployment is configuring everything to work together.
The SmartFrog solution to this mirrors the problem: configuration is deployment.
Instead of trying to remember what to do to bring up a complex system, or have a fifty-page document listing the actions to take, operations and developers create a description of the installation process that the computer can itself understand.
Something that people write, but which a program can take and turn into a functional system, deployed over the target hosts.
The underpinning of SmartFrog is effectively a domain-specific language for modeling complex systems.
Each model describes what is to be deployed, and how it is to be configured.
It’s a simple language, not much more than nested templates of namevalue pairs.
Firstly, all templates can be inherited and overwritten, so you do not need to repeat definitions.
Base templates can be taken, extended, and used to describe specific installations on specific systems.
If two parts of a deployment description take the same value, such as the port of a web server, then that value is not.
A language that describes complex configurations is useless unless you have a way.
It builds up a graph of nested templates, expands all inherited templates, resolves the references and produces an expanded, resolved graph of name-value pairs.
Every SmartFrog component represents something that can be configured and started.
Running components can be pinged to see if they are healthy and, eventually, stopped.
The actual sequencing of starting and stopping components is handled by the container components in which the child components are declared.
These components are just Java classes with an RMI interface for remote access.
Simple components implement their functionality inside the class, just as Ant tasks do.
More complex components bind to native applications, such as the Apache HTTPD web server.
To summarize: SmartFrog is a language for describing configurations, a runtime to instantiate those configurations, and an extensible set of components that can be deployed and undeployed.
Now let’s look a bit deeper, focusing on how to use SmartFrog for deploying our web application and database.
For our deployment, we need to get the MySQL JDBC driver into a directory in the application server where it can be used to set up the data source.
Inserting the JAR inside a WAR or EAR file doesn’t work, because it isn’t visible to the right classloader.
A manual deployment document, would say something like “download the MySQL JDBC driver version 5.0.4, verify its checksum, and copy it into the lib/ subdirectory of the chosen JBoss configuration.” We need a SmartFrog descriptor that does the same thing, somehow marking the destination directory as something that will be defined later.
The SmartFrog language SmartFrog comes with a new language—it isn’t XML.
The language’s author, Patrick Goldsack doesn’t think XML documents are easily writeable or, more importantly, readable by humans, and sometimes you have to agree with him.
Instead, a new language has been written, with the JavaCC tool used to create the parser.1
It consists of templates, surrounded by curly-braces and name-value pairs inside the templates.
Listing 16.1 shows the descriptor to install the MySQL JDBC driver.
Puppet, a Ruby-based configuration management tool, has a very similar non-XML syntax.
Listing 16.1 A SmartFrog file to fetch and deploy the MySQL JDBC driver.
This template solves one tiny part of the bigger deployment and can be reused wherever we need to install the MySQL JDBC drivers.
Putting aside how it all works for a moment, the language has some interesting features.
Where did the templates come from? They came from the files named in the #include statements at the beginning of the document.
These are all resources in JAR files on the classpath.
Component developers provide the initial templates with the implementations of the components, so that component users can import them and start using them immediately.
The next point to note is that although the names of things appear to be simple, values can be more complex.
There are some primitive types—string, integers, and Boolean values among them—but there are a few that are different, like LAZY PARENT:repo.
That is a reference, a link to the attribute repo of the parent component.
It’s marked as LAZY, meaning this reference should be dynamically resolved at runtime.
Non-lazy references are evaluated earlier on, before the runtime starts bringing up the components.
This refers to the version attribute of the jdbcJAR template in the InstallDrivers template.
The other trick is that when components deploy, they can add new attributes.
A LAZY reference to an attribute can refer to something that isn’t defined in any descriptor, but is picked up at deployment time.
This can be used for things such as setting the absolute path of a created file or the JDBC URL to a running database.
This means that the destDir attribute is To Be Defined.
That value must be set in the template before deployment begins, but this particular template doesn’t know what the value will be.
It means the template can be deployed only as part of something bigger.
The power  of composition The deployment descriptor that sets up the database driver describes one small, albeit critical, part of a big deployment.
Just as a big program is built from small classes, a big deployment is created from small deployment descriptors, each of which solves a specific problem.
Listing 16.2 shows the deployment descriptor that deploys the application server, the database, the web application, and a component that checks the resulting site’s “happy page.”
It configures the database, copies the JDBC JAR to the application server’s library directory, and shows the web application where to go.
It also has a monitoring component, which, after a sixtysecond delay, starts checking the health of the application.
This is the secret of making big things manageable, by inheriting templates, then expanding or overriding them.
We just need to fill in those attributes that aren’t yet there, those marked as TBD, or the deployment will not even begin.
The SmartFrog runtime The SmartFrog runtime takes a description and turns it into deployed components.
Here is one that fills in all the undefined attributes with data that’s valid for a target Windows system.
This description can be deployed on any Windows system that has JBoss, the JDK, and the diary application in the configured locations.
This single-host configuration can be kept under revision control, with descriptors for different hosts and platforms alongside it.
Bind the configuration of JBoss to the parent component’s settings.
Every host that can be a target for deployment must run a copy of the SmartFrog daemon.
This is the core SmartFrog runtime running as a service or Unix daemon.
It listens on the network port 3800 for requests to deploy a tree of SmartFrog templates and an application, turning them in to deployed components.
If part of the graph needs to be hosted on another machine, then it hands off that part of the graph to the remote system.
There is no central controller of the machinesjust a set of hosts that trust each other.
It does have its limitations, primarily versioning, long-distance communications, and the problem of distributed garbage collection.
After reading in the descriptor, the SmartFrog runtime creates a distributed graph of things to deploy.
By default, every component is deployed in the same host and JVM as its parent.
To deploy our multi-tier application across a cluster of machines, we would need a new descriptor, one that declares new locations for some components, simply by declaring target hosts inside the descriptor:
This descriptor uses the sfProcessHost attribute to tell the runtime where to host the different parts of the template graph and, later, the components.
The application is deployed on the host “pelvoux” b, as will all children except for the database c and the happy page d, which are hosted on “eiger.” The home directory is picked up from the Java property user.home on the remote system e.
That gives us a twotier server and, by hosting the happy page remotely, checks that the web site’s firewall is open.
The description is taken and turned into a graph of component descriptions across machines.
The way the sfProcessHost attribute of the database and happy templates were set shows another trick of the SmartFrog language.
Deployment descriptors can inject data into templates, by giving the full path to the attribute, here database: sfProcessHost.
This is useful for putting last-minute information in, although it’s a bit like using Ant properties to configure an existing target.
It’s useful, but does make the deployment more brittle, as you effectively code in more assumptions about the structure of the template.
Once deployed, the components will run on two different machines.
Even though they are now distributed, the graph of components is still intact; components can navigate around and locate their parent or other components.
This allows components to share runtime information, by adding new attributes or changing existing ones.
They can also talk directly to each other using the Java RMI protocol.
Having a single graph of components across multiple servers lets the developers and their build files have control of the complete deployment.
Our build file can start or stop the application in one go.
Once the deployment graph has been built up, the next step is to instantiate the components.
The runtime instantiates the top node in the graph, here the HomeLinuxCluster node, and starts to deploy it.
This is quite a complex process, as the SmartFrog runtime tries to move all components through their lifecycle according to the policy implemented by their parent components; deployment is choreographed.
Figure 16.3 shows the lifecycle of a single component; child components follow the same lifecycle by default.
Figure 16.2 The SmartFrog daemons build up a distributed graph of things to deploy.
The lifecycle of an application, a hierarchical tree of components, is shown in table 16.2:
Table 16.2 Lifecycle actions that SmartFrog components support, and how they react.
The graph is constructed The graph of component descriptions is created, with each template on its chosen host.
The Java class implementing the root component is created and bound to the graph.
The root component is deployed The class’s sfDeploy() method is called; attributes can be read or written; children can be instantiated and deployed.
The root component is started The class’s sfStart() method is called to start actual work, such as the database or the application server.
The root component is terminated The component stops what it’s doing, unexports its RMI interface, and cleans up.
A component is pinged The class’s sfPing() method is called.
It should check its health and throw an exception if it’s unhappy.
A component is destroyed Once all references to a component are lost, the JVM may garbage-collect the class behind it.
A component fails If the component fails to deploy, start, or respond to a ping request, it is considered to have failed.
Ant tasks: they are both Java classes that you configure through text files; they merely know more about deploying and cleaning up afterwards than Ant tasks.
The biggest difference is that by default, parent nodes deploy all their children in parallel—Ant executes tasks in sequence, except inside the <parallel> container task.
The component lifecycle in action When our HomeLinuxCluster application is deployed, the SmartFrog runtime instantiates, deploys, and then starts the components.
Some components, such as the ones to download and copy the MySQL driver, are contained entirely in the implementation classes.
Others, like the JBoss and MySQL support components, are separate processes—Java and native, respectively.
The happy component is special in that it only deploys its action child after a sixty-second pause; it’s an instance of the Delay component, which manages its child’s lifecycle according to its own policy.
The application server is running, and the diary WAR file has been deployed to it by copying into the deploy directory of the.
The happy component’s action fetches the happy.jsp page every time its parent pings it.
While the system is running, the root runtime will ping the HomeLinuxCluster.
The server will be healthy if JBoss is still running; the same for the remote database.
The happy component considers itself happy if it can retrieve the happy.jsp JSP page from the web site.
If it gets an error code or cannot connect, it reports an error to its parent.
The result is that it becomes impossible to deploy an unhappy application.
Just as Ant has built-in tasks and third-party tasks and Antlibs, SmartFrog has components, which come in JARs that contain the template .sf files, RMI interface and proxy classes, and the implementation classes.
The daemons can download classes and SmartFrog descriptors using Java’s remote classloading facility.
A list of URLs needs to be supplied the daemons on startup, or they can be set in a deployment descriptor.
Anubis A fault-tolerant tuple space for building a high-availability system out of a cluster of machines.
Filesystem Files, directories, temporary files and directories, self-deleting files, copy, touch, mkdir, and the like.
SmartDomains CERN components to deploy Xen virtual machines; SourceForge hosted.
We also have experimented with the testing framework, which can integrate testing with deployment.
All of these packages come as separate JARs, which need to be on the classpath of the daemons, either on startup or through dynamic downloads.
Running SmartFrog For operations, the SmartFrog daemon can be started on the command line, or as a service when the machine boots.
For developers’ use, there’s a broad set of Ant tasks that can deploy and undeploy applications, start and stop the daemons, and even bring up a management window to view the system and its log.
There are also Eclipse and NetBeans plugins for in-IDE work.
Let’s use it to integrate deployment of local and clustered applications into the build.
There you can find the latest releases, documentation, and links to the source code repository.
When you are starting off with the tool, you will need.
The core comes with documentation and a set of Ant tasks for calling SmartFrog from Ant.
Follow the instructions to install it and add its bin directory to the path if you want the command-line tools.
If it isn’t included in the distribution, download the sf-www JAR file containing web application support and add it to SFHOME/lib, where SFHOME is the SmartFrog directory.
Be aware that the tool is released under the Lesser GPL (LGPL) license.
This has implications if you were to redistribute modified versions of the JAR files to third parties, but not if you only use the tool internally.
The Ant tasks Table 16.4 lists the tool’s Ant tasks.
The rest are for starting and stopping daemons and for deploying, pinging, and undeploying applications.
To use them we need the SmartFrog JARs, including the sf-tasks JAR on the task’s classpath.
This target depends on the tests-packaged target and adds the test classes as a JAR.
This is in case we want to deploy a component that runs JUnit tests, integrating testing with the deployment.
With this classpath set up, we can declare the tasks:
The sf-tasks JAR declares the tasks in an antlib.xml file under the URI antlib:
All tasks also support a nested <classpath> element for adding extra libraries to the classpath—but it’s easier to declare the tasks with everything needed on the classpath.
Although we’ve shown some examples of them, the full language and how to write deployment descriptors and components are beyond the scope of this book.
We’ll just deploy the descriptor to start the web application and JBoss, and deploy it from Ant.
First, we need to declare the local configuration, which we do in listing 16.3
This descriptor takes the abstract system definition b and fills in the system-specific values c.
Finally, it declares the sfConfig template that names what is to be deployed d.
This name is special; it and everything under it gets deployed.
In the build file, we need to identify this file and the host for the deployment:
These properties deploy to the local host, with a descriptor explicitly bound to a specific machine.
Other systems will have to point to their own descriptor via a.
There are currently versions for Windows, Linux standalone, and a Linux cluster, all of which extend the same base configuration and change a few options.
General purpose .sf files contain configuration details for a specific part of the big system, such as JBoss, the database, and the MySQL drivers.
In a separate directory, we’ve added system-specific configurations that hard-code in the locations of MySQL and JBoss on different platforms.
Individual developers can add their own custom .sf files in this target’s directory, targeting specific machines.
All of these configuration files need to be packaged into a JAR file, so that the operations can be given a JAR file they can sign and use in a secure SmartFrog environment—one in which the daemons  only load classes and .sf files from signed JAR files.
For now, we just package the JAR and skip the signing:
Every descriptor is a resource in a Java package but is built into a separate JAR from the application.
That lets us change this JAR and sign it without having to rebuild the application itself.
We can deploy them or even check them for validity in advance of a deployment.
Preflight validation We use the term preflight checks for the static validation of the files; it’s a term derived from airplane safety checks:
The <parse> task parses the deployment descriptor b and checks that all included files can be found and are syntactically correct.
It checks that all attributes marked as TBD have been overwritten with valid data, and that there are no unresolved non-lazy references.
Many components have an optional “schema” declaration that lists the type of some attributes and whether they are compulsory or optional.
This is one disadvantage of runtime binding—you find out what’s missing only when you deploy.
Preflight checks are invaluable while working on the deployment descriptors themselves, after which point they mostly become a few seconds of delay.
If the property skip.preflight is set, the check is skipped; developers can set this property once the descriptor is stable.
Starting the daemon To actually deploy the application, we need a running daemon on the target host.
One of the tasks, <run>, does a deployment in its own process, blocking Ant until it’s finished.
This task is easier to integrate with an Ant workflow, but it doesn’t give you the option of deploying to a remote host.
This makes it less useful than the <deploy> task, which can deploy to the host specified in the host attribute, once a daemon is running on the target system.
A SmartFrog daemon can be started on the command line by running the sfDaemon batch file/shell script:
At this point, the GUI opens up and displays a log window.
A menu action will bring up the management console, which shows all deployed components and lets developers view their attributes or terminate all or part of a deployed application.
The <startdaemon> task can start the daemon from inside Ant.
It doesn’t bring up the logging window, so it’s useful for unattended operations.
The <propertyfile> element identifies a file of properties, here marked with optional="true" to indicate that it doesn’t have to be present c.
The PROPERTY and IPROPERTY references resolve string or integer properties from either the process starting the deployment or, if LAZY, the running daemon itself:
This property binding can pass extra information down or pick up state information about the target host, such as the value of the java.io.tmpdir temporary directory property.
The <stopdaemon> task can shut down a local or remote daemon:
The failonerror attribute is set to false because it doesn’t normally matter if the daemon is already stopped.
The SmartFrog task suite includes a new task, <faultingwaitfor>, that fails the build if the nested condition isn’t met:
Clearly, we need to go back and run the daemon before deploying.
The task parses the descriptor and expands it completely, resolving all non-LAZY references.
It then connects to the target daemon and deploys the descriptor:
Once the expanded and resolved descriptor is submitted to the daemon, the task returns.
The deployment is still going on, as the components declared in it get moved through their lifecycle to the instantiated, initialized, and—finally—the running state.
Checking on the application We can check what is going on with the deployed application by starting the management console.
This brings up the application of figure 16.6 to view the deployed applications.
The console lets you view and manipulate the graph, terminating components or detaching parts of the tree from their parents, so that they can live a separate life.
The basic check of pinging an application to verify that it’s alive is so common that it can be run from Ant; there’s no need to use the console.
Pinging an application The <ping> task probes an application to see if it’s healthy:
Figure 16.6 The management console, showing the status of the deployed application.
The elements under the happy component have not yet been deployed, as it delays for a minute before starting happy page checks.
This message indicates that there was an unresolvable reference, diary, presumably because there was no active application called “diary.” A successful ping verifies that the application is present and healthy:
Being able to integrate health checks—such as the happy page checks—into the application’s deployment descriptor transforms the notion of what constitutes a valid deployment.
All the static and runtime configuration information from the deployment can feed straight into health checks, checks that can be performed by the framework.
All components that have termination routines will do their work.
Forked Java and native processes will be stopped, files and directories cleaned up, databases taken offline.
We’ve just used SmartFrog to deploy our database, application server, and application.
With the appropriate configuration files, it can deploy to the local system or to a nearby cluster.
SmartFrog is probably the most advanced distributed deployment framework available and, being open source, is free for anyone to use.
However, it does take a bit of time to become familiar with the tool, the language, and the error messages.
Using the tool, we can trigger deployment from our builds, either to a developer’s.
That not only enables us to automate deployment to the development systems, but to the production systems as well.
With automated deployment integrated into the build process, we can then move to a deployment-centric build-and-test cycle.
This chapter has looked at how to deploy successfully, by automating deployment and integrating it into the build cycle.
But just adding new development tools isn’t enough; the process must adapt.
Just as adding unit testing to a build file added testing to our edit-and-build cycle, adding deployment can change how we deliver software.
When software is handed off to an operations team, the classic “waterfall” development process starts sneaking in, as figure 16.7 shows.
Figure 16.7 The waterfall process is inflexible and widely discredited.
But look how an iterative development cycle can revert to a waterfall at the final stage.
The result can be a hard-to-deploy application that doesn’t suit the needs of the operations team.
What can be done? How about integrating deployment with testing and the continuous integration process? The goal is to produce a development lifecycle more like figure 16.8, in which deployment is part of the normal develop-and-test loop.
In this lifecycle, the operations team owns the test hosts.
Developers deploy to those machines, running their tests on or against them.
This ensures that operations get the experience of deploying and managing the application, while developers can write code that works on the production systems.
There’s a small logistical problem here: how to integrate deployment with a test run.
A list of JUnit test case classes in the named package.
This component can be deployed under a TestRunner component, which can run.
The HTML test listener creates XHTML pages as the results come in: there’s no need to wait for the whole run to finish before the results are created.
Admittedly, the pages are not very pretty—there’s clearly scope for improving the presentation and analysis of the test results collected from tens or hundreds of systems.
But it’s a start and, being live, it’s convenient on big test runs.
Developers will have to work with operations earlier than normal; operations have to start supporting many more systems earlier on.
This is exactly why you need to do it: the two teams need to work together right from the outset of a project!
Continuous deployment With deployment automated and testing integrated with the deployment, then one more action becomes possible.
The team’s continuous integration server can redeploy the application and its functional tests.
It could even be set up to redeploy to production on a nightly basis if the functional tests pass, of course!
If you intend to explore this possibility, here’s some advice:
Restrict the update frequency to a limited rate, such as once per day.
If your customers think you can roll out a patch in half an hour, they will demand all bugs fixed within an hour.
Having a delay built in to the process reduces stress and encourages better fixes.
This whole notion of continuous deployment goes hand in hand with the “everlasting beta” phenomenon of web site development, in which the site is never stable and new features appear daily.
It goes even better with web service development, in which you are trying to develop a public API for use by external callers.
Just as agile development encourages developers to work with the customers, agile web site and service development requires the team to integrate with operations, and to rapidly evolve the live application based on the experience of real users.
Integrating deployment into the continuous integration process makes agile development possible, while exposing a web site or service to outside callers.
We’ve explored the challenges of deploying to production servers and have proposed some ways to survive that phase of a project:
Ant’s <sql> task can set up a database setup, while SmartFrog can completely automate the deployment of complex applications.
SmartFrog can transform how developers and operations teams deploy things, and it lets you integrate deployment into the build cycle.
We’ve closed with the idea of continuous deployment—having the continuous integration server handle deployment to the staging and perhaps even the production system.
This is one of the many leading-edge techniques you can explore once you have deployment under control.
And that’s it! We’ve reached the end of the part of the book that covers applying Ant.
We started the section with a build file for a single library, and we’ve gone through third-party tasks and Antlibs.
We’ve looked at large projects with multiplebuild files, using Ivy for dependency management, and we’ve applied these techniques to build a web application and then an Enterprise Java application.
Then we’ve shown how to use a continuous integration server to build and test everything, before getting into the world of automated deployment.
The remaining two chapters go low-level into Ant’s own source code to look at how to extend Ant with new tasks, datatypes, and resources.
We first cover writing custom Ant tasks and the essentials of Ant’s API.
Then, we explore scripting inside Ant build files and, finally, creating your own datatypes.
This section enables you to extend Ant to meet the specific needs of your projects and to provide redistributable tasks for your application.
It may be that something major is absent, like having no way to run a custom test framework.
It may even be that a common Ant task doesn’t work quite right.
Ant can be extended through Java classes, and it takes only a small amount of Java coding to write a new Ant task.
If the problem lies in the actual Ant source itself, then the fact that an entire Ant source tree is a download away comes into play.
If Ant doesn’t work right, then it can be fixed.
This chapter will show you how to write tasks in Java.
When used by other projects, it becomes a third-party task, and if it becomes part of Ant’s own source tree, it becomes a built-in task.
We’ve been using the latter two task types throughout the book, but now it’s time to write our own, custom tasks.
Add methods that Ant can use to set attributes and add elements.
Here’s a first task, one that prints out a message:
In a build file that compiles the task, we can use the task after it’s been compiled by declaring it just as we do for third-party tasks:
This task is similar to <echo> and does nothing but log the value assigned to the message attribute.
Ant saw that the build file set the message attribute and matched that to the setMessage(String) method on the task.
Before the task saw the attribute, Ant had already expanded the properties.
This is part of Ant’s API for tasks: a method in the parent class to log text.
This little task, then, is the foundation of all custom Ant tasks.
It has an attribute that Ant supports through introspection and an execute()method that does some work through Ant’s APIs.
In this chapter we’re going explore that feature creep—Ant’s task model and API.
We’ll show the reader how to write and test Ant tasks.
Authors of existing Ant tasks should read this chapter, because the testing mechanism and resource API is neweveryone will learn something!
Let’s start by looking at how Ant configures and runs a task.
Ant’s mapping from XML task declarations to Java classes is a miracle of data binding.
We will soon show you how attributes and elements are mapped to Java methods, but before that comes the task lifecycle.
There are different stages in the processing of a build file, and the objects that implement tasks are used throughout the stages.
When Ant parses the build file, it creates an instance of the task's implementation class for every declaration of the task in the file.
Ant then calls methods on the task to tell it about its hosting project and target, as well as which line of the build file contains it.
Ant configures it with the attribute and element values in the build file, using reflection.
If any method called throws a BuildException, the task has failed.
Instantiated tasks remain around after they’re executed; they may be executed.
However, such a task will not have access to most of Ant’s API, so it is of limited use.
Ant’s API is the real Ant—the language behind the XML and something tasks make full use of—hence something task developers need to know.
You don’t need to understand all of the classes and structures that make up the Ant codebase, but several key classes are worth knowing about, as they crop up frequently.
With Ant’s source code being open, it’s easy to learn Ant’s secrets by looking at the source code and its JavaDocs.
A Project hosts Target and Task instances; targets (and any other TaskContainer) can also contain tasks.
Task classes are themselves extensions of the abstract ProjectComponent class, which is an indirect parent of everything from datatypes to conditions.
There also are a few utility classes that crop up repeatedly, BuildException being one of them.
Let’s take a quick look at some of the main methods in the core classes.
Consult the JavaDocs and the source for the full details.
With a project reference, you can run targets, read and write Ant properties, or even add new targets and tasks.
A project routes all I/O in the build, from logged messages to System.out, System.in, and System.err.
It’s here that listeners, loggers, and input handlers integrate with Ant.
The project holds the graph of targets and tasks; there are operations to manipulate this graph.
Other methods add or retrieve references, create tasks and types, and log output messages.
All Ant datatypes given an id attribute are registered by the Ant runtime, so they can be resolved in a task.
Many other things internal to Ant are stored as references.
It’s the best way to add the private state to a project.
If you add new private references to a project, pick names that are sufficiently obscure that nobody uses them for datatype references.
It’s how Ant creates separate classloaders for different tasks and programs run within Ant.
Task createTask(String name) This method creates a task with the given name.
For example, a Java task instance could be created in an operation such as.
Ant itself had to undergo a major cleanup once this problem was noticed.
Current code just creates an instance and then configures it.
The bindToOwner() call copies all binding information from the current task.
If you forget to do this, the created task will probably throw an exception.
This call creates a datatype of the given name, such as a <fileset>:
Again, the warnings of the createTask() method apply; this method is rarely used in Ant’s own source.
Instead, a new FileSet() operation is followed by the method called to bind the created class to the project instance.
Hashtable getProperties() This call returns a copy of the current set of properties.
Altering these properties doesn’t affect the property values in the project.
String getProperty(String name) The getProperty method returns the value of an Ant property, or null if it isn’t defined.
Ant automatically expands properties in attributes before handing the value to the task, so this method is rarely needed.
Tasks that implement if= and unless= tests for properties use this method.
This method can be used to patch up any text between the tags of an XML element as, unlike attributes, that text is not automatically expanded.
Classes that know which project they belong to, but not which task they are working with, sometimes use this method.
Immutability is enforced here: the property will not be changed if it already exists.
The Target class implements the TaskContainer interface, telling Ant that it supports tasks nested inside.
Targets contain all the information they were given in the XML file—name, depends, description, if, unless—and a list of nested ProjectComponent instances.
They have a link back to the owning project, their location in the build file, and a list of all the targets that they depend on.
Custom tasks rarely need to make calls on their containing target; they cannot even rely on there being one.
ProjectComponent ProjectComponent is an abstract base class for anything that can be declared inside a build file.
Two direct descendants are the Task and Datatype classes, as are conditions.
Project getProject() This call returns the project in which the component is executing.
This will never be null unless whoever created the task forgot to bind it to the project.
There are five logging levels, listed in descending priority in table 17.1
A BuildLogger is capable of filtering the output based on the logging level selected.
Normally, tasks print errors and warnings at the relevant levels, diagnostics for build file authors at the verbose level, and internal task debugging information at the lower debug level.
Tasks should normally log liberally, especially at the verbose level, showing the details of what’s going on.
The MSG_INFO level shouldn’t be over-used, as it’s seen by default.
The void setProject(Project project) method binds a ProjectComponent to a Project instance.
Table 17.1 The mapping from API log levels to build file output levels.
Whenever a task creates anything that is a subclass of ProjectComponent, it.
Task The abstract Task class is the base class for Ant tasks and the main unit of work during an Ant build.
Most custom tasks extend this class and, at a minimum, implement their own execute method.
Being an extension of ProjectComponent, the getProject() and log() methods are there to provide access to the owning Project and to log messages.
When a task creates a new task to delegate some work, it should call the bindToOwner(Task owner) method to propagate shared information—the hosting project, the task name and location, and other things.
The robust way to create a task is as follows:
After the init() call, you can proceed to configure it and then finally call execute()
There’s a full example of this delegation in work in section 17.8
At the point this method is called, Ant will have passed down to the task information all attributes and nested content.
The execute method should validate the task’s configuration, then do the requested work.
Location getLocation() With this call, Ant returns the location of this task—the build file and the task’s position within it.
Target getOwningTarget() This is a method that returns the Target that contains this task.
If it’s null, then the task was not declared inside any target: you cannot rely on tasks having an owning target.
Tasks that need to do early initialization can override it.
Why do that instead of doing the work in the constructor? Because at the time the init() method is called, the task has been bound to its owning project.
The task can safely do work that involves a Project instance.
This is probably the most ubiquitous of all Ant classes, and it’s used to signal any kind of problem.
The class is a RuntimeException, which means that it doesn’t need to be declared on methods.
It’s the underpinning of Ant’s failure-handling, such as in the following attribute setter:
The class has a Location attribute that’s set to the location in the build file that’s at fault.
Ant contains a set of utility classes that provide common operations for task authors to use.
It is much better to use these library classes instead of reimplementing their code, as they are stable, well-tested, and work well across different Java versions.
ClasspathUtils This can create a classpath and load classes from it.
Tasks that dynamically load new classes from user-supplied classpaths need this.
DateUtils This class simplifies working with dates, with methods to parse ISO8601 dates and date-time strings down to java.util.Date instances.
JavaEnvUtils This useful class contains operations to work with the JRE and JDK, including determining JRE version and executing programs in the JRE.
The FileUtils class is one of the most commonly used utility classes.
It contains filesystem support that adapts itself for different platforms, with operations to copy files, resolve filenames, split paths, and convert between filenames and URLs.
This method appears everywhere Ant methods clean up open files, in the finally clause.
File resolveFile(File file, String filename) This invaluable method resolves and normalizes a file path relative to another File if the filename isn’t already an absolute path.
StringUtils This class contains some minor String and StringBuffer operations, most importantly one to split up a String into a Vector of elements based on a separator.
To break up a String by lines, use "\n" as the separator.
ResourceUtils This is a utility class providing operations that work on resources, rather than just files.
There are operations to compare resources for text or binary equality, contentEquals() and compareContent(), and others, including copyResource() to copy resources.
Tasks can use this method to select resources for processing, e.g., copying, compiling, or some other action that brings the destination up-to-date.
How about a task that sets a property to the size of the file or files passed in? This task—let’s call it <filesize>—will have two attributes:
Later, in section 17.6, we’ll extend the task to take multiple files as nested elements.
There is no init() for initialization; all the work is done in the execute()method, which first checks that it’s configured right and that the file exists, and then sets a property to the size of the file.
If any of the preconditions aren’t met, Ant throws a BuildException.
Property The name of a property in which to store the result Yes.
File The file to analyze Yes, and the file must exist.
Listing 17.1 A task to set a property to the size of a file.
This is a fully functional task, with validation and logging in only thirty-five lines.
How are they set? How is this going to work? What’s going on?
The answer is “magic,” or code complex enough that it may as well be.
Ant is automatically translating strings in XML attributes to strongly typed Java classes.
It’s calling the bean-style setters on the class, then the execute() method—all through Java reflection!
The compile target compiles the task b, and the define target defines it in its own namespace c.
That target doesn’t depend on the compile stage, because we want to be able to define the task without triggering a rebuild; this will be useful when testing.
However, the filesize target does depend on the compilation stage, so when it’s run it should execute the new task.
Running the task The filesize target builds, defines, then calls our new task without any parameters.
What’s going to happen? There’s one way to find out:
Of course not: the task wasn’t configured because we haven’t gotten Ant to do that yet!
Tasks are described in a build file as XML data.
When Ant encounters a new XML element, it looks up its list of types and creates an instance of the Java class registered for that element.
The hard part is configuring that instance, passing the XML attributes, nested elements, and body text to the task.
Ant handles this using reflection, mapping the XML values to Java types, and then passing them to the instance that it’s configuring.
Ant scans the class for specially named methods and invokes them with the data.
Ignoring XML namespaces, an XML attribute consists of a name and a string value.
Ant takes the value of each attribute and expands all properties with Project.
It searches for all methods with the name of the attribute prefixed by “set”
For our <filesize> task, the property attribute is mapped to the setProperty method:
A String parameter is the most straightforward attribute type since it can be set directly to the text:
A String type is only the first of many types that can be used, though.
Most of the main Java types can be passed in.
Boolean values Many times a task simply needs to have a true/false option.
Because of implicit attribute expansion, our task doesn’t know the difference when the build file writer sets the attribute using a property:
The setFailonerror method is invoked with true in both cases.
Accepting numbers Ant supports attributes that accept all the Java primitives numbers and their Object equivalents:
The character passed in will be the first character of the attribute value; Ant will ignore any additional characters.
Files or directories Many tasks take a file or directory as an attribute.
If a task implements a setter with a java.io.File parameter, Ant will configure the task with File instances.
Ant converts the path string in the XML file to the local format, resolves any relative path against the project’s base directory, then calls the task’s setter method with a File object representing the path.
For <filesize>, the setFile() method means that the task has a file attribute:
The task must do its own validation if it wants to restrict the argument to only a directory or only a normal file.
It’s already got the setters it needs—we just need to pass the information down as attributes:
Run this and we see the result we’ve been waiting for:
It has attributes set by Ant, and when it runs, it sets a property in the project.
A bit of packaging and it will be ready for redistribution.
All of the Java code in this book comes with tests.
Should Ant tasks be any different? Of course not! But how do you test an Ant task?
Each test method would run different targets in the same build file, then check the state of the project and its captured log afterwards.
There is an Ant task to test Ant tasks: AntUnit.
AntUnit is an Antlib, a library of extra tasks with the filename ant-antunit.
It contains the Java tasks and macros to turn build files into test suites.
The Antlib includes a set of assertion tasks to check the state of the project in the test.
To use AntUnit in a build file, you need to declare the Antlib namespace in the project or in targets that use the new tasks:
Table 17.2 The Ant tasks and types that come with AntUnit, excluding the assertions.
This will test all test targets in the file test.xml, printing the results to the console.
These test targets can verify that something worked or, with <expectfailure>, that something failed.
The <expectfailure> task wraps a sequence of Ant declarations and executes them in turn.
If the sequence completes without throwing a BuildException, the <expectfailure> task throws an exception: the sequence was meant to fail.
If an exception is thrown, it is caught and its message verified against the value of the expectedMessage attribute:
This condition works only under an AntUnit-controlled test, when a special logger saves all the output to a buffer.
This assertion is one of the many provided by the toolkit—table 17.3
All of them can be used in test targets to check that the Ant task is behaving as expected.
These assertions are used to verify that the tasks being tested have the desired effect.
It’s time to add tests to the build by running the <antunit> against our own build file:
Calling this target will trigger a run against the test targets in the file and the optional setUp and tearDown targets.
The antunit target is set to depend upon the ready-to-run target, which ensures that the source is compiled before the tests run.
If they did, every test target would also have the complete compile step re-executed, because when AntUnit runs a target, it runs all its dependencies.
Instead, the test targets depend on the define target, which declares the custom task with <taskdef>
The first test target runs the task with valid arguments and asserts that the property is set:
We want the task to fail in the test, so we use <expectfailure> to run the task and look for the exception message it should raise:
The final test is one that asks for the size of a nonexistent file.
The expectedMessage attribute lists only part of the fault string; the full message contains the path of the absent file, which isn’t a constant.
The <expectfailure> task does a substring search on the message, so only a portion of the error message is needed.
The tests passed, the report is printed, and the build succeeds.
What if the opposite had happened, and a test had failed? To see that, we need a test that fails.
How about a test that expects <filesize> to fail if it’s passed a directory?
Will this work or not? Without an explicit test for directories, it must depend on what happens when File.length() is invoked on a directory.
Debugging a task How do you debug a task? It’s no harder than debugging other Java programs.
You need to set the IDE up to run Ant under the debugger, setting up the command line with the build file and target needed to invoke the task.
The classpath for debugging must include Ant, the tasks being debugged, and.
When we run Ant, it will load the build file and create the task, halting at any breakpoints set in the task’s source.
Debugging the task is then no harder—or easier—than debugging any other Java program.
The limitations of AntUnit AntUnit is new and still acquiring all the features you expect from a test framework.
The biggest limitation is that the test builds are all executed in the same JVM—that of the project calling <antunit>
If there’s any state stored in static variables, it’s accessible to all tasks in all the tests.
One build file could contaminate all the other tests, and that would be hard to fix.
Having built and tested the task, let’s return to the details of how Ant configures tasks.
There are two more attribute options we can work with: enumerated attributes and user-defined types.
Many tasks restrict attributes to a limited set of values—an enumeration.
Doing so provides a basic enumeration that Ant tasks and types can use.
This enum is passed to the task in an attribute-setter method.
The Ant runtime finds the enumeration instance whose name matches the attribute value, passing that instance to the task.
Listing 17.3 shows a task that uses this Day enumeration, printing out its name and index value when executed.
Listing 17.3 Java enumerations can be used as task attributes.
Using Day with the example task, we can pass down an in-range value:
Ant will throw an exception if the attribute doesn’t match a case-sensitive comparison with any of the allowed values.
The AntUnit tests for the two enumeration examples show AntUnit at work.
Tasks that log information at the verbose and debug levels are easy to test.
Java enum types are the best way to restrict the value of an attribute to a fixed range.
Ant validates and converts the string, so the task developers can work with an enum that can be used in switch statements and the like.
If the enumerated or the built-in attribute mappings don’t cut it, there is one last option: a custom, user-defined type.
Any Java class that has a public String constructor can be used as an attribute.
This task takes a very large integer and prints it as an engineering value; any exponent is rounded to the nearest multiple of three.
Any Java class that takes a string constructor is valid, so many classes in the Java libraries can be used as Ant attributes.
Like attributes, Ant invokes specially named task methods when it encounters nested elements.
There are four distinct scenarios that Ant handles using the specially named methods, listed in table 17.4
We strongly recommend that you use the add, addXXX or addConfiguredXXX methods for nested task elements, because they allow you to support subclasses of the same types.
For example, a custom extension to the FileSet class could be passed to our <fileset> task.
The addConfigured method is useful if your task needs a fully populated object immediately, rather than waiting for the execute method; however, in practice it’s.
Table 17.4 Methods that tasks can implement to support nested elements.
Use the create- method in situations when you need to construct the object itself, perhaps because it doesn’t have a no-argument constructor or because additional steps are needed beyond what the add-prefixed methods provide.
Ant uses the type of the method’s single parameter to identify the supported datatype, while the method name determines the element’s name.
A method called addSource would take an element called source, independent of its type.
Tasks that support nested elements need to handle the case where more than one element is supplied.
This task adds them to a Union instance, which is the class behind the <union> resource:
This collection will now aggregate all resource collections passed down to the task.
When Ant executes the task, we’ll need to get those collections, or at least their inner resources, back.
All resource collections have an iterator() method that provides access to the nested resources.
When our task wants to retrieve the resources, it just asks for this iterator:
It can then iterate through the elements, measuring their size.
The <filesize> task measures the length of files, not strings or any other kind of resource.
How can we restrict the task to file resources only?
The solution is to check the type of every resource, only converting it to a FileResource if it is one.
A FileResource is a special resource that represents files, and it has a getFile() method to return the file it represents.
Ant constructs the object using a no-arg constructor; no pre-population.
Ant constructs the object using a no-arg constructor, but pre-population is needed.
This is all we need to support file resources, nested in arbitrary resource collection types.
Admittedly, most of those files are going to come in as <filesets>, which are the near-universal representation of files and file groups in Ant, but there are more.
By using resources, our task will be flexible and extensible, supporting new collection types from Ant and third parties.
There’s just one price: we need to know our way around the resource classes.
Ant resources are its emerging conceptual model for files, paths, URLs, and other data sources.
Older tasks invariably work with a few datatypes, such as the Path and FileSet types and perhaps the FileList—all of which are now resource collections.
Resource-enabled tasks can work with any of these collections as well as new ones.
To do so, task authors have to know how they are implemented in Java.
The resource class hierarchy is complex enough that we’re going to have to resort to UML diagrams—in plural—to describe them.
Consult the Ant JavaDocs and source to explore the resource class hierarchy in more detail.
Figure 17.2 outlines the Resource classes that ship with Ant.
The Resource class represents a source or destination of data.
Resources have a size, a lastModified timestamp, and a name.
There are public methods to access all of these values, methods that subclasses often override.
The method that all subclasses override is getInputStream(), which returns an InputStream reading in the contents of the resource.
These are implemented as classes that implement the ResourceCollection interface and its three methods:
Figure 17.2 The Resource class has many derivatives, any of which can be added to resource collections.
The Resource class itself implements the ResourceCollection interface, implying that all resources are sources of resources.
This is a little trick that makes it easy to treat single resources as single-element collections: the iterator() operator returns an iterator class and then returns the resource itself, once.
It’s complex because the ResourceCollection interface was retrofitted to the existing datatypes.
Most tasks don’t have to worry about all these details.
It shouldn’t matter to a task how resources are specified, aggregated, or sorted.
All the tasks need to worry about is the contents of the collections.
The methods and attributes have been stripped off as the diagram is complex enough already.
This task should add up the size of all Java source files.
Filesets have a base directory and, when evaluated, return only those files under the base directory that match the nested patterns.
This scanning of the directory tree takes place every time the iterator() method is called, so it should not be done lightly.
A filelist is a resource collection containing an ordered list of files.
Unlike a fileset, its iterator() returns the files in the list, whether or not they exist.
This is why our task has to handle missing files by throwing a BuildException.
Paths are also resource collections, so they too can be passed down:
Running this task tells us the total size of the JAR files on the classpath:
This chapter has shown how to write and test Ant tasks.
It has one big topic left to cover: how to delegate work to other tasks and Java programs, and how to set up the classpaths to do so.
Many of Ant’s tasks are just simple wrappers around Java or native programs.
Most of the SCM tasks simply set up the command line arguments for the native command-line tools.
There’s a lot of duplicate work going on here, so obviously some shared classes do the heavy lifting.
Every Ant task that’s implemented in Java can be created in another Ant task simply by calling its constructor.
It can then be configured by using its Java API—the setters and other methods designed for Ant to call by introspection.
To delegate work to other tasks, create the task, call init() to initialize it, configure it, and finally call execute()
We’re going to use the Java class to run any Java program with arguments built from nested resources.
This task, <runjava> will have the following attributes and elements:
The <runjava> task is going to expose only a fraction of the Java task’s features.
Some options will be set to sensible values, while the rest will be left alone.
This is pretty much the same code as for the <filesize> task: support for nested.
The harder bits are the other attributes, especially supporting class and classpath set-up.
Do not use this! Ant will try to load the class from Ant’s current classloader.
Tasks are much more flexible if they take the classname string and support configurable classpaths.
Classpaths can be specified by a reference to an existing classpath, a path attribute, or as a nested path.
After classes and classloaders are no longer needed, references to them should be set to null and the cleanup() method of the classloader should be called to force it to close any open JAR files.
Because we are delegating to the <java> task, only the first two items are our problem; the rest is the Java task’s.
Our task needs only a simple string to store the classname:
This does, however, leave the problem of setting up the classpath itself, which forces us to look at how Ant handles paths and datatype references.
There are three ways to let build files pass a classpath to a task:
Write an add-, addConfigured-, or create- method call that takes a Path parameter.
This results in a nested element that takes a full path datatype.
This creates an attribute that takes the reference ID of a predefined path.
By convention, the method should have “refid” or “ref” in its name.
The task has to create a path and bind the reference to it.
In such a situation, it’s best to treat the <path> elements equally to the other collections.
Our new <runjava> task will support all three methods: the classpath attribute, the classpath element, and a classpathref attribute.
A classpath attribute First, comes the field to store the path:
A classpath element The path element is no more complex.
Robust tasks may check for duplicate attempts to set the classpath and then fail on the second attempt, unlike this example, which will accept the last path given to it by the runtime:
Incidentally, Ant always adds elements in the order in which they appear in the build file; there’s no way to guarantee that the same behavior happens with attributes, as the XML parser can return them in any order.
This is consistent with XML’s rules, in which the order of attributes is unimportant but the order of elements is highly significant.
A classpath reference Supporting  references to predefined paths is essential for path reuse.
This method creates a new Path instance bound to the project, and then calls its setRefid() method to set its refid attribute.
Just as Ant’s tasks can be delegated to, all Ant datatypes can be created, configured, and fed into tasks.
The Path class The Path class is so common it deserves a closer look.
It’s an extension of the Union resource collection, with extra knowledge about Java classpath.
Some of the most interesting methods are listed in table 17.5
The Path class can be fed into other Ant tasks or used with utility classes such as ClasspathUtils, which creates an AntClassloader instance from a path instance.
It’s a resource collection, from which iterator() and add() are useful methods.
String[] list() Returns an array of path elements from the Path instance.
Resource[] listResources() Gets all path elements as an array of resources.
String toString() Returns the full path as a completely resolved and platform-specific string.
Provides an array of path elements from a single path containing elements separated by colon (:) or semicolon (;) separators.
To create a new task, just call the task’s constructor.
Some of the information—the project and owning target—is absolutely critical, as tasks always assume that getProject() never returns null.
Other attributes—name, location, description, and type—let the delegate inherit the public name and location of its owner.
In the build log, it won’t appear to be a separate task.
After copying this data, we call the task’s init() method to begin its lifecycle.
Once initialized, the task can be configured using the appropriate set-, create- and add- methods, mimicking in Java the attribute and element declarations in a build file.
In our <runjava> task, we set the fork and failonerror attributes to true, then pass down our classpath and classname.
This will run the Java program with the specified classpath and arguments, check the return value and report a failure if appropriate, and route all I/O to Ant’s own process.
Running the task As usual, some AntUnit tests verify that everything works.
This test creates a path b which we pass by reference to the <runjava> task c, a task set to run a ListArgs program.
Two resources are passed as arguments, a filename and a string.
What is the program? It’s the program we wrote back in chapter 2, the one that lists all of its arguments.
We’ve come full circle and are now running it in Ant via a task we’ve written ourselves:
The program can now be passed anything that fits into a resource collection.
It could be a path; it could be a fileset.
We’ve just extended Ant’s resource collection support to an external Java program.
If the task is bundled into a JAR, it can be distributed to anyone who needs it.
This is why custom tasks are so powerful: they can be reused by anyone.
There are only a few minor aspects of task coding to cover, a mixed bag of techniques that task authors should know of.
Error handling Ant will catch any exceptions thrown from a task and fail the build.
Tasks should throw a BuildException, which is a RuntimeException subclass, when they wish a build to fail for any reason—including from inside attribute or element setters.
The failOnError attribute is a common pattern in Ant tasks; if set to false it implements a simple “ignore any errors” policy.
Here’s a simple task that can be told to not fail:
Tasks that follow this pattern should enable failOnError by default, forcing a build file writer to explicitly turn it off, if desired.
This is consistent with the design of all new Ant tasks.
Swallowed exceptions should still be logged at the verbose level, because it’s very useful when tracking down obscure problems.
Handling inline text Many Ant tasks support plain text inside an XML element.
Adding an addText() method to your task tells Ant to allow nested text.
What has gone wrong? Well, for historical reasons (bad planning), Ant doesn’t expand the properties in the text before passing it to the addText() method.
You would not be able to use a property inside the message.
We need to explicitly expand the properties in this text, as in.
This is an easy mistake to make, because attribute setters do expand properties.
Some of the Ant tasks themselves (like <sql>) have made this mistake in the past.
Ant’s binding from XML to Java makes it easy to write a task or set of tasks.
Tasks are easier to use if they’re bundled up as a library, in a JAR file.
To make it even easier, the JAR file can contain property or XML files listing the types and tasks in the library.
The first step in building the task library is to compile the source into a JAR.
What is absolutely critical is the <copy> task that pulls in everything in the source tree with the suffixes .xml or .properties b:
Declaring a tasks.properties file The original way of declaring tasks was a properties file, by convention a file called tasks.properties in the same package as the tasks themselves.
This file lists the names of the tasks and the classes that implement them:
At this point, the new tasks are ready for use:
Task declaration through property files is a long-standing feature of Ant.
Although this format is easy to write, it’s limited, and the tasks aren’t automatically loaded when Ant encounters namespaces with antlib: URIs.
Declaring an antlib.xml file The successor to the properties file is a full-blown XML file, antlib.xml, listing the task and types in the library.
When an Antlib is loaded, all tasks inside it are executed inside the current project.
The result is that all the tasks defined in the library become available in the project.
It lets you define new Ant types and tasks in the same classloader.
It makes presetdef and macrodef declarations act as part of the library.
It allows scripting languages to be used to write tasks in the library.
It integrates with the Antlib: URI mechanism for automatic library loading.
For our task suite, the Antlib file is just a simple <taskdef> sequence:
The Antlib XML file is loaded by the same classloader as the task classes, so it isn’t needed.
The new Antlib can be loaded just like any of the Antlibs we’ve been using since.
This declaration loads the tasks straight into the main Ant namespace, ready for use.
The onerror attribute tells the task what to do if the resource file cannot be found: we want the build to halt, instead of display a warning message in the log.
Once the Antlib is loaded, the tasks are ready for use:
Most importantly, if the uri attribute is set to the antlib: URI of the package in which the antlib.xml file lives, the file is automatically located:
This is one step away from full dynamic loading, in which the namespace declaration alone is enough to pull in the tasks:
Running this target will force Ant to load the antlib.xml file from the classpath when an element in the namespace is encountered; failing with an “unknown task” error if the Antlib cannot be found.
You have to set up the build with the task on the classpath, which you can do in the build after the distribution JAR has been created:
Both tasks can be used in the antlib.xml file to define new tasks, tasks that then are available to any users of the Antlib.
What happens when the Antlib library is loaded into a namespace? How can declarations in the antlib.xml file predict which namespace they will be loaded into, so that they can correctly reference other things defined in the build file?
The problem is solved by having a special namespace URI to refer to the current Antlib, the string "ant.current"
It needs to be declared as a namespace b, and then the declared prefix can be used to identify tasks defined in the Antlib c.
It’s a bit of a hack, but it allows end users to declare the library into any namespace of their choice.
The main way to extend Ant is by writing new tasks.
This is normally done in Java, taking advantage of the full API that Ant provides.
With the information in this chapter, you now have an understanding of how to do this and how Ant binds to tasks.
The trick to writing a good task is to use the core classes such as Project and Task, to delegate work to other Ant tasks, and to accept the common datatypes built into Ant.
The AntUnit library makes it easy to write test targets for a new task.
That isn’t yet another XML format to learn, because it’s really a tiny subset of Ant itself.
We are now half-way through our exploration of Ant’s internals.
Now that we’ve covered writing Java tasks and Ant’s own classes, we can go on to write tasks in scripting languages and to match the custom tasks with custom datatypes.
Monitoring the build process with custom build listeners and loggers.
We’ve just been extending Ant with custom tasks in Java.
We’re going to go through all these extension mechanisms in turn to see what they are and how to write, test, and use them.
In the previous chapter, we looked at writing an Ant task and the Ant API.
Although we are extending Ant in different ways in this chapter, everything covered in the previous chapter about Ant’s Java API and how to test tasks is still essential.
The first thing we’re going to do is extend Ant via scripts—that is, small bits of code implemented in an interpreted language.
These code fragments can be placed inline in build files or hosted in text files alongside the build.
Almost all of Ant’s official extension mechanisms can be implemented in inline scripts, including tasks, conditions, filters, and more.
This means you can solve complex build problems without resorting to Java code.
Java is good for reusable tasks and datatypes, but it’s overkill for a one-off problem.
Ant uses the Apache Bean Scripting Framework (BSF) for its primary scripting support.
To use BSF scripting languages inside a build file, Ant needs three things:
Script authors have a broad choice of languages, the complete list of which is found on the BSF pages at http://jakarta.apache.org/bsf/projects.html.
Of all the languages, those that seem to work best with Ant are Jython, JRuby, and BeanShell.
Otherwise, the choice comes down to which language you know, prefer, or think would be best for your career.
To use any BSF-based script language, you need the language-specific JAR and a compatible version of bsf.jar on Ant’s classpath.
Ant’s fetch.xml file has a "script" target that will pull down BSF with Jython, JRuby, and BeanShell.
Java 1.6 adds a built-in script engine to the JVM, along with a JavaScript implementation.
The language name is used in the language attributes of Ant's scripting tasks.
With the BSF and relevant language JARs on the classpath, we can use them in the <script> task.
Doing so executes a piece of script during the build, as in listing 18.1
The <script> task has one mandatory attribute, language, which must be set to the language of the script.
This works well for Jython, as Python’s indentation rules make it tricky to use inline in an XML file.
The self object is a reference to the Script task instance.
This reference is useful for logging messages, using the log methods that Task provides.
It doesn’t integrate well with Ant, and it’s showing its age.
There are newer ways to use script inside Ant, especially <scriptdef>
It’s a piece of inline code and isn’t very reusable.
Why not write a whole task in a scripting language, complete with attributes and elements? That is what <scriptdef> is for.
It lets you define an Ant task inside a build file in a scripting language.
This target declares a new task, random b in the namespace http://antbook .org/script c.
We then declare two attributes, max and property d, to configure the task.
The <scriptdef> task can define tasks with attributes and elements but not with nested text, except within elements.
Nor can you declare the type of an attribute—they’re always strings.
That means all the attribute-type conversion magic to set up paths and files is lost, though Ant will expand properties before passing them down to the task.
Our <random> task is written in Jython, Python’s port to the JVM.
Python uses indentation as a way of marking code blocks, the way Java uses the curly braces, so it’s sensitive to layout in the XML file.
Ant creates an attributes hash table containing all attributes that are passed to the task, indexing the attributes by their names.
Ant expects the script to handle missing elements and attributes itself, either by raising an exception or skipping some work.
The resulting fault trace is pretty messy, as the exception gets passed into Jython and then out again; in the process, Ant somehow ends up with a complete stack trace.
Passing exceptions across JVM languages is clearly something that could be improved.
AntUnit and <scriptdef> make a very good combination; they let you write both your code and your tests in build files, or at least in interpreted languages.
Because the scripting languages have full access to the Ant runtime and the Java libraries, they can do things normal tasks cannot do, yet you have a fast, iterative development cycle.
Nested elements in scripted tasks Alongside attributes and text come nested elements.
These are slightly trickier because the <scriptdef> declaration has to declare the type of the element to create, as well as its name.
The task also has to deal with the possibility of multiple elements of the same name being set.
Here is a JRuby script that supports a <classpath> element:
The <element> declaration has to state the type of the element b.
This can be a known Ant type, or it can be the full name of any Java class, a class that must have a public, no-argument constructor.
When the script is executed, Ant builds up a list for every named element handed in.
All these lists are stored in the elements HashMap, which can then be queried for a named list c.
If no element was supplied, the result will be null.
Here, in Ruby, the keyword is nil, but the meaning is clear.
Testing this task is straightforward; a test target invokes it with a couple of paths:
Another test checks that calling the task with no nested elements throws the expected exception.
With element support working, our scripts have access to Ant’s full suite of datatypes: paths, files, resources, and the like.
The simplest way is to embed the scripts in a build file, perhaps one that’s pulled in via <import>
The other way is to declare them in an Antlib.
Build files can use the tasks just by declaring the Antlib URI.
If you’re creating an Antlib for a project, you can declare <scriptdef> scripts inline in the antlib.xml file itself.
When the Antlib is packaged and distributed as a JAR file, the scripted tasks are available to all users.
The file attribute of <scriptdef> takes a file only in the local file system.
That means that developers cannot currently keep all of the script files in separate .js, .py, or .ruby files in the JAR and then refer to them in <scriptdef> declarations.
That is something that will be fixed in the future.
Java 6, where script support is built into the Java runtime.
As a result, you can use JavaScript in your Ant builds without needing any extra libraries:
To use this script, we have had to do two things.
Selecting language= "javascript" is the first of these b; this tells Ant to use JavaScript.
Ant then needs to choose which scripting engine to use.
The default value, manager="auto", tells Ant to use the BSF manager if it’s present and if the requested language is available.
By asking for the javax manager c, we get the Java 6 manager, bypassing BSF.
There’s also the option of setting manager="bsf" to only check the BSF script manager for the specific language.
You need to set the manager attribute only if you really want to select a particular implementation.
The fact that JavaScript is built into the runtime makes it very appealing: if all developers are using Java 6, then the build files can use JavaScript inline without any external dependencies.
Tasks defined with <scriptdef> are true peers of Java tasks, rather than second-class citizens.
Why didn’t we cover them in the previous chapter? The answer is that Java is the main language for writing tasks today, especially those that integrate with other Java applications or libraries.
The other reason is that to introduce scripting at the same time as Ant’s internal API would be too much at one time.
You need to know Ant’s API and how to use AntUnit first.
There are other places that you can use scripting languages in Ant, which we’ll cover as we go through Ant’s remaining extension points; we’ll start with conditions.
A nice place to start is with conditions, which are little classes that evaluate to true or false.
It’s easy to write a new condition that all these tasks can use.
To implement a condition, we write a Java class that implements Ant’s Condition interface and its boolean eval() method.
This method must return true if the condition holds and false if it doesn’t.
Attributes and nested elements come via Ant’s normal XML binding mechanism.
If the condition extends the ProjectComponent class, Ant even binds it to the current project, allowing the condition to log messages or manipulate the project’s state.
Listing 18.3 shows a simple condition, one that tests for the contents of the value attribute being an even number.
Once compiled and declared, the condition is ready for use in any of the conditional tasks.
Declaring the condition is slightly different from declaring a task, because <typedef> is used.
When Ant encounters a datatype in a build file, it converts it to Java objects.
Ant’s type system also includes any Java classes that implement specific interfaces—in this case Ant’s Condition interface.
Ant uses introspection to see what a declared datatype can do and to allow the custom types to be used inside tasks or.
Listing 18.3 A condition to test for a number being even.
For Ant conditions, that means any task with the method add(Condition) can host the condition.
To tell Ant about our new condition, we use the <typedef> task:
After the condition has been declared as an Ant type, it’s ready for use.
We can even use it inside third-party tasks that take conditions, such as AntUnit’s <assertTrue> assertion:
Because conditions are so easy to write and because they slot so easily into so many.
Scripted conditions If a project has a one-off test they need to make in a build, the developers can implement the condition in script, using the <scriptcondition> condition.
Here’s the test for a single number “3” being even:
The self.value attribute is used to store the result of the evaluation.
If it’s set to true by the script, then the condition is true; if it isn’t set or if it’s set to false, then the condition evaluates to false.
There’s no easy way to pass data down to a <scriptcondition>, and the script has to be repeated wherever it is used—unless the file attribute is used to point to a file containing the script.
As well as writing custom conditions, you can write tasks that accept conditions and evaluate them.
The best way to do this is to extend Ant’s ConditionBase class.
This class supports all the conditions built into Ant and adds an add(Condition) method at the end to contain third-party conditions.
The class does nothing with the conditions it collects, leaving it up to the subclass, which must implement an execute() method.
Here’s a task that counts the number of nested tasks passing or failing:
The output of this task is what we would hope—two tests passing and one failing, as shown here:
It’s important to know that simply implementing add(Condition) doesn’t give classes access to Ant’s built-in conditions, because they aren’t actually declared as Ant types.
Extending the ConditionBase class is the best way to support the built-in conditions.
It’s easy to add a new one, and not that much harder to support conditions in a custom task.
Either action makes it easy for Ant extension libraries to integrate with existing tasks and build processes.
One very interesting extension point to Ant is the resource class hierarchy.
Equally powerful is that the resource type itself can be extended.
The key method to override is usually getInputStream(), which returns the contents of the resource as input stream.
There are some other methods that can be useful to override, specifically getLastModified() and getSize()
These methods return the timestamp of the resource (or 0L if it isn’t known) and the resource size, respectively.
The base class actually has setSize() and setLastModified() methods that can be used in build files to set the values; this may be adequate for the custom resource.
The example resource we’ve chosen is one to create a stream of random characters from any cryptographically strong random number generator that is in the Java runtime, using the java.security APIs to access the generator.
This resource has no last-modified time, but it will have a size—a size set in the build file.
Listing 18.4 A custom resource to generate strongly random data.
As such, a resource has access to the project and a logger, and Ant binds to it just as it binds to any other XML element—by creating an instance and then configuring it through its XML-to-Java mapping code.
There’s also the strange stuff in the getInputStream() method b, which is the other part of reference handling and is something that we’ll cover in section 18.3.2
Ant requires resources to state whether the data they refer to is present, and when it was last modified.
This resource is always found c, and it has no last-modified date d.
The heavy lifting—the random number creation—is implemented in the create() method e.
This method creates a SecureRandom number source that is then fed into the constructor of a helper class, RandomInputStream.
When the resource is asked for its input stream, a new generator is created and a new.
This means that unless a seed is used to set the randomness, the input stream will be different every time getInputStream() is called.
Every call will result in a new stream of finite length, full of random data1that is, as soon as we can use it in a build file.
To use our random data source, we need to tell Ant about it, which brings the <typedef> task into play again:
Once defined, the resource can be used in any task that supports resources, such as the <loadresource> task, which fills in a property from a resource:
To save random data to a file, we could use it inside the <copy> task:
The name attribute had to be set here, so that <copy> would know what filename to create at the far end.
Datatypes differ from other Ant components in the notion of “references.” If we give a datatype, including a resource, an id attribute, Ant will file it away for reuse:
During development the size check was accidentally omitted, so the stream was infinite.
Alternatively, we could declare a specific resource type, in which case the type of the declared resource must match that of the reference:
How does this work? Every “relevant” operation on a datatype needs to be redirected to any reference type, if it’s present.
This explains the code in the getInputStream() method in listing 18.4
The method had to call isReference() to see if it was a reference, and, if so, get the target resource through the call to getCheckedRef()
It is the target resource whose getInputStream() method is finally invoked.
Every operation that acts on the referenced datatype needs to resolve references this way.
The <touch> task can update the resource with this interface.
There’s an implicit assumption that touching a resource updates its last-modified time, and that getLastModified() will (approximately) reflect the new value.
New to Ant 1.7, they aren’t universally supported across tasks, especially third-party ones.
They offer lots of places to improve the build process, as anything that can act as a source of data or locator of files can now plug directly into other tasks.
Some projects should be able to do very creative things with them, such as directly feeding remote data into other applications.
We may even see custom resource collections, which group resources differently or we may provide sets of resources from new locations.
The tools are there—we’ll have to wait and see what people do with them.
There are three more Ant datatypes we want to look at: selectors, mappers, and filters.
A selector is an Ant type that can filter filesets to decide whether to include a file.
As well as the built-in set, you can add new selectors to a project, writing them in Java or in scripting languages.
The ReadOnlySelector that does this is quite short and sweet:
Ant’s documentation already provides extensive coverage of writing custom selectors, so we don’t cover it in detail here.
The main actions are extending BaseExtendSelector and implementing the isSelected method.
Custom selectors also can take parameters using nested <param> tags.
After the usual steps of compiling the source and creating a JAR file, the selector can be declared with a <typedef> command:
A custom selector can support attributes and nested elements and text, through the appropriate set- and add- methods.
Once declared, the selector can then be used inside any fileset, as with this copy operation:
This example is from the AntUnit test that verifies that the writeable file wasn’t copied.
Selectors are easy to test in AntUnit; the setUp target builds a directory with.
The tests themselves copy files from the selection to a temporary directory, which the build file then probes for files that should and should not have been copied.
Finally the tearDown target cleans up the temporary directory, ready for the next test.
If you don’t want the overhead and build stages of a Java selector, a <scriptselector> can implement the selector inline.
Here’s the selector re-implemented as a piece of BeanShell script:
Since we’ve just avoided writing and compiling a Java selector, this is clearly a good way to do a one-off selection.
Many of the same tasks that take selectors also support the <mapper> datatype, which can rename files as they’re moved, copied, or imported into archives.
Listing 18.5 shows a custom mapper that makes filenames uppercase.
A custom mapper must implement Ant’s FileNameMapper interface, which has.
We ignore two of the methods, and only implement the mapFileName() method.
This method takes a string, the path of the file relative to the base of the fileset, and returns an array of strings, the mapped paths.
Mappers are free to return multiple mappings for the same source file.
As before, the Ant type needs to be compiled, packaged, and declared using <typedef>, possibly in an antlib.xml file.
Once declared, the mapper can be used in a <copy> operation:
The result of this is that files beneath the directory src are copied with uppercase file and directory names into build/dest.
The parent directories of the fileset, everything up to and including src, aren’t mapped.
Mappers work only on relative paths, such as turning source directory trees into dotted Java package names.
Mappers crop up a lot in Ant’s internals, as they can encode any transformation from input files to output files, such as those of Java RMI stub files.
Such mappers aren’t declared as Ant datatypes, and they remain hidden from build files.
Scripted mappers You can avoid writing and building a mapper in Java for a one-off file mapping by using the <scriptmapper> mapper for coding the mapping inline.
Here’s the previous <copy> task, this time with the mapping in BeanShell:
The source variable holds the source filename, and every call to self .addMappedName() adds another mapped name to be returned.
As with any other Ant extension, we need to test our mappers, which brings us back to AntUnit.
Some source files matching the relevant pattern should be set up, then mapped into a target directory and the results validated.
The <resourcecount> task, which counts the number of resources in a nested resource collection, comes in handy, as it can count the number of files matching a pattern:
This test case asserts that no files matching the (case sensitive) org/** pattern was pulled in by the mapper.
A similar check can count the number of files matching ORG/** to assert that uppercase files were created instead.
Just as mappers can transform the names of files, filters can transform their content.
They are Ant types that are chained together during some operations.
Once set up in a filter chain, the entire data source is pumped through a character at a time, with each filter fetching the character from its predecessor, possibly transforming it, and then passing it up to the next entry.
Custom filters can perform custom transformations on the data stream, including adding or removing data from it.
The simple filter of listing 18.6 converts lowercase text to uppercase.
Listing 18.6 A custom filter to convert read text to uppercase.
This constructor takes the source of data to read in.
The chain method comes from the ChainableReader interface and allows our class to be linked to another filter, passing the modified stream through to it.
The most important method is read(), which Ant calls when another character is needed.
Our implementation calls the predecessor in the chain and then returns the result of converting lowercase characters to uppercase in the process.
As with the other Ant types, you need to declare the filter before use:
We can now test this class inside the <concat> task, passing a string through the filter chain:
Admittedly, case conversion is a pretty simple operation, but it shows what can be done.
Arbitrary post-processing can be performed on the input to or output from a Java or native program.
Read in the next character from the chain and convert it.
If we don’t want to write Java for a quick bit of text filtering, the <scriptfilter> type can help.
This can host a piece of script that will be invoked once for each token to be filtered.
The test uses the <loadresource> task to load a string into a property by way of the filter.
Every character gets converted into uppercase during the read, which is what we then test for.
Filter summary Filters are the last of the main Ant extension points, though a thorough browse of the Ant documentation will undoubtedly uncover a few more.
Filters are useful for preand post-processing of text, such as input to or output from a native program.
Whenever you <copy> a file with property expansion, that’s a filter at work.
If you need to process streamed data in a build, then write a filter either inline or in Java.
The other way that Ant can be extended is much more low-level.
You can write new classes that change how Ant prints out the build or handles input.
Let’s build up to it by hooking into Ant’s I/O and eventing infrastructure.
It’s time for the final bit of extending Ant—time to go low-level and look at topics that most people avoid.
We’re going to look at the internals of Ant, first by customizing its I/O mechanisms.
Ant prints to the console, and in IDEs Ant’s output appears in windows in the editor.
How does Ant do this? Through two tightly related concepts: listeners and loggers.
These receive lifecycle events and the output of a build, and both can be extended by custom versions.
Let’s first take a look at the UML for the BuildListener and BuildLogger interfaces, shown in figure 18.1
A BuildListener is a Java class that receives notifications of various build, target, and task lifecycle events during a build.
The events are build started/finished, target started/finished, task started/finished, and message logged.
Ant internally attaches some of its own build listeners to catch events, particularly the build-finished event, which triggers cleanups.
This BuildEvent encapsulates all the details of the event being triggered, as listed in table 18.2
Table 18.2 The different BuildListener callbacks and the BuildEvent data they can expect.
For all the finished events, a non-null exception inside the BuildEvent implies that the task, target, or build failed.
Figure 18.1 The BuildListener and BuildLogger receive lifecycle events, events which are described by BuildEvent objects.
Table 18.2 The different BuildListener callbacks and the BuildEvent data they can expect.
The BuildLogger interface builds on its parent BuildListener by adding access to the output and error print streams.
Two additional methods that the BuildLogger interface extends beyond BuildListener allow for setting the emacs mode and the message output level.
The DefaultLogger reacts to the emacs switch by generating output formatted for IDE integration, as its formatting of error locations in files is something most IDEs can parse.
The message output level is used to filter the output based on the logging level.
Every Ant project has one and only one logger, which is hooked up to the output streams and attached to the project as a listener to receive lifecycle events.
Users can select a specific logger on the command line via the -logger switch; otherwise they get the default one.
The -quiet, -verbose, and -debug switches can move the logging level up or down from its default of “info.” Because only one logger is allowed, IDEs don’t let you switch from their custom loggers, which are needed to integrate Ant with the editor.
As we stated, a listener is a Java class that implements BuildListener.
It records the frequency of start and finish events and then, when the build finishes, prints them to System.out.
The Project attribute is always set, and depending on where the message originated, the target and task attributes may also be set.
This listener delegates most of the work to a helper class—the Tracker classwhich tracks notifications.
This class tracks the start and finish events for each category, as well as the last exception thrown on a failure:
This class’s toString() method prints the statistics for that particular category.
When a StatsListener instance receives a buildFinished() notification, it prints out all the trackers’ statistics at that point.
Assuming that we receive such a message at the end of the build, this should give the statistics of the build.
There’s no test suite for listeners and loggers, no equivalent to AntUnit.
Ant’s original test-harness JAR can be used to run Ant from JUnit tests, setting up a project and making JUnit assertions about the results.
Suspiciously, there are no tests for any of Ant’s own listeners or loggers, just a MockBuildListener that appears in notification dispatch tests.
We’ll break our test-first rule and mimic the Ant team by running our code, instead of rigorously testing it.
To run our listener, all we need to do is run Ant from the command line with the -listener argument pointing to the new class and the newly created JAR appearing on the classpath:
Construct and return a string containing everything that has been recorded.
This is actually mildly interesting, especially on a big project.
The project count remains at one, even when multiple <ant> calls have invoked other build files.
Listeners are notified only on the big builds starting and finishing, not on subsidiary projects.
What happens on a failing build? For that, we need a build file that fails, such as that in listing 18.8
This build file shows that when a task fails, the containing target and build file also are notified.
There’s one more experiment to do: use the Ant-contrib <trycatch> task.
The success and failure messages are for the logger in section 18.7.2
If a task fails, it is signalled as such to a listener, even if an outer container catches the failure and discards the exception.
The next coding exercise after a custom listener is a custom logger, which is a class that implements BuildLogger.
This is simply a BuildListener with four additional methods to handle the system output and error streams as well as the setting of some output options.
The easiest way to do some logging, if all you want to do is slightly tweak the normal output, is to extend Ant’s DefaultLogger class, which is Ant’s normal logger.
Listing 18.9 A new logger, an extension of the normal one, that replaces the normal success/failure message with one from project properties.
Here the -logger option set the classname of the new logger, which was already on the classpath.
The -q option turned off all the output except for errors and warnings, a feature of the DefaultLogger class that subclasses get for free.
Being able to change the message in advance is one thing, but setting it inside the build is something else that’s potentially useful.
In the build file of listing 18.8, the success and failure properties were set in the build; these are the properties that should propagate to the output text:
This build file gives us a custom error message for our end users.
We do need to make sure the new logger is selected for every build.
This can be done with the ANT_OPTS environment variable, such as here in a bash configuration file:
This will switch to the new logger on all command-line runs.
Avoiding trouble in custom listeners and loggers The Ant documentation warns against loggers or listeners printing to System.out or System.err, because doing so can create an infinite loop.
The logger is handed two PrintStream instances for output; it should use these.
Listeners are not really meant to generate output, but if they must, they should do it to some other device such as a file.
In fact, you can get away with printing to System.out and System.err, as long as you don’t do so in messageLogged() events.
One trouble spot is the state of a project during lifecycle events.
When a listener receives a buildStarted event, the project isn’t yet fully configured.
Its tasks aren’t defined, and the default properties aren’t set up.
Similarly, when a listener has its buildFinished() method called, the build is already finished.
The listener can examine the project and its properties, but not run any targets or tasks.
A project calls all the listeners in sequence, in the build’s current thread.
When the <parallel> task is used to run tasks in a new thread, the notifications from those tasks are raised in the new thread.
Historically, Ant loggers have been used for generating custom reports.
Nowadays, it’s the job of the continuous integration server to generate the HTML reports and the emails, and the IDE has probably taken over from the command line as the main way of launching Ant.
In either situation, do not attempt to use your own loggers.
The IDE and continuous integration developers will have written their own loggers, loggers that should be left alone.
If you want Ant to send out emails when a build fails, have a continuous integration tool do the work.
It will catch and report problems that the loggers won’t get, such as a missing build.xml file.
There’s no need to re-implement what existing continuous integration tools can do better.
Listeners are less troublesome, as a project can have any number of active listeners, and listeners can be added or removed during the build.
There aren’t any tasks to do this, other than the <record> task, which records events to a file.
The opposite of Ant’s output system is its mechanism for handling user input.
Although Ant is designed to run without user intervention, sometimes builds use the <input> task to ask for input from the user.
Doing so delegates the task to an InputHandler, which is a class that handles all input from the user.
The default handler reads from System.in., expecting input from a user at the console.
The first of these handlers reads input from a property file, while the GreedyInputHandler reads the whole input stream into a single <input> request.
Selecting this handler with the -inputhandler option lets Ant integrate into a Unix-style pipes-and-brackets setup:
We aren’t covering the details of how to write a new InputHandler—only mentioning that it is possible.
If you’re writing an IDE or continuous integration tool, then consult Ant’s documentation and source for details on how to integrate Ant’s input handling.
The final way to extend Ant is to embed it inside another Java program.
Java IDEs do this to integrate Ant with their GUI.
It crops up in products such as Apache Tomcat, where <javac> compiles down JSP pages into .class files.
When embedded, Ant becomes a library that can execute built-in or custom tasks.
It has become a simple workflow tool with tasks for compiling and running Java programs.
You can either create a build.xml file and hand it off, or create a Project instance with tasks and targets via Java operations.
Interestingly, there’s one way that is hard to use Ant: its static entry point, Main.main() calls System.exit() at the end.
If you want to embed this class, you have to subclass its Main and override its exit() method.
You can still use Ant’s launcher application, specifying the new entry point via the -main argument.
However, we’ll ignore this route, as fully embedded Ant is more interesting.
What’s important is that a Project instance has been created and set up, with logging all wired up and running at the “info” level.
If we had wanted to expand properties or resolve paths, we would have had to invoke the relevant Project methods before calling the task methods.
Here is our test run of Ant inside the output of the <java> task: exec:
This shows that we’ve written a new entry point to Ant.
It isn’t as complex as Ant’s own Main class, but it shows the basic techniques of running Ant and Ant tasks.
You just create a project, create and configure tasks, then bind them to the project before you run them.
Don’t expect a very long-lived build not to leak memory.
If you distribute a version of Ant that can be directly invoked by end users, or if you put your version of Ant on the CLASSPATH—which has the same effectyou take on all support responsibilities.
The Ant team doesn’t support any problems related to random redistributions of Ant.
Never use an Ant task without creating a Project and binding the task to it with setProject()
Tasks depend on a project instance for logging and many other operations, and they break horribly if getProject()==null.
This chapter finishes our coverage of Ant with a look at the final ways to extend Ant.
Alongside Ant tasks come Ant types—types that are defined with <typedef>
There’s nothing wrong with a bit of script to solve a problem that Ant cannot normally handle.
Custom conditions let you add new tests into a build.
Ant can set properties from the condition or block until it passes.
You can even write custom tasks that accept nested conditions, both built-in and third-party.
Ant resources are one example of an Ant datatype—an XML element that can be shared between tasks.
Resources can provide a source of data to any resource-enabled application.
They let any such task access data from any source, without having to know or care where it comes from.
Custom mappers and selectors can control how Ant processes sets of files.
Mappers translate one filename to other filenames, and a custom one can provide new mapping rules.
Selectors nest within filesets, allowing sophisticated filtering of files within a directory tree.
Writing a custom selector can add enormous capabilities to file selection, such as the read-only file selector we developed here.
Mappers are useful to identify the output files generated from source files, which is why they’re used internally in many tasks.
Filters allow for powerful data transformations, and chaining filters together accomplishes something similar to piping commands from one to another in Unix shell scripting.
These are not ways to enhance a build file; rather, they are new ways to integrate Ant with other applications.
Build listeners and loggers let you capture output or generate new messages.
You can also embed Ant inside another Java program, simply by creating a new Project class and adding configured task classes.
The most important point we can leave you with is this: familiarize yourself with all of Ant’s out-of-the-box capabilities before beginning customizations.
Very likely, you will find that Ant can already handle your needs.
Consult Ant’s documentation, this book, and online resources such as Ant’s user and developer email lists, where you’ll find a helpful and often quick-responding crew of Ant users from around the world—including ourselves.
Sun distributes their versions under http://java.sun.com/javase/—you need to download the appropriate JDK for your system.
After installing the JDK, Ant requires the environment variable JAVA_HOME to be set to the directory into which it was installed.
Installation If there is one area where Ant could be improved, it’s in the area of installation.
It still has a fairly manual installation process, and a few things can go wrong.
Here’s a summary of how to install Ant, and also a troubleshooting guide in case something goes awry.
Some Ant tasks depend on this, since they run these programs.
The core stages of the Ant installation process are the same regardless of the platform:
Set up some environment variables to point to the JDK and Ant.
Add any optional libraries to Ant that you desire or need.
The exact details vary from platform to platform, and as Ant works to varying degrees on everything from laptops to mainframes, it isn’t possible to cover all the possible platforms you may want to install Ant onto; instead we’ll cover only the Windows and Unix/Linux platforms.
Binary distributions should work out of the box, whereas source editions need to be built using the Ant bootstrap scripts.
When downloading a binary version, get either the latest release build, or a beta release of the version about to be released.
Nightly builds are incomplete and built primarily as a test, rather than for public distribution.
Then unzip it to where you want the files to live, making sure that the unzip tool preserves directory structure.
This new directory you’ve created and installed Ant into is called “Ant home.”
You should add the bin subdirectory, here c:\java\ant\bin,  to the end of the Path environment variable so Ant can be called from the command line.
You also should set the ANT_HOME environment variable to point to the Ant home directory.
The batch file that starts Ant can usually just assume that ANT_HOME is one directory up from where the batch file lives, but sometimes it’s nice to know for sure.
After closing the dialog box, any new console windows or applications started should pick up the altered settings.
You can check by typing SET at the command prompt, which should include lines like the following:
If these variables aren’t set, try logging out and in again.
This is also a good time to check that the CLASSPATH environment variable isn’t.
If it is, all JAR files listed in it get picked up by Ant, which can cause confusion.
If there’s a quote inside the CLASSPATH, or if it ends in a backslash, Ant will fail with an obscure error.
The correct setting—if it has to be used at all—would be.
To test that Ant is installed, type ant -version at a newly opened console.
The printed version number must match that of the version you’ve just downloaded; anything else means there’s still a problem.
The Kaffe and Classpath projects provide a runtime that’s good for running code, but the official JDK is still best for development.
Many Linux distributions provide a packaged distribution, such as an RPM or .deb file.
Those that are created by the JPackage team at http://www.jpackage.org/ are the best; they integrate the installation with the operating system’s library management tools.
Their web site shows how you can use yum or apt to subscribe to their releases.
Sun provides RPM packages that are not fully compatible with JPackage installations.
To stop this value from changing every time you update the JDK, set up a symbolic link, such as /usr/lib/jvm/jdk, to point to the JDK you want to use.
Then set up the login scripts of yourself—or all users in the system—to have the JAVA_HOME environment variable set to this location.
In the bash script language, it would be something like.
You should also add the bin subdirectory of the JDK to the PATH environment variable.
To test for the JDK, you should try running java first:
If this picks up another Java runtime from the one you’ve just installed, then you have an existing runtime installed somewhere.
Once java works, check that javac is on the command line, as is tnameserv.
The latter is available only if the JDK’s bin directory is on the path.
If it’s found, the JDK is installed, and it’s time to install Ant.
As with the JDK, the JPackage team provides a version of Ant.
You can install it and have it set up Ant to work from the command line.
AIX, Solaris, and HPUX users will encounter problems if they use the version that ships with their OS, as it cannot handle long filenames:
The result should be the version of Ant that’s installed:
A different version means that there’s a conflicting copy of Ant on the path, perhaps one preinstalled by the system administrators.
There’s a place where Ant options (such as ANT_OPTS) can be set in Unix, the .antrc file in the user’s home directory, which is read in by the Ant shell script.
Other mechanisms for starting Ant under Unix, such as the Perl or Python scripts, don’t read this file.
These can be used to configure Ant’s scripts—but have no effect on IDE-hosted Ant.
One use of the environment variable is to set up the proxy settings for Ant tasks and Java programs hosted in Ant’s JVM.
They will be visible as Ant properties, but they are interpreted by the JVM, not Ant.
This could be useful, for example, if you always want to use Ant’s NoBannerLogger to remove the output from empty targets.
Any Ant command-line parameter can be set in this environment variable.
This is where you discover that a consequence of free, open source software is that nobody staffs the support lines apart from other users of the tool.
Because Ant does work for most developers, any installation that doesn’t work is almost always due to some local configuration problem.
Something is missing or misconfigured or, perhaps, some other piece of software is interfering with Ant.
The best source of diagnostics is built into Ant—the -diagnostics command:
This will list out Ant’s view of its own state.
If you get an error instead, something has gone horribly wrong, either with the command line or the classpath.
If it works, it will display information about the installation, including Ant’s JAR versions, which tasks cannot be instantiated, system properties, the classpath, and other relevant information.
This output may help to determine the cause of any installation or configuration problems.
It is also important data to include in any bug report filed against installation problems.
If you cannot get Ant to work, consult the Ant user mailing list.
Ant does work on most people’s machines, so if there’s a problem it is in the local system.
These are hard bugs to track down, and nobody else can do it but you.
All you are likely to get is advice such as “reset CLASSPATH” or “remove the RPM and try again,” because there are no obvious answers to these problems.
Test Run java from the command line; if this isn’t a known command, then either Java isn’t installed or the path is wrong.
Fix Install the JDK; set up JAVA_HOME to point to the install location.
Add the bin directory to the PATH, and log out and in again.
Without the JDK, some Ant tasks will fail with “class not found” exceptions.
The environment variable JAVA_HOME is used to find the JDK—if it isn’t set, Ant will warn you on startup with an error message:
This may just be a warning, but it’s a warning that some tasks will not work properly.
Test 1 Run javac from the command line; if this isn’t a known command, then either Java isn’t installed or the path is wrong.
Verify that the file tools.jar can be found in the subdirectory JAVA_HOME/lib.
Fix Install the JDK; set up JAVA_HOME to point to the install location.
Problem: Ant not on the path Ant is started by a platform-dependent batch file or shell script, or by a portable script in a language such as Perl or Python.
If the path doesn’t include Ant’s bin directory, these scripts aren’t found and so Ant cannot start.
Test Run ant -version from the command line: a version number and build time should appear.
If the command interpreter complains that ant is unknown, then the path is wrong.
If the error is that the Java command is unknown, then the problem is actually with the Java installation, covered earlier.
Fix Modify the environment path variable to include the Ant scripts, log out, and reboot or otherwise reload the environment to have the change applied.
This may be an older version of Ant, or the installation may be incomplete.
Test Run ant -diagnostics to get a diagnostics output, including ant.home.
Fix Remove or rename other copies of the batch files/shell scripts, or reorder your path.
Problem: Ant fails with an error about a missing task or library This can mean that a library containing needed task definitions is missing.
Usually the problem is missing third-party libraries, though it also can be caused by a custom build of Ant that was compiled without those libraries.
Test Run ant -diagnostics to see if the task is present.
Fix Consult Ant’s documentation to see what extra libraries are needed—download.
Test Look at the value of ANT_HOME and verify it’s correct.
Fix Either set the variable to the correct location, or omit it.
Problem: Incompatible Java libraries on the classpath If you set up the CLASSPATH environment variable with a list of commonly needed JAR files, there’s a risk that the libraries listed clash with the versions Ant needs.
Test Look at the value of CLASSPATH and verify it’s empty.
Run Ant with the -noclasspath option to omit the classpath.
This can cause problems if any code in the extension libraries (such as jaxp.jar) tries to locate classes loaded under a different classloader.
Test Look in the JRE/lib/ext directory for any JAR files that have crept in as extension libraries and that are confusing Ant.
Fix Move the XML parser libraries to a different directory.
This exception happens when a library has been marked as sealed but another library implements classes in one of the packages of the sealed library.
This exception means there is an XML parser conflict, perhaps from an older version on the classpath or extension library, or perhaps from some other library that contains a sealed copy of the JAXP API.
The underlying cause will be one of the two problems above: extension library conflicts or classpath incompatibilities.
Fix The message should identify which libraries have sealing problems.
Use this to identify the conflict, and fix it, usually by removing one of the libraries.
You can unseal a JAR file by editing its manifest, but this only fixes a symptom of the conflict, not the underlying problem.
Problem: Calling Ant generates a Java usage message If the Java invocation string that the Ant launcher scripts is somehow corrupt, then the java program will not be able to parse it, so it will print a message beginning with  Usage: java [-options] class [args...]
Backslashes at the end and quotes in the middle are the common causes.
Test Examine the environment variables to see if there are any obvious errors.
Otherwise, unset each variable in turn until Ant works; this.
Unrecognized option: -3 Could not create the Java virtual machine.
If the variable contains a string that is mistaken for the name of the Java class to run as the main class, then a different error appears:
Test Examine ANT_OPTS and verify that the variable is unset or contains valid JVM options.
Problem: Incomplete source tree on a SYSV Unix installation The original SYSV Unix tar utility cannot handle the long filenames of the Java source tree, and doesn’t expand the entire Java source tree—files appear to be missing.
Fix Untar Ant using the GNU tar utility, or download the .zip file and use unzip.
To summarize, most common installation problems stem from incorrect environment settings, especially the CLASSPATH environment variable, or duplicate and conflicting Ant installations.
If these tests don’t identify the problem, call for help on the Ant user mailing list.
This is a mailing list where Ant users solve each other’s problems, be they related to installing Ant or getting a build to work.
This must consist of a single XML root element, which can contain other XML content nested inside.
All Ant documents must have project as the root element, so all Ant XML files should have a structure something like this:
Extended Markup Language (XML) provides a way of representing structured data that’s somewhat readable by both humans and programs.
It isn’t the easiest of representations for either party, but it lets people write structured files that machines can parse.
The strength of XML is that many tools can work with the XML without knowing what the final use is.
Once you’ve learned XML, you can recognize and navigate almost any XML document.
One such element in Ant is <echo>, which tells Ant to print a message:
This element would only actually print an empty string, because it contains no child.
Empty elements can be written in a special shorthand way:
To the XML parser, this means exactly the same as the previous declaration—an element declaration with no nested children.
Most Ant tasks provide extra information to Ant through child nodes, textual or XML, and attributes of the elements.
Attributes XML attributes are name=value assignments in the opening tag of an element.
This statement uses single quote characters and closes the element within the opening tag.
When executed, both Ant task declarations will have the same result:
Attributes cannot be declared in the closing tag of an element; this is illegal:
The XML parser will fail with an error before Ant even gets to see the file:
They do usually hint at the problem, and show the file and line at fault.
Incidentally, there’s no order to an element’s attributes; Ant may see them in a different order from that of the XML file.
Furthermore, you cannot have duplicate attributes with the same name.
For that, nested elements are a better way to represent the information.
Nested text and XML elements Many Ant elements support nested data.
One such type is text, which the <echo> task accepts as the message to display:
As before, this will print out a message to the screen.
Sometimes, the child elements are complex XML declarations of their own:
Unlike attributes, there’s no limit to the number of duplicate elements you can nest inside another element.
This allows a <target> to contain a sequence of tasks:
While elements support nested text, they cannot support binary data.
Binary data XML cannot contain binary data; it has to be encoded using techniques like base-64 encoding.
This should be familiar to anyone who has written a lot of low-level HTML content.
Escaping characters is most common in XML attributes, such as setting the passwords to remote FTP or HTTP servers.
Table B.1 lists the most common symbols that you must escape in an Ant file.
Because escaping characters can become very messy and inconvenient, XML provides a mechanism for allowing unescaped text within a CDATA section.
In Ant’s build files, CDATA sections typically appear around script blocks or SQL commands.
Table B.1 How to escape common characters so that the XML parser or Ant can use them.
Even within CDATA or Unicode escaping, not all characters are allowed.
The other main content that people will find in build files are comments—a good build file is documented.
This is very important in an Ant build file, because documentation of the stages in the build process is so critical.
It is also useful for commenting out sections during development.
You cannot use double minus symbols “--” in a comment: this is an XML rule that hits some Ant users.
You have to use something like the following style instead:
This inconveniences some Ant users, but there’s nothing the Ant team can do about it; it’s a quirk of history that everyone has to live with.
Chapter 9 introduces XML namespaces, as a way of keeping Ant tasks from different sources separate, so it’s important to understand them too.
This section of the XML Primer is therefore only relevant from chapter 9 onwards.
In a namespace aware XML-parser, you can declare elements and attributes in different namespaces and mix them together by prefixing each use with a namespace prefix.
There is no requirement for any file to be retrievable from any namespace that’s given by a URL.
A namespace is bound to a prefix when the namespace is declared in an xmlns declaration, inside an XML element:
Such a prefix can be used in the element in which it is declared and in all nested elements—but not anywhere else.
To use elements or attributes in a namespace, they must be declared with the prefix in front of the name:
Attributes are automatically in the same namespace as their element, so the following are equivalent:
Nested elements are not automatically in the same namespace as their parent element; you need to explicitly declare them.
The name of the prefix is irrelevant; only the URI string of the namespace is rel530 APPENDIX B XML PRIMER.
If you omit a prefix from an XML namespace declaration, it becomes the default namespace.
This is the namespace into which all nested elements belong unless they have a prefix or the default namespace is redefined.
Do not redefine the default namespace in Ant—the tool does not like it.
That is the extent of the rules for XML namespaces across all XML 1.0-based applications.
Ant bends these rules a bit to make nested elements easier to use:
If an element or attribute is in the "" namespace, and yet the name of the element/ attribute matches one that a (namespaced) task expects, Ant assumes you meant the element or attribute in the specific namespace.
What does that mean? It means that Ant takes a more relaxed view of namespaces than do most XML tools, because Ant would be unusable without it.
This is convenient, but it can teach Ant users bad habits.
Incidentally, you cannot use Ant properties in the namespace URI; this doesn’t work:
Namespace best practices To keep the complexity of XML namespaces manageable in a build file, here are some good practices:
Use the same prefix for the same namespace across all your build files.
Never declare a new default namespace in Ant projects; don’t use an xmnls="http://antbook.org" declaration.
They usually have an Ant-aware editor, which lets you navigate around targets and add Ant tasks to them.
Eclipse and NetBeans let you debug Ant, setting breakpoints in a build file.
IntelliJ IDEA doesn’t offer this feature, but it does determine which properties are unset and highlights them in the text editor—reducing the need for a debugger somewhat.
This gives the users the best of both worlds—great environments for developing their application, and a build tool that can compile, test, and deploy that application.
Developers should choose whatever editor supports their needs, without worrying about whether it supports Ant, because the answer is always “of course it does.” Even so, there are some differences in features and ease of configuration and use.
Eclipse lets you run Ant in a new JVM, which can help.
A custom input handler is used to bring up a dialog whenever <input> asks for user input.
Custom listeners and loggers are used to capture the build’s output and to aid debugging.
Because a private Ant version is used, the classpath may be different from the command line.
To compound the problem, you cannot run ant -diagnostics against the IDE’s version of Ant, because that is a command-line option.
IDEs invariably provide some way to add new files to the classpath, either globally or for individual projects.
Use this to add essential JAR files to the private Ant runtime’s classpath.
The other classpath problem is that of selecting all JARs needed to compile the application.
If you’re using files in a lib/ directory, selecting these files is trivial.
If you use Ivy, it’s useful to have a target to copy all JARs from the compile or run configurations into a single directory, so that you can point the IDE at them.
In either case, if you want the source and JavaDocs for the libraries, you’ll need to download them from the relevant projects’ sites.
Another issue with IDEs is that there is usually a lag between an Ant version being released and support for it in IDEs.
The IDEs usually let you switch the IDE to another version of Ant on the hard disk.
This lets you use the same Ant installation for the command line and the IDE.
We use this method whenever we can, partly for consistency, but also because as Ant developers, we’re always updating our Ant version.
The IDEs all support this, although they may not recognize new tasks or types correctly, marking them as unknown or invalid.
This is something you just have to accept until a later IDE release ships.
Now, with those details out of the way, let’s look at the Ant support in the three main Java IDEs, starting with Eclipse.
The IBM-founded Eclipse IDE is a general-purpose development framework targeting Java development, with emerging support for C++, Ruby, and Python.
It is clearly the dominant player in the Java IDE market.
It assumes that you have all your projects in a single “workspace,” and it manages the links between them.
Eclipse has a way of organizing things, and if you embrace that way, the IDE works.
This design can, unfortunately, make it hard to integrate with Ant.
Eclipse supports simple Ant-based projects with a single build file for a single module.
However, once you have multiple build files importing each other, things can break down.
This is a shame, because apart from this, the IDE supports Ant very nicely.
Figure C.1 shows the IDE debugging an Ant build file: you can halt the build at a breakpoint and view Ant’s properties.
In this run, the IDE has just fielded an <input> request by bringing up a dialog prompting for the data.
It has stopped at a breakpoint and is showing the current set of Ant properties.
Eclipse supports multiple projects in a workspace ; each project can either live under the IDE’s workspace directory, or it can be remotely linked in.
For projects that build under Ant, this linking-in process doesn’t seem to work that well if you have build files that call other build files through <import>
Whenever you open a file called build.xml, Eclipse will recognize it as an Ant file and open it in its Ant-specific editor.
To open any other XML file in this editor, select the file and bring up the “open with…” dialog with the right mouse button; ask for the Ant Editor.
Attribute completion, with a popup listing of values for attributes that take Boolean or enumerated values.
Go-to-declaration (the F3 key) to follow the definition of a target in a target’s depends list.
The ability to set breakpoints on a line of the build.
A list of targets, with the ability to navigate to, run, or debug any target.
Eclipse can run build file targets when the user asks to “build” or “clean” an eclipse project.
This method offers the tightest integration, as it hands off the important work—the build—to Ant.
To tell Eclipse to build a project with Ant, you need to create a new builder, which you can do from the project properties dialog of figure C.2
You can add a build file as a builder through the project's properties dialog, where it can live alongside the projects' other builders.
The first action is to select the build file and its base directory, which figure C.4 shows.
Alongside selecting the build file, setting the base directory is the most important action.
You must point it at the directory in which the build file lives, after which everything but <import> should work.
Keep the “Set an Input handler” option checked, to ensure the IDE brings up a dialog box when user input is needed.
The “Refresh” tab controls whether the IDE should reload its files after a build.
It is safer (albeit slower) to enable a full refresh.
More relevant is the tab that lets you bind the targets to run from the IDE actions, the “Targets” tab of figure C.5
Figure C.3 You can add any program as a new builder, but only Ant has built-in support from the IDE.
Figure C.4 The first step to setting up the new builder is to set the path to the build file and its base directory.
You also can add arguments to the command line, but not -lib or -logger related, as the IDE is in charge of those.
To set a target for a particular IDE action, press the “Set Targets” button to the right of each action’s target list, which will show the form of figure C.6
For a full or manual build, Eclipse normally runs the default target of a project, the one declared in the <project> declaration.
To change it to another target, you need to unselect the default target and select the desired target.
Navigation through the list is easier in a big project if you sort the targets and hide all “internal” targets—those targets without any description attribute.
The Ant builder dialog also lets you customize the Ant runtime used—the JRE, the environment variables, and the Ant implementation to use.
It is normally better to configure Ant for the entire IDE, rather than do it on a project-by-project basis, except for projects with very special needs.
Configuring Ant under Eclipse To configure the Ant runtime, go to the Preferences dialog under the Window menu and search for Ant options.
Pressing the “Ant Home” button brings up a dialog in which you can select the base directory of an Ant installation.
Eclipse will then automatically select all the JAR files in the lib subdirectory, using this as the base classpath for Ant.
This list does not get updated when new JAR files are added to ANT_HOME/ lib.
If you add new JAR files to this directory, you need to update the JAR list in the IDE by reselecting the Ant Home directory again (figure C.7)
Figure C.5 You can add any number of targets to the IDE actions.
Figure C.6 Selecting targets to run in response to IDE actions.
Avoid setting up long lists of targets for each action; it’s better to create new targets in the build file with the appropriate dependencies.
This lets you use it from the command line and makes it available to other developers.
Figure C.7 To choose and configure the Ant runtime in Eclipse, go through the Preferences dialog to the Ant runtime, and select a new Ant Home, the base directory of the Ant installation.
The “Tasks” tab of the settings window lists the extra tasks added to Ant by Eclipse.
Build files that run only under Eclipse are free to use any of these tasks, including the JDT compiler adapter, which lets you use the Java Development Tools (JDT) compiler inside Ant.
If you want to use any of these tasks on a build outside Eclipse, you need to add the same JARs to Ant’s classpath.
The <import> problem When the IDE creates an Eclipse project from an existing Java Ant project, it seems to copy the build file over to its own workspace.
For most tasks this fact is not obvious, because all file references get resolved relative to the base directory of the build, which Eclipse sets to be the original directory of the project.
There is one task, however, that deliberately does not resolve references relative to the basedir attribute of a project, and that is <import>
This is because it’s designed to support chained imports across multiple files, something that wouldn’t work if all paths were relative to the base directory of the main build file.
The easiest way to show this problem is to create a “Java Project from Existing Ant Build File,” selecting a build file that uses <import> to import another project.
The IDE copies over the build file into its workspace directory, setting up a link to the source tree.
As a result, all the other files in the project are absent, and so the imported file isn’t found:
This will make the <import> work under Eclipse, yet it will still allow the build to work on the command line.
A better solution is to create a custom build file purely for the IDE, one that uses a basedir-relative import to pull in the main project.
This file pulls in the real build file from where it lives, after which all imports will chain properly.
There’s one final option: check out the entire source tree as a single project.
Once you do that you can set up Ant builders for any part of the source tree.
This workaround removes much of the value of Eclipse, however, as you lose most of the Java development tool support.
Of course, projects that avoid using <import> don’t suffer from this problem.
Do we advise this? No, because <import> is key to scalable Ant-based projects.
Incompatibilities between Eclipse and Ant are something that will have to be resolved, somehow.
Apart from the <import> problem, it hosts Ant very well, and is an editor that offers task and property completion and build file debugging.
At the same time, it likes to take total control of where your files live and how they are built.
You need to commit to it—in which case there are workarounds for the <import> problem—or avoid it.
Currently at revision 5.5, the tool is built around Ant.
Whenever you tell the IDE to create a new project, it creates a new Ant build file, one that you can use and extend.
Sun also keeps reasonably up-to-date with Ant versions, a benefit of their frequent release cycle.
Here are some of the things you can do with Ant under NetBeans:
Tell the IDE which targets to use to clean, compile, run, and test a project.
Bind menu items and keyboard shortcuts to targets in a build file.
In some ways it is scary that a build tool needs a debugger, but if you have a build that needs to be debugged, you will be grateful! Consider installing NetBeans for this feature alone.
Another nice touch of the IDE is its JUnit test support.
When you run your normal JUnit targets, the reports get pulled into the IDE’s JUnit results.
Once you have a complex test run—such as those which test remote web sites or Enterprise applications—you will appreciate the value of this.
Adding an Ant project to NetBeans It’s slightly tricky to get an existing Ant project building under NetBeans, because it has strong expectations of what a build file must contain.
You need to create a new project, selecting the “with Existing Ant Script” option, as in figure C.9
Before you do this, run the dist and test targets at least once from the command line, to.
Figure C.8 NetBeans running a build file that prompts for input.
We've set a breakpoint on the following task, so the debugger will halt the build.
You need to be able to select these directories when setting up the output paths for the build.
The first step to creating the project is to point the IDE at the base directory of the chosen project.
It will look for the build.xml file and extract the project name from it.
If you want to use a differently named build file, this is the dialog to select it.
After binding to a project, you need to map IDE actions to specific Ant targets, as in figure C.10
NetBeans will guess the appropriate target from common names—the only target we hand-entered was the name of the “Run” target.
After the target bindings, two more forms are presented: one to identify the folders for Java source and test files—and the directories into which they are compiled—and the other to add any extra JARs to the compile classpath.
This is solely for the IDE to provide type completion and detect errors; it doesn’t alter the build file.
Once this is done, you’re good to go! You can now use the “Build” menu to build the project, or hit the F11 key.
This will run your test target, presenting the results in the JUnit results pane.
The result is a very tight development cycle using Ant for the entire build process—either the built-in Ant or an external copy.
It will create a subdirectory, nbproject, to hold extra project information, such as custom build files for the debugging actions.
Configuring Ant NetBeans lets you configure Ant from the “Miscellaneous” section of the Options dialog, where you can change to a new version of Ant other than that built into the IDE, and define new Ant properties.
Figure C.10 Binding targets to the stages in the build file.
You don't need a target for every action at this point, as you can edit these bindings later.
We’ve entered the location of our command line Ant distribution.
Checking the "Save Files" option tells the IDE to save all files before running a build.
However, if you have any build file that assumes that the ant.home property is set, the “Manage Properties” dialog is the place to set it.
However, from an Ant perspective, NetBeans is a very nice IDE.
What’s nice about it is that it’s much less dictatorial about how your project is laid out than is Eclipse.
It doesn’t try to tell you how to work, but adapts to you and your Ant build.
In particular, there’s no need to duplicate any information in the IDE about the build, other than the location of input and output directories, and the library dependencies.
Given it also has a good Java application profiler, it’s well worth a play.
The tool has led the way in refactoring and testing and is strongly Ant-aware.
Here are some of the Ant features that IDEA 6.0 offers:
Property management: renaming; highlighting of undefined properties, jumping to property declarations, and property completion while typing.
Ability to mark targets as items to run before/after compilation, and before a run or debug.
This is good for code generation, post processing, and deployment.
The IDE is very code-centric: it assumes that the core artifacts of your project are JAR files and tests; this is not a process-driven platform.
In particular it doesn’t try to impose any kind of structure on your code or build files, other than the standard practice of separate source and test package trees.
There’s no Ant debugger in this IDE, but it does highlight any properties that it thinks are undefined.
It knows which tasks set properties and how, and it detects references to properties that have not been set by any task in any target on the current target’s dependency list.
This is very useful, although it doesn’t extend to third-party tasks or tasks added to Ant after the IDE’s release.
Adding an Ant project To add an Ant project you need to create an IDE project which can do the basic compile under the IDE; then you need to add Ant build files to the project.
Create a new project, selecting the JVM and directory of the project.
Start with a “single module project” and choose the target project type, such as Java, EJB, Web, or Java EE—Java being the simplest.
When asked to create a source directory, point it at the existing source directory.
Configure the output directory to match that of Ant, such as build/classes.
In the project browser window, find the build file and click the right mouse button to bring up the context menu; choose “Add as Ant Build.”
In the same window, select the module at the root with the right mouse button, and bring up the “module settings” dialog.
The connection is failing, and an error is appearing in the messages window.
Clicking on the error brings up the line of the build file.
The goal for configuring the test and source directories is to have the IDE compile into the same source tree as Ant.
That lets Ant do the main build, generating classes the IDE will use when running or debugging the application.
Under the Window menu, select “Ant Build” to see the build window.
This is a list of build files and their targets.
Doubleclick a target to run it, or bring up the context menu with the right mouse button for more options, such as the keyboard shortcuts.
Because the IDE is doing the main build, there is some duplication of effort.
Whenever you test or run the project from the IDE, it compiles the code itself, rather than delegating to Ant.
What you can do is pull Ant actions into the build by selecting any target in the Ant Build menu, popping up the context menu, and picking “Execute On….” You can then request that the target runs before or after the compilation stage.
A target that creates Java source, such as a WSDL-to-Java operation, or the Java from XML activity of chapter 13, should be set to run before the build.
Post-processing activities such as an <rmic> compile can be set to run afterwards.
If you select “Execute Target before Run/Debug,” you can run the target before running, debugging, or testing the application.
Figure C.13 shows the dialog that’s presented, from which you can choose which activities require the target to run.
Once you start mixing IDE actions and build file actions it does get a bit hard to determine what exactly is going on.
The NetBean policy of “Everything is in the build files” makes it all more transparent, and there’s a lot less duplication of settings.
However, IDEA let’s developers set up keyboard shortcuts to any target and, in so doing, hand off most of the build to the Ant.
Even so, it’s useful to keep the IDE able to build and test the application, as that way you can start debugging sessions off test runs.
Figure C.13 Selecting the actions for which we want a deployment target to run; here the Tomcat and JUnit activities both trigger the action.
You may find that you will need IDE-specific targets for such actions, targets that do not depend on the complete build taking place.
This is because the IDE has already taken on much of the build.
In IDEA, you can configure Ant on a project-by-project basis.
After adding or creating a project, select any of its targets in the Ant Build window and request the properties dialog, either by the context menu or the toolbar.
Switch to any Ant version installed on the local system.
Figure C.14 shows the dialog where these settings can be changed.
By telling the IDE to run the build in the background and to not hide the messages window if the build was successful, you can host long-running builds, such as a deploy-and-test sequence, while getting on with writing more code or tests.
The other nice thing the IDE does is that you can tell any run or test activity to run Ant before starting.
Any of its “run configurations” can be set to build the application in the IDE first or to run any Ant target.
That means that you can set up Ant to build the application, including complex activities such as dynamically generating Java source or running the rmic compiler against compiled code, before running or debugging an application.
IDEs are great at debugging and editing text; Ant is good at building, testing, and.
Figure C.14 IntelliJ IDEA lets developers run a version of Ant in the file system, rather than the built-in version, with extra JARS on the classpath and with custom properties.
So why try to unify the IDE environments? Ant can be all the commonality of the build process developers need.
Here are our recommended tactics to combine IDEs and Ant in a team project:
The boost to productivity and morale here can outweigh most compatibility issues.
Make sure there is a standard IDE installed on everyone’s machine, regardless of the developer’s ultimate choice of tool.
If pair-programming techniques are being used, a common IDE is invaluable.
Integrate tests into the build process, so they are run every build and deploy cycle.
Testing and deployment are key reasons for developers to use Ant over the IDE’s own compiler.
Any customizations should be in peruser properties, not private build files.
Have standard target names across projects (a general Ant best practice)
Some developers may miss the total integration of a pure IDE build; adding unit tests and deployment to the Ant build, surpassing what the IDE build could do, could help bring them on board.
Offering them not only the choice of which IDE to use, but also the funding to buy a commercial product, could also help with motivation.
Ant in Action, 2nd Edition contents preface to the second edition foreword to the first edition preface to the first edition acknowledgments about this book about the authors about the cover illustration Introduction to the Second Edition Welcome to Ant in Action.
Understanding Ant datatypes and properties 3.1.1 What is an Ant datatype?
Testing with JUnit 4.1 What is testing, and why do it?
Continuous integration 15.1.1 What do you need for continuous integration?
