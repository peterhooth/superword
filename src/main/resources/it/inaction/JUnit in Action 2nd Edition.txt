The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
That’s what mathematics taught me—never stop until you get it done, and not just in a good way but in the best way.
When I started writing software, I found that the same principles apply.
I knew some colleagues who were neglectful of their work, and I saw how their results suffered from that.
They were impatient to finish their tasks, not worrying about the quality of the software they produced, let alone searching for the best possible solution.
For those guys, reusing the same code meant simply copying and pasting it everywhere they needed it.
I saw how being impatient to finish the task as quickly as possible led to that same task being reopened again and again, because of bugs and problems with the code as written.
I had the opportunity to work for Hewlett Packard, not only with the technical team, but also with the project managers on every level, and from them I learned the secret of delivering a quality software product.
Later, I became involved with the Apache Software Foundation (ASF), where I had the chance to work with some of the best software developers on the planet.
I studied their best practices and habits of writing code, writing test cases and sharing information among ourselves, and I was able to apply the things I learned to projects for some of the biggest clients of HP.
Gradually I got interested in the question of ensuring the sustainable quality of a software product.
PREFACExx proposed that I write an up-to-date revision of the bestselling book he authored five years ago.
The plan was clear, but I needed some soul mates to help me achieve it.
They both agreed to help with some of the chapters.
Things moved faster after that, and we spent a year and a half writing with the primary goal of revising Vince’s work.
If someone had told me in the beginning how hard it would be, I wouldn’t have believed him.
And that is why I feel that I need I to express my sincere gratitude to the Manning team—they made the whole journey a lot easier.
Now that the book is finished and you hold it in your hands, I hope you enjoy it.
It has been a rough journey to get it done, but here it is.
I know you’ll learn a lot of new things from our book, the way I’m sure you’ll improve the quality of your softwareyou’ve already taken the first step.
This book is the sum of four years of research and practice in the testing field.
The practice comes from my IT consulting background, first at Octo Technology and then at Pivolis; the research comes from my involvement with open source development at night and on weekends.
Since my early programming days in 1982, I’ve been interested in writing tools to help developers write better code and develop more quickly.
This interest has led me into domains such as software mentoring and quality improvement.
These days, I’m setting up continuous-build platforms and working on development best practices, both of which require strong suites of tests.
The closer these tests are to the coding activity, the faster you get feedback on your code—hence my interest in unit testing, which is so close to coding that it’s now as much a part of development as the code that’s being written.
This background led to my involvement in open source projects related to software quality:
JUnit in Action is the logical conclusion to this involvement.
We all want to write code that works—code that we can be proud of.
How often have you heard this: “We wanted to write tests, but we were under pressure and didn’t have enough time to do it”; or, “We started writing unit tests, but after two weeks our momentum dropped, and over time we stopped writing them.”
This book will give you the tools and techniques you need to write quality code.
It demonstrates hands-on how to use the tools in an effective way, avoiding common pitfalls.
It will help you introduce unit testing in your day-to-day development activity and develop a rhythm for writing robust code.
Most of all, this book will show you how to control the entropy of your software instead of being controlled by it.
It is lovely to gaze out at the churning sea from the safety of the shore when someone else is out there fighting the waves, not because you’re enjoying their troubles, but because you yourself are being spared.
This is exactly the feeling you’ll experience when you know you’re armed with a good suite of tests.
You’ll see others struggling, and you’ll be thankful that you have tests to prevent anyone (including yourself) from wreaking havoc in your application.
First of all, the project wouldn’t have started if not for Michael Stephens and Marjan Bace of Manning.
After that, any coherence the book exhibits is largely due to our developmental editor, Sebastian Stirling.
Special thanks to Ivan Ivanov who did the final technical proofread of the book shortly before it went to press.
We’d also like to thank all the developers who spent time reading this manuscript during its development and pointing out the problems.
The following reviewers proved invaluable in the evolution of this book from a manuscript to a book that’s worth a reader’s investment of time and money: Robert Wenner, Paul Holser, Andy xxiii.
Finally, we’d like to extend a sincere thank-you to the people who participated in the Manning Early Access Program; those who left feedback in the Author Online forum had a strong impact on the quality of the final printed product.
A special thank-you goes to my sister, who showed me the real meaning of the word courage.
Another big thank-you goes to my cousin Ivan Ivanov, who made me start this crazy computer journey in my life.
I’m also grateful for all of the English teachers I’ve had in my life—thank you.
This book wouldn’t be here if it weren’t for the hard work of Vincent Massol—thank you for making this possible.
Finally, I’d like to thank both Felipe Leme and Gary Gregory for being such great coworkers.
Vincent Massol Back in 2003, JUnit in Action was the first book I ever wrote.
I had no idea how long the writing process would take.
The great thing about long-running tasks is that when they’re done you reap the benefits for a long time, enjoying it even more.
It’s always with the same initial trepidation that I follow JUnit in Action sales and I’m delighted that seven years later the first edition is still selling.
Although a good portion of the book is still valid, most of the examples and frameworks have evolved and new ones have surfaced.
It was a real pleasure for me that Petar agreed to write this second edition, giving the book a second life.
You’ll see that Petar, Felipe, and Gary have done a wonderful job of updating the book with a lot of exciting new topics.
Gary Gregory I’d like to thank my parents for getting me started on my journey, providing me the opportunity for a great education, and giving me the freedom to choose my path.
I’m eternally grateful to my wife, Lori, and my son, Alexander, for giving me the time to pursue a project like this one.
Along the way, I’ve studied and worked with truly exceptional individuals too numerous to name.
Finally, I thank my coauthors and all of the people at Manning for their support, professionalism, and great feedback.
Perhaps you’ve worked with the previous versions of the JUnit framework in the past, perhaps you’ve worked with other testing frameworks, or perhaps this is your first step into the testing world.
Whichever path has led you here, you’re probably interested in improving your software process and the quality of the software you write.
The goal of this book is to give the basic foundation you need—and much more.
The world of software testing consists of many projects that solve specific tasks, of testing different components and layers of your application.
The central player in this world is the JUnit framework.
Written by Erich Gamma and Kent Beck about a decade ago, this framework has become the de facto standard in Java testing.
Unlike the old version of JUnit, the 4.x versions introduce a new approach and rewrite of the whole framework.
Hence the need for an up-to-date copy of the first edition.
In this second edition of the book, we introduce the core concepts you need to know in order to start testing your projects with the JUnit framework.
But that’s not the whole picture! This book will not only teach you how to write your test cases with the JUnit framework; it will also guide you through the process of writing your code, giving you suggestions of how to make it more testable.
This book will also teach you about fundamental software development principles like test-driven development.
It will also guide you step by step through the process of testing each and every layer of a typical Java EE application: the front layer, with external tools like Selenium and JSFUnit; the business layer, with tools like Cactus, mock objects, and stubs; and finally the database and JPA layer, with tools like DBUnit.
The book is organized into several parts, the goal being to walk you through JUnit in a sequence of increasing complexity.
The first part contains the preliminary chapters that introduce the technological context of the framework, give a high-level overview of the architecture, and present a bare-bones HelloWorld sample application to get your environment up and running.
After this brief introduction, we set off into a series of chapters that cover the core concepts and components of the framework one by one.
We take time to explain the functionality of each component in depth.
The second part of the book deals with the different techniques of testing: the mock approach and the in-container approach.
It introduces some new tools to create the fake objects we need.
The third and fourth parts of the book look into detailed explanations of third-party tools/JUnit extensions that we use to test the different layers of our applications.
In addition, the book has several appendixes that will help you to switch easily to the latest version of JUnit and integrate easily with your favorite IDE.
The gentle introduction defines what testing is, how to perform it efficiently, and how to write your first test cases.
This chapter is a must to give you the confidence to realize that testing is something natural that should always happen during development.
Chapter 2 dives into the architecture of JUnit and shows how it’s organized.
We introduce most of the common features of JUnit in this chapter.
In chapter 3 we start to build a sample real-life application.
You get to know several design patterns and use them to build our application.
Chapter 4 looks at several important aspects: the need for unit testing, the various flavors of software tests that exist, and the difference between those kinds of tests.
We also give handy advice on how to set up different development and testing environments.
We go on to answer several key questions, such as how to improve your tests, how to improve your test coverage, and how to design your application architecture in such a way that your application will be easily testable.
The last point is a brief introduction to the test-driven development (TDD) approach.
Chapter 6 takes a closer look at stubbing as a technique for faking system resources that normally aren’t available.
We use an example of stubbing a servlet container by using the Jetty embedded servlet container.
This technique is useful when you program against a closed API and you can’t modify or instantiate the available resources.
In this chapter, we give an example of mocking a servlet and testing it by using two of the most popular frameworks, EasyMock and JMock.
Chapter 8 briefly introduces the final technique that we can use when we’re missing important system objects: in-container testing.
Chapter 8 also serves as a summary chapter for this part of the book, so it compares the previously discussed approaches: stubs, mocks, and incontainer testing.
Chapter 9 is the opening chapter for the third part of the book.
In this part, we focus on the integration of JUnit with various build frameworks; specifically in this chapter, we introduce the Ant build framework.
We show you how to execute your tests automatically and how to produce efficient, great-looking reports with the results of the execution.
We run some of the examples from the previous chapter using the Ant framework.
Chapter 10 continues the approach of introducing build frameworks and integrating JUnit with them.
Chapter 11 is dedicated to the theory of continuous integration (CI)—building our project and executing our tests in a continuous manner in order to make sure none of our changes break the project.
We take a closer look at two of the most popular software projects for practicing continuous integration: CruiseControl and Hudson.
We also take the opportunity to import some of our previous examples into both of the tools, set them up, and execute them.
This part deals with various JUnit extensions, which enhance the testing framework to do specific tasks that normally aren’t possible.
Also in this last part of the book, we walk through all the layers of a typical application and explain how to test those layers.
Chapter 12 deals with the presentation layer of a web application.
We introduce the HtmlUnit and Selenium tools and show exactly how to use them.
Chapter 13 continues with the presentation layer of a web application, but this time we focus on one of the hardest parts: Ajax.
We detail what Ajax is and why it’s difficult to test, and we also describe various testing scenarios.
Finally, we introduce the JsUnit project and give some special hints on testing a Google Web Toolkit (GWT) application.
For this purpose, we introduce the first in-container testing framework ever made: the Apache Cactus project.
Chapter 15 reveals techniques that are specifically applicable for testing JSF applications.
Chapter 16 is for those of you who are interested in OSGi applications.
It starts with a brief introduction of what OSGi means.
Then we introduce the JUnit4OSGi extension of JUnit and show several techniques for testing OSGi applications, using both mocking and in-container testing.
Chapter 17 is the first of the last three chapters, which deal with database testing.
Here we tell you everything you need to know about a project called DBUnit.
We demonstrate several techniques for testing your database, regardless of the persistence technology that you use.
Chapter 18 reveals all the secrets of JPA testing: testing multilayered applications and JPA persistence-layer applications.
Here we demonstrate techniques for making your tests more efficient.
We introduce a new project that will help you to test your Spring applications: Unitils.
Code conventions The following typographical conventions are used throughout the book:
In addition, in the code listings you might occasionally find.
This page contains a folder structure of all the submodules for the different chapters.
Each of the subfolders contains a build script to compile and package, and you can execute the tests associated with it.
Instructions on how to install the application are contained in a README file in that download.
We should make a couple of points about the source code.
Initially we wanted to have a large-scale application demonstrating the various testing approaches in the application layers.
Later, we realized the difficulties of having such a large-scale application, and instead we followed the folder-structure notation; each chapter has a source code example associated with it.
All of them contain a Maven build script, and some of them contain an Ant build script as well.
In order to run the examples in the book, you will need to have Maven2 installed on your computer.
Author Online The purchase of JUnit in Action, Second Edition, includes free access to a private forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the authors and other users.
This page provides information on getting on the forum once you’re registered, what kind of help is available, and the rules of conduct in the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue among individual readers and between readers and authors can take place.
It’s not a commitment to any specific amount of participation on the part of the authors, whose contribution to the book’s forum remains voluntary (and unpaid)
We suggest you try asking the authors some challenging questions, lest their interest stray!
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
About the title By combining introductions, overviews, and how-to examples, the In Action books are designed to help with learning and remembering.
According to research in cognitive science, the things people remember are things they discover during selfmotivated exploration.
Although no one at Manning is a cognitive scientist, we’re convinced that for learning to become permanent, it must pass through stages of exploration, play, and, interestingly, retelling of what is being learned.
People understand and remember new things, which is to say they master them, only after actively exploring them.
An essential part of an In Action book is that it is example driven.
It encourages the reader to try things out, to play with new code, and to explore new ideas.
There is another, more mundane reason for the title of this book: our readers are busy.
They use books to do a job or solve a problem.
They need books that allow them to jump in and jump out easily and learn just what they want just when they want it.
The books in this series are designed for such readers.
For many years he has been the Jakarta Cactus lead developer and part of the Apache Maven development team.
In addition, he is also a member of the JCP, leader of the Bulgarian Java User Group (BGJUG), and a frequent speaker at OpenFest, ApacheCON, CommunityONE, and many other conferences.
Born and raised in Bulgaria, Petar graduated with honors in mathematics from Sofia University.
He spent many years working in Germany and the Netherlands for companies like Unic and Hewlett Packard.
Now he is back in lovely Sofia, working predominantly with Phamola, his own company, which assists and advises clients on how to excel through technology.
Vincent is also the CTO of XWiki SAS, a company offering services around the XWiki open source project.
Vincent lives in Paris, France, and can be found online at www.massol.net.
He lives in Los Angeles with his wife, their son, golf clubs, and assorted surfboards.
The same figure appeared on the cover of the first edition of the book, and we have not been successful in the intervening years in finding an accurate translation of the figure caption, in spite of having asked our first edition readers to help out.
Please post any new suggestions in the Author Online forum for the second edition.
General collection of costumes currently used in the nations of the known world, designed and printed with great exactitude by R.M.V.A.R.
This work is very useful especially for those who hold themselves to be universal travelers.
Although nothing is known of the designers, engravers, and workers who colored this illustration by hand, the “exactitude” of their execution is evident in this drawing, which is just one of many in this colorful collection.
Their diversity speaks vividly of the uniqueness and individuality of the world’s towns and regions just 200 years ago.
The collection brings to life a sense of isolation and distance of that period—and of every other historical period except our own hyperkinetic present.
Dress codes have changed since then and the diversity by region, so rich at the time, has faded away.
It is now often hard to tell the inhabitant of one continent from another.
Perhaps, trying to view it optimistically, we have traded a cultural and visual diversity for a more varied personal life.
Or a more varied and interesting intellectual and technical life.
We at Manning celebrate the inventiveness, the initiative, and, yes, the fun of the computer business with book covers based on the rich diversity of regional life of two centuries ago, brought back to life by the pictures from this collection.
Ever since then, the popularity of the framework has been growing, and it’s now the de facto standard for unit testing Java applications.
We cover the newest version of JUnit, 4.6, and we talk about many features that were included after the first edition of the book.
At the same time, we focus on some other interesting techniques in testing your code: mock objects, JUnit extensions, testing different layers of your application, and many more.
We focus on the other tools and techniques later in the book.
The first chapter gives you a quick introduction to the concepts of testing.
You’ll jump straight to the code and see how to write a simple test, execute it, and see the results.
We build a bigger project and walk through the code.
We not only explain the JUnit concepts, widgets, and guts, but we also show you the best practices in writing a test case and demonstrate them with the project we build.
The third chapter is dedicated to tests as a whole.
We describe different kinds of tests and the scenarios to which they apply.
We also explore the various platforms (development, production, and so on) and show you which tests and which scenarios are best to execute there.
The last chapter in this part of the book is dedicated to improving your testing skills.
We show you how to measure your test coverage and how to improve it.
We also explain how to produce testable code before you write your tests as well as how to write the tests before you write a single line of code.
During development, the first thing we do is run our own programmer’s “acceptance test.” We code, compile, and run.
The test may just be clicking a button to see if it brings up the expected menu.
Nevertheless, every day, we code, we compile, we run, and we test.
When we test, we often find issues—especially on the first run.
Most of us quickly develop a pattern for our informal tests: we add a record, view a record, edit a record, and delete a record.
Running a little test suite like this by hand is easy enough to do, so we do it—over and over again.
Never in the field of software development was so much owed by.
It can be a pleasant break from deep thought and hardcoding.
When our little click-through tests finally succeed, there’s a feeling of accomplishment: Eureka! I found it!
Rather than run the test by hand, they prefer to create a small program that runs the test automatically.
Playtesting code is one thing; running automated tests is another.
If you’re a play-test developer, this book is for you.
We’ll show you how creating automated tests can be easy, effective, and even fun.
If you’re already “test-infected,”1 this book is also for you.
Some developers feel that automated tests are an essential part of the development process: you can’t prove a component works until it passes a comprehensive series of tests.
Two developers felt that this type of unit testing was so important that it deserved its own framework.
In 1997, Erich Gamma and Kent Beck created a simple but effective unit testing framework for Java, called JUnit.
Their work followed the design of an earlier framework Kent Beck had created for Smalltalk, called SUnit.
Developers incorporate the framework into their own application and extend it to meet their specific needs.
Frameworks differ from toolkits by providing a coherent structure, rather than a simple set of utility classes.
Erich Gamma is one of the Gang of Four who gave us the now-classic Design Patterns book.3 We know Kent Beck equally well for his groundbreaking work in the software discipline known as Extreme Programming (http://www.extremeprogramming.org)
JUnit (http://www.junit.org) is open source software, released under IBM’s Common Public License Version 1.0 and hosted on SourceForge.
The Common Public License is business friendly: people can distribute JUnit with commercial products without a lot of red tape or restrictions.
JUnit quickly became the de facto standard framework for developing unit tests in Java.
The underlying testing model, known as xUnit, is on its way to becoming the standard framework for any language.
The JUnit team did not invent software testing or even the unit test.
Originally, the term unit test described a test that examined the behavior of a single unit of work.
For example, IEEE has defined unit testing as “Testing of individual hardware or software units or groups of related units” (emphasis added).4
In this book, we use the term unit test in the narrower sense of a test that examines a single unit in isolation from other units.
We focus on the type of small, incremental tests that programmers apply to their own code.
Here’s a generic description of a typical unit test from our perspective: “Confirm that the method accepts the expected range of input and that the method returns the expected value for each input.”
This description asks us to test the behavior of a method through its interface.
If we give it value x, will it return value y? If we give it value z instead, will it throw the proper exception?
Within a Java application, the “distinct unit of work” is often (but not always) a single method.
By contrast, integration tests and acceptance tests examine how various components interact.
A unit of work is a task that isn’t directly dependent on the completion of any other task.
Unit tests often focus on testing whether a method follows the terms of its API contract.
Like a written contract by people who agree to exchange certain goods or services under specific conditions, an API contract is a formal agreement made by the signature of a method.
A method requires its callers to provide specific object references or primitive values and returns an object reference or primitive value.
If the method can’t fulfill the contract, the test should throw an exception, and we say that the method has broken its contract.
In this chapter, we walk through creating a unit test for a simple class from scratch.
We start by writing a test and its minimal runtime framework, so you can see how we used to do things.
Then we roll out JUnit to show you how the right tools can make life much simpler.
Often the unit tests help define the API contract by demonstrating the expected behavior.
The notion of an API contract stems from the practice of, popularized by the Eiffel programming language (http://archive.eiffel.com/doc/manuals/technology/contract)
For our first example, we create a simple calculator class that adds two numbers.
Our calculator provides an API to clients and doesn’t contain a user interface; it’s shown in listing 1.1
Although the documentation isn’t shown, the intended purpose of the Calculator’s add(double, double) method is to take two doubles and return the sum as a double.
The compiler can tell us that it compiles, but we should also make sure it works at runtime.
A core tenet of unit testing is, “Any program feature without an automated test doesn’t exist.”5 The add method represents a core feature of the calculator.
What’s missing is an automated test that proves our implementation works.
We don’t even have a user interface with which to enter a pair of doubles.
We could write a small command-line program that waited for us to type in two double values and then displayed the result.
Then we’d also be testing our own ability to type numbers and add the result ourselves.
This is much more than what we want to do.
We want to know if this unit of work will add two doubles and return the correct sum.
We don’t want to test whether programmers can type numbers!
Meanwhile, if we’re going to go to the effort of testing our work, we should also try to preserve that effort.
It’s good to know that the add(double,double) method worked when we wrote it.
But what we really want to know is whether the method will.
Isn’t the add method too simple to break? The current implementation of the add method is too simple to break.
If add were a minor utility method, then we might not test it directly.
In that case, if add did fail, then tests of the methods that used add would fail.
The add method would be tested indirectly, but tested nonetheless.
In the context of the calculator program, add isn’t just a method; it’s a program feature.
In order to have confidence in the program, most developers would expect there to be an automated test for the add feature, no matter how simple the implementation appears.
In some cases, we can prove program features through automatic functional tests or automatic acceptance tests.
Starting from scratch work when we ship the rest of the application or whenever we make a subsequent modification.
If we put these requirements together, we come up with the idea of writing a simple test program for the add method.
The test program could pass known values to the method and see if the result matches our expectations.
We could also run the program again later to be sure the method continues to work as the application grows.
What’s the simplest possible test program we could write? What about the CalculatorTest program shown in listing 1.2?
It creates an instance of Calculator, passes it two numbers, and checks the result.
If the result doesn’t meet our expectations, we print a message on standard output.
If we compile and run this program now, the test will quietly pass, and all will seem well.
But what happens if we change the code so that it fails? We’ll have to watch the screen carefully for the error message.
We may not have to supply the input, but we’re still testing our own ability to monitor the program’s output.
The conventional way to signal error conditions in Java is to throw an exception.
Let’s throw an exception instead to indicate a test failure.
Meanwhile, we may also want to run tests for other Calculator methods that we haven’t written yet, like subtract or multiply.
Moving to a modular design would make it easier to catch and handle exceptions as well as extend the test program later.
Working from listing 1.3, at B we move the test into its own testAdd method.
It’s now easier to focus on what the test does.
We can also add more methods with more unit tests later, without making the main method harder to maintain.
At C, we change the main method to print a stack trace when an error occurs and then, if there are any errors, end by throwing a summary exception.
Now that you’ve seen a simple application and its tests, you can see that even this small class and its tests can benefit from the bit of scaffolding code we’ve created to run and manage test results.
As an application gets more complicated and tests more involved, continuing to build and maintain our own custom testing framework becomes a burden.
Next, we take a step back and look at the general case for a unit testing framework.
These seemingly minor improvements in the CalculatorTest program highlight three rules that (in our experience) all unit testing frameworks should follow:
The “slightly better” test program comes close to following these rules but still falls short.
For example, in order for each unit test to be truly independent, each should run in a different class instance and ideally in a different class loader instance.
We can now add new unit tests by adding a new method and then adding a corresponding try/catch block to main.
This is a step up, but it’s still short of what we’d want in a real unit test suite.
Our experience tells us that large try/catch blocks cause maintenance problems.
We could easily leave out a unit test and never know it!
It would be nice if we could add new test methods and continue working.
But how would the program know which methods to run? Well, we could have a simple.
A registration method would at least inventory which tests are running.
Another approach would be to use Java’s reflection and introspection capabilities.
A program could look at itself and decide to run whatever methods follow a certain naming convention—like those that begin with test, for example.
Making it easy to add tests (the third rule in our earlier list) sounds like another good rule for a unit testing framework.
The support code to realize this rule (via registration or introspection) wouldn’t be trivial, but it would be worthwhile.
There’d be a lot of work up front, but that effort would pay off each time we added a new test.
It also supports using a different class instance and class loader instance for each test and reports all errors on a test-by-test basis.
Now that you have a better idea of why you need a unit testing framework, let’s look specifically at JUnit.
The JUnit team has defined three discrete goals for the framework:
Next, before we get into the action, we’ll show you how to set up JUnit.
In order to use JUnit to write your application tests, you need to add the JUnit JAR file to your project’s compilation classpath and to your execution classpath.
JUnit contains several test samples that you’ll run to get familiar with executing JUnit tests.
Unzip the distribution zip file to a directory on your computer system (for example, C:\ on Windows or /opt/ on UNIX)
You’re now ready to run the tests provided with the JUnit distribution.
JUnit comes complete with Java programs that you can use to view the result of a test, including a text-based test runner with console output (figure 1.2)
The AllTests class contains a main method to execute the sample tests:
Notice that the JUnit text test runner displays passing tests with a dot.
In part 3 of the book, we look at running tests using the Ant build tool and also the.
JUnit has many features that make it easy to write and run tests.
Separate test class instances and class loaders for each unit test to avoid side effects.
Without further ado, let’s turn to listing 1.4 and see what the simple Calculator test looks like when written with JUnit.
Figure 1.1 Execution of the JUnit distribution sample tests using the text test runner.
This is a much simpler test; let’s walk through it.
The only restriction is that the class must be public; we can name it whatever we like.
It’s common practice to end the class name with Test.
A best practice is to name test methods following the testXXX pattern.
You can name your methods as you like; as long as they have the @Test annotation, JUnit will execute them.
At D, we start the test by creating an instance of the Calculator class (the “object under test”), and at E, as before, we execute the test by calling the method to test, passing it two known values.
At F, the JUnit framework begins to shine! To check the result of the test, we call an assertEquals method, which we imported with a static import on the first line of the class.
We pass 0 as the delta because we’re adding integers.
When we called the calculator object, we tucked the return value into a local double named result.
If the actual value isn’t equal to the expected value, JUnit throws an unchecked exception, which causes the test to fail.
Most often, the delta parameter can be zero, and we can safely ignore it.
It comes into play with calculations that aren’t always precise, which includes many floatingpoint calculations.
If the actual value is within the range expected - delta and expected + delta, the test will pass.
You may find it useful when doing mathematical computations with rounding or truncating errors or when asserting a condition about the modification date of a file, because the precision of these dates depends on the operating system.
Let’s first compile the code by opening a command shell in that directory and typing the following (we’ll assume we have the javac executable on our PATH):
We’re now ready to start the console test runner, by typing the following:
Figure 1.2 Execution of the first JUnit test CalculatorTest using the text test runner.
In addition, we can run the test automatically through the JUnit framework.
When we run the test from the command line (figure 1.2), we see the amount of time it took and the number of tests that passed.
There are many other ways to run tests, from IDEs like Eclipse to build tools like Ant.
This simple example gives you a taste of the power of JUnit and unit testing.
Every developer should perform some type of test to see if code works.
Developers who use automatic unit tests can repeat these tests on demand to ensure that new code works and doesn’t break existing tests.
Simple unit tests aren’t difficult to create without JUnit, but as tests are added and become more complex, writing and maintaining tests becomes more difficult.
JUnit is a unit testing framework that makes it easier to create, run, and revise unit tests.
In this chapter, we scratched the surface of JUnit by stepping through a simple test.
In chapter 2 we take a closer look at the JUnit framework classes (different annotations and assertion mechanisms) and how they work together to make unit testing efficient and effective.
Our solution is to write or reuse a framework to drive test code that exercises our program’s API.
As our program grows with new classes and new methods to existing classes, we need to grow our test code as well.
Experience has taught us that sometimes classes interact in unexpected ways; we need to make sure that we can run all of our tests at any time, no matter what code changes took place.
The question becomes, how do we run multiple test classes? And how do we find out which tests passed and which ones failed?
In this chapter, we look at how JUnit provides the functionality to answer those questions.
We begin with an overview of the core JUnit concepts—the test class, test suite, and test runner.
We take a close look at the core test runners and the test suite before we revisit our old friend the test class.
Then, in the next chapter, we use an example application to show you how to use these core JUnit concepts.
We demonstrate best practices for writing and organizing test code.
The requirements to define a test class are that the class must be public and contain a zero-argument constructor.
In our example, because we don’t define any other constructors, we don’t need to define the zero-argument constructor; Java creates it for us implicitly.
The requirements to create a test method are that it must be annotated with @Test, be public, take no arguments, and return void.
JUnit creates a new instance of the test class before invoking each @Test method.
This helps provide independence between test methods and avoids unintentional side effects in the test code.
Because each test method runs on a new test class instance, we can’t reuse instance variable values across test methods.
To perform test validation, we use the assert methods provided by the JUnit Assert class.
As you can see from the previous example, we statically import these methods in our test class.
Alternatively, we can import the JUnit Assert class itself, depending on our taste for static imports.
Table 2.1 lists some of the most popular assert methods.
Assert methods with two value parameters follow a pattern worth memorizing: the first parameter (A in the table) is the expected value, and the second parameter (B in the table) is the actual value.
It also provides the same methods with a different signature—without the message parameter.
It’s a best practice to provide an error message for all your assert method calls.
Recall Murphy’s Law and apply it here; when an assertion fails, describe what went wrong in a human-readable message.
Your test suite is a special test runner (or Runner), so you can run it as you would a test class.
Once you understand how a test class, Suite, and Runner work, you’ll be able to write whatever tests you need.
These three objects form the backbone of the JUnit framework.
On a daily basis, you need only write test classes and test suites.
The other classes work behind the scenes to bring your tests to life.
Use a test class to group together tests that exercise common behaviors.
In the remainder of this book, when we mention a test, we mean a method annotated with @Test; when we mention a test case (or test class), we mean a class that holds these test methods—a set of tests.
There’s usually a one-to-one mapping between a production class and a test class.
A test suite is a convenient way to group together tests that are related.
For example, if you don’t define a test suite for a test class, JUnit automatically provides a test suite that includes all tests found in the test class (more on that later)
A suite usually groups test classes from the same package.
We cover these runners later in this chapter and show you how to write your own test runners.
This assert invokes the equals() method on the first object against the second.
Whereas the previous assert method checks to see that A and B have the same value (using the equals method), the assertSame method checks to see if the A and B objects are one and the same object (using the == operator)
Let’s take a closer look at the responsibilities of each of the core objects that make up JUnit; see table 2.2
We can move on to explaining in detail the objects from this table that we’ve not seen yet: the test Runner and test Suite objects.
To run a basic test class, you needn’t do anything special; JUnit uses a test runner on your behalf to manage the lifecycle of your test class, including creating the class, invoking tests, and gathering results.
The next sections address situations that may require you to set up your test to run in a special manner.
One of these situations alleviates a common problem when creating tests: invoking tests with different inputs.
We discuss this specific scenario with an example in the next section before looking at the remaining test runners provided by JUnit.
The Parameterized test runner allows you to run a test many times with different sets of parameters.
Assert Lets you define the conditions that you want to test.
An assert method is silent when its proposition succeeds but throws an exception if the proposition fails.
Test A method with a @Test annotation defines a test.
To run this method JUnit constructs a new instance of the containing class and then invokes the annotated method.
Test class A test class is the container for @Test methods.
Suite The Suite allows you to group test classes together.
To run a test class with the Parameterized test runner, you must meet the following requirements.
The test class must carry the @RunWith annotation with the Parameterized class as its argument b.
You must declare instance variables used in the tests C and provide a method annotated with @Parameters D, here called getTestParameters.
This array length must match the number of arguments of the only public constructor.
In our case, each array contains three elements because the public constructor has three arguments.
Our example uses this method to provide the input and expected output values for the tests.
Because we want to test the add method of our Calculator program, we provide three parameters: expected value and two values that we add together.
At E we specify the required constructor for the test.
Note that this time our test case doesn’t have a no-argument constructor but instead has a constructor that accepts parameters for the test.
At F we finally implement the sum @Test method, which instantiates the Calculator program G, and assert calls for the parameters we’ve provided H.
Running this test will loop exactly as many times as the size of the collection returned by the @Parameters method.
The execution of this single test case has the same result as the execution of the following test cases with different parameters:
It’s worth stepping through the JUnit runtime to understand this powerful feature: JUnit calls the static method getTestParameters D.
Next, JUnit loops for each array in the getTestParameters collection D.
If there is more than one public constructor, JUnit throws an assertion error.
JUnit then calls the constructor E with an argument list built from the array.
When you compare the test results with the previous example, you see that instead of running one test, the parameterized JUnit test runner ran the same method three times, once for each value in our @Parameters collection.
The JUnit class Parameterized is one of JUnit’s many test runners.
A test runner allows you to tell JUnit how a test should be run.
When you’re first writing tests, you want them to run as quickly and easily as possible.
You should be able to make testing part of the development cycle: code-run-test-code (or test-code run-test if you’re test-first inclined)
There are IDEs and compilers for quickly building and running applications; JUnit lets you build and run tests.
It will start the test case as a JUnit 3.8 test case.
The Suite is also a runner that executes all the @Test annotated methods in a test class.
JUnit will use a default test runner if none is provided based on the test class.
If you want JUnit to use a specific test runner, specify the test runner class using the @RunWith annotation, as demonstrated in the following code:
Now that we’ve seen an overview of the different test runners and how to direct JUnit to use them, we look at various test runners in more detail.
Before JUnit 4, JUnit included Swing and AWT test runners; these are no longer included.
Those graphical test runners had a progress indicator running across the screen, known as the famous JUnit green bar.
JUnit testers tend to refer to passing tests as green bar and failing tests as red bar.
Figure 2.1 shows the Eclipse JUnit view after a green-bar test run.
Unlike other elements of the JUnit framework, there is no Runner interface.
To create your own test runner, you’ll need to extend the Runner class.
Please refer to appendix B, where we cover this topic in detail.
The definition is taken from the Portland Pattern Repository: http://c2.com/cgi/wiki?FacadePattern.
The test should run fine assuming the classpath is configured properly.
This is simple enough—at least as far as running a single test case is concerned.
The next step is to run more than one test class.
The Suite is a container used to gather tests for the purpose of grouping and invocation.
JUnit designed the Suite to run one or more test cases.
The test runner launches the Suite; which test case to run is up to the Suite.
You might wonder how you managed to run the example at the end of chapter 1, when you didn’t define a Suite.
To keep simple things simple, the test runner automatically creates a Suite if you don’t provide one of your own.
The default Suite scans your test class for any methods that you annotated with @Test.
Internally, the default Suite creates an instance of your test class for each @Test method.
JUnit then executes every @Test method independently from the others to avoid potential side effects.
If you add another test to the CalculatorTest class, like testSubtract, and you annotate it with the @Test, the default Suite would automatically include it.
The Suite object is a Runner that executes all of the @Test annotated methods in the test class.
Listing 2.3 shows how to compose multiple test classes in a single test suite.
All the @Test methods from these classes will be included in the Suite.
For the CalculatorTest in listing 2.1, you can represent the default Suite like this:
Because of the clever way JUnit is constructed, it’s possible to create a suite of test suites.
For example, listing 2.4 concatenates various files to show how test cases make up suites, which in turn make up a master suite.
Our simple test suites TestSuiteA and TestSuiteB have only one test case each, a simplification to abbreviate this example.
A real suite would contain more than one test class, like our master suite.
You can run any of the classes in this listing as a JUnit test, one of the two test classes, one of the two test suites, or the master test suite.
Figure 2.2 displays the result of running the master suite in Eclipse.
Test suites provide a powerful way to organize your tests.
The convenience isn’t unique to JUnit, as you’ll see in the next section, which will make us reconsider creating any JUnit suites at all.
Ant and Maven also provide ways to run groups of test classes and suites by allowing you to specify, with a type of regular expression, the names of test classes and suites to run.
In addition, IDEs like Eclipse allow you to run all test classes and Suites in a selected package or source directory.
This is enough to make us reconsider whether it’s worth creating JUnit Suites in the first place.
JUnit Suites are useful if you want to organize your tests in Java, independent of the capability of your build system, because it’s common for someone or a group other than the developers to maintain builds.
Similarly, you may wish to provide independence from any given IDE and its JUnit integration capabilities.
In this chapter, we introduced the core JUnit concepts and classes.
We showed you how to fuse a test class, a Suite, and a Runner.
Use a test class to test one domain object where each test method focuses on one domain method or a specific set of methods.
JUnit 4 makes extensive uses of annotations to define and manage tests.
You use a test suite to group related test classes together, allowing you to invoke them as a group.
You use a Runner to invoke unit tests and test suites.
In the next chapter, we introduce the Controller design pattern and build a sample Controller component application that we test with JUnit.
This way, we not only show you how to use the JUnit components we’ve been discussing so far, but we also introduce many JUnit best practices.
So far, we’ve made a JUnit survey and shown how to use it (chapter 1)
We also looked at JUnit internals, what the core classes and methods are, and how they interact with each other (chapter 2)
We now dive deeper by introducing a real-life component and testing it.
In this chapter, we implement a small application using the Controller design pattern.
We then test every part of the application using JUnit.
We also look at JUnit best practices when writing and organizing your tests.
Core Java EE Patterns describes a controller as a component that “interacts with a client, controlling and managing the handling of each request,” and tells us that it’s used in both presentation-tier and business-tier patterns.1
You’ll find controllers to be handy in a variety of applications.
For example, in a presentation-tier pattern, a web controller accepts HTTP requests and extracts HTTP parameters, cookies, and HTTP headers, perhaps making the HTTP elements easily accessible to the rest of the application.
A web controller determines the appropriate business logic component to call based on elements in the request, perhaps with the help of persistent data in the HTTP session, a database, or some other resource.
The Apache Struts framework is an example of a web controller.
Given the many uses for a controller, it’s no surprise that controllers crop up in a number of enterprise architecture patterns, including Page Controller, Front Controller, and Application Controller.2 The controller you’ll design here could be the first step in implementing any of these classic patterns.
Let’s work through the code for the simple controller, to see how it works, and then try a few tests.
If you’d like to follow along and run the tests as you go, all the source code for this chapter is available at SourceForge (http://junitbook.sf.net)
See appendix A for more about setting up the source code.
Looking over the description of a controller, four objects pop out: the Request, the Response, the RequestHandler, and the Controller.
Introducing the controller component a Request, dispatches a RequestHandler, and returns a Response object.
With a description in hand, you can code some simple starter interfaces, like those shown in listing 3.1
First, define a Request interface with a single getName method that returns the request’s unique name B, so you can differentiate one request from another.
As you develop the component, you’ll need other methods, but you can add those as you go along.
To begin coding, you need only return a Response object.
What the Response encloses is something you can deal with later.
For now, you need a Response type you can plug into a signature.
The next step is to define a RequestHandler that can process a Request and return your Response D.
RequestHandler is a helper component designed to do most of the dirty work.
It may call on classes that throw any type of exception.
Define a top-level method for processing an incoming request E.
After accepting the request, the controller dispatches it to the appropriate RequestHandler.
This method is at the top of the control stack and should catch and cope with all errors internally.
If it did throw an exception, the error would usually go up to the Java Virtual Machine (JVM) or servlet container.
The JVM or container would then present the user with one of those nasty white pages.
The addHandler method allows you to extend the Controller without modifying the Java source.
Design patterns in action: Inversion of Control Registering a handler with the controller is an example of Inversion of Control.
You may know this pattern as the Hollywood Principle, or “Don’t call us, we’ll call you.” Objects register as handlers for an event.
When the event occurs, a hook method on the registered object is invoked.
Inversion of Control lets frameworks manage the event lifecycle while allowing developers to plug in custom handlers for framework events.3
Next, add a protected method, getHandler, to fetch the RequestHandler for a given request C.
Java doesn’t require you to declare the RuntimeException in the method’s signature, but you can still catch it as an exception.
Your utility method then returns the appropriate handler to its caller E.
The processRequest method F is the core of the Controller class.
If an exception bubbles up, it’s caught in the ErrorResponse class, shown in listing 3.3
Finally, check to see whether the name for the handler has been registered G, and throw an exception if it has.
Looking at the implementation, note that the signature passes the request object, but you use only its name.
This sort of thing often occurs when an interface is defined before the code is written.
One way to avoid overdesigning an interface is to practice test-driven development (see chapter 5)
At this point, you have a crude but effective skeleton for the controller.
Table 3.1 shows how the requirements at the top of this section relate to the source code.
The next step for many developers would be to cobble up a stub application to go with the skeleton controller.
As test-infected developers, we can write a test suite for the controller without fussing with a stub application.
We can write a package and verify that it works, all outside a conventional Java application.
If we don’t write an automatic test now, the Bureau of Extreme Programming will be asking for our membership cards back!
Let’s do the same with the new set of unit tests.
How about a test case that instantiates the DefaultController class? The first step in doing anything useful with the controller is to construct it, so let’s start there.
It constructs the DefaultController object and sets up a framework for writing tests.
Start the name of the test case class with the prefix Test B.
The naming convention isn’t required, but by doing so, we mark the class as a test case so that we can easily recognize test classes and possibly filter them in build scripts.
Alternatively, and depending on your native language, you may prefer to postfix class names with Test.
Next, use the @Before annotated method to instantiate DefaultController C.
This is a built-in extension point that the JUnit framework calls between test methods.
At D you insert a dummy test method, so you have something to run.
As soon as you’re sure the test infrastructure is working, you can begin adding real test methods.
Use a best practice by throwing an exception for test code that you haven’t implemented yet E.
This prevents the test from passing and reminds you that you must implement this code.
Now that you have a bootstrap test, the next step is to decide what to test first.
This helps you to extract all of your common logic, like instantiating your domain objects and setting them up in some known state.
You can have as many of these methods as you want, but beware that if you have more than one of the @Before/@After methods, the order of their execution is not defined.
The methods that you annotate will get executed, only once, before/after all of your @Test methods.
Now that you have a bootstrap test, the next step is to decide what to test first.
We started the test case with the DefaultController object, because that’s the point of this exercise: to create a controller.
But how can you test to see if it works?
The purpose of the controller is to process a request and return a response.
But before you process a request, the design calls for adding a RequestHandler to do the processing.
So, first things first: you should test whether you can add a RequestHandler.
The tests you ran in chapter 1 returned a known result.
To see if a test succeeded, you compared the result you expected with whatever result the object you were testing returned.
To add a RequestHandler, you need a Request with a known name.
To check to see if adding it worked, you can use the getHandler method from DefaultController, which uses this signature:
This is possible because the getHandler method is protected, and the test classes are located in the same package as the classes they’re testing.
This is one reason to define the tests under the same package.
For the first test, it looks like you can do the following:
The next question is, “Where do these objects come from?” Should you go ahead and write some of the objects you’ll use in the application, such as a logon request?
The point of unit testing is to test one object at a time.
In an object-oriented environment like Java, you design objects to interact with other objects.
To create a unit test, it follows that you need two flavors of objects: the domain object you’re testing and test objects to interact with the object under test.
If you used another domain object, like a logon request, and a test failed, it would be hard to identify the culprit.
You might not be able to tell whether the problem was with the controller or the request.
So, in the first series of tests, the only class you’ll use in production is DefaultController.
If the classes are simple and likely to stay that way, then it’s easiest to code them as inner classes.
First, set up a request object B that returns a known name (Test)
The interface calls for a process method, so you have to.
JUnit best practices: unit test one object at a time A vital aspect of unit tests is that they’re finely grained.
A unit test independently examines each object you create, so that you can isolate problems as soon as they occur.
If you put more than one object under test, you can’t predict how the objects will interact when changes occur to one or the other.
When an object interacts with other complex objects, you can surround the object under test with predictable test objects.
Another form of software test, integration testing, examines how working objects interact with each other.
See chapter 4 for more about other types of tests.
You’re not testing the process method right now, so you have it return a SampleResponse object to satisfy the signature.
Go ahead and define an empty SampleResponse D so you have something to instantiate.
Pick an obvious name for the test method, and annotate your test method with the @Test annotation B.
This code gets to the point of the test: controller (the object under test) adds the test handler D.
Read back the handler under a new variable name E, and check to see if you get back the same object you put in F.
JUnit best practices: choose meaningful test method names You can see that a method is a test method by the @Test annotation.
You also must be able to understand what a method is testing by reading the name.
Although JUnit doesn’t require any special rules for naming your test methods, a good rule is to start with the testXXX naming scheme, where XXX is the name of the domain method to test.
As you add other tests against the same method, move to the testXXXYYY scheme, where YYY describes how the tests differ.
Don’t be afraid that the names of your tests are getting long or verbose.
As you’ll see by the end of the chapter, it’s sometimes not so obvious what a method is testing by looking at its assert methods.
Name your test methods in a descriptive fashion, and add comments where necessary.
Let’s test it! Although it’s simple, this unit test confirms the key premise that the mechanism for storing and retrieving RequestHandler is alive and well.
If addHandler or getRequest fails in the future, the test will quickly detect the problem.
As you create more tests like this, you’ll notice that you follow a pattern:
Set up the test by placing the environment in a known state (create objects, acquire resources)
The pretest state is referred to as the test fixture.
Confirm the result, usually by calling one or more assert methods.
Let’s look at testing the core purpose of the controller, processing a request.
Because you know the routine, we present the test in listing 3.7 and review it.
First, annotate the test with the @Test annotation and give the test a simple, uniform name B.
Set up the test objects and add the test handler C.
At D the code diverges from listing 3.6 and calls the processRequest method.
You verify that the returned Response object isn’t null E.
This is important because you call the getClass method on the Response object.
Once again, compare the result of the test against the expected SampleResponse class F.
At the same time, you don’t want to move it into a new @Before method because you aren’t sure which method will be executed first, and you may get an exception.
Instead, you can move it into the same @Before method.
As you add more test methods, you may need to adjust what you do in the @Before methods.
For now, eliminating duplicate code as soon as possible helps you write more tests more quickly.
JUnit best practices: explain the failure reason in assert calls Whenever you use any of the JUnit assert* methods, make sure you use the signature that takes a String as the first parameter.
This parameter lets you provide a meaningful description that’s displayed in the JUnit test runner if the assert fails.
Not using this parameter makes it difficult to understand the reason for a failure when it happens.
We move the instantiation of the test Request and RequestHandler objects to initialize B.
This saves us from repeating the same code in testAddHandler C and testProcessRequest D.
Also, we make a new @Before annotated method for adding the handler to the controller.
Note that you don’t try to share the setup code by testing more than one operation in a test method, as shown in listing 3.9 (an anti-example)
Each test method must be as clear and focused as possible.
When we wrote the testProcessRequest method in listing 3.7, we wanted to confirm that the response returned is the expected response.
The implementation confirms that the object returned is the object that we expected.
But what we’d like to know is whether the response returned equals the expected response.
What’s important is whether the class identifies itself as the correct response.
The assertSame method confirms that both references are to the same object.
The assertEquals method utilizes the equals method, inherited from the base Object class.
To see if two different objects have the same identity, you need to provide your own definition of identity.
For an object like a response, you can assign each response its own command token (or name)
JUnit best practices: one unit test equals one @Test method Don’t try to cram several tests into one method.
The result will be more complex test methods, which will become increasingly difficult to read and understand.
Worse, the more logic you write in your test methods, the more risk there is that they won’t work and will need debugging.
This slippery slope can end with writing tests to test your tests!
Unit tests give you confidence in a program by alerting you when something that had worked now fails.
If you put more than one unit test in a method, it becomes more difficult to zoom in on exactly what went wrong.
When tests share the same method, a failing test may leave the fixture in an unpredictable state.
Other tests embedded in the method may not run or may not run properly.
Your picture of the test results will often be incomplete or even misleading.
Because all the test methods in a test class share the same fixture, and JUnit can now generate an automatic test suite, it’s just as easy to place each unit test in its own method.
If you need to use the same block of code in more than one test, extract it into a utility method that each test method can call.
Better yet, if all methods can share the code, put it into the fixture.
Another common pitfall is to write test methods that don’t contain any assert statements.
When you execute those tests, you see JUnit flag them as successful, but this is an illusion of successful tests.
The only time when not using assert calls may be acceptable is when an exception is thrown to indicate an error condition.
For best results, your test methods should be as concise and focused as your domain methods.
Let’s test it! The empty implementation of SampleResponse didn’t have a name property you could test.
To get the test you want, you have to implement a little more of the Response class first.
Now that SampleResponse has an identity (represented by getName()) and its own equals method, you can amend the test method:
We’ve introduced the concept of identity in the SampleResponse class for the purpose of the test.
But the tests are telling you that this should have existed in the proper Response class.
As you see, tests can sometimes “talk” and guide you to a better design of your application.
To do this we need to test every condition under which our application might be executed.
We start investigating the exceptional conditions in the next chapter.
So far, your tests have followed the main path of execution.
If the behavior of one of your objects under test changes in an unexpected way, this type of test points to the root of the problem.
In essence, you’ve been writing diagnostic tests that monitor the application’s health.
Your diagnostics may test whether you’re following the database’s API.
If you open a connection but don’t close it, a diagnostic can note that you’ve failed to meet the expectation that all connections must be closed after use.
But what if a connection isn’t available? Maybe the connection pool is empty.
If the database server is configured properly and you have all the resources you need, this may never happen.
All resources are finite, and someday, instead of a connection, you may be handed an exception.
If you’re testing an application by hand, one way to test for this sort of thing is to turn off the database while the application is running.
Forcing error conditions is an excellent way to test your disaster-recovery capability.
Most of us can’t afford to do this several times a day—or even once a day.
In addition, many other error conditions aren’t easy to create by hand.
Testing the main path of execution is a good thing—and a requirement.
Testing exception handling is just as important and should also be included as a requirement.
JUnit best practices: test anything that could possibly fail Unit tests help ensure that your methods are keeping their API contracts with other methods.
If the contract is based solely on other components’ keeping their contracts, then there may not be any useful behavior for you to test.
But if the method changes the parameter’s or field’s value in any way, then you’re providing unique behavior that you should test.
The method is no longer a simple go-between—it’s a method with its own behavior that future changes could conceivably break.
If a method is changed such that it isn’t simple anymore, then you should add a test when that change takes place but not before.
As the JUnit FAQ puts it, “The general philosophy is this: if it can’t break on its own, it’s too simple to break.” This is also in keeping with the Extreme Programming rule: “No functionality is added early.”
We’re all human, and often we tend to be sloppy when it comes to exception cases.
Even textbooks scrimp on error handling so as to simplify the examples.
As a result, many otherwise great programs aren’t error proofed before they go into production.
If properly tested, an application should not expose a screen of death but should trap, log, and explain all errors gracefully.
Unit tests can simulate exceptional conditions as easily as normal conditions.
Other types of tests, like functional and acceptance tests, work at the production level.
Whether these tests encounter systemic errors is often a matter of happenstance.
During our original fit of inspired coding, we had the foresight to code an error handler into the base classes.
As you saw back in listing 3.2, the processRequest method traps all exceptions and passes back a special error response instead:
If you’re coding them by hand in a text editor, then yes, you might want to test them.
It’s surprisingly easy to miscode a setter in a way that the compiler won’t catch.
But if you’re using an IDE that watches for such things, then your team might decide not to test simple JavaBean properties.
This leaves creating a test method that registers the handler and tries processing a request—for example, like the one shown in listing 3.12
If you ran this test through JUnit, it would fail! A quick look at the message tells you two things.
First, you need to use a different name for the test request, because there’s already a request named Test in the fixture.
Second, you may need to add more exception handling to the class so that a RuntimeException isn’t thrown in production.
As to the first item, you can try using the request object in the fixture instead of your own, but that fails with the same error.
Moral: Once you have a test, use it to explore alternative coding strategies.
If you remove from the fixture the code that registers a default SampleRequest and SampleHandler, you introduce duplication into the other test methods—not good.
Better to fix the SampleRequest so it can be instantiated under different names.
Introduce a member field to hold the request’s name and set it to the previous version’s default B.
Next, introduce a new constructor that lets you pass a name to the request C, to override the default.
At D you introduce an empty constructor, so existing calls will continue to work.
Finally, call the new constructor instead E, so the exceptional request object doesn’t conflict with the fixture.
If you added another test method that also used the exception handler, you might move its instantiation to the @Before fixture, to eliminate duplication.
Because the duplication hasn’t happened yet, let’s resist the urge to anticipate change and let the code stand (Extreme Programming’s “No functionality is added early” rule)
JUnit best practices: let the test improve the code Writing unit tests often helps you write better code.
The reason is simple: a test case is a user of your code.
It’s only when using code that you find its shortcomings.
Don’t hesitate to listen to your tests and refactor your code so that it’s easier to use.
By writing the tests first, you develop your classes from the point of view of a user of your code.
During testing, you found that addHandler throws an undocumented RuntimeException if you try to register a request with a duplicate name.
By undocumented, we mean that it doesn’t appear in the signature.
Looking at the code, you see that getHandler throws a RuntimeException if the request has not been registered.
Whether you should throw undocumented RuntimeException exceptions is a larger design issue.
For now, let’s write some tests that prove the methods will behave as designed.
Listing 3.14 shows two test methods that prove addHandler and getHandler will throw runtime exceptions when expected.
Annotate your method with the @Test annotation to denote that it’s a test method B.
Because we’re going to test an exceptional condition and we expect that the test method will produce an exception of some kind, we need also to specify what kind of an exception we expect to be raised.
We do this by specifying the expected parameter of the @Test annotation.
Because this test represents an exceptional case, append NotDefined to the standard testGetHandler prefix.
Doing so keeps all the getHandler tests together and documents the purpose of each derivation.
At D, you create the request object for the test, also giving it an obvious name.
Pass the (unregistered) request to the default getHandler method E.
Because this request has no handler attached, a RuntimeException should be raised.
You follow the same pattern F as the first method:
The controller class is by no means finished, but you have a respectable first iteration and a test suite proving that it works.
Now you can commit the controller package, along with its tests, to the project’s code repository and move on to the next task on your list.
So far, we’ve tested our application for proper functionality—when supplied with the right data, not only does it behave in the expected manner, but it also produces.
JUnit best practices: make exception tests easy to read Normally the expected parameter in the @Test annotation clearly tells the developers  that an exception of that type should be raised.
Besides naming your test methods in an obvious fashion to denote that this method is testing an exceptional condition, you can also place some comments to highlight the line of the code that produces the expected exception.
JUnit best practices: let the test improve the code An easy way to identify exceptional paths is to examine the different branches in the code you’re testing.
By branches, we mean the outcome of if clauses, switch statements, and try-catch blocks.
When you start following these branches, sometimes you may find that testing each alternative is painful.
If code is difficult to test, it’s usually just as difficult to use.
When testing indicates a poor design (called a code smell, http://c2.com/cgi/wiki?CodeSmell), you should stop and refactor the domain code.
A test is your code’s first “customer,” and as the maxim goes, “the customer is always right.”
Now we want to look at another aspect of testing our application: scalability.
We’re going to write some tests and expect that they run below a given time barrier.
To do this, JUnit provides us with another parameter to the @Test annotation called timeout.
In this parameter, you can specify your time barrier in terms of milliseconds, and if the test takes more time to execute, JUnit will mark the test as failed.
For example, let’s look at the code in listing 3.15
We start by specifying the timeout parameter in milliseconds, which we expect to be our time barrier B.
Then C we declare the Request, Response, and RequestHandler objects we’re going to use in the test.
At D we start a for loop to create 99,999 SampleRequest objects  and add them along with a handler to the controller.
After that, we invoke the processRequest() method of the controller and assert E that we get a nonnull Response object and also that the Response we get isn’t an ErrorResponse.
You might consider the 130 milliseconds time barrier to be optimistic, and you’re right.
This time barrier was the lowest possible on my machine.
But the execution time depends on the hardware it runs on (processor speed, memory available, and so on) and also on the software it runs on (mainly the operating system, but also the Java version, and so on)
Further, when adding more functionality in the processRequest() method, the time barrier we’ve chosen will become insufficient for our needs.
We get to the point where a few timeout tests might fail the whole build for some developers.
In JUnit 3.x we had to change the name of the test method (to not start with the test prefix)
As you can see, the only thing we’ve added is the @Ignore annotation B to the method.
This annotation accepts the value parameter, which lets us insert a message as to why we skip the test.
As we mentioned, in JUnit 3 the only way to skip the execution of a test method was to rename it or comment it out.
This gives you no information whatsoever as to how many tests were skipped.
The statistics show that people are easily infected with the unit testing philosophy.
Once you get accustomed to writing tests and see how good it feels to have someone protecting you from possible mistakes, you’ll wonder how it was possible to live without unit testing before.
As you write more and more unit tests and assertions, you’ll inevitably encounter the problem that some of the assertions are big and hard to read.
JUnit best practice: always specify a reason for skipping a test As you saw in the previous listing, we specified why we needed to skip the execution of the test.
First, you notify your fellow developers why you want to skip the execution of the test, and second, you prove to yourself that you know what the test does, and you don’t ignore it just because it fails.
What we do in this example is construct a simple JUnit test, exactly like the ones we’ve been constructing so far.
We have a @Before fixture B, which will initialize some data for our test, and then we have a single test method C.
In this test method you can see that we make a long and hard-to-read assertion D (maybe it’s not that hard to read, but it’s definitely not obvious what it does at first glance)
Our goal is to simplify the assertion we make in the test method.
To solve this problem we’re going to present a library of matchers for building test expressions.
Hamcrest (http://code.google.com/p/hamcrest/) is a library that contains a lot of helpful matcher objects (known also as constraints or predicates), ported in several languages (Java, C++, Objective-C, Python, and PHP)
Note that Hamcrest isn’t a testing framework itself, but rather it helps you declaratively specify simple matching rules.
These matching rules can be used in many different situations, but they’re particularly helpful for unit testing.
Listing 3.18 is the same test method, this time written using the Hamcrest library.
Here we reuse listing 3.17 and add another test method to it.
This time we import the needed matchers and the assertThat method B, and after that we construct a test method.
In the test method we use one of the most powerful features of the matchers—they can nest within each other C.
Introducing Hamcrest matchers with or without Hamcrest matchers is a personal preference.
What Hamcrest gives you that standard assertions don’t provide is a human-readable description of an assertion failure.
If you followed the examples in the two previous listings, you’ve probably noticed that in both the cases we construct a List with the “x”, “y”, and “z” as elements in it.
After that we assert the presence of either “one”, “two”, or “three”, which means that the test, as written, will fail.
The result from the execution is shown in figure 3.1
As you can see from the two screens, the one on the right gives a lot more details, doesn’t it? Table 3.2 lists some of the most commonly used Hamcrest matchers.
Table 3.2 Some of the most commonly used Hamcrest matchers.
Useful in some cases where you want to make the assert statement more readable.
Figure 3.1 The screen on the left shows the stack trace from the execution of the test without using Hamcrest, and the one on the right shows the same thing using Hamcrest.
All of them seem straightforward to read and use, and remember that you can combine them with each other.
It’s easy to write your own matchers that check a certain condition.
The only thing you need to do is implement the Matcher interface and an appropriately named factory method.
You can find more on how to write custom matchers in appendix D of this book, where we provide a complete overview of how to write your own matchers.
Because this chapter covers testing a realistic component, let’s finish up by looking at how you set up the controller package as part of a larger project.
In chapter 1, you kept all the Java domain code and test code in the same folder.
They were introductory tests on an example class, so this approach seemed simplest for everyone.
In this chapter, you’ve begun to build real classes with real tests, as you would for one of your own projects.
Accordingly, you’ve set up the source code repository as you would for a real project.
Mixing this in with the domain classes would not have been a big deal.
But experience tells us that soon you’ll have at least as many test classes as you have domain classes.
Placing all of them in the same directory will begin to create file-management issues.
It will become difficult to find the class you want to edit next.
Test whether given numbers are close to, greater than, greater than or equal to, less than, or less than or equal to a given value.
Table 3.2 Some of the most commonly used Hamcrest matchers (continued)
Meanwhile, you want the test classes to be able to unit test protected methods, so you want to keep everything in the same Java package.
The solution is to have one package in two folders.
Figure 3.2 shows a snapshot of how the directory structure looks in a popular integrated development environment (IDE)
This is the code for the third chapter, so we used ch03-mastering for the top-level project directory name (see appendix C)
Under the root directory, we created separate src/main/java and src/main/test folders.
The working interfaces and classes go under src/main/java; the classes we write for testing only go under the src/main/test directory.
Beyond eliminating clutter, a separate-but-equal directory structure yields several other benefits.
Right now, the only test class has the convenient Test prefix.
Later you may need other helper classes to create more sophisticated tests.
It may not be convenient to prefix all of these classes with Test, and it becomes harder to tell the domain classes from the test classes.
Using a separate test folder also makes it easy to deliver a runtime JAR with only the domain classes.
JUnit best practices: same package, separate directories Put test classes in the same package as the class they test but in a parallel directory structure.
You need tests in the same package to allow access to protected methods.
You want tests in a separate directory to simplify file management and to clearly delineate test and domain classes.
Figure 3.2 A separate-but-equal filing system keeps tests in the same package but in different directories.
As you saw in chapter 1, it isn’t hard to jump in and begin writing JUnit tests for your applications.
In this chapter, we created a test case for a simple but complete application controller.
Rather than test a single component, the test case examined how several components worked together.
We started with a bootstrap test case that could be used with any class.
Then we added new tests to the test case one by one until all of the original components were under test.
Because the assertions were getting more and more complicated, we found a way to simplify them by means of the Hamcrest matchers.
We expect this package to grow, so we created a second source code directory for the test classes.
Because the test and domain source directories are part of the same package, we can still test protected and package default members.
In the next chapter, we put unit testing in perspective with other types of tests that you need to perform on your applications.
We also talk about how unit testing fits in the development lifecycle.
Earlier chapters in this book took a pragmatic approach to designing and deploying unit tests.
This chapter steps back and looks at the various types of software tests and the roles they play in the application’s lifecycle.
Why would you need to know all this? Because unit testing isn’t something you do out of the blue.
In order to become a well-rounded developer, you need to understand unit tests compared to functional, integration, and other types of tests.
Once you understand why unit tests are necessary, then you need to know how far to take your tests.
When your program dies, it is an “idiosyncrasy.” Frequently, crashes are.
The main goal of unit testing is to verify that your application works as expected and to catch bugs early.
Although functional testing accomplishes the same goal, unit tests are extremely powerful and versatile and offer much more than verifying that the application works.
Unit tests are the first type of test any application should have.
If you had to choose between writing unit tests and writing functional tests, you should choose the latter.
In our experience, functional tests are able to cover about 70 percent of the application code.
If you wish to go further and provide more test coverage, then you need to write unit tests.
Unit tests can easily simulate error conditions, which is extremely difficult to do with functional tests (it’s impossible in some instances)
Unit tests provide much more than just testing, as explained in the following sections.
Imagine you’re on a team working on a large application.
Unit tests allow you to deliver quality code (tested code) without having to wait for all the other components to be ready.
On the other hand, functional tests are more coarse grained and need the full application (or a good part of it) to be ready before you can test it.
A passing unit test suite confirms your code works and gives you the confidence to modify your existing code, either for refactoring or to add and modify new features.
As a developer, you’ll get no better feeling than knowing that someone is watching your back and will warn you if you break something.
A suite of unit tests reduces the need to debug an application to find out why something is failing.
Whereas a functional test tells you that a bug exists somewhere in the implementation of a use case, a unit test tells you that a specific method is failing for a specific reason.
You no longer need to spend hours trying to find the problem.
Without unit tests, it’s difficult to justify refactoring, because there’s always a relatively high risk that you may break something.
Why would you chance spending hours of debugging time (and putting the delivery at risk) only to improve the implementation or change a method name? Unit tests provide the safety net that gives you the confidence to refactor.
Let’s move on with our implementation and try to improve it further.
Unit tests are a first-rate client of the code they test.
They force the API under test to be flexible and to be unit testable in isolation.
You usually have to refactor your code under test to make it unit testable (or use the TDD approach, which by definition spawns code that can be unit tested; see the next chapter)
It’s important to monitor your unit tests as you create and modify them.
If a unit test is too long and unwieldy, it usually means the code under test has a design smell and you should refactor it.
You may also be testing too many features in one test.
JUnit best practice: refactor Throughout the history of computer science, many great teachers have advocated iterative development.
Niklaus Wirth, for example, who gave us the now-ancient languages Algol and Pascal, championed techniques like stepwise refinement.
For a time, these techniques seemed difficult to apply to larger, layered applications.
Project managers looked to up-front planning as a way to minimize change, but productivity remained low.
The rise of the xUnit framework has fueled the popularity of agile methodologies that once again advocate iterative development.
Agile methodologists favor writing code in vertical slices to produce a working use case, as opposed to writing code in horizontal slices to provide services layer by layer.
When you design and write code for a single use case or functional chain, your design may be adequate for this feature, but it may not be adequate for the next feature.
To retain a design across features, agile methodologies encourage refactoring to adapt the code base as needed.
But how do you ensure that refactoring, or improving the design of existing code, doesn’t break the existing code? This answer is that unit tests tell you when and where code breaks.
In short, unit tests give you the confidence to refactor.
The agile methodologies try to lower project risks by providing the ability to cope with change.
But the foundation on which all these principles rest is a solid bed of unit tests.
If a test can’t verify a feature in isolation, it usually means the code isn’t flexible enough and you should refactor it.
On one side is a 300-page document describing the API, and on the other are some examples showing how to use it.
Unit tests are exactly this: examples that show how to use the API.
Next, we create a new account with a balance of 1000 C and the amount to transfer D.
Unit tests tell you, at the push of a button, if everything still works.
Furthermore, unit tests enable you to gather code-coverage metrics (see the next chapter) showing, statement by statement, what code the tests caused to execute and what code the tests did not touch.
You can also use tools to track the progress of passing versus failing tests from one build to the next.
You can also monitor performance and cause a test to fail if its performance has degraded compared to a previous build.
There are other ways of categorizing software tests, but we find these most useful for the purposes of this book.
Please note that this section is discussing software tests in general, not just the automated unit tests covered elsewhere in the book.
In figure 4.1, the outermost tests are broadest in scope.
As you move from the inner boxes to the outer boxes, the software tests get more functional and require that more of the application be present.
Next, we take a  look at the general test types.
We’ve mentioned that unit tests each focus on a distinct unit of work.
What about testing different units of work combined into a workflow? Will the result of the workflow do what you expect? How well will the application work when many people are using it at once? Different kinds of tests answer these questions; we categorize them into four varieties:
Let’s look at each of the test types, starting with the innermost after unit testing and working our way out.
Examining the interaction between components, possibly running in their target environment, is the job of integration testing.
Table 4.1 describes the various cases under which components interact.
Just as more traffic collisions occur at intersections, the points where objects interact are major contributors of bugs.
Being able to code to the test dramatically increases a programmer’s ability to write well-behaved objects.
For example, a web application contains a secure web page that only authorized clients can access.
If the client doesn’t log in, then trying to access the page should result in a redirect to the login page.
A functional unit test can examine this case by sending an HTTP request to the page to verify that a redirect (HTTP status code 302) response code comes back.
Depending on the application, you can use several types of functional tests, as shown in table 4.2
Objects The test instantiates objects and calls methods on these objects.
Services The test runs while a servlet or EJB container hosts the application, which may connect to a database or attach to any other external resource or device.
Subsystems A layered application may have a front end to handle the presentation and a back end to execute the business logic.
Tests can verify that a request passes through the front end and returns an appropriate response from the back end.
Functional testing within a framework focuses on testing the framework API (from the point of view of end users or service providers)
Functional testing of a GUI verifies that all features can be accessed and provide expected results.
The tests access the GUI directly, which may in turn call several other components or a back end.
There may be a presentation subsystem, a business logic subsystem, and a data subsystem.
Layering provides flexibility and the ability to access the back end with several different front ends.
Each layer defines an API for other layers to use.
Usually, you implement this with software like JMeter,1 which automatically sends preprogrammed requests and tracks how quickly the application responds.
These tests usually don’t verify the validity of responses, which is why we have the other tests.
You normally perform stress tests in a separate environment, typically more controlled than a development environment.
The stress test environment should be as close as possible to the production environment; if not, the results won’t be useful.
Let’s prefix our quick look at performance testing with the often-quoted numberone rule of optimization: “Don’t do it.” The point is that before you spend valuable time optimizing code, you must have a specific problem that needs addressing.
Aside from stress tests, you can perform other types of performance tests within the development environment.
A profiler can look for bottlenecks in an application, which the developer can try to optimize.
You must be able to prove that a specific bottleneck exists and then prove that your changes remove the bottleneck.
Unit tests can also help you profile an application as a natural part of development.
With JUnit, you can create a performance test to match your unit test.
The example uses the timeout parameter B on the @Test annotation to set a timeout in milliseconds on the method.
If this method takes more than 5,000 milliseconds to run, the test will fail.
An issue with this kind of test is that you may need to update the timeout value when the underlying hardware changes, the OS changes, or the test environment changes to or from running under virtualization.
The customer or a proxy usually conducts acceptance tests to ensure that the application has met whatever goals the customer or stakeholder defined.
Usually they start as functional and performance tests, but they may include subjective criteria like “ease of use” and “look and feel.” Sometimes, the acceptance suite may include a subset of the tests run by the developers, the difference being that this time the customer or QA team runs the tests.
For more about using acceptance tests with an agile software methodology, visit the wiki site regarding Ward Cunningham’s fit framework (http://fit.c2.com/)
Writing unit tests and production code takes place in tandem, ensuring that your application is under test from the beginning.
We encourage this process and urge programmers to use their knowledge of implementation details to create and maintain unit tests that can be run automatically in builds.
Using your knowledge of implementation details to write tests is also known as white box testing.
Your application should undergo other forms of testing, starting with unit tests and finishing with acceptance tests, as described in the previous section.
As a developer, you want to ensure that each of your subsystems works correctly.
As you write code, your first tests will probably be logic unit tests.
As you write more tests and more code, you’ll add integration and functional unit tests.
Listing 4.2 Enforcing a timeout on a method with JUnit.
Test types may be working on a logic unit test, an integration unit test, or a functional unit test.
Figure 4.3 illustrates how these three flavors of unit tests interact.
The sliders define the boundaries between the types of unit tests.
Using this type of testing will increase your test code coverage, which will increase your confidence in making changes to the existing code base while minimizing the risk of introducing regression bugs.
Strictly speaking, functional unit tests aren’t pure unit tests, but neither are they pure functional tests.
They’re more dependent on an external environment than pure unit tests are, but they don’t test a complete workflow, as expected by pure.
Table 4.3 Three flavors of unit tests: logic, integration, and functional.
Logic unit test A test that exercises code by focusing on a single method.
You can control the boundaries of a given test method using mock objects or stubs (see part 2 of the book)
Integration unit test A test that focuses on the interaction between components in their real environment (or part of the real environment)
Functional unit test A test that extends the boundaries of integration unit testing to confirm a stimulus response.
For example, a web application contains a secure web page that only authorized clients can access.
If the client doesn’t log in, then trying to access the page should result in a redirect to the login page.
A functional unit test can examine this case by sending an HTTP request to the page to verify that a redirect (HTTP status code 302) response code comes back.
Figure 4.3 Interaction among the three unit test types: functional, integration, and logic.
We put functional unit tests in our scope because they’re often useful as part of the battery of tests run in development.
A typical example is the StrutsTestCase framework (http://strutstestcase.sourceforge.net/), which provides functional unit testing of the runtime Struts configuration.
These tests tell a developer that the controller is invoking the appropriate software action and forwarding to the expected presentation page, but they don’t confirm that the page is present and renders correctly.
Having examined the various types of unit tests, we now have a complete picture of application testing.
We develop with confidence because we’re creating tests as we go, and we’re running existing tests as we go to find regression bugs.
When a test fails, we know exactly what failed and where, and we can then focus on fixing each problem directly.
Before we close this chapter, we focus on one other categorization of software tests: black box and white box testing.
This categorization is intuitive and easy to grasp, but developers often forget about it.
We start by exploring black box testing, with a definition.
The test relies solely on the external system interface to verify its correctness.
As the name of this methodology suggests, we treat the system as a black box; imagine it with buttons and LEDs.
We don’t know what’s inside or how the system operates.
All we know is that by providing correct input, the system produces the desired output.
All we need to know in order to test the system properly is the system’s functional specification.
The early stages of a project typically produce this kind of specification, which means we can start testing early.
Anyone can take part in testing the system—a QA engineer, a developer, or even a customer.
The simplest form of black box testing would try to mimic manually actions on the user interface.
Another, more sophisticated approach would be to use a tool for this task, such as HTTPUnit, HTMLUnit, or Selenium.
We discuss most of these tools in the last part of the book.
At the other end of the spectrum is white box testing, sometimes called glass box testing.
In contrast to black box testing, we use detailed knowledge of the implementation to create tests and drive the testing process.
Not only is knowledge of a component’s implementation required, but also of how it interacts with other components.
For these reasons, the implementers are the best candidates to create white box tests.
Which one of the two approaches should you use? Unfortunately, there’s no correct answer, and we suggest that you use both approaches.
In some situations, you’ll need user-centric tests, and in others, you’ll need to test the implementation details of the system.
By making the customer think about the application, they can also clarify what the system should do.
Another issue is that a valid result on the screen doesn’t always mean the application is correct.
White box tests are usually easier to write and run, but the developers must implement them.
On the other hand, black box tests can bring more value than white box tests.
Although these test distinctions can seem academic, recall that divide and conquer doesn’t have to apply only to writing production software; it can also apply to testing.
We encourage you to use these different types of tests to provide the best code coverage possible, thereby giving you the confidence to refactor and evolve your applications.
Product release cycles are getting shorter, and we need to react to change quickly.
Development must be the art of writing complete and tested solutions.
To accommodate rapid change, we must break with the waterfall model where testing follows development.
Late-stage testing doesn’t scale when change and swiftness are paramount.
When it comes to unit testing an application, you can use several types of unit tests: logic, integration, and functional unit tests.
They also complement the other software tests that are performed by quality assurance personnel and by the customer.
In the next chapter, we continue to explore the world of testing.
We present best practices, like measuring test coverage, writing testable code, and practicing test-driven development (TDD)
This part of the book reveals the various strategies and techniques used in testing.
Here we take a more scientific and theoretical approach to explain the differences.
We describe incorporating mock objects, or stubs, and dive into the details of in-container testing.
The first chapter of this part describes different techniques for improving the quality of your tests—measuring test coverage, practicing test-driven development, and writing testable code.
The sixth chapter of the book is dedicated to stubs.
We look into another solution to isolate the environment and make our tests seamless.
The seventh chapter starts by explaining what mock objects are.
We give a thorough overview of how to construct and use mock objects.
We also give a realworld example showing not only where mock objects fit best but also how to benefit by integrating them with JUnit tests.
The last chapter describes a totally different technique: executing tests inside a container.
This solution is different from the previous ones, and just like them it has its pros and cons.
We start by presenting an overview of what in-container means and how it’s achieved, and at the end of the chapter we compare the mocks/stubs approach to the in-container approach.
In the previous chapters, we introduced testing software and started exploring testing with JUnit.
Now that we’re writing test cases, it’s time to measure how good these tests are by using a test coverage tool to report what code is exercised by the tests and what code is not.
We also discuss how to write code that’s easy to test.
Writing unit tests gives you the confidence to change and refactor an application.
As you make changes, you run tests, which gives you immediate feedback on new features under test and whether your changes break existing tests.
The issue is that these changes may still break existing untested functionality.
In order to resolve this issue, we need to know precisely what code runs when you or the build invokes tests.
Ideally, our tests should cover 100 percent of our application code.
Let’s look in more detail at what benefits test coverage provides.
Using black box testing, we can create tests that cover the public API of an application.
Because we’re using documentation as our guide and not knowledge of the implementation, we don’t create tests, for example, that use special parameter values to exercise special conditions in the code.
One metric of test coverage would be to track which methods the tests call.
This doesn’t tell you whether the tests are complete, but it does tell you if you have a test for a method.
Figure 5.1 shows the partial test coverage typically achieved using only black box testing.
You can write a unit test with intimate knowledge of a method’s implementation.
If a method contains a conditional branch, you can write two unit tests, one for each branch.
Because you need to see into the method to create such a test, this falls under white box testing.
You can achieve higher test coverage using white box unit tests because you have access to more methods and because you can control both the inputs to each method and the behavior of secondary objects (using stubs or mock objects, as you’ll see in later chapters)
Because you can write white box unit tests against protected, packageprivate, and public methods, you get more code coverage.
Cobertura is a code coverage tool that integrates with JUnit.
In order to measure test coverage, Cobertura creates instrumented copies of class files you specify.
This process, called byte-code instrumentation, adds byte codes to existing compiled code to enable logging of what executed byte codes.
Instead of, or in addition to, running the normally compiled unit tests, you run the compiled and instrumented tests.
Define a COBERTURA_HOME environment variable and add it to the execution PATH environment variable.
The COBERTURA_HOME folder contains several command-line scripts we use in this section.
Although our examples drive Cobertura from the command line, note that the program also provides Ant tasks.
We start by compiling our test cases with the following command:
The --destination parameter specifies where to place the instrumented classes.
Next, we run the unit tests against the instrumented code.
Cobertura integrates with JUnit and Ant, but it’s also tool agnostic and can work with any other testing framework.
To run your tests, you need to place two resources on your CLASSPATH:
If you don’t specify this property, Cobertura will create a file called cobertura.ser in the current directory.
After you run these scripts, you’ll get your instrumented classes in the instrumented folder and a code coverage file for a given test run.
The destination parameter specifies the output directory for the report.
The reports folder contains the HTML report shown in figure 5.3
Cobertura shows code coverage not only by package but also by class.
You can select any of the classes in the report to see the extent to which that particular class was tested.
The report shows good test coverage of the squareRoot method in the Calculator class.
On the other hand, we have zero executions of the sum method.
Measuring test coverage code coverage of the Calculator class, indicating that developers need to create more tests.
Depending on how you compose your application, it might not be possible to reach all code in the test environment.
If we can achieve higher test coverage with white box unit tests, and we can generate reports to prove it, do we need to bother with black box tests?
The black box tests in figure 5.1 are verifying interactions between objects.
The white box unit tests in figure 5.2, by definition, don’t test object interactions.
If you want to thoroughly test your application, including how runtime objects interact with each other, you need to use black box integration tests as well as white box tests.
We’ve completed our overview of code coverage and Cobertura to see precisely which parts of an application unit tests exercise.
Let’s now move on to how different implementation techniques affect how to write tests for an application.
This chapter is dedicated to best practices in software testing.
We’re now ready to get to the next level: writing code that’s easy to test.
Sometimes writing a single test case is easy, and sometimes it isn’t.
It all depends on the level of complexity of the application.
A best practice avoids complexity as much as possible; code should be readable and testable.
In this section, we discuss some best practices to improve your architecture and code.
Remember that it’s always easier to write easily testable code than it is to refactor existing code to make it easily testable.
If you change the signature of a public method, then you need to change every call site in the application and unit tests.
Even with refactoring wizards in tools like Eclipse, you must always perform this task with care.
In the open source world, and for any API made public by a commercial product, life can get even more complicated—many people use your code, and you should be careful of the changes you make to stay backward compatible.
Public methods become the articulation points of an application among components, open source projects, and commercial products that usually don’t even know of one another’s existence.
Imagine a public method that takes a distance as a double parameter and a black box test to verify a computation.
At some point, the meaning of the parameter changes from miles to kilometers.
Without a unit test to fail and tell you what’s wrong, you may spend a lot of time debugging and talking to angry customers.
This example illustrates that you must test all public methods.
For nonpublic methods, you need to go to a deeper level and use white box tests.
Your unit tests should instantiate the class you want to test, use it, and assert its correctness.
What happens when your class instantiates, directly or indirectly, a new set of objects? Your class now depends on these classes.
Writing testable code should reduce dependencies as much as possible.
If your classes depend on many other classes that need to be instantiated and set up with some state, then your tests will be complicated—you may need to use some complicated mock-objects solution (see chapter 6 for mock objects)
A solution to reducing dependencies is to separate your code between methods that instantiate new objects (factories) and methods that provide your application logic.
Every time we instantiate the Vehicle object, we also instantiate the Driver object.
The solution would be to have the Driver interface passed to the Vehicle class, as in listing 5.2
This allows us to produce a mock Driver object (see chapter 6) and pass it to the Vehicle class on instantiation.
By striving for better test coverage, we add more and more test cases.
In each of these test cases, we do the following:
By doing work in the constructor (other than populating instance variables), we mix the first and second points in our list.
It’s a bad practice not only from architectural point of view (we’ll do the same work every time we instantiate our class) but also because we always get our class in a predefined state.
The Law of Demeter, or Principle of Least Knowledge, is a design guideline that states that one class should know only as much as it needs to know.
In this example, we pass to the Car constructor a Context object.
This is a violation of the Law of Demeter, because the Car class needs to know that the Context object has a getDriver method.
If we want to test this constructor, we need to get hold of a valid Context object before calling the constructor.
If the Context object has a lot of variables and methods, we could be forced to use mock objects (see chapter 7) to simulate the context.
The proper solution is to apply the Principle of Least Knowledge and pass references to methods and constructors only when we need to do so.
In our example, we should pass the Driver to the Car constructor, as in the following:
That illustrates a key concept: Require objects, don’t search for objects, and ask only for objects that your application requires.
Be careful with global state because global state makes it possible for many clients to share the global object.
This can have unintended consequences if the global object is not coded for shared access or if clients expect exclusive access to the global object.
Without instantiating the database first, you won’t be able to make a reservation.
Internally, the Reservation uses the DBManager to access the database.
Unless documented, the Reservation class hides its dependency on the database manager from the programmer because the API doesn’t give us a clue.
In this example, the Reservation object is constructed with a given database manager.
Strictly speaking, the Reservation object should be able to function only if it has been configured with a database manager.
Avoid global state; when you provide access to a global object, you share not only that object but also any object to which it refers.
You can live in a society where everyone (every class) declares who their friends (collaborators) are.
If I know that Joe knows Mary but neither Mary nor Joe knows Tim, then it is safe for me to assume that if I give some information to Joe he may give it to Mary, but under no circumstances will Tim get hold of it.
Now, imagine that everyone (every class) declares some of their friends (collaborators), but other friends (collaborators which are singletons) are kept secret.
Now you are left wondering how in the world did Tim got hold of the information you gave to Joe.
If you are the person who built the relationships (code) originally, you know the true dependencies, but anyone who comes after you is baffled, since the friends which are declared are not the sole friends of objects, and information flows in some secret paths which are not clear to you.
Although we just discouraged you from using global state, the Singleton3 is a useful design pattern that ensures a class has only one instance.
Most often, the implementation defines a private constructor and a static variable.
Here, you access the singleton with the static final field INSTANCE.
Alternatively, the class can use lazy initialization to create the instance, for example:
The Singleton design pattern needs to make sure the object is instantiated only once.
To ensure this, we hide the constructor by making it private.
As with a private method, you can’t call and test a private constructor explicitly.
You have a choice: you can rely on code coverage to check that all private methods are tested, or you change access modifiers to open the class to explicit testing of those methods.
The obvious drawback of a singleton is that it introduces global state into your application.
The INSTANCE field in the first example is a global variable.
Static methods, like factory methods, are useful, but large groups of utility static methods can introduce issues of their own.
In order to achieve isolation you need some articulation points in your code, where you can easily substitute your code with the test code.
With polymorphism (the ability of one object to appear as another object) the method you’re calling isn’t determined at compile time.
You can easily use polymorphism to substitute application code with the test code to force certain code patterns to be tested.
The opposite situation occurs when you use nothing but static methods.
Then you practice procedural programming, and all of your method calls are determined at compile time.
You no longer have articulation points that you can substitute.
Sometimes the harm of static methods to your test isn’t big, especially when you choose some method that ends the execution graph, like Math.sqrt()
On the other hand, you can choose a method that lies in the heart of your application logic.
Writing testable code that case, every method that gets executed inside that static method becomes hard to test.
Static code and the inability to use polymorphism in your application affect your application and tests equally.
No polymorphism means no code reuse for both your application and your tests.
This can lead to code duplication in the application and tests, something we try to avoid.
At runtime, code can’t change an inheritance hierarchy, but we can compose objects differently.
We strive to make our code as flexible as possible at runtime.
This way we can be sure that it’s easy to switch from one state of our objects to another, and that makes our code easily testable.
On the other hand, we could add a Credentials instance variable to those servlets that need it and make our classes easier to test by instantiating the Credentials variable only when we need it.
As mentioned previously, we do only the following in our tests:
For example, it could be difficult to instantiate our class if it’s too complex.
One of the main ways to decrease complexity is to try to avoid long switch and if statements.
Every time we want to add a new document type, we add additional case clauses.
If that happens often in your code, you’ll have to change it in every place that it occurs.
Every time you see a long conditional statement, think of polymorphism.
Polymorphism is a natural object-oriented way to avoid long conditionals, by breaking a class into several smaller classes.
Several smaller components are easier to test than one large complex component.
In the given example, we can avoid the conditional by creating different document types like WordDocument, PDFDocument, and XMLDocument, each one implementing a printDocument() method.
This will decrease the complexity of our code and will make it easier to read.
In chapter 3, we designed an application controller and quickly wrote some tests to validate your design.
As we wrote the tests, the tests helped improve the initial design.
As you write more unit tests, positive reinforcement encourages you to write them earlier.
As you design and implement, it becomes natural to wonder about how you’ll test a class.
Following this methodology, more developers are making the leap from testfriendly designs to test-driven development.
Let’s move on and see how we can adapt our development lifecycle to enforce the testdriven development approach.
When you develop code, you design an application programming interface (API) and then implement the behavior promised by the interface.
When you unit test code, you verify the promised behavior through a method’s API.
The test is a client of the method’s API, just as your domain code is a client of the method’s API.
The conventional development cycle goes something like this: code, test, (repeat), commit.
Developers practicing TDD make a seemingly slight but surprisingly effective adjustment: test, code, (repeat), commit.
The test drives the design and becomes the method’s first client.
Listing 5.7 illustrates how unit tests can help design the implementation.
The getBalanceOk method shows that the getBalance method of Account returns the account balance as a long and that this balance can be set in the Account constructor.
At this point, the implementation of Account is purely hypothetical, but writing the unit tests allows you to focus on the design of the code.
As soon as you implement the class, you can run the test to prove that the implementation works.
If the test fails, then you can continue working on the implementation until it passes the test.
When the test passes, you know that your code fulfills the contract.
When you use the test as the method’s first client, it becomes easier to focus purely on the API.
Someone new to the project can understand the system by studying the functional test suite (high-level UML diagrams also help)
To analyze a specific portion of the application in detail, they can drill down into individual unit tests.
Earlier, we said that TDD tweaks the development cycle to go something like test, code, (repeat), and ship.
The problem with this chant is that it leaves out a key step.
It should go more like this: test, code, refactor, (repeat), and ship.
Eliminating duplication ensures that you write code that’s not only testable but also maintainable.
When you eliminate duplication, you tend to increase cohesion and decrease dependency.
These are hallmarks of code that’s easier to maintain over time.
Other coding practices have encouraged us to write maintainable code by anticipating change.
In contrast, TDD encourages us to write maintainable code by eliminating.
Developers following this practice have found that test-backed, wellfactored code is, by its very nature, easy and safe to change.
Now that we’ve described the cycle—test, code, refactor, (repeat), and ship—of testdriven development, we show next how testing fits into development overall.
Testing occurs at different places and times during the development cycle.
We first introduce a development lifecycle and then use it as a base for deciding what types of tests are executed when.
Figure 5.5 shows a typical development cycle we’ve used effectively in both small and large teams.
JUnit best practice: write failing tests first If you take the TDD development pattern to heart, an interesting thing happens: before you can write any code, you must write a test that fails.
Why does it fail? Because you have not written the code to make it succeed.
Faced with this situation, most of us begin by writing a simple implementation to let the test pass.
Now that the test succeeds, you could stop and move on to the next problem.
Being a professional, you’d take a few minutes to refactor the implementation to remove redundancy, clarify intent, and optimize the investment in the new code.
But as long as the test succeeds, technically you’ve finished.
The end game? If you always test first, you’ll never write a line of new code without a failing test.
Figure 5.5 A typical application development lifecycle using the continuous integration principle.
One important rule is usually to commit (aka check in), up to several times per day, to your Source Control Management (SCM) system (SVN, CVS, ClearCase, and the like)
But it’s important to commit only something that “works.” To ensure this, you run a local build with Ant or Maven.
Integration platform—This platform builds the application from its various components (which may have been developed by different teams) and ensures that they all work together.
This step is extremely valuable, because problems are often discovered here.
Acceptance platform/stress test platform—Depending on the resources available to your project, this can be one or two platforms.
The stress test platform exercises the application under load and verifies that it scales correctly (with respect to size and response time)
The acceptance platform is where the project’s customers accept (sign off on) the system.
It’s highly recommended that the system be deployed on the acceptance platform as often as possible in order to get user feedback.
It’s optional, and small or noncritical projects can do without it.
We now show how testing fits in the development cycle.
Figure 5.6 highlights the different types of tests you can perform on each platform.
On the development platform, you execute logic unit tests (tests that can be executed in isolation from the environment)
These tests execute quickly, and you usually execute them from your IDE to verify that any change you’ve brought to the code has not broken anything.
They’re also executed by your automated build before you commit the code to your SCM.
You could also execute integration unit tests, but they often take much longer, because they need some part of the environment to be set up (database, application server, and the like)
In practice, you’d execute only a subset of all integration unit tests, including any new integration unit tests you’ve written.
The integration platform usually runs the build process automatically to package and deploy the application and then executes unit and functional tests.
Usually, only a subset of all functional tests is run on the integration platform, because compared to the target production platform, it’s a simple platform that lack elements (for example, it may be missing a connection to an external system being accessed)
All types of unit tests are executed on the integration platform (logic unit tests, integration unit tests, and functional unit tests)
Time is less important, and the whole build can take several hours with no impact on development.
On the acceptance platform/stress test platform, you reexecute the same tests executed by the integration platform; in addition, you run stress tests (performance and load tests)
The acceptance platform is extremely close to the production platform, and more functional tests can also be executed.
It’s always a good habit to try to run on the (pre-)production platform the tests you ran on the acceptance platform.
Doing so acts as a sanity check to verify that everything is set up correctly.
Human beings are strange creatures, always tending to neglect details.
In a perfect world, we’d have all four platforms to run our tests on.
In the real world, however, most of the software companies try to skip some of the platforms we listed—or the concept of testing as a whole.
As a developer who bought this book, you already made the right decision: more tests, less debugging!
Are you going to strive for perfection, stick to everything that you learned so far, and let your code benefit from that?
Figure 5.6 The different types of tests performed on each platform of the development cycle.
This chapter was mainly dedicated to some advanced techniques in unit testing: checking your test coverage and improving it, designing your code to be easily testable, and practicing test-driven development (TDD)
The next chapter will take you to the next level of testing your code.
This next level involves using not only JUnit as a testing framework but also including other frameworks and tools, and it introduces the concept of mocking.
JUnit best practice: continuous regression testing Most tests are written for the here and now.
You write a new feature, and you write a new test.
You see whether the feature plays well with others and whether the users like it.
If everyone is happy, you can lock the feature and move on to the next item on your list.
Most software is written in a progressive fashion: you add one feature and then another.
Most often, each new feature is built over a path paved by existing features.
If an existing method can service a new feature, you reuse the method and save the cost of writing a new one.
Sometimes you need to change an existing method to make it work with a new feature.
When this happens, you need to confirm that all the old features still work with the amended method.
A strong benefit of JUnit is that the test cases are easy to automate.
When a change is made to a method, you can run the test for that method.
If that test passes, then you can run the rest.
If any fail, you can change the code (or the tests) until all tests pass again.
Using old tests to guard against new changes is a form of regression testing.
Any kind of test can be used as a regression test, but running unit tests after every change is your first, best line of defense.
The best way to ensure that regression testing takes place is to automate your test suites.
See part 3 of the book for more about automating JUnit.
As you develop your applications, you’ll find that the code you want to test depends on other classes, which themselves depend on other classes, which then depend on the environment.
For example, you might be developing an application that uses JDBC to access a database, a Java EE application (one that relies on a Java EE container for security, persistence, and other services), an application that accesses a file system, or an application that connects to some resource using HTTP, SOAP, or another protocol.
Starting in this chapter, we look at using JUnit to test an application that depends on external resources.
For applications that depend on a specific runtime environment, writing unit tests is a challenge.
Your tests need to be stable, and when run repeatedly, they need to yield the same results.
You need a way to control the environment in which the tests run.
One solution is to set up the real required environment as part of the tests and run the tests from within that environment.
In some cases, this approach is practical and brings real benefits (see chapter 8, which discusses in-container testing)
But it works well only if you can set up the real environment on your development and build platforms, which isn’t always feasible.
For example, if your application uses HTTP to connect to a web server provided by another company, you usually won’t have that server application available in your development environment.
Therefore, you need a way to simulate that server so you can still write and run tests for your code.
Alternatively, suppose you’re working with other developers on a project.
What if you want to test your part of the application, but the other part isn’t ready? One solution is to simulate the missing part by replacing it with a fake that behaves the same way.
There are two strategies for providing these fake objects: stubbing and using mock objects.
Stubs, the original solution, are still very popular, mostly because they allow you to test code without changing it to make it testable.
This chapter is dedicated to stubbing, whereas chapter 7 covers mock objects.
Stubs are a mechanism for faking the behavior of real code or code that isn’t ready yet.
Stubs allow you to test a portion of a system even if the other part isn’t available.
Stubs usually don’t change the code you’re testing but instead adapt to provide seamless integration.
The intent is to replace a complex behavior with a simpler one that allows independent testing of some part of the real code.
Here are some examples of when you might use stubs:
With stubs, you aren’t modifying the objects under test, and what you are testing is the same as what will execute in production.
A build or developer usually executes tests involving stubs in their running environment, providing additional confidence.
On the downside, stubs are usually hard to write, especially when the system to fake is complex.
The stub needs to implement the same logic as the code it’s.
In general, stubs are better adapted for replacing coarse-grained portions of code.
You usually use stubs to replace a full-blown external system such as a file system, a connection to a server, a database, and so forth.
Stubs can replace a method call to a single class, but it’s more difficult.
To demonstrate what stubs can do, let’s build some stubs for a simple application that opens an HTTP connection to a URL and reads its content.
The remote web resource is a servlet, which generates an HTML response.
The web resource in figure 6.1 is what we called the “real code” in the stub definition.
Our goal in this chapter is to unit test the getContent method by stubbing the remote web resource, as demonstrated in figure 6.2
You replace the servlet web resource with the stub, a simple HTML page returning whatever you need for the TestWebClient test case.
This approach allows you to test the getContent method independently of the implementation of the web resource (which in turn could call several other objects down the execution chain, possibly down to a database)
The important point to notice with stubbing is that we didn’t modify getContent to accept the stub.
In order to allow stubbing, the target code needs to have a well-defined interface and allow plugging in of different implementations (a stub, in our case)
Figure 6.1 The sample application opens an HTTP connection to a remote web resource.
The web resource is the “real code” in the stub definition.
Let’s look at a stub in action using the simple HTTP connection example.
Listing 6.1 from the example application demonstrates a code snippet opening an HTTP connection to a given URL and reading the content found at that URL.
Imagine the method is one part of a bigger application that you want to unit test.
Figure 6.2 Adding a test case and replacing the real web resource with a stub.
We start B by opening an HTTP connection using the HttpURLConnection class.
We then read the stream content until there’s nothing more to read C.
One might argue that a better implementation should throw an exception.
There are two possible scenarios in the example application: the remote web server (see figure 6.1) could be located outside the development platform (such as on a partner site), or it could be part of the platform where you deploy the application.
But in both cases, you need to introduce a server into your development platform in order to be able to unit test the WebClient class.
One relatively easy solution would be to install an Apache test server and drop some test web pages in its document root.
Fortunately, an easier solution exists using an embedded web server.
Because we’re testing in Java, the easiest solution is to use a Java web server that you can embed in the test case.
You can use the free and open source Jetty server for this exact purpose.
In this book, we use Jetty to set up our stubs.
We use Jetty because it’s fast (important when running tests), it’s lightweight, and your test cases can programmatically control it.
In addition, Jetty is a very good web, servlet, and JSP container that you can use in production.
You seldom need this for most tests, but it’s always nice to use best-of-breed technology.
Reliance on the environment You need to be sure the full environment is up and running before the test starts.
If the web server is down and you execute the test, it’ll fail and you’ll spend time debugging the failure.
You’ll discover that the code is working fine and it’s only a setup issue generating a false failure.
When you’re unit testing, it’s important to be able to control as much as possible of the environment in which the tests execute, such that test results are reproducible.
Separated test logic The test logic is scattered in two separate locations: in the JUnit test case and in the test web page.
You need to keep both types of resources in sync for the tests to succeed.
Difficult tests to automate Automating the execution of the tests is difficult because it involves deploying the web pages on the web server, starting the web server, and then running the unit tests.
Using Jetty allows you to eliminate the drawbacks outlined previously: the JUnit test case starts the server, you write the tests in Java in one location, and automating the test suite is a nonissue.
Thanks to Jetty’s modularity, the real point of the exercise is to stub only the Jetty handlers and not the whole server from the ground up.
In order to understand how to set up and control Jetty from your tests, let’s implement a simple example.
Listing 6.2 shows how to start Jetty from Java and how to define a document root (/) from which to start serving files.
We start by creating the Jetty Server object B and specifying in the constructor which port to listen to for HTTP requests (port 8080)
Next, we create a Context object C that processes the HTTP requests and passes them to various handlers.
We map the context to the already-created server instance and to the root (/) URL.
The setResourceBase method sets the document root from which to serve resources.
On the next line, we attach a ResourceHandler handler to the root to serve files from the file system.
Because this handler will return an HTTP 403-Forbidden error if we try to list the content of a directory, we specify the resource base to be a file.
In this example, we specify the file pom.xml in the project’s directory.
Now that you’ve seen how to run Jetty as an embedded server, we show next how to stub the server’s resources.
You now know how to easily start and configure Jetty, so let’s focus on the HTTP connection unit test.
You’ll write a first test that verifies you can call a valid URL and get its content.
To verify that the WebClient works with a valid URL, you need to start the Jetty server before the test, which you can implement in a test case setUp method.
You can also stop the server in a tearDown method.
Listing 6.3 First test to verify that WebClient works with a valid URL.
Alternatively, you can configure Jetty to use your own custom Handler that returns the string "It works" instead of getting it from a file.
This is a much more powerful technique, because it lets you unit test the case when the remote HTTP server returns an error code to your WebClient client application.
This class creates a handler B by extending the Jetty AbstractHandler class and implementing a single method, handle.
Jetty calls the handle method to forward an incoming request to our handler.
The last step is to set the response content length to be the length of the string written to the output stream (this is required by Jetty) and then send the response E.
Listing 6.4 Create a Jetty Handler that returns "It works" when called.
The solution shown in listing 6.3 isn’t optimal because JUnit will start and stop the server for every test method.
These annotations let you execute code before and after all @Test methods in a class.
Isolating each test versus performance considerations In previous chapters, we went to great lengths to explain why each test should run in a clean environment (even to the extent of using a new class loader instance)
But sometimes there are other considerations to take into account.
Test suites that take a long time to execute are a handicap; you’ll be tempted not to execute them often, which negates the regression feature of unit testing.
Depending on the situation, you may choose to have longer-running tests that execute in a clean environment or instead tune the tests for performance by reusing some parts of the environment.
In the example at hand, you use different handlers for different tests, and you can be fairly confident they won’t interfere with each other.
If you run the test in Eclipse, you’ll see the result in figure 6.4—our test passes.
So far, so good—our tests have been testing the good side of our code.
Figure 6.4 Result of the first working test using a Jetty stub.
JUnit starts the server before the first test, and the server shuts itself down after the last test.
Now that you have the first test working, let’s see how to test for server failure conditions.
Let’s add a test for an invalid URL—a URL pointing to a file that doesn’t exist.
This case is quite easy, because Jetty already provides a NotFoundHandler handler class for that purpose.
You only need to modify the TestWebClient setUp method as follows (changes are in bold):
Adding a new test in TestWebClient is also a breeze:
In similar fashion, you can easily add a test to simulate the server having trouble.
A test like this would be very difficult to perform if you didn’t choose an embedded web server like Jetty.
You’ve now been able to fully unit test the getContent method in isolation by stubbing the web resource.
What have you really tested? What kind of test have you achieved? You’ve done something quite powerful: you’ve unit tested the method, but at the same time, you’ve executed an integration test.
In addition, not only have you tested the code logic, but you’ve also tested the connection part that’s outside the code (through the Java HttpURLConnection class)
It can take a Jetty novice half a day to learn enough about Jetty to set it up correctly.
In some instances, you’ll have to debug stubs to get them to work properly.
Keep in mind that the stub must remain simple and not become a full-fledged application that requires tests and maintenance.
If you spend too much time debugging your stubs, a different solution may be called for.
In these examples, you need a web server—but another example and stub will be different and will need a different setup.
Experience helps, but different cases usually require different stubbing solutions.
The example tests are nice because you can both unit test the code and perform some integration tests at the same time.
More solutions that are lightweight focus on unit testing the code without performing integration tests.
The rationale is that although you need integration tests, they could run in a separate test suite or as part of functional tests.
In the next section, we look at another solution that can still qualify as stubbing.
It’s simpler in the sense that it doesn’t require you to stub a whole web server.
It brings you one step closer to the mock object strategy, which is described in the following chapter.
Doing so will prevent you from effectively testing the connection, but that’s fine because it isn’t your real goal at this point.
Functional or integration tests will test the connection at a later stage.
When it comes to stubbing the connection without changing the code, we benefit from Java’s URL and HttpURLConnection classes, which let us plug in custom protocol handlers to process any kind of communication protocol.
You can have any call to the HttpURLConnection class redirected to your own class, which will return whatever you need for the test.
A better implementation would use a TestSetup class, such that this is performed only once during the whole test suite execution.
You could also use anonymous inner classes for conciseness, but that approach would make the code more difficult to read.
The last step is to create a stub implementation of the HttpURLConnection class so you can return any value you want for the test.
Listing 6.7 shows a simple implementation that returns the string "It works" as a stream to the caller.
HttpURLConnection is an abstract public class that doesn’t implement an interface, so you extend it and override the methods wanted by the stub.
In this stub, you provide an implementation for the getInputStream method because it’s the only method used by your code under test.
Should the code to test use more APIs from HttpURLConnection, you’d need to stub these additional methods.
This is where the code would become more complex—you’d need to reproduce completely the same behavior as the real HttpURLConnection.
For example, at B, you test that if setDoInput(false) has been called in the code under test, then a call to the getInputStream method returns a ProtocolException.
Fortunately, in most cases, you need to stub only a few methods and not the whole API.
Figure 6.5 shows the result of the execution of the test in Eclipse.
As you can see, it’s much easier to stub the connection than to stub the web resource.
This approach doesn’t bring the same level of testing (you aren’t performing integration tests), but it enables you to more easily write a focused unit test for the WebClient logic.
In this chapter, we demonstrated how using a stub has helped us unit test code accessing a remote web server using the Java HttpURLConnection API.
In particular, we showed how to stub the remote web server by using the open source Jetty server.
Jetty’s embeddable nature lets you concentrate on stubbing only the Jetty HTTP request handler, instead of having to stub the whole container.
We also demonstrated a more lightweight solution by stubbing the Java HttpURLConnection class.
The next chapter demonstrates a technique called mock objects that allows finegrained unit testing, which is completely generic, and (best of all) forces you to write good code.
Although stubs are very useful in some cases, some consider them more a vestige of the past, when the consensus was that tests should be a separate activity and shouldn’t modify existing code.
The new mock objects strategy not only allows modification of code but  favors it.
Using mock objects is more than a unit testing strategy; it’s a completely new way of writing code.
Unit testing each method in isolation from the other methods or the environment is certainly a nice goal.
How do you perform this feat? You saw in chapter 6 how the stubbing technique lets you unit test portions of code by isolating them from the environment (for example, by stubbing a web server, the file system, a database, and so on)
What about fine-grained isolation, like being able to isolate a method call to another class? Is that possible? Can you achieve this without deploying huge amounts of energy that would negate the benefits of having tests?
Programming today is a race between software engineers striving to build bigger and better idiot-proof.
The mock objects strategy allows you to unit test at the finest-possible level and develop method by method, while providing you with unit tests for each method.
Testing in isolation offers strong benefits, such as the ability to test code that has not yet been written (as long as you at least have an interface to work with)
In addition, testing in isolation helps teams unit test one part of the code without waiting for all the other parts.
The biggest advantage is the ability to write focused tests that test only a single method, without side effects resulting from other objects being called from the method under test.
Writing small, focused tests is a tremendous help; small tests are easy to understand and don’t break when other parts of the code are changed.
Remember that one of the benefits of having a suite of unit tests is the courage it gives you to refactor mercilessly—the unit tests act as a safeguard against regression.
If you have large tests and your refactoring introduces a bug, several tests will fail; that result will tell you that there’s a bug somewhere, but you won’t know where.
With fine-grained tests, potentially fewer tests will be affected, and they’ll provide precise messages that pinpoint the exact cause of the breakage.
Mock objects (or mocks for short) are perfectly suited for testing a portion of code logic in isolation from the rest of the code.
Mocks replace the objects with which your methods under test collaborate, offering a layer of isolation.
But this is where the similarity ends, because mocks don’t implement any logic: they’re empty shells that provide methods to let the tests control the behavior of all the business methods of the faked class.
We discuss when to use mock objects in section 7.6 at the end of this chapter, after we show them in action on some examples.
In this section, we present an application and a test using mock objects.
The AccountService class offers services related to Accounts and uses the AccountManager to persist data to the database (using JDBC, for example)
Although this process is required to ensure the application works end to end, it’s too much work when you want to unit test only your code logic.
Listing 7.1 presents a simple Account object with two properties: an account ID and a balance.
The AccountManager interface that follows manages the lifecycle and persistence of Account objects.
We’re limited to finding accounts by ID and updating accounts.
Listing 7.2 shows the transfer method for transferring money between two accounts.
It uses the AccountManager interface we previously defined to find the debit and credit accounts by ID and to update them.
Figure 7.1 In this simple bank account example, we use a mock object to test an account transfer method.
For that purpose, we use a mock implementation of the AccountManager interface (listing 7.3)
We do this because the transfer method is using this interface, and we need to test it in isolation.
The addAccount method uses an instance variable to hold the values to return B.
Because we have several account objects that we want to be able to return, we store the Account objects to return in a HashMap.
This makes the mock generic and able to support different test cases: one test could set up the mock with one account, another test could set it up with two accounts or more, and so forth.
In C we implement a method to retrieve the account from the accounts mapwe can retrieve only accounts that have been added before that.
Unit testing with mock objects method updates an account but doesn’t return any value D.
When it’s called by the transfer method, it will do nothing, as if the account had been correctly updated.
As usual, a test has three steps: the test setup B, the test execution C, and the verification of the result D.
During the test setup, we create the MockAccountManager object and define what it should return when called for the two accounts we manipulate (the sender and beneficiary accounts)
We’ve succeeded in testing the AccountService code in isolation of the other domain object, AccountManager, which in this case didn’t exist, but which in real life could have been implemented using JDBC.
At this point in the chapter, you should have a reasonably good understanding of what a mock is.
In the next section, we show you that writing unit tests with mocks leads to refactoring your code under test—and that this process is a good thing!
JUnit best practices: don’t write business logic in mock objects The most important point to consider when writing a mock is that it shouldn’t have any business logic.
It must be a dumb object that does only what the test tells it to do.
This characteristic is exactly the opposite of stubs, which contain all the logic (see chapter 6)
First, mock objects can be easily generated, as you’ll see in following chapters.
Second, because mock objects are empty shells, they’re too simple to break and don’t need testing themselves.
Some people used to say that unit tests should be totally transparent to your code under test, and that you should not change runtime code in order to simplify testing.
This is wrong! Unit tests are first-class users of the runtime code and deserve the same consideration as any other user.
If your code is too inflexible for the tests to use, then you should correct the code.
For example, what do you think of the following piece of code?
Does the code look fine to you? We can see two issues, both of which relate to code flexibility and the ability to resist change.
The first problem is that it isn’t possible to decide to use a different Log object, because it’s created inside the class.
For testing, for example, you probably want to use a Log that does nothing, but you can’t.
As a rule, a class like this should be able to use whatever Log it’s given.
The goal of this class isn’t to create loggers but to perform some JDBC logic.
It may sound okay right now, but what happens if you decide to use XML to store the configuration? Again, it shouldn’t be the goal of this class to decide what implementation to use.
JUnit best practices: test only what can possibly break You may have noticed that we didn’t mock the Account class.
The reason is that this data access object class doesn’t need to be mocked—it doesn’t depend on the environment, and it’s simple.
Our other tests use the Account object, so they test it indirectly.
If it failed to operate correctly, the tests that rely on Account would fail and alert us to the problem.
An effective design strategy is to pass to an object any other object that’s outside its immediate business logic.
The choice of peripheral objects can be controlled by someone higher in the calling chain.
Ultimately, as you move up in the calling layers, the decision to use a given logger or configuration should be pushed to the top level.
This strategy provides the best possible code flexibility and ability to cope with changes.
And, as we all know, change is the only constant.
Refactoring all code so that domain objects are passed around can be time consuming.
You may not be ready to refactor the whole application just to be able to write a unit test.
Fortunately, there’s an easy refactoring technique that lets you keep the same interface for your code but allows it to be passed domain objects that it shouldn’t create.
This makes the code more flexible because it introduces an interface (which will be easy to mock), and the implementation of the Configuration interface can be anything we want (including using resource bundles)
The class can be controlled from the outside (by its caller)
Meanwhile, we haven’t broken the existing interface, because we’ve only added a new constructor.
With this refactoring, we’ve provided a trapdoor for controlling the domain objects from your tests.
We retain backward compatibility and pave an easy refactoring path for the future.
Calling classes can start using the new constructor at their own pace.
Should you worry about introducing trapdoors to make your code easier to test? Here’s how Extreme Programming guru Ron Jeffries explains it:
My car has a diagnostic port and an oil dipstick.
There is an inspection port on the side of my furnace and on the front of my oven.
My pen cartridges are transparent so I can see if there is ink left.
And if I find it useful to add a method to a class to enable me to test it, I do so.
It happens once in a while, for example in classes with easy interfaces and complex inner function (probably starting to want an Extract Class)
I just give the class what I understand of what it wants, and keep an eye on it to see what it wants next.1
Design patterns in action: Inversion of Control Applying the IoC pattern to a class means removing the creation of all object instances for which this class isn’t directly responsible and passing any needed instances instead.
The instances may be passed using a specific constructor, using a setter, or as parameters of the methods needing them.
It becomes the responsibility of the calling code to correctly set these domain objects on the called class.2
At B, we use a mock logger that implements the Log interface but does nothing.
We’ve been able to completely control our logging and configuration behavior from outside the code to test, in the test code.
As a result, our code is more flexible and allows for any logging and configuration implementation to be used.
You’ll see more of these code refactorings in this chapter and later ones.
One last point to note is that if you write your test first, you’ll automatically design your code to be flexible.
Flexibility is a key point when writing a unit test.
If you test first, you won’t incur the cost of refactoring your code for flexibility later.
To see how mock objects work in a practical example, let’s use the simple application that opens an HTTP connection to a remote server and reads the content of a page.
Let’s now unit test it using a mock object approach to simulate the HTTP connection.
In addition, you’ll learn how to write mocks for classes that don’t have a Java interface (namely, the HttpURLConnection class)
We show a full scenario in which you start with an initial testing implementation, improve the implementation as you go, and modify the original code to make it more flexible.
We also show how to test for error conditions using mocks.
As you dive in, you’ll keep improving both the test code and the sample application, exactly as you might if you were writing the unit tests for the same application.
In the process, you’ll learn how to reach a simple and elegant testing solution while making your application code more flexible and capable of handling change.
We want to be able to unit test the getContent method in isolation from the web resource.
Figure 7.2 The sample HTTP application before introducing the test.
The MockURL class stands in for the real URL class, and all calls to the URL class in getContent are directed to the MockURL class.
As you can see, the test is the controller: it creates and configures the behavior the mock must have for this test; it (somehow) replaces the real URL class with the MockURL class; and it runs the test.
Figure 7.3 shows an interesting aspect of the mock objects strategy: the need to be able to swap the mock into the production code.
The perceptive reader will have noticed that because the URL class is final, it’s not possible to create a MockURL class that extends it.
In the coming sections, we demonstrate how to perform this feat in a different way (by mocking at another level)
In any case, when using the mock objects strategy, swapping in the mock instead of the real class is the hard part.
This may be viewed as a negative point for mock objects, because we usually need to modify our code to provide a trapdoor.
Ironically, modifying code to encourage flexibility is one of the strongest advantages of using mocks, as explained in section 7.3.1
The example in listing 7.6 demonstrates a code snippet that opens an HTTP connection to a given URL and reads the content found at that URL.
Let’s imagine that it’s one method of a bigger application that we want to unit test, and let’s unit test that method.
Figure 7.3 The steps involved in a test using mock objects.
Admittedly, this isn’t the best possible errorhandling solution, but it’s good enough for the moment.
And our tests will give us the courage to refactor later.
The idea is to be able to test the getContent method independently of a real HTTP connection to a web server.
Unfortunately, this approach doesn’t work! The JDK URL class is a final class, and no URL interface is available.
Listing 7.6 A sample method that opens an HTTP connection.
We explored this solution in chapter 6, so let’s find a technique that uses mock objects: refactoring the getContent method.
If you think about it, this method does two things: it gets an HttpURLConnection object and then reads the content from it.
How does this solution let us test getContent more effectively? We can now apply a.
The test now becomes the following (differences are shown in bold):
Listing 7.7 Extracting retrieval of the connection object from getContent.
This is a common refactoring approach called method factory refactoring, which is especially useful when the class to mock has no interface.
The strategy is to extend that class, add some setter methods to control it, and override some of its getter methods to return what we want for the test.
In the case at hand, this approach is okay, but it isn’t perfect.
It’s a bit like the Heisenberg uncertainty principle: the act of subclassing the class under test changes its behavior, so when we test the subclass, what are we truly testing?
This technique is useful as a means of opening up an object to be more testable, but stopping here means testing something that’s similar to (but not exactly the same as) the class we want to test.
It isn’t as if we’re writing tests for a third-party library and can’t change the code—we have complete control over the code to test.
We can enhance it and make it more test friendly in the process.
Let’s apply the Inversion of Control pattern, which says that any resource we use needs to be passed to the getContent method or WebClient class.
This means we’re pushing the creation of the HttpURLConnection object to the caller of WebClient.
But the URL is retrieved from the HttpURLConnection class, and the signature doesn’t look nice.
The role of classes implementing the ConnectionFactory interface is to return an InputStream from a connection, whatever the connection might be (HTTP, TCP/IP, and so on)
This refactoring technique is sometimes called a class factory refactoring.3
The WebClient code then becomes as shown in listing 7.9
Changes from the initial implementation in listing 7.6 are shown in bold.
This solution is better because we’ve made the retrieval of the data content independent of the way we get the connection.
The new implementation can work with any standard protocol (file://, http://, ftp://, jar://, and so forth) or even your own custom protocol.
For example, listing 7.10 shows the ConnectionFactory implementation for HTTP.
Now we can easily test the getContent method by writing a mock for ConnectionFactory (see listing 7.11)
As usual, the mock doesn’t contain any logic and is completely controllable from the outside (by calling the setData method)
In the process we had to refactor it for the test, which led to a more extensible implementation that’s better able to cope with change.
Mocks replace real objects from the inside, without the calling classes being aware of it.
In the examples so far, we’ve used them only to emulate real behaviors, but we haven’t mined all the information they can provide.
It’s possible to use mocks as probes by letting them monitor the method calls the object under test makes.
One of the interesting calls we could monitor is the close method on the InputStream.
We haven’t been using a mock object for InputStream so far, but we can easily create one and provide a verify method to ensure that close has been called.
Then, we can call the verify method at the end of the test to verify that all methods that should have been called were called (see listing 7.13)
We may also want to verify that close has been called exactly once and raise an exception if it was called more than once or not at all.
For example, a database connection mock could verify that the close method on the connection is called exactly once during any test that involves code using this mock.
To see an example of an expectation, look at listing 7.13
In the case of the MockInputStream class, the expectation for close is simple: we always want it to be called once.
But most of the time, the expectation for closeCount depends on the code under test.
Note that we call the verify method of MockInputStream at the end of the test to ensure that all expectations are met.
The result of running the test is shown in figure 7.4
The test fails with the message close() should have been called once and once only.
The same error would be raised if we were closing it twice or more, because the test verifies that it’s called once and only once.
For example, if you have a component manager calling different methods of your component lifecycle, you might expect them to be called in a given order.
Or, you might expect a given value to be passed as a parameter to the mock.
The general idea is that, aside from behaving the way you want during a test, your mock can also provide useful feedback on its usage.
The next section demonstrates the use of some of the most popular open source mocking frameworks—they’re powerful enough for our needs, and we don’t need to implement our mocks from the beginning.
So far we’ve been implementing the mock objects we need from scratch.
As you can see, it’s not a tedious task but rather a recurring one.
You might guess that we don’t need to reinvent the wheel every time we need a mock.
And you’re right—there are many good projects already written that can help us facilitate the usage of mocks in our projects.
In this section we take a closer look at two of the most widely used mock frameworks: the EasyMock and the JMock.
We try to rework the example HTTP connection application so that we can demonstrate how to use the two frameworks.
EasyMock (http://easymock.org/) is an open source framework that provides useful classes for mocking objects.
To use the framework you need to download the zip archive from the website of the project, unpack it somewhere, and include the contained easymock.jar in your classpath.
To show you how easy it is to construct mock objects in your test cases using EasyMock, we revise some of the mocks we constructed in the previous sections.
We start with a simple one: reworking the AccountService test from listing 7.2
We start the listing by defining the imports from the EasyMock library that we need B.
EasyMock relies heavily on the staticimport feature of Java 5+
In C we declare the object that we’d like to mock.
The reason behind this is simple: the core EasyMock framework can mock only interface objects.
In D we call the createMock method to create a mock of the class that we want.
In E, as in listing 7.4, we create two account objects that we’re going to use in our tests.
Once we’ve finished defining the expectations, we need to call the replay method to announce it H.
In I we call the transfer method to transfer some money between the two accounts, and in J we assert the expected result.
With EasyMock we can call the verify method with any mock object , to verify that the method-call expectations we declared were triggered.
What we’d like is to test the getContent method of the WebClient.
For this purpose we need to mock all the dependencies to that method.
In this example we have two dependencies: one is the ConnectionFactory and one is the InputStream.
It looks like there’s a problem because EasyMock can mock only interfaces, and the InputStream is a class.
To be able to mock the InputStream class, we’re going to use the Class Extension of EasyMock.
Class Extension is an extension project of EasyMock that lets you generate mock objects4 for classes and interfaces.
JUnit best practices: EasyMock object creation Here is a nice-to-know tip on the createMock method.
If you check the API of EasyMock, you’ll see that the createMock method comes with numerous signatures.
So which one should you use? The first one is better.
If you use the second one and your expectations aren’t met, then you’ll get an error message like the following:
As you can see, this message isn’t as descriptive as we want it to be.
If we use the first signature instead, and we map the class to a given name, we get something like the following:
We start the listing by importing the objects that we need B.
That’s it! Now we’re ready to create mock objects of classes and interfaces using the statically imported methods of classextensions.
In C, as in the previous listings, we declare the objects that we want to mock, and in D we call the createMock method to initialize them.
In E we define the expectation of the stream when the read method is invoked (notice that to stop reading from the stream, the last thing to return is -1), and in.
Now we need to denote that we’ve finished declaring our expectations; we do this by calling the replay method G.
The rest is invoking the method under test H and asserting the expected result I.
We also add another test to simulate a condition when we can’t close the InputStream.
We define an expectation where we expect the close method of the stream to be invoked J, and on the next line we declare that an IOException should be raised if this call occurs.
As the name of the framework suggests, using EasyMock is easy, and you should use it whenever possible.
But to make you aware of the entire mocking picture, we’d like to introduce another framework, so you have a better taste of what mocking is.
So far we showed how to implement our own mock objects and how to use the EasyMock framework.
In this section we introduce the JMock framework (http:// jmock.org/), so that we can have a full view of the different mocking techniques.
As in the previous section, we start with a simple example: reworking listing 7.4 by means of JMock.
As always, we start the listing by importing all the necessary objects we need B.
As you can see, unlike EasyMock, the JMock framework doesn’t rely on any static import features.
In C we instruct JUnit to use the JMock runner that comes with the framework.
In D we declare the context Mockery object that will serve us to create mocks and to define expectations.
In E we declare the AccountManager that we’d like to mock.
Just like EasyMock, the core JMock framework provides mocking only of interfaces.
As in any of the previous listings, we declare two accounts that we’re going to use to transfer money between G.
This is because we use them in an inner class defined in a different method.
In H we start declaring the expectations by constructing a new Expectations object.
In I we declare the first expectation, each expectation having the following form:
All the clauses are optional, except for the bold ones: invocation-count and mockobject.
We need to specify how many invocations will occur and on which object.
In J we start the transfer from one account to the other, and after that we assert the expected results.
It’s as simple as that! But wait; what happened with the verification of the invocation count? In all of the previous examples we needed to verify that the invocations of the expectations happened the expected number of times.
You can see how to implement a custom JUnit runner in appendix B of the book.
Well, with JMock you don’t have to do that; the JMock JUnit runner takes care of this, and in case any of the expected calls were not made, the test will fail.
Once again, we start the test case by instructing JUnit to use the JMock test runner.
This will save us the explicit verification of the expectations.
To tell JMock to create mock objects not only for interfaces but also for classes, we need to set the imposteriser property of the context C.
That’s all; now we can continue creating mocks the normal way.
In D we declare and initialize the two objects we’d like to create mocks of.
Notice the fine way we declare the consecutive execution of the read() method of the stream F and also the returned values.
In G we call the method under test, and in H we assert the expected result.
For a full view of how to use the JMock mocking library, we also provide another @Test method, which tests our WebClient under exceptional conditions.
In I we declare the expectation of the close() method being triggered, and in J we instruct JMock to raise an IOException when this trigger happens.
As you can see, the JMock library is as easy to use as the EasyMock one.
Whichever you prefer to use is up to you, as long as you remember that what increases your software quality isn’t the framework you use but rather how much you use it.
This chapter described a technique called mock objects that lets you unit test code in isolation from other domain objects and from the environment.
When it comes to writing fine-grained unit tests, one of the main obstacles is to extract yourself from the executing environment.
We’ve often heard the following remark: “I haven’t tested this method because it’s too difficult to simulate a real environment.” Well, not any longer!
In most cases, writing mock object tests has a nice side effect: it forces you to rewrite some of the code under test.
You hardcode unnecessary couplings between the classes and the environment.
It’s easy to write code that’s hard to reuse in a different context, and a little nudge can have a big effect on other classes in the system (similar to the domino effect)
With mock objects, you must think differently about the code and apply better design patterns, like interfaces and Inversion of Control.
Mock objects should be viewed not only as a unit testing technique but also as a design technique.
A new rising star among methodologies called test-driven development (TDD) advocates writing tests before writing code.
Although writing mock objects is easy, it can become tiresome when you need to mock hundreds of objects.
In the following chapters, we present several open source frameworks that automatically generate ready-to-use mocks for your classes, making it a pleasure to use the mock objects strategy.
This chapter examines one approach to unit testing components in an application container: in-container unit testing, or integration unit testing.
We discuss incontainer testing pros and cons and show what can be achieved using the mock objects approach introduced in chapter 7, where mock objects fall short, and how in-container testing enables you to write integration unit tests.
Finally, we compare the stubs, mock objects, and in-container approaches we’ve already covered in this second part of this book.
Let’s start with the example servlet in listing 8.1, which implements the HttpServlet method isAuthenticated, the method we want to unit test.
This servlet, although simple enough, allows us to show the limitation of standard unit testing.
In order to test the method isAuthenticated, we need a valid HttpServletRequest.
Because HttpServletRequest is an interface, we can’t just call a new HttpServletRequest.
The same is true for other server-side objects like HttpSession.
JUnit alone isn’t enough to write a test for the isAuthenticated method and for servlets in general.
A container offers services for the components it’s hosting, such as lifecycle, security, transaction, distribution, and so forth.
In the case of servlets and JSPs, the container is a servlet container like Jetty or Tomcat.
There are other types of containers, for example, EJB, database, and OSGi containers.
As long as a container creates and manages objects at runtime, we can’t use standard JUnit techniques to test those objects.
Although mocking works, we need to write a lot of code to create a test.
We start by importing the necessary classes and methods using Java 5 static imports B.
We use the EasyMock class extensively, which has similar syntax to the JUnit Hamcrest matchers.
Next, we declare instance variables for the objects C we want to mock, HttpServletRequest and HttpSession.
Invoke the replay method to finish declaring our expectations F.
Mocking a minimal portion of a container is a valid approach to testing components.
But mocking can be complicated and require a lot of code.
As with other kinds of tests, when the servlet changes, the test expectations must change to match.
This avoids the need to mock any objects; we access the objects and methods we need in the real container.
For our example, we need HttpServletRequest and HttpSession to be real objects managed by the container.
Using a mechanism to deploy and execute our tests in a container, we have in-container testing.
Next, we see what options are available to implement in-container tests.
We have two architectural choices to drive in-container tests: server-side and clientside.
As we stated previously, we can drive the tests directly by controlling the serverside container and the unit tests.
Alternatively, we can drive the tests from the client side, as shown in figure 8.1
Once the tests are packaged and deployed in the container and to the client, the JUnit test runner executes the test classes on the client (1)
A test class opens a connection via a protocol like HTTP(S) and calls the same test case on the server side (2)
The server returns the result from the tests back to the client (4), which an IDE or Ant or Maven can gather.
Our example uses a servlet container, but there are many different types of containers: servlet, database, OSGi, SIP, and the like.
In all of these cases, we can apply the in-container testing strategy.
In the third part of the book, we present different open source projects that use this strategy.
Table 8.1 lists the types of containers and the testing frameworks we use in later chapters.
Although the open source world offers other projects, we cover the more mature projects listed in the table.
Container type In-container testing framework to use Chapter with detailed description.
This section draws from the many questions in forums and mailing lists asking about the pros and cons of stubs, mock objects, and in-container testing.
Stubs work well to isolate a given class for testing and asserting the state of its instances.
For example, stubbing a servlet container allows us to track how many requests were made, what the state of the server is, or what URLs where requested.
Using mocks, however, we can test the state of the server and its behavior.
When using mock objects, we code and verify expectations; we check at every step to see whether tests execute domain methods and how many times tests call these methods.
One of the biggest advantages of stubs over mocks is that stubs are easier to understand.
Stubs isolate a class with little extra code compared to mock objects, which require an entire framework to function.
The drawback of stubs is that they rely on external tools and hacks, and they don’t track the state objects they fake.
Going back to chapter 6, we easily faked a servlet container with stubs; doing so with mock objects would be much harder because we’d need to fake container objects with state and behavior.
The biggest advantage of mock objects3 over in-container testing is that mocks don’t require a running container in order to execute tests.
The tests can’t verify the interaction between components and container.
The tests also don’t test the interaction between the components themselves as they run in the container.
The problem with functional tests is that they’re coarse grained and test only a full use case—you lose the benefits of fine-grained unit testing.
You won’t be able to test as many different cases with functional tests as you will with unit tests.
For example, you may have many mock objects to set up, which may prove to be considerable overhead.
Obviously, the cleaner the code (small and focused methods), the easier tests are to set up.
Another important drawback to mock objects is that in order to set up a test, you usually must know exactly how the mocked API behaves.
It’s easy to know the behavior of your own API, but it may not be so easy for another API, such as the Servlet API.
Even though containers of a given type all implement the same API, not all containers behave the same way.
In Tomcat, the returned content type is text/xml, but in Orion, it’s text/html.
This is an extreme example; all servlet containers implement the Servlet API in pretty much the same way.
But the situation is far worse for the various Java EE APIsespecially the EJB API.
Implementers can interpret an API specification differently, and a specification can be inconsistent, making it difficult to implement.
In addition, containers have bugs; the previous example may have been a bug in Orion 1.6.0
Although it’s unfortunate, you’ll have to deal with bugs, tricks, and hacks for various third-party libraries in any project.
To wrap up this section, we summarize the drawbacks of unit testing with mock objects:
This approach doesn’t test interactions with the container or between the components.
It requires excellent knowledge of the API to mock, which can be difficult (especially for external libraries)
So far, we’ve described the advantages of in-container unit testing.
But there are also a few disadvantages, which we now discuss.
If you wish to write integration unit tests for another component model, chances are such a framework exists.
With mock objects, because the concept is generic, you can test almost any API.
In most cases, you can use Ant or Maven to execute tests, which also provides the ability to run a build in a continuous integration server (CIS, see chapter 11)
Alternatively, IDEs can execute tests that use mock objects as normal JUnit tests.
We strongly believe that in-container testing falls in the category of integration testing.
This means that you don’t need to execute your in-container tests as often as normal unit tests and will most likely run them in a CIS, alleviating the need for IDE integration.
For a test to run in a container, you need to start and manage the container, which can be time consuming.
For example, if a unit test hits a database, the database must be in an expected state before the test starts (see database application testing in chapter 17)
In terms of execution time, integration unit tests cost more than mock objects.
Consequently, you may not run them as often as logic unit tests.
Because the application and its tests run in a container, your application must be packaged (usually as a WAR or EAR file) and deployed to the container.
You must then start the container and run the tests.
On the other hand, because you must perform these exact same tasks for production, it’s a best practice to automate this process as part of the build and reuse it for testing purposes.
As one of the most complex tasks of a Java EE project, providing automation for packaging and deployment becomes a win-win situation.
The need to provide in-container testing will drive the creation of this automated process at the beginning of the project, which will also facilitate continuous integration.
To further this goal, most in-container testing frameworks include support for build tools like Ant and Maven.
This will help hide the complexity involved in building various runtime artifacts as well as with running tests and gathering reports.
A standard design goal is to separate presentation from business layers.
For example, you should implement the code for a tag that retrieves a list of customers from a database with two classes.
One class implements the tag and depends on the Taglib API, whereas the other implements the database access and depends not on the Taglib API but on database classes like JDBC.
You can test the business logic class with JUnit and mock objects and use in-container testing to validate the tag class.
In-container testing requires more setup than mock objects but is well worth the effort.
You may not run the in-container tests as often, but they can confirm that your tags will work in the target environment.
Although you may be tempted to skip testing a component, such as a taglib, reasoning that functional tests will eventually test the tag as a side effect, we recommend that you fight this temptation.
Fine-grained tests can be run repeatedly and tell you when, where, and why your code breaks.
You have the ability to test completely your components, not only for normal behavior but also for error conditions.
For example, when testing a tag accessing a database, you should confirm that the tag behaves as expected when the connection with the database is broken.
This would be hard to test in automated functional tests, but it’s easy to test when you combine in-container testing and mock objects.
When it comes to unit testing an application in a container, we’ve shown that standard JUnit tests come up short.
Although testing with mock objects works, it misses some scenarios like integration tests to verify component interactions in and with a container.
In order to verify behavior in a container, we need a technique that addresses testing from an architectural point of view: in-container testing.
Although complex, in-container testing addresses these issues and provides developers with the confidence necessary to change and evolve their applications.
This chapter provided the foundation for the last part of this book, where we continue to explore such testing frameworks.
Next, we start the third part of this book by integrating JUnit into the build process, a tenet of test-driven development.
This part of the book deals with a very important aspect in the development cycle of every project: the build process.
The importance of the build process is reconsidered more often these days, especially in large projects.
That’s why we dedicate a whole part of this book to integration between JUnit and two of the most important build tools: Ant and Maven.
The ninth chapter gives you a quick introduction to Ant and its terminology: tasks, targets, and builds.
We discuss how to start your tests as part of your Ant build lifecycle, and we also show how to produce some fancy reports with the results of the JUnit execution.
This chapter serves as a basis for most of the rest of the book, because you need a good knowledge of Ant to be able to grasp all of the Ant integration sections in the latter chapters.
The tenth chapter guides you through the same concepts, but this time by means of another popular tool called Maven.
We show you how to include the execution of your tests in the Maven build lifecycle and how to produce nice HTML reports by means of some of the Maven plug-ins.
The last chapter in this part of the book is devoted to continuous integration (CI) tools.
This practice is highly recommended by extreme programmers and helps you maintain a code repository and automate the build on it.
This is helpful in building large projects that depend on several other projects that change often (as any open source project does)
In this chapter, we look at Apache Ant1 or Ant for short, a free and open source build tool with direct support for JUnit.
We show you how to be more productive with JUnit by running tests as part of the build.
We also show you how to set up your environment to build Java projects, manage JAR file dependencies, execute JUnit tests, and generate JUnit reports.
It’s supposed to be automatic, but you still have to press the button.
For unit tests to be effective, they must be part of the development routine.
Most development cycles begin by checking out code from the source code repository.
Before making any changes, prudent developers first run all unit test suites.
Many teams have a rule that all unit tests in the repository must pass.
Before starting any development, you should check to make sure all tests pass.
You should always be sure that your work starts from a known stable baseline.
If you’re a test-driven development (TDD) practitioner, you’ll start by writing new tests for the use case (for more about TDD, see chapter 5)
In general, the tests will show that your use case isn’t supported by not compiling or failing when executed.
Once you write the code to implement (correctly) the use case, the tests pass, and you can check in your code.
Non-TDD practitioners implement the use case first and then write the tests.
Once the tests pass, the developer checks in the code.
Before you move on to code the next feature, you should have tests to prove the feature works.
After you implement the feature, you can run the tests for the entire project, ensuring that the new feature didn’t break any existing tests.
If the existing code needs to change to accommodate the new feature, you should update the tests first and then make the changes.
If you test rigorously, both to help you write new code (TDD) and to ensure existing code works with new features (regression testing2), you must continually run the unit tests as a normal part of the development cycle.
You need to be able to run these tests automatically and effortlessly throughout the day.
Running a single JUnit test case against a single class isn’t difficult.
But it isn’t practical to run continuous tests in a project with hundreds or even thousands of classes.
A project that’s fully tested can have as many test classes as production classes.
You can’t expect developers to run an entire set of regression tests every day manually.
Therefore, you need a way to run tests easily and automatically.
Because you’re writing so many tests, you need to write and run tests in the most effective way possible.
Ant is the de facto standard tool for building Java applications; it’s an excellent tool for managing and automating JUnit tests.
Compiling and testing a single class, like the DefaultController class from chapter 3, isn’t difficult.
Compiling a larger project with multiple classes can be a huge headache if your only tool is the javac command-line compiler.
When the number of classes goes up, more classes need to be on the compiler classpath.
Usually, in any one build you’ll have changed only a few classes, which leads us to the issue of minimizing how.
Rerunning your JUnit tests by hand after each build can be equally inconvenient for the same reasons.
Ant is not only an essential tool for building applications but also a great way to run your JUnit tests.
One reason for Ant’s popularity is that it’s more than a tool: Ant is a framework for running code.
In addition to using Ant to configure and launch a Java compiler, you can use it to copy files, run JUnit test suites, and create reports.
You configure Ant through an XML document called a build file, which is named build.xml by default.
The Ant build file describes each task that you want to perform in your project.
A build file can have several targets, or entry points, so that you can run a single target or chain several targets together.
Let’s look at using Ant to run tests automatically as part of the build.
If you don’t have Ant installed, see the following sidebar.
Now that you’re familiar with Ant and have it installed, let’s get started with a project.
Unzip the zip distribution file to a local directory (for example, C:\Ant)
Add an ANT_HOME variable to your environment with this directory as the value, for example:
Edit your PATH environment variable to include the %ANT_HOME%\bin folder:
To enable Ant’s JUnit task, copy the file junit.jar to the %ANT_HOME%\lib folder.
Ant will add the JAR file to the classpath for your build.
When you build a project, you often want to produce more than binary code.
For a distribution, you may also want to generate Javadocs.
For an internal development build, you might skip Javadoc generation.
At times, you may want to run a build from scratch.
You may also want to compile only the classes that have changed.
To help you manage the build process, a build file may have several targets, encapsulating the different tasks needed to create your application and related resources.
To make the build files easier to configure and reuse, Ant lets you define property elements (similar to a constant in programming, or a final field in Java)
Build file—Each build file is usually associated with a particular development project.
Ant uses the project XML tag as the root element in a build file to define the project.
It also lets you specify a default target, so you can run Ant without any parameters.
Target—When you run Ant, you can specify one or more targets to execute.
If you ask Ant to run one target, Ant will execute its dependent targets first.
This lets you create, for example, a distribution target that depends on other targets, like clean, compile, javadoc, and war.
Untar the Ant tarball to a local directory (for example, /opt/ant)
We recommend that you specify the location of your JDK as the JAVA_HOME environment variable:
Ant will add the JAR file to the classpath for your build.
As we mentioned, Ant isn’t so much a tool as a framework for running tools.
You can use property elements to set the parameters a tool needs and a task to run that tool.
A great number of tasks come bundled with Ant, but you can also write your own custom tasks.
For more about developing with Ant, we recommend reading Ant in Action3 and exploring the Ant website.
This segment of the build file sets the default target and the properties your targets and tasks will use.
The listing starts B by giving the project the name example and setting the default target to test.
Because programmers may store JAR files in different locations, it’s a good practice to use a build.properties file to define their locations.
Many open source projects provide a sample build.properties file you can copy and edit to match your environment.
We use the property Ant task to define the directories for production and test source code D as well as the location of the directories for compiled production and test code E.
It’s a good practice to define source and test directories in separate locations for both source and compiled code.
In our example, we end up with four property definitions.
Splitting out compiled production and test code makes it easy to build other artifacts such as JAR files because all compiled production code is rooted in its own directory.
An important aspect of Ant properties is that they’re immutable.
Once you set a property value, you can’t change it.
If you try to redefine the value with another property element, Ant ignores the request.
For simple projects, running the javac compiler from the command line is easy enough.
For products with multiple packages and source directories, the configuration of the javac compiler becomes more complicated.
Ant provides a task for almost all build-related jobs that you’d otherwise do on the command line, including dealing with repositories like CVS.
You can also find tasks not defined by Ant itself that deal with just about anything.
We mentioned CVS, but if you’re using Subversion for revision control, you can use SvnAnt4 out of the Subclipse project.
A standard practice is to create compile targets in your build file to invoke the Ant javac task to compile production and source code.
The javac task lets you set all compiler options, such as the source and destination directories, through task attributes.
Listing 9.2 shows the production and test compile targets that call the Java compiler.
When you call the compile.test target, if Ant hasn’t called the compile.java target yet, it does so immediately.
Ant targets, projects, properties, and tasks already on your classpath, you don’t need to specify it in the javac task to compile your tests.
You need to add a nested classpath element G in order to add the production classes you just compiled to the classpath because the test classes call the production classes.
Last, we have a compile target H that depends on the compile.java and compile.
We can get Ant to perform both steps as part of the same build target.
We declare the test target and define it to depend on the compile target B.
If we ask Ant to run the test target, it’ll run the compile target before running the test target (unless Ant has already called compile)
The only task defined in this target is to call junit C.
The junit printsummary attribute causes the task to print a one-line summary at the end of the test.
Setting fork to yes forces Ant to use a separate Java Virtual Machine for each test.
Although this is a performance hit, it’s a good practice if you’re worried about interference between test cases.
The haltonfailure and haltonerror attributes direct the build to stop if any test returns an error or a failure.
In Ant, an error is an unexpected error, like an exception, whereas a failure is a failed assert call.
We configure the junit task formatter to use plain text and output the test result to the console D.
The test name attribute defines the class name of the test to run E.
Finally, we extend the classpath to use for this task to include the production and test classes we just compiled F.
This makes up our first test target; next, we run the Ant build.
Now that you’ve assembled the build file, you can run it from the command line by changing to your project directory and typing ant.
We can now build and test the project at the same time.
If any of the tests fail, the haltonfailure and haltonerror settings will stop the build, bringing the problem to our attention.
So far, we’ve looked at one way to execute tests with Ant.
We now look at another aspect of the build process: dependency management.
In order to automate management of dependencies for Ant projects, we introduce and use the Apache Ivy5 project.
Figure 9.1 Running the build file from the command line.
When your project is small, it can be easy to deal with the JAR files your code depends on.
In our previous example, we depended on only one JAR file, junit.jar.
For larger projects, your build may end up depending on dozens of libraries.
These dependencies can trip up new developers or downstream developers of the project.
Having to know what the JAR dependencies are and where to get them on the web should not be an impediment to developers using your project.
Maven dependency management is based on the concept of one or more (internet, network, or local) repositories containing JARs from many projects from all over the open source world.
The developer lists dependencies for a project in a configuration file and lets Maven download the right files to a local repository cache on your machine.
You can then add the JARs to your classpath based on their location in the local repository.
Apache Ivy6 is a popular open source dependency management tool (used for recording, tracking, resolving, and reporting dependencies), focusing on flexibility and simplicity.
Although available as a standalone tool, Ivy works particularly well with Ant, providing a number of powerful Ant tasks ranging from dependency resolution to reporting and publication.
It’s out of the scope of this book to cover Ivy in depth, but we rework our build file with dependency management using Ivy.
Ivy works in the same manner as Maven and even uses Maven repositories for resolving and downloading dependencies.
You specify the dependencies for your project in a file named, by default, ivy.xml.
Ivy will download all the dependencies listed in the ivy.xml file into a local cache directory.
Listing 9.4 shows the build file with changes highlighted in bold.
Listing 9.4 Adding the Ivy changes to the build file.
The file ivy.xml defines the project dependencies as shown in listing 9.5
First, the root tag ivy-module B defines the version of Ivy we want to use (in this case 2.0)
Then, the info tag C defines the organization and the module name for which we’re defining dependencies.
Finally, the dependencies nested element D is where we specify our dependencies.
In the next section, we try out another kind of report.
The console output from figure 9.1 might be acceptable when you’re running tests interactively, but you can’t use the output if you want to examine the results later.
A (cron) job or a continuous integration server (see chapter 11) might run the tests automatically every day such that the console output might not be available.
The junit task can produce XML documents detailing the results of various test runs.
Another optional Ant task, junitreport, transforms this XML into HTML using an XSL stylesheet.
The result is a report you can view with any web browser.
Figure 9.3 shows a report page for the example project.
Listing 9.6 shows the changes necessary (in bold) to the build file to generate this report.
Listing 9.6 Adding a JUnitReport task to the build file.
First, we define a property for the location where the report will be generated B and then create that directory C.
We modify the junit task to output the test results as.
We begin the report target by creating the directory where the HTML report will be generated G.
We call the junitreport task to create the report H.
The task scans the XML test results specified as an Ant fileset I and generates the HTML report to our specified location J.
Our current build file calls the junit task through the test target to invoke specific test cases.
Although this is fine for a small set of test classes, it may become unwieldy for larger sets of classes.
One way to remedy this situation is to group tests together in a test suite and invoke the test suite from Ant.
Alternatively, you can direct Ant to batch tests together by using wildcards to find tests by class names.
The future of JUnit reports The Ant junit task produces XML output containing detailed results of the execution of JUnit tests.
This task isn’t the only tool to produce XML documents of this format; so does Maven’s Surefire plug-in.
Note that in the previous version of JUnit there was no special status for skipped tests.
We should expect the XML schema for JUnit reports to evolve, a topic currently under discussion on the Apache Ant7 wiki.
The tests property defines B the class name pattern used by the batchtest element C later in the listing.
Defining this property allows us to override it from the command line or by another property definition.
This allows us, for instance, to run a single test case or provide a value that runs a narrower set of tests.
This technique provides us a shortcut to run the test for any given class we’re working on while leaving the default value to execute the full set of tests.
The batchtest element C makes the test target and our build more flexible.
It’s always a good practice to include a clean target to remove all build-generated files.
Doing so lets us build from first principles (in our case, Java source files), removing potential side effects from obsolete classes.
Typically, a dist (for distribution) target generates all project distributable files and depends on the clean target.
You should give thought to your test class names such that you can match them using a reasonable pattern.
Although automated tests can find a significant number of bugs, manual testing is still required to find as many bugs as possible.
Are you sure about that? Some test-first enthusiasts are now reporting remarkably low numbers of bug counts, approximately one to two per month or fewer.
In this chapter, we introduced Apache Ant, one of the best tools for building Java software.
We looked at the basics of an Ant build file and described key tasks: javac junit and junitreport.
These tasks allow you to compile Java code, run JUnit tests, and create HTML test reports.
We also introduced Apache Ivy to manage with ease your project’s JAR file dependencies.
Ivy resolves and downloads JAR file dependencies for your build.
In the following chapters, we continue to explore the continuous integration paradigm with Maven, another tool for building software.
We also look at how Maven handles dependency management in comparison to Ivy.
In this chapter we discuss and reveal another common build system tool called Maven.
We also present a brief introduction to this build system, which will be useful if you’re new to it or need a way to start your tests continuously.
People sometimes come to Maven thinking that it will be something like Ant.
Once they discover that it’s totally different, they get frustrated.
This is why we spend the first few pages of this chapter explaining what’s most essential in order to understand Maven: how it’s different from Ant.
The conventional view serves to protect us from the painful job of thinking.
Maven’s features that, we present some real-world examples of compiling your test cases and running them as well as producing fancy reports.
By the end of this chapter, you’ll know how to set up your environment on your machine to build Java projects with Maven, including managing their dependencies, executing JUnit tests, and generating JUnit reports.
Once you’ve used Ant on several projects, you’ll notice that projects almost always need the same Ant scripts (or at least a good percentage of them)
These scripts are easy enough to reuse through cutting and pasting, but each new project requires a bit of fussing to get the Ant buildfiles working just right.
In addition, each project usually ends up having several subprojects, each of which requires you to create and maintain an Ant buildfile.
Maven (http://maven.apache.org/) picks up where Ant leaves off, making it a natural fit for many teams.
Like Ant, Maven is a tool for running other tools, but Maven is designed to take tool reuse to the next level.
If Ant is a sourcebuilding framework, then Maven is a source-building environment.
In order to understand better how Maven works, you need to understand the key points (principles) behind Maven.
Maven was designed to take the build systems to the next level, beyond Ant.
You need to become familiar with the things that the Maven community didn’t like in Ant and how it tried to avoid them in designing Maven, that is, the core reason for starting Maven as a whole new project.
From the beginning of the Maven project, certain ground rules were in place for the entire software architecture of the project.
These rules aim to simplify development with Maven and to make it easier for developers to implement the build system.
One of the fundamental ideas of Maven is that the build system should be as simple as possible—software engineers should not spend a lot of time implementing the build system.
It should be easy enough to start a new project from scratch and then rapidly begin developing the software, not spend valuable time designing and implementing a build system.
In this section we describe each of the core Maven principles in detail, and we explain what they mean to us, from a developer’s point of view.
This feature is a software design principle that aims to decrease the number of configurations a software engineer needs to make, in favor of introducing a number of conventional rules that the developers need to follow.
This way you, as a developer, can skip the tedious configuration required for every single project, and you can focus on the more important parts of your work.
Convention over configuration is one of the strongest principles of the Maven project.
As an example of its application, let’s look at the directory structure of the build process.
When the Maven project was started, some of the initial Maven developers noticed that for every Ant buildfile, the person who writes the buildfile has to design.
And although the Ant community tries to imply some directory names and directory structure, there’s still no official specification (convention) of how a directory should be named.
For instance, many people declare the target.dir property to denote the directory that holds their compiled classes, whereas others may be accustomed to using the build.dir property and it seems unnatural to them to use target.dir.
Also, many people place their source code in the src/ directory, but others put it in the src/java/ directory.
The Maven team decided that instead of allowing the software engineers to choose the build structure every time themselves, they’d introduce a convention for this.
This resulted in what’s now called the “Maven convention of directory structure.” With Maven, instead of defining all the directories you need, they’re defined for you.
For example, the src/ main/java/ directory is the Maven convention for where the Java code for the project resides, src/main/test/ is where the unit tests for the project reside, target is the build directory, and so on.
Later on when we discuss the plug-ins themselves, you’ll see how every plug-in has a default state already defined, so you don’t need to define it again.
That sounds great, but aren’t we losing some of the flexibility of the project? What if we want to use Maven, and our source code resides in another directory? Maven is great at this.
It provides the convention, but you still can, at any point, override the convention and use the configuration of your choice.
At the time the project was started, the de facto build system for Java projects was Ant.
With Ant you have to distribute the dependencies of your project with the project itself.
Maven introduced the notion of a central repository—a location on the internet where all kinds of artifacts (dependencies) are stored.
The Maven build tool resolves them by reading your project’s build descriptor, downloading the necessary versions of the artifacts, and then including them in the classpath of your application.
This way you only need to list your dependencies in the dependencies section of your build descriptor, as shown here:
Then you’re free to build the software on any other machine.
There’s no need to bundle the dependencies with your project and so forth.
But Maven introduced also the concept of the local repository.
Also, after you’ve built your project, your artifacts are installed in the local repository for later use by some other projects—simple and neat.
The Maven project is built around the idea of defining the process of building, testing, and distributing a particular artifact.
This way we can use Maven for building the project’s artifact, or cleaning the project’s directory structure, or generating the project’s documentation.
The activities we use Maven for define the three built-in lifecycles of Maven:
Each of these lifecycles comprises several phases, and in order to pass through a certain lifecycle, the build must follow its phases.
Here’s a list of all the phases of the default lifecycle:
Validate—Validate that the project is correct and all necessary information is available.
If you remember, with Ant we had targets with almost the same names.
And yes, the targets in Ant are the analogue of the phases in Maven, with one exception.
In Ant you write the targets and you specify which target depends on which other target.
And Maven invokes these phases in a strict order: they get executed sequentially in the order listed to complete the lifecycle.
This means that if you invoke any of them, for example, if you type.
One last thing—it’s useful to think of all these phases as extension points.
At any moment you can attach additional Maven plug-ins to the phases and orchestrate the order and the way these plug-ins are executed.
The last feature of Maven that we’ll mention is its plug-in-based architecture.
At the beginning of this chapter, we said that Ant is a source-building framework and Maven is a source-building environment.
The core of the project is small, but the architecture of the project allows multiple plug-ins to be attached to the core, and so Maven builds an environment where different plug-ins can get executed.
Each of the phases in a given lifecycle has a number of plug-ins attached to that phase, and Maven invokes them when passing through the given phase in the order in which the plug-ins are declared.
Apart from these core Maven plug-ins, there are also dozens of other Maven plug-ins for every kind of situation you may need—WAR plug-in, Javadoc plug-in, AntRun plugin—you name it.
Plug-ins are declared in the plugins section of your build configuration file, for instance:
As you can see, every plug-in declaration specifies groupId, artifactId, and version.
With this the plug-ins look like dependencies, don’t they? Yes, they do, and they’re handled the same way—they’re downloaded into your local repository the same way dependencies are.
If you’re using a Maven version pre-2.0.9, Maven will try to download the latest version available.
But as of Maven version 2.0.9, the versions of most plug-ins are locked down in the Super POM, so it won’t download the latest version anymore.
Locking down plug-in versions is highly recommended to avoid autoupdating and nonreproducible builds.
Tons of additional plug-ins are available outside the Maven project but can be used with Maven.
The reason for this is that it’s extremely easy to write plug-ins for Maven.
If you remember, Ant has a buildfile, by default named build.xml, that holds all of the information for our build.
In that buildfile we specify all the things that we want to accomplish in the form of tasks and targets.
What’s the analogue in Maven of Ant’s build.xml? Maven also has a build descriptor that’s by default called pom.xml, shortened from Project Object Model (POM)
In contrast to Ant, in Maven’s project descriptor we don’t specify the things we want to do; we specify general information for the project itself, as in listing 10.1
It looks simple, doesn’t it? But one big question arises at this moment: “How is even Maven capable of building our source code with so little information?”
The answer lies in the inheritance feature of the pom.xmls: every simple pom.xml inherits most of its functionality from a Super POM.
This analogue goes even further: Maven pom.xmls can inherit from each other, just as in Java some classes can act as.
For instance, if we want to use the pom from listing 10.1 for our parent, all we have to do is change its packaging value to pom.
Parent and aggregation (multimodule) projects can have only pom as a packaging value.
We also need to define in our parent the child modules, as shown in listing 10.2
We declare that this pom is an aggregation module by declaring the package to be of pom type B and adding a modules section C.
The modules section lists all the child modules that our module has, by providing the relative path to the project directory (example-module)
Remember that this pom.xml resides in the directory that the parent pom.xml has declared (example-module)
First, because we inherit from some other pom, there’s no need to specify groupId and version for the child pom; Maven expects they’re the same as the parent’s.
Going with the similarity to Java, it seems reasonable to ask, “What kind of objects can poms inherit from their parents?” Here’s a list of all the elements that a pom can inherit from its parent:
And again, each of these elements specified in the parent pom get automatically specified in the child pom.
Now that you’ve seen the differences between Ant and Maven, it’s time to move on and start building our projects with Maven.
This is our work directory, and here we set up the Maven examples.
After you press Enter and wait for appropriate artifacts to download, you should see a directory named maven-sampling being created.
If you look inside that directory, you should see the directory structure being created, as shown in figure 10.1
As a result, this Maven plug-in created a new project with a new directory structure, following the convention of the directory structure.
Further, it created a sample App.java class with a main method and a corresponding AppTest.java file that’s a unit test for our application.
Now, most likely, after looking at this directory structure, you’re quite familiar with which files stay in src/main/java and which files stay in src/ test/java.
Download the latest distribution from http://maven.apache.org/ and unzip/ untar it in the directory of your choice (for example, C:\maven on Windows or /opt/maven on UNIX)
The first time you execute a plug-in, make sure your internet connection is on, because Maven will automatically download from the web all the third-party JARs the plug-in requires.
The Maven plug-in also generated a pom.xml file for us.
Let’s open it and examine the different parts of the descriptor, shown in listing 10.4
It starts with a global <project> tag with the appropriate namespaces, inside which we place all of our components:
Notice that this is the value that we provided on the command line when invoking Maven.
The groupId acts as the Java packaging in the filesystem; it groups together different projects from one organization, company, group of people, and so on.
Again, the value here is the one we specified on the command line.
The artifactId represents the name the project is known by.
This ending denotes that this artifact is still in development mode; we haven’t released it yet.
Now that we have our project descriptor, let’s improve it a little, as shown in listing 10.5
After that, we can make some additional information in the pom.xml more descriptive, like adding a developers section.
This information not only makes the pom.xml more descriptive, but it will also be included later on when we build the website.
Listing 10.6 continues the previous one, showing the organization, description, and inceptionYear elements.
No problem—Maven offers additional plug-ins to import the project into our favorite IDE.
For instance, we use Eclipse to show you how this import happens.
Again, open a terminal and navigate to the directory that contains your project descriptor (pom.xml)
The downloadSources parameter that we specify in the command line is optional.
By using it we instruct the plug-in to also download source attachments.
You can also download the Javadoc attachments by setting the optional downloadJavadocs parameter to true on the command line.
Now you can import your project into Eclipse and examine it; you’ll notice that all of the dependencies that are listed in the pom.xml file are now added to your buildpath.
But wait a second, how are we supposed to do that? We don’t have any files to generate the documentation from.
This is another one of Maven’s great features—with the little configuration and description that we have, we can produce a fully functional website skeleton.
Maven should start downloading its plug-ins, and after their successful installation, it will produce the nice website you see in figure 10.2
Maven uses the target/ directory for all the needs of the build itself.
The convention continues even beneath this directory: source code is compiled in the target/classes/ directory, and the documentation is generated in target/site/
After you examine the project, you’ll probably notice that this website is more like a skeleton of a website.
That’s true, but remember that we entered only a small amount of data to begin with.
We could enter more data and web pages in the src/site directory, and Maven will include it in the website, generating full documentation.
You’ve seen what Maven is and how to use it to start a project from scratch.
You’ve also seen how to generate the project’s documentation and how to import our project in Eclipse.
To continue, we can get the source code from the first part of the book and place it in the src/main/java directory, where Maven expects it to be.
Also, we can get the tests for the sampling project and place them in the src/test/java directory (again a convention)
Now it’s time to invoke Maven and instruct it to compile the source code and tests and also to execute the tests.
But first we need to clean the project from our previous activities:
Figure 10.2 Maven produces nice website documentation for the project.
Like any other build system, Maven is supposed to build your projects—compile your software and package in an archive.
As we mentioned at the beginning of the chapter, every task in Maven in done by an appropriate plug-in, the configuration of which happens in the <plugins> section of our project descriptor.
To compile your source code, all you need to do is invoke the compile phase on the command line:
But before invoking the compile phase, as already discussed, Maven will go through the validate phase and will download all of the dependencies that are listed in the pom.xml and include them in the classpath of the project.
Once the compilation process is completed, you can go to the target/classes/ directory, and you should see the compiled classes there.
Let’s move on and try to configure the Compiler plug-in.
So far, we’ve used the conventional Compiler plug-in, and everything worked well.
There you list each of the plug-ins that you want to configure.
You need to enter the configuration parameters in the plug-in’s configuration section.
You can get a list of parameters for every plug-in from the Maven website.
The same thing happens with the process of unit testing your project; Ant uses the junit task and executes the test cases that we’ve selected, whereas Maven uses—guess what?a plug-in.
Notice the italics in the previous sentence; the Surefire plug-in is used to execute the unit tests for your code, but these unit tests aren’t necessarily JUnit tests.
There are also other frameworks for unit testing, and the Surefire plug-in can execute their tests, too.
All you need to do is invoke the test phase of Maven.
Maven first cleans the target/ directory, then compiles the source code and the tests, and finally executes all of the tests that are in the src/test/java directory (remember the convention)
The output should be similar to that shown in figure 10.3
Hopefully there’s a parameter for the plug-in that allows us to specify a pattern of test cases that we want to be executed.
The configuration of the Surefire plug-in is done in the same way as the configuration of the Compiler plug-in, and it’s shown in listing 10.8
As you saw in the previous chapter, Ant has a task for generating nice reports out of JUnit’s XML output.
As you’ve already guessed, the job for producing these reports is done by a Maven plug-in.
The bad side of Maven invoke it by running a certain phase (as we did with both the Compiler plug-in and the Surefire plug-in), but instead we have to call it directly from the command line:
When we do so, Maven will try to compile the source files and the test cases and then invoke the Surefire plug-in to produce the plain text and XML-formatted output of the tests.
If you try to open the generated HTML report, it should look something like the one shown in figure 10.4
In the next section, we cover some of Maven’s weaker points.
So far, we’ve discussed Maven as the tool that’s going to take the place of Ant.
But as with all other things in life, it’s not all a bed of roses.
Maven has been out since 2002, and still Ant is the de facto standard for building software.
But things seem to break when you need to do some of the unconventional things.
What’s great about Maven is that it will set up a frame for you and will constrain you to think inside that frame—to think the Maven way and do things the Maven way.
When you work with Maven for a certain period of time, you’ll inevitably need to copy a file from one place to another.
We think you’ll be surprised to see that there’s no copy plug-in in Maven, in contrast with Ant, which has the copy task.
You have to deal with the situation, and if you investigate further and think “the Maven way,” it may turn out that you never needed to copy that file.
In most cases Maven won’t let you do any nonsense.
It will restrict you and show you the way things need to be done.
Ant works the other way: it’s a powerful tool, and you can do whatever you need to do.
And again, it’s up to you to decide which of these tools you want to use.
Some companies hire build engineers, who are considered to have the appropriate knowledge, so for them Ant is no danger at all—it’s just a powerful tool.
We have a friend who once had a job interview.
Without even letting the interviewer to finish his sentence, my friend replied, “Yes, it is.” We’re not sure what his answer would have been if he’d been asked about Maven.
In this chapter we briefly introduced you to what Maven is and how to use it in a development environment to build your source code.
We discussed in detail all of the features of Maven that make it unique compared to any other build system.
We also looked at two of Maven’s plug-ins in detail: the Compiler plug-in and the Surefire plug-in.
With this information you should be able not only to start and execute your tests but also to produce nice HTML reports of the test results.
In the next chapter we close the automation part of the book, by introducing different continuous integration build tools, such as CruiseControl and Hudson.
We show you the benefits of using such tools and also how to install and configure these tools.
In the two previous chapters we described ways to execute our tests automatically by using tools such as Ant and Maven.
Now it’s time to go to the next level: automatically executing the build and the tests at a regular interval by using some other popular tools.
In this chapter we introduce the paradigm of continuous integration and show you how to schedule your project to be built automatically in a certain timeframe.
Integrating the execution of JUnit tests as part of your development cycle—code : run : test : code (or test : code : run : test if you’re test-first inclined)—is an important Continuous integration tools.
A great many of the projects out there, however, have modular architecture, where different developers on the team work on different modules of the project.
Each developer takes care of developing their own module and their own unit tests to make sure their module is well tested.
Modules interact with each other, so we need to have all the different modules assembled to see how they work together.
In order for the application to be test proven, we need another sort of test: integration or functional tests.
As you already saw in chapter 3, these tests test the interaction between different modules.
But almost always integration tests are time consuming, and as a single developer you may not have all the modules built on your machine.
Therefore, it makes no sense to run all the integration tests during development.
That’s because at development we’re focused on only our module, and all we want to know is that it works as a single unit.
During development we care mostly that if we provide the right input data, the module not only behaves as expected but also produces the expected result.
Test-driven development taught us to test early and test often.
Executing all our unit, integration, and functional tests every time we make a small change would slow us immensely.
To avoid this, we execute at development time only the unit tests—as early and as often as reasonable.
Integration tests should be executed independently from the development process.
The best way is to execute them at a regular interval (say, 15 minutes)
This way, if something gets broken, you’ll hear about it within 15 minutes, and there’s a better chance for you to fix it.
DEFINITION1 Continuous integration (CI)—Continuous integration is a software development practice whereby members of a team integrate their work frequently.
Usually each person integrates at least daily, leading to multiple integrations per day.
Each integration is verified by an automated build (including a test) to detect integration errors as quickly as possible.
Many teams find that this approach leads to significantly reduced integration problems and allows for the development of cohesive software more rapidly.
To get the integration tests executed at a regular interval, we also need to have the modules of the system prepared and built.
After the modules are built and the integration tests are executed, we’d like to see the results of the execution as quickly as possible.
This definition is taken from a marvelous article by Martin Fowler and Matthew Foemmel.
We need a software tool to do all of the following steps automatically:
Build each of the modules and execute all of the unit tests to verify that the different modules work as expected in isolation.
First, what’s the difference between a human executing all these steps and a tool doing so? The answer is, there’s no difference and there shouldn’t be! Apart from the fact that no one can bear doing such a job, if you take a close look at the first item in the list, you’ll see that we check out the project from the source control system.
We do that as if we were new members of the team and just started with the project—with a clean checkout in an empty folder.
Then, before moving on, we want to make sure that all of the modules work properly in isolation, because if they don’t, it makes little sense to test whether they integrate well with the other modules.
The last step in the proposed scenario is to notify the developers about the test results.
The notification could be done with an email, or an ICQ message, or by publishing the reports from the tests on a web server.
The CI tool interacts with the source control system to get the project (1)
Finally (4), the CI tool publishes the results and blows the whistle so that everybody can see them.
The four steps are general and could be greatly improved.
For instance, it would be better to check to see if any changes have been made in the source control system.
Otherwise, we waste the CPU power of our machine, knowing for sure that we’ll get the same results.
Now that we agree that we certainly need a tool to continuously integrate our projects, let’s see which open source solutions we might want to use (there’s no sense in reinventing the wheel when there are good tools already made for us)
The first open source2 project we’re going to look at is called CruiseControl (CC) (http://cruisecontrol.sourceforge.net/) and is currently the de facto standard when it comes to continuous build process.
This project was created by a company called ThoughtWorks and was the first continuous integration server available.
The first thing to do before we can start building our code continuously is to manage our resources.
By this we mean finding a suitable host machine for our CruiseControl server.
We need to manage the resources we have in such manner as to dedicate a separate machine to continuous integration.
We may not need this at the beginning, but with time our project will get bigger, and the CI build will take longer, so it’s always better to have a separate machine for the integration build.
Another good practice is to create an ad hoc user on the host machine for the CC server.
This user will have the right permissions to start the continuous server and execute the builds.
Once you find the host for the CruiseControl server, it’s time to install it.
As this book is being written, the latest version of CruiseControl is 2.8.3, and we use the binary distribution.
Once you download the distribution, you need to extract it into a folder (probably the best place to extract the zip is in the home directory of the CC user you just created), which from now on we refer to as $CC_HOME.
There are several things to notice in the folder structure within the $CC_HOME folder.
As you can see, the CruiseControl package comes with an Apache Ant distribution.
But this doesn’t mean that you can’t use Maven as a build system.
This folder is where you store the projects you want to build continuously.
If you look in that folder you’ll see a sample project called connectfour.
This is a checked-out project, so before moving on it’s good to take a brief look at it.
The CruiseControl framework is distributed under its own BSD-style license.
Now that we’ve found a host for our CruiseControl installation and we’ve seen how the project is structured, let’s start using it by setting up a project to build.
If you’re using Subversion as your revision control system, you can easily check out your project by first navigating to the $CC_HOME\projects folder and then executing the following command there:
You need to specify the URL of your source repository and also the name of the folder in which to check out the project.
Then Subversion will check out a local copy of your project in the theNameOfMyProject folder.
Now that we have the project checked out in the projects folder, it’s time to configure CruiseControl to build it the way we want.
You could choose to use this distribution or any later version.
By default there’s a sample project already set up in this folder called connectfour.
This config file describes the build for the connectfour project.
We’ll walk through it and explain what the different parts mean, so that later you can modify the script with your project’s configurations.
We start with a global cruisecontrol tag b, which is required, and inside that global tag we list all the projects we want to build C.
We can have multiple projects being built, but that means that we must give all the projects a distinct name (using the name attribute of the project tag)
The listeners element D is used mainly to enlist different pluggable listener instances.
These listeners are notified of every project build event (like the start or end of the project)
Listeners in CruiseControl are interesting creatures, so we’d like here to spend a few lines on them.
As you’ve probably noticed, building the project on a regular schedule wastes a lot of resources.
CruiseControl is smart in that it doesn’t fire a new build unless it detects some changes in the source control system.
Otherwise, the build would be pointless, because we’d get the same results.
But how do we distinguish whether the CruiseControl server was up and skipped the build because there were no changes in the source control system or the server was down? The solution is the listeners.
You can configure CruiseControl’s listeners to log everything in a file, somewhere on the filesystem.
The upside is that the events that activate the listeners are triggered regardless of the state of the source control system.
This way you can keep track of whether a build was attempted.
The next thing in our config.xml is the bootstrappers element E.
This is a container element to define some actions that need to be taken before the build is executed.
Again, the bootstrappers are run regardless of whether a build is necessary or not.
In our config file we’ve specified an antbootstrapper F that will invoke the clean target of our project’s build descriptor and will clean all the resources we’ve used during the previous build.
The modificationset element G defines the sets of files and folders that CruiseControl will monitor for changes.
One thing to remember here is that the build is attempted only when a change is detected on any of the sets listed in the modificationset element.
The schedule element H is the one that schedules the build.
The interval parameter specifies the build time interval (in seconds), and inside the schedule element we list the type of build system we’re using.
We need to specify the home folder of the build system and also the buildfile to execute.
In our config file we’ve specified Ant, but you can use Maven without any problems as well.
The log section I is optional and is used to specify where the logs of the execution should be stored.
The merge element inside it tells CruiseControl which logs of the build execution are valuable and should be stored in the log directory of the CruiseControl execution.
In our example, we only care about the .xml files from the JUnit execution, so we’re going to store them in the log folder.
The last section is the publishers section J, and it’s used to make the final steps of the scheduled build.
The same ways the bootstrappers are executed every time before the build, the publishers are executed every time after the build, regardless of the result of the build.
In publishers we can specify what whistles are blown as the build finishes.
It could be sending an email, publishing the produced artifacts somewhere on the internet, or posting a message on the Jabber Messenger.
We do that only if the result from the execution is successful (see the onsuccess element)
So far, so good—we’re able to start CruiseControl, and according to the configuration in the listing, it will build our project every 300 seconds (if there’s a change in the source control system)
Navigate to the $CC_HOME folder and start the CruiseControl server by issuing the following command:
CruiseControl will be executed and will look for the config.xml file to read.
If you’ve done everything right, after execution you should see something similar to figure 11.2
Now it’s time to make a change in the repository, and hopefully after 300 seconds you’ll see the build being executed again.
Here’s an interesting question: how often should you build your projects? This is a tough question, and it all depends on you and on the time you need to make a new build.
It makes little sense to make a build every minute when you need more than a minute to execute the build itself.
Keep in mind that this is a book about software testing, and we all propagate a lot of testing, using not only JUnit but also all kinds of integration and functional testing tools.
This means that production builds tend to be time consuming.
You need to check to make sure that the .sh script has executable modifiers set.
CruiseControl also provides a way to control your scheduled builds through a nice GUI, by starting a Jetty instance.
By default, this Jetty instance is started on port 8080, but you can change it (in case that port is already taken)
To do so you need to change the port property in your jetty.xml file in the etc folder.
You can check the GUI by visiting http://localhost:8888/ cruisecontrol/ in a browser.
You should be able to see something like the screen displayed in figure 11.3
As you can see, there some nice details on how many builds were iterated, some detailed statistics on how many of them failed, and, of course, the JUnit logs.
There’s also an RSS feed that you could subscribe to, to get the results from the execution.
Some things are working, but currently the way they are doesn’t give us much data.
If we look in the console or the GUI, we can see that the build is going well, but it’s a tedious task to look there all the time.
What if we had some way to get the results from the execution directly into our email or, even better, get the email from CruiseControl only in case things go bad?
This is possible, and all we have to do is add another publisher in our config.xml.
The section we want to add is shown in listing 11.2
The htmlemail publisher defines the notification emails to be sent.
We start by defining the mailhost to use for sending the emails B and also from what address the emails are coming C.
These two, along with the buildresultsurl parameter (the location at which our build results reside), are required.
We’re able to specify a custom CSS stylesheet D and the path to our logs E.
The last touch would be to create aliases to which persons the notifications should go.
We use the map element to map the alias to an email.
After that, we specify on what occasion we want those guys to receive email notifications.
By default, CruiseControl delivers notification on both success and failure.
But that’s too much information for the development team, and that’s why we’ve listed them to receive only emails on failure and when the problems get fixed.
Now it’s time to restart the CruiseControl server and break the build on purpose.
As you can see, the report gives you information on not only which JUnit test failed but also the last guy to make a commit in the source control system.
It’s pretty easy to determine which member of the team gets the blame for breaking the integration.
CruiseControl has a pluggable architecture, and as you saw, you can plug different listeners, or bootstrappers, to do things before, during, or after the build execution.
You can also specify different publishers for different ways of notifying the results from the build execution.
Along with the htmlemail method we already covered, there’s a publisher to send an instant message on Yahoo! Messenger or on Jabber Messenger or by posting the results on a blog and collecting them through an RSS feed.
Let’s move on and take a look at another continuous integration server called Hudson.
After we’ve covered both of them, you can compare them and choose whichever one you want to use.
As we mentioned in the beginning of the chapter, CruiseControl was probably one of the first continuous integration servers available.
But there are a whole bunch of other software tools out there, trying to compete with CruiseControl by introducing some interesting new features.
Some of those tools aren’t even free (like AntHill Pro, Hudson, or Cruise4), and those include not only the product you purchase but also training and support.
For the sake of completeness, we need to cover another tool.
Remember, your software quality will improve not from the tool you choose to use but rather from the fact that you decided to practice continuous integration!
Hudson (http://hudson-ci.org/) is an open source project for continuous build.
Like any other software for continuous build, it’s based on the idea of being able to continuously poll the source code from the source control system and, in case it detects changes, to fire up a build.
Why do we cover it in this chapter? First, because it has become popular, and second, because it’s very different from CruiseControl.
Also make sure your JAVA_HOME environment variable points to where you’ve installed Java.
You go to the project’s website and download the latest version of Hudson.
At the time this book is being written, the latest version is 1.352
The Hudson distribution comes as a single WAR file, as opposed to CruiseControl, where the distribution is a zip.
You don’t need to extract the WAR file, because Hudson comes with a Winstone servlet container.
You can start the server from the command line with the following command:
Note first that we start Hudson on a port other than 8080 (simply because the examples in the book require this port to be free), and second, if you start Hudson this way, all of your logs will go to the console.
In order to start using the server, you need to navigate to http://localhost:8888/
If no errors occur, you should see something similar to what is shown in figure 11.5
Cruise and CruiseControl aren’t the same! Although they both originated from the same company, ThoughtWorks, CruiseControl was open sourced and is free to use.
There’s also a way to specify different command-line parameters, such as the one to redefine the port on which the server is started or the root under which the application is started.
Also, if you don’t want to use the Winstone servlet container, you can use any other servlet container you want.
If you stick with that solution, you’ll be forced to follow the installation procedures specific to the servlet container you use.
Hudson’s advantage over CruiseControl is easier configuration, which is done through the web interface.
Once you’ve installed Hudson, it’s time to start the configuration.
You should see the Hudson welcome screen, and there should be a Manage Hudson link on the left side.
Click it, and you’ll be given a list of additional links leading to the parts of the installation you want to configure.
Click Configure System, and it will open a web page similar to the one shown in figure 11.6
As you already saw, Hudson, in contrast to CruiseControl, comes with no Ant installation, so the tool needs to know where you’ve installed Ant, Maven, JDK, and the like.
You need to specify this information on the configuration page shown in figure 11.6
The first line on the configuration page is Home Directory.
The home directory of Hudson is an interesting creature, so we devote a subsection to it.
Inside the home directory, Hudson keeps a configuration file B, various plug-ins C, and all the jobs that it runs D.
The jobs, as they’re known in Hudson, are different projects that you build.
Each job can have multiple builds, so you can easily follow which one failed and the cause for the failure.
Moving forward in the configuration page, there are also some options to specify the path to your Ant installation (in case the project you want to build uses Ant) or.
You can also specify a JDK installation, a CVS installation, and email notification installations (such as the email server, username, and password)
Take note here that you don’t specify the path to your build.xml files, but instead you point to the place where Ant was installed, so that later on Hudson can to talk to that Ant installation and issue the ant –f build.xml command.
Now that you’ve configured Hudson to find the installations of Ant, Maven, and the others, you can move on and configure a new job.
To configure a new job with Hudson, first navigate to the main screen and select the New Job link from the list on the left side.
After that you’ll be presented with a sample form to fill in.
You need to specify a name for the job, and make sure you choose one of the presented build options.
Click OK, and you’ll be presented the job-configuration screen shown on figure 11.7
Here you’re given the ability to configure the way you want to build your job.
You use the first lines to specify or change the name and the description of the job.
After that are some options regarding the source control management (SCM) system you use (Subversion, CVS, and the like)
The next section tunes the settings for the build triggers—on what occasion you want to trigger your build.
You’re presented with several options: poll the SCM system to check whether a build is needed, build the project periodically, build it after some dependent projects were built, and so on.
Let’s select the Poll The SCM trigger; a field opens where we need to specify on what interval of time we want the poll to happen.
This field uses a nice syntax that follows the syntax of the UNIX cron tool.
We’d like to have our project executed every hour, so we specify @hourly in the.
You can learn more about the cron syntax if you click the corresponding question mark next to the trigger.
You can specify to execute a shell script, a Windows batch file, an Ant build file, or a Maven build file.
On any of these you can specify any parameters, targets, goals, and so on.
You can also arrange multiple build steps, such as first invoking a shell script and then running Ant.
There’s also the option to rearrange all these steps by dragging and dropping.
The options listed there will help you publish the artifact, publish the Javadoc, build some other project, send an email with the build results, or anything else you need.
After doing all this, you should save the job configuration.
This will lead you to the project’s home page (shown in figure 11.8)
From the job’s home page you can keep track of the current job.
From the menu on the left side, you can choose to see the changes someone has made on the job, inspect the workspace of the job, delete the project, configure it, or schedule another build.
You can also subscribe to the build results RSS feeds.
We don’t want to wait another hour for the build to be triggered, so let’s execute it right now.
On the job’s home page, click the Build Now link and wait for the build to finish.
Now you can see the results of the build execution on the job’s home page.
You can see not only when the last build was run, but also when the last successful build happened.
Clicking any build number lets you explore the build itself: which modules were built, which tests failed or succeeded, and, most important, why (see figure 11.9)
Once you spend some time using Hudson, you’ll probably find it a lot easier to use.
Its entire configuration is done through a nice web interface, and it’s relatively.
But the nicest thing is that the web interface is intuitive.
Another reason for this is that Hudson is currently undergoing rapid development.
It’s a mature project with a large community of developers who constantly improve the codebase.
From that point of view, it’ll be interesting to see how the project will evolve in time.
In general, the software tests are there to help you find your own errors.
You execute the unit tests every time you make any changes on the code you develop.
This way, they cover your back and will alert you whenever you introduce a new bug into the system.
The continuous integration servers have exactly the same purpose: they cover your back and alert you the moment you break the integration with the other modules of the system.
And because you can’t run the CI tools manually every time you make a small change in the system, they run on a separate host and try to continuously integrate your software.
But again, they protect you and will alert you to a change that breaks the integration.
We’ve heard many excuses from people as to why you shouldn’t use CI tools, and the winner is this one: “I don’t see the profit in using it.” And usually this comes from people who’ve never used a CI server.
We all make errors—face it! You and we, and everybody else—it’s human to err.
And no matter how good you are, you’ll sometimes make a mistake and introduce a bug into the system.
Knowing this, it seems reasonable to have something notify you when those errors occur.
Once you start using a CI tool, no matter which one—CruiseControl or Hudson or anything elseyou’ll see how good it is to know that something is watching your back and that an empty mailbox means that nothing is broken.
In this chapter we looked at two of the most popular CI tools: CruiseControl and Hudson.
The reason is to make you understand that continuous integration is an important concept in the modern software development lifecycle and that it makes absolutely no difference which tool you use, but it makes a great difference whether you use CI or not.
With this chapter we close the part of the book that deals with integrating JUnit with the build process.
You should now be ready to run your build and execute your tests with Ant or Maven and also to set up a continuous integration build and execute your builds and tests on a scheduled basis.
This way you have JUnit tests that protect your modules from new bugs, and you also have a workspace where you continuously execute your build and run your tests to see if anything got broken during the integration.
You’re now fully automated and ready to move on with the next chapters.
The next part of the book deals with testing different layers of your application.
We look at some examples of how to execute tests against the presentation layer and also against the database and persistence layers.
We show how to test your GUI components, and you can include those tests and run them in the continuous integration environment you just learned about.
This last part of the book deals with different kinds of JUnit extensions.
We cover all types of external projects that try to extend JUnit to the point where the testing framework falls short.
They test different aspects and layers of an enterprise application.
We show you how to test your presentation layer with these projects.
We go into details of not only how to set up your projects but also some best practices in testing your presentation layer.
Chapter 13 is dedicated, again, to testing your front end.
In that chapter we discuss how to test the Ajax part of your application.
We also introduce the JsUnit project and give some special hints on testing a Google Web Toolkit (GWT) application.
Chapter 14 introduces the Cactus project, which focuses on testing your Java EE core components (JSPs, servlets, EJBs, and the like)
Chapter 15 describes the JSFUnit project, which tests your JSF-based applications.
Chapter 16 is a small chapter that concentrates on testing component-oriented applications.
These chapters talk about the DBUnit project and give recipes for testing the JPA part of an application.
The last chapter describes how to create your own extensions by means of the Unitils and JUnit-addons projects.
Simply stated, presentation-layer testing means finding bugs in the graphical user interface (GUI) of an application.
Finding errors here is as important as finding errors in other application tiers.
A bad user experience can cost you a customer or discourage a web surfer from visiting your site again.
Furthermore, bugs in the user interface may cause other parts of the application to malfunction.
Because of its nature and interaction with a person, GUI testing presents unique challenges and requires its own set of tools and techniques.
We address here what can be objectively, or programmatically, asserted about the GUI.
Outside the scope of this discussion are whether the choice of subjective Presentation-layer testing.
If debugging is the process of removing software bugs, then programming must.
What we can test is the content of web pages to any level of detail (we could include spelling), the application structure or navigation (following links to their expected destination, for example), and the ability to verify user stories with acceptance tests.1 We can also verify that the site works with required browsers and operating systems.
We look at two free open source tools to implement presentation-layer tests within JUnit: HtmlUnit and Selenium.
HtmlUnit is a 100 percent Java headless browser framework that runs in the same virtual machine as your tests.
Use HtmlUnit when your application is independent of operating system features and browser-specific implementations not accounted for by HtmlUnit, like JavaScript, DOM, CSS, and so on.
Selenium drives various web browsers programmatically and checks the results from JUnit.
Selenium also provides a simple IDE to record and play back tests and can generate test code.
Use Selenium when you require validation of specific browsers and operating systems, especially if the application takes advantage of or depends on a browser’s specific implementation of JavaScript, DOM, CSS, and the like.
It allows tests to imitate programmatically the user of a browser-based web application.
The framework lets you test all aspects of a web application.
We describe here the most common tasks; for the rest, you’ll find the API quite intuitive and easy to use.
In the remainder of this HtmlUnit section when we talk about “testing with a web browser,” it’s with the understanding that we’re testing by emulating a specific web browser.
You can test now, assuming you can connect to the internet.
The test will go to the HtmlUnit website, navigate the Javadoc, and make sure a class has the proper documentation.
We get to the home page from the web client C and then to the list of classes on the page in the bottomleft frame D.
Next, we get the link for the class we’re interested in and click it as a user would E.
This gives us a new page for the link we clicked, which we then query for the first paragraph element F.
Finally, we check that the paragraph starts with the text we expect should be there G and release resources H.
This example covers the basics: getting a web page, navigating the HTML object model, and asserting results.
You’ll notice a lack of standard JUnit assertions in the code that navigates the HTML model.
If HtmlUnit doesn’t find an element or encounters a problem, it will throw an exception on our behalf.
When you write an HtmlUnit test, you write code that simulates the action of a user sitting in front of a web browser: You get a web page, enter data, read text, and click buttons and links.
Instead of manually manipulating the browser, you programmatically control an emulated browser.
At each step, you can query the HTML object model and assert that values are what you expect.
The framework will throw exceptions if it encounters a problem, which allows your test cases to avoid checking for these errors, thereby reducing clutter.
As you’re familiar with by now, JUnit provides a class called Assert to allow tests to fail when they detect an error condition.
Assert is the bread and butter of any unit test.
HtmlUnit itself uses WebAssert notNull extensively to guard against null parameters.
Make sure to check the WebAssert class before you write code that may duplicate its functionality.
If a method you need is absent, you should consider creating your own assert class for additional HTML assertions.
In order to specify which browser to emulate, you provide the WebClient constructor with a BrowserVersion.
You’ll probably want to test your application with the most common version of Internet Explorer and Firefox.
For our purposes, we define our test matrix to be all HtmlUnit-supported web browsers.
Listing 12.2 uses the JUnit Parameterized feature to drive the same test with all browsers in our text matrix.
Based on our previous example, we made the following changes: We used the Parameterized JUnit test runner b.
We added a BrowserVersion instance variable C to track the browser context.
We added the method getBrowserVersions D to return a list of BrowserVersion objects corresponding to the browsers we want to test.
The signature of this method must be @Parameters public static java.util.
This array length must match the number of arguments of the only public constructor.
In our case, each array contains one element because the public constructor has one argument.
JUnit loops for each array in the getBrowserVersions collection D.
If there’s more than one public constructor, JUnit will throw an assertion error.
JUnit calls the constructor with an argument list built from the array elements.
In our case, JUnit calls the one argument constructor with the only element in the array.
We repeat the process for the next array in the getBrowserVersions collection D.
When you compare the test results with the previous example, you’ll see that instead of running one test, the parameterized JUnit test runner ran the same method four times, once for each value in our @Parameters collection.
You may not always want to use actual URL addressed pages as test fixtures, HTTP, files, or otherwise.
Next, we show you how to embed and run HTML in the unit test code itself.
The framework allows you to plug a mock2 HTTP connection into a web client.
In listing 12.3, we set up a mock connection with a default HTML response string.
The test can then get this default page by using any URL value.
We start by defining our expected HTML page title and HTML test fixture.
Then we create the web client, a MockWebConnection b, and install the HTML fixture as the default response for the mock connection C.
We can then set the web client’s connection to our mock connection D.
We’re now ready to go, and we get the test page.
Any URL will do here because we set up our HTML fixture as the default response.
Finally we check that the page title matches our HTML fixture.
To configure a test with multiple pages, you call one of the MockWebConnection setResponse methods for each page.
The code in listing 12.4 sets up three web pages in a mock connection.
This example installs three pages b in the mock connection and tests getting each page C and verifying each page title D.
HtmlUnit provides an object model that parallels the HTML object model.
You’ll use it to navigate through your application’s web pages.
To get to an HTML page, you always start with a WebClient and call getPage:
HtmlPage is HtmlUnit’s model of an HTML page returned from a server.
Once you have a page, you access its contents in one of three ways:
The HtmlPage API provides methods reflecting the HTML element model: for anchors, getAnchorByName, getAnchors, and others; for a body, getBody; for forms, getFormByName, getForms; for frames, getFrameByName, getFrames; for meta tags, getMetaTags.
We explore specifically how to work with form and frame elements in the following sections.
This discussion applies to all methods that return a List: getAnchors, getForms, and getFrames.
You should consider the implication of addressing these lists with indexes.
The index access creates an assumption in your test that the HTML form you want to test will always be the first form in the list.
If the page changes and the search form changes position, your test will fail, even though the page’s functionality may not have.
By addressing the form by index, you’re explicitly testing the form order on the page.
Address an element only through a list index if you want to test the order of an element in that list.
The benefit is that when you change the form order on a page, the form name doesn’t have to change, but the form index must change.
Lists are useful when the order of its elements matter.
You may want to assert that an anchor list is alphabetical or that a product list is in ascending price order.
As you just saw, HtmlPage allows you to get specific elements by name.
HtmlPage also lets you get to any element by name, ID, or access key with any of the methods starting with getElementBy such as getElementById, getElementsByName, and others.
These methods allow you to ask generic questions about the HTML model.
Note two changes in the code: First, we cast the result to the desired type unless we can work with an HtmlElement.
Second, because element names aren’t unique in a page, getElementsByName returns a list of HtmlElement, which is why we have the call to get.
If you can address the desired element by ID, you can use getElementById and do away with the get call.
Calling get introduces some brittleness to this test because we’re introducing a dependency on the list order.
If we wanted a more resilient test, and the element didn’t contain an ID, we’d need to resort to one of the following:
Neither option is appealing, so the lesson here is to use HTML IDs if you can.
This will allow you to create tests that are more resistant to change.
Use XPath3 for complex searches to reduce test code complexity.
XPath is a language specified by the W3C for querying nodes in an XML document.
We won’t cover the XPath language itself here; we focus on its usage in HtmlUnit to perform two types of tasks: getting to a specific element and gathering data.
Because DomNode implements both methods, it’s accessible not only to HtmlPage but to all DomNode subclasses, which include the HTML classes.
Knowing which XPath expression to use can involve a lot of trial and error.
Note that expressions generated automatically from such tools usually suffer from the same indexing issue we discussed earlier in section 12.3.7 “Accessing elements by name versus index.” By inspecting the code, you can create the following expression, which is more resilient to changes in the page:
This is in contrast to the first expression, which drills down to a known spot in the page or fails along the way if an element is missing.
We look next at a powerful XPath feature supported by the HtmlUnit API: the ability to collect data.
This feature allows us to perform, with one expression, a query that returns a data set.
For example, this expression returns an anchor list from the Java 6 Javadoc page for all package names:
To see this XPath expression in action, go to the Java 6 Javadoc page:
From that page, we can gather all links that point to a Java package:
HtmlUnit includes the Apache Xalan XPath implementation, which supports only 1.0
You’ll also need to write some code, an advanced endeavor.
Tests check for error conditions with the JUnit Assert class and the HtmlUnit WebAssert class and by letting the HtmlUnit API throw unchecked exceptions.
We already covered the WebAssert class in section 12.3.1, “HTML assertions.” For example, if you query for a form with an invalid name by calling HtmlPage getFormByName, you’ll get the exception.
If you call WebClient getPage and the page doesn’t exist, you’ll get the exception.
To verify that a method throws an expected exception, annotate the method with the expected attribute:
Because these exceptions are all unchecked, you don’t have to throw them from your methods, but you’ll need to remember to catch them if you want to examine.
Although not explicitly documented in the WebAssert Javadoc, WebAssert methods will throw exceptions for unexpected conditions.
This may not be acceptable, especially if you’re testing integration with thirdparty sites or if the exception is due to a shortcoming in the Mozilla JavaScript library or in HtmlUnit itself.
To disable these messages, you need to tell the logger to skip warnings and report only severe problems.
The following example sets all HtmlUnit loggers to the severe level:
HtmlUnit uses Apache Commons Logging to do its logging, which in turns uses the JRE logging facility by default.
Apache Commons Logging doesn’t allow you to reconfigure logs generically; you must do so with the actual log implementation.
You can navigate through an application and the web in general by getting an HTML page and then clicking a link or clicking a user interface element like a button.
You can get a page by URL or URL string, for example:
If a page is absent or isn’t reachable, the API throws an exception.
For example, continuing from the previous example, we enter a web query and click the Search button:
You can call the click and dblClick methods on all classes descending from HtmlElement.
Click methods simulate clicking an element (remember, HtmlUnit is an emulator) and return the page in the window that has the focus after the element has been clicked.
HtmlElement is the base class for all HTML elements except frame and iframe.
You can code the Enter key with the '\n' character.
Hitting the Enter key or any key may not be enough or the right process to test.
You can set the focus to any element with the HtmlPage method setFocusedElement.
Be aware that this will trigger any onfocus and onblur event handlers.
Let’s now put these concepts together with another example and test forms.
You can call one of the HtmlForm getInput methods to get HTML input elements and then simulate user input with setValueAttribute.
The following example focuses on the HtmlUnit mechanics of driving a form.
First, we create a simple page to display a form with an input field and Submit button.
We include form validation via JavaScript alerts in listing 12.5 as a second path to test.
The section “Testing JavaScript alerts” describes this in more detail.
This form looks like figure 12.1 when you click the button without input.
We test normal user interaction with the form in listing 12.6
We create the web client, get the page containing the form, and get the form.
Next, we get the input text field from the form, emulate the user typing in a value b, and then get and click the Submit button C.
We get a page back from clicking the button, and we make sure it’s the expected page.
If at any step, the framework doesn’t find an object, the API throws an exception and the test automatically fails.
This allows you to focus on the test and let the framework handle failing your test if the page or form isn’t as expected.
You then call FrameWindow getEnclosedPage to get the HTML page in that frame.
This example uses getFrameByName to get frames and then calls getEnclosedPage.
Unit tests can use the list API getFrames as well, but we point you to the issues discussed in section 12.3.7, “Accessing elements by name versus index,” earlier in this chapter.
The intermediary FrameWindow returned by getFrameByName isn’t used in this example.
Note that it represents the actual web window for a frame or iframe and provides APIs to dig deeper through the GUI such as getFrameElement, which returns a BaseFrame.
BaseFrame in turn provides access to attributes like longdesc, noresize, scrolling, and so on.
By now, you should have the hang of using the API, so let’s move on to JavaScript, CSS, and other topics.
To deal with JavaScript alert and confirm calls, you can provide the framework with callbacks routines.
We reuse our form example from section 12.3.12, “Testing forms with HtmlUnit,” which includes JavaScript validation code to alert the user of empty input values.
The test in listing 12.8 loads our form page and checks calling the alert when the form detects an error condition.
In a second example, we enhance our existing test from section 12.3.12 to ensure that normal operation of the form doesn’t raise any alerts.
Our test will install an alert handler that gathers all alerts and checks the result after the page has been loaded.
Let’s work through the example: We start by creating the web client and alert handler b, which we install in the web client C.
Clicking the button returns a page object, which we use to check that the page has not changed by comparing current and previous page titles.
We also check that the page has not changed by comparing current and previous page objects.
Note that this comparison uses Object equals, so we’re really asking whether the page objects are identical.
This might not be a great test if a future version of the framework implements equals in an unexpected manner.
Finally, we get the list of alert messages that were raised E, create a list of expected alert messages, and compare the expected and actual lists.
Next, listing 12.9 rewrites the original form test to make sure that normal operation raises no alerts.
To customize the alert behavior, you need to implement your own AlertHandler.
Listing 12.10 will cause your test to fail when a script raises the first alert.
JUnit tip When using any assertion that use the equals methods, make sure you understand the semantics of the equals implementation of the objects you’re comparing.
The default implementation of equals in Object returns true if the objects are the same.
You can apply the same principles to test JavaScript confirm calls by installing a confirm handler in the web client with setConfirmHandler.
You can toggle CSS support on and off in a web client by calling setCssEnabled.
When calling APIs, the standard HtmlUnit behavior is to throw an exception when encountering a problem.
In contrast, when HtmlUnit detects a CSS problem, it doesn’t throw an exception; instead, it reports problems to the log through the Apache Commons Logging11 library.
To install an error handler, use the setCssErrorHandler method on a web client.
For example, the following causes all CSS problems to be ignored:
If you want any CSS problem to cause test failures, create an error handler that always rethrows the CSSException it’s given.
You’ll find that many websites have expired or incorrectly configured SSL certificates.
By default, the Java runtime throws exceptions if it detects errors.
Using this API causes HtmlUnit to use an insecure SSL handler, which trusts everyone.
Now, that we’ve covered testing from the client point of view, let’s go to the server side and examine how HtmlUnit can be used for in-container testing with the Cactus framework.
Cactus12 is a free, open source test framework for unit testing server-side Java code including servlets, EJBs, and much more.
Chapter 14, “Server-side Java testing with Cactus,” discusses Cactus in detail.
Where does HtmlUnit fit in? Let’s look at the various opportunities to test an application from the inside out:
In the standard HtmlUnit unit test scenario, HtmlUnit drives the test.
More specifically, JUnit invokes your unit test classes and methods, from which you call HtmlUnit to emulate a web browser to test your application.
Cactus unit testing manages a different interaction; Cactus calls your HtmlUnit unit tests at just the right time to verify that the web pages returned to the client.
The main difference here is that HtmlUnit unit testing takes place in-container instead of through an emulated web client.
Because HtmlUnit tests normally work with HtmlPage objects, we need to plug into the Cactus test execution at the point where a page is about to be returned to the client.
Cactus tests for Java-based code like servlets are subclasses of org.apache.
If the test class contains a method whose name starts with end, Cactus will call this method with a WebResponse, which contains the contents of the server’s response.
Take great care to import the appropriate WebResponse class for your tests, because three variations are supported:
There are a couple of things to note in this example:
The ServletTestCase provides the following instance variables for your use:
This servlet returns an HTML document with a title and a single paragraph.
The next step is to flesh out our end method; we need to get an HtmlPage from the WebResponse argument and validate its contents.
Getting an HtmlPage from a WebResponse requires parsing the HTML.
Cactus tip If your test class doesn’t contain a begin method, the end method name must be end.
If your test class includes a begin method, the end method name must match, for example, beginFoo and endFoo; otherwise the end method won’t be called.
We create a WebClient only to fulfill the needs of the HTMLParser API, which requires a WebWindow, which the WebClient holds.
Once you have an HtmlPage, you’re back to using the standard HtmlUnit API.
We’ve finished covering HtmlUnit for this chapter; the API is intuitive and straightforward, so we invite you to explore the rest on your own.
Let’s now look at Selenium, a testing framework that differs from HtmlUnit in a fundamental way: instead of emulating a web browser, Selenium drives a real web browser process.
Selenium13 is a free open source tool suite used to test web applications.
Selenium’s strength lies in its ability to run tests against a real browser on a specific operating system.
This is unlike HtmlUnit, which emulates the browser in the same VM as your tests.
This strength comes at a cost: the complexity of setting up and managing the Selenium runtime.
Although Selenium provides many components, we consider the following components: the Selenium Remote Control (RC) server, IDE, and client driver API.
The Selenium IDE is a Firefox add-on used to record, play back, and generate tests in many languages, including Java.
The Selenium client driver API is what tests call to drive the application; it communicates to the remote control server, which in turns drives the web browser.
The client driver connects to the server over TCP/IP; the server doesn’t need to run in the JVM or even on the same physical machine.
Selenium recommends14 running the server on many different machines, with different operating systems and browser installations.
A test connects to a server by specifying a hostname and port number to the DefaultSelenium class.
The IDE generates code against the client driver API but doesn’t support change management.
Cactus tip In Cactus, you don’t use getPage to get an HtmlPage.
Introducing Selenium should consider the IDE a one-way, use-once tool you use to get started for any given test case.
You must handle any change in the application by manually changing the generated tests.
Because the IDE records everything you do, you should plan in advance which user stories you want to verify and create one or more test cases for each.
At any point in the recording, you can ask the IDE to generate an assertion from the browser’s context menu; the current web page selection determines the choices.
To install Selenium, see appendix E, “Installing software.” Once you’ve done that, we can start generating Selenium tests.
The Selenium IDE is a great way to get up and running fast.
Before you record a test, edit the package name and class name in the IDE Source pane to match the directory and Java filename you desire.
Note that the generated code is JUnit 3 code, and as such it subclasses the Selenium class SeleneseTestCase.
The same user interaction as in our first HtmlUnit test generated the following example.
Go to Google, enter a query, and click to go to the expected site.
This initializes the selenium instance variable to a DefaultSelenium instance.
In the test method, we start by opening the home page C.
Next, we set the value of the input field D, as if a user had typed it in, and we click the Search button E.
The click argument is a Selenium locator, which here is the button name (more on the locator concept later)
We wait for the new page to load and assert the opened page’s title.
Then, we click a link F using a Selenium link locator.
Again, we wait for the new page to load and assert the opened page’s title.
Selenium tests subclass SeleneseTestCase, which in turn subclasses JUnit’s TestCase class.
You’ll note that methods aren’t annotated; Selenium-generated tests are JUnit 3 tests.
The immediate issue raised by running within the JUnit 3 framework is the performance of a test class.
Each time JUnit calls a test method, JUnit also calls the setUp and tearDown methods; this means starting and stopping a web browser, which is slow.
If your browser requirements are different from Firefox 3, what you recorded may not play back the same in a different browser.
Web pages can behave differently, sometimes in a subtle manner, from browser to browser.
In addition, pages can contain scripts to customize behavior based on the host browser.
Serverside code can customize replies based on the agent making the request.
Consider these issues before generating code from Firefox with the Selenium IDE; you may need to write the tests from scratch, a la HtmlUnit, if your application has code paths for a non-Firefox browser, such as Internet Explorer or Safari.
Next, we look at what it takes to run Selenium tests.
To run Selenium tests, you must use the Selenium server included in the Selenium Remote Control download.
Assuming the JVM is on your PATH, type the following:
Selenium: under the hood The Selenium server launches the web browser and acts as a proxy server to your tests; the server then runs the tests on your behalf.
This architecture works for any browser and operating system combination; you can also use it to test Ajax applications.
This proxy server setup is why you may get certificate warnings.
When you run tests, you’ll see two browser windows open and close.
The first will contain the tested application; the second will display commands sent to the browser and log entries if you have logging enabled.
If you’re building with Ant or Maven, you can manage the lifecycle of the Selenium server from these tools.
We recommend that you manage the server from the test class or suite directly, as we show next.
This allows you, as a developer, to run the tests directly from the command line or an IDE like Eclipse.
This is a problem because the performance associated with a default SeleneseTestCase is bad; JUnit starts and stops a browser around each test method invocation through the setUp and tearDown methods.
We present a two-stage solution to this problem by first managing a server for all test methods in a given class and then managing a server for all classes in a test suite.
The test manages two static variables: a Selenium client driver.
The @BeforeClass method starts the Selenium server and then the Selenium client D.
The @AfterClass method stops the client and then the server E.
This class doesn’t subclass SeleneseTestCase to avoid inheriting its setUp and tearDown methods, which respectively start and stop a web browser.
If you want to subclass SeleneseTestCase, make sure you override the setUp and tearDown methods to do nothing.
If you aren’t going to manage a Selenium server farm for different browsers and operating systems, using this class as a superclass for tests offers a simple solution to get you up and running managing the Selenium server within your tests and VM.
The drawback to this approach is that JUnit starts and stops the Selenium server for each test class.
To avoid this, you could create a test suite with first and last test classes that start and stop the server, but you’ll need to remember to do this for each suite, and you’ll also need to share the Selenium server through what amounts to a global variable.
Our second solution, in listing 12.15, creates a JUnit Suite class to manage a Selenium server.
This custom suite will start the Selenium server, run all the test classes in the suite, and then stop the server.
Listing 12.15 A test suite to manage a Selenium server.
The key to this class is our implementation of the run method E.
We clone the method from the superclass and insert calls to our methods to start F and stop G the Selenium server.
The rest of the code consists of duplicating constructors from the superclass D.
This allows us to write our test suite simply and succinctly as follows:
Each test class in the suite is responsible for connecting to the local server.
You can further enhance the suite to customize these settings.
Now that we can generate and run Selenium tests, let’s focus on writing our own.
With an efficient test infrastructure in place, we can now explore writing individual tests with Selenium.
We look at how to test for multiple browsers and how to navigate the object model, and we work through some example tests.
Note that experimental17 browser launchers exist for elevated security privileges and proxy injection.
We can apply the same JUnit @Parameterized feature we used with HtmlUnit in order to run the same test class with more than one browser.
In listing 12.16, we rework our previous example with class-level and instance-level JUnit initialization in order to combine the ability to run all tests with one client driver instance and then repeat the test for different browsers.
By contract with JUnit, this method must return a Collection of arrays; in our case, we return a list of browser launch strings, one for each browser we want to test.
You’ll need to have both browsers installed on your machine for this to work.
When running the test class, JUnit creates test class instances for the cross product of the test methods and the test collection elements.
The selenium instance variable gets its value from a lazy-initialized static variable C.
This can work only by using a @Before method and lazy initializing our client driver.
Remember, we want our test class to reuse the same driver instance for each test method in a given parameterized run.
We have an @AfterClass method G to clean up the driver at the end of the class run.
Even though we use a static Map C to save our driver across test runs, there’s only one driver in the map at any given time.
The getSelenium method E can safely stop F the current driver when creating a new driver because we know that JUnit finished one of its parameterized runs.
Now that you know how to run tests efficiently for a browser suite, let’s survey the API used to navigate an application.
This interface contains more than 140 methods and provides all of the services and setting toggles needed to write tests.
Although there’s no object model per se, the API provides some methods to work with certain types of elements.
For example, getAllFields returns the IDs of all input fields on a page.
Here’s a brief sample of how tests can manipulate page elements:
We now look at the different ways to access elements.
In our first example, we saw HTML elements referred to by locators.
Selenium provides a String format to address elements with different schemes.
The two locators we saw are the default scheme, id, and link used to find anchor elements.
To get the value of a field, for example, you’d write.
Although a generated test method throws an exception, it doesn’t throw any checked exceptions, nor do APIs you use to write tests.
The generated code and APIs throw unchecked exceptions to make sure your tests fail under the proper conditions.
Let’s now look at various examples of using the API and navigating an application.
The API doesn’t provide explicit support for forms; instead, you work with forms as you would any other elements, calling APIs for typing, clicking, and pressing keys.
The following example recasts the HtmlUnit example from the “Testing forms with HtmlUnit” section to the Selenium API.
To remind you, in the HtmlUnit section, we created a simple page to display a form with an input field and a Submit button.
We included form validation via JavaScript alerts in the example as a second path to test, as described in the section “Testing JavaScript alerts.”
We test normal user interaction with the form as follows:
We open the form page, type in a value, and click the Submit button to go to the next page.
Finally, we make sure we land on the right page.
If at any step Selenium can’t find an object, the framework throws an exception and your test automatically fails.
This allows you to focus on the test and let the framework handle failing your test if the page or form is not as expected.
A test can check to see whether a JavaScript alert has taken place.
We reuse our form example from section 12.3.12, “Testing forms with HtmlUnit,” which includes JavaScript validation code to alert the user of empty input values.
The following test loads our form page and checks that the browser raised the alert when the error condition occurred.
The key method is getAlert, which returns the most recent JavaScript alert message.
Calling getAlert has the same effect as clicking OK in the dialog box.
We open the form page, save the current page title, and click the Submit button.
This raises the alert because we didn’t type in a value.
Next, we call getAlert b to check whether the code raised the correct alert.
Finally, we make sure we’re still on the same page by comparing the new page title with the saved title.
We don’t need to create a test to check whether an alert has taken place during normal operation of our page.
If the test generates an alert but getAlert doesn’t consume it, the next Selenium action will throw a SeleniumException, for example:
Selenium provides the ability to capture a screen shot at the time of failure to subclasses of SeleneseTestCase.
By default, the screen shot is written to a PNG file in the Selenium server directory with the same name as the test name given to the SeleneseTestCase String constructor.
JavaScript alerts generated from a page’s onload event handler aren’t supported.
If this happens, JavaScript will open a visible dialog box, and Selenium will wait until someone clicks the OK button.
We’ve added a new method called captureScreenshot b, which takes a Throwable argument and calls the Selenium captureScreenshot method C.
To avoid repeating this code pattern in every method that wants to capture a screen shot on failure requires extending JUnit, which is beyond the scope of this section.
This concludes our Selenium survey; next, we contrast and compare HtmlUnit and Selenium before presenting our chapter summary.
Here’s a recap of the similarities and differences you’ll find between HtmlUnit and Selenium.
The similarities are that both are free and open source and both require Java 5 as the minimum platform requirement.
The major difference between the two is that HtmlUnit emulates a specific web browser, whereas Selenium drives a real web browser process.
When using Selenium, the browser itself provides support for JavaScript.
Use HtmlUnit when Use HtmlUnit when your application is independent of operating system features and browser-specific implementations not accounted for by HtmlUnit, like JavaScript, DOM, SCC, and so on.
Use Selenium when Use Selenium when you require validation of specific browsers and operating systems, especially if the application takes advantage of or depends on a browser’s specific implementation of JavaScript, DOM, CSS, and the like.
The HtmlUnit pros are that it’s a 100 percent Java solution, it’s easy to integrate in a build process, and Cactus can integrate HtmlUnit code for in-container testing, as can other frameworks.
HtmlUnit provides an HTML object model, which can validate web pages to the finest level of detail.
HtmlUnit also supports XPath to collect data; Selenium XPath support is limited to referencing elements.
The Selenium pros are that the API is simpler and drives native browsers, which guarantees that the behavior of the tests is as close as possible to a user installation.
In this chapter, we examined presentation-layer testing and explored the use of two free open source tools to test the user interface of a web application: HtmlUnit and Selenium.
HtmlUnit is a 100 percent Java solution with no external requirements; it offers a complete HTML object model, which, although creating rather verbose test code, offers great flexibility.
Selenium is a more complex offering; it includes a simple IDE and many complementary components.
The strength of the product comes from its architecture, which allows the embeddable Selenium Remote Control server to control different browsers on assorted operating systems.
The Selenium API is much simpler and flatter than with HtmlUnit, resulting in more concise test code.
Use HtmlUnit when your application is independent of operating system features and browser-specific implementations of JavaScript, DOM, CSS, and so on.
Use Selenium when you require validation of specific browsers and operating systems, especially if the application takes advantage of or depends on a browser’s specific implementation of JavaScript, DOM, CSS, and so on.
In the next chapter, we add a layer of complexity by considering Ajax technologies in our applications and test cases.
It’s a continuation of chapter 12, which discusses presentation-layer testing in general and introduces two of the libraries and tools used in this chapter: HtmlUnit and Selenium.
As in the previous chapter, our goal is finding bugs in the graphical user interface of an application.
We describe a divide-and-conquer approach by breaking up tests into three groups: functional testing, testing client-side scripts, and testing server services.
Why are Ajax applications difficult to test? the technologies relevant to testing each tier.
We end by looking at the unique testing challenges presented by Google Web Toolkit (GWT) applications.
We start by reviewing Ajax and why testing is difficult.
Critical to the user experience, they left behind the need to constantly reload or refresh an entire web page to keep any portion of its information updated.
Although still browser based, these applications started to give the web the look and feel of what had been strictly the domain of desktop applications.
Although Ajax is often associated with its all-uppercase sibling AJAX, the acronym, it’s today much more than Asynchronous JavaScript and XML.
You build an Ajax application by combining the following technologies: CSS, DOM, JavaScript, server-side scripting, HTML, HTTP, and web remoting (XMLHttpRequest)
Beyond its associated technologies, Ajax reflects the mindset of a new breed of web applications built on standards and designed to give users a rich and interactive experience.
In this chapter, we study how to test these applications.
To understand the challenge of testing an Ajax application, let’s look at a webclassic interaction and then step through the stages of an Ajax application interaction.
In a web-classic interaction, the user opens the browser on a page, and each time the page needs data, it asks the server for a new page.
In an Ajax application, the page communicates with the server to get data for the part of the page that needs updating and then updates only that part of the page.
The user starts by opening an Ajax application’s start page in a browser; this causes the HTML page to load.
The browser displays the HTML using any associated CSS and runs client-side JavaScript code to set up the page’s event handlers.
The page is now ready to respond to user interactions.
The user interacts with the page, triggering a JavaScript event handler.
In an application like Google Suggest, each keystroke creates a server request for a list of suggestions that are displayed in a drop-down list box.
The JavaScript event handler builds an XHR object and calls the server with a specific request using HTTP.
The server processes the request and returns a response using HTTP.
The browser invokes the XHR callback and uses the data returned by the server, in the form of XML or text, to update the page in the browser.
In order to update the page, the callback function uses the DOM API to modify the model, which the browser displays immediately.
Figure 13.1 New pages are downloaded for each interaction in a web-classic application.
Figure 13.2 Relevant portions of a page downloaded for each Ajax web application interaction.
This interaction is quite different from the web-classic architecture where a page is loaded, a user interacts with the page causing another page to load, and then the cycle repeats.
With Ajax, the page is loaded once, and everything happens within that page.
JavaScript code runs in the page to perform I/O with the server and updates the inmemory DOM of the page, which the browser displays to the user.
The challenge in writing tests for the application interaction described previously is the asynchronous aspect of HTTP communications and the DOM manipulation by JavaScript code.
The difficulty is how to drive a self-changing application when those changes are asynchronous to the test itself.
In addition to testing the traditional page state described in chapter 12, you should also test an Ajax application’s best practices.
Tests should exercise features4 like drag and drop, form validation and submission, event handling, back button, refresh button, undo and redo commands, fancy navigation, state management and caching, and user friendliness (latency, showing progress, timing out, and multiple clicks)
Further complicating matters, different implementations of Ajax component technologies like JavaScript, DOM, and XMLHttpRequest exist in different browsers from different vendors.
Although various free and open source libraries abstract these differences away, an application is nonetheless more complicated to test.
You may want to ensure test coverage for all code paths for all supported browsers on all supported operating systems.
Next, we split up testing this complex application stack into more manageable tiers through functional testing, testing client-side scripts, and testing server services.
Before we jump into test code, we survey the testing patterns we use to verify the various aspects of an Ajax application.
Functional testing drives the whole application from the client-browser and usually ends up exercising all application layers.
Client-side script unit testing covers the JavaScript scripts running in the browser.
Service testing verifies services provided by the server and accessed from JavaScript XHR objects.
Let’s look at these types of tests in more detail before we turn to implementation.
Functional testing drives the whole application from the client browser and usually ends up exercising all application layers.
As always with browser-emulation software, the key is how well it supports JavaScript.
Here we divide the scripts into two piles: the ones that use XHR to manipulate a DOM and the ones that don’t.
Although we can test some script functions and libraries independently from their hosting page where scripts call XHR objects and modify the DOM of the current page, we need a JavaScript engine and browser; the browser in turn may be emulated or live.
For other scripts, we’re testing a library of functions, and we prefer to deliver these functions in standalone files as opposed to embedded in HTML pages.
We can test all scripts through functional tests of the pages that call on them, but we want to provide a lighter-weight test pass that’s more along the line of a true unit test.
Heavier-weight and slower functional tests using web browsers should ideally be reserved for when scripts can’t be otherwise verified.
What should you look for in a JavaScript testing framework? From the TDD point of view, the most important feature is the ability to automate tests.
We must be able to integrate JavaScript testing in our Ant or Maven build.
Second, we want the ability to run the tests from JUnit.
Although JUnit or Ant integration can be custom coded for a specific JavaScript testing framework, JsUnit provides this functionality out of the box.
JsUnit is a free, open source framework integrated with JUnit that goes one step further by providing advanced Selenium-type distributed configuration options.
JsUnit is to JavaScript testing what Selenium is to web application testing.
JsUnit allows you to test JavaScript, from JUnit, by controlling a web browser process on a local or remote machine.
We discuss JsUnit in action in the section “JavaScript testing with JsUnit.”
Service testing verifies services provided by the server and accessed from JavaScript XHR objects.
Because HTTP is the standard used by XHR objects to communicate with the server, we can use any HTTP client to test the service independently of XHR and the browser.
We examine HttpClient in action in the section “Testing services with HttpClient.”
In this chapter, we test server services for Ajax applications, which we distinguish from testing web services, a different web standard.
We now examine each technique in more detail with implementation examples.
We now look at testing the application stack by continuing our demonstration of Selenium and HtmlUnit started in the previous chapter on presentation layer testing.
We show you how to use Selenium and HtmlUnit to write the same kind of tests.
We show how to deal with the asynchronous aspect of an Ajax application before going on to test specific components that make up the stack, such as JavaScript libraries and server-provided services (as opposed to web services)
The key aspect of writing tests for an Ajax application is to know when an action causes a change in the DOM.
We show how to do this and create functional tests using Selenium and then HtmlUnit.
If you’re not familiar with Selenium, please consult the previous chapter to get an understanding of the fundamentals we build on here.
We pick up Selenium where we left off, this time testing Ajax, a more advanced task.
Once a page is loaded in the browser, Selenium sees that version of the page.
To see the updated DOM, you must use a different API than what we showed you in the previous chapter.
Let’s take as an example the form in figure 13.3
When you click the Get Message button, the page queries the server and returns a simple text message: Hello World.
JavaScript code then updates the DOM with the message, and the browser displays the input field value, as you can see in the screenshot.
Listing 13.1 shows the HTML source for the page, which includes the JavaScript to perform the HTTP XML Request.
The form F defines a button, which when clicked invokes the JavaScript function setMessage C.
The first thing the setMessage function does is call newXHR b, which creates a new HTTP XML request object.
To run this example locally with IIS, you’ll need to create a virtual directory for the example webapp directory and use the IIS Permission Wizard to grant it default rights.
In order to test the form and check that the message is what we expect it to be, we use the same JUnit scaffolding from the previous chapter and first set up a test suite to manage the Selenium server:
Without knowing anything about Ajax, you might create the test method testFormNo in listing 13.2
The call takes a Selenium locator to retrieve the content of the element named serverMessage.
The return value is the empty string C because the DOM model in the object model isn’t up to date.
Perhaps we need to wait for the server to do its work and the result to come back.
Calling waitForPageToLoad doesn’t work because the code doesn’t reload the page.
Recall that this is an Ajax application; pages don’t reload.
Fortunately, Selenium provides a single powerful API for this purpose: waitForCondition.
Alternatively, without the refactoring, the JavaScript expression in the first argument to waitForCondition reads:
The waitForCondition method waits for a given condition to become true or a timeout to expire.
The method takes two arguments: the first is JavaScript code where the last expression must evaluate to a Boolean; the second is a timeout String expressed in milliseconds.
The art of testing Ajax with Selenium is about embedding JavaScript in your JUnit code.
This can be confusing because you’re embedding JavaScript in Java, but it’s what’s required because the Selenium server controlling the web browser will run the JavaScript for you.
In contrast, let’s go back to HtmlUnit and see how this test looks in that framework.
Wait for condition API tip In order for JavaScript to access the application window, you must use the following expression:
Though the HtmlUnit API is more verbose than that of Selenium, you write tests entirely in Java.
As usual, we start by creating an HtmlUnit web client and getting the application’s start page C.
We get our button, we click it, and the result is a new page D.
From this page, we get the entry field that was updated through the DOM by the XHR call and assert that the contents are what we expect E.
The general pattern with HtmlUnit is to get a page, find the element, click it, and check the resulting page contents.
Because the test thread can finish before HtmlUnit reads the Ajax response from the server, you must synchronize the test code with the response to guarantee predictable results from run to run.
Although a simple approach is to sleep the thread for a while, HtmlUnit provides APIs to guarantee that Ajax tests are synchronous and predictable.
This example illustrates this with the call to setAjaxController b.
By default, a web client initializes itself with an instance of AjaxController, which leaves Ajax calls asynchronous.
If you want finer-grained control over the behavior of tests, the framework provides experimental9 APIs to wait for various JavaScript tasks to complete.
The return value is the number of jobs still executing or scheduled for execution.
You’ve now seen how to create functional tests with Selenium and HtmlUnit.
Although this can prove that an application works from the perspective of a client, it doesn’t assert the quality of the underlying building blocks.
It’s now time to dive deeper into testing by dealing with testing these building blocks: JavaScript and server services.
Here, you face the same choice you had between HtmlUnit and Selenium: do you want to emulate a browser or drive a live browser? We look next at two JavaScript testing frameworks, RhinoUnit and JsUnit.
RhinoUnit is like HtmlUnit, a 100 percent Java solution, and JsUnit is akin to Selenium in that it drives local or remote web browsers.
To wrap up JavaScript testing, we use JSLint to check our code against best practices.
RhinoUnit allows you to run JavaScript unit tests from Ant.
If you’re on an older version of Java, you’ll need the Apache Bean Scripting Framework10 (BSF) and the Mozilla Rhino JavaScript engine as documented in appendix E, “Installing software.”
HtmlUnit 2.5 Javadoc warns that these APIs may change behavior or may not exist in future versions.
As a bonus, RhinoUnit includes JSLint,11 which allows you to check from Ant that your JavaScript code follows best practices.
We start the test by including the library of functions we want to test b with a call to eval.
Note that the path to the file is relative to where we’re running the test from; in this case, it’s the project’s root directory.
Next, we must call testCases, passing in test as the first variable C, followed by our test functions.
You can pass in any number of functions; note the comma separating the test functions.
The assert.that call is how to make an assertion in RhinoUnit.
The first value is the value we’re testing, the actual value; the second value, the predicate, defines the actual test.
The RhinoUnit site lists13 the functions you can use in addition to eq; these are the most widely used:
The function isFalse(message) tests that the actual value is false, displaying an optional message if it isn’t.
The example calls the match function first to assert that “in” is in the test string b and then to check that “out” isn’t C.
The assert object contains other useful functions: fail is like the JUnit fail method.
The various mustCall functions check that the tests causing the given functions have or have not been invoked.
In order to run the tests, you’ll need an Ant build script.
Listing 13.4 is the sample build script used to run our examples.
We start our Ant script by defining properties for the locations of directories and files.
Next, we define a rhinounit Ant script C and its arguments by loading its source from rhinoUnitAnt.js.
We call the script D with a fileset pointing to our JavaScript unit test source, where we include all files with the js extension.
In the same way that we defined a script for rhinounit and ran our tests, we define a script for JSLint E to help us detect scripting issues, and we finish by running this JSLint script F on our source directory.
For more on JSLint, please see section 13.6, “Checking best practices with JSLint.”
JsUnit15 is a JavaScript unit-testing framework written in JavaScript and in Java.
We use it from Ant to drive a web browser in order to validate the same JavaScript we just tested in the previous section.
JsUnit is similar to Selenium in that it controls a web browser and the tests run in that browser’s JavaScript engine.
RhinoUnit tip When you invoke the rhinounit Ant script, make sure you point the rhinoUnitUtilPath argument to the location of the rhinoUnitUtil.js file, for example:
Let’s start by showing how to run a test from JsUnit and then automating the test from Ant.
The source code for this chapter includes a copy of JsUnit; for details please see appendix E.
You write a JsUnit test by creating an HTML page containing JavaScript test functions.
You use HTML only as the container for the JavaScript.
You define the JsUnit test in the HTML head element and start with the references needed to bring in the JsUnit framework b and the JavaScript code to test C.
JsUnit tip The references in the link href and script src attributes are relative to the location of the HTML test file.
A JavaScript script element defines the test functions for our factorial D and regular expression E tests.
As with JUnit 3, we define test functions with the function name prefix test.
The set of JsUnit assert functions is smaller than the set of JUnit assert methods and is listed here.
Like JUnit, the API defines a version of assert functions with and without a message argument.
The square brackets in the following list denote that the argument is optional.
Like JUnit, JsUnit lets you use setUp and tearDown functions.
JsUnit calls your setUp function before each test function and your tearDown function after each test function.
JsUnit supports an equivalent to @BeforeClass if you define a function called setUpPage.
JsUnit calls setUpPage once after the page is loaded but before it calls any test functions.
When your setUpPage function ends, it must set the variable setUpPageStatus to 'complete' to indicate to JsUnit that it can proceed to execute the page.
As in JUnit, you can group JsUnit tests into a suite of related tests.
Listing 13.6 wraps our previous test page into a test suite.
JsUnit versus JUnit JsUnit differs from JUnit in that JsUnit doesn’t define the order of test function invocation; in JUnit, the order of methods in the source file defines the invocation order.
In addition, although JUnit creates a new test object instance to invoke each method, JsUnit doesn’t use a corresponding action, like reloading the page, which means that JsUnit preserves page-variable values across test function invocations.
To define a test suite, create a function called suite b, which returns a JsUnitTestSuite object.
You then build up a suite object by adding test pages or other suite objects.
In our example, we add one page C, the page we previously defined by calling the addTestPage function.
The rest of the code in this HTML page is the same as our previous example with the exception that we don’t need to refer to our JavaScript factorial library.
To add a test suite to another test suite, create a new JsUnitTestSuite object, and call the addTestSuite API.
This allows you to organize your tests just as you can in JUnit.
Listing 13.7 defines and adds two test suites to a main test suite.
JsUnit addTestPage tip The addTestPage argument is a location relative to the test runner page you’ll use.
We now show you how to run the tests manually during development and then through Ant for builds.
In figure 13.4, we show the result of running our test suite with the familiar green bar.
The result will show you the green bar with 90 successful tests.
Now that we have a manual way to run tests, let’s move on to automating tests with Ant.
The Ant build file will manage web browsers, invoke tests, and create reports.
Listing 13.8 shows the build.xml file that excerpts invoke our test suite.
JsUnit tip: Status Aborted or tests time out JsUnit doesn’t give you much feedback when something goes wrong.
If you see Aborted in the JsUnit Status field, check your paths starting with link href and script src and then addTestPage.
We start our build.xml file by defining the location of the JsUnit installation directory.
Then we define which web browsers JsUnit will use to test our code with the property browserFileNames C.
Next, we define logsDirectory D to hold the directory location for test report XML files.
The property timeoutSeconds E is a timeout in seconds to wait for a test run to complete; if absent, the default value is 60 seconds.
The url property F defines which test runner to use and which test or test suite it should invoke.
It’s worth breaking down this URL into its component parts.
The URL starts with http://localhost:8080 because we’re running our tests locally.
All of this yields the first part of the URL: http://localhost:8080/jsunit/jsunit/testRunner.html.
The testPage URL parameter points to the test page or suite page to run.
It too starts with the same local server plus the jsunit servlet prefix and is followed by the path to the test suite page relative to where the test is run.
Put it all together and we have the complete URL.
Next, we give Ant all of the JAR files needed to run JsUnit G, and then we can proceed to running our test with the target standalone_test H, which we invoke from the command line in the build.xml directory with a simple call to Ant by typing the following in a console:
Ant starts, and you’ll see the web browser open, run the tests in the test runner page, and close.
You’ll also see a couple of pages of Ant and JsUnit output on the console detailing the test run, too much to reproduce here.
We can look for the next-to-last line of Ant output for the familiar BUILD SUCCESSFUL message.
We wrap up this section by noting that more advanced test configurations are possible with JsUnit because it provides support for driving farms of JsUnit servers.
A JsUnit server is what allows tests to be performed from Ant; it acts under the covers of our examples to drive web browsers on the local machine or on remote machines and also creates the result logs.
JsUnit Firefox tip: permission denied If you get a permission denied error in Firefox, set the security.fileuri.
Should you use RhinoUnit or JsUnit? The answer to this question is quite similar to the HtmlUnit versus Selenium question, which we presented in the previous chapter.
The similarity is that both are free and open source.
The major difference between the two is that RhinoUnit emulates a web browser, whereas JsUnit drives a real web browser process.
When using JsUnit, the browser itself provides support for JavaScript.
The RhinoUnit pros are that it’s a 100 percent Java solution and is easy to integrate in a build process.
The JsUnit pros are that it drives native browsers and can manage a farm of JsUnit test servers.
You’ve seen how to test client-side scripts with RhinoUnit and JsUnit.
Next, we show how you can check best practices with JSLint.
We now move on to checking our code for best practices with JSLint.17 As we did for the unit-testing script, we use scripts to define and run JSLint from Ant:
Use RhinoUnit when Use RhinoUnit when your application is independent of operating system features and browser-specific implementations of JavaScript, DOM, CSS, and so on.
Use JsUnit when Use JsUnit when you require validation of specific browsers and operating systems, especially if the application takes advantage of or depends on a browser’s specific implementation of JavaScript, DOM, CSS, and so on.
We start by defining an Ant script for JSLint b and then call the script C and pass it the source location to our JavaScript library directory.
JSLint is quite verbose and comprehensive in its output; please see the JSLint18 website for details.
We started by looking at functional testing from the client perspective.
Next, we dove into testing one of the underlying building blocks of an Ajax application: clientside scripts with RhinoUnit and JsUnit.
We also checked these scripts for best practices with JSLint.
Next, we move to the server side and another building block: testing server-side services with HttpClient.
JSLint tip When you invoke the JSLintant Ant script, make sure you point the JSLintpath argument to the location of the full JSLint.js file, for example:
The idea behind testing the application services layer separately is to validate each service independently from HTML, JavaScript, and DOM and how the application uses the data.
An application calls a service from JavaScript through the XMLHttpRequest object.
Our goal is to emulate an XMLHttpRequest object by using HTTP as the transport mechanism and XML and JSON as example data formats.
We use the Apache Commons HttpClient to provide HTTP support, Java’s built-in XML support, and jslint4java to check that JSON documents are well formed.
To simplify this example, we’ve made the chapter’s example webapp directory an IIS virtual directory so that we can run the unit tests from Ant locally.
A production Ant build would start and stop a web container like Jetty around the unit test invocations.
Our first XML service test in listing 13.10 makes sure that we’re getting back from the server the expected XML document.
The test starts by creating an Apache Commons HttpClient b and defining the HTTP GET method C with a URL for our XML document fixture.
The URL specified in the GetMethod constructor is application specific and must include parameters if appropriate for a given test.
The test then executes the HTTP GET method D and reads the data back from the server.
Note the use of the Apache Commons IO API IOUtils.toString to read the response stream in a string as a one-liner E.
The code does this synchronously, unlike a standard Ajax application.
We then guarantee that HttpClient resources are freed by calling releaseConnection from a finally block F.
We can now check that the data from the server is as expected.
You could also use Java regular expressions to do some further XML string-based checks; next, we use an important XML feature: XML validation.
If you can parse an XML document, you know that it’s well formed, meaning that the XML syntax is obeyed, nothing more.
In this next example, the schema is stored in a file called personal.xsd.
Listing 13.11 uses the standard Java XML APIs to validate the XML document returned from a server call against an XML Schema.
This example starts as the previous one did, but after the test executes the HTTP GET method, we read the server response directly with an XML parser b.
If the DOM document parses successfully, we know the document is well formed, a nice sanity check.
If we don’t get a valid XML document from the server, we might have a server error message in the response or a bug in server-side XML document generation.
We can now move to the meat of the test, XML validation.
Testing services with HttpClient of grammar to use, in our case, XML Schema C, and load the XSD schema file in a Schema instance D.
Finally, we can create an XML Validator for our schema and validate the DOM document E.
At this point, we know that our document is valid and well formed.
The next step would be to check that the application data is as expected.
The DOM document API can be painful to use, so at this point you have several options.
You can use Java’s XPath20 support to check the contents of a document.
You can also use Sun’s JAXB21 framework, although it’s not trivial, to transform XML into POJOs.
In this first example in listing 13.12, we show a simple check of a JSON document.
In this test, we perform the same steps as our first XML example: we create an HttpClient, an HTTP GET method that we execute, and then read the results from the server into a String b.
Unlike XML, JSON has no APIs to support checks for wellformed and valid documents.
We strip whitespaces from our JSON document fixture, the server response, and compare the two C.
Although this check is brute force, we use it to provide a simple check that’s free of formatting issues.
The next-best thing we can do is implement a well-formed check by parsing the document.
Although www.json.org lists many libraries, including Java libraries to parse JSON, we use the JSLint wrapper jslint4java24 to go beyond a simple well-formed check.
As you saw earlier in the RhinoUnit section, JSLint provides lint-style reporting for JavaScript.
Our test starts as usual, and we check for results using a JSLint object b.
We don’t provide options in this example, but the JSLint class provides an addOption method.
The test calls the lint method by specifying two arguments: a String describing the source location and another String for the JavaScript code to check, in this case, a JSON document C.
The test uses the lint results to create a message String D used in the Assert call.
If there’s a problem, the test provides the assertEquals call E with a full description of all issues JSLint found.
You’ll notice that jslint4java is always behind JSLint in terms of features and fixes.
This is because jslint4java embeds JSLint (fullJSLint.js) in its JAR file.
In this section, you’ve seen how to validate server-side services that participate in an Ajax application independently of the pages and code using them.
We’ve separated our tests along the boundary of the Ajax architecture.
Let’s now consider a different way to build, run, and test an Ajax application with the Google Web Toolkit.
The Google Web Toolkit (GWT)25 is a free, open source framework used to create JavaScript frontends to web applications.
To this end, Google provides the Google Plug-in for Eclipse; you develop and test in Java, and when your application is ready for deployment, GWT translates your Java into JavaScript.
Because Java and JavaScript aren’t the same, you should test in both hosted and web modes.
It’s important to understand that GWTTestCase doesn’t account for testing the user interface of an application.
You use GWTTestCase to test the asynchronous portions of the application normally triggered by user actions.
This means that you must factor your application and test cases with this element in mind.
The tests can’t rely on any user interface element driving the application.
Testing the GUI requires using the techniques presented in this and the previous chapters; you can create functional GUI tests with Selenium or HtmlUnit.
Let’s first look at how to create a GWTTestCase manually before we show how to use junitCreator.
The example we use in this section is adapted from the GWT StockWatcher example26 and extended with an RPC.
Figure 13.5 shows what the application looks like running in hosted mode.
Our example tests the StockWatcher remote procedure call (RPC) to get stock price information for an array of stock symbols.
We focus on RPC, because it’s the heart of GWT JUnit testing.
GWTTestCase tip You use the GWTTestCase class to test the application logic of the web client, not the user interface.
Although seemingly an obstacle, this forces you to factor your GWT application cleanly between code for user interaction and application logic.
This is a design best practice that you should follow.
We start our asynchronous GWT RPC example test in familiar GWT territory with listing 13.14, where the refreshWatchList method performs a standard GWT RPC call.
The implementation of refreshWatchList follows the standard pattern for GWT RPC; the method creates a new StockPriceService instance b and defines the service callback.
The callback defines two methods; in onFailure C we save the given exception, and in onSuccess D, which is typed for our application model (StockPrice[]), we update the application.
Next, we call service’s getPrices method E with input data and our callback.
The key point to remember is that the call to the getPrices method is asynchronous, so the call to refreshWatchList is also asynchronous.
To create a GWT test case, you start by creating a subclass of GWTTestCase, along the lines of listing 13.15
A GWT test case must extend the GWT class GWTTestCase b and implement a method with the signature public String getModuleName() to return the name of the module being tested C.
Next, we call the refreshWatchList method in listing 13.14, which performs the asynchronous RPC E.
We need to allow the test method to complete while allowing assertions to run.
To do so, we use a GWT Timer F to schedule our assertions.
Testing Google Web Toolkit applications starts by checking that the stock watcher RPC was able to run in the first place.
We do this by checking to see whether the asynchronous callback caught an exception G.
This arrangement is helpful in determining an incorrect test setup, in particular as it relates to the classpath and module file (see the tip in the section on running tests)
If no exception is present, the validity checks on the StockWatcher object can proceed.
We check that table headers are still there H and then check the contents of the table.
I for values we expect to be returned from the service.
In this test, we changed the stock GWT example to return predictable values instead of randomly generated values.
If the test calls the GWTTestCase finishTest J method before the delay period expires, then the test succeeds.
Now that the Timer object is in place, we call delayTestFinish to tell GWT to run this test in asynchronous mode.
You give the method a delay period in milliseconds much longer than what is expected to run the test setup, do the RPC, and perform the assertions.
When the test method exits normally, GWT doesn’t mark the test as finished; instead, the delay period starts.
If the test calls the GWTTestCase finishTest method before the delay period expires, then the test succeeds.
If an exception propagates to GWT, then the test fails with that exception.
The argument is a delay in milliseconds, after which control returns to the caller.
We just examined asynchronous testing in GWT; next, we show how to use junitCreator to create starter tests and how to run the tests.
The junitCreator utility allows you to create a GWTTestCase based on a module to which you then add your own test methods.
For subsequent tests, you may prefer to clone a template class or write test case classes from scratch.
To get your command processor to find junitCreator and other GWT programs, remember to add GWT to your path.
The .launch files are Eclipse launch configurations and the .cmd files are commandline scripts.
Use these files to invoke the generated test case in web or hosted mode.
You may need to adapt the scripts for your location of the JUnit and GWT .jar files.
You right-click a test case class and choose Run As or Debug As and then choose GWT JUnit Test to run the test in hosted mode or GWT JUnit Test (Web Mode) to run the test in web mode, as shown in figure 13.6
To use Ant, you need to make sure your build file points to the GWT SDK.
Using a GWT example build file as a template, edit the gwt.sdk property to point to the GWT directory.
A GWTTestCase subclass can override the JUnit methods setUp and tearDown with the following restrictions:
The benefit of using a test suite with GWT goes beyond grouping related tests together.
A performance gain is possible by using a GWT test suite.
This causes all tests with the same module name to run one after the other.
To create a test suite, you can start with the Eclipse JUnit Test Suite Wizard.
For example, to create a test suite that includes all test cases in a package, go to the Packages view, right-click a package, and choose New and then Other.
Listing 13.16 shows the generated code with two changes we explain next.
We made the class extend GWTTestSuite b to make this suite a GWT test suite.
We also replaced the String in the TestSuite constructor with the class name of the generated class C.
This allows us to double-click the class name in the JUnit view and jump to an editor for that test suite.
The rest is standard JUnit code; we call addTestSuite with a class object to add a test case to the suite.
In addition to the requirements for running a GWT test case, you must configure a GWT test suite with more memory than the default settings allocate.
Configure the Java VM running the tests with at least 256 megabytes of RAM.
You must also add to the classpath the source directories for application and test code.
To wrap up GWT testing, recall that GWT test cases verify the asynchronous aspects of your application, not the user interface.
To test the GUI, use functional tests with Selenium or HtmlUnit.
Although we’ve finished our brief tour of GWT testing, it’s worth noting that GWT includes the Speed Tracer tool to help you identify and fix performance problems by visualizing instrumentation data taken from the browser.
In this chapter, we built on what you learned in chapter 12 about testing the presentation layer of applications, specifically as it relates to Ajax applications.
We showed that Ajax applications use many technologies layered in broad tiers: HTML and JavaScript are used on the client; HTTP, XML, and JSON provide communication and data services; and the server side is viewed as a black box implementing services accessed over the internet with HTTP.
We used functional tests for the whole application stack as it appears to a user by driving and testing the application with HtmlUnit and Selenium.
We isolated JavaScript into libraries and tested those independently with RhinoUnit and JsUnit.
You use RhinoUnit when your application is independent of operating system features and browser-specific implementations of JavaScript, DOM, CSS, and the like.
You use JsUnit when you require validation of specific browsers and operating systems, especially if the application takes advantage of or depends on a browser’s specific implementation of JavaScript, DOM, CSS, and so on.
Finally, we looked at the unique challenge posed by GWT, a framework that translates your Java code to JavaScript.
This chapter concludes our survey of user interface testing, and we now move to the server side and testing with Cactus.
In the second part of the book we explained what mock objects are and how to benefit from using them.
We also described different techniques for unit testing your server-side code, and we even compared these techniques against each other.
The one thing that you should be aware of now is that there is no absolute truth—the best techniques to use depend on the situation you’re currently in.
For example, in most cases you might find server-side testing with mocks and.
That’s why we cover the in-container testing approach deeper in the book.
Furthermore, this chapter focuses on the in-container testing methodologies by means of one of the most popular in-container testing frameworks: Cactus.
We start by introducing the Cactus framework and then show you some real-world examples of how to use Cactus.
We begin by explaining what’s so special about Cactus and the order of execution of Cactus tests.
We then build a sample application that uses some components from the Java EE spec, and we write the tests for those components with the help of Cactus.
The next step is to execute those tests; we show a sample integration between Cactus and some of the most popular build tools (Ant and Maven)
But we go a bit further than that: we demonstrate the tight integration between Cactus and other projects, such as Cargo and Jetty.
Before we go any further, I’d like to clarify the definitions just mentioned.
When we say Cactus is a framework, we mean that it provides an API that you have to extend in order to use it.
Also, in-container means that (as you’ll see later in the chapter) the tests get executed inside the virtual machine of the container.
And finally, Cactus is an extension of JUnit for two reasons: First, it extends JUnit by empowering it with new functionality (Cactus makes JUnit tests get executed inside the container, something which otherwise wouldn’t be possible)
And second, Cactus’s API extends JUnit’s API; in low-level software engineering terms, it extends some of JUnit’s classes and overrides some of JUnit’s methods.
In later sections, we explain in more detail how it works.
You need this knowledge before you experiment with a sample application, because Cactus is different from the normal unit testing frameworks.
Cactus executes the tests inside the container, which on its own raises a lot of questions, so we try to answer all of them here.
As we mentioned in the previous section, the Cactus project is used for testing the core Java EE components (JSPs, tag libraries, servlets, filters, and EJBs)
What’s worth mentioning is that this is the only focus of the Cactus project.
It doesn’t test any specific framework (look at the next chapter if your application is framework specific), because it isn’t intended to do so.
A lot of the emails that come from the Cactus.
Testing with Cactus mailing list ask if people can use Cactus for testing an application based on a specific framework (like Struts, JSF, or Spring)
There are quite a few tools dedicated to such testing, and we cover some of them later in the book.
Most of those tools are based on Cactus and require Cactus in their classpath, but again Cactus is designed for in-container testing of the components from the Java EE spec.
Because Cactus is an extension of JUnit, every Cactus test is a JUnit test by itself.
The reverse isn’t true; most of the JUnit tests are Cactus tests.
So what distinguishes the Cactus tests from the JUnit tests? You need to stick to a couple of rules in order to use Cactus.
We already discussed in chapter 8 what in-container testing means.
Back then, we had a web application that uses servlets.
We want to unit test the isAuthenticated method in listing 14.1 from a SampleServlet servlet.
In order to be able to test this method, we need to get hold of a valid HttpServletRequest object.
Unfortunately, it isn’t possible to call new HttpServletRequest to create a usable request.
JUnit alone isn’t enough to write a test for the isAuthenticated method.
Listing 14.1 Sample of a servlet method to unit test.
As you can see, the Cactus test case meets all of our requirements.
It gives us access to the container objects, inside our JUnit test cases.
As you can see from the previous listing, writing a Cactus test case involves several key points:
The Cactus test case must extend one of the following, depending on what type of component you’re testing: ServletTestCase, JSPTestCase, or FilterTestCase.
This is a rule: because Cactus extends the 3.8.x version of JUnit, your test cases always have to extend one of the latter classes.
The Cactus framework exposes the container objects (in this case the HttpServletRequest and HttpSession objects) to your tests, making it easy and quick to write unit tests.
You get a chance to implement two new methods: beginXXX and endXXX.
These two new methods are executed on the client side, and you can use them to place certain values in the request object or to get certain values from the response object.
In order for Cactus to expose the container objects, Cactus needs to get them from the JVM they live in, and because the container is the only one managing the lifecycle of these objects, Cactus tests need to interact directly with the container.
This leads us to the conclusion that Cactus tests must be deployed inside the container.
The last of these points tells us that Cactus tests live in the container JVM.
This brings us to the next issue: if Cactus tests live in the container JVM, how are they executed? Also, how do we see the result from their execution?
Before we rush into the details, you need to understand a bit more about how Cactus works.
The lifecycle of a Cactus test is shown in figure 14.1
Say, now we have a sample servlet that we want to test and also a test written for that particular servlet.
What we need to do now is package the servlet and the test, along with the necessary Cactus libraries, and deploy the package in the server.
Once we start the server, we have the test and the servlet in both: deployed in the container and in our workspace on the client side.
You can submit the client-side Cactus test to a JUnit test runner, and the runner starts the tests.
Client side refers to the JVM in which you started the JUnit test runner.
On the client side, the Cactus logic is implemented in the YYYTestCase classes that your tests extend (where YYY can be Servlet, Jsp, or Filter)
By overriding runBare, Cactus can implement its own test logic, as described later.
On the server side, the Cactus logic is implemented in a proxy redirector (or redirector for short)
The beginXXX method lets you pass information to the redirector.
The servlet redirector is implemented as a servlet; this is the entry point in the container.
The Cactus client side calls the servlet redirector by opening an HTTP connection to it.
The beginXXX method sets up HTTP-related parameters that are set in the HTTP request received by the servlet redirector.
This method can be used to define HTTP POST/GET parameters, HTTP cookies, HTTP headers, and so forth.
In this case, the ServletTestCase code opens an HTTP connection to the servlet redirector (which is a servlet)
Note that this is the second instance created by Cactus; the first one was created on the client side (by the JUnit TestRunner)
Then the redirector retrieves container objects and assigns them in the YYYTestCase instance by setting class variables.
The servlet redirector is able to do this because it’s a servlet.
Upon returning from the test, it stores the test result in the ServletConfig servlet object along with any exception that might have been raised during the test, so the test result can.
The redirector needs a place to temporarily store the test result because the full Cactus test is complete only when the endXXX method has finished executing (see step 5)
The redirector calls the JUnit setUp method of YYYTestCase, if there is one.
The testXXX method calls the class/methods under test, and finally the redirector calls the JUnit tearDown method of the TestCase, if there is one.
Once the client side has received the response from its connection to the redirector, it calls an endXXX method (if it exists)
This method is used so that your tests can assert additional results from the code under test.
For example, if you’re using a ServletTestCase, FilterTestCase, or JspTestCase class, you can assert HTTP cookies, HTTP headers, or the content of the HTTP response:
In step 3, the redirector saves the test result in a variable stored with the ServletConfig object.
The Cactus client side now needs to retrieve the test result and tell the JUnit test runner whether the test was successful, so the result can be displayed in the test runner GUI or console.
To do this, the YYYTestCase opens a second connection to the redirector and asks it for the test result.
This process may look complex at first glance, but this is what it takes to be able to get inside the container and execute the test from there.
Fortunately, as users, we’re shielded from this complexity by the Cactus framework.
You can use the provided Cactus frontends to start and set up the tests.
Now that you’ve seen what Cactus tests are and how they work, let’s take a closer look at more component-specific tests.
As you already saw, Cactus is designed for testing the core components from the Java EE spec.
When you unit test servlet and filter code, you must test not only these objects but also any Java class calling the Servlet/Filter API, the JNDI API, or any backend services.
Starting from this section, we build a real-life sample application that will help demonstrate how to unit test each of the different kinds of components that make up a full-blown web application.
This section focuses on unit testing the servlet and filter parts of that application.
Later subsections test the other common components (JSPs and EJBs)
The goal of this sample Administration application is to let administrators perform database queries on a relational database.
Administrators can perform queries such as listing all the transactions that took place during a given time interval, listing the transactions that were out of service level agreement (SLA), and so forth.
We set up a typical web application architecture (see figure 14.2) to demonstrate how to unit test each type of component (filter, servlet, JSP, and EJB)
The application first receives from the user an HTTP request containing the SQL query to execute.
The request is caught by a security filter that checks whether the SQL query is a SELECT query (to prevent modifying the database)
If not, the user is redirected to an error page.
If the query is a SELECT, the AdminServlet servlet is called.
The servlet performs the requested database query and forwards the results to a JSP page, which displays the results.
The page uses JSP tags to iterate over the returned results and to display them in HTML tables.
The JSPs contain only layout/style tags (no Java code in scriptlets)
Then, in the following subsections, we test the other components of the Administration application.
In this section, we focus on using Cactus to unit test the AdminServlet servlet (see figure 14.2) from the Administration application.
Let’s test AdminServlet by writing the tests before we write the servlet code.
This strategy is called test-driven development, or test first, and it’s efficient for designing extensible and flexible code and making sure the unit test suite is as complete as possible.
We use it as a base sample in this chapter to see how to unit test servlets, filters, JSPs, taglibs, and database applications.
Before we begin coding the test, let’s review the requirement for AdminServlet.
The servlet should extract the needed parameter containing the command to execute from the HTTP request (in this case, the SQL command to run)
Then it should fetch the data using the extracted command.
Finally, it should pass the control to the JSP page for display, passing the fetched data.
Let’s call the methods corresponding to these actions getCommand, executeCommand, and callView, respectively.
Remember that we haven’t yet written the code under test.
The AdminServlet class doesn’t exist, and our code doesn’t compile (yet)
We extended the ServletTestCase B, because the component that we want to test is a servlet.
We also set a request parameter in the beginXXX method C that we assert to be present in the testXXX method D.
Once we’ve written the test case, we can go on and implement the bare minimum of code that will allow us to compile the project.
We need to implement a sample servlet with a getCommand method.
This is the minimum code that allows the TestAdminServlet to compile successfully.
The code compiles okay, but there’s one more thing that we have to think about.
What you probably notice at this point is that if this test gets executed it will fail, because of the null object that we return.
That said, we always have to ensure that tests fail if we provide corrupt data, as in the previous example.
At this point, we need to ensure that the error is reported successfully.
And after that, when we implement the code under test, the tests should succeed, and we’ll know we’ve accomplished something.
It’s a good practice to ensure that the tests fail when the code fails.
JUnit best practice: always verify that the test fails when it should fail It’s a good practice to always verify that the tests you’re writing work.
Be sure a test fails when you expect it to fail.
If you’re using the test-driven development methodology, this failure happens as a matter of course.
After you write the test, write a skeleton for the class under test (a class with methods that return null or throw runtime exceptions)
If you try to run your test against a skeleton class, it should fail.
If it doesn’t, fix the test (ironically enough) so that it does fail! Even after the case is fleshed out, you can vet a test by changing an assertion to look for an invalid value that should cause it to fail.
Listing 14.5 Implementation of getCommand that makes the tests pass.
The code in this listing is a simple implementation, but it’s enough for our needs.
We want our code not only to compile but also to pass the tests.
The executeCommand method is responsible for obtaining data from the database.
That leaves the callView method, along with the servlet’s doGet method, which ties everything together by calling our different methods.
One way of designing the application  is to store the result of the executeCommand method in the HTTP servlet request.
The  request is passed to the JSP by the callView method (via servlet forward)
The JSP can then access the data to display by getting it from the request (possibly using a useBean tag)
This is a typical MVC Model 2 pattern used by many applications and frameworks.
We still need to define what objects executeCommand will return.
The BeanUtils package in the Apache Commons (http://commons.apache.org/beanutils/) includes a DynaBean class that can expose public properties, like a regular JavaBean, but we don’t need to hardcode getters and setters.
In a Java class, we access one of the dyna properties using a map-like accessor:
The BeanUtils framework is nice for the current use case because we retrieve arbitrary data from the database.
We can construct dynamic JavaBeans (or DynaBeans) that we use to hold database data.
The mapping of a database to DynaBeans is covered in the last section.
This is in contrast to the monumental methodologies, which advocate fully designing the solution before starting development.
When you’re developing using the TDD approach, the tests are written first—you only have to implement the bare minimum to make the test pass in order to achieve a fully functional piece of code.
The requirements have been fully expressed as test cases, and you can let yourself be led by the tests when you’re writing the functional code.
This utility method creates arbitrary DynaBean objects, like those that will be returned by executeCommand.
In testCallView, we place the DynaBeans in the HTTP request where the JSP can find them.
There’s nothing we can verify in testCallView, so we don’t perform any asserts there.
But Cactus supports asserting the result of the execution of a JSP page.
Because this would be JSP testing, we show how it works in section 14.4 (“Testing JSPs”)
Listing 14.7 shows the callView method that we use to forward the execution to the JSP in order to display the results.
We don’t have a test yet for the returned result, so not returning anything is enough.
To begin, we need to verify that the test results are put in the servlet request as an attribute.
This code leads to storing the command execution result in doGet.
But where do we get the result? Ultimately, from the execution of executeCommand—but it isn’t implemented yet.
The typical solution to this kind of deadlock is to have an executeCommand that does nothing in AdminServlet.
Then, in our test, we can implement executeCommand to return whatever we want:
We can now store the result of the test execution in doGet:
Notice that we need the catch block because the servlet specification says doGet must throw a ServletException.
Because executeCommand can throw an exception, we need to wrap it into a ServletException.
Listing 14.7 Implementation of callView that makes the test pass.
If you run this code, you’ll find that you’ve forgotten to set the command to execute in the HTTP request as a parameter.
You need a beginDoGet method to do that, such as this:
First, the call to callView isn’t present in doGet; the tests don’t yet mandate it.
They will, but not until we write the unit tests for our JSP.
Second, we throw a RuntimeException object if executeCommand is called B.
We could return null, but throwing an exception is a better practice.
An exception clearly states that we haven’t implemented the method.
If the method is called by mistake, there won’t be any surprises.
Listing 14.8 Implementation of doGet that makes the tests pass.
JUnit best practice: throw an exception for methods that aren’t implemented When you’re writing code, there are often times when you want to execute the code without having finished implementing all the methods.
For example, if you’re writing a mock object for an interface and the code you’re testing uses only one method, you don’t need to mock all methods.
A good practice is to throw an exception instead of returning null values (or not returning anything for methods with no return value)
There are two good reasons: doing this states clearly to anyone reading the code that the method isn’t implemented, and it ensures that if the method is called, it will behave in such a way that you can’t mistake skeletal behavior for real behavior.
So far, we’ve discussed the Administrator application and shown how to test one part of it, the servlet part.
Now it’s time to move on and concentrate on probably the most difficult-to-test part of the application, the frontend.
In this section, we continue with the Administration application we introduced in the previous section.
Here, we concentrate on testing the view components—namely the JavaServer Pages (JSPs)
We call the application by sending an HTTP request (from our browser) to the AdminServlet (figure 14.2)
We pass a SQL query to run as an HTTP parameter, which is retrieved by the AdminServlet.
The security filter intercepts the HTTP request and verifies that the SQL query is harmless (it’s a SELECT query)
Then, the servlet executes the query on the database, stores the resulting objects in the HTTP Request object, and calls the Results View page.
The JSP takes the results from the Request and displays them, nicely formatted, using custom JSP tags from our tag library.
First, let’s remove any doubt: what we call unit testing a JSP isn’t about unit testing the servlet that’s generated by the compilation of the JSP.
We also assume that the JSP is well designed, which means there’s no Java code in it.
If the page must handle any presentation logic, the logic is encapsulated in a JavaBean or in a taglib.
We can perform two kinds of tests to unit test a JSP: test the JSP page itself in isolation and/or test the JSP’s taglibs.
We can isolate the JSP from the backend by simulating the JavaBeans it uses and then verifying that the returned page contains the expected data.
Because mock objects (see chapter 7) operate only on Java code, we can’t use a pure mock objects solution to unit test our JSP in isolation.
We could also write functional tests for the JSP using a framework such as HttpUnit.
But doing so means going all the way to the backend of the application, possibly to the database.
With a combination of Cactus and mock objects, we can prevent calling the backend and keep our focus on unit testing the JSPs themselves.
We can also unit test the custom tags used in the JSP.
The strategy for unit testing JSPs in isolation with Cactus is defined in figure 14.3
Still in testXXX, we perform a forward to call the JSP under test.
Cactus calls endXXX, passing to it the output from the JSP.
This allows us to assert the content of the output and verify that the data we set up found its way to the JSP output, in the correct location on the page.
In the servlet section (“Testing servlets and filters”), we defined that the results of executing the SQL query would be passed to the JSP by storing them as a collection of DynaBean objects in the HttpServletRequest object.
Thanks to the dynamic nature of DynaBeans, we can easily write a generic JSP that will display any data contained in the DynaBeans.
We can create a generic table with columns corresponding to the fields of the DynaBeans, as shown in listing 14.9
We use both JSTL tags and custom taglibs to write the JSP.
The JSTL tag library is a standard set of useful and generic tags.
It’s divided into several categories (core, XML, formatting, and SQL)
The category used here is the core, which provides output, management of variables, conditional logic, loops, text imports, and URL manipulation.
The second reason is that it gives us a chance to write and unit test custom taglibs of our own.
The callView method from the AdminServlet forwards control to the Results View JSP, as shown in listing 14.10
Listing 14.10 shows a unit test for callView that sets up the DynaBean objects in the Request, calls callView, and then verifies that the JSP output is what we expect.
We start by defining the createCommand method B, which puts several DynaBeans in the request C.
Then in the testCallView D method (remember that it’s executed on the server side) we instantiate the servlet to test E, set the DynaBeans in the request E, and call the JSP F to display the result.
In H we assert different statements against the response of the server, in order to verify that the JSP displays the results properly.
We use the Cactus HttpUnit integration in the endCallView method to assert the returned HTML page.
With HttpUnit, we can view the returned XML or HTML pages.
In listing 14.10, we use the provided HTML DOM to verify that the returned web page contains the expected HTML table.
In this section we described how to test the frontend of the Administrator application.
What we’re still missing is a few pages that will reveal to us how to unit test the AdministratorBean EJB, which executes our queries on the database.
The secrets of EJB testing are covered in the next section.
Testing EJBs has a reputation of being a difficult task.
One of the main reasons is that EJBs are components that run inside a container.
You need to either abstract out the container services used by your code or perform in-container unit testing.
In this section, we demonstrate different techniques that can help you write EJB unit tests.
We also continue developing our Administrator application, showing you the module that executes the SQL queries.
The architecture of the Administrator application goes like this: The command to be executed gets through the filter, which determines whether it’s a SELECT query.
It receives the HTTP requests and calls the getCommand method to extract the SQL query from it.
It then calls executeCommand to execute the database call (using the extracted SQL query) and return the results as a Collection.
The results are then put in the HTTP request (as a request attribute) and, at last, doGet calls callView to invoke the JSP page that presents the results to the user.
So far, we’ve given no implementation of the executeCommand method.
The idea behind it would be to call a given EJB, which would execute the query on a given database.
One simple implementation of the executeCommand method would be as follows:
We call the execute method from the servlet with the given query; there we try to obtain a valid connection and execute the query B.
After that we create a RowSetDynaClass object from the ResultSet C, and we return its rows D.
In order to test the EJB with Cactus, we have to instantiate it and then assert against the result of the execution.
We can use, again, mock objects to simulate the JNDI lookup, but this approach is unnecessarily complicated, so we won’t list it here.
Let’s look at the test case for the EJB in listing 14.12, and then we’ll go through it and discuss it.
Then in each test method we invoke the methods on the EJB with different parameters and assert the validity of the result D.
So far, we’ve covered what you need in order to write Cactus test cases.
Before we rush into the section that deals with execution of our Cactus tests, it’s essential that you get familiar with a project called Cargo.
The tight integration between Cargo and Cactus is one of the new features that facilitate running your tests.
At the beginning of the chapter, we mentioned that Cactus test cases are executed inside the container.
In this section and in the ones that follow we focus on how to integrate execution of Cactus tests in your build lifecycle.
In order to keep the Extreme Programming principles, you need to execute the tests every time you make a build.
This requires a tight integration between Cactus and the build system—Ant or Maven.
But before we jump into that integration, you need to become familiar with a project they both rely on: Cargo.
The aim of the project is to automate container management in a generic way so that we could use the same mechanism to start and deploy a WAR file with Tomcat as we could with WebLogic or almost any other application server.
It provides an API around most of the existent Java EE containers for managing those containers (starting, stopping, and deploying)
After this brief introduction to the secrets of Cargo you’re probably asking yourself, what’s the connection between Cargo and Cactus? The Cactus team realizes that the idea behind the project is great, but it seems as though there’s too much of a burden regarding the process of executing the tests.
Once written, the tests need to be packaged in a WAR or EAR archive; then the application descriptors need to be patched with the appropriate redirectors.
After that, before the execution gets started, the archive needs to be deployed in a container that’s already started.
You’ve probably already noticed the three italicized words in the previous sentence.
And you’ve probably already guessed that the main idea of the Cactus development team was to hide all the complexity regarding the management of the container by means of Cargo.
If we use Cargo’s Ant tasks (or Maven plug-ins) to start the container, then deploy the WAR/EAR in it, and then stop the container we have achieved the Extreme Programming principles of continuous integration.
Our build is fully automated, isn’t it? That’s all true, but deploying the archive with the tests by itself doesn’t do anything magical—we still need to figure out a.
We also need a way to prepare the archive for deployment.
This is all part of the tight integration between Cactus and the various build systems.
The first way to fire up Cactus tests that we’re going to show seems to be the most common one.
If you’re new to Ant, we’d like to recommend to you Ant in Action, by Steve Loughran and Erik Hatcher—a marvelous book.
Cactus comes bundled with two kinds of Ant tasks: the first one will facilitate you in preparing the archive (WAR or EAR) to hold the test cases, and the second will invoke the tests.
We go through these tasks one after the other and show you how to use them.
The process of preparing the archive for executing the tests is called cactification.
Imagine you have at some point an archive (a WAR or EAR file), which is the application that you want to deploy.
The cactification process includes adding the required JARs into the lib folder of the archive and also patching the web.xml to include desired Cactus redirectors.
According to the type of archive that you want to cactify, there are two different tasks that you may want to use: cactifywar and cactifyear.
Before we rush into describing these tasks, let’s first take a minute to focus on the build.xml skeleton that we use for our presentation purposes (see listing 14.13)
As you can see, one of the first things to do is declare some properties in the init target B.
In this target we also resolve the additional dependencies with Ivy C and construct a classpath refid D.
The prepare target prepares the folder structure for the build E, and after that we use the taskdef task F to define the external tasks (remember that cactifyXXX and cargo tasks are external tasks that come with Cactus/Cargo; they aren’t part of the official Ant tasks)
Then we compile our code G and produce either a JAR file containing the EJBs H or a WAR file I containing the servlets (depending on what part of the code we test)
We might use also a separate target to produce the EAR file J that we will need.
Now that you’re familiar with the structure of the build.xml, it’s time to focus on the first set of tasks that Cactus provides.
The cactifywar task extends the built-in war Ant task so it also supports all attributes and.
Nothing explains better than an example, so let’s start the test cases from this chapter using Ant.
We walk through the build.xml in listing 14.14 and then discuss it.
In the cactifywar target we call the cactifywar task, which we imported in the first steps (in the load.tasks target)
As you can see, the cactifywar task takes the following parameters: srcfile, destfile, and a list of redirectors we want to define.
There’s a bunch of other, nonrequired parameters, all of which are perfectly documented on the Cactus website (http://jakarta.apache.org/cactus), where you can find additional help.
Also, once again, because the cactifywar task extends the war task, you can pass all the parameters for the war task to it—they’re all valid.
In the srcfile attribute you specify the archive file of the application that you want to cactify.
The important thing to notice here is that you may need to specify not only the name of the file but also the destination path to it.
In the destfile parameter, you specify the name of the cactified file to produce.
You also may want to describe a list of redirectors in the cactifywar task.
This list of redirectors describes URL patterns to map the Cactus test redirectors to the nested elements filterredirector, jspredirector, and servletredirector.
If you don’t specify those elements, the test redirectors will be mapped to the default URL pattern.
It’s a bit different from cactifywar, because in most cases EAR applications contain a WAR archive that needs to be cactified.
The cactifyear task is, again, an external task that comes from the Cactus team and extends the Ant ear task.
This way it accepts all of the parameters that are valid for the ear task.
Let’s now execute our tests from an EAR archive (listing 14.15), and then we’ll walk through the example application and discuss the different aspects.
Once again, we use the build.xml skeleton from listing 14.13, and here we list the target that’s responsible for the cactification of the already packaged EAR file.
As you can see, the cactifyear task accepts the srcfile and destfile parameters again B.
Their meaning here is exactly the same as for the cactifywar task.
This element has all the parameters of the cactifywar task except the destfile parameter.
The web application will always be named cactus.war and placed in the root of the EAR.
The cactus task is used to execute the Cactus tests, and because every Cactus test is a pure JUnit test, you’ve probably already figured out what task the cactus task extends.
This way, all the parameters that the junit task accepts are also valid for the cactus task.
Listing 14.16 The test target to demonstrate the cactus task.
As you can see, the cactus task is used in connection with the cargo tasks (this is why we introduced Cargo a while ago)
With the one declaration in the listing, we’ve defined all the necessary information to start and execute tests and stop the container.
The warfile/earfile parameter B is used to specify the name of an archive file that we’re going to deploy.
This has to be the name of our cactified WAR/EAR file that holds our test cases as well as the classes that we want to test (it’s the result of the corresponding cactify task)
It’s also obligatory to add a containerset nested element C.
In this nested element we specify a number of cargo tasks D, which will define the containers in which to execute our tests.
When we say that we specify cargo tasks, we mean it—these are pure cargo tasks, and they can take any parameter that a normal cargo task can take: proxy, timeout, server port, and so on.
In the given cargo tasks we have to specify the ID of the containers, the configuration of the containers, and the deployable (the cactified archive that contains our test cases)
Cargo tasks can work with installed local or remote instances of a container; you need only specify the home directory of that container.
But there’s more; these tasks also let you specify the so-called ZipURLInstaller.
In this installer you specify a URL to a given container archive, and Cargo will download the given container from there, extract the container from the archive, start the container, and deploy the given deployable.
Then Cactus will execute the tests inside the container, and Cargo will stop the container.
That’s a fully automated cycle, and the Cactus project comes with several sample applications that use this automation.
Our application is done; not only this, but it’s also well tested using Cactus.
You already saw how to execute your tests with Ant, so it’s time to show one final way to execute the given tests, this time using another build tool: Maven.
Another common approach for executing your Cactus tests is including them in your Maven1 build.
Many people use Maven as their build system, and until version 1.8.1 of Cactus, the only way of executing Cactus tests with Maven was calling the Ant plug-in for Maven and executing the tests via Ant.
The latest version of Cactus, however, contains a cactus-maven2 plug-in that significantly facilitates the cactification process.
The Cactus Maven plug-in consists of two MOJOs (Maven POJOs, or plain old Java objects) you can use for cactification of a WAR or EAR.
Let’s walk through the examples and see how to use them.
Listing 14.17 shows a common pom.xml file that we enhance later.
This is a basic pom.xml, and we can use it for all our purposes: compile our source code and package our application in a WAR archive.
There’s nothing fancy here; we’re just using the corresponding plug-ins of Maven.
Now we can add the Cactus plug-ins to prepare the archive for deployment.
The Cactus plug-in is nothing more than a declaration of several other plug-ins, in the correct order.
Listing 14.18 Build section of the pom.xml to enable Cactus tests execution.
As we mentioned already, all we do is define three plug-ins, one after another in the correct order.
We use it for cactification of the WAR file we got from the previous listing.
Again, as in the cactifywar task, we specify srcfile and destfile parameters C.
We also specify which test classes to include D and which libraries to include in the WEB-INF/ lib folder E.
In this case, we want only the commons-beanutils, because our tests use it.
It’s important to specify the execution order of the plug-ins.
In Maven we specify the execution order by attaching every plug-in goal to a single phase.
This phase, as its name implies, is executed by Maven just before the integrationtest phase (in which we’re going to execute our tests)
This is perfect, because we want our package cactified before we execute our tests.
But wait a second; didn’t we attach the cactifywar goal to the same phase? What will be the execution order here? In this case, we need two different goals attached to the same phase, because we have more than one thing to do before the integration phase of Maven.
In this situation, the cactus plug-in will execute first, because it’s declared right before the cargo plug-in.
And here you can see why we insisted that the order of declaration of the plug-ins in the <build> section is so.
This way we first prepare the cactified archive for deployment, and then we start the container with this archive as a deployable.
This is normal; we need to stop the container once the tests are executed.
As you already saw in chapter 10, this Maven plug-in is responsible for executing JUnit tests.
Because every Cactus test is also a JUnit test, we can use this plug-in to execute our Cactus tests.
There are a couple of things to notice in its declaration.
As you can see, we declare the skip parameter with true.
That’s because the Surefire plug-in is by default attached to the test phase.
We surely don’t want it attached to this phase, so we declare the skip parameter to true, which will cause the plug-in to skip the execution.
Further in the declaration we attach the test goal with the integration-test phase (where we want it to be), and we declare the skip parameter with false.
This will cause the plug-in to execute the tests in the integration-test phase, just as we want it to happen.
The Cactus Ant task does it for you, but the Surefire plug-in doesn’t, so that’s what we do in the last part.
To execute Cactus tests from an EAR file, we have to follow the same procedure as for the WAR file, except for the cactification of the archive.
That’s why we cover the cactifyear Cactus plug-in in this section.
All we have to do here is provide the srcfile and destfile parameters B.
In the <cactusWar> section C, we describe parameters related to the WAR application inside our EAR file, such as the context of the application D, which test classes to include E, and which version of the web.xml will be used.
In G we attach the plug-in to the pre-integration phase of Maven.
The rest of the pom.xml is the same as the one in the previous section, so we won’t discuss it further.
Now we move on and show you one other way of executing your Cactus tests.
Jetty is not only a servlet container, but it also provides you with an API for manipulating the container.
This API is used by Cactus to fire up the container for you, execute the tests, and then stop the container—all with a single command.
We looked at executions through JUnit’s own test runner, with the Ant and Maven test runners, and also through Jetty.
As you already know, Cactus tests are also JUnit tests, so the question, “What is the analogue of the JUnit text test runner for Cactus?” seems valid and reasonable.
JUnit’s test runner communicates directly with the JVM in which the execution takes place and gets the result from there.
Cactus tests are executed in the server JVM, so we need to find a way to communicate with the server (tell it to invoke the tests and get the results)
The easiest way to communicate with the server is via a browser.
In order to do this, we need to take care of a couple of things.
First, we need to declare the ServletTestRunner servlet in the application’s web.xml.
Once it’s declared, we’re going to use this servlet in our URL in the browser to tell the server to invoke the tests.
We need to call the server with the following request in the browser:
Here you need to replace server, port, mywebapp, and mytestcase with the correct values of your server address, port number, context, and the fully qualified name (that is, with packages) of your TestCase class containing a suite() method.
After executing the given URL in the browser, the server should respond with the result shown in figure 14.4
If you see a blank page, click the View Source option of your browser.
It means your browser doesn’t know how to display XML data.
Okay, that’s nice, but what if you want HTML instead of XML? Don’t worry; there’s a solution.
The .xsl stylesheet will generate the HTML report you’re familiar with, so you can view it from within your browser.
When it comes to unit testing container applications, pure JUnit unit tests come up short.
A mock objects approach (see chapter 7) works fine and should be used.
But it misses a certain number of tests—specifically integration tests, which verify that components can talk to each other, that the components work when run inside the container, and that the components interact properly with the container.
In order to perform these tests, an in-container testing strategy is required.
In the realm of Java EE components, the de facto standard framework for incontainer unit testing is Jakarta Cactus.
In this chapter, we ran through some simple tests using Cactus, in order to get a feel for how it’s done.
We also discussed how Cactus works, so we’re now ready to in-container test our Java EE applications.
Testing the components from the Java EE spec is nice, but it isn’t the whole picture.
In the next chapters, we explore one of the most widely used MVC frameworks, JSF.
We also introduce the JSFUnit project, which will let you test your JSF application inside the container.
Figure 14.4 XML result in the browser from Cactus tests.
The Cactus framework we introduced in chapter 14 is great for testing most of the Java EE specifications.
On a daily basis, the majority of Java developers write Java EE applications, all of which are based on the Java EE spec.
In this chapter we take a closer look at the newest member of the Java EE specthe JavaServer Faces (JSF) technology.
It’s a standard specification developed through the Java Community Process (JCP) for building Java web-based user interfaces.
We start the chapter by explaining what JSF is, the problems of testing JSF Testing JSF applications.
Introducing JSF applications, and how to use the JSFUnit project to solve them.
We then implement a sample MusicStore application and demonstrate the power of JSFUnit.
Apart from the specification we mentioned, people refer to JSF1 as the implementation of this specification: a server-side, user-interface, component framework for Java technology–based applications.
This means that different organizations can implement the specification and produce different frameworks, all compatible with the specification.
In this book, we use the Apache MyFaces implementation because we consider it the most robust.
Figure 15.1 shows an architectural overview of a sample JSF application.
The JSF framework was intended to simplify development of web application.
For this reason it’s designed to get developers to think in terms of components, managed beans, page navigations, and so on, and not in terms of technical details such as request, response, session, and the like.
This request needs to be translated in a way that your application can understand.
The JSF framework is also responsible for translating and visualizing responses from the application in a way that the browser can display.
The framework provides a large number of tag libraries that developers can use to visualize absolutely anything.
In the course of the implementation we explain the different parts of the application, and further in the chapter we test our sample application.
In the previous section we described the parts of a typical JSF application.
We now introduce a real application, the MusicStore application, which we refine and test throughout this chapter.
The MusicStore is a simple JSF application that presents different kinds of music albums, which the user can navigate through and purchase.
We start the implementation with a simple POJO (plain old Java object) representing an Album.
The Album is a simple POJO that contains several properties B: the name of the album, author of the album, and so on.
We have also added getters and setters for those properties and a constructor C to instantiate different albums.
Listing 15.2 shows a manager class used to manipulate the albums.
We avoid additional layers such as interacting with a database.
That’s why we start the implementation with a declaration and initialization of a list of five hardcoded Albums B.
Normally we’d invoke a database layer to get these albums, but for the sake of simplicity, we use hardcoded values.
To keep the code listing simple and readable, we removed the Album declarations.
Next, we implement a number of methods: one for retrieving all the available albums.
Listing 15.3 implements the bean that communicates with the frontend.
For every attribute we want to display in the frontend, we specify a corresponding property of the bean B.
Depending on whether you only read or also write in your property, every property needs to have a getter method C and a setter method D.
JSPs interact with this bean, and the bean itself interacts with the AlbumManager.
Before this bean can expose itself to the JSPs, we need to configure the bean in a configuration file called faces-config.xml.
We start with an application declaration B, which contains some information that’s global for the whole application (like the view handlers, the locale parameters, and so on)
The managed-bean declaration C is used to expose our beans to the JSPs.
Notice the managed-bean-name declaration D; this name will be used in the JSP to reference the bean with a class defined in the managed-bean-class declaration.
The only thing that we’re missing to have the full picture of the MusicStore application is the page to display all the albums available.
Listing 15.5 The JSP to display all the available albums.
We need to import the tag libraries that we want to use in this JSP B.
All that our JSP is doing is iterating over all the albums C and displaying the information for every album that we want.
We use the standard JSTL tag libraries C as well core JSF tag libraries D.
The JSP contains a link that to a managed bean we haven’t defined yet:
We want to implement the following: the list_albums.jsp presents all the albums to the user, and on clicking the name of an album you’re redirected to a page that displays the album details.
The new page then allows you to purchase the album or navigate back to the listing page.
Listing 15.6 gives the code for the bean that displays the details for the selected album.
The bean tracks the request B and the album that the user has selected C.
The method that’s called when the user clicks the name of a given album is showAlbumDetails D.
In this method, we get the request and extract a parameter with the name "albumName" E.
Then we use the AlbumManager to extract the Album object F.
Pay attention to the way we extract the request G; if the attribute of the bean is null, we get it from the FacesContext object.
In the next section, we mock the request object and set it as an attribute to the bean.
The last method is the purchase method H; we call this method when a user wants to purchase a given album.
Notice that we stop the execution exactly for a second and a half because we want to simulate that this method is time consuming, which we address when we write performance tests in the last section of this chapter.
Now let’s move on to the JSP that provides the details for the bean (shown in listing 15.7)
This JSP lists all details for the selected album B.
We use RichFaces components to declare these buttons and to implement Ajax behavior.
Notice the reRender attribute in the definition: this attribute holds the ID of another component that needs to be rerendered when we get the response from the server.
In our case, the rerendered component is a text component that displays the status of the purchase E.
In order to see it in action, you have to get the book source code online.
We’ve provided a Maven script with a configured Jetty plug-in; all you have to do is go to the command line, navigate to the folder that contains the pom.xml file for the project, and invoke the Jetty plug-in:
The first image displays all the available albums, and if you click any given album, you should see the second part of the figure, where you can see the details for the album and purchase it.
Moving on, in the next sections we describe how to test the various parts of this application.
As you’ve seen so far, JSF applications typically consist of POJOs (called managed beans) and some frontend JSPs.
Here, our managed beans are simple and therefore easy to unit test.
Indeed, the managed beans are easy to unit test, but the hard part comes when you want to include interaction with a container in your tests.
Normally tests give you the security you need to mercilessly refactor your application.
They provide you with confidence that you haven’t introduced a new bug into the system.
What could possibly break in a normal JSF application? Here’s list of typical problems that might occur in a JSF application:
JSFUnit static analysis can solve all of these problems, as demonstrated in listing 15.8
JSFUnit will parse the config files we’ve specified, and it will check the following:
The problems that might occur when developing JSF applications have different approaches for solving them.
Let’s try to categorize the various ways to test a JSF application.
This is probably the most straightforward approach.2 With JSF applications, a form of black box testing would be to open a web browser and start clicking pages to verify that everything works as expected.
One way to automate black box testing would be to.
We highly recommend you read chapter 5 before continuing with this section.
The point is that in most cases using plain Selenium or HtmlUnit to test your JSF application falls short in several ways.
It’s hard to validate expected HTML (which can be complicated by the use of Ajax), and in case you succeed, your tests may fail on minor HTML changes.
Another approach you might take in order to test the MusicStore application is the white box approach.
In this case you test only the server-side classes using mocks,3
The managed beans are so simple that to test them you don’t need any kind of.
The problems appear when you want to involve objects like FacesContext, which you normally don’t have access to.
For example, let’s test the showAlbumDetails method of the AlbumDetailsBean from listing 15.6 using the JMock library.
This method extracts a parameter from the request, so we need to mock the request and pass different values for the parameter, as shown in listing 15.9
We start by defining the Mockery context B and the object that we want to mock C.
After that, in the @Before method we initialize the request object to be ready for usage D.
The next step defines several test methods, each of which will test the showAlbumDetails method by specifying a different parameter in the request.
For each of those methods we define the expectations E, initialize AlbumDetailsBean F, set the mock request to the bean G, and execute the method under test H.
The final test is to perform the assertions that we want I.
Although this kind of testing is easy, the drawback is that only the server-side logic is tested.
These tests don’t test the interaction with the HTML pages.
Next we look at a non-trivial case, when you want to execute your tests inside the container and test all layers.
The JSFUnit framework builds on Cactus,4 which we discussed in the previous chapter, so the lifecycle of a sample test is the same.
JSFUnit provides an API to gain access to all of the JSF-related objects in your tests.
Let’s start with simple a JSFUnit test-case example, shown in listing 15.10
As with Cactus, we create a test case by extending the Cactus ServletTestCase class B.
Because Cactus extends JUnit 3, our test-case class needs to contain one or more testX (where X is user defined) methods C.
In these test methods we perform the testing, using the JSFUnit API.
In the example, we create a JSFSession object D, which points to the root of our application.
From the JSFSession object, we get two important objects: JSFServerSession E and JSFClientSession.
We use the JSFClientSession to emulate the browser and test the HTML; the JSFServerSession is used to gain access to the JSF state and invoke operations on the managed beans.
We check that the viewID is correct F and that the managed bean always returns a list of Albums of size 5 G.
Testing JSF with JSFUnit looks much like testing with Cactus, but calls the API for testing the JSF components.
Because JSFUnit is built on Cactus, we have the same features as we have with Cactus (including the beginX and endX methods we already covered in chapter 14)
JSFUnit tests are executed the same way as Cactus tests.
We strongly encourage you to go back and read chapter 14 if you skipped it and then come back to this chapter.
To execute the tests you can use Ant or Maven (see chapter 14), or you can execute your tests from a browser.
After you package the application, deploy it to a servlet container and use a URL like the following:
You’ll see the results of the test execution, as shown in figure 15.3
The Ajax elements of an application are scary for most developers to test.
But as you saw in chapter 13, it isn’t that hard to test Ajax.
In this section we focus on testing the Ajax layer of our application, this time using JSFUnit.
Figure 15.3 Results from the execution of our JSFUnit tests in a browser.
Although we already covered this page in listing 15.7, let’s review it briefly.
In our JSP, we use the RichFaces and Ajax4jsf tag libraries to implement the desired Ajax behavior.
We start by defining two rich:panel components B and E to hold the rest of our components.
In the first panel, we put a commandButton to submit the form to the bean specified in the action parameter C.
The command button also submits a status parameter to the bean D.
This attribute specifies an ID of another component that needs to be rerendered.
You can see the screen for this JSP in figure 15.2
You can use the JSFClientSession and click anything, regardless of the fact that the request to the bean will be submitted via JavaScript.
The code for these tests is shown in listing 15.12
As usual, we start the implementation by declaring a test case by extending the ServletTestCase B.
At F we extract the value of the status parameter of the managed bean and assert its value G.
On the client side, we also get the value of the span element on the page H and assert its content I.
The JSFUnit project provides tight integration with the RichFaces project; if you find this treatment brief, you can use the RichFacesClient JSFUnit class.
RichFacesClient provides methods for testing drag-and-drop behavior, sliders, calendars, and other JSF widgets.
In this section, we show how JSFUnit and HtmlUnit can work together.
Chapter 12 shows how to get to an HtmlUnit HtmlPage to start testing a page, for example:
Listing 15.13 shows how to use HtmlUnit in a JSFUnit test:
We start our test case again by extending the Cactus ServletTestCase class B.
In our test C, we create a new JSFSession object with the request URL that we want to invoke.
From the client object, we get an HtmlTable using an XPath expression reflecting the DOM tree structure of our document E.
We assert that the table isn’t null and that the number of.
From the table we retrieve the second row, the first cell out of it, and an HTMLAnchor out of the cell F.
Again, we assert that the link isn’t null G, and we click it H.
The last assertion I compares the title of the page that we got from clicking the link object; we want to make sure that we’re taken to the new page.
JSFUnit provides the option to test the performance of an application.
Going back to our MusicStore application, we’d like to ensure that the purchase method of our AlbumDetailsBeam is always executed in less than a second and a half.
In most cases, we want to time different phases of the application.
We declare a new context parameter with a value pointing to the timer-config.xml file, which holds the declaration for the timer.
We start by creating a new test case B and a test method C.
Next, we click the PurchaseButton to initiate a request to the managed bean F.
We create a JSFTimer to measure the execution time G, and we assert that the totalTime of the execution is less than 1600 milliseconds H.
We can also measure the execution time against some specific Phase I.
The JSFUnit API provides for timing the execution interval upon any of the standard JSF phases:
Once you have your test cases written, you can proceed and refactor your application mercilessly, and you’re assured that as long as the tests pass and the bar is green, everything is okay.
You can write your tests and assert that the execution of a given method always takes less than a given time barrier.
Now you’re free to improve the logic behind that method, and you’ll always be sure that the invocation of the given URL will take no more time than the time barrier allows.
Black box testing is brittle, limited, and hard to perform.
The white box approach (mocking approach) also has its disadvantages; mock tests are always fine grained, which means that the interaction among the different components isn’t tested well.
Also, once written, the tests need to be rewritten in case of small cosmetic changes to the application.
JSFUnit, on the other hand, builds on the Cactus project and uses the incontainer testing strategy.
JSFUnit provides static and performance analysis mechanisms to fully inspect our applications.
In this chapter we also showed how to test RichFaces Ajax components.
Starting with chapter 12, we discussed the challenges encountered when testing the frontend layer of a sample Java EE application.
In the next chapter, we talk about one of the most recent booms in the Java world, OSGi, and the modularity that it provides.
So far, we’ve been testing everything from the Java EE spec.
All the Java EE components that we’ve dealt with (JSPs, tag libraries, servlets, filters, EJBs, and so on) have been available for a long time.
In this chapter we discuss a technology that became popular relatively recently and is getting more popular every day: OSGi.
We start the chapter by introducing OSGi1 We then walk through the basic OSGi concepts and provide easy-to-grasp examples by means of the calculator application.
Theory is when you know something, but it doesn’t work.
Programmers combine theory and practice: Nothing works and they don’t know why.
In the second part of the chapter, we show how to test our OSGi calculator bundle by introducing the JUnit4OSGi framework.
The term OSGi2 usually refers to two things: the OSGi alliance (http://osgi.org/) and the OSGi service platform.
The initial companies involved in the alliance included Sun Microsystems, Ericsson, IBM, and others.
The idea was to create a standards organization for defining specifications for a Java-based service platform, which could also be remotely managed.
This platform consists of multiple bundles (aka modules) that can be installed, started, stopped, updated, and uninstalled remotely and dynamically.
These operations can be performed at runtime without an application restart.
The specification this alliance deals with is the OSGi service platform, which defines a component and service model.
All implementations of the OSGi framework need to provide an environment for the applications to run in.
A bundle is the smallest organization unit in OSGi—a collection of classes, resources, and configuration files, in which the bundle declares its dependencies.
The key mission of a bundle is to declare and use services.
The OSGi service platform provides a context where all the running services are registered.
This bundle context is injected into every bundle during its startup.
The lifecycle of a given OSGi service is shown in figure 16.1
As we mentioned, a bundle can be in different states.
It may seem a bit confusing now, but the example we provide will clear things up, so let’s move on and implement our first OSGi service.
In the first chapter of the book, we implemented a simple calculator application.
That application was simple enough to demonstrate the basic concepts of unit testing.
We also implement a sample client for that service and a test bundle for the client.
Finally, we install the three bundles in the Apache Felix environment.
The implementation of our calculator service starts with the interface in listing 16.1
Apache Felix—open source implementation of the service platform The OSGi alliance defines a number of specifications for the OSGi service platform.
The implementation of these specifications can be done by anyone.
Extract the archive and create an environment variable called FELIX_HOME pointing to the Felix folder.
This JAR is used to instantiate the Felix console to remotely operate the different services.
The interface defines the four methods we want to implement in our service.
The first one  B parses a line of user input.
The user is supposed to input several numbers separated by spaces, and this method parses the input as numbers.
The add method C sums all the given numbers and returns the result.
There is also a multiply method D, which multiplies numbers and returns the result.
As we already discussed, every OSGi service exposes a certain interface to other services.
This listing declares a class that implements the interface in listing 16.1
The details of the implementation aren’t relevant to OSGi, so we’ll skip it.
There’s no big difference between testing OSGi services and POJOs.
This chapter doesn’t cover unit testing alone; it covers integration testing of OSGi services.
Before we continue discussing the integration testing of the service that we just implemented, we create a bundle to hold our service and install the service with Apache Felix, an open source implementation of the OSGi R4 Service Platform.
To create the bundle we need an implementation of the BundleActivator interface to register and unregister the service, as shown in listing 16.3
We start by importing the required classes B from felix.jar in the FELIX_HOME bin/ directory.
Next, we declare our class to implement the BundleActivator interface C.
In D and F, we implement the required start and stop methods, which define how the bundle will behave once it’s started or stopped.
In the start method, we register the CalculatorService interface with the given BundleContext E.
By doing so we notify the framework of our service, and our next step is to expose the interface to other services.
Our stop method F doesn’t need to do anything because Felix automatically unregisters the service when it stops.
To expose our calculator service so that it can be used by other services, we need to include it in a bundle: a JAR file containing all of our classes and the MANIFEST.MF file shown in listing 16.4
Every OSGi bundle is a JAR file, and what distinguishes the JAR as an OSGi bundle is the MANIFEST.MF file.
In the MANIFEST.MF file, we specify different kinds of metadata for our bundle.
The first few lines provide the name, description, vendor, and version of the bundle B.
The Bundle-Activator C specifies the full name of the Activator class (the one that implements BundleActivator)
Which package is exported is specified in the Export-Package clause D.
A bundle can also use and demand services; we specify which services we use with the last clause.
There we define the packages that are needed by our service.
The final step is to compile the source code and produce the bundle (the JAR file)
The result should be the same: a JAR file containing the classes and the MANIFEST.MF file in a META-INF folder.
Now that we have the bundle JAR that contains our service, we use the Felix console and install the bundle.
Navigate to the FELIX_HOME directory and execute the following command:
The Felix console is up and running, and you can now manage your services.
The first thing you can do from here is list all the services that are installed.
For a complete list of all the commands you can use, type in the help command:
We first copy the JAR file to the FELIX_HOME/bundle folder.
Next, we create a sample client application for our CalculatorService.
The idea behind the client application is to demonstrate how to make another bundle that uses the first one, the CalculatorService.
We start by implementing the BundleActivator to override the behavior on start and stop.
The first line must contain the operation the user wants to perform, and the next line should be in the form of several numbers separated by spaces.
The client first invokes the parseUserInput method and with the result calls the corresponding add/multiply and print methods of the service we already installed and prints the result on the screen.
Notice that this time we also import the service from the previous listings.
Keep that in mind when it comes to describing services in the MANIFEST file.
At C we implement the BundleActivator interface and its two methods at D and.
At E we get a ServiceReference from the context using our service’s class name.
If this reference isn’t null, we get the CalculatorService interface from the context at F, by providing the reference to the service we already have.
At G we read two lines of user input from the command line.
The first line contains the operation we want to perform (add or multiply), and the second line contains a set of numbers separated by spaces.
We need to package it in a bundle with a corresponding manifest file.
Listing 16.6 contains the MANIFEST.MF file for the client application.
The only things different from the previous manifest file are the name of the bundle and the activator class.
Notice that we also specify in the Import-Package directive a few more packages:
You do this the exact same way we already showed—no matter what script you use, the result should be identical.
Get the JAR file that’s generated from the build and copy it to the FELIX_HOME/ bundle folder.
Go to the Felix command-line tool and invoke the two commands for installing and starting the bundle:
Again, you need to remember the assigned BundleID for this bundle and specify it when starting the bundle.
If you don’t remember it, use the ps command to find it.
Once the bundle is started, you will be asked to enter an operation and numbers separated by spaces.
Our client application implementation is so simple that we skipped data validation and exception handling.
What happens if we were to enter a blank line or a random string? An exception would be raised, something we don’t want to happen.
How do we test that the client application behaves in the expected way? We need to have some kind of integration tests that we execute inside the service platform (just like in chapter 14 where we executed Cactus tests inside the servlet container)
In the next section, we introduce an OSGi testing framework called JUnit4OSGi; we also write some tests with that framework and include them in a separate test bundle.
When it comes to testing OSGi services, we can apply the same categorization that we made in the second part of the book.
The normal JUnit tests that you already know how to write would exercise each of your services on its own.
Our attention is mainly focused on the integration tests that test the interaction between the different services.
So for implementing integration tests there are different approaches that you might take; we already covered those different approaches in the second part of the book, and they include black box testing, using mock objects, and incontainer testing.
As for black box testing, there isn’t much you can do.
A form of black box testing would be to get an OSGi container, install your services there, and hand it to someone to start playing around and test it “in the black.”
We take a closer look at the other two approaches in the next sections.
Now that we have our services written, let’s examine them.
The calculator service exposes an API used by the CalculatorClient service to compute some sums or multiplications.
The CalculatorClient service, on the other hand, reads some data input from the command line.
This means that if we write a test for this service and invoke it, we have to enter some data on the command line, and because there’s no restriction on the data that we can enter, there’s no way to specify our assertions.
There’s also no way to automate those tests, because we always have to have someone entering data on the command line when the tests are executed.
Listing 16.7 shows the refactored class, where the changed lines are marked in bold.
We start by extracting all the data the user normally enters on the command line (the operation and the numbers separated by spaces) as instance variables to the class B.
We extract the result from the computation as a local variable C, and we provide getters and setters for both B and C.
Next, in the start method we add a check to see if the user data has been set up D, and in case it hasn’t we call the initUserInput method G.
As you can see in E, everywhere in our code that we want to use the user data, we use the getter methods of the class.
At this point we’re sure that this data will be set up, either by the setter methods or by the initUserInput method, which reads the data from the command line.
In F we check the command we want to issue and call the corresponding method in the CalculatorService accordingly.
Notice that the result is this time kept in a local variable, to which we have access through the getter methods.
The initUserInput method G is called when we have no user data defined through the setter methods.
This method has the responsibility for reading the operation we want to issue H as well as the numbers on which we want to issue the command I.
The class contains one entry-point method called start, which we want to unit test.
In order to do this we must obtain a valid BundleContext object, because the method defines it as a parameter.
BundleContext itself is an interface, so we have no way to instantiate.
Chapter 7 introduced mock objects, and we saw mock objects frameworks that can produce fake instances of interfaces that we can use in tests.
That’s exactly what we’re going to do in listing 16.8
We start by creating a new JUnit test case B that will exercise our CalculatorService.
Just as we did in chapter 6, we define the Mockery context C and the objects that we want to mock D.
The @Before method executes before every test method, so that’s where we create the mock objects E.
At F, we define the expectation for the mock objects.
We got lucky—these expectations turn out to be the same for both tests, which is why we can extract them to the @Before method.
We have two tests: one to test the add method of the service G and another to test the multiply method.
The last step is to invoke the start method with our fake object J and assert that the expected results are good.
As you can see, using mock objects to test our service requires quite a bit of preparation.
We need to take care of the proper instantiation and configuration of the mock objects.
The next section introduces the JUnit4OSGi project, which implements an incontainer strategy for testing the CalculatorClient service.
The JUnit4OSGi framework is a simple OSGi testing framework that comes from the iPOJO subcomponent of the Apache Felix project.
You can download the JARs from the website of the project, or if you’re using Maven, you can declare them as dependencies in your Maven project.
Then install them one by one with the Felix command-line tool.
Once they’re installed and started, you’ll get the chance to use one extra command on the command line:
This command will invoke all the JUnit and JUnit4OSGi tests that are present in the bundle with the given [BundleID] and are listed in the manifest descriptor.
Remember that every external package you use needs to be declared in the MANIFEST.MF file of the bundle.
Every JUnit4OSGi test case needs to extend from the OSGiTestCase class C.
We start our first test method at D and get a ServiceReference of the service we wrote and deployed E.
We get the ServiceReference from the context object that JUnit4OSGi provides us.
The framework gives us the access to the BundleContext to check the status of the given service, something that we do in F.
By asserting the ServiceReference isn’t null, we make sure that the service is installed and started correctly.
The following test methods test the service methods; by having the service reference we can get hold of the service itself (at G) and invoke different methods (at H) to see that it behaves the expected way I.
It’s a best practice to separate all your junit-osgi tests in a separate bundle, and that’s what we’re going to do.
Go to the calculator-test folder of the source code of the book, and use Maven or Ant to package the bundle that contains the test from the previous listing.
After you’ve done this, copy the resultant JAR file and paste it in the FELIX_HOME/bundle folder so that it’s easy to install.
The next step is to install and start the service in the bundle the way we described at the beginning of the chapter.
Our final step is to call the test inside the container.
If you’re using the source code for the book and there are no errors, the result should be the same as this:
As we already mentioned, the junit command is currently available only for the Apache Felix implementation of OSGi.
So how can we run our tests if we use any of the other implementations of OSGi? In that case we have to use the GUI runner that comes with the JUnit4OSGi project.
After the bundle is started, you’ll see a Java pop-up that lets you select the bundles that contain JUnit tests.
You can select as many as you want, and after clicking the Execute button you should see the result: a green bar for passing tests and a red bar for failing tests (see figure 16.2)
This example of executing unit tests through the GUI runner concludes our look at JUnit4OSGi and this chapter on OSGi.
In this chapter, we discussed testing OSGi bundles, a technology that is getting more and more popular.
We introduced the technology and its key concepts, the bundle and service.
You should also be able to test all your OSGi bundles using JUnit4OSGi and to apply different techniques for testing your bundles, like testing with mock objects, integration testing, and so on.
In the following chapters, we begin to study the backend layer of applications.
We show you different techniques for testing Hibernate and JPA as well as integration testing of your data access layer.
Figure 16.2 Executing the JUnit tests with the GUI runner.
The persistence layer (or, roughly speaking, the database access code) is undoubtedly one of the most important parts of any enterprise project.
Despite its importance, the persistence layer is hard to unit test, mainly because of the following issues:
Unit tests must exercise code in isolation; the persistence layer requires interaction with an external entity, the database.
Unit tests must be easy to write and run; code that accesses the database can be cumbersome.
Unit tests must be fast to run; database access is relatively slow.
Dependency is the key problem in software development at all scales....
We call these issues the database unit testing impedance mismatch, in reference to the object-relational impedance mismatch (which describes the difficulties of using a relational database to persist data when an application is written using an objectoriented language)
The database-testing mismatch can be minimized using specialized tools, one of them being DbUnit.
In this chapter, we show how DbUnit can be used to test database code, and we not only describe its basic concepts but also present techniques that make its usage more productive and the resulting code easier to maintain.
Let’s take a deeper look at the three issues that compose the database unit testing impedance mismatch.
From a purist point of view, tests that exercise database access code can’t be considered unit tests because they depend on an external entity, the almighty database.
What should they be called then? Integration tests? Functional tests? Nonunit unit tests?
Well, the answer is, there is no secret formula! In other words, database tests can fit into many categories, depending on the context.
Pragmatically speaking, though, database access code can be exercised by both unit and integration tests:
Unit tests are used to test classes that interact directly with the database (like DAOs)
Such tests guarantee that these classes execute the proper SQL statements, assemble the right objects, and so on.
Although these tests depend on external entities (such as the database and/or persistence frameworks), they exercise classes that are building blocks in a bigger application (and hence are units)
Despite the theoretical part of the issue, there’s still a practical question: can’t the data present in the database get in the way of the tests?
If you don’t have a clue as to what we’re talking about, don’t panic! JPA testing and its issues will be explained in detail in the next chapter.
Yes, it can, so before you run the tests, you must assure that the database is in a known state.
Fortunately, there are plenty of tools that can handle this task, and in this chapter we analyze one of them, DbUnit.
It doesn’t matter how much a company, project manager, or technical leader praises unit tests; if they’re not easy to write and run, developers will resist writing them.
Moreover, writing code that accesses the database isn’t the sexiest of tasks.
One would have to write SQL statements, mix many levels of try-catch-finally code, convert SQL types to and from Java, and so on.
Therefore, in order for database unit tests to thrive, it’s necessary to alleviate the database burden on developers.
Luckily again, there are tools that provide such alleviation, and DbUnit is one of them.
Let’s say you’ve overcome the first two issues and have a nice environment, with hundreds of unit tests exercising the objects that access the database, and where a developer can easily add new ones.
This is the hardest issue, because it can’t always be solved.
Typically, the delay is caused by the database access per se, because the database is probably a remote server, accessed by dozens of users.
A possible solution is to move the database closer to the developer, by either using an embedded database (if the application uses standard SQL that enables a database switch) or locally installing lighter versions of the database.
Notice that the fundamental characteristic of an embedded database is that it’s managed by the application and not the language it’s written in.
For instance, both HSQLDB and Derby support client/server mode (besides the embedded option), although SQLite (which is a C-based product) could also be embedded in a Java application.
In the following sections, we show how DbUnit (and, to a lesser degree, embedded databases) can be used to solve the database unit testing impedance mismatch.
At about the same time, Richard Dallaway wrote an online article titled “Unit testing database code” (http://dallaway.com/acad/dbunit.html), which inspired the creation of DbUnit.
Since then, DbUnit has became the de facto Java framework for database testing, and its development has had its up and downs.
After a period of high activity, when most of its codebase was created, it faced a long drought.
Fortunately, though, new developers jumped in and, during the time this book was written, several new versions have been cut, providing many improvements and bug fixes.
Although DbUnit comprises hundreds of classes and interfaces, DbUnit usage roughly consists of moving data to and from the database, and that data is represented by datasets (more specifically, classes that implement the IDataSet interface)
In the following subsections, we examine the basic usage of datasets and some other DbUnit artifacts.
Throughout this chapter, we use DbUnit to unit test the persistence layer of a Java application.
In order to simplify, this layer consists of only the interface defined in listing 17.1
The DAO implementation (using plain JDBC) isn’t shown here but is available for download at the book’s website.
The User object will be mapped in the database by the users table, which can be created using the SQL statement shown in listing 17.3
Finally, the examples will use HSQLDB as the database, because it’s Java based and doesn’t require any further configuration.
The simplest—and fastest—mode is as an in-memory embedded database, and that’s the mode used in the examples.
The sample application is available in two flavors: Maven and Ant.
To run the tests on Maven, type 'mvn clean test'
The application is also available as two Eclipse projects, one with the required libraries (under the lib directory) and another with the project itself.
Let’s start by writing a unit test for the getUserById() method.
First, we need to analyze what the method does: it fetches data from the relational database, creates a Java object, populates that object with the fetched data, and then returns the object.
Consequently, our test case must prepare the database with the proper data, run the code being tested, and verify that the object returned contains the expected data.
The latter two steps can be done with trivial Java code, whereas the former needs interaction with a database—that’s where DbUnit is handy.
Let’s see how this data could be represented on these two different dataset implementations, first in the XmlDataSet format (listing 17.4)
The XmlDataSet format is self-described, but it has two problems.
As you can see in the previous example, a simple row in a table required 16 lines of XML code.
The advantage of this format is that it follows a well-defined DTD (available inside DbUnit’s JAR), which could avoid problems caused by bad XML syntax.
But that brings up the second issue: DbUnit doesn’t validate the DTD (that DOCTYPE line could be removed or even changed to any garbage, and the result would be the same)
Although the lack of XML validation is a DbUnit bug, the verboseness of the format is a design option.
A much simpler option is to use FlatXmlDataSet, where each line describes a row in the database.
Listing 17.5 shows the same dataset using the flat XML format.
The FlatXmlDataSet format is much clearer and easier to maintain,4 so we use it in our examples.
This format has its issues as well, which we will cover later in the chapter.
Our test case B doesn’t need to extend any DbUnit or JUnit class.
Although it doesn’t sound like a big deal in this example, being forced to extend a superclass could be a big limitation in real life, especially if you use different testing frameworks such as TestNG.5
Remember that test methods (such as C and E) are still code, and hence “real code” best practices should be applied to them as well.
DatabaseTestCase, which would make it hard to use DbUnit with TestNG.
Alternatively, if a connection pool was used instead, these methods could be defined at @Before/@After, respectively.
Notice that in this example, both connection and dbunitConnection were created using hardcoded values; in real projects, a better practice would be to define these settings externally, such as in a property file.
That would allow the same tests to be run in different environments, such as using alternate databases or getting the connection from a pooled data source.
Getting an IDataSet from an XML file is so common that it deserves its proper method F.
It’s also a good practice to load these XML files as resources in the classpath, instead of physical files in the operating system.
Because an NPE is a sure bet to cause a lot of headaches, adding an assertNotNull() G with a meaningful message is a one-liner that can save you time troubleshooting.
We have a dataset (setupDataSet) with the data we want to insert and a connection to the database (dbunitConnection)
All that’s left is the class responsible to do the dirty work, and that’s DatabaseOperation or, more precisely, one of its subclasses.
In this case, we use CLEAN_INSERT, which first deletes all rows from all tables defined in dataset and then inserts the new ones.
A final note about transactions: in order to keep this example simple, we aren’t dealing with transactions at all, and every database operation is done in one transaction (using JDBC’s autocommit feature)
Although this simplification is fine here, usually the test cases must be aware of the transaction semantics.
The exact approach depends on many factors, like the type of test (unit or integration) being written and the underlying technologies used in the code tested (like pure JDBC or ORM frameworks).6
Chapter 18 offers more detailed insight on transactions in JPA-based test cases.
DatabaseOperation is the class used to send datasets to the database.
Although DbUnit makes good use of interfaces and implementations, DatabaseOperation is one of the few concepts that isn’t defined by an interface.
Instead, it’s defined as an abstract class with an abstract method (execute(), which takes as parameters a dataset and a database connection)
The reason for such different design is to facilitate its use, because the abstract class also defines constants for its implementations (DbUnit was created on Java 1.3/1.4 when there was no native enum), so the operations can be executed with just one line of code, as we saw in the first example.
The implementations provided by DatabaseOperation as static fields are as follows:
Similarly to UPDATE, DbUnit will throw an exception if any row already exists.
Because rows are inserted in the order in which they appear in the dataset, care must be taken when tables have foreign keys: rows must be defined in the dataset using the right insertion order.
REFRESH—This is a mix of INSERT and UPDATE: rows that exist in the dataset but not in the database are inserted, but rows that exist in both are updated.
DELETE—Delete from the database only the rows present in the dataset, in the reverse order in which they appear in the dataset.
As you add more test cases, the proportion tends to revert, although it’s common to add more helper methods as well.
These helpers can typically be reused by other test classes.
Consequently, it’s good practice to create a superclass that defines only these infrastructure methods and then make the real test classes extend this superclass.
This best practice applies not only to DbUnit-based tests but also to testing in general.
TRUNCATE—Same purpose as DELETE_ALL, but faster, because it uses the SQL’s TRUNCATE TABLE.
The only drawback is that not all databases support such SQL operation.
It creates a DatabaseOperation that will wrap another operation inside a database transaction.
It’s particularly useful in cases where tables have circular dependency and rows can’t be inserted outside a transaction with deferred constraints.
That’s it; we wrote our first DbUnit test case and set the foundation for most of the tests to come.
Another common use of datasets is to assert that the database has the right data after an insert or update.
Back to our DAO example: we need a test case for the addUser() method, and the workflow for this test is the opposite from getUserById()’s test.
Here we first create a User object, ask our DAO to persist it, then use a DbUnit dataset to assert that the data was properly inserted.
The code snippet in listing 17.7 is our first attempt at such a test case.
The @Test method testAddUser() first creates and populates a User object C.
Next, we ask DAO to persist the User object and check that it generates a valid ID D (a positive value, in this case) and that the returned ID matches the object.
Such checking is important in situations where the caller needs to use the new ID (for instance, in a web page that generates a link to edit the newly created object)
It may sound trivial and redundant in this case, but you’d be surprised by how often the ID isn’t set correctly in more complex combinations (like multilayered applications with Spring managing transactions and Hibernate being used as the persistence layer)
At E we use the same dataset (user.xml) and same method (getDataSet()) as the previous example—it’s always a good practice to reuse code and testing artifacts.
Because both JUnit’s Assert and DbUnit’s Assertion have assertEquals() methods, if you static import both, chances are a call to assertEquals() will reference the wrong one.
Best practice: use a helper class to create and assert object instances In the previous example, testGetUserById() fetched an object from the database and asserted its attributes, but testAddUser() did the opposite (instantiated a new object, filled its attributes, and then inserted it in the database)
As your test cases grow, more and more tests will need to do the same.
To avoid the DRY (don’t repeat yourself) syndrome, it’s better to create a helper class containing methods and constants for these tasks.
Doing so improves reuse in the Java classes and facilitates maintenance of dataset files.
If you use Java 5 and static imports, accessing members in this helper class is simple.
Listing 17.8 shows a revised version of testAddUser() using this practice.
Listing 17.8 Revised version of testAddUser(), using a helper class.
This is fine in our example, where the database has only one table and the database access is fast (because it’s an embedded database)
In most other cases, though, it would be overkill—either the test would fail because it would return tables that we’re not interested in or it would be slow to run.
The simplest way to narrow the field is by filtering the tables returned by createDataSet(), by passing an array containing the name of the tables that should be returned.
Applying that change to the previous example, we have the following:
A similar approach is to use a FilteredDataSet to wrap the dataset containing the full database:
A FilteredDataSet decorates a given dataset using an ITableFilter, which in turn is a DbUnit interface that defines which tables belongs to a dataset and in what order they should be retrieved.
Finally, a third option is to use a QueryDataSet, where you explicitly indicate which table should be present in the dataset.
The next example returns a dataset that has the exact same contents as the previous example:
Comparing the three options, the overloaded createDataSet() is obviously simpler in this case.
But the other options have their usefulness in different scenarios:
Using a query, you can narrow the field even more, by selecting only the rows the test case is interested in, which is useful when the database contains a lot of data, for example:
FilteredDataSet can be used with any ITableFilter, such as a filter that returns tables in the right foreign-key dependency order (as will be shown in section 17.6)
If you run the previous test method alone, it will pass.
This is a common problem when using DbUnit—and one of the most annoying.
A test case passes when it’s run alone but fails when run as part of a suite.
In this particular case, our user.xml dataset has only one row, and this is what we assume the database should contain after we insert the User object.
When many tests are run, it fails because the database contains something else.
We could say the culprit is the previous test, which did not clean itself up.7 It’s the test case’s responsibility to make sure the database is in a.
The DbUnit documentation states: “Good setup don’t need cleanup” and you “should not be afraid to leave your trace after a test.” This isn’t always true, though, as you’ll see in section 17.8
Asserting database state with datasets known state before the test is run.
This is achieved by using the same dataset and a DELETE_ALL operation:
If we add this code and run the whole test again, the test fails:
Now the number of rows is correct (because we deleted all rows from the users table before running the test), but the ID of the inserted row doesn’t match what we expect.
This is another common problem, and it happens frequently when the database generates the ID.
There are many solutions for this issue, ranging from simple (like ignoring the ID column in the comparison) to sophisticated (like taking control of how IDs are generated—we show this approach in the next chapter)
Listing 17.9 shows the full method for our new test case.
In the next section we examine a better (although not yet optimal) approach, where the database information (table and column names) is contained just in the dataset file.
DbUnit provides a simple yet powerful IDataSet implementation called ReplacementDataSet.
In the following sections, we explore how it can be used to solve some common problems.
Let’s try a different approach for the “same dataset, different IDs” issue.
Instead of ignoring the ID column, couldn’t we dynamically change a dataset value before the test case uses it?
Changing the data inside a dataset would be quite complicated.
Fortunately, though, DbUnit provides the ReplacementDataSet class, which decorates an existing dataset to dynamically replace tokens, according to your needs.
Back to our problem: first we need to change the dataset XML file, by replacing the hardcoded IDs by a token (we used [ID] in this case, but it could anything, as long as that string doesn’t occur somewhere else in the dataset)
Next, we change the test case class, with the changed (and new) methods shown in listing 17.11
Listing 17.10 user-token.xml, a dataset that uses a token for IDs.
Notice that the original dataset remains intact, and the method returns a new dataset.
Next, we call G the new method, which reads the original XML file and returns a dataset with the [ID] dynamically replaced.
In the second test, we use DELETE_ALL H to clean up the database; note that the IDs are irrelevant.
But if we used another DatabaseOperation (like DELETE), we’d need to use a decorated dataset here as well.
For the next part of the test I, we need to use a decorated dataset in the assertion; we use the ID returned by the DAO itself.
If the test still fails because of a wrong ID, then something is wrong with the DAO class.
Another situation where a ReplacementDataSet is useful is to represent NULL values (SQL’s NULL, not Java’s null) in a dataset.
The way DbUnit handles NULL in FlatXmlDataSet files is tricky and deserves clarification:
If a column exists in the database but is missing in an XML line, then the value of that column (for that row in the dataset) is assumed to be NULL.
But that applies only if the column was present in the first line of XML.
DbUnit uses the first line to define which columns a table is made of.
This is true unless the database columns are defined in a DTD!
Best practice: don’t hardcode values This example uses hard coded 1s in method calls:
Would you have the same doubts? This example illustrates how a subtle change (which costs just a few seconds of a developer’s time) makes code much more understandable (and consequently easier to maintain)
Create variables or constants whenever appropriate, even if the variable will be used only once.
This situation has improved in more recent versions of DbUnit.
Although the idiosyncrasy still exists, at least now DbUnit is aware of the problems it can cause and logs a warning message whenever it finds a line in the XML file with different columns than the first one.
Listing 17.12 user-ok.xml, where the first line has all columns.
Now let’s write a test case (listing 17.14) that does the following:
The reason for such failure is item 2: DbUnit uses the content of the first line to define how many columns it is expecting thereafter.
Listing 17.14 Test case that demonstrates the missing column issue.
All we need is to define a new token (say, [NULL]) and add another replacement role:
This is the same method we used before; we just added a new replacement role here B.
In C, in order to simplify, we’re using the method that expects an ID and passing a bogus value (-1), because it won’t be replaced anyway (as the dataset doesn’t have any [ID] token)
Better yet, the tokens to be replaced shouldn’t be passed as parameters (we look at how to do that later, in section 17.7.3)
If in E we compared actualDataSet against revertedDataSet (which is the original XML file with the proper [NULL] tokens replaced), the assertion would still fail.
In order to solve this issue without changing the order in the dataset.
Transforming data using ReplacementDataSet (whose wrong order is the whole purpose of the example), we wrapped the dataset in a SortedDataSet D, which returns a new dataset with the tables sorted by the order in which its columns were defined in the database.
In this example, it would sort first by ID, which is the first column in the CREATE TABLE statement.
If any two or more lines had the same ID (which isn’t the case here, because ID is the primary key), then it would sort them by username (the second column), first_name (third column), and so on.
Unfortunately, DbUnit supports only physical locations, so the DTD path must be relative to the project’s root directory or an absolute path in the filesystem.
Ideally, DbUnit should support looking up the DTDs in the classpath.
Using a DTD adds more validation to the datasets (which can prevent other errors), at the cost of a more complicated initial setup (creating the DTD, making sure it’s in the right place, and so on)
On the other hand, using [NULL] makes the datasets clearer, because its presence explicitly indicates that a value is NULL.
Hence, the decision depends more on the project context and personal preferences than on the technical merits of the approach per se.
If that happens, it means DbUnit logging is somehow disabled.
There’s no API or DbUnit configuration file to explicitly enable logging.
Briefly, you need to add to the project’s classpath a JAR containing a real SL4J implementation.
In our cases we didn’t see any log because the project’s Ant script is explicitly using sl4j-nop.jar, which doesn’t log anything (this is also the default implementation included in your Maven project if you just add DbUnit as a project dependency)
If you don’t use any logging framework, the easiest solution (the one that requires no extra configuration) is to add sl4j-simple.jar.
This provider sends info messages to System.out, sends warnings and errors to System.err, and ignores all other logging levels (like debug and trace)
Adding sl4j-simple.jar to the classpath and running the test case again, we get the aforementioned warning:
Whenever you’re facing problems that sound like a DbUnit bug or usage issue, try enabling the lower logging levels9 like debug or trace.
DbUnit will output a lot of debugging information, which will hopefully help you resolve the issue.
So far in the examples, we created dataset XML files from scratch, in a bottom-up approach.
This is the ideal situation when you’re doing pure TDD, but often you need to create these files from the data already in the database.
For instance, you might be working on a big project, where the database development is done by a separate team of DBAs and a QA team maintains a database instance full of testing data.
Typically in these situations, your Java code (and test cases) will have to deal with complex scenarios, like tables with dozens of columns and many foreign key relationships.
It would be unpractical and error prone to create the datasets.
Advanced techniques from scratch, so you can leverage the existing data to create the initial files and then prune the data your test cases don’t need.
Even if your project is simpler and you can create the XML files from scratch in your typical development cycle, you may face bugs that are hard to reproduce in a test case.
For instance, the user accesses a web application, executes a couple of inserts and updates, and then a page displays a table with incorrect data.
In this case, instead of trying to reproduce all steps through code in your test case, you could just manually reproduce the steps, then export the relevant database contents to a dataset, and reproduce only the buggy method call in the test case.
This simple approach works fine most of the time, but it has a drawback: the tables in the dataset are created in no particular order.
Therefore, if one table has a foreign key constraint with another table, and they’re generated in the wrong order, attempts to insert the dataset into the database will mostly likely fail (because of constraint violations)
In this section, we analyze techniques that make DbUnit usage easier to understand and maintain.
These techniques don’t employ any particular DbUnit feature, just advanced Java and JUnit APIs.
If you look at the previous examples from a higher level, you might realize they all follow the same workflow:
The most common—and simpler—way to implement the template pattern in Java is through an abstract superclass that implements the template and defines abstract methods for the steps the subclasses must implement.
This isn’t a good approach in our case, because it would allow each subclass to have only one test method, which in turn would require dozens or even hundreds of test classes in a typical project.
A second approach is to create an interface that defines the steps the template method isn’t responsible for and receive an implementation (which is typically an anonymous class) of that interface as parameter.
Design patterns in action: Template Method The Template (or Template Method) is a behavioral design pattern described in the classic GoF10 book.
In this pattern, a superclass defines the overall skeleton of an algorithm (that is, the template) but leaves some details to be filled in by subclasses.
We start by defining the interface TemplateWorker B, which will be implemented as an inner class by the test cases that use the template method.
In the template method runTemplateTest C, notice that D, E, and F match the three workflow steps described previously.
In the first test method, testGetUserById(), it isn’t necessary to check the database state after this test case is run, so null is returned G.
The problem with this approach is that it’s too verbose and unnatural, because we have to create an inner class on each test method and do the work inside that class (instead of inside the method)
In the next section we show a much cleaner approach.
Since their introduction to the Java language, annotations have grown in popularity and are used by many development tools, such as JUnit itself (we’ve been using JUnit annotations, such as @Test, throughout this book)
What most developers don’t realize, though, is that they don’t need to limit themselves to using third-party annotations; they can create their own project-specific annotations.
Although Joshua Bloch preaches the opposite in Effective Java Second Edition,11 we believe that custom annotations can boost a project’s productivity, particularly in the test-cases arena.
That being said, let’s use custom annotations as a third approach to the template pattern implementation.
The annotation attribute setUpDataSet B defines the dataset used to prepare the database.
If not specified, the default value is "/empty.xml", which will clean up the entire database.
Similarly, assertDataSet() defines the dataset that will be used to check the database state after the test is executed.
Because not all test cases must check that (typically, test cases for methods that load data don’t), the default is "" C, which in our case means no dataset.
Notice that the meaning of an annotation value is relevant to.
Listing 17.19 empty.xml, dataset used to clean up the database.
Because it isn’t possible to use null, we use the empty string to indicate no dataset.
Compare this new test class with the previous example (listing 17.19)
This technique is explained in more detail in appendix B.
The variable id C can’t be passed around in annotations, because it can have dynamic values (like in testAddUser()), and annotations can receive only literals (because they’re defined at compile time), so it must be shared among test cases.
Such an approach might annoy purists, but keeping state in tests is not only acceptable in some cases but often is the best approach for a given problem.
The state (id) in this case is passed around only to solve an issue caused by the way the tests are executed.
The template method invokeTestMethod() D defines the three steps of the workflow described earlier.
First, the annotation is read E and the dataset is used only if the annotation value isn’t an empty string F.
If later on we create a test case where it’s necessary to replace two ids, and we know they will be generated in sequence, the method is.
And the dataset XML would have both [ID] and [ID2]:
A much cleaner approach would be to figure out the tokens dynamically, where the following apply:
Fortunately, there’s a standard Java technology that fits perfectly in this description, the Expression Language (EL)
And the getReplacedDataSet () would not require id parameters anymore; instead, the id would be bound to the EL context:
Notice that, despite EL being a standard API, it’s still necessary to create an ELContext implementation (ELContextImpl, in our example), and that isn’t a trivial task (because of lack of documentation)
It’s out of the scope of this book to explain how that class was implemented (although the code is available for download), but a quick explanation can be found at the author’s blog (http://weblogs.java.net/blog/felipeal/)
We start by creating a new EL context object B before each test and making it available through the method getContext()E, so the test cases could bind more objects to the context as needed.
Using the EL context, the id is bound C before the setup dataset is read and bound again D before the assert dataset is read.
This is necessary because the test case might have changed the id (like on testAddUser()), and the id is represented by a primitive type (if it was a mutable object, this second bind would not be necessary)
This is necessary because row() is used during XML parsing, and FlatXmlDataSet parses the XML in the constructor.
This is a bad practice on DbUnit’s part—a constructor shouldn’t call methods that can be overridden by subclasses.
Finally, we get to where the EL engine does its job of evaluating the expression J, according to the values bound in the context.
Now that we’ve covered some of the advanced techniques you can use with DbUnit, we look next at best practices that are specific to database access testing.
Throughout this chapter, we described in detail best practices that apply to our examples.
When you run a database test case, the test can leave the database in an unknown state.
Furthermore, the actions of other users can affect the test results.
One solution to this problem is to have each developer and build machine use their own database.
If you’re fortunate enough to be developing an application that can be run in different database products (for instance, if it uses only standard SQL statements or if the SQL is managed by an ORM tool), then the best approach is to use an embedded database.
Not only would each developer would have their own instance, but the database access would be fast.
If the embedded database approach isn’t possible, then you should try to install a matching database in each developer’s machine (many database vendors, such as Oracle, provide a light version of their product, a good option for this approach)
If neither the embedded nor the light database is possible, then try to allocate one database instance for each developer in the database server.
In the worst case, if not even that is possible (too many instances could be costly in resources or in license fees), allocate a few instances to be used for test cases and a few others for regular development.
If you can implement the embedded database approach, but the final application will be deployed in another database product, make sure the application is tested against the target database.
A reasonable approach is to let the developers use the embedded database but have a daily or continuous build use the target database.
Listing 17.25 user-EL.xml, dataset that uses EL syntax for tokens.
Don’t make the mistake of assuming databases can be substituted at will; there are always incompatibilities, even if you use an ORM tool.
As the old sayings goes, “everything that goes up must come down.” If you write a test case that verifies an object is correctly stored in the database, chances are you should write the test that asserts it’s loaded correctly.
And if you keep that in mind when you write one of them, it makes it easy to write the other one; you could reuse the same dataset or even write special infrastructure to handle both.
All versions of testGetUserById() used in this chapter covered just one scenario: the database contained a row with the tested ID and only that row.
That was enough for the purpose of describing the techniques, but in a real project you should test other scenarios, such as testing an ID that doesn’t exist in the database, testing an empty database, testing when more than one row is available, and testing joins when multiple rows are available.
The last scenario deserves special attention, because it’s a common issue when you use an ORM tool and you’re testing a method that uses a complex query where one or more tables are selected using a join.
Let’s say a User object has a List<Telephone> relationship, the Telephone class is mapped to a telephones table with a foreign key to the users table, you’re using JPA in the persistence layer, and the getUserById() must do a join fetch to get all telephones in just one query (if you aren’t familiar with these concepts, don’t worry; we explain them better in chapter 18)
You write just one test case, where the dataset contains only one row in both users and telephone rows, and you implement the JPA query as something like "from User user left join fetch user.telephones where user.id = ?"
Then once the application is in production, and a user happens to have more than one telephone, your code returns two users for the query.
After half a day of debugging, you figure out the fix would be to change the query to "select distinct(user) from User user left join fetch user.telephones where user.id = ?"
Had your initial test case covered more than the canonical scenario, you wouldn’t have had this bug.
As your application grows and you write more database test cases, your datasets become hard to manage.
This is probably the biggest DbUnit drawback, and unfortunately it’s a problem without a clear solution.
The best “practice” here is to be aware of this problem and plan in advance.
With this in mind, the following techniques can mitigate the problem:
Or, in a more sophisticated approach, you could keep smaller XML files for groups of objects and then use XML include or CompositeDataSet to join them.
Keep in mind, though, that any of these approaches brings more complexity to the test cases, and you might end up with something that’s harder to maintain than a good, old search and replace.
In all examples so far, the test cases set up the database but didn’t bother to clean it up after they did their job.
This is typically a good practice and is indeed one of the best practices endorsed by DbUnit.
The problem of not cleaning up the database after the test is done is that a test case could make another test’s life miserable if it inserts data that’s hard to be cleaned, like rows with foreign keys.
Back to the users/telephones tables example, let’s say a test case adds one row to each table, with the telephones row having a foreign key to users, and this test case doesn’t clean up these rows after it’s run.
Then a second test case is going to use the same users row, but it doesn’t care about the telephones table.
If this test tries to remove the users row at setup, it will fail because of a foreign key violation.
So, long story short, although typically a good setup doesn’t need cleanup, it doesn’t hurt to clean up, especially when the test case inserts rows with foreign key constraints.
The persistence layer is undoubtedly one of the most important parts of any enterprise application, although testing it can be a challenge: test cases must be agile, but database characteristics make them bureaucratic.
Although JUnit itself doesn’t have an answer to this problem, many tools do.
In this chapter, we used DbUnit to validate an application’s database access.
DbUnit is a stable and mature project, comprising a few dozen interfaces, implementations, and helpers.
Despite this high number of classes, DbUnit usage is relatively simple, because it consists of setting up the database before a test is run and comparing its state afterwards.
Although DbUnit is a great tool, it’s a low-level library, which provides the basic blocks for database testing.
To use it efficiently, it’s necessary to define infrastructure classes and methods that at the same time leverage DbUnit strengths and provide.
With creativity, experience, and planning, it’s possible to write DbUnit tests in an efficient and enjoyable way.
In this chapter, we demonstrated through progressive examples how to use DbUnit to populate the database before tests, assert the database contents after the tests, create datasets from existing databases, and use advanced APIs to make tests easier to write and maintain.
In the next chapter, we show how to extend these techniques to JPAbased applications, and in chapter 19 we cover tools that enhance JUnit, including Unitils, which provides some of this chapter’s techniques out of the box.
Most Java applications need to store data in persistent storage, and typically this storage is a relational database.
There are many approaches to persisting the data, from executing low-level SQL statements directly to more sophisticated modules that delegate the task to third-party tools.
Unfortunately we need to deal with the object relational (O/R) impedance mismatch, and to do so you need to understand.
Although discussing the different approaches is almost a religious matter, we assume it’s a good practice to abstract data persistence to specialized classes, such as DAOs.
The DAOs themselves can be implemented in many different ways:
Writing brute-force SQL statements (such as the UserDaoJdbcImpl example in chapter 17)
Using third-party tools (like Apache iBATIS or Spring JDBC templates) that facilitate JDBC programming.
Delegating the whole database access to tools that map objects to SQL (and vice-versa)
Over the last few years, the third approach has become widely adopted, through the use of ORM (object-relational mapping) tools such as Hibernate and Oracle TopLink.
JPA, as the name states, is just an API; in order to use it, you need an implementation.
Fortunately, most existing tools adhere to the standard, so if you already use an ORM tool (such as Hibernate), you could use it in JPA mode, that is, using the Java Persistence APIs instead of proprietary ones.
First, we show how to test layers that use DAOs in a multilayered application.
Then we explain how to test the JPA-based DAO (data access object) implementation, using DbUnit.2
And although this chapter focuses on JPA and Hibernate, the ideas presented here apply to other ORM tools as well.
By multilayered (or multitiered) application we mean an application whose structure has been divided in layers, with each layer responsible for one aspect of the application.
A typical example is a three-tiered web application comprising of presentation, business, and persistence layers.
Ideally, all of these layers should be tested, both in isolation (through unit tests) and together (through integration tests)
In this section, we show how to unit test the business layer without depending on its lower tier, the persistence layer.
The examples in this chapter test the business and persistence layers (presentation layer testing was covered in chapter 11) of an enterprise application.
The persistence layer comprises a User object and an UserDao interface, similar to those defined in.
Some people may argue that two standards already existed before JPA: EJB entities and JDO.
Well, EJB entities were too complicated, and JDO never took off.
And in this aspect, this chapter is an extension of chapter 17; if you aren’t familiar with DbUnit, we recommend you read that chapter first.
This chapter’s sample application also has a business layer interface (UserFacade, defined in listing 18.2), which in turn deals with DTOs (data transfer objects), not the persistent objects directly.
Therefore, we need a UserDto class (also defined in listing 18.2)
Listing 18.2 Business layer interface (UserFacade) and transfer object (UserDto)
Finally, because the persistence layer will be developed using JPA, a new implementation (UserDaoJpaImpl) is necessary, and its initial version is shown in listing 18.3.3
The sample application is available in two flavors, Maven and Ant.
To run the tests on Maven, type 'mvn clean test'
The application is also available as two Eclipse projects, one with the required libraries (under the lib directory) and another with the project itself.
Because each application layer has different characteristics and dependencies, the layers require different testing strategies.
If your application has been designed to use interfaces and implementations, it’s possible to test each layer in isolation, using mocks or stubs to implement other layers’ interfaces.
The reason is that the JPA implementation is much simpler, just a few lines of nonplumbing code.
Before the test is run, we prepare the fixtures B that will be used in the test methods, the object being tested C, and a mock D.
Then, on the test case we create a User object E and set the mock expectation F to return it when requested (see more about mock expectations in chapter 6)
Notice that newUser() belongs to EntitiesHelper, which provides methods to create new entities (like User) and assert the properties of existing ones (EntitiesHelper was introduced in chapter 17, and new methods are added in this chapter; the full code isn’t listed here but is available for download)
Finally, on G the method being tested is called, and the result is checked on H (which is another method defined in EntitiesHelper)
This test case seems pretty much complete, and it almost is, except that it exercises only the best-case scenario.
But what if the DAO didn’t find the request user? To be complete, the test cases must also exercise negative scenarios.
Let’s try to add a new test case that simulates the DAO method returning null:4
This makes sense, because the Facade method isn’t checking to see if the object return by the DAO is null.
It could be fixed by adding the following lines after the user is retrieved from the DAO:
Once this test case is added and the method fixed, our Facade is complete and fully tested, without needing a DAO implementation.
Such separation of functionalities in interfaces and implementations greatly facilitates the development of multilayered applications, because each layer can be.
How do we know the DAO returns null in this case? And who defined what the Facade should return? What happens in these exceptional cases should be documented in the DAO and Facade methods’ Javadoc, and the test cases should follow that contract.
In our example, we don’t document that on purpose, to show how serious that lack of documentation can be.
Using this approach, the whole business layer could be developed and unit tested without depending on the persistence layer, which would free the business developers from database worries.
It’s still necessary to test everything together though, but that could be achieved through integration tests.
A good compromise is to write many small (and fast) unit tests that extensively exercise individual components and then a few (and slower) integration tests that cover the most important scenarios.
Similarly, we could test the persistence layer using mocks for the JPA interfaces.
But this approach isn’t recommended, because mocks only emulate API calls, and that wouldn’t be enough, for a few reasons.
First, the API is part of JPA-based development; it’s still necessary to annotate classes and provide configuration files.
Second, even if the JPA part is correctly configured, there are still third parties involved: the JPA vendor (like Hibernate), the vendor’s driver (such as HibernateDialect implementations) for the database being used, not to the mention the database itself.
Many things could go wrong (like vendor or drivers bugs, the use of table names that are illegal for a given database, transaction issues, and the like) at runtime that wouldn’t be detected by using mocks for testing.
For the persistence layer, it’s important to test real access to the database, as we demonstrate in the next section.
When you use JPA (or any other ORM software) in your application, you’re delegating the task of persisting objects to and from the database to an external tool.
But in the end, the results are the same as if you wrote the persistence code yourself.
Who let the transactions out? In a JPA-based application, it’s paramount to start and commit transactions, and the methods that use an EntityManager have two options: either they handle the transactions themselves, or they rely on the upper layers for this dirty job.
Typically, the latter option is more appropriate, because it gives the caller the option to invoke more than one DAO method in the same transaction.
Looking at our examples, UserDaoJpaImpl follows this approach, because it doesn’t deal with transaction management.
But if you look at its caller, UserFacadeImpl, it doesn’t handle transactions either! So, in our application example, who is responsible for transaction management?
But in a real project, these pieces would be assembled by a container, like a Java EE application server or Spring, and this container would be responsible for wrapping the Facade methods inside a JPA transaction and propagating it to the DAO object.
In our DAO test cases, we play the role of the container and explicitly manage the transactions.
Aspects of JPA testing essence, JPA testing isn’t much different than testing regular database access code, and hence most of the techniques explained in chapter 17 apply here.
A few differences and caveats are worth mentioning though, and we cover them in the next subsections.
Initially, you need to define how your objects will be mapped to the database tables, typically through the use of Java annotations.
Then you use an EntityManager object to send these objects to or from the database: you can create objects, delete them, fetch them using JPA queries, and so on.
Consequently, it’s a good practice to test these two aspects separately.
For each persistent object, you write a few test cases that verify that they’re correctly mapped (there are many caveats on JPA mapping, particularly when dealing with collections)
Then you write separate unit tests for the persistence code itself (such as DAO objects)
We present practical examples for both tests in the next subsections.
A way to improve the access time is to use an in-memory embedded database, but the drawback is that this database might not be totally compatible with the database the application uses.
But when you use JPA, database compatibility isn’t an issue—quite the opposite.
The JPA vendor is responsible for SQL code generation,5 and vendors typically support all but the rarest databases.
It’s perfectly fine to use a fast embedded database (like HSQLDB or its successor, H2) for unit tests.
Better yet, the project should be set in such a way that the embedded database is used by default, but databases could be easily switched.
That would take advantage of the best of both worlds: developers would use the fast mode, whereas official builds (like nightly and release builds) would switch to the production database (guaranteeing the application works in the real scenario)
On the other hand, committing a transaction is not only an expensive operation, but it also makes the database changes permanent.
This is the ideal scenario; some applications might still need to manually issue a few SQL commands because of JPA bugs or performance requirements.
These cases are rare, though, and they could be handled separately in the test cases.
So what is the better approach, to commit or not? Again, there’s no secret ingredient, and each approach has its advantages.
We, in particular, prefer the commit option, because of the following aspects:
If you’re using an embedded database, speed isn’t an issue.
You could even recreate the whole database before each test case.
As we mention in section 18.8.5, test cases can do cleanup on teardown when necessary.
And again, when using an embedded database, the cleanup is a cheap operation.
If you roll back the transaction, the JPA vendor might not send the real SQL to the database (for instance, Hibernate issues the SQL only at commit or if session.flush() is explicitly called)
There might be cases where your test case passes, but when the code is executed in real life, if fails.
For these reasons, the test cases in this chapter manually handle the transaction’s lifecycle.
Okay, we lied: before our first test, we need some more ado! More specifically, we need to define the infrastructure classes that our real test case classes will extend (remember section 17.3’s best practice: define a superclass for your database tests)
Notice that this initialization doesn’t contain any information about the JPA provider (Hibernate) or database (HSQLDB) used in the test case.
Although when you use JPA you don’t need to deal with low-level JDBC interfaces directly, DbUnit needs a database connection.
You could define the JDBC settings for this connection in a properties file, but that would be redundant; it’s better to extract the JDBC connection from JPA’s entity manager.
This is the way JPA is supposed to be used, and this object creation is cheap.
Because our test cases will manually manage transactions, it’s convenient to create helper methods for this task.
Such a split is a good practice, because if the test cases must change.
As a final note, some frameworks—such as Spring and Unitils (which we cover in the next chapter)—already provide a similar setup.
So even though this infrastructure is based on real projects (it was not created for the book samples), provides flexibility (and can be adapted to your project needs), and is simple enough (just a few classes), you might prefer—or it might be more suitable for your project—to use such frameworks instead.
Now that the infrastructure is set, let’s move on to our tests, starting with the JPA entity mapping tests.
The first step in JPA development is mapping your objects to tables.
Although JPA does a good job of providing default values for most mappings, it’s still necessary to tune it up, typically through the use of Java annotations.
And as you annotate the persistent classes, you want to be sure they’re correctly mapped, so you write test cases that exercise the mapping alone (without worrying about how your final code is going to call the JPA API)
What could be wrong? Unfortunately, despite the fact that JPA is a powerful and helpful tool, many.
If you don’t set the relationship annotations (like @OneToMany) correctly, JPA will create many weird mappings, sometimes even unnecessary tables.
When dependent objects must be persisted automatically, it’s important to verify that the proper cascade options have been set.
In order to avoid these issues, it’s a good practice to write a couple (at least two: one for loading, another for saving) of unit tests for each primary persistent entity in the system.
You don’t need to write tests for all of them, though.
Some entities are too simple or can be indirectly tested (like the Telephone entity in our examples)
Notice that the IDs are dynamically defined using EL expressions, so the test cases succeed regardless of the order in which they’re executed.
Listing 18.10 Entity mapping unit tests for User and Telephone classes.
As explained earlier, we opted for manual transaction control, so the test case explicitly starts and finishes the transaction.
Because we’re testing the entity mappings and not the real persistence layer code such as DAO implementations, we deal with the persistence manager directly.
Once the object is read from the database, we assert that it has the expected values, using our old friend EntitiesHelper F; id and phoneId (which are defined in the superclass and have the initial value of 1) are updated to reflect the objects that were loaded from the database.
If they aren’t updated, the next test case could fail (if it saves objects, which is the case in our example)
The test case for saving an entity is pretty much orthogonal to the load entity test case; nothing new here.
Although these tests run fine and pass, the way the IDs (id and phoneId) are handled is far from elegant and presents many flaws.
For instance, if a test case fails, the IDs aren’t updated, and then other tests running after it will fail as well.
Sure, a try/ finally block would fix this particular issue, but that would make the test even uglier and more verbose.
What should we do to solve these ID issues? Well, don’t throw the book away (yet)—a solution for this problem follows.
In JPA, every persistent entity needs an ID, which is the equivalent of a primary key.
When a new entity is persisted, the JPA engine must set its ID, and how its value is determined depends on the mapping.
Other values are IDENTITY (for databases that support autogenerated primary keys), SEQUENCE (uses database sequences, where supported), and TABLES (uses one or more tables only for the purpose of generating primary keys)
A fifth option would be omitting @GeneratedValue, which means the user (and not the JPA engine) is responsible for setting the IDs.
In most cases, AUTO is the best choice; it’s the default option for the @GeneratedValue annotation.
Back to our test cases, the dataset files contain references to the ID columns (both as primary key in the users table and foreign key on telephones), and because we’re using the same dataset for both load and save, the ID values must be defined dynamically at the time the test is run.
If we used distinct datasets, the IDs wouldn’t matter on the load test (because JPA wouldn’t be persisting entities, only loading them), and for the save tests, we could ignore them.
The problem with this option is that it makes it harder to write and maintain the test cases.
Because our goal is always to facilitate long-term maintenance, we opted for using the same dataset, and hence we need a solution for the ID synchronization problem.
Listing 18.10 tried to solve the problem the simplest way, by letting the test case update the IDs, but that approach had many issues.
A better approach is to integrate the dataset ID’s maintenance with the JPA’s entity ID’s generation, and there are two ways to achieve such integration: taking control of ID generation or being notified of the generated IDs.
Generating IDs for persistent objects isn’t a simple task, because there are many complex aspects to be taken into account, such as concurrent ID generation in different transactions.
Besides, when you use this approach, your test cases won’t be reflecting the real application scenario (and hence they could hide potential problems)
For these reasons, we choose the second approach: being notified of the generated IDs.
Using pure JPA, it’s possible to define a listener for entity lifecycle events (like object creation, update, and deletion)
But this approach doesn’t work well in our case, because these events don’t provide a clear way to obtain the ID of the saved objects.
Hibernate provides its own API for lifecycle events, with listeners for many pre and post events.
Listing 18.12 Changes to ELContextImpl to support the id function.
This defines the value returned by the ID function when no ID was set for a given class.
This initializes the map of static methods that define EL functions.
This is the id function properly speaking, which returns either the initial ID or the value present in the ID’s map.
The ID map must be reset before each method, so load methods always start with id=1 (or whatever value is defined at B)
This is the helper method used by test cases that need to know the base ID for a given class.
But first we need a listener, as defined in listing 18.14
Notice that when you set the listeners for a lifecycle event in the properties, you’re defining all listeners, not just adding new ones.
Instead of binding the id variable to the EL context before and after invoking the test, it resets the ID’s map before each test is run.
Once you’re assured the persistence entities are correctly mapped, it’s time to test the application code that effectively uses JPA, such as DAOs.
The test cases for JPA-based DAOs are similar to the entity mapping tests you saw in the previous section; the main difference (besides the fact that you use DAO code instead of direct JPA calls) is that you have to cover more scenarios, paying attention to some tricky issues.
Let’s start with the simplest cases, the same cases for getUserById() and addUser() we implemented in chapter 17 (where the DAOs were implemented using pure JDBC), plus a test case for testRemoveUser()
Compare this test case with chapter 17’s latest version (listing 17.24) of the equivalent test; they’re almost the same, the only differences being the pretest method prepareDao() (which instantiates a DAO object and sets its EntityManager), the local variable representing the user’s ID (because of the Hibernate ID generation integration we discussed in the previous section), and the transaction management calls.
But now the User object could also have a list of telephones; it’s necessary, then, to add analogous test cases to handle this scenario, as shown in listing 18.19
Note that if the User/Telephone relationship was mandatory (and not optional), the new tests would replace the old ones (instead of being added to the test class)
Listing 18.19 New test cases on UserDaoJpaImplTest to handle user with telephone.
The reason? Cascade deletes are taken into account only when the EntityManager’s remove() method is called, and our DAO used a JPA query to delete the user.
Regardless of this delete issue, the tests presented so far cover only the easy scenarios: saving, loading, and deleting a simple user, with or without a telephone.
It’s a good start but not enough; we need to test negative cases as well.
For the addUser() method, the only negative case is receiving a null reference.
The getUserById() method has at least two negative cases: handling an ID that doesn’t exist in the database and loading from an empty database.
In our sample application, a null reference should be returned in these cases (but another valid option would be to throw an exception)
Negative cases for removeUser() would be the same as for getUserById(), and in our example they don’t need to be tested because nothing happens when the user doesn’t exist (if an exception should be thrown, we should exercise these scenarios)
Now we have a fully tested DAO implementation, right? Well, not really.
Once you put this code into production (or hand it to the QA team for functional testing), a few bugs are bound to happen.
Running the test case after this change fails because of the same exception our poor user faced in the real application.
The solution then is to fix the JPA query to eagerly fetch the telephones, as shown here:
Let’s change the method again, using getResultList() instead of getSingleResult():
Although the change itself is simple, this issue illustrates the importance of negative tests.
Once these two fixes are in place, the application will run fine for awhile, until a user has two or more telephones, which would cause the following exception:
What’s happening now is that a query that was supposed to return one user is returning two—weird!
But again, because the testing infrastructure is already in place, it’s easy to reproduce the problem.
What about the issue itself? The solution is to use the distinct keyword in the query, as shown here:
Listing 18.21 shows the final version of UserDaoJpaImpl, with all issues fixed.
Because we’re just starting development, it’s easy to realize that the violated constraint is the telephones foreign key on the users table.
But imagine on down the road you face a bug report with a similar problem—would you know what constraint FKC50C70C5B99FE3B2 refers to?
By default, Hibernate doesn’t generate useful names for constraints, so you get gibberish like FKC50C70C5B99FE3B2
But as more entities are added to the application, it’s easy for a developer to forget to use this annotation, and such a slip could stay undetected for months, until the application is hit by a foreign key violation bug (whose violated constraint would be a mystery)
As you can imagine, there’s a solution for this problem: writing a test case that verifies that all generated foreign keys have meaningful names.
The skeleton for this test is relatively simple: you ask Hibernate to generate the schema for your application as SQL statements and then check for invalid foreign key names.
The tricky part is how to generate the SQL code in a string, using only the project JPA settings.
We could do further diligence and present a more elegant solution, because this is a book (and hence educational)
But when you’re writing unit tests in real life, many times you have to be pragmatic and use a quickand-dirty solution for a given problem, so we decided to take this approach here as well.
Although we’re testing only foreign key names, we could test other aspects of the generated schema, so we create a generic SchemaTest class.
The test case method by itself is quite simple: it calls the superclass analyzeSchema() method (described shortly), passing as a parameter an inner class that will do the job.
This regular expression is used to identify whether a line defines a foreign key.
This regular expression is used to extract the foreign key name.
The SQL representing the schema generation is parsed in two levels: first, split() is used to break the lines, and then a regular expression D checks for lines that define a foreign key.
We could use only one regular expression to handle both, but the result would be more complex.
Once we find an invalid foreign key, we buffer it and fail later, with the failure message containing all violations.
Although it would be simpler to fail right away, this approach is more helpful, because it requires just one run to spot all invalid names.
The meaning of a valid foreign key is up to the project.
It knows how to create a Java string containing the database schema but doesn’t know what to do with it, so it passes this string to a SqlHandler, which was passed as a parameter.
SchemaExport is the Hibernate class (part of the Hibernate Tools project) used to export the schema.
The problem is, SchemaExport either exports it to the system output or to a file.
Ideally, it should allow the schema to be exported to a Writer or OutputStream object, so we could pass a StringWriter as a parameter to this method.
Because this isn’t the case, we have to use a second hack: replace the System.out when this method is executed and then restore it afterwards.
Yes, we know this is ugly, but we warned you.
Using ORM tools (such as Hibernate and JPA) greatly simplifies the development of database access code in Java applications.
But regardless of how great the technology is or how much it automates the work, it’s still necessary to write test cases for code that uses it.
We also showed how to leverage DbUnit and advanced techniques to effectively test the JPA-based persistence layer, first testing the entity mappings and then the DAOs, properly speaking.
We also demonstrated how to use JPA-generated IDs in your DbUnit datasets.
And although JPA testing is an extension of database testing (which was covered in chapter 17), it has its caveats, such as lazy initialization exceptions, duplicated objects returned in queries, cascade deletes, and generation of weird constraint names.
The examples in this chapter demonstrated how to deal with such issues.
Throughout this final part of the book, we’ve analyzed tools focused on testing specific technologies, such as AJAX applications and database access.
In this final chapter, we evaluate tools that don’t fit a particular niche but rather facilitate overall test development by providing helper methods and plumbing infrastructure.
By using such tools, the developer can focus on the real functionality being tested, which can greatly improve productivity.
Functionally speaking, we analyze tools that automate mock usage, provide a wider number of assertion methods, use reflection to access private members of tested objects, and make DbUnit usage easier.
Such feature overlap might sound redundant (the classic NIH1 syndrome), but this diversity allows you to choose the most appropriate tool for your needs.
Let’s take a brief look at the tools analyzed and how to run this chapter’s examples.
All of these tools are open source projects; some are active and mature, and others have been stalled in development for quite awhile.
Following are descriptions of all tools analyzed in this chapter.
Although it’s a relatively new framework (created at the end of 2006), it’s a mature project and has been designed from the ground up with modern testing concepts in mind.
JUNIT-ADDONS Created in 2002, JUnit-addons is the oldest tool analyzed in this chapter.
As the website (http://sourceforge.net/projects/junit-addons) states, “JUnit-addons is a collection of helper classes for JUnit.” Sounds quite simple, and indeed it is.
Similarly to Unitils, FEST also works with JUnit or TestNG2 and is based on modules.
Although most of the modules provide functionalities already offered by other tools, they do it in different ways, which might sound more natural for developers used to the JMock style of declarations, more specifically, to the fluent interface style, as defined at http://martinfowler.com/bliki/FluentInterface.html.
But regardless of these overlapping features, it offers a module (FEST Swing) that’s quite unique, because it provides support for GUI testing.
It’s the latest offspring of this new breed of general-purpose testing libraries (which also includes Unitils and FEST), and at the time this chapter was written it was still in its infancy.
Although all of its features we analyze in this chapter are provided by other tools, this project also offers unique features, such as a module to test Guice3-based applications; if it fulfills its ambitious goal of providing “Powerful projects for everyday needs!” it could be another valuable asset in the toolbox.
The test cases for this sample application are available in two flavors: Maven and Ant.
To run the tests on Maven, run mvn clean test.
Similarly, to run them using Ant, type ant clean test.
Some of these tools might require esoteric dependencies at runtime (for instance, Unitils database support uses Spring for transaction management), but all such dependencies are commented in the build.xml file.
The application is also available as two Eclipse projects, one with the required libraries and another with the project itself.
Now that all introductions have been made, let’s get down to business.
When you use mocks in your test cases,4 the test method is typically structured as follows:
In this section, we analyze three tools that provide infrastructure for transparent mock usage, and we refactor the existing UserFacadeImplTest (originally defined in listing 18.5) to use each of them.
Before we dig into Unitils mock support, let’s first see how Unitils works so we can configure it properly.
Unitils is configured through standard Java properties (those defined by a pair of strings in the form name=value), and these properties can be defined in three distinct files.
As the name implies, it provides default values for most of the properties, so Unitils could be used out of the box without custom configuration.
This configuration mechanism allows a high degree of flexibility, which can be dangerous.
If tests rely too heavily on the user-specific properties, they might be hard to reproduce.
Ideally, the whole test suite should be runnable using only the projectspecific properties, and the user-specific ones should be used only in some particular cases, like when each user has its own testing database.
That being said, each module has its own properties, and even which modules are available are defined by a property, unitils.modules, which by default includes all modules.
For our mock example (listing 19.1), it isn’t necessary to change any properties, although the mock configuration is quite extensive (two modules are involved, as you can see in the listing)
The contents of this file are also documented online, at http://unitils.org/unitils-default.properties.
The home directory is defined by the Java system property user.home.
Listing 19.1 UserFacadeImpl test refactored to use Unitils mock support.
In order to use Unitils, first the test class must either extend the superclass that provides support for the testing framework being used (like UnitilsJUnit4) or annotate it with the proper JUnit runner; in this example, we opted for the latter C.
The next step is to declare fields representing the object being tested and the mocks; Unitils provides annotations for both, as shown in D.
Notice that the @Mock annotation will create an EasyMock mock; although Unitils mock support is provided through modules, currently only EasyMock is available.
Next comes the test method itself E, whose content is pretty much the same as before; the only differences are that it uses Unitils’ replay() method instead of EasyMock’s (that’s why in B we statically imported any methods explicitly, instead of using *) and it isn’t necessary to call verify() (Unitils will automatically do that after the test is run, although such behavior could also be changed by modifying a property)
Although the core of the class (the test method itself) is the same, this new example requires much less setup.
It might not sound like a big difference in these two simple examples, but in a real project, with dozens or even hundreds of such test cases, such small local improvement results in a big gain in the global productivity.
Behind the scenes, Unitils uses two modules, inject and easymock.
If this property isn’t overridden, Unitils will load all modules, which means more implicit setup methods need to be called before and after each test.
If you don’t need all modules, set this property with just the necessary ones.
If you forget to include a module, Unitils won’t instantiate the attributes that use annotations from that module, and the test case will eventually throw an exception.
Besides configuring which modules are used, you can also change some module behavior through module-specific properties.
It’s also possible to set the mock behavior mode (lenient or strict), even if the order of calls should be taken into account.
These dummies are convenient for cases where your tested objects need a valid reference to another object, but the behavior of that object is irrelevant for the test case where it’s used.
FEST-Mocks doesn’t require the test class B to extend any class or to use any special runner; as a drawback, it’s necessary to manually instantiate the mocks and objects being tested C.
All it does is provide an abstract template class that must be extended on each test case D, which in turn must explicitly set the expectations and run the code to be tested.
Then when the method run() is called E, it executes a workflow similar to that described at the beginning of this section (the main difference is that the verify step isn’t optional, and verify() is always called)
Overall, FEST-Mocks is a bit convoluted, because it explicitly uses the Template Design Pattern, but in a complex way.
Its creators claim that separating the mock’s expectation and code being tested makes the test case clear.
Although we agree that the result is clear to read, it seems less natural and more verbose to develop.
Mycila mock support is similar to Unitils in the way that you mark your mock attributes with annotations.
Unlike Unitils, however, you still need to do some manual setup in a @Before method, such as creating the objects being tested and calling the Mycila initialization method.
Listing 19.3 UserFacadeImpl test refactored to use Mycila EasyMock plug-in.
That method then scans the test class looking for @Mock annotations and does the proper EasyMock setup when they’re found, such as in C.
Mock injection is the only mock support Mycila provides; the test method itself F is responsible for calling replay() and verify() in the mocks.
Mycila also supports other mock frameworks, such as JMock (also analyzed in chapter 6) and Mockito (http://mockito.org)
Listing 19.4 shows the same example using the JMock plug-in.
Listing 19.4 UserFacadeImpl test refactored to use Mycila JMock plug-in.
This new example is similar to the previous one; the test setup E is even exactly the same.
The only differences are the dao reference D being marked with a @Mock annotation defined in another package B, the need for a Mockery object C, and the way expectations are set and verified (F and G respectively)
So, given the mock support offered by these three tools, which one should you use in your project? If you’re looking for transparent EasyMock usage, Unitils is clearly the best option, because it requires less effort in the test cases (no setup or calls to verify) and is highly configurable.
But if you need to use JMock or prefer a clear separation between expectations and tested code, then Mycila or FEST, respectively, is the more suitable option.
In chapter 17 we presented an in-house framework that uses Java annotations to facilitate usage of DbUnit datasets in test cases.
Wouldn’t it be nice if such a framework was offered out of the box? Well, guess what? Unitils’ dbunit module provides exactly that!
Unitils provides four modules related to database testing: database, dbunit, hibernate, and jpa.
The database module is mainly responsible for providing a database connection that will be used by tests and managing transactions, although it offers other features, such as a database maintainer that can be used to synchronize the developer’s databases.
Then the dbunit module scans the test class for annotations that define which DbUnit datasets should be used on each test.
The main differences are where the datasets are located (relative to the class’s package directory in the classpath) and also the fact that the annotations could be defined at class or method levels (class level is useful when many methods use the same dataset; individual methods could then override it by using the annotation again with different values)
Let’s rewrite these two test cases using Unitils, starting with the JDBC version in listing 19.5
Then in D we have tests that load data from the database, so we use the @DataSet annotation to define a dataset that will be used to prepare the database before the test.
Finally, on E we have a test case where data is inserted into the database and DbUnit is used to compare the results; we use @ExpectedDataSet in this case.
The JPA example is pretty much the same; the main difference is the code to set up the EntityManager and the DAO.
Listing 19.6 shows the new test case, focusing on test setup and showing only one test method.
The only differences in this test case are that in B two annotations are used to mark the EntityManager (which will be injected by Unitils before the tests are run), and then in C the EntityManager is passed to our DAO.
The database information is set on B, and C lists the modules used in these tests.
Notice that C could be omitted, because by default Unitils uses all modules.
As shown in these examples, Unitils DbUnit support is similar to the framework developed in chapter 17, although each has its pros and cons.
For instance, Unitils allows you to use merged datasets, whereas the in-house framework provides EL support.
Another major difference is that Unitils automatically manages the transaction.
From the tools analyzed, Unitils is the only one currently supporting DbUnit, although Mycila seems to have plans to provide a DbUnit plug-in in the near future.
Having a powerful toolbox of assertion methods readily available is an important asset in test case development.
When you’re writing a test case, your focus is on the test case logic; assertions are just an accessory.
For example, if you need to assert that a variable is greater than a certain number, your first instinct is to write something like assertTrue(x>42)
Sure, the clarity is improved but at the cost of productivity: you now need to create a long string, which contains the name of the variable (X), the operand (greater), and the current value concatenated, which is not only boring but also error prone.
JUnit provides a handful of assertions out of the box through the org.junit.
Assert class, and although such features cover the basic needs, they come up short in some particular cases, such as comparing collections or properties in a JavaBean (not to mention the example in the previous paragraph)
Fortunately, many thirdparty libraries provide complementary assertions, and in this section we analyze a few of them.
JUnit-addons provides a bunch of XXXAssert classes (such as ListAssert and FileAssert) in the junitx.framework package, and each of them provides static methods aimed to assert specific objects.
Although some of the assertion features they offer are now present in JUnit 4.x,10 some are still surprisingly missing.
For instance, JUnit’s Assert class provides assertTrue(), assertFalse(), and assertEquals() methods, but there isn’t an assertNotEquals() method, which is provided by JUnit-addons’ Assert.
Let’s start with our prologue example, comparing a variable to a number.
For that purpose, JUnit-addons offers the ComparableAssert class, which provides methods to.
For instance, to assert that a variable x is greater than 42, we’d write.
And in the case of failure, the message would be.
The next interesting class is ListAssertions, which, as the name implies, provides methods to verify lists.
Although the message contains a clear description of the list’s content, it’s not easy to realize why they aren’t equal.
The reason is that JUnit’s assertEquals() method treats the list as any other object, delegating its message formatting to Java’s String class.
That’s a better message, although it doesn’t inform us as to where the lists are different, only that one element is missing.
Technically speaking, it provides four methods, because each assert method has an overloaded version that also takes a message.
It’s still a conceptual bug; such behavior would make more sense comparing sets.
You’ll see better options for list comparison, but don’t throw the JAR away yet, because it provides the useful assertContains() method.
That class is powerful, though, because it knows how to compare many types of objects, from simple JavaBeans to collections and Hibernate proxies, using reflection.
More specifically, it uses reflection to compare the value of each field and offers options to harden or relax the comparison (for instance, comparing only fields when a value isn’t null or ignoring dates)
Because the modes parameter uses a vararg, it’s optional, so if you don’t pass a comparator mode, it does a strict comparison in all fields.
Here, and throughout this whole chapter, we’re ignoring the most complete version that also takes a message, because it isn’t relevant to what’s being explained.
Not only did it find the difference, but it printed both brief and detailed messages of what went wrong (and all assertion methods from this class behave this way)
Be aware that the order is important here, because fields are ignored only when their value is null in the expected parameter.
The idea behind this behavior is that in many cases the functionality being tested doesn’t fill every field of an object, and having to compare all of them would require a lot more work.
The message in this case would be slightly different, though, as shown here:
Once you break the learning curve barrier, you get a valuable tool for your day-to-day assertions.
And not only does it supports a great variety of objects, but the assertions are expressed in a different syntax, similar to the Hamcrest syntax (described in chapter 3) but even more natural, because the assertion methods can be chained.
That method is overloaded a dozen times, each with a different type for the actual parameter and returning the proper assertion class for that type.
Assuming a variable x of type int, that assertion would be.
These methods, in turn, also return IntAsserts, allowing many calls to be chained, as follows:
You can make this easier to read if you static import Assertions and split the methods one per line:
All asserts have common methods, like as(String description), which can be used to describe the actual object.
In the previous example, we could describe it as "X":
Overall, FEST-Assert assertions are easy to use and straightforward, especially when using an IDE with autocompletion.
But for comparison purposes, here are some examples of collection assertions, similar to the ones we looked at so far (using other tools)
Unfortunately though, this module was not widely available at the time the book was written, so we had to skip it.
As you can see in this section, custom assertions are provided by many tools.
Whatever your assertion needs are, most likely one (or more) of these tools supports it.
And by using them, you can easily increase the productivity and/or clarity of the failure messages.
The gain might sound small, but every small improvement adds up when you write hundreds or even thousands of test cases.
Ideally, test cases should not know anything about the internal state of the test objects.
But the truth is, even in a perfect world where classes were designed with good encapsulation and testability in mind, sometimes it’s still necessary to bypass encapsulation and access internal members.
When fields of tested objects are private, most likely they will be accessed through reflection later on by some framework class.
So, instead of complaining about privacy concerns, why don’t you play by the same rules and use reflection to access these fields in the test cases?
For instance, the UserFacadeImpl object defined in listing 19.4 has a private reference to a UserDao, and so far this reference could be set only through a public setUserDao() method.
But if that method isn’t available, we could use reflection to access it instead.
In this section we show how to do so, first using an in-house utility class and then analyzing two tools that provide such features for free.
If you eventually need to set a private field in an object being tested, your first attempt might be using the reflection API directly in the test method.
This isn’t a good approach, though, because the API is cumbersome to use, and most likely such a need will arise in other test cases.
In situations like this, it’s better to add a new method to an existing utility class or create a new one if none exists yet.
This particular method requires three parameters: a reference to the object whose field will be set, the name of the field, and its new value.
And because you’ll probably need to get the value of the field at some point, why not add a helper method for that as well? Listing 19.8 shows these two methods.
Listing 19.8 Initial implementation of TestingHelper using the reflection API directly.
Using reflection to bypass encapsulation callers don’t need to worry about checked exceptions.
The dirtiest part is getting the Field reference G; this method has to scan all methods of the object’s class H and its superclass J, and once the method is found, it must be made accessible I.
Going back to our UserFacadeImpl example, once it doesn’t offer a setter for UserDao and these new helper methods are available, we could rewrite the facade.
Anyway, the fact that such access is allowed or not by default is out of the scope of the book.
Even if the JVM had a more strict default behavior, you could still configure the SecurityManager to lower the restrictions in your test case environment.
This might sound like black magic, but what happens is that the compiler knows what type is expected and does the proper casting (because the method returns a parameterized type, <T>)
Bypassing encapsulation, or how I learned to stop worrying and love reflection One of the first things you learn when studying object-oriented languages is that they’re built on three pillars: encapsulation, inheritance, and polymorphism.
And encapsulation best practice dictates that attributes should be defined as private and accessed only through getters and setters.
As a good student and disciplined developer, you follow that practice and happily declare all your object fields as private, using a few hotkeys from your favorite IDE to generate those boring getters and setters.
Then after a hard day at the office, you decide to read a few more pages of this book at home to relax, when you read something disturbing: those tightly encapsulated fields that you protected with so much care can be easily accessed throughout reflection! Your whole world falls apart, and your first instinct is to sell all your Java books and buy Y2K survival kits with the few bucks you get from the sale.
Well, if that happens to you, please go back to the couch and relax again: you can only bypass encapsulation if you grant the permissions to the JVM to do so.
You could argue that the permission is granted by default and hence the fields are wide open, but the truth is that such behavior is convenient most of the times, at least in the Java SE environment.
Using PrivateAccessor directly, we’d rewrite our set DAO statement at setFixtures() as.
A better approach would be to use PrivateAccessor indirectly in the TestingHelper methods, rather than in the test cases themselves, as shown in listing 19.9
PrivateAccessor also provides a few more methods, such as invoke() (to invoke any instance or static method), and overloaded versions of getField() and setField() to deal with static methods.
FEST-Reflect offers helper classes to access all sorts of Java entities, such as fields, methods, constructors, and even inner classes.
For instance, to create an instance of a User object, you use the constructor() method:
Behind the scenes, constructor() returns a TargetType object, whose method in() returns an Invoker, which in turn creates the actual User instance through the newInstance() method.
All these methods use parameterized arguments and return values (similar to the get() method in listing 19.9), so the result can be used without an explicit cast.
The only catch is that most of the time you must pass the expected class as a parameter somewhere in the chain; for instance, to use FEST to set our DAO, it would be necessary to explicitly indicate that the field is of type UserDao, as shown here:
Because of such requirements, TestingHelper couldn’t be rewritten using FEST without changing its signature to include the field class (as shown in listing 19.10), which would also require a change in the set DAO statement:
Given these two alternatives, the choice again depends on personal style: FESTReflect is more powerful and follows a more natural syntax than its JUnit-addons counterpart, at the cost of being more complex (its syntax is less natural for developers who are not used to it) and requiring one more piece of information (the type of the field being accessed)
In this chapter we analyzed a few tools that complement JUnit, provide additional features, or make some tasks more productive.
We learned three different ways to use mocks in our test cases in a more productive way, how to leverage DbUnit usage through third-party annotations, many custom assertions that cover a wide variety of object comparisons, and how to access private fields without dealing with the low-level reflection API directly.
Some of the features analyzed were offered by more than one tool, which might make it hard to decide which one to use.
Although we described the pros and cons of each option, typically the best option depends on the project’s needs and your personal style.
What is most important, though, is to be aware that such tools exist, so you can evaluate them early in the project.
The sooner such tools are used, the more time is saved because of productivity gains.
As you’ve probably seen, the new 4.x version of JUnit is a completely new framework.
It’s more of a totally new project than a bug-fixing improvement of the old one.
But it still deals with unit testing your Java code.
That said, we will try to define all the differences between the latest version of JUnit and the 3.x version.
A.1 Global changes This section discusses the changes in the requirements for using JUnit.
A.2 Changes in the API The changes listed here concern the inner structure of JUnit: all the new features added to the API that we need to know.
The new version of JUnit is built on the idea of backward compatibility.
That’s why they included all the new features in a new package, org.junit.
The old package, junit.framework, is also bundled in the distribution.
If you’re maintaining tests written prior to JUnit 3.8.1, your class needs a String constructor, for example:
This is no longer required with JUnit 3.8.1 and later.
In the new version of JUnit, your test cases no longer need to extend the junit.
Instead, any public class with a zero-argument public constructor can act as a test class.
In the new version of JUnit, test names no longer need to follow the testXXX pattern.
Instead, any method that you want to be considered a test method should be annotated with the @Test annotation.
For instance, the method shown in listing A.2 is a valid test method.
These methods (or fixtures, as they’re called) are executed right before/after each of the tests gets executed.
Their purpose is to execute some common logic before or after each of the tests.
So how can we execute some common logic before/after each of the tests?
You can annotate any of the methods you want to execute before your tests with the @Before annotation.
And you can have as many annotated methods as you want.
In the 3.x version of JUnit, you can have only one setUp() and only one tearDown() method (the method name restricts you)
There is no guarantee, whatsoever, that any of the @Before methods will be executed before/after the other ones.
You need to remember that the @Before annotated methods must be public by signature and return void.
One of the features that people wanted most from JUnit is the ability to execute some common logic before/after a whole set of tests.
For instance, if your setUp() or tearDown() methods consume a lot of resources (like opening a connection to a database or creating some file structure), you’ll find it useful to have a way of executing the setUp() or tearDown() methods not before every test but instead before a whole set of tests.
In listing A.4 we show how to use these annotations in your tests.
Again, the same condition applies as with the @Before/@After annotated methods; no guarantee whatsoever is given for the order of execution.
Sometimes, because of a system reconfiguration or a change of client requirements, you need to skip the execution of a certain test method.
In the 3.x version of JUnit, the only way to do that is to comment the whole test method or rename the method.
This way, if you start commenting your test methods, you get no detailed statistics of how many methods were skipped during test execution.
You can use it to annotate any of your test methods, with the result that this method will be skipped at the time of test execution.
The best part is that at the end of the execution you get detailed statistics of not only how many of your tests were successful or failed but also the number of the skipped tests.
As a result, all of the tests in that class will be skipped.
The question that you’ll ask now is, “How on earth do I get the assert methods in my class?”
The answer to this question is one of the new features of Java 5: static imports.
With Java’s static imports, you can write the code shown in listing A.5
At B we use the static import feature to import one of the assert methods we need, and later on, at C, we call it.
One of the most important aspects of unit testing is the ability to test exceptional situations in your code.
You need to make sure that whenever an exception is thrown, it’s handled properly.
In listing A.6 you can see how exception handling is done in the previous version of JUnit.
Basically, what you do is surround the troublesome code with a try-catch block B and after that deliberately fail the test C if the exception was not thrown.
A good practice is also to place a dummy assert in the catch clause D just to make sure you always pass through the catch clause.
Every time you want to make sure that an exception is thrown, you can use an expected parameter with the @Test annotation.
Listing A.7 shows you how to ensure that an Arithmetic exception is thrown.
As you can see in B, we’ve added the expected attribute to the @Test annotation to denote that this method is supposed to throw an exception of the provided type.
Another parameter you can add to the @Test annotation is the timeout parameter.
With this parameter, you can specify a value in milliseconds that you expect to be the upper limit of the time you spend executing your test.
What happens when the time runs out? An exception will be raised, and the test will marked as failed, telling you that the test couldn’t finish execution in the given Test timeout parameter.
With no GUI test runners included in the distribution, the only way to glimpse the old green bar is to use your favorite IDE; they all have JUnit 4.x support.
The old way of constructing sets of your tests involved writing a suite() method and manually inserting all the tests that you want to be present in the suite.
Because the new version of JUnit is annotation oriented, it seems somehow logical that the construction of suites is also done by means of annotation.
The first annotation lets you define test runners that you can use to run your tests.
The @RunWith annotation accepts a parameter called value, where you need to specify the test runner to run your suite: Suite.class.
This test runner is included with JUnit, along with some other runners.
But in this annotation you can also specify a custom runner.
You can find out how to implement your own JUnit runners in appendix B.
The second annotation declares all the tests that you want to include in the suite.
You list the classes that hold your tests in the value parameter of the @SuiteClasses annotation.
Some people find the way test suites are done in JUnit 4.x unnatural, and indeed, at first sight it is.
But once you start writing tests, you’ll see that there’s nothing unnatural.
As you saw, the @RunWith annotation lets you define a test runner to use.
The Suite test runner that we already presented lets you run your test cases in a suite.
Another test runner that’s bundled with the JUnit distribution is the Parameterized test runner.
This test runner lets you run the same tests with different input test data.
We know that an example is worth several pages of explanation, so listing A.10 shows the example.
Imagine that we have a squareRoot method in our Calculator class that we want to test.
This listing demonstrates how to test this method with different input test data.
We start by declaring the use of the Parameterized test runner at B.
Then, at C, we define the static data() method that we annotate with @Parameters, denoting that this method returns the actual test data for our test.
At D we fill in the test data and return it.
At E we start the test method, and inside it we instantiate.
After that, at F we assert the correctness of the result of the Calculator’s squareRoot() method with all of the test data.
If you run this example, you’ll get a result like what you’d see if you’d just run five distinct assertions like the following:
It also reveals the integration between JUnit and the Hamcrest matcher—something that allows you to write useful match statements to simplify your assertions.
In version 4.4 of JUnit, the Hamcrest assertion mechanism was incorporated.
The Hamcrest assertion mechanism was introduced in JMock and provides a new, robust, fluent API for assertions.
With Hamcrest assertions, you’re able to write more readable and flexible test assertions like these in listing A.11
The Hamcrest matchers that come with JUnit are the first third-party classes included in JUnit.
If you find them insufficient, you can always get the full Hamcrest project from the internet and use any of those matchers with JUnit.
As a developer, you always strive to execute your tests against various scenarios.
But sometimes you have no control over the environment the tests are run in, as in the case when the test depends on the operating system path-separator character (in Windows it is \ and in UNIX it is /)
This may mandate that your test cases run in Windows boxes and not in Linux boxes.
It would be good if you could make an assumption as to what the character separator is and execute the tests only if your assumption is correct.
You can assume that a given condition is set and then assert that the tests pass.
As you can see, we use the assumeThat method to make sure the file separator is UNIX style B and then assert that the database configuration file path is correct C.
What happens if the assumption is wrong, and we’re running the tests on a Windows box? Our tests are marked as passing, regardless of the assertions we make!
The assertXXX methods in JUnit 3.x are thorough enough, but they lack any method to compare equality of arrays.
These methods, given two arrays, start by checking the sizes of the arrays.
If the sizes are the same, they will call the equals() method on each of the elements.
From version 4.6 on of the JUnit framework, you can use a convenient new method for testing the equality of two arrays of doubles:
This compares every element of the first element with the corresponding element of the second array using the given delta.
With the new version of JUnit, that’s no longer true, and in case of wrong assertions you see the java.lang.
As we already discussed in chapter 2, the backbone of JUnit consists of three classes—TestClass, Runner, and Suite—the latter one being a Runner itself.
This means that once we understand how those classes operate, we can write whatever tests we need with JUnit.
If you find JUnit insufficient for your testing needs, you can extend the JUnit API with custom classes.
Since JUnit is open source you can rebuild or extend.
There is no obvious benefit in extending the TestClass class.
On the other hand, the Runner class is especially designed to be easily extensible for our needs.
This appendix gives a brief introduction to how to extend the JUnit API with custom Runner objects.
It also describes how to extend the Hamcrest API with custom matchers so that we eventually customize not only our runners but also our assert calls.
Imagine you’re designing an application for a group of developers.
You want them to plug their code into your application as painlessly as possible, and at the same time you don’t want them to change it.
What can you do? You can provide some points in your application where the developers can intercept the invocation of the program and introduce their own logic.
Others don’t need to change your code; they can simply plug their code into your framework.
B.2 Implementing a custom runner Let’s start implementing our own custom runner.
As you might have already guessed, we want to develop a custom runner that implements the Interceptor pattern we just discussed.
We want to define an Interceptor interface, which will be implemented by the various interceptors that we have.
That’s why we start with the Interceptor interface, shown in listing B.1
Design patterns in action: Interceptor The Interceptor pattern can be described as a method that intercepts a business method invocation.
The first one is an Interceptor interface that defines one or more methods that your interceptors will implement.
The next is a Delegate object that holds a list of interceptors.
The Delegate object is called at the intercepting points of your application, and it invokes the interceptors one after another.
We use this generic interface to define the two methods B in which we define our custom logic to plug into the program execution.
The normal way of implementing the Interceptor pattern would be to call the interceptor methods with some kind of a context object, so that these methods could monitor and gain access to our application.
This can also allow our application to get some feedback from the execution of the interceptor methods.
But for our needs it’s sufficient to implement the interceptor methods with no input parameters.
After release 4.5, tests in JUnit are executed in a block of statements.
That block of statements contains all the features a test might have attached: @Before/@After methods, timeout seconds, ignore features, and the like.
Based on these features, different kinds of actions are performed.
The custom runner invokes the block of statements, which reaches our statement at some point, and then our statement starts executing the interceptors we’ve defined.
Then our statement invokes the wrapped block of statements in order to proceed with the execution in a normal manner.
Listing B.1 Interceptor interface defining the methods for any of our interceptors.
Our statement holds a statement to invoke C and a list of Interceptor implementations D that were added to it J.
In the constructor we initialize the invoker statement E that we’re wrapping.
The evaluate method F implements the entire logic of the statement; it iterates over all the interceptors and invokes their interceptBefore method G.
Next we invoke the evaluate method of the wrapped statement object H and again iterate over all the interceptors, this time to invoke the interceptAfter method I.
This creates a Statement one level closer to the core of the statement block.
And because the core of the statement block is the test itself, we wrap the test in our custom statement.
Our custom JUnit runner is finished! But in order to use it we need some sample implementations of the Interceptor interface.
Here comes the real question: “What kind of events do we want to plug into the execution of the tests?”
One of the simplest implementations that could possibly come to mind is a logging implementation; we log a message before and after execution of every test.
This implementation is simple, but it’s enough for what we currently need.
The code implements the Interceptor interface B (to make it a valid interceptor according to our terms) and gives body to the interceptBefore C and interceptAfter methods D.
And we use our JUnit runner just like we use any other JUnit runner.
The test case we provide starts with the @RunWith annotation.
With this annotation we indicate which JUnit runner we want to execute our tests; in this case we want the InterceptorRunner that we just implemented B.
In this scenario we have only one test method, defined just for testing purposes D.
Listing B.5 Executing a test case using our custom runner.
But let’s move on and make another interceptor, this time more valuable than just logging some text to the screen.
Listing B.6 shows a sample interceptor that’s used for timing purposes.
Again, for this interceptor to be a valid interceptor according to our terms, it needs to implement the Interceptor interface B.
In C we declare a local timer variable of type Timer, which we use to time the execution of the test method.
Implementing a custom runner interceptBefore method starts the timer D, and the interceptAfter method stops it and prints the execution time on the screen E.
You might be wondering why we need these interceptors; they simply execute some common logic before/after the tests (the same way the @Before/@After methods do)
The point is that sometimes it’s hard to determine how much time is required for a test method to execute.
In our case we have the longSetUp() and longTearDown() methods (at D), which also could take a lot of time, thus making it harder to determine  the execution time of the testDummy() method itself.
Upon executing the test case, we should see something like the following in the console, despite the fact that the JUnit runner might show a different value (figure B.1)
B.3 Implementing a custom matcher As you write more and more tests, you’ll see that sometimes it’s difficult to read an assert statement at first glance.
No matter how familiar you are with the code, you’ll always need a few seconds to understand the assert statement.
One way to simplify it is by introducing a new method (like the containsADigit method we added)
Another way is to add Hamcrest matchers; that will greatly simplify the assert statement.
But we might use this assert statement a lot in our tests, and it’s very cumbersome to copy and paste the same long assert everywhere.
Wouldn’t it be great if there was a way to implement a custom Hamcrest matcher, so that the assert statement would be simplified?
Fortunately, there is a way, and that’s exactly what we show next.
We start by implementing a simple matcher that checks whether a given string is null or empty.
Next, we override the matches method C, and we implement our logic on what occasion the matcher will match the conditions; our condition is matched successfully if the string parameter to which it is applied is not null and is different from an empty string.
This means that the method that has to implement our logic can never accept a null object.
It will always accept a parameter of type T, which has already been checked for null value and can never be null.
We want to implement this check, however, so we stick with the BaseMatcher<T> class.
This description is used in our test cases in case the matcher can’t match the condition.
In E we provide two factory methods (denoted by the @Factory annotation)
We call these methods from our test cases to create an instance of our matcher.
We provide two methods, only for readability purposes; sometimes it will be more readable to call one of them and sometimes the other.
Listing B.9 shows the corresponding test class that uses our test methods.
All we need to do is use the static import feature of Java 1.5 and import the matcher that we want to use B.
After that, we implement three test methods using the custom matcher we just implemented.
The first two deliberately test failing conditions, and we denote this with the expected parameter of the @Test annotation C.
We also use the two factory methods D, the second one in conjunction with one of the core matchers of Hamcrest.
Now we demonstrate another example of making a custom matcher, this time by using TypeSafeMatcher<T>
For this case, we implement a custom Hamcrest matcher, which checks to see if a given password is valid.
We start the implementation of our custom matcher by extending the TypeSafeMatcher class B.
In C we override the matchesSafely method and implement our code logic on what occasion the matcher should match the condition.
In our case, the condition is that the password be longer than six characters and contain a digit and a special symbol.
The describeTo method F appends a description for our matcher to show in the log in case the condition does not match.
Next, in G and H we provide two static factory methods that construct an instance of our matcher.
We provide two methods instead of one because sometimes it’s more readable to use one name rather than the other.
Let’s see our matcher in action! It’s time to create a test case and use the matcher.
The results from the execution are shown in figure B.2
Figure B.2 Results from executing the tests, including the custom matcher.
In B we import the matchers that we want to use as static imports.
Notice that we import our matchers the exact same way as we would any of the Hamcrest core matchers.
Then we can use any of them, again as we would any Hamcrest matcher C.
This appendix gives an overview of the book’s source code, where to find it, how to install it, and how to run it.
When we were writing this book, we decided to donate all of the book’s source code to the Apache Software Foundation, because we used many Apache frameworks in the making of this book.
Therefore, we’ve made our source code available as open source on SourceForge at http://junitbook.sourceforge.
We’re also committed to maintaining this source code and fixing it if bugs are found, as a standard open source project.
C.1 Getting the source code There are two possibilities for getting the source code on your local machine:
Use an SVN client and get the source from SVN HEAD.
Each directory represents the source code for a chapter of the book (except the repository/ directory, which contains external JARs required by the chapter projects)
The mapping between chapter names and directory names is listed in table C.1
A project is a way to regroup Java sources, test sources, configuration files, and so on under a single location.
Figure C.1 Directory structure for the source code, shown here in Windows Explorer.
Note that the directory and file icons are decorated by the TortoiseSVN client.
We use various build tools (Ant and Maven) for the different projects, as explained in the chapter matching each project.
It contains the different external libraries (JARs) that all the other projects need in order to compile and run.
As a convenience, we make them readily available to prevent you from having to fish for them all over the Net.
We recommend using these versions when you try the book examples.
Table C.1 Mappings between chapter names and source directory names.
In this appendix, you’ll get to know the two most popular Java IDEs: the Eclipse IDE and the NetBeans IDE.
We cover installation procedures, configuration, and how to run the book’s source code from within the IDEs.
D.1 JUnit integration with Eclipse This section serves as a quick start to get you up and running with Eclipse and with the integrated JUnit.
Installing Eclipse is very simple; the process consists of downloading Eclipse from http://eclipse.org/ and then unzipping it to somewhere on your hard drive.
It’s extremely easy to set up an Eclipse project, because we provide the Eclipse project files with the book’s source code distribution.
Please refer to appendix C, “The source code for the book,” for directory structure organization and project names.
The first Eclipse project to import corresponds to the ch01-jumpstart/ directory.
This project contains the source code for the first chapter of the book.
Point the Project Content to the ch01-jumpstart/ directory on your hard disk.
Repeat the process for all the projects you wish to see in your Eclipse workspace.
If you import all the projects, you should end up with the workspace shown in figure D.1
To run a JUnit test in Eclipse, select the Java perspective ( ), click the test class to execute, click the Run As icon arrow ( ), and select JUnit Test.
Figure D.1 Eclipse workspace when all the book projects have been imported.
Before running Ant scripts, make sure you’ve added the Ivy ivy.jar library to your Ant classpath (it’s needed by our Ant scripts)
To execute a target from an Ant buildfile, first tell Eclipse to display the Ant view by clicking the Window > Show View menu item and selecting Ant.
Then, click the icon to add a buildfile to the Ant view.
For example, add the build.xml file from the ch06-stubs project.
The Ant view now lists all the Ant targets it has found in the build.xml file, highlighting the default target (see figure D.5)
To execute a target, select it and click the button.
Figure D.6 shows the result of executing the compile target.
Note that Eclipse captures the Ant output and displays it in the console view at the bottom right of the figure.
Figure D.5 The Ant view displays all the Ant targets found in build.xml.
For full details on how to run Ant scripts from Eclipse, please see the integrated Eclipse Help; click Help > Help Contents.
JUnitMAX requires a $2 monthly subscription, but it offers quite a few features that you might find interesting.
Let’s take another look at the very first test case that we introduced in this book (listing D.1)
One of the main features of JUnitMAX is that it’s tightly integrated in your development cycle.
For instance, let’s get an Eclipse with JUnitMAX installed and type in the code from listing D.1
After you press Ctrl+S to save your file, Eclipse will compile it, and JUnitMAX will run the test for you, showing you the error we just introduced (figure D.7)
The test errors and failures are shown as compilation problems.
You can also see them on a project level in the package explorer; this way the risk of missing them and committing broken tests is almost zero.
Now it’s time to fix the error and save the document.
You see that the plug-in ran our test again and flagged it with a blue icon to denote that it passed.
Figure D.7 Introducing an error in our test case causes JUnitMAX to execute the tests and report the error as a red marker at the beginning of the line.
JUnitMAX runs your tests during development time, thus saving you the time required to explicitly run your tests through a separate interface—Ant, Maven, or some other graphic runner.
Another neat feature of this JUnit plug-in is the special order in which different tests are executed.
When you execute your tests with the JUnitMAX plug-in, it will take note of this and will run your recently failed tests first.
This way, you get valuable feedback on the status of your tests very early in the running phase.
The plug-in also executes your short tests before longer ones, again giving you valuable information early.
Obviously, we need some time to see why the things are breaking down.
But how do we know what the last stable version of a test was? How do we easily revert to the last version of our test case in which all the tests passed?
The JUnitMAX plug-in, however, offers an easier solution to this problem.
You right-click your test in the package explorer of Eclipse and select the Revert To Last Green item.
D.3 JUnit integration with NetBeans This section serves as a reference guide to how to work with the NetBeans IDE and how to write, execute, and compose JUnit tests from within NetBeans.
First, you need to go to the download section of the NetBeans website (http://www.netbeans.org/downloads/index.html)
You need to select from the upper-right corner of the page the platform on which you’ll install the software as well as the type of installation you want.
In this book we use the 6.5 version of the NetBeans project installed on Windows.
Figure D.8 Our test passes when we fix the error and save the file.
The second step consists of downloading the appropriate installer (for Windows machines it’s an .exe wizard, and for UNIX it’s an .sh script)
The last step is to follow the wizard and actually install the software.
Luckily enough, the NetBeans IDE recognizes the Eclipse configuration files, so you can easily import any Eclipse project into NetBeans and start working.
We used the Eclipse IDE to develop the software that comes with the book, so if you want to import it, all you have to do is follow these steps:
Select the projects that you want to import (figure D.9)
Once you import the projects into your workspace, we can demonstrate how to run some of the tests there.
Before we do that, there are several things to consider.
First, take a closer look at any of the projects.
You’ll see something like the tree shown in figure D.10
These compilation problems are due to the fact that NetBeans, unlike Eclipse, explicitly keeps two separate directories: one for the source files and one for the test-source files.
For each of these folders NetBeans keeps a different set of libraries to include in the classpath.
As you see in the figure, NetBeans detects src\test\java as a source folder, and the source-folder classpath probably doesn’t contain the JUnit JAR file.
To see all the libraries that are included in the classpath, right-click your project, choose Properties, and in the box that appears choose the Libraries tag from the left tree.
As you will see, the Compile Tests tab contains the JUnit JAR, so indeed the compilation problems were caused by the fact that NetBeans recognizes the folder as a source folder and not as a source-test folder.
To solve this problem, again right-click your project, choose Properties, and in the Sources tag remove your src\test\java folder from the Source Package Folders list and add it to the Test Package Folders list.
The final setup should look like the one shown in figure D.11
To execute a JUnit test from within NetBeans, you need to make sure the test resides in the correct folder (the test-source folder)
Unlike Eclipse, NetBeans will never execute a JUnit test if it resides in the source folder (if your class resides in the source folder and you try to execute it, NetBeans will start looking for a main method instead of running the class as a test)
Given that you already made sure all of your tests are located in the test-source folder, you have several options.
No matter what method you use, the result should be similar to the screen shown in figure D.12
Figure D.12 Result from executing a sample JUnit test case.
To run an Ant script from NetBeans, first you need to open the Files perspective.
You can do that by choosing the Window > Files menu.
There should be Run Target item in the context menu.
From there you can choose the targets that you want to execute.
You can also specify which of the targets should be your default target.
If you want to add additional libraries to the Ant installation, it’s good to know that NetBeans uses its own Ant installation.
Figure D.13 shows the output of the execution of an Ant build script in NetBeans.
E.1 Installing HtmlUnit Download HtmlUnit from http://htmlunit.sourceforge.net/ and unzip it to your local drive.
Add all JAR files to the classpath except for the three XML libraries: XML-APIs, Xalan, and Xerces.
Starting with version 1.4.2, Java ships with an XML parser.
Move all XML libraries to a subdirectory of HTmlUnit-2.7/lib called endorsed.
For example, the lib directory should contain the following files:
In addition, you must add the following to your Java VM invocation:
If you’re lucky enough to develop on Java 6, your command line classpath doesn’t need to list each HtmlUnit JAR file; instead, it can contain.
In Eclipse, you must set up a JRE as shown in figure E.1
The two key items in this JRE configuration are as follows:
Default VM Arguments sets the endorsed path to our HtmlUnit lib/endorsed directory.
Eclipse will automatically set up all other entries in the JRE System Libraries list.
Depending on which container you pick, and the version of Cactus and HtmlUnit, you might have to adjust your installation.
The HtmlUnit 2.7 HTML parser depends on the Xerces and Xalan classes, so make sure you also make these JAR files available to your web application or container.
For the details regarding the installation and management of applications and tests within particular containers, we refer you to the Cactus documentation.
E.3 Installing Selenium Download Selenium Remote Control (Selenium RC) from http://seleniumhq.org/ download/, unzip it to your local drive, and add the Selenium Java client driver to your classpath.
So, for Java 5 and earlier, use the following steps.
The Ant script task is an optional task package with the core tasks.
It requires Apache BSF, which you can download from http://jakarta.apache.org/site/downloads/downloads_bsf.cgi.
Next, get the Mozilla Rhino JavaScript (http://www.mozilla.org/rhino/) engine from http://www.mozilla.org/rhino/download.html.
Apache BSF uses Apache Commons Logging, which has been included with Ant since version 1.6
This build is located in the jsunit directory in the chapter 13 source.
Designed for productivity, it has extensions for newer application styles—like Ajax and HTML-based presentation layersand for application frameworks like EJB and OSGi.
JUnit in Action, Second Edition is an entirely revised and up-todate guide to unit testing Java applications.
It provides techniques for solving real-world problems such as using mocks for testing isolation, in-container testing for Java EE and database applications, and test automation.
In an example-driven style, it covers JUnit 4.8 innovations such as the new annotations that simplify test writing, improved exception handling, and the new assertion methods.
Along the way, you’ll learn to integrate JUnit with other important open source frameworks and tools.
Petar Tahchiev is a soft ware engineer with HP and the Jakarta Cactus lead developer.
Gary Gregory is a Java developer with 20+ years of experience who currently develops application servers for legacy integration.
Vincent Massol was the author of the fi rst edition of JUnit in Action.
Part 2 – Different testing strategies Test coverage and development.
Part 3 – JUnit and the build process Running JUnit tests from Ant.
Ajax testing 13.1 Why are Ajax applications difficult to test?
