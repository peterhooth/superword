The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
It’s wondrous, with treasures to satiate desires both subtle and gross; but it’s not for the timid.
When I began writing this book, I spent quite a bit of time searching for a good quote to start things off.
I chose the quote from Alice in Wonderland to capture the flavor of statistical analysis today—an interactive process of exploration, visualization, and interpretation.
The second quote reflects the generally held notion that R is difficult to learn.
What I hope to show you is that is doesn’t have to be.
But there is rhyme and reason to the apparent madness.
With guidelines and instructions, you can navigate the tremendous resources available, selecting the tools you need to accomplish your work with style, elegance, efficiency—and more than a little coolness.
I first encountered R several years ago, when applying for a new statistical consulting position.
The prospective employer asked in the pre-interview material if I was conversant in R.
Following the standard advice of recruiters, I immediately said yes, and set off to learn it.
As I tried to learn the language (as fast as possible, with an interview looming), I found either tomes on the underlying structure of the language or dense treatises on specific advanced statistical methods, written by and for subject-matter experts.
The online help was written in a Spartan style that was more reference than tutorial.
Every time I thought I had a handle on the overall organization and capabilities of R, I found something new that made me feel ignorant and small.
To make sense of it all, I approached R as a data scientist.
I thought about what it takes to successfully process, analyze, and understand data, including.
Then I tried to understand how I could use R to accomplish each of these tasks.
Then, about a year ago, Marjan Bace (the publisher) called and asked if I would like to write a book on R.
The book you’re holding is the one that I wished I had so many years ago.
I have tried to provide you with a guide to R that will allow you to quickly access the power of this great open source endeavor, without all the frustration and angst.
However, learning R has taken my career in directions that I could never have anticipated.
Marjan Bace, Manning publisher, who asked me to write this book in the first place.
Sebastian Stirling, development editor, who spent many hours on the phone with me, helping me organize the material, clarify concepts, and generally make the text more interesting.
He also helped me through the many steps to publication.
Karen Tegtmeyer, review editor, who helped obtain reviewers and coordinate the review process.
Mary Piergies, who helped shepherd this book through the production process, and her team of Liz Welch, Susan Harkins, and Rachel Schroeder.
The peer reviewers who spent hours of their own time carefully reading through the material, finding typos and making valuable substantive suggestions: Chris Williams, Charles Malpas, Angela Staples, PhD, Daniel Reis Pereira, Dr.
The many Manning Early Access Program (MEAP) participants who bought the book before it was finished, asked great questions, pointed out errors, and made helpful suggestions.
Each contributor has made this a better and more comprehensive book.
I would also like to acknowledge the many software authors that have contributed.
They include not only the core developers, but also the selfless individuals who have created and maintain contributed packages, extending R’s capabilities greatly.
Appendix F provides a list of the authors of contributed packages described in this book.
I have tried to represent their contributions accurately, and I remain solely responsible for any errors or distortions inadvertently included in this book.
I really should have started this book by thanking my wife and partner, Carol Lynn.
Although she has no intrinsic interest in statistics or programming, she read each chapter multiple times and made countless corrections and suggestions.
No greater love has any person than to read multivariate statistics for another.
Just as important, she suffered the long nights and weekends that I spent writing this book, with grace, support, and affection.
There is no logical explanation why I should be this lucky.
There are two other people I would like to thank.
One is my father, whose love of science was inspiring and who gave me an appreciation of the value of data.
Gary got me interested in a career in statistics and teaching when I thought I wanted to be a clinician.
If so, then R is for you! R has become the world-wide language for statistics, predictive analytics, and data visualization.
It offers the widest range available of methodologies for understanding data, from the most basic to the most complex and bleeding edge.
As an open source project it’s freely available for a range of platforms, including Windows, Mac OS X, and Linux.
Additionally, R is supported by a large and diverse community of data scientists and programmers who gladly offer their help and advice to users.
Although R is probably best known for its ability to create beautiful and sophisticated graphs, it can handle just about any statistical problem.
The base installation provides hundreds of data-management, statistical, and graphical functions out of the box.
But some of its most powerful features come from the thousands of extensions (packages) provided by contributing authors.
It can be hard for new users to get a handle on what R is and what it can do.
Even the most experienced R user is surprised to learn about features they were unaware of.
It will introduce you to the most important functions in the base installation and more than 90 of the most useful contributed packages.
Throughout the book, the goal is practical application—how you can make sense of your data and communicate that understanding to others.
You’ll be able to apply a variety of techniques for visualizing data, and you’ll have the skills to tackle both basic and advanced data analytic problems.
No background in statistical programming or the R language is assumed.
Although the book is accessible to novices, there should be enough new and practical material to satisfy even experienced R mavens.
But I have tried to write each chapter in such a way that both beginning and expert data analysts will find something interesting and useful.
This book is designed to give you a guided tour of the R platform, with a focus on those methods most immediately applicable for manipulating, visualizing, and understanding data.
Chapter 1 begins with an introduction to R and the features that make it so useful as a data-analysis platform.
The chapter covers how to obtain the program and how to enhance the basic installation with extensions that are available online.
The remainder of the chapter is spent exploring the user interface and learning how to run programs interactively and in batches.
Chapter 2 covers the many methods available for getting data into R.
The first half of the chapter introduces the data structures R uses to hold data, and how to enter data from the keyboard.
The second half discusses methods for importing data into R from text files, web pages, spreadsheets, statistical packages, and databases.
We review methods of creating graphs, modifying them, and saving them in a variety of formats.
Chapter 4 covers basic data management, including sorting, merging, and subsetting datasets, and transforming, recoding, and deleting variables.
We then discuss how to write your own R functions and how to aggregate data in various ways.
Chapter 6 demonstrates methods for creating common univariate graphs, such as bar plots, pie charts, histograms, density plots, box plots, and dot plots.
Each is useful for understanding the distribution of a single variable.
Chapter 7 starts by showing how to summarize data, including the use of descriptive statistics and cross-tabulations.
We then look at basic methods for understanding relationships between two variables, including correlations, t-tests, chi-square tests, and nonparametric methods.
Chapter 8 introduces regression methods for modeling the relationship between a numeric outcome variable and a set of one or more numeric predictor variables.
Methods for fitting these models, evaluating their appropriateness, and interpreting their meaning are discussed in detail.
Chapter 9 considers the analysis of basic experimental designs through the analysis of variance and its variants.
Here we are usually interested in how treatment combinations or conditions affect a numerical outcome variable.
Methods for assessing the appropriateness of the analyses and visualizing the results are also covered.
Starting with a discussion of hypothesis testing, the chapter focuses on how to determine the sample size necessary to detect a treatment effect of a given size with a given degree of confidence.
This can help you to plan experimental and quasi-experimental studies that are likely to yield useful results.
Chapter 12 presents analytic methods that work well in cases where data are sampled from unknown or mixed distributions, where sample sizes are small, where outliers are a problem, or where devising an appropriate test based on a theoretical distribution is too complex and mathematically intractable.
The chapter starts with a discussion of generalized linear models and then focuses on cases where you’re trying to predict an outcome variable that is either categorical (logistic regression) or a count (Poisson regression)
One of the challenges of multivariate data problems is simplification.
Chapter 14 describes methods of transforming a large number of correlated variables into a smaller set of uncorrelated variables (principal component analysis), as well as methods for uncovering the latent structure underlying a given set of variables (factor analysis)
The many steps involved in an appropriate analysis are covered in detail.
In keeping with our attempt to present practical methods for analyzing data, chapter 15 considers modern approaches to the ubiquitous problem of missing data values.
Several of the best are described here, along with guidance for which ones to use when and which ones to avoid.
Chapter 16 wraps up the discussion of graphics with presentations of some of R’s most advanced and useful approaches to visualizing data.
This includes visual representations of very complex data using lattice graphs, an introduction to the new ggplot2 package, and a review of methods for interacting with graphs in real time.
The afterword points you to many of the best internet sites for learning more about R, joining the R community, getting questions answered, and staying current with this rapidly changing product.
In order to make this book as broadly applicable as possible, I have chosen examples from a range of disciplines, including psychology, sociology, medicine, biology, business, and engineering.
None of these examples require a specialized knowledge of that field.
The datasets used in these examples were selected because they pose interesting questions and because they’re small.
This allows you to focus on the techniques described and quickly understand the processes involved.
The datasets are either provided with the base installation of R or available through add-on packages that are available online.
To get the most out of this book, I recommend that you try the examples as you read them.
Finally, there is a common maxim that states that if you ask two statisticians how to analyze a dataset, you’ll get three answers.
The flip side of this assertion is that each answer will move you closer to an understanding of the data.
I make no claim that a given analysis is the best or only approach to a given problem.
Using the skills taught in this text, I invite you to play with the data and see what you can learn.
For example, path_to_my_ file would be replaced with the actual path to a file on your computer.
Many of the listings in this book capture interactive sessions.
When you see code lines that start with >, don’t type the prompt.
Code annotations are used in place of inline comments (a common convention in Manning books)
Additionally, some annotations appear with numbered bullets like q that refer to explanations appearing later in the text.
To save room or make text more legible, the output from interactive sessions may include additional white space or omit text that is extraneous to the point under discussion.
Purchase of R in Action includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the author and from other users.
This page provides information on how to get on the forum once you’re registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialog between individual readers and between readers and the author can take place.
It isn’t a commitment to any specific amount of participation on the part of the author, whose contribution to the AO forum remains voluntary (and unpaid)
We suggest you try asking the authors some challenging questions, lest his interest stray!
The AO forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
He has more than 20 years of experience providing research and statistical consultation to organizations in health care, financial services, manufacturing, behavioral sciences, government, and academia.
Kabacoff was a professor of psychology at Nova Southeastern University in Florida, where he taught graduate courses in quantitative methods and statistical programming.
For the past two years, he has managed Quick-R, an R tutorial website.
The book includes finely colored illustrations of figures from different regions of Croatia, accompanied by descriptions of the costumes and of everyday life.
Zadar is an old Roman-era town on the northern Dalmatian coast of Croatia.
It’s over 2,000 years old and served for hundreds of years as an important port on the trading route from Constantinople to the West.
Situated on a peninsula framed by small Adriatic islands, the city is picturesque and has become a popular tourist destination with its architectural treasures of Roman ruins, moats, and old stone walls.
The figure on the cover wears blue woolen trousers and a white linen shirt, over which he dons a blue vest and jacket trimmed with the colorful embroidery typical for this region.
Dress codes and lifestyles have changed over the last 200 years, and the diversity by region, so rich at the time, has faded away.
It’s now hard to tell apart the inhabitants of different continents, let alone of different hamlets or towns separated by only a few miles.
Perhaps we have traded cultural diversity for a more varied personal life—certainly for a more varied and fast-paced technological life.
Manning celebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional life of two centuries ago, brought back to life by illustrations from old books and collections like this one.
Welcome to R in Action! R is one of the most popular platforms for data analysis and visualization currently available.
It is free, open-source software, with versions for Windows, Mac OS X, and Linux operating systems.
This book will provide you with the skills needed to master this comprehensive software, and apply it effectively to your own data.
Part I covers the basics of installing the software, learning to navigate the interface, importing data, and massaging it into a useful format for further analysis.
The chapter begins with an overview of R and the features that make it such a powerful platform for modern data analysis.
After briefly describing how to obtain and install the software, the user interface is explored through a series of simple examples.
Next, you’ll learn how to enhance the functionality of the basic installation with extensions (called contributed packages), that can be freely downloaded from online repositories.
The chapter ends with an example that allows you to test your new skills.
Once you’re familiar with the R interface, the next challenge is to get your data into the program.
In today’s information-rich world, data can come from many sources and in many formats.
Chapter 2 covers the wide variety of methods available for importing data into R.
The first half of the chapter introduces the data structures R uses to hold data and describes how to input data manually.
The second half discusses methods for importing data from text files, web pages, spreadsheets, statistical packages, and databases.
From a workflow point of view, it would probably make sense to discuss data management and data cleaning next.
However, many users approach R for the first time out of an interest in its powerful graphics capabilities.
The chapter reviews methods for creating graphs, customizing them, and saving them in a variety of formats.
The chapter describes how to specify the colors, symbols, lines, fonts, axes, titles, labels, and legends used in a graph, and ends with a description of how to combine several graphs into a single plot.
Once you’ve had a chance to try out R’s graphics capabilities, it is time to get back to the business of analyzing data.
Significant time must often be spent combining data from different sources, cleaning messy data (miscoded data, mismatched data, missing data), and creating new variables (combined variables, transformed variables, recoded variables) before the questions of interest can be addressed.
Chapter 4 covers basic data management tasks in R, including sorting, merging, and subsetting datasets, and transforming, recoding, and deleting variables.
It covers the use of numeric (arithmetic, trigonometric, and statistical) and character functions (string subsetting, concatenation, and substitution) in data management.
A comprehensive example is used throughout this section to illustrate many of the functions described.
Next, control structures (looping, conditional execution) are discussed and you will learn how to write your own R functions.
Writing custom functions allows you to extend R’s capabilities by encapsulating many programming steps into a single, flexible function call.
Finally, powerful methods for reorganizing (reshaping) and aggregating data are discussed.
Reshaping and aggregation are often useful in preparing data for further analyses.
After having completed part 1, you will be thoroughly familiar with programming in the R environment.
You will have the skills needed to enter and access data, clean it up, and prepare it for further analyses.
You will also have experience creating, customizing, and saving a variety of graphs.
How we analyze data has changed dramatically in recent years.
With the advent of personal computers and the internet, the sheer volume of data we have available has grown enormously.
Companies have terabytes of data on the consumers they interact with, and governmental, academic, and private research institutions have extensive archival and survey data on every manner of research topic.
Gleaning information (let alone wisdom) from these massive stores of data has become an industry in itself.
At the same time, presenting the information in easily accessible and digestible ways has become increasingly challenging.
The science of data analysis (statistics, psychometrics, econometrics, machine learning) has kept pace with this explosion of data.
Before personal computers and the internet, new statistical methods were developed by academic researchers who published their results as theoretical papers in professional journals.
It could take years for these methods to be adapted by programmers and incorporated into the statistical packages widely available to data analysts.
Statistical researchers publish new and improved methods, along with the code to produce them, on easily accessible websites.
Produce report Figure 1.1 Steps in a typical data analysis.
The advent of personal computers had another effect on the way we analyze data.
When data analysis was carried out on mainframe computers, computer time was precious and difficult to come by.
Analysts would carefully set up a computer run with all the parameters and options thought to be needed.
When the procedure ran, the resulting output could be dozens or hundreds of pages long.
The analyst would sift through this output, extracting useful material and discarding the rest.
Many popular statistical packages were originally developed during this period and still follow this approach to some degree.
With the cheap and easy access afforded by personal computers, modern data analysis has shifted to a different paradigm.
Rather than setting up a complete data analysis at once, the process has become highly interactive, with the output from each stage serving as the input for the next stage.
An example of a typical analysis is shown in figure 1.1
At any point, the cycles may include transforming the data, imputing missing values, adding or deleting variables, and looping back through the whole process again.
The process stops when the analyst believes he or she understands the data intimately and has answered all the relevant questions that can be answered.
The advent of personal computers (and especially the availability of high-resolution monitors) has also had an impact on how results are understood and presented.
A picture really can be worth a thousand words, and human beings are very adept at extracting useful information from visual presentations.
Modern data analysis increasingly relies on graphical presentations to uncover meaning and convey results.
To summarize, today’s data analysts need to be able to access data from a wide range of sources (database management systems, text files, statistical packages, and spreadsheets), merge the pieces of data together, clean and annotate them, analyze them with the latest methods, present the findings in meaningful and graphically.
As you’ll see in the following pages, R is a comprehensive software package that’s ideally suited to accomplish these goals.
It’s an open source solution to data analysis that’s supported by a large and active worldwide research community.
But there are many popular statistical and graphing packages available (such as Microsoft Excel, SAS, IBM SPSS, Stata, and Minitab)
Most commercial statistical software platforms cost thousands, if not tens of thousands of dollars.
Just about any type of data analysis can be done in R.
If you want to visualize complex data, R has the most comprehensive and powerful feature set available.
From its inception it was designed to support the approach outlined in figure 1.1
For example, the results of any analytic step can easily be saved, manipulated, and used as input for additional analyses.
Getting data into a usable form from multiple sources can be a challenging proposition.
It can write data out to these systems as well.
It’s easily extensible and provides a natural language for quickly programming recently published methods.
In fact, new methods become available for download on a weekly basis.
If you’re a SAS user, imagine getting a new SAS PROC every few days.
If you don’t want to learn a new language, a variety of graphic user interfaces (GUIs) are available, offering the power of R through menus and dialogs.
It’s likely to run on any computer you might have (I’ve even come across guides for installing R on an iPhone, which is impressive but probably not a good idea)
You can see an example of R’s graphic capabilities in figure 1.2
This graph, created with a single line of code, describes the relationships between income, education, and prestige for blue-collar, white-collar, and professional jobs.
Additionally, the largest outlier in each scatter plot has been automatically labeled.
For now, trust me that they’re really cool (and that the statisticians reading this are salivating)
Figure 1.2 Relationships between income, education, and prestige for blue-collar (bc), white-collar (wc), and professional jobs (prof)
Source: car package (scatterplotMatrix function ) written by John Fox.
Graphs like this are difficult to create in other statistical programming languages but can be created with a line or two of code in R.
Education and (to lesser extent) prestige are distributed bi-modally, with more scores in the high and low ends than in the middle.
Chapter 8 will have much more to say about this type of graph.
The important point is that R allows you to create elegant, informative, and highly customized graphs in a simple and straightforward fashion.
Creating similar plots in other statistical languages would be difficult, time consuming, or impossible.
Because it can do so much, the documentation and help files available are voluminous.
Additionally, because much of the functionality comes from optional modules created by independent contributors, this documentation can be scattered and difficult to locate.
In fact, getting a handle on all that R can do is a challenge.
The goal of this book is to make access to R quick and easy.
We’ll tour the many features of R, covering enough material to get you started on your data, with pointers on where to go when you need to learn more.
Follow the directions for installing the base product on the platform of your choice.
Later we’ll talk about adding functionality through optional modules called packages (also available from CRAN)
Appendix H describes how to update an existing R installation to a newer  version.
You can enter commands one at a time at the command prompt (>)   or run a set of commands from a source file.
There are a wide variety of data types, including vectors, matrices, data frames (similar to datasets), and lists (collections of objects)
Most functionality is provided through built-in and user-created functions, and all data objects are kept in memory during an interactive session.
Other functions are contained in packages that can be attached to a current session as needed.
However, you won’t find many programs written that way, because it’s not standard syntax, there are some situations in which it won’t work, and R programmers will make fun of you.
Again, doing so is uncommon and isn’t recommended in this book.
Any text appearing after the # is ignored by the R interpreter.
On a Mac, double-click the R icon   in the Applications folder.
For Linux, type R at the command prompt of a terminal window.
Any of these will start the R interface   (see  figure 1.3 for an example)
To get a feel for the interface, let’s work through a simple contrived example.
You’re interested in the distribution of the weights and their relationship to age.
You’ll enter the age and weight data as vectors, using the function c() , which combines its arguments into a vector or list.
Then you’ll get the mean and standard deviation of the weights, along with the correlation between age and weight, and plot the relationship between age and weight so that you can inspect any trend visually.
The q() function , as shown in the following listing, will end the session and allow you to quit.
The relationship can also be seen in the scatter plot in figure 1.4
Not surprisingly, as infants get older, they tend to weigh more.
The scatter plot in figure 1.4 is informative but somewhat utilitarian and unattractive.
In later chapters, you’ll see how to customize graphs to suit your needs.
A sample of the graphs produced is included in figure 1.5
To see a complete list of demonstrations, enter demo() without parameters.
Figure 1.4 Scatter plot of infant weight (kg) by age (mo)
Figure 1.5 A  sample of the graphs created with the demo() function.
The built-in help system provides details, references, and examples of any function contained in a currently installed package.
Help is obtained using the functions listed in table 1.2
Search the help system for instances of the string foo.
RSiteSearch("foo") Search for the string foo in online help manuals and archived mailing lists.
The function help.start() opens a browser window with access to introductory and advanced manuals, FAQs, and reference materials.
The RSiteSearch() function searches for a given topic in online help manuals and archives of the R-Help discussion list and returns the results in a browser window.
The vignettes returned by the vignette() function   are practical introductory articles provided in PDF format.
As you can see, R provides extensive help facilities, and learning to navigate them will definitely aid your programming efforts.
It’s a rare session that I don’t use the ? to look up the features (such as options or return values) of some    function.
The    workspace is your current R working environment and includes any user-defined objects (vectors, matrices, functions, data frames, or lists)
At the end of an R session, you can save an image of the current workspace that’s automatically reloaded the next time R starts.
Doing so allows you to select a previous command, edit it if desired, and resubmit it using the Enter key.
The current working directory is the directory R will read files from and save results to by default.
You can find out what the current working directory is by using the getwd() function.
You can set the current working directory by using the setwd() function.
If you need to input a file that isn’t in the current working directory, use the full pathname in the call.
Always enclose the names of files and directories from the operating system in quote marks.
Some standard commands for managing your workspace are listed in table 1.3
To see these commands in action, take a look at the following listing.
Listing 1.2 An example of commands used to manage the R workspace.
Next, a vector with 20 uniform random variates is created, and summary statistics and a histogram based on this data are generated.
Finally, the command history is saved to the file .Rhistory , the workspace (including vector x) is saved to the file .RData , and the session is ended.
Note the forward slashes in the pathname of the setwd() command.
Even when using R on a Windows platform, use forward slashes in pathnames.
Also note that the setwd() function   won’t create a directory that doesn’t exist.
If necessary, you can use the dir.create() function   to create a directory, and then use setwd() to change to its location.
It’s a good idea to keep your projects in separate directories.
I typically start an R session by issuing the setwd() command   with the appropriate path to a project, followed by the load() command   without options.
This lets me start up where I left off in my last session and keeps the data and settings separate between projects.
Just navigate to the project directory and double-click on the saved image file.
Doing so will start R, load the saved workspace, and set the current working directory to this    location.
By default, launching R starts an interactive session with input from the keyboard and output to the screen.
But you can also process commands from a script file (a file containing R statements) and direct output to a variety of destinations.
The   source("filename") function   submits a script to the current session.
If the filename doesn’t include a path, the file is assumed to be in the current working directory.
By convention, script file names end with an .R extension, but this isn’t required.
By default, if the file already exists, its contents are overwritten.
Include the option append=TRUE to append text to the file rather than overwriting it.
Including the option split=TRUE will send output to both the screen and the output file.
Issuing the command sink() without options will return output to the screen    alone.
Although    sink() redirects text output, it has no effect on graphic output.
To redirect graphic output, use one of the functions listed in table 1.4
In addition, the text output will be appended to the file myoutput , and the graphic output will be saved to the file mygraphs.pdf.
This time, no text or graphic output is saved to files.
In section 1.5 you’ll learn how to run a program   in batch mode.
But some of its most exciting features are available as optional modules that you can download and install.
There are over 2,500 user-contributed modules called packages that you can download from http://cran.r-project.org/web/packages.
They provide a tremendous range of new capabilities, from the analysis of geostatistical data   to protein mass spectra processing to the analysis of psychological tests! You’ll use many of these optional packages in this book.
Figure 1.6 Input with the source() function   and output with the sink() function.
Packages    are collections of R functions, data, and compiled code in a well-defined format.
The directory where packages are stored on your computer is called the library.
The function .libPaths() shows you where your library is located, and the function library() shows you what packages you’ve saved in your library.
They provide a wide range of functions and datasets that are available by default.
Once installed, they have to be loaded into the session in order to be used.
The command search() tells you which packages are loaded and ready to    use.
There    are a number of R functions that let you manipulate packages.
Once you select a site, you’ll be presented with a list of all available packages.
If you know what package you want to install, you can do so directly by providing it as an argument to the function.
For example, the gclus package contains functions for creating enhanced scatter plots.
But like any software, packages are often updated by their authors.
It lists the packages you have, along with their version numbers, dependencies, and other    information.
Installing   a package downloads it from a CRAN mirror site and places it in your library.
To use it in an R session, you need to load the package using the library() command.
For example, to use the packaged gclus issue the command library(gclus)
Of course, you must have installed a package before you can load it.
You’ll only have to load the package once within a given session.
If desired, you can customize your startup environment to automatically load the packages you use most often.
When    you load a package, a new set of functions and datasets becomes available.
Small illustrative datasets are provided along with sample code, allowing you to try out the new functionalities.
The help system contains a description of each function (along with examples), and information on each dataset included.
Using help() with any of these function or dataset names will provide further details.
The same information can be downloaded as a PDF manual from CRAN.
Most    of the time, you’ll be running R interactively, entering commands at the command prompt and seeing the results of each statement as it’s processed.
Occasionally, you may want to run an R program in a repeated, standard, and possibly unattended fashion.
For example, you may need to generate the same report once a month.
You can write your program in R and run it in batch mode.
How you run R in batch mode depends on your operating system.
On Linux or Mac OS X systems, you can use the following command in a terminal window:
By convention, infile is given the extension .R   and outfile is given extension .Rout.
For additional details on how to invoke R, including the use of command-line options, see the “Introduction to R” documentation available from    CRAN (http://cran.r-project.org)
Common mistakes in R programming There are some common mistakes made frequently by both beginning and experienced R programmers.
If your program generates an error, be sure the check for the following:
Using the wrong case —help(), Help(), and HELP() are three different functions (only the first will work)
Forgetting to include the parentheses in a function call —for example, help() rather than help.
Even if there are no options, you still need the ()
Using the \ in a pathname on Windows —R sees the backslash character as an escape character.
Using a function from a package that’s not loaded —The function order.
If you try to use it before loading the package, you’ll get   an    error.
The error messages in R can be cryptic, but if you’re careful to follow these points, you should avoid seeing many of them.
One of the most useful design features of R is that the output of analyses can easily be saved and used as input to additional analyses.
Let’s walk through an example, using one of the datasets that comes pre-installed with R.
First, run a simple linear regression predicting miles per gallon (mpg) from car weight (wt), using the automotive dataset mtcars.
The    results are displayed on the screen and no information is saved.
Next, run the regression, but store the results in an object:
The assignment has created a list object called lmfit that contains extensive information from the analysis (including the predicted values, residuals, regression coefficients, and more)
Although no output has been sent to the screen, the results can be both displayed and manipulated further.
Typing summary(lmfit) displays a summary of the results, and plot(lmfit) produces diagnostic plots.
To predict miles per gallon from car weight in a new set of data, you’d use predict(lmfit, mynewdata)
To see what a function returns, look at the Value section of the online help for that function.
This tells you what’s saved when you assign the results of that function to an    object.
Programmers    frequently ask me if R can handle large data problems.
Typically, they work with massive amounts of data gathered from web research, climatology, or genetics.
Because R holds objects in memory, you’re typically limited by the amount of RAM available.
But there are two issues to consider: the size of the dataset and the statistical methods that will be applied.
The management and analysis of very large datasets is discussed in    appendix G.
We’ll  finish this chapter with an example that ties many of these ideas together.
Open the general help and look at the “Introduction to R” section.
Load the package and read the description of the dataset Arthritis.
Print out the Arthritis dataset   (entering the name of an object will list it)
It basically shows that arthritis patients receiving treatment improved much more than patients receiving a placebo.
The code required is provided in the following listing, with a sample of the results displayed in figure 1.7
As this short exercise demonstrates, you can accomplish a great deal with a small amount of code.
In this chapter, we looked at some of the strengths that make R an attractive option for students, researchers, statisticians, and data analysts trying to understand the meaning of their data.
We walked through the program’s installation and talked about how to enhance R’s capabilities by downloading additional packages.
We explored the basic interface, running programs interactively and in batches, and produced a few sample graphs.
You also learned how to save your work to both text and graphic files.
Because R can be a complex program, we spent some time looking at how to access the extensive help that’s available.
We hope you’re getting a sense of how powerful this freely available software can be.
Now that you have R up and running, it’s time to get your data into the mix.
In the next chapter, we’ll look at the types of data R can handle and how to import them into R from text files, other programs, and database management  systems.
The first step in any data analysis is the creation of a dataset containing the information to be studied, in a format that meets your needs.
The first part of this chapter (sections 2.1–2.2) describes the wealth of structures that R can use for holding data.
In particular, section 2.2 describes vectors, factors, matrices, data frames, and lists.
Familiarizing yourself with these structures (and the notation used to access elements within them) will help you tremendously in understanding how R works.
You might want to take your time working through this section.
The second part of this chapter (section 2.3) covers the many methods available for importing data into R.
These data sources can include text files, spreadsheets, statistical packages, and database management systems.
For example, the data that I work with typically comes from SQL databases.
On occasion, though, I receive data from legacy DOS systems, and from current SAS and SPSS databases.
It’s likely that you’ll only have to use one or two of the methods described in this section, so feel free to choose those that fit your situation.
Once a dataset is created, you’ll typically annotate it, adding descriptive labels for variables and variable codes.
A  dataset is usually a rectangular array of data with rows representing observations and columns representing variables.
Table 2.1 provides an example of a hypothetical patient dataset.
Different traditions have different names for the rows and columns of a dataset.
Statisticians refer to them as observations and variables, database analysts call them records and fields, and those from the data mining/machine learning disciplines call them examples and attributes.
We’ll use the terms observations and variables throughout this book.
You can distinguish between the structure of the dataset (in this case a rectangular array) and the contents or data types included.
In the dataset shown in table 2.1, PatientID is a row or case identifier, AdmDate is a date variable, Age is a continuous variable, Diabetes is a nominal variable, and Status is an ordinal variable.
This diversity of structures provides the R language with a great deal of flexibility in dealing with data.
The data types or modes that R can handle include numeric, character, logical (TRUE/FALSE), complex (imaginary numbers), and raw (bytes)
In R, PatientID, AdmDate, and Age would be numeric variables, whereas Diabetes and Status would be character variables.
Additionally, you’ll need to tell R that PatientID is a case identifier, that AdmDate contains dates, and that Diabetes and Status are nominal.
They differ in terms of the type of data they can hold, how they’re created, their structural complexity, and the notation used to identify and access individual elements.
Let’s look at each structure in turn, starting with vectors.
Some definitions There are several terms that are idiosyncratic to R, and thus confusing to new users.
In R, an object  is anything that can be assigned to a variable.
Objects have a mode (which describes how the object is stored) and a class (which tells generic functions like print how to handle it)
A data frame is a structure in R that holds data and is similar to the datasets found in standard statistical packages (for example, SAS, SPSS, and Stata)
You can have variables of different types (for example, numeric, character) in the same data frame.
Data frames are the main structures you’ll use to store datasets.
Most other terms should be familiar to you and follow the terminology used in statistics and computing in general.
Vectors    are one-dimensional arrays that can hold numeric data, character data, or logical data.
The combine function c() is used to form the vector.
Here, a is numeric vector, b is a character vector, and c is a logical vector.
Note that the data in a vector must only be one type or mode (numeric, character, or logical)
You can refer to elements of a vector using a numeric vector of positions within brackets.
The colon operator used in the last statement is used to generate a sequence of numbers.
A    matrix is a two-dimensional array where each element has the same mode (numeric, character, or logical)
The option byrow indicates whether the matrix should be filled in by row (byrow=TRUE) or by column (byrow=FALSE)
Then you create a 2x2 matrix with labels and fill the matrix by rows w.
Finally, you create a 2x2 matrix and fill the matrix by columns e.
You can identify rows, columns, or elements of a matrix by using subscripts and brackets.
X[i,] refers to the ith row of matrix X, X[,j] refers to jth column, and X[i, j] refers to the ijth element, respectively.
The subscripts i and j can be numeric vectors in order to select multiple rows or columns, as shown in the following listing.
Matrices are two-dimensional and, like vectors, can contain only one data type.
When there are more than two dimensions, you’ll use arrays (section 2.2.3)
When there are multiple modes of data, you’ll use data frames   (section 2.2.4)
Arrays   are similar to matrices but can have more than two dimensions.
They’re created with an array function of the following form:
The following listing gives an example of creating a three-dimensional (2x3x4) array of numbers.
As you can see, arrays   are a natural extension of matrices.
It’s similar to the datasets you’d typically see in SAS, SPSS, and Stata.
Data frames are the most common data structure you’ll deal with in R.
The patient dataset in table 2.1 consists of numeric and character data.
Because there are multiple modes of data, you can’t contain this data in a matrix.
In this case, a data frame would be the structure of choice.
A data frame is created with the data.frame() function :
Names for each column can be provided with the names function.
Each column must have only one mode, but you can put columns of different modes together to form the data frame.
Because data frames are close to what analysts typically think of as datasets, we’ll use the terms columns and variables  interchangeably when discussing data frames.
There are several ways to identify the elements of a data frame.
You can use the subscript notation you used before (for example, with matrices) or you can specify column names.
Using the patientdata data frame created earlier, the following listing demonstrates these approaches.
The $ notation in the third example is new q.
It’s used to indicate a particular variable from a given data frame.
For example, if you want to cross tabulate diabetes type by status, you could use the following code:
It can get tiresome typing patientdata$ at the beginning of every variable name, so shortcuts are available.
You can use either the attach() and detach() or with() functions to simplify your code.
The   attach() function adds the data frame to the R search path.
When a variable name is encountered, data frames in the search path are checked in order to locate the variable.
Using the mtcars data frame from chapter 1 as an example, you could use the following code to obtain summary statistics for automobile mileage (mpg), and plot this variable against engine displacement (disp), and weight (wt):
The detach() function removes the data frame from the search path.
Note that detach() does nothing to the data frame itself.
The statement is optional but is good programming practice and should be included routinely.
I’ll sometimes ignore this sage advice in later chapters in order to keep code fragments simple and short.
The limitations with this approach are evident when more than one object can have the same name.
Here we already have an object named mpg in our environment when the mtcars data frame   is attached.
In such cases, the original object takes precedence, which isn’t what you want.
The attach() and detach() functions are best used when you’re analyzing a single data frame and you’re unlikely to have multiple objects with the same name.
In any case, be vigilant for warnings that say that objects are being masked.
The limitation of the with() function is that assignments will only exist within the function brackets.
It will save the object to the global environment outside of the with() call.
I think that ultimately the choice is a matter of preference and should be based on what you’re trying to achieve and your understanding of the implications.
In    the patient data example, patientID is used to identify individuals in the dataset.
In R, case identifiers can be specified with a rowname option   in the data frame function.
As    you’ve seen, variables can be described as nominal, ordinal, or continuous.
Status (poor, improved, excellent) is a good example of an ordinal variable.
You know that a patient with a poor status isn’t doing as well as a patient with an improved status, but not by how much.
Continuous variables can take on any value within some range, and both order and amount are implied.
Categorical (nominal) and ordered categorical (ordinal) variables in R are called factors.
Factors are crucial in R because they determine how data will be analyzed and presented visually.
Any analyses performed on the vector diabetes will treat the variable as nominal and select the statistical methods appropriate for this level of measurement.
For vectors representing ordinal variables, you add the parameter ordered=TRUE to the factor() function.
Additionally, any analyses performed on this vector will treat the variable as ordinal and select the statistical methods appropriately.
By default, factor levels for character vectors are created in alphabetical order.
This worked for the status factor, because the order “Excellent,” “Improved,” “Poor” made sense.
There would have been a problem if “Poor” had been coded as “Ailing” instead, because the order would be “Ailing,” “Excellent,” “Improved.” A similar problem exists if the desired order was “Poor,” “Improved,” “Excellent.” For ordered factors, the alphabetical default is rarely sufficient.
You can override the default by specifying a levels option.
Be sure that the specified levels match your actual data values.
Any data values not in the list will be set to missing.
The following listing demonstrates how specifying factors and ordered factors impact data analyses.
Then you specify that diabetes is a factor and status is an ordered factor.
The function str(object)   provides information on an object in R (the data frame in this case) w.
It clearly shows that diabetes is a factor and status is an ordered factor, along with how it’s coded internally.
Note that the summary() function   treats the variables differently e.
It provides the minimum, maximum, mean, and quartiles for the continuous variable age, and frequency counts for the categorical variables diabetes and   status.
Lists    are the most complex of the R data types.
Basically, a list is an ordered collection of objects (components)
A list allows you to gather a variety of (possibly unrelated) objects under one name.
For example, a list may contain a combination of vectors, matrices, data frames, and even other lists.
In this example, you create a list with four components: a string, a numeric vector, a matrix, and a character vector.
You can combine any number of objects and save them as a list.
You can also specify elements of the list by indicating a component number or a name within double brackets.
In this example, mylist[[2]] and mylist[["ages"]] both refer to the same four-element numeric vector.
First, they allow you to organize and recall disparate information in a simple way.
It’s up to the analyst to pull out the components that are needed.
You’ll see numerous examples of functions that return lists in later chapters.
A note for programmers Experienced programmers typically find several aspects of the R language unusual.
Here are some features of the language you should be aware of:
But the dollar sign ($)   has a somewhat analogous meaning, identifying the parts of an object.
For example, A$x refers to variable x in data frame A.
Assigning a value   to a nonexistent element of a vector, matrix, array, or list will expand that structure to accommodate the new value.
The vector x   has expanded from three to seven elements through the assignment.
Programmers looking for stylistic guidance may also want to check out Google’s R Style Guide (http://google-styleguide.googlecode.com/svn/trunk/google-r-style .html)
Now   that you have data structures, you need to put some data in them! As a data analyst, you’re typically faced with data that comes to you from a variety of sources and in a variety of formats.
Figure 2.2 Sources of data that can be imported into R.
The definitive guide for importing data in R is the R Data Import/Export manual   available at http://cran.r-project.org/doc/manuals/R-data.pdf.
As you can see in figure 2.2, R can import data from the keyboard, from flat files, from Microsoft Excel and Access, from popular statistical packages, from specialty formats, and from a variety of relational database management systems.
Because you never know where your data will come from, we’ll cover each of them here.
You only need to read about the ones you’re going to be using.
Perhaps   the simplest method of data entry is from the keyboard.
The edit() function in R will invoke a text editor that will allow you to enter your data manually.
Create an empty data frame (or matrix) with the variable names and modes you want to have in the final dataset.
Invoke the text editor on this data object, enter your data, and save the results back to the data object.
In the following example, you’ll create a data frame named mydata with three variables: age (numeric) , gender (character) , and weight (numeric)
You’ll then invoke the text editor, add your data, and save the results.
Assignments like age=numeric(0)   create a variable of a specific mode, but without actual data.
Note that the result of the editing is assigned back to the object itself.
The edit() function   operates on a copy of the object.
If you don’t assign it a destination, all of your edits will be lost!
Figure 2.3 Entering data via the built-in editor on a Windows platform.
The results of invoking the edit() function   on a Windows platform can be seen in figure 2.3
In this figure, I’ve taken the liberty of adding some data.
If you click on a column title, the editor gives you the option of changing the variable name and type (numeric, character)
You can add additional variables by clicking on the titles of unused columns.
When the text editor is closed, the results are saved to the object assigned (mydata in this case)
Invoking mydata <- edit(mydata) again allows you to edit the data you’ve entered and to add new data.
This method of data entry works well for small datasets.
For larger datasets, you’ll probably want to use the methods we’ll describe next: importing data from existing text files, Excel spreadsheets, statistical packages, or database management   systems.
You   can import data from delimited text files using read.table() , a function that reads a file in table format and saves it as a data frame.
Note that the sep parameter allows you to import files that use a symbol other than a comma to delimit the data values.
The default is sep="", which denotes one or more spaces, tabs, new lines, or carriage returns.
This behavior may not always be desirable (for example, a variable containing respondents’ comments)
You can suppress this behavior in a number of ways.
Including the option stringsAs Factors=FALSE will turn this behavior off for all character variables.
Alternatively, you can use the colClasses option   to specify a class (for example, logical, numeric, character, factor) for each column.
The read.table() function   has many additional options for fine-tuning the data import.
For example, the functions file() , gzfile() , bzfile() , xzfile() , unz() , and url()   can be used in place of the filename.
The file() function   allows the user to access files, the clipboard, and C-level standard input.
The gzfile() , bzfile() , xzfile() , and unz() function s let the user read compressed files.
The url() function   lets you access internet files through a complete URL that includes http://, ftp://, or file://
For convenience, complete URLs (surrounded by "" marks) can usually be used directly in place of filenames as well.
The    best way to read an Excel file is to export it to a comma-delimited file from within Excel and import it to R using the method described earlier.
On Windows systems you can also use the RODBC package   to access Excel files.
The first row of the spreadsheet should contain variable/column names.
You can then use the following code to import the data:
Here, myfile.xls is an Excel file, mysheet is the name of the Excel worksheet to read from the workbook, channel is an RODBC connection object returned by odbcConnectExcel() , and mydataframe is the resulting data frame.
Excel 2007 uses an XLSX file format, which is essentially a zipped set of XML files.
The xlsx package can be used to access spreadsheets in this format.
Be sure to download and install it before first use.
The read.xlsx() function   imports a worksheet from an XLSX file into a data frame.
The simplest format is read.xlsx(file, n)   where file is the path to an Excel 2007 workbook and n is the number of the worksheet to be imported.
Increasingly,    data is provided in the form of files encoded in XML.
For example, the XML package written by Duncan Temple Lang   allows users to read, write, and manipulate XML files.
Coverage of XML is beyond the scope of this text.
In   webscraping, the user extracts information embedded in a web page available over the internet and saves it into R structures for further analysis.
One way to accomplish this is to download the web page using the readLines() function   and manipulate it with functions such as grep() and gsub()
For complex web pages, the RCurl and XML packages   can be used to extract the information desired.
Alternatively, you can use the spss.get() function   in the Hmisc package.
First, download and install the Hmisc package   (the foreign package   is already installed by default):
In this code, mydata.sav is the SPSS data file to be imported, use.value.
A    number of functions in R are designed to import SAS datasets, including read.
Unfortunately, if you’re using a recent version of SAS (SAS 9.1 or higher), you’re likely to find that these functions don’t work for you because R hasn’t caught up with changes in SAS file structures.
You can save the SAS dataset   as a comma-delimited text file from within SAS using PROC EXPORT, and read the resulting file into R using the method described in section 2.3.2
Alternatively, a commercial product called Stat Transfer (described in section 2.3.12) does an excellent job of saving SAS datasets (including any existing variable formats) as R data    frames.
The ncdf and ncdf4 packages   provide high-level R interfaces to netCDF data files.
The ncdf package provides support for data files created with Unidata’s netCDF library (version 3 or earlier) and is available for Windows, Mac OS X, and Linux platforms.
In this example, all the data from the variable myvar , contained in the netCDF file mynetCDFfile , is read and saved into an R array called myarray.
Note that both ncdf and ncdf4 packages   have received major recent upgrades and may operate differently than previous versions.
HDF5    (Hierarchical Data Format) is a software technology suite for the management of extremely large and complex data collections.
These files can be read back into R at a later time.
At present, support for the HDF5 format in R is extremely   limited.
Some packages provide access through native database drivers, whereas others offer access via ODBC or JDBC.
Using R to access data stored in external DMBSs can be an efficient way to analyze large datasets (see appendix G), and leverages the power of both SQL and R.
Perhaps   the most popular method of accessing a DBMS in R is through the RODBC package, which allows R to connect to any DBMS that has an ODBC driver.
The first step is to install and configure the appropriate ODBC driver for your platform and database—they’re not part of R.
If the requisite drivers aren’t already installed on your machine, an internet search should provide you with options.
Once the drivers are installed and configured for the database(s) of your choice, install the RODBC package.
The primary functions included with the RODBC package are listed in table 2.2
Write or update (append=TRUE) a data frame to a table in the ODBC database.
The RODBC package   allows two-way communication between R and an ODBC-connected SQL database.
This means that you can not only read data from a connected database into R, but you can use R to alter the contents of the database itself.
Assume that you want to import two tables (Crime and Punishment) from a DBMS into two R data frames called crimedat and pundat , respectively.
You can accomplish this with code similar to the following:
Here, you load the RODBC package and open a connection to the ODBC database through a registered data source name (mydsn) with a security UID (rob) and password (aardvark)
The connection string is passed to sqlFetch, which copies the table Crime into the R data frame crimedat.
You then run the SQL select statement against the table Punishment and save the results to the data frame pundat.
The sqlQuery() function   is very powerful because any valid SQL statement can be inserted.
This flexibility allows you to select specific variables, subset the data, create new variables, and recode and rename existing    variables.
The   DBI package provides a general and consistent client-side interface to DBMS.
Building on this framework, the RJDBC package provides access to DBMS via a JDBC driver.
Be sure to install the necessary JDBC drivers for your platform and database.
These packages provide native database drivers for their respective databases but may not be available on all platforms.
Before    we end our discussion of importing data, it’s worth mentioning a commercial product that can make the task significantly easier.
It’s available for Windows, Mac, and Unix platforms and supports the latest versions of the statistical packages we’ve discussed so far, as well as ODBC-accessed DBMSs such as Oracle, Sybase, Informix,    and   DB/2
Data    analysts typically annotate datasets to make the results easier to interpret.
Typically annotation includes adding descriptive labels to variable names and value labels to the codes used for categorical variables.
One approach is to use the variable label as the variable’s name and then refer to the variable by its position index.
Consider our earlier example, where you have a data frame containing patient data.
The second column, named age, contains the ages at which individuals were first hospitalized.
Clearly this new name is too long to type repeatedly.
Instead, you can refer to this variable as patientdata[2] and the string "Age at hospitalization (in years)" will print wherever age would’ve originally.
Obviously, this isn’t an ideal approach, and you may be better off trying to come up with better names    (for example, admissionAge)
The    factor() function   can be used to create value labels for categorical variables.
Here levels indicate the actual values of the variable, and labels refer to a character vector containing the    desired    labels.
We’ll    end this chapter with a brief summary of useful functions for working with data objects (see table 2.3)
The statement rm(list = ls()) will remove most objects from the working environment.
The functions head()   and tail() are useful for quickly scanning large datasets.
For example, head(patientdata) lists the first six rows of the data frame, whereas tail(patientdata) lists the last six.
We’ll cover functions such as length(), cbind(), and rbind() in the next chapter.
One of the most challenging tasks in data analysis is data preparation.
We’ve made a good start in this chapter by outlining the various structures that R provides for holding data and the many methods available for importing data from both keyboard and external sources.
In particular, we’ll use the definitions of a vector, matrix, data frame, and list again and again in later chapters.
Your ability to specify elements of these structures via the bracket notation will be particularly important in selecting, subsetting, and transforming data.
As you’ve seen, R offers a wealth of functions for accessing external data.
This includes data from flat files, web files, statistical packages, spreadsheets, and databases.
Although the focus of this chapter has been on importing data into R, you can also export data from R into these external formats.
Exporting data is covered in appendix C, and methods of working with large datasets (in the gigabyte to terabyte range) are covered in appendix G.
Once you get your datasets into R, it’s likely that you’ll have to manipulate them into a more conducive format (actually, I find guilt works well)
In chapter 4, we’ll explore ways of creating new variables, transforming and recoding existing variables, merging datasets, and selecting observations.
But before turning to data management tasks, let’s spend some time with R graphics.
Many readers have turned to R out of an interest in its graphing capabilities, and I don’t want to make you wait any longer.
In the next chapter, we’ll jump directly into the creation of graphs.
Our emphasis will be on general methods for managing and customizing graphs that can be applied throughout the remainder of this    book.
On many occasions, I’ve presented clients with carefully crafted statistical results in the form of numbers and text, only to have their eyes glaze over while the chirping of crickets permeated the room.
Yet those same clients had enthusiastic “Ah-ha!” moments when I presented the same information to them in the form of graphs.
Many times I was able to see patterns in data or detect anomalies in data values by looking at graphs—patterns or anomalies that I completely missed when conducting more formal statistical analyses.
Human beings are remarkably adept at discerning relationships from visual representations.
A well-crafted graph can help you make meaningful comparisons among thousands of pieces of information, extracting patterns not easily found through other methods.
This is one reason why advances in the field of statistical graphics have had such a major impact on data analysis.
Data analysts need to look at their data, and this is one area where R shines.
In this chapter, we’ll review general methods for working with graphs.
Then we’ll look at how to modify the features that are found in any graph.
These features include graph titles, axes, labels, colors, lines, symbols, and text annotations.
Our focus will be on generic techniques that apply across graphs.
In later chapters, we’ll focus on specific types of graphs.
Finally, we’ll investigate ways to combine multiple graphs into one overall graph.
In a typical interactive session, you build a graph one statement at a time, adding features, until you have what you want.
The second statement opens a graphics window and generates a scatter plot between automobile weight on the horizontal axis and miles per gallon on the vertical axis.
In R, graphs are typically created in this interactive fashion (see figure 3.1)
You can save your graphs via code or through GUI menus.
To save a graph via code, sandwich the statements that produce the graph between a statement that sets a destination and a statement that closes that destination.
In addition to pdf(), you can use the functions win.metafile(), png(), jpeg(), bmp(), tiff(), xfig(), and postscript() to save graphs in other formats.
Note: The Windows metafile format is only available on Windows platforms.
On a Windows platform, select File > Save As from the graphics window, and choose the format and location desired in the resulting dialog.
On a Mac, choose File > Save As from the menu bar when the Quartz graphics window is highlighted.
On a Unix platform, the graphs must be saved via code.
In appendix A, we’ll consider alternative GUIs for each platform that will give you more options.
Creating a new graph by issuing a high-level plotting command such as plot(), hist() (for histograms), or boxplot() will typically overwrite a previous graph.
How can you create more than one graph and still have access to each? There are several methods.
First, you can open a new graph window before creating a new graph:
Each new graph will appear in the most recently opened window.
On a Windows platform, you must use a two-step process.
After opening the first graph window, choose History > Recording.
Then use the Previous and Next menu items to step through the graphs that are created.
Third and finally, you can use the functions dev.new(), dev.next(), dev.prev(), dev.set(), and dev.off() to have multiple graph windows open at one time and choose which output are sent to which windows.
But you can also use graphical parameters to specify fonts, colors, line styles, axes, reference lines, and annotations.
In this chapter, we’ll start with a simple graph and explore the ways you can modify and enhance it to meet your needs.
Then we’ll look at more complex examples that illustrate additional customization methods.
The focus will be on techniques that you can apply to a wide range of the graphs that you’ll create in R.
The lattice package has its own methods for customizing a graph’s appearance.
In other chapters, we’ll explore each specific type of graph and discuss where and when they’re most useful.
Let’s start with the simple fictitious dataset given in table 3.1
It describes patient response to two drugs at five dosage levels.
Table 3.1 Patient response to two drugs at five dosage levels.
A simple line graph relating dose to response for drug A can be created using.
In this case, plot(x, y, type="b") places x on the horizontal axis and y on the vertical axis, plots the (x, y) data points, and connects them with line segments.
The option type="b" indicates that both points and lines should be plotted.
You can customize many features of a graph (fonts, colors, axes, titles) through options called graphical parameters.
One way is to specify these options through the par() function.
Values set in this manner will be in effect for the rest of the session or until they’re changed.
Specifying par() without parameters produces a list of the current graphical settings.
Adding the no.readonly=TRUE option produces a list of current graphical settings that can be modified.
Continuing our example, let’s say that you’d like to use a solid triangle rather than an open circle as your plotting symbol, and connect points using a dashed line rather than a solid line.
The first statement makes a copy of the current settings.
You then generate the plot and restore the original settings.
A second way to specify graphical parameters is by providing the optionname=value pairs directly to a high-level plotting function.
In this case, the options are only in effect for that specific graph.
Not all high-level plotting functions allow you to specify all possible graphical parameters.
See the help for a specific plotting function (such as ?plot, ?hist, or ?boxplot) to determine which graphical parameters can be set in this way.
The remainder of section 3.3 describes many of the important graphical parameters that you can set.
As you’ve seen, you can use graphical parameters to specify the plotting symbols and lines used in your graphs.
For example, lwd=2 generates a line twice as wide as the default.
The pch= option specifies the symbols to use when plotting points.
Some functions (such as lines and pie) accept a vector of values that are recycled.
For example, if col=c(“red”, “blue”)and three lines are plotted, the first line will be red, the second blue, and the third red.
You can specify colors in R by index, name, hexadecimal, RGB, or HSV.
The function rgb()creates colors based on red-green-blue values, whereas hsv() creates colors based on hue-saturation values.
See the help feature on these functions for more details.
Glynn has created an excellent online chart of R colors, available at http://research.stowers-institute.
You’ll see examples that use color parameters throughout this chapter.
Graphic parameters are also used to specify text size, font, and style.
Font family and style can be controlled with font options (see table 3.5)
Whereas font size and style are easily set, font family is a bit more complicated.
This is because the mapping of serif, sans, and mono are device dependent.
If you’re satisfied with this mapping, you can use parameters like family="serif" to get the results you want.
On Windows, you can create this mapping via the windowsFont() function.
In this case, par(family="A") will specify an Arial Black font.
If graphs will be output in PDF or PostScript format, changing the font family is relatively straightforward.
Finally, you can control the plot dimensions and margin sizes using the parameters listed in table 3.6
Let’s use the options we’ve covered so far to enhance our simple example.
The code in the following listing produces the graphs in figure 3.7
First you enter your data as vectors, then save the current graphical parameter settings (so that you can restore them later)
Additionally, lines will be twice the default width and symbols will be 1.5 times the default size.
Axis text will be set to italic and scaled to 75 percent of the default.
The first plot is then created using filled red circles and dashed lines.
The second plot is created using filled green filled diamonds and a blue border and blue dashed lines.
Note that parameters set with the par() function apply to both graphs, whereas parameters specified in the plot functions only apply to that specific graph.
Looking at figure 3.7 you can see some limitations in your presentation.
The graphs lack titles and the vertical axes are not on the same scale, limiting your ability to compare the two drugs directly.
In the next section, we’ll turn to the customization of text annotations (such as titles and labels) and axes.
For more information on the graphical parameters that are available, take a look at help(par)
Many high-level plotting functions (for example, plot, hist, boxplot) allow you to include axis and text options, as well as graphical parameters.
For example, the following adds a title (main), subtitle (sub), axis labels (xlab, ylab), and axis ranges (xlim, ylim)
Again, not all functions allow you to add these options.
See the help for the function of interest to see what options are accepted.
For finer control and for modularization, you can use the functions described in the remainder of this section to control titles, axes, legends, and text annotations.
NoTe Some high-level plotting functions include default titles and labels.
You can remove them by adding ann=FALSE in the plot() statement or in a separate par() statement.
Figure 3.8 Line plot of dose versus response for drug A with title, subtitle, and modified axes.
Use the title() function to add title and axis labels to a plot.
The format is title(main="main title", sub="sub-title", xlab="x-axis label", ylab="y-axis label")
Graphical parameters (such as text size, font, rotation, and color) can also be specified in the title() function.
For example, the following produces a red title and a blue subtitle, and creates green x and y labels that are 25 percent smaller than the default text size:
Rather than using R’s default axes, you can create custom axes with the axis() function.
When creating a custom axis, you should suppress the axis automatically generated by.
The option axes=FALSE suppresses all axes (including all axis frame lines, unless you add the option frame.plot=TRUE)
The following listing is a somewhat silly and overblown example that demonstrates each of the features we’ve discussed so far.
At this point, we’ve covered everything in listing 3.2 except for the line()and the mtext() statements.
By using the line() statement instead, you can add new graph elements to an existing graph.
You’ll use it again when you plot the response of drug A and drug B on the same graph in section 3.4.4
The mtext() function is used to add text to the margins of the plot.
Notice that each of the graphs you’ve created so far have major tick marks but not minor tick marks.
To create minor tick marks, you’ll need the minor.tick() function in the Hmisc package.
The current length of the major tick mark can be retrieved using par("tck")
For example, the following statement will add one tick mark between each major tick mark on the x-axis and two tick marks between each major tick mark on the y-axis:
The length of the tick marks will be 50 percent as long as the major tick marks.
The abline() function is used to add reference lines to our graph.
Other graphical parameters (such as line type, color, and width) can also be specified in the abline() function.
When more than one set of data or group is incorporated into a graph, a legend can help you to identify what’s being represented by each bar, pie slice, or line.
A legend can be added (not surprisingly) with the legend() function.
You can give an x,y coordinate for the upper-left corner of the legend.
You can use locator(1), in which case you use the mouse to indicate the location of the legend.
You can also use the keywords bottom, bottomleft, left, topleft, top, topright, right, bottomright, or center to place the legend in the graph.
If you use one of these keywords, you can also use inset= to specify an amount to move the legend into the graph (as fraction of plot region)
If the legend labels colored lines, specify col= and a vector of colors.
If the legend labels point symbols, specify pch= and a vector of point symbols.
If the legend labels line width or line style, use lwd= or lty= and a vector of widths or styles.
To create colored boxes for the legend (common in bar, box, or pie charts), use fill= and a vector of colors.
Other common legend options include bty for box type, bg for background color, cex for size, and text.col for text color.
Let’s take a look at an example using our drug data (listing 3.3)
Again, you’ll use a number of the features that we’ve covered up to this point.
Almost all aspects of the graph in figure 3.10 can be modified using the options discussed in this chapter.
Additionally, there are many ways to specify the options desired.
The final annotation to consider is the addition of text to the plot itself.
Text can be added to graphs using the text() and mtext() functions.
Alternatively, the text can be placed interactively via mouse by specifying location as locator(1)
If you specify pos, you can specify offset= in percent of character width.
You can specify line= to indicate the line in the margin starting with 0 (closest to the plot area) and moving out.
Other common options are cex, col, and font (for size, color, and font style, respectively)
The text() function is typically used for labeling points as well as for adding other text annotations.
Specify location as a set of x, y coordinates and specify the text to place as a vector of labels.
The x, y, and label vectors should all be the same length.
An example is given next and the resulting graph is shown in figure 3.11
Here we’ve plotted car mileage versus car weight for the 32 automobile makes provided in the mtcars data frame.
The text() function is used to add the car makes to the right of each data point.
The point labels are shrunk by 40 percent and presented in red.
As a second example, the following code can be used to display font families:
The results, produced on a Windows platform, are shown in figure 3.12
Here the par() function was used to increase the font size to produce a better display.
The resulting plot will differ from platform to platform, because plain, mono, and serif text are mapped to different font families on different systems.
Finally, you can add mathematical symbols and formulas to a graph using TEX-like rules.
You can also try demo(plotmath) to see this in action.
A portion of the results is presented in figure 3.13
The plotmath() function can be used to add mathematical symbols to titles, axis labels, or text annotation in the body or margins of the graph.
You can often gain greater insight into your data by comparing several graphs at one time.
So, we’ll end this chapter by looking at ways to combine more than one graph into a single image.
Figure 3.12 examples of font families on a Windows platform.
At this point, don’t worry about the specific types of graphs being combined; our focus here is on the general methods used to combine them.
The creation and interpretation of each graph type is covered in later chapters.
With the par() function, you can include the graphical parameter  mfrow=c(nrows, ncols) to create a matrix of nrows x ncols plots that are filled in by row.
Alternatively, you can use mfcol=c(nrows, ncols) to fill the matrix by columns.
For example, the following code creates four plots and arranges them into two rows and two columns:
Note that the high-level function hist()includes a default title (use main="" to suppress it, or ann=FALSE to suppress all titles and labels)
The layout() function has the form layout(mat) where mat is a matrix object specifying the location of the multiple plots to combine.
Optionally, you can include widths= and heights= options in the layout()
Absolute widths (in centimeters) are specified with the lcm() function.
Figure 3.16 Graph combining three figures using the layout() function with default widths.
Additionally, the figure in the bottom-right cell is one-fourth the width of the figure in the bottom-left cell:
As you can see, the layout() function gives you easy control over both the number.
Figure 3.17 Graph combining three figures using the layout() function with specified widths.
There are times when you want to arrange or superimpose several figures to create a single meaningful plot.
Doing so requires fine control over the placement of the figures.
In the following listing, two box plots are added to a scatter plot to create a single enhanced graph.
Similarly, I chose 0.65 to pull the right-hand box plot closer to the scatter plot.
Figure 3.18 A scatter plot with two box plots added to the margins.
NoTe The amount of space needed for individual subplots can be device dependent.
If you get “Error in plot.new(): figure margins too large,” try varying the area given for each portion of the overall graph.
You can use fig= graphical parameter to combine several plots into any arrangement within a single graph.
With a little practice, this approach gives you a great deal of flexibility when creating complex visual presentations.
In this chapter, we reviewed methods for creating graphs and saving them in a variety of formats.
The majority of the chapter was concerned with modifying the default graphs produced by R, in order to arrive at more useful or attractive plots.
You learned how to modify a graph’s axes, fonts, symbols, lines, and colors, as well as how to add titles, subtitles, labels, plotted text, legends, and reference lines.
You saw how to specify the size of the graph and margins, and how to combine multiple graphs into a single useful image.
Our focus in this chapter was on general techniques that you can apply to all graphs (with the exception of lattice graphs in chapter 16)
For example, chapter 7 covers methods for graphing a single variable.
In chapter 16, we discuss advanced graphic methods, including lattice graphs (graphs that display the relationship between variables, for each level of other variables) and interactive graphs.
Interactive graphs let you use the mouse to dynamically explore the plotted relationships.
In other chapters, we’ll discuss methods of visualizing data that are particularly useful for the statistical approaches under consideration.
In the previous chapter we discussed a range of methods for inputting or importing data into R.
Unfortunately, in the real world your data is rarely usable in the format in which you first get it.
In the next chapter we look at ways to transform and massage our data into a state that’s more useful and conducive to analysis.
In chapter 2, we covered a variety of methods for importing data into R.
Unfortunately, getting our data in the rectangular arrangement of a matrix or data frame is the first step in preparing it for analysis.
To paraphrase Captain Kirk in the Star Trek episode “A Taste of Armageddon” (and proving my geekiness once and for all): “Data is a messy business—a very, very messy business.” In my own work, as much as 60 percent of the time I spend on data analysis is focused on preparing the data for analysis.
I’ll go out a limb and say that the same is probably true in one form or another for most real-world data analysts.
One  of the topics that I study in my current job is how men and women differ in the ways they lead their organizations.
Do men and women in management positions differ in the degree to which they defer to superiors?
One way to address these questions is to have bosses in multiple countries rate their managers on deferential behavior, using questions like the following:
Each row represents the ratings given to a manager by his or her boss.
In a real-world study, you’d probably use 10–20 such items to improve the reliability and validity of the results.
You can create a data frame containing the data in table 4.1 using the following code.
In order to address the questions of interest, we must first address several data management issues.
We’ll also need to recode values like 99 for age to missing.
There may be hundreds of variables in a dataset, but we may only be interested in a few.
To simplify matters, we’ll want to create a new dataset with only the variables of interest.
Past research suggests that leadership behavior may change as a function of the manager’s age.
To examine this, we may want to recode the current values of age into a new categorical age grouping (for example, young, middle-aged, elder)
We might want to focus on deferential behavior during the recent global financial crisis.
We’ll work through each of these issues in the current chapter, as well as other basic data management tasks such as combining and sorting datasets.
Then in chapter 5 we’ll look at some advanced  topics.
In    a typical research project, you’ll need to create new variables and transform existing ones.
A wide array of operators and functions can be included in the expression portion of the statement.
Ultimately, you want to incorporate new variables into the original data frame.
The following listing provides three separate ways to accomplish this goal.
The one you choose is up to you; the results will be the same.
Personally, I prefer the third method, exemplified by the use of the transform() function.
It simplifies inclusion of as many new variables as desired and saves the results to the data    frame.
Recoding    involves creating new values of a variable conditional on the existing values of the same and/or other variables.
Create a pass/fail variable based on a set of cutoff scores Replace miscoded values with correct values.
To recode data, you can use one or more of R’s logical operators   (see table 4.3)
Let’s say that you want to recode the ages of the managers in our leadership dataset from the continuous variable age to the categorical variable agecat (Young, Middle Aged, Elder)
First, you must recode the value 99 for age to missing with code such as.
Once missing values for age have been specified, you can then use the following code to create the agecat variable:
You include the data frame names in leadership$agecat to ensure that the new variable is saved back to the data frame.
The within() function   is similar to the with() function   (section 2.2.4) , but allows you to modify the data frame.
First, the variable agecat variable is created and set to missing for each row of the data frame.
Remember that agecat is a character variable; you’re likely to want to turn it into an ordered factor, as explained in section 2.2.5
Several packages offer useful recoding functions; in particular, the car package’s recode() function   recodes numeric and character vectors and factors very simply.
Finally, R ships with cut() , which allows you to divide the range of a numeric variable into intervals, returning a    factor.
If    you’re not happy with your variable names, you can change them interactively or programmatically.
Let’s say that you want to change the variables manager to managerID and date to testDate.
Programmatically, the reshape package   has a rename() function   that’s useful for altering the names of variables.
The reshape package   has a powerful set of functions for altering the structure of a dataset.
In   a project of any size, data is likely to be incomplete because of missed questions, faulty equipment, or improperly coded data.
In R, missing values are represented by the symbol NA (not available)
Impossible values (for example, dividing by 0) are represented by the symbol NaN (not a number)
Unlike programs such as SAS, R uses the same missing values symbol for character and numeric data.
The function is.na() allows you to test for the presence of missing values.
This means that you can’t use comparison operators to test for the presence of missing values.
For example, the logical test myvar == NA is never TRUE.
Instead, you have to use missing values functions, like those in this section, to identify the missing values in R data objects.
As   demonstrated in  section 4.3, you can use assignments to recode values to missing.
Any value of age that’s equal to 99 is changed to NA.
Be sure that any missing data is properly coded as missing before analyzing the data or the results will be    meaningless.
Once    you’ve identified the missing values, you need to eliminate them in some way before analyzing your data further.
The reason is that arithmetic expressions and functions that contain missing values yield missing values.
Both y and z will be NA (missing) because the third element of x is missing.
Luckily, most numeric functions have a na.rm=TRUE option   that removes missing.
When using functions with incomplete data, be sure to check how that function.
Functions allow you to transform data with flexibility and ease.
You can remove any observation with missing data by using the na.omit() function.
Let’s apply this to our leadership dataset in the following listing.
Any rows containing missing data are deleted from leadership before the results are saved to newdata.
Deleting all observations with missing data (called listwise deletion ) is one of several methods of handling incomplete datasets.
If there are only a few missing values or they’re concentrated in a small number of observations, listwise deletion can provide a good solution to the missing values problem.
But if missing values are spread throughout the data, or there’s a great deal of missing data in a small number of variables, listwise deletion can exclude a substantial percentage of your data.
Dates    are typically entered into R as character strings and then translated into date variables that are stored numerically.
In our leadership dataset, date is coded as a character variable in mm/dd/yy format.
Once the variable is in date format, you can analyze and plot the dates using the wide range of analytic techniques covered in later chapters.
Sys.Date() returns today’s date and date() returns the current date and time.
The format() function   takes an argument (a date in this case) and applies an output format (in this case, assembled from the symbols in table 4.4)
The important result here is that there are only two more days until the weekend!
Finally, you can also use the function difftime() to calculate a time interval and.
Who knew? Final test: On which day of the week was I born?
Although    less commonly used, you can also convert date variables to character variables.
Date values can be converted to character values using the as.character() function :
To learn more about formatting dates and times, see help(ISOdatetime)
If you need to do complex calculations with dates, the fCalendar package   can also help.
It provides a myriad of functions for dealing with dates, can handle multiple time zones at once, and provides sophisticated calendar manipulations that recognize business days, weekends,    and    holidays.
In   the previous section, we discussed how to convert character data to date values, and vice versa.
Type conversions in R work in a similar fashion to those in other statistical programming languages.
For example, adding a character string to a numeric vector converts all the elements in the vector to character values.
You can use the functions listed in table 4.5 to test for a data type and to convert it to a given type.
When combined with the flow controls (such as if-then) that we’ll discuss in chapter 5, the is.datatype() function   can be a powerful tool, allowing you to handle data in different ways, depending on its type.
Additionally, some R functions require data of a specific type (character or numeric, matrix or data frame) and the as.datatype() will let you transform your data into the format required prior to    analyses.
Sometimes,   viewing a dataset in a sorted order can tell you quite a bit about the data.
For example, which managers are most deferential? To sort a data frame in R, use the order() function.
Prepend the sorting variable with a minus sign to indicate a descending order.
The following examples illustrate sorting with the leadership data frame.
If    your data exist in multiple locations, you’ll need to combine them before moving forward.
This section shows you how to add columns (variables) and rows (observations) to a data frame.
To    merge two data frames (datasets) horizontally, you use the merge() function.
In most cases, two data frames are joined by one or more common key variables (that is an inner join)
Horizontal joins like this are typically used to add variables to a data frame.
This function will horizontally concatenate the objects A and B.
For the function to work properly, each object has to have the same number of rows and be sorted in the same    order.
To   join two data frames (datasets) vertically, use the rbind() function :
The two data frames must have the same variables, but they don’t have to be in the same order.
If dataframeA has variables that dataframeB doesn’t, then before joining them do one of the following:
Vertical concatenation is typically used to add observations to a    data   frame.
These features can be used to select and exclude variables, observations, or both.
The following sections demonstrate several methods for keeping or deleting variables and observations.
It’s   a common practice to create a new dataset from a limited number of variables chosen from a larger dataset.
In chapter 2, you saw that the elements of a data frame are accessed using the notation dataframe[row indices, column indices]
Leaving the row indices blank (,) selects all the rows by default.
Here, variable names (in quotes) have been entered as column indices, thereby selecting the same columns.
This example uses the paste() function   to create the same character vector as in the previous example.
For example, if a variable has several missing values, you may want to drop it prior to further analyses.
In order to understand why this works, you need to break it down:
This works because prepending a column index with a minus sign (-)   excludes that column.
The choice will depend on which is easier to code.
If there are many variables to drop, it may be easier to keep the ones that remain, or   vice versa.
Selecting   or excluding observations (rows) is typically a key aspect of successful data preparation and analysis.
In each of these examples, you provide the row indices and leave the column indices blank (therefore choosing all columns)
Let’s break down this line of code in order to understand it:
The function which() gives the indices of a vector that are TRUE.
In the third example, the attach() function   is used so that you don’t have to prepend the variable names with the data frame names.
Convert the date values read in originally as character values to date values using the format mm/dd/yy.
Because the default for the as.Date() function   is yyyy-mm-dd, you don’t have to supply it here.
Finally, select cases meeting your desired criteria as you did in the previous   example.
The    examples in the previous two sections are important because they help describe the ways in which logical vectors and comparison operators are interpreted within R.
Understanding how these examples work will help you to interpret R code in general.
Now that you’ve done things the hard way, let’s look at a shortcut.
The subset function is probably the easiest way to select variables and observations.
Here, it provides all variables in a data frame between the from variable and the to variable,    inclusive.
Sampling    from larger datasets is a common practice in data mining and machine learning.
For example, you may want to select two random samples, creating a predictive model from one and validating its effectiveness on the other.
The sample() function enables you to take a random sample (with or without replacement) of size n from a dataset.
You could take a random sample of size 3 from the leadership dataset using the statement.
The first argument to the sample() function   is a vector of elements to choose from.
Here, the vector is 1 to the number of observations in the data frame.
The second argument is the number of elements to be selected, and the third argument indicates sampling without replacement.
The sample() function   returns the randomly sampled elements, which are then used to select rows from the data frame.
Until    now, you’ve been using R statements to manipulate data.
But many data analysts come to R well versed in Structured Query Language   (SQL)
It would be a shame to lose all that accumulated knowledge.
Therefore, before we end, let me briefly mention the existence of the sqldf package.
If you’re unfamiliar with SQL, please feel free to skip this section.
In the first example, you selected all the variables (columns) from the data frame mtcars, kept only automobiles (rows) with one carburetor (carb), sorted the automobiles in ascending order by mpg, and saved the results as the data frame newdf.
The option row.names=TRUE carried the row names from the original data frame over to the new one.
In the second example, you printed the mean mpg and disp within each level of gear for automobiles with four or six cylinders (cyl)
Experienced SQL users will find the sqldf package   a useful adjunct to data management in R.
We covered a great deal of ground in this chapter.
We looked at the way R stores missing and date values and explored various ways of handling them.
You learned how to determine the data type of an object and how to convert it to other types.
You used simple formulas to create new variables and recode existing variables.
I showed you how to sort your data and rename your variables.
You learned how to merge your data with other datasets both horizontally (adding variables) and vertically (adding observations)
Finally, we discussed how to keep or drop variables and how to select observations based on a variety of criteria.
In the next chapter, we’ll look at the myriad of arithmetic, character, and statistical functions that R makes available for creating and transforming variables.
After exploring ways of controlling program flow, you’ll see how to write your own functions.
We’ll also explore how you can use these functions to aggregate and summarize your data.
By the end of chapter 5 you’ll have most of the tools necessary to manage complex datasets.
In chapter 4, we reviewed the basic techniques used for managing datasets within R.
In the first part we’ll take a whirlwind tour of R’s many functions for mathematical, statistical, and character manipulation.
To give this section relevance, we begin with a data management problem that can be solved using these functions.
After covering the functions themselves, we’ll look at one possible solution to the data management problem.
Next, we cover how to write your own functions to accomplish data management and analysis tasks.
First, you’ll explore ways of controlling program flow, including looping and conditional statement execution.
Then we’ll investigate the structure of user-written functions and how to invoke them once created.
Then, we’ll look at ways of aggregating and summarizing data, along with methods of reshaping and restructuring datasets.
To  begin our discussion of numerical and character functions, let’s consider a data management problem.
A group of students have taken exams in Math, Science, and English.
You want to combine these scores in order to determine a single performance indicator for each student.
They have widely different means and standard deviations, so averaging them doesn’t make sense.
You must transform the exam scores into comparable units before combining them.
Second, you’ll need a method of determining a student’s percentile rank on this score in order to assign a grade.
Third, there’s a single field for names, complicating the task of sorting students.
You’ll need to break apart their names into first name and last name in order to sort them properly.
Each of these tasks can be accomplished through the judicious use of R’s numerical and character functions.
After working through the functions described in the next section, we’ll consider a possible solution to this data management  challenge.
In   this section, we’ll review functions in R that can be used as the basic building blocks for manipulating data.
They can be divided into numerical (mathematical, statistical, probability) and character functions.
After we review each type, I’ll show you how to apply functions to the columns (variables) and rows (observations) of matrices and data frames (see section 5.2.6)
Table 5.2    lists common mathematical functions along with short examples.
Logarithm of x to the base n For convenience log(x) is the natural logarithm.
Data transformation is one of the primary uses for these functions.
For example, you often transform positively skewed variables such as income to a log scale before further analyses.
Mathematical functions will also be used as components in formulas, in plotting functions (for example, x versus sin(x)) and in formatting numerical values prior to printing.
The examples in table 5.2 apply mathematical functions to scalars (individual numbers)
When these functions are applied to numeric vectors, matrices, or data frames, they operate on each individual value.
Many of these functions have optional parameters that affect the outcome.
Use the help() function   to learn more about each function and its arguments.
Column center (center=TRUE) or standardize (center=TRUE, scale=TRUE) data object x.
To see these functions in action, look at the next listing.
This listing demonstrates two ways to calculate the mean and standard deviation of a vector of numbers.
It’s instructive to view how the corrected sum of squares (css) is calculated in the second approach:
Writing formulas in R has much in common with matrix manipulation languages such as MATLAB (we’ll look more specifically at solving matrix algebra problems in appendix E)
To standardize each column to an arbitrary mean and standard deviation, you can use code similar to the following:
Using the scale() function   on non-numeric columns will produce an error.
To standardize a specific column rather than an entire matrix or data frame, you can use code such as.
We’ll use the scale() function in the solution to the data management challenge in   section 5.3
You    may wonder why probability functions aren’t listed with the statistical functions (it was really bothering you, wasn’t it?)
Although probability functions are statistical by definition, they’re unique enough to deserve their own section.
Probability functions are often used to generate simulated data with known characteristics and to calculate probability values within user-written statistical functions.
To see how these work, let’s look at functions related to the normal distribution.
Examples of the density (dnorm), distribution (pnorm), quantile (qnorm) and random deviate generation (rnorm) functions are given in table 5.5
Plot the standard normal curve on the interval [–3,3] (see below)
What is the area under the standard normal curve to the left of z=1.96?
Each    time you generate pseudo-random deviates, a different seed, and therefore different results, are produced.
To make your results reproducible, you can specify the seed explicitly, using the set.seed() function.
By setting the seed manually, you’re able to reproduce your results.
This ability can be helpful in creating examples you can access at a future time and share with   others.
In    simulation research and Monte Carlo studies, you often want to draw data from multivariate normal distribution with a given mean vector and covariance matrix.
The mvrnorm() function   in the MASS package   makes this easy.
In listing 5.3, you set a random number seed so that you can reproduce the results at a later time q.
For convenience, the results are converted from a matrix to a data frame, and the variables are given names.
Note that because a correlation matrix is also a covariance matrix, you could’ve specified the correlations structure directly.
The probability functions in R allow you to generate simulated data, sampled from distributions with known characteristics.
Statistical methods that rely on simulated data have grown exponentially in recent years, and you’ll see several examples of these in later    chapters.
Although    mathematical and statistical functions operate on numerical data, character functions extract information from textual data, or reformat textual data for printing and reporting.
For example, you may want to concatenate a person’s first name and last name, ensuring that the first letter of each is capitalized.
Or you may want to count the instances of obscenities in open-ended feedback.
Some of the most useful character functions are listed in table 5.6
Note that the functions grep(), sub(), and strsplit() can search for a text string (fixed=TRUE) or a regular expression (fixed=FALSE) (FALSE is the default)
Regular expressions provide a clear and concise syntax for matching a pattern of text.
The expression therefore matches hat, cat, and at, but not bat.
To learn more, see the regular expression entry in   Wikipedia.
The functions in table 5.7 are also quite useful for data management and manipulation, but they don’t fit cleanly into the other categories.
To create an ordered factor, include the option ordered_result = TRUE.
Divides a continuous variable x into n intervals, by selecting n+1 equally spaced rounded values.
Concatenates the objects in … and outputs them to the screen or to a file (if one is declared)
The last example in the table demonstrates the use of escape characters in printing.
When cat concatenates objects for output, it separates each by a space.
That’s why you include the backspace (\b) escape character   before the period.
How you apply the functions you’ve covered so far to numbers, strings, and vectors is intuitive and straightforward, but how do you apply them to matrices and data frames? That’s the subject of the next section.
One   of the interesting features of R functions is that they can be applied to a variety of data objects (scalars, vectors, matrices, arrays, and data frames)
The mean() function   took the average of all 12 elements in the matrix.
Listing 5.5 Applying a function to the rows (columns) of a matrix.
Because FUN can be any R function, including a function that you write yourself (see section 5.4), apply() is a powerful mechanism.
While apply() applies a function over the margins of an array, lapply() and sapply() apply a function over a list.
You’ll see an example of sapply (which is a user-friendly version of lapply) in the next section.
You now have all the tools you need to solve the data challenge in section 5.1, so let’s give   it a   try.
The options(digits=2) limits the number of digits printed after the decimal place and makes the printouts easier to read.
Because the Math, Science, and English tests are reported on different scales (with widely differing means and standard deviations), you need to make them comparable before combining them.
One way to do this is to standardize the variables so that each test is reported in standard deviation units, rather than in their original scales.
You can then get a performance score for each student by calculating the row means using the mean() function   and adding it to the roster using the cbind() function :
The quantile() function   gives you the percentile rank of each student’s performance score.
Using logical operators, you can recode students’ percentile ranks into a new categorical grade variable.
This creates the variable grade in the roster data frame.
You’ll use the strsplit() function   to break student names into first name and last name at the space character.
Applying strsplit() to a vector of strings returns a list:
You can use the sapply() function   to take the first element of each component and put it in a firstname vector, and the second element of each component and put it in a lastname vector.
Because you no longer need the student variable, you’ll drop it (with the –1 in the roster index)
Finally, you can sort the dataset by first and last name using the order() function :
Now it’s time to look at control structures and user-written functions.
In   the normal course of events, the statements in an R program are executed sequentially from the top of the program to the bottom.
But there are times that you’ll want to execute some statements repetitively, while only executing other statements if certain conditions are met.
First you’ll go through the constructs used for conditional execution, followed by the constructs used for looping.
For the syntax examples throughout this section, keep the following in mind:
After we discuss control-flow constructs, you’ll learn how to write your functions.
Looping    constructs repetitively execute a statement or series of statements until a condition isn’t true.
The   for loop executes a statement repetitively until a variable’s value is no longer contained in the sequence seq.
A   while loop executes a statement repetitively until the condition is no longer true.
Make sure that the statements inside the brackets modify the while condition so that sooner or later it’s no longer true—otherwise the loop will never end! In the previous example, the statement.
If you instead added 1 on each loop, R would never stop saying Hello.
This is why while loops can be more dangerous than other looping constructs.
Looping in R can be inefficient and time consuming when you’re processing the rows or columns of large datasets.
Whenever possible, it’s better to use R’s builtin numerical and character functions in conjunction with the apply family   of functions.
In    conditional execution, a statement or statements are only executed if a specified condition is met.
The    if-else control structure executes a statement if a given condition is true.
Optionally, a different statement is executed if the condition is false.
In the first instance, if grade is a character vector, it’s converted into a factor.
In the second instance, one of two statements is executed.
If grade isn’t a factor (note the ! symbol), it’s turned into one.
If it is a factor, then the message is   printed.
The   ifelse construct is a compact and vectorized version of the if-else construct.
Use ifelse when you want to take a binary action or when you want to input and output vectors from the    construct.
It’s easiest to understand how switch works by looking at the example in the following listing.
This is a silly example but shows the main features.
You’ll learn how to use switch in user-written functions in    the    next    section.
One    of R’s greatest strengths is the user’s ability to add functions.
In fact, many of the functions in R are functions of existing functions.
The object returned can be any data type, from scalar to list.
Say you’d like to have a function that calculates the central tendency and spread of data objects.
The function should give you a choice between parametric (mean and standard deviation) and nonparametric (median and median absolute deviation) statistics.
Additionally, the user should have the choice of automatically printing the results, or not.
Unless otherwise specified, the function’s default behavior should be to calculate parametric statistics and not print the results.
To see your function in action, first generate some data (a random sample of size 500 from a normal distribution):
Next, let’s look at a user-written function that uses the switch construct.
This function gives the user a choice regarding the format of today’s date.
Values that are assigned to parameters in the function declaration are taken as defaults.
In the mydate() function , long is the default format for dates if type isn’t specified:
Note that the cat() function   is only executed if the entered type doesn’t match "long" or "short"
It’s usually a good idea to have an expression that catches user-supplied arguments that have been entered incorrectly.
Several functions are available that can help add error trapping and correction to your functions.
You can use the function warning() to generate a warning message, message() to generate a diagnostic message, and stop() to stop execution of the current expression and carry out an error action.
After creating your own functions, you may want to make them available in every session.
Appendix B describes how to customize the R environment so that user-written functions are loaded automatically at startup.
You can accomplish a great deal using the basic techniques provided in this section.
Together, they provide a significant level of detail and breadth of examples.
Now that we’ve covered user-written functions, we’ll end this chapter with a discussion of data aggregation and    reshaping.
When you aggregate data, you replace groups of observations with summary statistics based on those observations.
When you reshape data, you alter the structure (rows and columns) determining how the data is organized.
This section describes a variety of methods for accomplishing these tasks.
In the next two subsections, we’ll use the mtcars data frame that’s included with the base installation of R.
The   transpose (reversing rows and columns) is perhaps the simplest method of reshaping a dataset.
Use the t() function   to transpose a matrix or a data frame.
In the latter case, row names become variable (column) names.
Listing 5.9 uses a subset of the mtcars dataset in order to conserve space on the page.
You’ll see a more flexible way of transposing data when we look at the shape package later in this   section.
It’s    relatively easy to collapse data in R using one or more by variables and a defined function.
As an example, we’ll aggregate the mtcars data by number of cylinders and gears, returning means on each of the numeric variables (see the next listing)
When you’re using the aggregate() function , the by variables must be in a list (even if there’s only one)
The function specified can be any built-in or user-provided function.
This gives the aggregate command a great deal of power.
But when it comes to power, nothing beats the reshape    package.
The   reshape package is a tremendously versatile approach to both restructuring and aggregating datasets.
Because of this versatility, it can be a bit challenging to learn.
We’ll go through the process slowly and use a small dataset so that it’s clear what’s happening.
Basically, you’ll “melt” data so that each row is a unique ID-variable combination.
Then you’ll “cast” the melted data into any shape you desire.
During the cast, you can aggregate the data with any function you wish.
The dataset you’ll be working with is shown in table 5.8
For example, the measured value 5 in the first row is.
When    you melt a dataset, you restructure it into a format where each measured variable is in its own row, along with the ID variables needed to uniquely identify it.
If you melt the data from table 5.8, using the following code.
Now that you have your data in a melted form, you can recast it into any shape, using the cast()    function.
The   cast() function starts with melted data and reshapes it using a formula that you provide and an (optional) function used to aggregate the data.
Because the formulas on the right side (d, e, and f) don’t include a function, the data is reshaped.
In contrast, the examples on the left side (a, b, and c) specify the mean as an aggregating function.
Thus the data are not only reshaped but aggregated as well.
As you can see, the flexibility provided by the melt() and cast() functions   is amazing.
There are many times when you’ll have to reshape or aggregate your data prior to analysis.
For example, you’ll typically need to place your data   in what’s called.
Figure 5.1 Reshaping data with the melt()   and cast() function s.
This chapter reviewed dozens of mathematical, statistical, and probability functions that are useful for manipulating data.
We saw how to apply these functions to a wide range of data objects, including vectors, matrices, and data frames.
We learned to use control-flow constructs for looping and branching to execute some statements repetitively and execute other statements only when certain conditions are met.
You then had a chance to write your own functions and apply them to data.
Finally, we explored ways of collapsing, aggregating, and restructuring your data.
Now that you’ve gathered the tools you need to get your data into shape (no pun intended), we’re ready to bid part 1 goodbye and enter the exciting world of data analysis! In upcoming chapters, we’ll begin to explore the many statistical and graphical methods available for turning    data into   information.
In part 1, we explored the R environment and discussed how to input data from a wide variety of sources, combine and transform it, and prepare it for further analyses.
Once your data has been input and cleaned up, the next step is typically to explore each variable one at a time.
This provides you with information about the distribution of each variable, which is useful in understanding the characteristics of the sample, identifying unexpected or problematic values, and selecting appropriate statistical methods.
Next, a subset of variables is typically studied two at a time.
This step can help you to uncover basic relationships among variables, and is a useful first step in developing more complex models.
Part 2 focuses on graphical and statistical techniques for obtaining basic information about data.
Chapter 6 describes methods for visualizing the distribution of individual variables.
For categorical variables, this includes bar plots, pie charts, and the newer fan plot.
For numeric variables, this includes histograms, density plots, box plots, dot plots, and the less well-known violin plot.
Each type of graph is useful for understanding the distribution of a single variable.
Chapter 7 describes statistical methods for summarizing individual variables and bivariate relationships.
This chapter starts with coverage of descriptive statistics for numerical data based on the dataset as a whole, and on subgroups of interest.
Next, the use of frequency tables and cross-tabulations for summarizing categorical data is described.
The chapter ends with coverage of basic inferential methods for understanding relationships between two variables at a time, including bivariate correlations, chi-square tests, t-tests, and nonparametric methods.
When you have finished part 2, you will be able to use basic graphical and statistical methods available in R to describe your data, explore group differences, and identify significant relationships among variables.
Whenever we analyze data, the first thing that we should do is look at it.
For each variable, what are the most common values? How much variability is present? Are there any unusual observations? R provides a wealth of functions for visualizing data.
In this chapter, we’ll look at graphs that help you understand a single categorical or continuous variable.
In both cases, the variable could be continuous (for example, car mileage as miles per gallon) or categorical (for example, treatment outcome as none, some, or marked)
In later chapters, we’ll explore graphs that display bivariate and multivariate relationships among variables.
In the following sections, we’ll explore the use of bar plots, pie charts, fan charts, histograms, kernel density plots, box plots, violin plots, and dot plots.
Some of these may be familiar to you, whereas others (such as fan plots or violin plots) may be new.
Our goal, as always, is to understand your data better and to communicate this understanding to others.
Bar    plots display the distribution (frequencies) of a categorical variable through vertical or horizontal bars.
In its simplest form, the format of the barplot() function   is.
In the following examples, we’ll plot the outcome of a study investigating a new.
The data are contained in the Arthritis data frame distributed with the vcd package.
Note that the vcd package isn’t needed to create bar plots.
We’re loading it in order to gain access to the Arthritis dataset.
But we’ll need the vcd package when creating spinogram, which are described in section 6.1.5
If   height   is a vector, the values determine the heights of the bars in the plot and a vertical bar plot is produced.
Including the option horiz=TRUE produces a horizontal bar chart instead.
The main option adds a plot title, whereas the xlab and ylab options   add x-axis and y-axis labels, respectively.
In the Arthritis study, the variable Improved records the patient outcomes for individuals receiving a placebo or drug.
You can graph the variable counts using a vertical or horizontal bar plot.
The code is provided in the following listing and the resulting graphs are displayed in figure 6.1
What happens if you have long labels? In section 6.1.4, you’ll see how to tweak labels so that they don’t    overlap.
If    height is a matrix rather than a vector, the resulting graph will be a stacked or grouped bar plot.
If beside=FALSE (the default), then each column of the matrix produces a bar in the plot, with the values in the column giving the heights of stacked “sub-bars.” If beside=TRUE, each column of the matrix represents a group, and the values in each column are juxtaposed rather than stacked.
You can graph the results as either a stacked or a grouped bar plot (see the next listing)
The first barplot function   produces a stacked bar plot, whereas the second produces a grouped bar plot.
We’ve also added the col option   to add color to the bars plotted.
The legend.text parameter provides bar labels for the legend (which are only useful when height is a matrix)
In chapter 3, we covered ways to format and place the legend to maximum benefit.
See if you can rearrange the legend to avoid overlap with the    bars.
You can create bar plots that represent means, medians, standard deviations, and so forth by using the aggregate.
The following listing shows an example, which is displayed in figure 6.3
Listing 6.3 sorts the means from smallest to largest q.
Also note that use of the title() function   w is equivalent to adding the main option in the plot call.
The bars can be connected with straight line segments using the lines() function.
You can also create mean bar plots with superimposed confidence intervals using the barplot2() function   in the gplots package.
There    are several ways to tweak the appearance of a bar plot.
For example, with many bars, bar labels may start to overlap.
Specifying values smaller than 1 will shrink the size of the labels.
Optionally, the names.arg argument   allows you to specify a character vector of names used to label the bars.
You can also use graphical parameters to help text spacing.
An example is given in the following listing with the output displayed in figure 6.4
Figure 6.3 Bar plot of mean illiteracy rates for US regions sorted by rate.
The par() function   allows you to make extensive modifications to the graphs that R produces by default.
Before   finishing our discussion of bar plots, let’s take a look at a specialized version called a spinogram.
In a spinogram, a stacked bar plot is rescaled so that the height of each bar is 1 and the segment heights represent proportions.
Spinograms are created through the spine()   function of the vcd package.
The larger percentage of patients with marked improvement in the Treated condition is quite evident when compared with the Placebo condition.
In addition to bar plots, pie charts are a popular vehicle for displaying the distribution of a categorical variable.
Whereas   pie charts are ubiquitous in the business world, they’re denigrated by most statisticians, including the authors of the R documentation.
They recommend bar or dot plots over pie charts because people are able to judge length more accurately than volume.
Perhaps for this reason, the pie chart options in R are quite limited when compared with other statistical software.
Four examples are given in the next listing; the resulting plots are provided in figure 6.6
First you set up the plot so that four graphs are combined into one q.
Then you input the data that will be used for the first three graphs.
For the second pie chart w, you convert the sample sizes to percentages and add the information to the slice labels.
Be sure to download and install this package before using it for the first time.
If statisticians dislike pie charts, they positively despise 3D pie charts (although they may secretly find them pretty)
This is because the 3D effect adds no additional insight into the data and is considered distracting eye candy.
The fourth pie chart demonstrates how to create a chart from a table e.
In this case, you count the number of states by US region, and append the information to the labels before producing the plot.
Pie charts make it difficult to compare the values of the slices (unless the values are appended to the labels)
In an attempt to improve on this situation, a variation of the pie chart, called a fan plot , has been developed.
In R, it’s implemented through the fan.plot() function   in the plotrix package.
Consider the following code and the resulting graph (figure 6.7):
In a fan plot , the slices are rearranged to overlap each other and the radii have been modified so that each slice is visible.
Here you can see that Germany is the largest slice.
France appears to be half as large as Germany and twice as large as Australia.
Remember that the width of the slice and not the radius is what’s important here.
As you can see, it’s much easier to determine the relative sizes of the slice in a fan plot than in a pie chart.
Now that we’ve covered pie and fan charts, let’s move on to histograms.
Unlike bar plots and pie charts, histograms describe the distribution of a continuous    variable.
Histograms    display the distribution of a continuous variable by dividing up the range of scores into a specified number of bins on the x-axis and displaying the frequency of scores in each bin on the y-axis.
The option freq=FALSE creates a plot based on probability densities rather than frequencies.
The default produces equally spaced breaks when defining the cells of the histogram.
The first histogram q demonstrates the default plot when no options are specified.
In this case, five bins are created, and the default axis labels and titles are printed.
For the second histogram w, you’ve specified 12 bins, a red fill for the bars, and more attractive and informative labels and title.
The third histogram e maintains the colors, bins, labels, and titles as the previous plot, but adds a density curve and rug plot overlay.
The density curve is a kernel density estimate and is described in the next section.
It provides a smoother description of the distribution of scores.
You use the lines() function   to overlay this curve in a blue color and a width that’s twice the default thickness for lines.
Finally, a rug plot is a onedimensional representation of the actual data values.
If there are many tied values, you can jitter the data on the rug plot   using code like the following:
The fourth histogram r is similar to the second but has a superimposed normal curve and a box around the figure.
The code for superimposing the normal curve comes from a suggestion posted to the R-help mailing list by Peter Dalgaard.
In    the previous section, you saw a kernel density plot superimposed on a histogram.
Technically, kernel density estimation is a nonparametric method for estimating the probability density function of a random variable.
Although the mathematics are beyond the scope of this text, in general kernel density plots can be an effective way to view the distribution of a continuous variable.
The format for a density plot (that’s not being superimposed on another graph) is.
Because the plot() function   begins a new graph, use the lines() function   (listing 6.6) when superimposing a density curve on an existing graph.
Two kernel density examples are given in the next listing, and the results are plotted in figure 6.9
In the first plot, you see the minimal graph created with all the defaults in place.
In the second plot, you add a title, color the curve blue, fill the area under the curve with solid red, and add a brown rug.
The polygon() function   draws a polygon whose vertices are given by x and y (provided by the density() function   in this case)
This is a highly underutilized approach, probably due to a general lack of easily accessible software.
Be sure to install the sm package before first use.
The par() function   is used to double the width of the plotted lines (lwd=2) so that they’d be more readable in this book q.
The sm packages   is loaded and the mtcars data frame is attached.
Then a legend is added to the plot via the legend() function.
The locator(1) option indicates that you’ll place the legend interactively by clicking on the graph where you want the legend to appear.
The second option provides a character vector of the labels.
The third option assigns a color from the vector colfill to each level of cyl.f.
As you can see, overlapping kernel density plots can be a powerful way to compare groups of observations on an outcome variable.
Here you can see both the shapes of the distribution of scores for each group and the amount of overlap between groups.
The moral of the story is that my next car will have four cylinders—or a battery.
Box plots are also a wonderful (and more commonly used) graphical approach to visualizing distributions and differences among groups.
Figure 6.10 Kernel density plots of mpg by number of cylinders.
By default, each whisker extends to the most extreme data point, which is no more than the 1.5 times the interquartile range for the box.
Values outside this range are depicted as dots (not shown here)
There doesn’t appear to be any outliers, and there is a mild positive skew (the upper whisker is longer than the lower whisker)
Box   plots can be created for individual variables or for variables by group.
An example of a formula is y ~ A, where a separate box plot for numeric variable y is generated for each value of categorical variable A.
Adding the option varwidth=TRUE   will make the box plot widths proportional to the square root of their sample sizes.
In the following code, we revisit the impact of four, six, and eight cylinders on auto mpg with parallel box plots.
You can see in figure 6.12 that there’s a good separation of groups based on gas mileage.
You can also see that the distribution of mpg for six-cylinder cars is more symmetrical than for the other two car types.
Cars with four cylinders show the greatest spread (and positive skew) of mpg scores, when compared with six- and eight-cylinder cars.
The following code will create notched box plots for our mpg example:
The col option   fills the box plots with a red color, and varwidth=TRUE produces box plots with widths that are proportional to their sample sizes.
You can see in figure 6.13 that the median car mileage for four-, six-, and eightcylinder cars differ.
Figure 6.12 Box plots of car mileage versus number of cylinders.
Figure 6.13 Notched box plots   for car mileage versus number of cylinders.
Finally, you can produce box plots for more than one grouping factor.
Listing 6.9 provides box plots for mpg versus the number of cylinders and transmission type in an automobile.
Again, you use the col option   to fill the box plots with color.
In this case, there are six box plots and only two specified colors, so the colors repeat three times.
From figure 6.14 it’s again clear that median mileage decreases with cylinder.
For four- and six-cylinder cars, mileage is higher for standard transmissions.
But for eight-cylinder cars there doesn’t appear to be a difference.
Figure 6.14 Box plots for car mileage versus transmission type and number of cylinders.
Before   we end our discussion of box plots, it’s worth examining a variation called a violin plot.
A violin plot is a combination of a box plot and a kernel density plot.
You can create one using the vioplot() function   from the vioplot package.
Be sure to install the vioplot package   before first use.
The names parameter provides a character vector of labels for the violin plots, and col is a vector specifying the colors for each violin plot.
Note that the vioplot() function   requires you to separate the groups to be plotted into separate variables.
Figure 6.15 Violin plots of mpg versus number of cylinders.
Violin plots are basically kernel density plots superimposed in a mirror image fashion over box plots.
Here, the white dot is the median, the black boxes range from the lower to the upper quartile, and the thin black lines represent the whiskers.
Again, this may be due to a lack of easily accessible software.
We’ll end this chapter with a look at dot plots.
Unlike the graphs you’ve seen previously, dot plots plot every value for   a    variable.
Dot    plots provide a method of plotting a large number of labeled values on a simple horizontal scale.
You create them with the dotchart() function , using the format.
You can add a groups option   to designate a factor specifying how the elements of x are grouped.
If so, the option gcolor controls the color of the groups label and cex controls the size of the labels.
Figure 6.16 Dot plot of mpg for each car model.
The graph in figure 6.16 allows you to see the mpg for each make of car on the same horizontal axis.
Dot plots typically become most interesting when they’re sorted and grouping factors are distinguished by symbol and color.
In this example, the data frame mtcars is sorted by mpg (lowest to highest) and saved as data frame x.
A character vector (color) is added to data frame x and contains the values "red", "blue", or "darkgreen" depending on the value of cyl.
In addition, the labels for the data points are taken from the row names of the data frame (car makes)
The color of the points and labels are derived from the color vector , and points are represented by filled circles.
In figure 6.17, a number of features become evident for the first time.
Again, you see an increase in gas mileage as the number of cylinders decrease.
For example, the Pontiac Firebird, with eight cylinders, gets higher gas mileage than the Mercury 280C and the Valiant, each with six cylinders.
It’s also clear that the Toyota Corolla gets the best gas mileage by far, whereas the Lincoln Continental and Cadillac Fleetwood are outliers on the low end.
You can gain significant insight from a dot plot in this example because each point is labeled, the value of each point is inherently meaningful, and the points are arranged in a manner that promotes comparisons.
But as the number of data points increase, the utility of the dot plot decreases.
Jacoby (2006) provides a very informative discussion of the dot plot and provides R code for innovative applications.
Additionally, the Hmisc package offers a dot plot function (aptly named dotchart2) with a number of additional features.
Figure 6.17 Dot plot of mpg for car models grouped by number of cylinders.
In this chapter, we learned how to describe continuous and categorical variables.
We saw how bar plots and (to a lesser extent) pie charts can be used to gain insight into the distribution of a categorical variable, and how stacked and grouped bar charts can help us understand how groups differ on a categorical outcome.
We also explored how histograms, kernel density plots, box plots, rug plots, and dot plots can help us visualize the distribution of continuous variables.
Finally, we explored how overlapping kernel density plots, parallel box plots, and grouped dot plots can help you visualize group differences on a continuous outcome variable.
In later chapters, we’ll extend this univariate focus to include bivariate and multivariate graphical methods.
You’ll see how to visually depict relationships among many variables at once, using such methods as scatter plots, multigroup line plots, mosaic plots, correlograms, lattice graphs, and more.
In the next chapter, we’ll look at basic statistical methods for describing distributions and bivariate relationships numerically, as well as inferential methods for evaluating whether relationships among variables exist or are due to sampling   error.
In previous chapters, you learned how to import data into R and use a variety of functions to organize and transform the data into a useful format.
Once your data is properly organized and you’ve begun to explore the data visually, the next step will typically be to describe the distribution of each variable numerically, followed by an exploration of the relationships among selected variables two at a time.
After a new drug trial, what’s the outcome (no improvement, some improvement, marked improvement) for drug versus placebo groups? Does the gender of the participants have an impact on the outcome?
What’s the correlation between income and life expectancy? Is it significantly different from zero?
Are you more likely to receive imprisonment for a crime in different regions of the United States? Are the differences between regions statistically significant?
In this chapter we’ll review R functions for generating basic descriptive and inferential statistics.
First we’ll look at measures of location and scale for quantitative variables.
Then we’ll learn how to generate frequency and contingency tables (and associated chi-square tests) for categorical variables.
Next, we’ll examine the various forms of correlation coefficients available for continuous and ordinal variables.
Finally, we’ll turn to the study of group differences through parametric (t-tests) and nonparametric (Mann–Whitney U test, Kruskal–Wallis test) methods.
Although our focus is on numerical results, we’ll refer to graphical methods for visualizing these results throughout.
The statistical methods covered in this chapter are typically taught in a first-year undergraduate statistics course.
Alternatively, there are many informative online resources available (such as Wikipedia) for each of the topics covered.
In this section, we’ll look at measures of central tendency, variability, and distribution shape for continuous variables.
Our focus will be on miles per gallon (mpg), horsepower (hp), and weight (wt)
First we’ll look at descriptive statistics for all 32 cars.
Then we’ll examine descriptive statistics by transmission type (am) and number of cylinders (cyl)
When it comes to calculating descriptive statistics, R has an embarrassment of riches.
Let’s start with functions that are included in the base installation.
Then we’ll look at extensions that are available through the use of user-contributed packages.
In the base installation, you can use the summary() function to obtain descriptive statistics.
The summary() function provides the minimum, maximum, quartiles, and the mean for numerical variables and frequencies for factors and logical vectors.
You can use the apply() or sapply() function from chapter 5 to provide any descriptive statistics you choose.
Typical functions that you can plug in here are mean, sd, var, min, max, median, length, range, and quantile.
The function fivenum() returns Tukey’s five-number summary (minimum, lower-hinge, median, upper-hinge, and maximum)
Surprisingly, the base installation doesn’t provide functions for skew and kurtosis, but you can add your own.
The example in the next listing provides several descriptive statistics, including skew and kurtosis.
This will be most evident if you graph the data.
Several user-contributed packages offer functions for descriptive statistics, including Hmisc, pastecs, and psych.
The describe() function in the Hmisc package returns the number of variables and observations, the number of missing and unique values, the mean, quantiles, and the five highest and lowest values.
If basic=TRUE (the default), the number of values, null values, missing values, minimum, maximum, range, and sum are provided.
If desc=TRUE (also the default), the median, mean, standard error of the mean, 95 percent confidence interval for the mean, variance, standard deviation, and coefficient of variation are also provided.
Finally, if norm=TRUE (not the default), normal distribution statistics are returned, including skewness and kurtosis (and their statistical significance), and the Shapiro–Wilk test of normality.
A p-value option is used to calculate the confidence interval for the mean (.95 by default)
As if this isn’t enough, the psych package also has a function called describe() that provides the number of nonmissing observations, mean, standard deviation, median, trimmed mean, median absolute deviation, minimum, maximum, range, skew, kurtosis, and standard error of the mean.
Attaching package: 'psych' The following object(s) are masked from package:Hmisc : describe.
I told you that it was an embarrassment of riches!
How does R know which one to use? Simply put, the package last loaded takes precedence, as seen in listing 7.5
Here, psych is loaded after Hmisc, and a message is printed indicating that the describe() function in Hmisc is masked by the function in psych.
When you type in the describe() function and R searches for it, R comes to the psych package first and executes it.
You have to give R more information to find it.
Now that you know how to generate descriptive statistics for the data as a whole, let’s review how to obtain statistics for subgroups of the data.
When comparing groups of individuals or observations, the focus is usually on the descriptive statistics of each group, rather than the total sample.
Again, there are several ways to accomplish this in R.
We’ll start by getting descriptive statistics for each level of transmission type.
You can use the aggregate() function (section 5.6.2) to obtain descriptive statistics by group, as shown in the following listing.
You use the assignment to provide a more useful column label.
Unfortunately, aggregate() only allows you to use single value functions such as mean, standard deviation, and the like in each call.
The doBy package and the psych package also provide functions for descriptive statistics by group.
Again, they aren’t distributed in the base installation and must be installed before first use.
The summaryBy() function in the doBy package has the format.
Variables on the left of the ~ are the numeric variables to be analyzed and variables on the right are categorical grouping variables.
The function can be any built-in or usercreated R function.
An example using the mystats() function you created in section 7.2.1 is shown in the following listing.
The describe.by() function contained in the psych package provides the same descriptive statistics as describe, stratified by one or more grouping variables, as you can see in the following listing.
Unlike the previous example, the describe.by() function doesn’t allow you to specify an arbitrary function, so it’s less generally applicable.
But this will only work if there are no empty cells when the grouping variables are crossed.
Finally, you can use the reshape package described in section 5.6.3 to derive descriptive statistics by group in a flexible way.
If you haven’t read that section, I suggest you review it before continuing.
In the final example of this section, we’ll apply the reshape approach to obtaining descriptive statistics for each subgroup formed by transmission type and number of cylinders.
For descriptive statistics, we’ll get the sample size, mean, and standard deviation.
The code and results are shown in the following listing.
Listing 7.10 Summary statistics by group via the reshape package.
Personally, I find this approach the most compact and appealing.
Data analysts have their own preferences for which descriptive statistics to display and how they like to see them formatted.
Choose the one that works best for you, or create your own!
Numerical summaries of a distribution’s characteristics are important, but they’re no substitute for a visual representation.
They can provide insights that are easily missed by reliance on a small set of descriptive statistics.
The functions considered so far provide summaries of quantitative variables.
The functions in the next section allow you to examine the distributions of categorical variables.
In this section, we’ll look at frequency and contingency tables from categorical variables, along with tests of independence, measures of association, and methods for graphically displaying results.
We’ll be using functions in the basic installation, along with functions from the vcd and gmodels package.
In the following examples, assume that A, B, and C represent categorical variables.
The data for this section come from the Arthritis dataset included with the vcd package.
In the following sections, we’ll use each of these functions to explore categorical variables.
We’ll begin with simple frequencies, followed by two-way contingency tables, and end with multiway contingency tables.
The first step is to create a table using either the table() or the xtabs() function, then manipulate it using the other functions.
You can generate simple frequency counts using the table() function.
For two-way tables, the format for the table() function is.
Alternatively, the xtabs() function allows you to create a contingency table using formula style input.
In general, the variables to be cross-classified appear on the right of the formula (that is, to the right of the ~) separated by + signs.
If a variable is included on the left side of the formula, it’s assumed to be a vector of frequencies (useful if the data have already been tabulated)
You can generate marginal frequencies and proportions using the margin.table() and prop.table() functions, respectively.
The index (1) refers to the first variable in the table() statement.
Here, the index (2) refers to the second variable in the table() statement.
You can use the addmargins() function to add marginal sums to these tables.
For example, the following code adds a sum row and column:
When using addmargins(), the default is to create sum margins for all variables in a table.
In the table, you see that 25 percent of those patients with marked improvement received a placebo.
To include NA as a valid category in the frequency counts, include the table option useNA="ifany"
A third method for creating two-way tables is the CrossTable() function in the gmodels package.
If you have more than two categorical variables, you’re dealing with multidimensional tables.
Both table() and xtabs() can be used to generate multidimensional tables based on three or more categorical variables.
The margin.table(), prop.table(), and addmargins() functions extend naturally to more than two dimensions.
Additionally, the ftable() function can be used to print multidimensional tables in a compact and attractive manner.
The code in q produces cell frequencies for the three-way classification.
The code also demonstrates how the ftable() function can be used to print a more compact and attractive version of the table.
The code in w produces the marginal frequencies for Treatment, Sex, and Improved.
The code in e produces the marginal frequencies for the Treatment x Improved classification, summed over Sex.
The proportion of patients with None, Some, and Marked improvement for each Treatment x Sex combination is provided in r.
In general, the proportions will add to one over the indices not included in the prop.table() call (the third index, or Improve in this case)
You can see this in the last example, where you add a sum margin over the third index.
While contingency tables tell you the frequency or proportions of cases for each combination of the variables that comprise the table, you’re probably also interested in whether the variables in the table are related or independent.
You can apply the function chisq.test() to a two-way table in order to produce a chi-square test of independence of the row and column variables.
The p-values are the probability of obtaining the sampled results assuming independence of the row and column variables in the population.
Because the probability is small for q, you reject the hypothesis that treatment type and outcome are independent.
Because the probability for w isn’t small, it’s not unreasonable to assume that outcome and gender are independent.
The warning message in listing 7.13 is produced because one of the six cells in the table (male-some improvement) has an expected value less than five, which may invalidate the chi-square approximation.
You can produce a Fisher’s exact test via the fisher.test() function.
Fisher’s exact test evaluates the null hypothesis of independence of rows and columns in a contingency table with fixed marginals.
In contrast to many statistical packages, the fisher.test() function can be applied to any two-way table with two or more rows and columns, not a 2x2 table.
The following code tests the hypothesis that Treatment and Improved variables are independent within each level Sex.
The test assumes that there’s no three-way (Treatment x Improved x Sex) interaction.
The results suggest that the treatment received and the improvement reported aren’t independent within each level of sex (that is, treated individuals improved more than those receiving placebos when controlling for sex)
The significance tests in the previous section evaluated whether or not sufficient evidence existed to reject a null hypothesis of independence between variables.
If you can reject the null hypothesis, your interest turns naturally to measures of association in order to gauge the strength of the relationships present.
The assocstats() function in the vcd package can be used to calculate the phi coefficient, contingency coefficient, and Cramer’s V for a two-way table.
The vcd package also provides a kappa() function that can calculate Cohen’s kappa and weighted kappa for.
Finally, correspondence analysis functions in the ca package allow you to visually explore relationships between rows and columns in contingency tables using various geometric representations (Nenadic and Greenacre, 2007)
We’ll end this section with a topic that’s rarely covered in books on R but that can be very useful.
What happens if you have a table but need the original raw data? For example, say you have the following:
There are many statistical functions in R that expect the latter format rather than the former.
You can use the function provided in the following listing to convert an R table back into a flat data file.
This function takes an R table (with any number of rows and columns) and returns a data frame in flat file format.
You can also use this function to input tables from published studies.
For example, let’s say that you came across table 7.2 in a journal and wanted to save it into R as a flat file.
Table 7.2 Contingency table for treatment versus improvement from the Arthritis dataset.
This next listing describes a method that would do the trick.
In this section, we’ll look at a variety of correlation coefficients, as well as tests of significance.
We’ll use the state.x77 dataset available in the base R installation.
There are also temperature and land area measures, but we’ll drop them to save space.
In addition to the base installation, we’ll be using the psych and ggm packages.
The Pearson product moment correlation assesses the degree of linear relationship between two quantitative variables.
Spearman’s Rank Order correlation coefficient assesses the degree of relationship between two rank-ordered variables.
Kendall’s Tau is also a nonparametric measure of rank correlation.
The cor() function produces all three correlation coefficients, whereas the cov() function provides covariances.
There are many options, but a simplified format for producing correlations is.
You can see, for example, that a strong positive correlation exists between income and high school graduation rate and that a strong negative correlation exists between illiteracy rates and life expectancy.
Notice that you get square matrices by default (all variables crossed with all other variables)
You can also produce nonsquare matrices; see the following example:
This version of the function is particularly useful when you’re interested in the relationships between one set of variables and another.
For that, you need tests of significance (described in section 7.3.2)
A partial correlation is a correlation between two quantitative variables, controlling for one or more other quantitative variables.
You can use the pcor() function in the ggm package to provide partial correlation coefficients.
The ggm package isn’t installed by default, so be sure to install it on first use.
In this case, 0.346 is the correlation between population and murder rate, controlling for the influence of income, illiteracy rate, and HS graduation rate.
The use of partial correlations is common in the social sciences.
The hetcor() function in the polycor package can compute a heterogeneous correlation matrix containing Pearson product-moment correlations between numeric variables, polyserial correlations between numeric and ordinal variables, polychoric correlations between ordinal variables, and tetrachoric correlations between two dichotomous variables.
Polyserial, polychoric, and tetrachoric correlations assume that the ordinal or dichotomous variables are derived from underlying normal distributions.
See the documentation that accompanies this package for more information.
Once you’ve generated correlation coefficients, how do you test them for statistical significance? The typical null hypothesis is no relationship (that is, the correlation in the population is 0)
You can use the cor.test() function to test an individual Pearson, Spearman, and Kendall correlation coefficient.
Unfortunately, you can test only one correlation at a time using cor.test.
Luckily, the corr.test() function provided in the psych package allows you to go further.
The corr.test() function produces correlations and significance levels for matrices of Pearson, Spearman, or Kendall correlations.
Listing 7.19 Correlation matrix and tests of significance via corr.test.
The use= options can be "pairwise" or "complete" (for pairwise or listwise deletion of missing values, respectively)
The method= option is "pearson" (the default), "spearman", or "kendall"
The pcor.test() function in the psych package can be used to test the conditional independence of two variables controlling for one or more additional variables, assuming multivariate normality.
Before leaving this topic, it should be mentioned that the r.test() function in the psych package also provides a number of useful significance tests.
The bivariate relationships underlying correlations can be visualized through scatter plots and scatter plot matrices, whereas correlograms provide a unique and powerful method for comparing a large numbers of correlation coefficients in a meaningful way.
The most common activity in research is the comparison of two groups.
Do patients receiving a new drug show greater improvement than patients using an existing medication? Does one manufacturing process produce fewer defects than another? Which of two teaching methods is most cost-effective? If your outcome variable is categorical, you can use the methods described in section 7.3
Here, we’ll focus on group comparisons, where the outcome variable is continuous and assumed to be distributed normally.
For this illustration, we’ll use the UScrime dataset distributed with the MASS package.
The categorical variable So (an indicator variable for Southern states) will serve as the grouping variable.
Are you more likely to be imprisoned if you commit a crime in the South? The comparison of interest is Southern versus non-Southern states and the dependent variable is the probability of incarceration.
A two-group independent t-test can be used to test the hypothesis that the two population means are equal.
Here, you assume that the two groups are independent and that the data are sampled from normal populations.
The optional data argument refers to a matrix or data frame containing the variables.
In contrast to most statistical packages, the default test assumes unequal variance and applies the Welsh degrees of freedom modification.
You can add a var.equal=TRUE option to specify equal variances and a pooled variance estimate.
By default, a twotailed alternative is assumed (that is, the means differ but the direction isn’t specified)
You wouldn’t expect the unemployment rate for younger and older males in Alabama to be unrelated.
When observations in the two groups are related, you have a dependent groups design.
A dependent t-test assumes that the difference between groups is normally distributed.
The mean difference (61.5) is large enough to warrant rejection of the hypothesis that the mean unemployment rate for older and younger males is the same.
What do you do if you want to compare more than two groups? If you can assume that the data are independently sampled from normal populations, you can use analysis of variance (ANOVA)
Feel free to abandon this section and jump to chapter 9 at any time.
If you’re unable to meet the parametric assumptions of a t-test or ANOVA, you can turn to nonparametric approaches.
For example, if the outcome variables are severely skewed or ordinal in nature, you may wish to use the techniques in this section.
If the two groups are independent, you can use the Wilcoxon rank sum test (more popularly known as the Mann–Whitney U test) to assess whether the observations are sampled from the same probability distribution (that is, whether the probability of obtaining higher scores is greater in one population than the other)
The optional data argument refers to a matrix or data frame containing the variables.
If you apply the Mann–Whitney U test to the question of incarceration rates from the previous section, you’ll get these results:
The Wilcoxon signed rank test provides a nonparametric alternative to the dependent sample t-test.
It’s appropriate in situations where the groups are paired and the assumption of normality is unwarranted.
The format is identical to the Mann–Whitney U test, but you add the paired=TRUE option.
Let’s apply it to the unemployment question from the previous section:
Again, you’d reach the same conclusion reached with the paired t-test.
In this case, the parametric t-tests and their nonparametric equivalents reach.
The nonparametric tests are more appropriate when the assumptions are grossly unreasonable (for example, rank ordered data)
When there are more than two groups to be compared, you must turn to other methods.
It contains population, income, illiteracy rate, life expectancy, murder rate, and high school graduation rate data for US states.
What if you want to compare the illiteracy rates in four regions of the country (Northeast, South, North Central, and West)? This is called a one-way design, and there are both parametric and nonparametric approaches available to address the question.
If you can’t meet the assumptions of ANOVA designs, you can use nonparametric methods to evaluate group differences.
If the groups are independent, a KruskalWallis test will provide you with a useful approach.
If the groups are dependent (for example, repeated measures or randomized block design), the Friedman test is more appropriate.
In both cases, data is an option argument specifying a matrix or data frame containing the variables.
First, you’ll have to add the region designations to the dataset.
The significance test suggests that the illiteracy rate isn’t the same in each of the four regions of the country (p <.001)
Although you can reject the null hypothesis of no difference, the test doesn’t tell you which regions differ significantly from each other.
A more elegant approach is to apply a simultaneous multiple comparisons procedure that makes all pairwise comparisons, while controlling the type I error rate (the probability of finding a difference that isn’t there)
The npmc package provides the nonparametric multiple comparisons you need.
To be honest, I’m stretching the definition of basic in the chapter title quite a bit, but because it fits well here, I hope you’ll bear with me.
The npmc() function in this package expects input to be a two-column data frame with a column named var (the dependent variable) and class (the grouping variable)
The following listing contains the code you can use to accomplish this.
You can see from the two-sided p-values (p.value.2s) that the South differs significantly from the other three regions, and that the other three regions don’t differ from each other.
In w you see that the South has a higher median illiteracy rate.
Note that npmc uses randomized values for integral calculations, so results differ slightly from call to call.
Examining group differences visually is also a crucial part of a comprehensive data analysis strategy.
It allows you to assess the magnitude of the differences, identify any distributional characteristics that influence the results (such as skew, bimodality, or outliers), and evaluate the appropriateness of the test assumptions.
In this chapter, we reviewed the functions in R that provide basic statistical summaries and tests.
We looked at sample statistics and frequency tables, tests of independence and measures of association for categorical variables, correlations between quantitative variables (and their associated significance tests), and comparisons of two or more groups on a quantitative outcome variable.
In the next chapter, we’ll explore simple and multiple regression, where the focus is on understanding relationships between one (simple) or more than one (multiple) predictor variables and a predicted or criterion variable.
Graphical methods will help you diagnose potential problems, evaluate and improve the fit of your models, and uncover unexpected gems of information in your data.
We move from describing the relationship between two variables, to modeling the relationship between a numerical outcome variable and a set of numeric and/or categorical predictor variables.
Chapter 8 introduces regression methods for modeling the relationship between a numeric outcome variable and a set of one or more predictor variables.
Chapter 8 provides step-by-step coverage of the methods available for fitting linear models, evaluating their appropriateness, and interpreting their meaning.
Chapter 9 considers the analysis of basic experimental and quasi-experimental designs through the analysis of variance and its variants.
Here we’re interested in how treatment combinations or conditions affect a numerical outcome variable.
The chapter introduces the functions in R that are used to perform an analysis of variance, analysis of covariance, repeated measures analysis of variance, multifactor analysis of variance, and multivariate analysis of variance.
Methods for assessing the appropriateness of these analyses, and visualizing the results are also discussed.
In designing experimental and quasi-experimental studies, it’s important to determine if the sample size is adequate for detecting the effects of interest (power analysis)
Starting with a discussion of hypothesis testing, the presentation focuses on how to use R functions to determine the sample size necessary to detect a treatment effect of a given size with a given degree of confidence.
This can help you to plan studies that are likely to yield useful results.
This includes the various types of two- and three-dimensional scatter plots, scatter plot matrices, line plots, and bubble plots.
It also introduces the useful, but less well-known, correlograms and mosaic plots.
Chapter 12 presents analytic methods that work well in cases where data are sampled from unknown or mixed distributions, where sample sizes are small, where outliers are a problem, or where devising an appropriate test based on a theoretical distribution is mathematically intractable.
The methods described in this chapter will allow you to devise hypothesis tests for data that do not fit traditional parametric assumptions.
After completing part 3, you’ll have the tools to analyze most common data analytic problems encountered in practice.
And you will be able to create some gorgeous graphs!
In many ways, regression analysis lives at the heart of statistics.
It’s a broad term for a set of methodologies used to predict a response variable (also called a dependent, criterion, or outcome variable) from one or more predictor variables (also called independent or explanatory variables)
In general, regression analysis can be used to identify the explanatory variables that are related to a response variable, to describe the form of the relationships involved, and to provide an equation for predicting the response variable from the explanatory variables.
For example, an exercise physiologist might use regression analysis to develop an equation for predicting the expected number of calories a person will burn while exercising on a treadmill.
The response variable is the number of calories burned (calculated from the amount of oxygen consumed), and the predictor variables might include duration of exercise (minutes), percentage of time spent at their target heart rate, average speed (mph), age (years), gender, and body mass index (BMI)
From a theoretical point of view, the analysis will help answer such questions as these:
From a practical point of view, the analysis will help answer such questions as the following:
Because regression analysis plays such a central role in modern statistics, we'll cover it in some depth in this chapter.
First, we’ll look at how to fit and interpret regression models.
Next, we’ll review a set of techniques for identifying potential problems with these models and how to deal with them.
Of all the potential predictor variables available, how do you decide which ones to include in your final model? Fourth, we’ll address the question of generalizability.
How well will your model work when you apply it in the real world? Finally, we’ll look at the issue of relative importance.
Of all the predictors in your model, which one is the most important, the second most important, and the least important?
As you can see, we’re covering a lot of ground.
Effective regression analysis is an interactive, holistic process with many steps, and it involves more than a little skill.
Rather than break it up into multiple chapters, I’ve opted to present this topic in a single chapter in order to capture this flavor.
As a result, this will be the longest and most involved chapter in the book.
Stick with it to the end and you’ll have all the tools you need to tackle a wide variety of research questions.
The term regression can be confusing because there are so many specialized varieties (see table 8.1)
In addition, R has powerful and comprehensive features for fitting regression models, and the abundance of options can be confusing as well.
Simple linear Predicting a quantitative response variable from a quantitative explanatory variable.
Polynomial Predicting a quantitative response variable from a quantitative explanatory variable, where the relationship is modeled as an nth order polynomial.
Multiple linear Predicting a quantitative response variable from two or more explanatory variables.
Multivariate Predicting more than one response variable from one or more explanatory variables.
Logistic Predicting a categorical response variable from one or more explanatory variables.
Poisson Predicting a response variable representing counts from one or more explanatory variables.
Cox proportional hazards Predicting time to an event (death, failure, relapse) from one or more explanatory variables.
Nonlinear Predicting a quantitative response variable from one or more explanatory variables, where the form of the model is nonlinear.
Nonparametric Predicting a quantitative response variable from one or more explanatory variables, where the form of the model is derived from the data and not specified a priori.
Robust Predicting a quantitative response variable from one or more explanatory variables using an approach that’s resistant to the effect of influential observations.
In this chapter, we’ll focus on regression methods that fall under the rubric of ordinary least squares (OLS) regression, including simple linear regression, polynomial regression, and multiple linear regression.
In OLS regression, a quantitative dependent variable is predicted from a weighted sum of predictor variables, where the weights are parameters estimated from the data.
Let’s take a look at a concrete example (no pun intended), loosely adapted from Fwa (2006)
An engineer wants to identify the most important factors related to bridge deterioration (such as age, traffic volume, bridge design, construction materials and methods, construction quality, and weather conditions) and determine the.
She collects data on each of these variables from a representative sample of bridges and models the data using OLS regression.
She fits a series of models, checks their compliance with underlying statistical assumptions, explores any unexpected or aberrant findings, and finally chooses the “best” model from among many possible models.
If she finds that some bridges deteriorate much faster or slower than predicted by the model, a study of these “outliers” may yield important findings that could help her to understand the mechanisms involved in bridge deterioration.
I’m a clinical psychologist and statistician, and I know next to nothing about civil engineering.
But the general principles apply to an amazingly wide selection of problems in the physical, biological, and social sciences.
Each of the following questions could also be addressed using an OLS approach:
Our primary limitation is our ability to formulate an interesting question, devise a useful response variable to measure, and gather appropriate data.
For the remainder of this chapter I’ll describe how to use R functions to fit OLS regression models, evaluate the fit, test assumptions, and select among competing models.
It’s assumed that the reader has had exposure to least squares regression as typically taught in a second semester undergraduate statistics course.
A number of excellent texts are available that cover the statistical material outlined in this chapter.
For most of this chapter, we’ll be predicting the response variable from a set of predictor variables (also called “regressing” the response variable on the predictor variables—hence the name) using OLS.
Xji is the jth predictor value for the ith observation.
Our goal is to select model parameters (intercept and slopes) that minimize the difference between actual response values and those predicted by the model.
Specifically, model parameters are selected to minimize the sum of squared residuals.
To properly interpret the coefficients of the OLS model, you must satisfy a number of statistical assumptions:
We could call this constant variance, but saying homoscedasticity makes me feel smarter.
If you violate these assumptions, your statistical significance tests and confidence intervals may not be accurate.
Note that OLS regression also assumes that the independent variables are fixed and measured without error, but this assumption is typically relaxed in practice.
In R, the basic function for fitting a linear model is lm()
The resulting object (myfit in this case) is a list that contains extensive information about the fitted model.
Other symbols can be used to modify the formula in various ways (see table 8.2)
Separates response variables on the left from the explanatory variables on the right.
For example, a prediction of y from x, z, and w would be coded y ~ x + z + w.
A prediction of y from x, z, and the interaction between x and z would be coded y ~ x + z + x:z.
A place holder for all other variables in the data frame except the dependent variable.
For example, if a data frame contained the variables x, y, z, and w, then the code y ~
For example, log(y) ~ x + z + w would predict log(y) from x, z, and w.
In addition to lm(), table 8.3 lists several functions that are useful when generating a simple or multiple regression analysis.
Each of these functions is applied to the object returned by lm() in order to generate additional information based on that fitted model.
Table 8.3 Other functions that are useful when fitting linear models.
When the regression model contains one dependent variable and one independent variable, we call the approach simple linear regression.
When there’s more than one predictor variable, you call it multiple linear regression.
We’ll start with an example of a simple linear regression, then progress to examples of polynomial and multiple linear regression, and end with an example of multiple regression that includes an interaction among the predictors.
Let’s take a look at the functions in table 8.3 through a simple regression example.
Having an equation for predicting weight from height can help us to identify overweight or underweight individuals.
The analysis is provided in the following listing, and the resulting graph is shown in figure 8.1
From the output, you see that the prediction equation is.
Because a height of 0 is impossible, you wouldn’t try to give a physical interpretation to the intercept.
The F statistic tests whether the predictor variables taken together, predict the response variable above chance levels.
Because there’s only one predictor variable in simple regression, in this example the F test is equivalent to the t-test for the regression coefficient for height.
For demonstration purposes, we’ve printed out the actual, predicted, and residual values.
Evidently, the largest residuals occur for low and high heights, which can also be seen in the plot (figure 8.1)
Polynomial regression allows you to predict a response variable from an explanatory variable, where the form of the relationship is an nth degree polynomial.
The I function treats the contents within the parentheses as an R regular expression.
Listing 8.2 shows the results of fitting the quadratic equation.
Figure 8.1 Scatter plot with regression line for weight predicted from height.
The amount of variance accounted for has increased to 99.9 percent.
In general, an nth degree polynomial produces a curve with n-1 bends.
Although higher polynomials are possible, I’ve rarely found that terms higher than cubic are necessary.
Before we move on, I should mention that the scatterplot() function in the car package provides a simple and convenient method of plotting a bivariate relationship.
The lty.smooth=2 option specifies that the loess fit be rendered as a dashed line.
Note that this polynomial equation still fits under the rubric of linear regression.
It’s linear because the equation involves a weighted sum of predictor variables (height and height-squared in this case)
In contrast, here’s an example of a truly nonlinear model:
Nonlinear models of this form can be fit with the nls() function.
Figure 8.3 Scatter plot of height by weight, with linear and smoothed fits, and marginal box plots.
You can tell at a glance that the two variables are roughly symmetrical and that a curved line will fit the data points better than a straight line.
When there’s more than one predictor variable, simple linear regression becomes multiple linear regression, and the analysis grows more involved.
Technically, polynomial regression is a special case of multiple regression.
We’ll use the state.x77 dataset in the base package for this example.
We want to explore the relationship between a state’s murder rate and other characteristics of the state, including population, illiteracy rate, average income, and frost levels (mean number of days below freezing)
Because the lm() function requires a data frame (and the state.x77 dataset is contained in a matrix), you can simplify your life with the following code:
This code creates a data frame called states, containing the variables we’re interested in.
We’ll use this new data frame for the remainder of the chapter.
A good first step in multiple regression is to examine the relationships among the variables two at a time.
The principal diagonal contains density and rug plots for each variable.
You can see that murder rate may be bimodal and that each of the predictor variables is skewed to some extent.
Murder rates rise with population and illiteracy, and fall with higher income levels and frost.
At the same time, colder states have lower illiteracy rates and population and higher incomes.
Figure 8.4 Scatter plot matrix of dependent and independent variables for the states data, including linear and smoothed fits, and marginal distributions (kernel density plots and rug plots)
Now let’s fit the multiple regression model with the lm() function (see the following listing)
When there’s more than one predictor variable, the regression coefficients indicate the increase in the dependent variable for a unit change in a predictor variable, holding all other predictor variables constant.
On the other hand, the coefficient for Frost isn’t significantly different from zero (p = 0.954) suggesting that Frost and Murder aren’t linearly related when controlling for the other predictor variables.
Taken together, the predictor variables account for 57 percent of the variance in murder rates across states.
Up to this point, we’ve assumed that the predictor variables don’t interact.
In the next section, we’ll consider a case in which they do.
Some of the most interesting research findings are those involving interactions among predictor variables.
Let’s say that you’re interested in the impact of automobile weight and horse power on mileage.
You could fit a regression model that includes both predictors, along with their interaction, as shown in the next listing.
Listing 8.5 Multiple linear regression with a significant interaction term.
You can see from the Pr(>|t|) column that the interaction between horse power and car weight is significant.
What does this mean? A significant interaction between two predictor variables tells you that the relationship between one predictor and the response variable depends on the level of the other predictor.
Here it means that the relationship between miles per gallon and horse power varies by car weight.
You can visualize interactions using the effect() function in the effects package.
This plot displays the relationship between mpg and hp at 3 values of wt.
You can see from this graph that as the weight of the car increases, the relationship between horse power and miles per gallon weakens.
For wt=4.2, the line is almost horizontal, indicating that as hp increases, mpg doesn’t change.
Unfortunately, fitting the model is only the first step in the analysis.
Once you fit a regression model, you need to evaluate whether you’ve met the statistical assumptions underlying your approach before you can have confidence in the inferences you draw.
In the previous section, you used the lm() function to fit an OLS regression model and the summary() function to obtain the model parameters and summary statistics.
Unfortunately, there’s nothing in this printout that tells you if the model you have fit is appropriate.
Your confidence in inferences about regression parameters depends on the degree to which you’ve met the statistical assumptions of the OLS model.
Although the summary() function in listing 8.4 describes the model, it provides no information concerning the degree to which you’ve satisfied the statistical assumptions underlying the model.
Why is this important? Irregularities in the data or misspecifications of the relationships between the predictors and the response variable can lead you to settle on a model that’s wildly inaccurate.
On the one hand, you may conclude that a predictor and response variable are unrelated when, in fact, they are.
On the other hand, you may conclude that a predictor and response variable are related when, in fact, they aren’t! You may also end up with a model that makes poor predictions when applied in real-world settings, with significant and unnecessary error.
Let’s look at the output from the confint() function applied to the states multiple regression problem in section 8.2.4
Additionally, because the confidence interval for Frost contains 0, you can conclude that a change in temperature is unrelated to murder rate, holding the other variables constant.
But your faith in these results is only as strong as the evidence that you have that your data satisfies the statistical assumptions underlying the model.
A set of techniques called regression diagnostics provides you with the necessary tools for evaluating the appropriateness of the regression model and can help you to uncover and correct problems.
Then we’ll look at newer, improved methods available through the car package.
R’s base installation provides numerous methods for evaluating the statistical assumptions in a regression analysis.
The most common approach is to apply the plot() function to the object returned by the lm()
Doing so produces four graphs that are useful for evaluating the model fit.
Figure 8.6 Diagnostic plots for the regression of weight on height.
To understand these graphs, consider the assumptions of OLS regression:
The Normal Q-Q plot (upper right) is a probability plot of the standardized residuals against the values that would be expected under normality.
If you’ve met the normality assumption, the points on this graph should fall on the straight 45-degree line.
You have to use your understanding of how the data were collected.
There’s no a priori reason to believe that one woman’s weight influences another woman’s weight.
If you found out that the data were sampled from families, you may have to adjust your assumption of independence.
In other words, the model should capture all the systematic variance present in the data, leaving nothing but random noise.
In the Residuals versus Fitted graph (upper left), you see clear evidence of a curved relationship, which suggests that you may want to add a quadratic term to the regression.
Finally, the Residual versus Leverage graph (bottom right) provides information on individual observations that you may wish to attend to.
The dependent variable value isn’t used to calculate an observation’s leverage.
Influential observations are identified using a statistic called Cook’s distance, or Cook’s D.
To be honest, I find the Residual versus Leverage plot difficult to read and not useful.
You’ll see better representations of this information in later sections.
To complete this section, let’s look at the diagnostic plots for the quadratic fit.
Figure 8.7 Diagnostic plots for the regression of weight on height and height-squared.
This second set of plots suggests that the polynomial regression provides a better fit with regard to the linearity assumption, normality of residuals (except for observation 13), and homoscedasticity (constant residual variance)
Observation 15 appears to be influential (based on a large Cook’s D value), and deleting it has an impact on the parameter estimates.
Your models should fit your data, not the other way around!
Finally, let’s apply the basic approach to the states multiple regression problem:
As you can see from the graph, the model assumptions appear to be well satisfied, with the exception that Nevada is an outlier.
Although these standard diagnostic plots are helpful, better tools are now available in R and I recommend their use over the plot(fit) approach.
The car package provides a number of functions that significantly enhance your ability to fit and evaluate regression models (see table 8.4)
Figure 8.8 Diagnostic plots for the regression of murder rate on state characteristics.
In addition, the gvlma package provides a global test for linear model assumptions.
Let’s look at each in turn, by applying them to our multiple regression example.
The qqPlot() function provides a more accurate method of assessing the normality assumption than provided by the plot() function in the base package.
It plots the studentized residuals (also called studentized deleted residuals or jackknifed residuals) against a t distribution with n-p-1 degrees of freedom, where n is the sample size and p is the number of regression parameters (including the intercept)
The qqPlot() function generates the probability plot displayed in figure 8.9
Hitting the Esc key, selecting Stop from the graph’s drop-down menu, or right-clicking on the graph will turn off this interactive mode.
When simulate=TRUE, a 95 percent confidence envelope is produced using a parametric bootstrap.
With the exception of Nevada, all the points fall close to the line and are within the confidence envelope, suggesting that you’ve met the normality assumption fairly well.
The question that you need to ask is, “Why does Nevada have a higher murder rate than predicted from population, income, illiteracy, and temperature?” Anyone (who hasn’t see Goodfellas) want to guess?
Take a look at the code in the next listing.
The residplot() function generates a histogram of the studentized residuals and superimposes a normal curve, kernel density curve, and rug plot.
Figure 8.10 Distribution of studentized residuals using the residplot() function.
As you can see, the errors follow a normal distribution quite well, with the exception of a large outlier.
Although the Q-Q plot is probably more informative, I’ve always found it easier to gauge the skew of a distribution from a histogram or density plot than from a probability plot.
As indicated earlier, the best way to assess whether the dependent variable values (and thus the residuals) are independent is from your knowledge of how the data were collected.
The car package provides a function for the Durbin–Watson test to detect such serially correlated errors.
You can apply the Durbin–Watson test to the multiple regression problem with the following code:
The nonsignificant p-value (p=0.282) suggests a lack of autocorrelation, and conversely an independence of errors.
The lag value (1 in this case) indicates that each observation is being compared with the one next to it in the dataset.
Although appropriate for time-dependent data, the test is less applicable for data that isn’t clustered in this fashion.
Note that the durbinWatsonTest() function uses bootstrapping (see chapter 12) to derive p-values.
Unless you add the option simulate=FALSE, you’ll get a slightly different value each time you run the test.
You can look for evidence of nonlinearity in the relationship between the dependent variable and the independent variables by using component plus residual plots (also known as partial residual plots)
The plot is produced by crPlots() function in the car package.
You’re looking for any systematic departure from the linear model that you’ve specified.
Nonlinearity in any of these plots suggests that you may not have adequately modeled the functional form of that predictor in the regression.
If so, you may need to add curvilinear components such as polynomial terms, transform one or more variables (for example, use log(X) instead of X), or abandon linear regression in favor of some other regression variant.
Figure 8.11 Component plus residual plots for the regression of murder rate on state characteristics.
The component plus residual plots confirm that you’ve met the linearity assumption.
The form of the linear model seems to be appropriate for this dataset.
The car package also provides two useful functions for identifying non-constant error variance.
The ncvTest() function produces a score test of the hypothesis of constant error variance against the alternative that the error variance changes with the level of the fitted values.
The spreadLevelPlot() function creates a scatter plot of the absolute standardized residuals versus the fitted values, and superimposes a line of best fit.
The score test is nonsignificant (p = 0.19), suggesting that you’ve met the constant variance assumption.
You can also see this in the spread-level plot (figure 8.12)
The points form a random horizontal band around a horizontal line of best fit.
If you’d violated the assumption, you’d expect to see a nonhorizontal line.
The suggested power transformation in listing 8.7 is the suggested power p (Y p) that would stabilize the nonconstant error variance.
For example, if the plot showed a nonhorizontal trend and the suggested power transformation was 0.5, then using Y  rather than Y in the regression equation might lead to a model that satisfies homoscedasticity.
If the suggested power was 0, you’d use a log transformation.
In the current example, there’s no evidence of heteroscedasticity and the suggested power is close to 1 (no transformation required)
Finally, let’s examine the gvlma() function in the gvlma package.
Written by Pena and Slate (2006), the gvlma() function performs a global validation of linear model assumptions as well as separate evaluations of skewness, kurtosis, and heteroscedasticity.
In other words, it provides a single omnibus (go/no go) test of model assumptions.
The following listing applies the test to the states data.
You can see from the printout (the Global Stat line) that the data meet all the statistical assumptions that go with the OLS regression model (p = 0.597)
Before leaving this section on regression diagnostics, let’s focus on a problem that’s not directly related to statistical assumptions but is important in allowing you to interpret multiple regression results.
Your independent variables include date of birth (DOB) and age.
But when you look at the individual regression coefficients for DOB and age, you find that they’re both nonsignificant (that is, there’s no evidence that either is related to grip strength)
The problem is that DOB and age are perfectly correlated within rounding error.
A regression coefficient measures the impact of one predictor variable on the response variable, holding all other predictor variables constant.
This amounts to looking at the relationship of grip strength and age, holding age constant.
It leads to large confidence intervals for your model parameters and makes the interpretation of individual coefficients difficult.
Multicollinearity can be detected using a statistic called the variance inflation factor (VIF)
For any predictor variable, the square root of the VIF indicates the degree to which the confidence interval for that variable’s regression parameter is expanded relative to a model with uncorrelated predictors (hence the name)
As a general rule, vif >2 indicates a multicollinearity problem.
The results indicate that multicollinearity isn’t a problem with our predictor variables.
These are data points that warrant further investigation, either because they’re different than other observations in some way, or because they exert a disproportionate amount of influence on the results.
Points in the Q-Q plot of figure 8.9 that lie outside the confidence band are considered outliers.
The car package also provides a statistical test for outliers.
The outlierTest() function reports the Bonferroni adjusted p-value for the largest absolute studentized residual:
Here, you see that Nevada is identified as an outlier (p = 0.048)
Note that this function tests the single largest (positive or negative) residual for significance as an outlier.
If it is significant, you must delete it and rerun the test to see if others are present.
Observations that have high leverage are outliers with regard to the other predictors.
In other words, they have an unusual combination of predictor values.
Observations with high leverage are identified through the hat statistic.
For a given dataset, the average hat value is p/n, where p is the number of parameters estimated in the model (including the intercept) and n is the sample size.
Figure 8.13 Index plot of hat values for assessing observations with high leverage.
Clicking on points of interest labels them until the user presses Esc, selects Stop from the graph drop-down menu, or right-clicks on the graph.
Here you see that Alaska and California are particularly unusual when it comes to their predictor values.
Alaska has a much higher income than other states, while having a lower population and temperature.
California has a much higher population than other states, while having a higher income and higher temperature.
These states are atypical compared with the other 48 observations.
High leverage observations may or may not be influential observations.
Influential observations are observations that have a disproportionate impact on the values of the model parameters.
Imagine finding that your model changes dramatically with the removal of a single observation.
It’s this concern that leads you to examine your data for influential points.
There are two methods for identifying influential observations: Cook’s distance, or D statistic and added variable plots.
Roughly speaking, Cook’s D values greater than 4/(n-k-1), where n is the sample size and k is the number of predictor variables, indicate influential observations.
You can create a Cook’s D plot (figure 8.14) with the following code:
The graph identifies Alaska, Hawaii, and Nevada as influential observations.
Deleting these states will have a notable impact on the values of the intercept and slopes in the.
Given a criterion of D=1, none of the observations in the dataset would appear to be influential.
Cook’s D plots can help identify influential observations, but they don’t provide information on how these observations affect the model.
For one response variable and k predictor variables, you’d create k added-variable plots as follows.
Added-variable plots can be created using the avPlots() function in the car package:
The graphs are produced one at a time, and users can click on points to identify them.
Press Esc, choose Stop from the graph’s menu, or right-click to move on to the next plot.
Figure 8.15 Added-variable plots for assessing the impact of influential observations.
The straight line in each plot is the actual regression coefficient for that predictor variable.
You can see the impact of influential observations by imagining how the line would change if the point representing that observation was deleted.
You can see that eliminating the point labeled Alaska would move the line in a negative direction.
You can combine the information from outlier, leverage, and influence plots into one highly informative plot using the influencePlot() function from the car package:
The resulting plot (figure 8.16) shows that Nevada and Rhode Island are outliers; New York, California, Hawaii, and Washington have high leverage; and Nevada, Alaska, and Hawaii are influential observations.
Observations depicted by large circles may have disproportionate influence on the parameters estimates of the model.
Having spent the last 20 pages learning about regression diagnostics, you may ask, “What do you do if you identify problems?” There are four approaches to dealing with violations of regression assumptions:
Deleting outliers can often improve a dataset’s fit to the normality assumption.
Influential observations are often deleted as well, because they have an inordinate impact on the results.
The largest outlier or influential observation is deleted, and the model is refit.
If there are still outliers or influential observations, the process is repeated until an acceptable fit is obtained.
Again, I urge caution when considering the deletion of observations.
Sometimes, you can determine that the observation is an outlier because of data errors in recording, or because a protocol wasn’t followed, or because a test subject misunderstood instructions.
In these cases, deleting the offending observation seems perfectly reasonable.
In other cases, the unusual observation may be the most interesting thing about the data you’ve collected.
Uncovering why an observation differs from the rest can contribute great insight to the topic at hand, and to other topics you might not have thought of.
Some of our greatest advances have come from the serendipity of noticing that something doesn’t fit our preconceptions (pardon the hyperbole)
When models don’t meet the normality, linearity, or homoscedasticity assumptions, transforming one or more variables can often improve or correct the situation.
Transformations typically involve replacing a variable Y with Y l.
Common values of l and their interpretations are given in table 8.5
If Y is a proportion, a logit transformation [ln (Y/1-Y)] is often used.
When the model violates the normality assumption, you typically attempt a transformation of the response variable.
You can use the powerTransform() function in the car package to generate a maximum-likelihood estimation of the power l most likely to normalize the variable X l.
In the next listing, this is applied to the states data.
The results suggest that you can normalize the variable Murder by replacing it with Murder0.6
This is consistent with the results of the Q-Q plot in figure 8.9
When the assumption of linearity is violated, a transformation of the predictor variables can often help.
The boxTidwell() function in the car package can be used to generate maximum-likelihood estimates of predictor powers that can improve linearity.
An example of applying the Box–Tidwell transformations to a model that predicts state murder rates from their population and illiteracy rates follows:
Again, these results are consistent with the component plus residual plots in figure 8.11
Finally, transformations of the response variable can help in situations of heteroscedasticity (nonconstant error variance)
You saw in listing 8.7 that the spreadLevelPlot() function in the car package offers a power transformation for improving homoscedasticity.
Again, in the case of the states example, the constant error variance assumption is met and no transformation is necessary.
Changing the variables in a model will impact the fit of a model.
Sometimes, adding an important variable will correct many of the problems that we’ve discussed.
Deleting variables is a particularly important approach for dealing with multicollinearity.
If your only goal is to make predictions, then multicollinearity isn’t a problem.
But if you want to make interpretations about individual predictor variables, then you must deal with it.
The most common approach is to delete one of the variables involved in the multicollinearity (that is, one of the variables with a vif >2 )
An alternative is to use ridge regression, a variant of multiple regression designed to deal with multicollinearity situations.
As you’ve just seen, one approach to dealing with multicollinearity is to fit a different type of model (ridge regression in this case)
If there are outliers and/or influential observations, you could fit a robust regression model rather than an OLS regression.
If you’ve violated the normality assumption, you can fit a nonparametric regression model.
If there’s significant nonlinearity, you can try a nonlinear regression model.
If you’ve violated the assumptions of independence of errors, you can fit a model that specifically takes the error structure into account, such as time-series models or multilevel regression models.
Finally, you can turn to generalized linear models to fit a wide range of models in situations where the assumptions of OLS regression don’t hold.
The decision regarding when to try to improve the fit of an OLS regression model and when to try a different approach, is a complex one.
It’s typically based on knowledge of the subject matter and an assessment of which approach will provide the best result.
Speaking of best results, let’s turn now to the problem of deciding which predictor variables to include in our regression model.
When developing a regression equation, you’re implicitly faced with a selection of many possible models.
Should you include all the variables under study, or drop ones.
There’s an old joke in statistics: If you can’t prove A, prove B and pretend it was A.
The relevance here is that if you transform your variables, your interpretations must be based on the transformed variables, not the original variables.
If the transformation makes sense, such as the log of income or the inverse of distance, the interpretation is easier.
But how do you interpret the relationship between the frequency of suicidal ideation and the cube root of depression? If a transformation doesn’t make sense, you should avoid it.
All things being equal, if you have two models with approximately equal predictive accuracy, you favor the simpler one.
The word “best” is in quotation marks, because there’s no single criterion you can use to make the decision.
The final decision requires judgment on the part of the investigator.
You can compare the fit of two nested models using the anova() function in the base installation.
A nested model is one whose terms are completely included in the other model.
In our states multiple regression model, we found that the regression coefficients for Income and Frost were nonsignificant.
You can test whether a model without these two variables predicts as well as one that includes them (see the following listing)
The anova() function provides a simultaneous test that Income and Frost add to linear prediction above and beyond Population and Illiteracy.
Because the test is nonsignificant (p = .994), we conclude that they don’t add to the linear prediction and we’re justified in dropping them from our model.
The Akaike Information Criterion (AIC) provides another method for comparing models.
The index takes into account a model’s statistical fit and the number of parameters needed to achieve this fit.
Models with smaller AIC values—indicating adequate fit with fewer parameters—are preferred.
The criterion is provided by the AIC() function (see the following listing)
The AIC values suggest that the model without Income and Frost is the better model.
Note that although the ANOVA approach requires nested models, the AIC approach doesn’t.
Comparing two models is relatively straightforward, but what do you do when there are four, or ten, or a hundred possible models to consider? That’s the topic of the next section.
Two popular approaches to selecting a final set of predictor variables from a larger pool of candidate variables are stepwise methods and all-subsets regression.
In stepwise selection, variables are added to or deleted from a model one at a time, until some stopping criterion is reached.
For example, in forward stepwise regression you add predictor variables to the model one at a time, stopping when the addition of variables would no longer improve the model.
In backward stepwise regression, you start with a model that includes all predictor variables, and then delete them one at a time until removing variables would degrade the quality of the model.
In stepwise stepwise regression (usually called stepwise to avoid sounding silly), you combine the forward and backward stepwise approaches.
Variables are entered one at a time, but at each step, the variables in the model are reevaluated, and those that don’t contribute to the model are deleted.
A predictor variable may be added to, and deleted from, a model several times before a final solution is reached.
The implementation of stepwise regression methods vary by the criteria used to enter or remove variables.
The stepAIC() function in the MASS package performs stepwise model selection (forward, backward, stepwise) using an exact AIC criterion.
In the next listing, we apply backward stepwise regression to the multiple regression problem.
For each step, the AIC column provides the model AIC resulting from the deletion of the variable listed in that row.
The AIC value for <none> is the model AIC if no variables are removed.
In the second step, Income is removed, decreasing the AIC to 93.76
Deleting any more variables would increase the AIC, so the process stops.
Although it may find a good model, there’s no guarantee that it will find the best model.
An approach that attempts to overcome this limitation is all subsets regression.
For example, if nbest=2, the two best one-predictor models are displayed, followed by the two best two-predictor models, followed by the two best three-predictor models, up to a model with all predictors.
All subsets regression is performed using the regsubsets() function from the leaps package.
You can choose R-squared, Adjusted R-squared, or Mallows Cp statistic as your criterion for reporting “best” models.
As you’ve seen, R-squared is the amount of variance accounted for in the response variable by the predictors variables.
Adjusted R-squared is similar, but takes into account the number of parameters in the model.
When the number of predictors is large compared to the sample size, this can lead to significant overfitting.
The Adjusted R-squared is an attempt to provide a more honest estimate of the population R-squared—one that’s less likely to take advantage of chance variation in the data.
The Mallows Cp statistic is also used as a stopping rule in stepwise regression.
It has been widely suggested that a good model is one in which the Cp statistic is close to the number of model parameters (including the intercept)
In listing 8.14, we’ll apply all subsets regression to the states data.
The results can be plotted with either the plot() function in the leaps package or the subsets() function in the car package.
Figure 8.17 Best four models for each subset size based on Adjusted R-square.
Here you see that a model with fewer predictors has a larger adjusted R-square (something that can’t happen with an unadjusted R-square)
The graph suggests that the two-predictor model (Population and Illiteracy) is the best.
In most instances, all subsets regression is preferable to stepwise regression, because more models are considered.
Figure 8.18 Best four models for each subset size based on the Mallows Cp statistic.
In general, automated variable selection methods should be seen as an aid rather than a directing force in model selection.
A well-fitting model that doesn’t make sense doesn’t help you.
Ultimately, it’s your knowledge of the subject matter that should guide you.
We’ll end our discussion of regression by considering methods for assessing model generalizability and predictor relative importance.
In the previous section, we examined methods for selecting the variables to include in a regression equation.
When description is your primary goal, the selection and interpretation of a regression model signals the end of your labor.
But when your goal is prediction, you can justifiably ask, “How well will this equation perform in the real world?”
By definition, regression techniques obtain model parameters that are optimal for a given set of data.
In OLS regression, the model parameters are selected to minimize the sum of squared errors of prediction (residuals), and conversely, maximize the amount of variance accounted for in the response variable (R-squared)
Because the equation has been optimized for the given set of data, it won’t perform as well with a new set of data.
We began this chapter with an example involving a research physiologist who wanted to predict the number of calories an individual will burn from the duration and intensity of their exercise, age, gender, and BMI.
If you fit an OLS regression equation to this data, you’ll obtain model parameters that uniquely maximize the R-squared for this particular set of observations.
But our researcher wants to use this equation to predict the calories burned by individuals in general, not only those in the original study.
You know that the equation won’t perform as well with a new sample of observations, but how much will you lose? Cross-validation is a useful method for evaluating the generalizability of a regression equation.
In cross-validation, a portion of the data is selected as the training sample and a portion is selected as the hold-out sample.
A regression equation is developed on the training sample, and then applied to the hold-out sample.
Because the hold-out sample wasn’t involved in the selection of the model parameters, the performance on this sample is a more accurate estimate of the operating characteristics of the model with new data.
In k-fold cross-validation, the sample is divided into k subsamples.
Each of the k subsamples serves as a hold-out group and the combined observations from the remaining k-1 subsamples serves as the training group.
The performance for the k prediction equations applied to the k hold-out samples are recorded and then averaged.
When k equals n, the total number of observations, this approach is called jackknifing.
You can perform k-fold cross-validation using the crossval() function in the bootstrap package.
The following listing provides a function (called shrinkage()) for cross-validating a model’s R-square statistic using k-fold cross-validation.
Using this listing you define your functions, create a matrix of predictor and predicted values, get the raw R-squared, and get the cross-validated R-squared.
The shrinkage() function is then used to perform a 10-fold cross-validation with the states data, using a model with all four predictor variables:
You can see that the R-square based on our sample (0.567) is overly optimistic.
A better estimate of the amount of variance in murder rates our model will account for with new data is the cross-validated R-square (0.448)
Note that observations are assigned to the k groups randomly, so you will get a slightly different result each time you execute the shrinkage() function.
You could use cross-validation in variable selection by choosing a model that demonstrates better generalizability.
This may make the two-predictor model a more attractive alternative.
All other things being equal, a regression equation that’s based on a larger training.
You’ll get less R-squared shrinkage and make more accurate predictions.
Up to this point in the chapter, we’ve been asking, “Which variables are useful for predicting the outcome?” But often your real interest is in the question, “Which variables are most important in predicting the outcome?” You implicitly want to rank-order the predictors in terms of relative importance.
There may be practical grounds for asking the second question.
For example, if you could rank-order leadership practices by their relative importance for organizational success, you could help managers focus on the behaviors they most need to develop.
If predictor variables were uncorrelated, this would be a simple task.
You would rank-order the predictor variables by their correlation with the response variable.
In most cases, though, the predictors are correlated with each other, and this complicates the task significantly.
There have been many attempts to develop a means for assessing the relative importance of predictors.
Standardized regression coefficients describe the expected change in the response variable (expressed in standard deviation units) for a standard deviation change in a predictor variable, holding the other predictor variables constant.
Note that because the scale() function returns a matrix and the lm() function requires a data frame, you convert between the two in an intermediate step.
The code and results for our multiple regression problem are shown here:
Here you see that a one standard deviation increase in illiteracy rate yields a 0.68 standard deviation increase in murder rate, when controlling for population, income, and temperature.
Using standardized regression coefficients as our guide, Illiteracy is the most important predictor and Frost is the least.
A function for generating relative weights is provided in the next listing.
In listing 8.17 the relweights() function is applied to the states data with murder rate predicted by the population, illiteracy, income, and temperature.
Based on the method of relative weights, Illiteracy has the greatest relative importance, followed by Frost, Population, and Income, in that order.
Relative importance measures (and in particular, the method of relative weights) have wide applicability.
They come much closer to our intuitive conception of relative importance than standardized regression coefficients do, and I expect to see their use increase dramatically in coming years.
Figure 8.19 Bar plot of relative weights for the states multiple regression problem.
Regression analysis is a term that covers a broad range of methodologies in statistics.
You’ve seen that it’s a highly interactive approach that involves fitting models, assessing their fit to statistical assumptions, modifying both the data and the models, and refitting to arrive at a final result.
In many ways, this final result is based on art and skill as much as science.
This has been a long chapter, because regression analysis is a process with many parts.
We’ve discussed fitting OLS regression models, using regression diagnostics to assess the data’s fit to statistical assumptions, and methods for modifying the data to meet these assumptions more closely.
We looked at ways of selecting a final regression model from many possible models, and you learned how to evaluate its likely performance on new samples of data.
Finally, we tackled the thorny problem of variable importance: identifying which variables are the most important for predicting an outcome.
In each of the examples in this chapter, the predictor variables have been quantitative.
However, there are no restrictions against using categorical variables as predictors as well.
Using categorical predictors such as gender, treatment type, or manufacturing process allows you to examine group differences on a response or outcome variable.
This chapter covers n Using R to model basic experimental designs.
In chapter 7, we looked at regression models for predicting a quantitative response variable from quantitative predictor variables.
But there’s no reason that we couldn’t have included nominal or ordinal factors as predictors as well.
When factors are included as explanatory variables, our focus usually shifts from prediction to understanding group differences, and the methodology is referred to as analysis of variance (ANOVA)
This chapter provides an overview of R functions for analyzing common research designs.
First we’ll look at design terminology, followed by a general discussion of R’s approach to fitting ANOVA models.
Then we’ll explore several examples that illustrate the analysis of common designs.
Along the way, we’ll treat anxiety disorders, lower blood cholesterol levels, help pregnant mice have fat babies, assure that pigs grow long in the tooth, facilitate breathing in plants, and learn which grocery shelves to avoid.
In addition to the base installation, we’ll be using the car, gplots, HH, rrcov, and mvoutlier packages in our examples.
Be sure to install them before trying out the sample code.
Experimental design in general, and analysis of variance in particular, has its own language.
Before discussing the analysis of these designs, we’ll quickly review some important terms.
We’ll use a series of increasingly complex study designs to introduce the most significant concepts.
Two popular therapies for anxiety are cognitive behavior therapy (CBT) and eye movement desensitization and reprocessing (EMDR)
You recruit 10 anxious individuals and randomly assign half of them to receive five weeks of CBT and half to receive five weeks of EMDR.
At the conclusion of therapy, each patient is asked to complete the State-Trait Anxiety Inventory (STAI), a self-report measure of anxiety.
In this design, Treatment is a between-groups factor with two levels (CBT, EMDR)
It’s called a between-groups factor because patients are assigned to one and only one group.
Because there are an equal number of observations in each treatment condition, you have a balanced design.
When the sample sizes are unequal across the cells of a design, you have an unbalanced design.
The statistical design in table 9.1 is called a one-way ANOVA because there’s a single classification variable.
Effects in ANOVA designs are primarily evaluated through F tests.
If the F test for Treatment is significant, you can conclude that the mean STAI scores for two therapies differed after five  weeks of treatment.
If you were interested in the effect of CBT on anxiety over time, you could place all 10 patients in the CBT group and assess them at the conclusion of therapy and again six months later.
Time is a within-groups factor with two levels (five  weeks, six months)
It’s called a within-groups factor because each patient is measured under both levels.
Because each subject is measured more than once, the design is also called repeated measures ANOVA.
If the F test for Time is significant, you can conclude that patients’ mean STAI scores changed between five weeks and six months.
If you were interested in both treatment differences and change over time, you could combine the first two study designs, and randomly assign five  patients to CBT and five  patients to EMDR, and assess their STAI results at the end of therapy (five weeks) and at six months (see table 9.3)
By including both Therapy and Time as factors, you’re able to examine the impact of Therapy (averaged across time), Time (averaged across therapy type), and the interaction of Therapy and Time.
The first two are called the main effects, whereas the interaction is (not surprisingly) called an interaction effect.
When you cross two or more factors, as you’ve done here, you have a factorial ANOVA design.
Crossing two factors produces a two-way ANOVA, crossing three factors produces a three-way ANOVA, and so forth.
When a factorial design includes both between-groups and within-groups factors, it’s also called a mixed-model ANOVA.
In this case you’ll have three F tests: one for Therapy, one for Time, and one for the Therapy x Time interaction.
A significant result for Therapy indicates that CBT and EMDR differ in their impact on anxiety.
Table 9.3 Two-way factorial ANOVA with one between-groups and one within-groups factor.
A significant Therapy x Time interaction indicates that the two treatments for anxiety had a differential impact over time (that is, the change in anxiety from five weeks to six months was different for the two treatments)
It’s known that depression can have an impact on therapy, and that depression and anxiety often co-occur.
Even though subjects were randomly assigned to treatment conditions, it’s possible that the two therapy groups differed in patient depression levels at the initiation of the study.
Any posttherapy differences might then be due to the preexisting depression differences and not to your experimental manipulation.
Because depression could also explain the group differences on the dependent variable, it’s a confounding factor.
And because you’re not interested in depression, it’s called a nuisance variable.
If you recorded depression levels using a self-report depression measure such as the Beck Depression Inventory (BDI) when patients were recruited, you could statistically adjust for any treatment group differences in depression before assessing the impact of therapy type.
In this case, BDI would be called a covariate, and the design would be called an analysis of covariance (ANCOVA)
Finally, you’ve recorded a single dependent variable in this study (the STAI)
You could increase the validity of this study by including additional measures of anxiety (such as family ratings, therapist ratings, and a measure assessing the impact of anxiety on their daily functioning)
When there’s more than one dependent variable, the design is called a multivariate analysis of variance (MANOVA)
If there are covariates present, it’s called a multivariate analysis of covariance (MANCOVA)
Now that you have the basic terminology under your belt, you’re ready to amaze your friends, dazzle new acquaintances, and discuss how to fit ANOVA/ANCOVA/ MANOVA models with R.
Although ANOVA and regression methodologies developed separately, functionally they’re both special cases of the general linear model.
However, we’ll primarily use the aov() function in this chapter.
The results of lm() and aov() are equivalent, but the aov() function presents these results in a format that’s more familiar to ANOVA methodologists.
For completeness, I’ll provide an example using lm()at the end of this chapter.
Table 9.4 describes special symbols that can be used in the formulas.
In this table, y is the dependent variable and the letters A, B, and C represent factors.
Separates response variables on the left from the explanatory variables on the right.
For example, a prediction of y from A, B, and C would be coded y ~ A + B + C.
A prediction of y from A, B, and the interaction between A and B would be coded y ~ A + B + A:B.
A place holder for all other variables in the data frame except the dependent variable.
For example, if a data frame contained the variables y, A, B, and C, then the code y ~
In this table, lowercase letters are quantitative variables, uppercase letters are grouping factors, and Subject is a unique identifier variable for subjects.
One-way ANCOVA with one covariate y ~ x + A.
Randomized Block y ~ B + A (where B is a blocking factor)
Repeated measures ANOVA with one within-groups factor (W) and one between-groups factor (B)
We’ll explore in-depth examples of several of these designs later in this chapter.
The order in which the effects appear in a formula matters when (a) there’s more than one factor and the design is unbalanced, or (b) covariates are present.
When either of these two conditions is present, the variables on the right side of the equation will be correlated with each other.
In this case, there’s no unambiguous way to divide up their impact on the dependent variable.
By default, R employs the Type I (sequential) approach to calculating ANOVA effects (see the sidebar “Order counts!”)
The first model can be written out as y ~ A + B + A:B.
When independent variables are correlated with each other or with covariates, there’s no unambiguous method for assessing the independent contributions of these variables to the dependent variable.
Consider an unbalanced two-way factorial design with factors A and B  and dependent variable y.
There are three effects in this design: the A and B main effects and the A x B interaction.
Type I (sequential) Effects are adjusted for those that appear earlier in the formula.
Type II (hierarchical) Effects are adjusted for other effects at the same or lower level.
The A:B interaction is adjusted for both A and B.
Type III (marginal) Each effect is adjusted for every other effect in the model.
Other programs such as SAS and SPSS employ the Type III approach by default.
The greater the imbalance in sample sizes, the greater the impact that the order of the terms will have on the results.
In general, more fundamental effects should be listed earlier in the formula.
In particular, covariates should be listed first, followed by main effects, followed by two-way interactions, followed by three-way interactions, and so on.
For main effects, more fundamental variables should be listed first.
Here’s the bottom line: When the research design isn’t orthogonal (that is, when the factors and/or covariates are correlated), be careful when specifying the order of effects.
Before moving on to specific examples, note that the Anova() function in the car package (not to be confused with the standard anova() function) provides the option of using the Type II or Type III approach, rather than the Type I approach used by the aov() function.
You may want to use the Anova() function if you’re concerned about matching your results to those provided by other packages such as SAS and SPSS.
In a one-way ANOVA, you’re interested in comparing the dependent variable means of two or more groups defined by a categorical grouping factor.
The two remaining conditions (drugD and drugE) represented competing drugs.
Which drug regimen produced the greatest cholesterol reduction (response)? The analysis is provided in the following listing.
The ANOVA F test for treatment tells you that the five drug regiments aren’t equally effective, but it doesn’t tell you which treatments differ from one another.
You can use a multiple comparison procedure to answer this question.
For example, the TukeyHSD() function provides a test of all pairwise differences between group means (see listing 9.2)
Note that the TukeyHSD() function has compatibility issues with package HH also used in this chapter; if HH is loaded, TukeyHSD() will fail.
The first par statement rotates the axis labels, and the second one increases the left margin area so that the labels fit (par options are covered in chapter 3)
The glht() function in the multcomp package provides a much more comprehensive set of methods for multiple mean comparisons that you can use for both linear models (such as those described in this chapter) and generalized linear models (covered in chapter 13)
The following code reproduces the Tukey HSD test, along with a different graphical representation of the results (figure 9.3):
In this code, the par statement increased the top margin to fit the letter array.
Groups (represented by box plots) that have the same letter don’t have significantly different means.
Figure 9.3 Tukey HSD tests provided by the multcomp package.
It also has the advantage of providing information on the distribution of scores within each group.
But competitor drugE was superior to both drugD and all three dosage strategies for our focus drug.
Multiple comparisons methodology is a complex and rapidly changing area of study.
As we saw in the previous chapter, our confidence in results depends on the degree to which our data satisfies the assumptions underlying the statistical tests.
In a one-way ANOVA, the dependent variable is assumed to be normally distributed, and have equal variance in each group.
You can use a Q-Q plot to assess the normality assumption:
The data fall within the 95 percent confidence envelope, suggesting that the.
For example, you can perform Bartlett’s test with this code:
Bartlett’s test indicates that the variances in the five groups don’t differ significantly (p = 0.97)
Other possible tests include the Fligner–Killeen test (provided by the fligner.test() function), and the Brown–Forsythe test (provided by the hov() function in the HH package)
Although not shown, the other two tests reach the same conclusion.
Finally, analysis of variance methodologies can be sensitive to the presence of outliers.
Taking the Q-Q plot, Bartlett’s test, and outlier test together, the data appear to fit the ANOVA model quite well.
This, in turn, adds to your confidence in the results.
A one-way analysis of covariance (ANCOVA) extends the one-way ANOVA to include one or more quantitative covariates.
This example comes from the litter dataset in the multcomp package (see Westfall et al., 1999)
The mean post-birth weight for each litter was the dependent variable and gestation time was included as a covariate.
Based on the group means provided by the aggregate() function, the no-drug group had the highest mean litter weight (32.3)
The ANCOVA F tests indicate that (a) gestation time was related to birth weight, and (b) drug dosage was related to birth weight after controlling for gestation time.
The mean birth weight isn’t the same for each of the drug dosages, after controlling for gestation time.
Because you’re using a covariate, you may want to obtain adjusted group meansthat is, the group means obtained after partialing out the effects of the covariate.
In this case, the adjusted means are similar to the unadjusted means produced by the aggregate() function, but this won’t always be the case.
The effects package provides a powerful method of obtaining adjusted means for complex research designs and presenting them visually.
As with the one-way ANOVA example in the last section, the F test for dose indicates that the treatments don’t have the same mean birth weight, but it doesn’t tell you which means differ from one another.
Again you can use the multiple comparison procedures provided by the multcomp package to compute all pairwise mean comparisons.
Additionally, the multcomp package can be used to test specific userdefined hypotheses about the means.
Suppose you’re interested in whether the no-drug condition differs from the threedrug condition.
The code in the following listing can be used to test this hypothesis.
Therefore, you can conclude that the no-drug group has a higher birth weight than drug conditions.
Other contrasts can be added to the rbind() function (see help(glht) for details)
In addition, standard ANCOVA designs assumes homogeneity of regression slopes.
In this case, it’s assumed that the regression slope for predicting birth weight from gestation time is the same in each of the four treatment groups.
A test for the homogeneity of regression slopes can be obtained by including a gestation*dose interaction term in your ANCOVA model.
A significant interaction would imply that the relationship between gestation and birth weight depends on the level of the dose variable.
The code and results are provided in the following listing.
The interaction is nonsignificant, supporting the assumption of equality of slopes.
If the assumption is untenable, you could try transforming the covariate or dependent variable, using a model that accounts for separate slopes, or employing a nonparametric ANCOVA method that doesn’t require homogeneity of regression slopes.
See the sm.ancova() function in the sm package for an example of the latter.
The ancova() function in the HH package provides a plot of the relationship between the dependent variable, the covariate, and the factor.
Note: the figure has been modified to display better in black and white and will look slightly different when you run the code yourself.
Here you can see that the regression lines for predicting birth weight from gestation time are parallel in each group but have different intercepts.
The lines are parallel because you’ve specified them to be.
This approach is useful for visualizing the case where the homogeneity of regression slopes doesn’t hold.
Figure 9.5 Plot of the relationship between gestation time and birth weight for each of four drug treatment groups.
This example uses the ToothGrowth dataset in the base installation to demonstrate a two-way between-groups ANOVA.
The table statement indicates that you have a balanced design (equal sample sizes in each cell of the design), and the aggregate statements provide the cell means and standard deviations.
The ANOVA table provided by the summary() function indicates that both main effects (supp and dose) and the interaction between these factors are significant.
Figure 9.6 Interaction between dose and delivery mechanism on tooth growth.
The plot provides the mean tooth length for each supplement at each dosage.
With a little finesse, you can get an interaction plot out of the plotmeans() function in the gplots package.
The graph includes the means, as well as error bars (95 percent confidence intervals) and  sample sizes.
Again, this figure has been modified to display more clearly in black and white and will look slightly different when you run the code yourself.
All three graphs indicate that tooth growth increases with the dose of ascorbic acid for both orange juice and Vitamin C.
Figure 9.7 Interaction between dose and delivery mechanism on tooth growth.
The mean plot with 95 percent confidence intervals was created by the plotmeans() function.
For 2mg of ascorbic acid, both delivery methods produced identical growth.
Of the three plotting methods provided, I prefer the interaction2wt() function in the HH package.
Although I don’t cover the tests of model assumptions and mean comparison procedures, they’re a natural extension of the methods you’ve seen so far.
Additionally, the design is balanced, so you don’t have to worry about the order of effects.
Figure 9.8 Main effects and two-way interaction for the ToothGrowth dataset.
In repeated measures ANOVA, subjects are measured more than once.
This section focuses on a repeated measures ANOVA with one within-groups and one between-groups factor (a common design)
We’ll take our example from the field of physiological ecology.
Physiological ecologists study how the physiological and biochemical processes of living systems respond to variations in environmental factors (a crucial area of study given the realities of global warming)
The photosynthetic rates of chilled plants were compared with the photosynthetic rates of nonchilled plants at several ambient CO2 concentrations.
Half the plants were from Quebec and half were from Mississippi.
Type is a between-groups factor and conc is a withingroups factor.
Listing 9.7 Repeated measures ANOVA with one between- and within-groups factor.
The ANOVA table indicates that the Type and concentration main effects and the Type x concentration interaction are all significant at the 0.01 level.
In order to demonstrate a different presentation of the interaction, the boxplot() function is used to plot the same data.
From either graph, you can see that there’s a greater carbon dioxide uptake in plants from Quebec compared to Mississippi.
The difference is more pronounced at higher ambient CO2 concentrations.
The litter data frame from section 9.4 is a good example.
When dealing with repeated measures designs, you typically need the data in long format before fitting your models.
In long format, each measurement of the dependent variable is placed in its own row.
The CO2 example in this section was analyzed using a traditional repeated measures ANOVA.
The approach assumes that the covariance matrix for any within-groups factor follows a specified form known as sphericity.
Specifically, it assumes that the variances of the differences between any two levels of the within-groups factor are equal.
In real-world data, it’s unlikely that this assumption will be met.
This has led to a number of alternative approaches, including the following:
Coverage of these approaches is beyond the scope of this text.
If you’re interested in learning more, check out Pinheiro and Bates (2000) and Zuur et al.
Up to this point, all the methods in this chapter have assumed that there’s a single dependent variable.
In the next section, we’ll briefly consider designs that include more than one outcome variable.
The following example is based on the UScereal dataset in the MASS package.
This listing uses the cbind() function to form a matrix of the three dependent variables (calories, fat, and sugars)
The aggregate() function provides the shelf means, and the cov() function provides the variance and the covariances across cereals.
The manova() function provides the multivariate test of group differences.
The significant F value indicates that the three groups differ on the set of nutritional measures.
Because the multivariate test is significant, you can use the summary.aov() function to obtain the univariate one-way ANOVAs.
Here, you see that the three groups differ on each nutritional measure considered separately.
Finally, you can use a mean comparison procedure (such as TukeyHSD) to determine which shelves differ from each other for each of the three dependent variables (omitted here to save space)
The first assumption states that the vector of dependent variables jointly follows a multivariate normal distribution.
You can use a Q-Q plot to assess this assumption (see the sidebar “A Theory Interlude” for a statistical explanation of how this works)
The code is provided in the following listing and the resulting graph is displayed in figure 9.11
If the data follow a multivariate normal distribution, then points will fall on the line.
The identify() function allows you to interactively identify points in the graph.
Here, the dataset appears to violate multivariate normality, primarily due to the observations for Wheaties Honey Gold and Wheaties.
You may want to delete these two cases and rerun the analyses.
The assumption is usually evaluated with a Box’s M test.
Unfortunately, the test is sensitive to violations of normality, leading to rejection in most typical cases.
Finally, you can test for multivariate outliers using the aq.plot() function in the mvoutlier package.
A robust version of the one-way MANOVA is provided by the Wilks.test() function in the rrcov package.
The adonis() function in the vegan package can provide the equivalent of a nonparametric MANOVA.
From the results, you can see that using a robust test that’s insensitive to both outliers and violations of MANOVA assumptions still indicates that the cereals on the top, middle, and bottom store shelves differ in their nutritional profiles.
In section 9.2, we noted that ANOVA and regression are both special cases of the same general linear model.
As such, the designs in this chapter could have been analyzed using the lm() function.
However, in order to understand the output, you need to understand how R deals with categorical variables when fitting models.
In this case you get the results shown in the next listing.
What are we looking at? Because linear models require numeric predictors, when the lm() function encounters a factor, it replaces that factor with a set of numeric variables representing contrasts among the levels.
If the factor has k levels, k-1 contrast variables will be created.
You can also create your own (we won’t cover that here)
By default, treatment contrasts are used for unordered factors and orthogonal polynomials are used for ordered factors.
Also called deviation contrasts, they compare the mean of each level to the overall mean across levels.
This produces coefficients similar to contrasts used in most SAS procedures.
With treatment contrasts, the first level of the factor becomes the reference group and each subsequent level is compared with it.
You don’t need a variable for the first group, because a zero on each of the four indicator variables uniquely determines that the patient is in the 1times condition.
You can see from the probability values in the output that each drug condition is significantly different from the first (1time)
You can change the default contrasts used in lm() by specifying a contrasts option.
You can change the default contrasts used during an R session via the options() function.
Although we’ve limited our discussion to the use of contrasts in linear models, note that they’re applicable to other modeling functions in R.
We reviewed the basic terminology used, and looked at examples of between and within-groups designs, including the one-way ANOVA, one-way ANCOVA, two-way factorial ANOVA, repeated measures ANOVA, and one-way MANOVA.
In addition to the basic analyses, we reviewed methods of assessing model assumptions and applying multiple comparison procedures following significant omnibus tests.
Finally, we explored a wide variety of methods for displaying the results visually.
If you’re interested in learning more about the design of experiments (DOE) using R, be sure to see the CRAN View provided by Groemping (2009)
In the next chapter, we’ll address issues of power analysis.
Power analysis helps us to determine the sample sizes needed to detect an effect of a given size with a given degree of confidence, and is a crucial component of research design.
Power analysis allows you to determine the sample size required to detect an effect of a given size with a given degree of confidence.
Conversely, it allows you to determine the probability of detecting an effect of a given size with a given level of confidence, under sample size constraints.
If the probability is unacceptably low, you’d be wise to alter or abandon the experiment.
In this chapter, you’ll learn how to conduct power analyses for a variety of statistical tests, including tests of proportions, t-tests, chi-square tests, balanced oneway ANOVA, tests of correlations, and linear models.
Because power analysis applies to hypothesis testing situations, we’ll start with a brief review of null hypothesis significance testing (NHST)
Then we’ll review conducting power analyses within R, focusing primarily on the pwr package.
Finally, we’ll consider other approaches to power analysis available with R.
To help you understand the steps in a power analysis, we’ll briefly review statistical hypothesis testing in general.
If you have a statistical background, feel free to skip to section 10.2
In statistical hypothesis testing, you specify a hypothesis about a population parameter (your null hypothesis, or H0)
You then draw a sample from this population and calculate a statistic that’s used to make inferences about the population parameter.
Assuming that the null hypothesis is true, you calculate the probability of obtaining the observed sample statistic or one more extreme.
If the probability is sufficiently small, you reject the null hypothesis in favor of its opposite (referred to as the alternative or research hypothesis, H1)
A sample of individuals is selected and randomly assigned to one of two conditions.
In the first condition, participants react to a series of driving challenges in a simulator while talking on a cell phone.
In the second condition, participants complete the same series of challenges but without a cell phone.
Based on the sample data, you can calculate the statistic.
If the null hypothesis is true and you can assume that reaction times are normally distributed, this sample statistic will follow a t distribution with 2n-2 degrees of freedom.
Using this fact, you can calculate the probability of obtaining a sample statistic this large or larger.
This predetermined cutoff (0.05) is called the significance level of the test.
Note that you use sample data to make an inference about the population it’s drawn from.
Your null hypothesis is that the mean reaction time of all drivers talking on cell phones isn’t different from the mean reaction time of all drivers who aren’t talking on cell phones, not just those drivers in your sample.
The four possible outcomes from your decision are as follows:
You’ve correctly determined that reaction time is affected by cell phone use.
You’ve concluded that cell phone use affects reaction time when it doesn’t.
Cell phone use affects reaction time, but you’ve failed to discern this.
Each of these outcomes is illustrated in the table below.
Null hypothesis significance testing is not without controversy and detractors have raised numerous concerns about the approach, particularly as practiced in the field of psychology.
They point to a widespread misunderstanding of p values, reliance on statistical significance over practical significance, the fact that the null hypothesis is never exactly true and will always be rejected for sufficient sample sizes, and a number of logical inconsistencies in NHST practices.
An in-depth discussion of this topic is beyond the scope of this book.
Interested readers are referred to Harlow, Mulaik, and Steiger (1997)
In planning research, the researcher typically pays special attention to four quantities: sample size, significance level, power, and effect size (see figure 10.1)
The significance level can also be thought of as the probability of finding an effect that is not there.
Power can be thought of as the probability of finding an effect that is there.
The formula for effect size depends on the statistical methodology employed in the hypothesis testing.
Figure 10.1 Four primary quantities considered in a study design power analysis.
Although the sample size and significance level are under the direct control of the researcher, power and effect size are affected more indirectly.
For example, as you relax the significance level (in other words, make it easier to reject the null hypothesis), power increases.
Your research goal is typically to maximize the power of your statistical tests while maintaining an acceptable significance level and employing as small a sample size as possible.
That is, you want to maximize the chances of finding a real effect and minimize the chances of finding an effect that isn’t really there, while keeping study costs within reason.
The four quantities (sample size, significance level, power, and effect size) have an intimate relationship.
We’ll use this fact to carry out various power analyses throughout the remainder of the chapter.
In the next section, we’ll look at ways of implementing power analyses using the R package pwr.
Later, we’ll briefly look at some highly specialized power functions that are used in biology and genetics.
Of the four quantities, effect size is often the most difficult to specify.
Calculating effect size typically requires some experience with the measures involved and knowledge of past research.
But what can you do if you have no clue what effect size to expect in a given study? You’ll look at this difficult question in section 10.2.7
In the remainder of this section, you’ll look at the application of pwr functions to common statistical tests.
Before invoking these functions, be sure to install and load the pwr package.
When the statistical test to be used is a t-test, the pwr.t.test() function provides a number of useful power analysis options.
Continuing the cell phone use and driving reaction time experiment from section 10.1, assume that you’ll be using a two-tailed independent sample t-test to compare the mean reaction time for participants in the cell phone condition with the mean reaction time for participants driving unencumbered.
Let’s assume that you know from past experience that reaction time has a standard deviation of 1.25 seconds.
Also suppose that a 1-second difference in reaction time is considered an important difference.
Assume that in comparing the two conditions you want to be able to detect a 0.5 standard deviation difference in population means.
Additionally, you can only afford to include 40 participants in the study.
What’s the probability that you’ll be able to detect a difference between the population means that’s this large, given the constraints outlined?
Assuming that an equal number of participants will be placed in each condition, you have.
Conversely, there’s a 86 percent chance that you’ll miss the effect that you’re looking for.
You may want to seriously rethink putting the time and effort into the study as it stands.
The previous examples assumed that there are equal sample sizes in the two groups.
If the sample sizes for the two groups are unequal, the function.
Try varying the values input to the pwr.t2n.test function and see the effect on the output.
For a one-way ANOVA, effect size is measured by f, where.
The pwr.r.test() function provides a power analysis for tests of correlation coefficients.
For example, let’s assume that you’re studying the relationship between depression and loneliness.
For linear models (such as multiple regression), the pwr.f2.test() function can be used to carry out a power analysis.
R2AB = variance accounted for in the population by variable set A and B together.
The first formula for f2 is appropriate when you’re evaluating the impact of a set of predictors on an outcome.
The second formula is appropriate when you’re evaluating the impact of one set of predictors above and beyond a second set of predictors (or covariates)
Let’s say you’re interested in whether a boss’s leadership style impacts workers’ satisfaction above and beyond the salary and perks associated with the job.
Leadership style is assessed by four variables, and salary and perks are associated with three variables.
Past experience suggests that salary and perks account for roughly 30 percent of the variance in worker satisfaction.
From a practical standpoint, it would be interesting if leadership style accounted for at least 5 percent above this figure.
In multiple regression, the denominator degrees of freedom equals N-k-1, where N is the number of observations and k is the number of predictors.
The pwr.2p.test() function can be used to perform a power analysis when comparing two proportions.
The alternative= option can be used to specify a two-tailed ("two.sided") or onetailed ("less" or "greater") test.
Let’s say that you suspect that a popular medication relieves symptoms in 60 percent of users.
A new (and more expensive) medication will be marketed if it improves symptoms in 65 percent of users.
How many participants will you need to include in a study comparing these two medications if you want to detect a difference this large?
You’ll use a one-tailed test because you’re only interested in assessing whether the new drug is better than the standard.
Difference of proportion power calculation for binomial distribution (arcsine transformation)
Chi-square tests are often used to assess the relationship between two categorical variables.
The null hypothesis is typically that the variables are independent versus a research hypothesis that they aren’t.
The summation goes from 1 to m, where m is the number of cells in the contingency table.
The function ES.w2(P) can be used to calculate the effect size corresponding.
As a simple example, let’s assume that you’re looking the relationship between ethnicity and promotion.
Your research hypothesis is that the probability of promotion follows the values in table 10.2
Table 10.2 Proportion of individuals expected to be promoted based on the research hypothesis.
Using this information, you can calculate the necessary sample size like this:
The results suggest that a study with 369 participants will be adequate to detect a relationship between ethnicity and promotion given the effect size, power, and significance level specified.
In power analysis, the expected effect size is the most difficult parameter to determine.
It typically requires that you have experience with the subject matter and the measures employed.
For example, the data from past studies can be used to calculate effect sizes, which can then be used to plan future studies.
But what can you do when the research situation is completely novel and you have no past experience to call upon? In the area of behavioral sciences, Cohen (1988) attempted to provide benchmarks for “small,” “medium,” and “large” effect sizes for various statistical tests.
Statistical method Effect size measures Suggested guidelines for effect size.
When you have no idea what effect size may be present, this table may provide some guidance.
Given the sample size limitations, you’re only likely to find an effect if it’s large.
It’s important to keep in mind that Cohen’s benchmarks are just general suggestions derived from a range of social research studies and may not apply to your particular field of research.
An alternative is to vary the study parameters and note the impact on such things as sample size and power.
For example, again assume that you want to compare five groups using a one-way ANOVA and a 0.05 significance level.
The following listing computes the sample sizes needed to detect a range of effect sizes and plots the results in figure 10.2
Listing 10.1 Sample sizes for detecting significant effects in a one-way ANOVA.
Graphs such as these can help you estimate the impact of various conditions on your experimental design.
For example, there appears to be little bang for the buck increasing the sample size above 200 observations per group.
We’ll look at another plotting example in the next section.
Before leaving the pwr package, let’s look at a more involved graphing example.
Suppose you’d like to see the sample size necessary to declare a correlation coefficient statistically significant for a range of effect sizes and power levels.
You can use the pwr.r.test() function and for loops to accomplish this task, as shown in the following listing.
Listing 10.2 Sample size curves for detecting correlations of various sizes.
With simple modifications, the same approach can be used to create sample size and power curve graphs for a wide range of statistical tests.
We’ll close this chapter by briefly looking at other R functions that are useful for power analysis.
Figure 10.3 Sample size curves for detecting a significant correlation at various power levels.
There are several other packages in R that can be useful in the planning stages of studies.
The piface package (see figure 10.4) provides a Java GUI for sample-size methods that interfaces with R.
The GUI allows the user to vary study parameters interactively and see their impact on other parameters.
Although the package is described as Pre-Alpha, it’s definitely worth checking out.
You can download the package source and binaries for Windows and Mac OS X from http://r-forge.r-project.org/projects/piface/
The package is particularly useful for exploring the impact of changes in sample size, effect size, significance levels, and desired power on the other parameters.
Other packages related to power analysis are described in table 10.4
The last five are particularly focused on power analysis in genetic studies.
Genome-wide association studies (GWAS) are studies used to identify genetic associations with observable traits.
For example, these studies would focus on why some people get a specific type of heart disease.
Finally, the MBESS package contains a wide range of functions that can be used for various forms of power analysis.
The functions are particularly relevant for researchers in the behavioral, educational, and social sciences.
In this chapter, we focused on the planning stages of such research.
Power analysis helps you to determine the sample sizes needed to discern an effect of a given size with a given degree of confidence.
It can also tell you the probability of detecting such an effect for a given sample size.
You can directly see the tradeoff between limiting the likelihood of wrongly declaring an effect significant (a Type I error) with the likelihood of rightly identifying a real effect (power)
The bulk of this chapter has focused on the use of functions provided by the pwr package.
These functions can be used to carry out power and sample size determinations for common statistical methods (including t-tests, chi-square tests, and tests of proportions, ANOVA, and regression)
Pointers to more specialized methods were provided in the final section.
The investigator varies the parameters of sample size, effect size, desired significance level, and desired power to observe their impact on each other.
The results are used to plan studies that are more likely to yield meaningful results.
Information from past research (particularly regarding effect sizes) can be used to design more effective and efficient future research.
An important side benefit of power analysis is the shift that it encourages, away from a singular focus on binary hypothesis testing (that is, does an effect exists or not), toward an appreciation of the size of the effect under consideration.
Journal editors are increasingly requiring authors to include effect sizes as well as p values when reporting research results.
This helps you to determine both the practical implications of the research and provides you with information that can be used to plan future studies.
In the next chapter, we’ll look at additional and novel ways to visualize multivariate relationships.
In chapter 6 (basic graphs), we considered a wide range of graph types for displaying the distribution of single categorical or continuous variables.
Chapter 8 (regression) reviewed graphical methods that are useful when predicting a continuous outcome variable from a set of predictor variables.
In chapter 9 (analysis of variance), we considered techniques that are particularly useful for visualizing how groups differ on a continuous outcome variable.
In many ways, the current chapter is a continuation and extension of the topics  covered so far.
In this chapter, we’ll focus on graphical methods for displaying relationships between two variables (bivariate relationships) and between many variables (multivariate relationships)
What’s the relationship between automobile mileage and car weight? Does it vary by the number of cylinders the car has?
How can you picture the relationships among an automobile’s mileage, weight, displacement, and rear axle ratio in a single graph?
When plotting the relationship between two variables drawn from a large dataset (say 10,000 observations), how can you deal with the massive overlap of data points you’re likely to see? In other words, what do you do when your graph is one big smudge?
How can you visualize the multivariate relationships among three variables at once (given a 2D computer screen or sheet of paper, and a budget slightly less than that for Avatar)?
These are the types of questions that can be answered with the methods described in this chapter.
The datasets that we’ll use are examples of what’s possible.
If the topic of automobile characteristics or tree growth isn’t interesting to you, plug in your own data!
These approaches are well known and widely used in research.
Next, we’ll review the use of correlograms for visualizing correlations and mosaic plots for visualizing multivariate relationships among categorical variables.
These approaches are also useful but much less well known among researchers and data analysts.
You’ll see examples of how you can use each of these approaches to gain a better understanding of your data and communicate these findings to others.
As    you’ve seen in previous chapters, scatter plots describe the relationship between two continuous variables.
In this section, we’ll start with a depiction of a single bivariate relationship (x versus y)
We’ll then explore ways to enhance this plot by superimposing additional information.
Next, we’ll learn how to combine several scatter plots into a scatter plot matrix so that you can view many bivariate relationships at once.
We’ll also review the special case where many data points overlap, limiting our ability to picture the data, and we’ll discuss a number of ways around this difficulty.
Finally, we’ll extend the two-dimensional graph to three dimensions, with the addition of a third continuous variable.
Each can help you understand the multivariate relationship among three variables at once.
The basic function for creating a scatter plot in R is plot(x, y), where x and y are numeric vectors denoting the (x, y) points to plot.
The code in listing 11.1 attaches the mtcars data frame and creates a basic.
As expected, as car weight increases, miles per gallon decreases, though the relationship isn’t perfectly linear.
The a bline() function is used to add a linear line of best fit, while the l owess() function is used to add a smoothed line.
This smoothed line is a nonparametric fit line based on locally weighted polynomial regression.
Figure 11.1 Scatter plot of car mileage versus weight, with superimposed linear and lowess fit lines.
The loess() function is a newer, formula-based version of lowess() and is more powerful.
The two functions have different defaults, so be careful not to confuse them.
The s catterplot() function in the c ar package offers many enhanced features and convenience functions for producing scatter plots, including fit lines, marginal box plots, confidence ellipses, plotting by subgroups, and interactive point identification.
For example, a more complex version of the previous plot is produced by the following code:
Here, the scatterplot() function is used to plot miles per gallon versus weight for automobiles that have four, six, or eight cylinders.
By default, subgroups are differentiated by color and plotting symbol, and separate linear and loess lines are fit.
By default, the loess fit requires five unique data points, so no smoothed fit is plotted for sixcylinder cars.
The i d.method option indicates that points will be identified interactively by mouse clicks, until the user selects Stop (via the Graphics or context-sensitive menu) or the Esc key.
The l abels option indicates that points will be identified with their row names.
Here you see that the Toyota Corolla and Fiat 128 have unusually good gas mileage, given their weights.
The l egend.plot option adds a legend to the upper-left margin and marginal box plots.
Figure 11.2 Scatter plot with subgroups and separately estimated fit lines.
The scatterplot() function has many features worth investigating, including robust options and data concentration ellipses not covered here.
Scatter plots help you visualize relationships between quantitative variables, two at a time.
But what if you wanted to look at the bivariate relationships between automobile mileage, weight, displacement (cubic inch), and rear axle ratio? One way is to arrange these six scatter plots in a matrix.
When there are several quantitative variables, you can represent their relationships in a scatter plot matrix, which is covered next.
There    are at least four useful functions for creating scatter plot matrices in R.
Analysts must love scatter plot matrices! A basic scatter plot matrix can be created with the p airs() function.
The following code produces a scatter plot matrix for the variables mpg, disp, drat, and wt:
All the variables on the right of the ~ are included in the plot.
Figure 11.3 Scatter plot matrix created by the pairs() function.
Here you can see the bivariate relationship among all the variables specified.
For example, the scatter plot between mpg and disp is found at the row and column intersection of those two variables.
Note that the six scatter plots below the principal diagonal are the same as those above the diagonal.
By adjusting the options, you could display just the lower or upper triangle.
For example, the option upper.panel=NULL would produce a graph with just the lower triangle of plots.
The s catterplotMatrix() function in the c ar package can also produce scatter plot matrices and can optionally do the following:
Here you can see that linear and smoothed (loess) fit lines are added by default and that kernel density and rug plots are.
The graph includes kernel density and rug plots in the principal diagonal and linear and loess fit lines.
The s pread=FALSE option suppresses lines showing spread and asymmetry, and the  lty.smooth=2 option displays the loess fit lines using dashed rather than solid lines.
Here, you change the kernel density plots to histograms and condition the results on the number of cylinders for each car.
By default, the regression lines are fit for the entire sample.
Including the option by.groups = TRUE would have produced separate fit lines by subgroup.
An interesting variation on the scatter plot matrix is provided by the c pairs() function in the g clus package.
The graph includes histograms in the principal diagonal and linear and loess fit lines.
Additionally, subgroups (defined by number of cylinders) are indicated by symbol type and color.
The function can also color-code the cells to reflect the size of these correlations.
The lowest correlation is between miles per gallon and rear axle ratio (0.68)
You can reorder and color the scatter plot matrix among these variables using the code in the following listing.
Listing 11.2 Scatter plot matrix produced with the gclus package.
The code in listing 11.2 uses the d mat.color(), o rder.single(), and c pairs() functions from the g clus package.
First, you select the desired variables from the mtcars data frame and calculate the absolute values of the correlations among them.
Next, you obtain the colors to plot using the dmat.color() function.
Given a symmetric matrix (a correlation matrix in this case), dmat.color() returns a matrix of colors.
The order.single() function sorts objects so that similar object pairs are adjacent.
In this case, the variable ordering is based on the similarity of the correlations.
Finally, the scatter plot matrix is plotted and colored using the new ordering (myorder) and the color list (mycolors)
The gap option adds a small space between cells of the matrix.
You can see from the figure that the highest correlations are between weight and displacement and weight and miles per gallon (red and closest to the principal diagonal)
The lowest correlation is between rear axle ratio and miles per gallon.
Figure 11.6 Scatter plot matrix produced with the cpairs() function in the gclus package.
Variables closer to the principal diagonal are more highly correlated.
When    there’s a significant overlap among data points, scatter plots become less useful for observing relationships.
Consider the following contrived example with 10,000 observations falling into two overlapping clusters of data:
Note that the overlap of data points makes it difficult to discern where the concentration of data is greatest.
If you generate a standard scatter plot between these variables using the following code.
The overlap of data points in figure 11.7 makes it difficult to discern the relationship.
They include the use of binning, color, and transparency to indicate the number of overprinted data points at any point on the graph.
The s moothScatter() function uses a kernel density estimate to produce smoothed color density representations of the scatterplot.
Using a different approach, the h exbin() function in the h exbin package provides.
Figure 11.8 Scatterplot using smoothScatter() to plot smoothed density estimates.
Finally, the i plot() function in the I DPmisc package can be used to display density.
Figure 11.9 Scatter plot using hexagonal binning to display the number of observations at each point.
Data concentrations are easy to see and counts can be read from the legend.
It’s useful to note that the s moothScatter() function in the b ase package, along with the i pairs() function in the I DPmisc package, can be used to create readable scatter plot matrices for large datasets as well.
What if you want to visualize the interaction of three quantitative variables at once? In this case, you can use a 3D scatter plot.
For example, say that you’re interested in the relationship between automobile mileage, weight, and displacement.
The scatterplot3d() function offers many options, including the ability to specify symbols, axes, colors, lines, grids, highlighting, and angles.
As a final example, let’s take the previous graph and add a regression plane.
The graph allows you to visualize the prediction of miles per gallon from automobile weight and displacement using a multiple regression equation.
The plane represents the predicted values, and the points are the actual values.
The vertical distances from the plane to the points are the residuals.
Points that lie above the plane are underpredicted, while points that lie below the line are over-predicted.
Three-dimensional    scatter plots are much easier to interpret if you can interact with them.
It creates a spinning 3D scatter plot that can be rotated with the mouse.
You can also add options like col and size to control the color and size of the points, respectively.
You should get a graph like the one depicted in figure 11.14
I think that you’ll find that being able to rotate the scatter plot in three dimensions makes the graph much easier to understand.
I’ll have more to say about the Rcmdr package    in    appendix A.
In    the previous section, you displayed the relationship between three quantitative variables using a 3D scatter plot.
Another approach is to create a 2D scatter plot and use the size of the plotted point to represent the value of the third variable.
You can create a bubble plot using the s ymbols() function.
This function can be used to draw circles, squares, stars, thermometers, and box plots at a specified set of (x, y) coordinates.
Let’s apply this to the mtcars data, plotting car weight on the x-axis, miles per.
The option inches is a scaling factor that can be used to control the size of the circles (the default is to make the largest circle 1 inch)
Here it is used to add the names of the cars to the plot.
From the figure, you can see that increased gas mileage is associated with both decreased car weight and engine displacement.
In general, statisticians involved in the R project tend to avoid bubble plots for the same reason they avoid pie charts.
Humans typically have a harder time making judgments about volume than distance.
But bubble charts are certainly popular in the business world, so I’m including them here for completeness.
This attention to detail is due, in part, to the central place that scatter plots hold in data analysis.
While simple, they can help you visualize your data in an immediate and straightforward manner, uncovering relationships that might otherwise    be    missed.
Bubble Plot with point size pr opor tional to displacement.
Figure 11.16 Bubble plot of car weight versus mpg where point size is proportional to engine displacement.
If    you connect the points in a scatter plot moving from left to right, you have a line plot.
The dataset Orange that come with the base installation contains age and circumference data for five orange trees.
Consider the growth of the first orange tree, depicted in figure 11.17
The plot on the left is a scatter plot, and the plot on the right is a line chart.
As you can see, line charts are particularly good vehicles for conveying change.
The graphs in figure 11.17 were created with the code in the following listing.
You’ve seen the elements that make up this code in chapter 3, so I won’t go into details here.
The main difference between the two plots in figure 11.17 is produced by the option type="b"
In general, line charts are created with one of the following two functions.
Figure 11.17 Comparison of a scatter plot and a line plot.
The option type= can take the values described in table 11.1
As you can see, type="p" produces the typical scatter plot.
The option type="b" is the most common for line charts.
The difference between b and c is whether the points appear or gaps are left instead.
The first runs, then rises, whereas the second rises, then runs.
Figure 11.18 type= options in the plot() and lines() functions.
There’s an important difference between the p lot() and l ines() functions.
The plot() function will create a new graph when invoked.
The lines() function adds information to an existing graph but can’t produce a graph on its own.
Because of this, the lines() function is typically used after a plot() command has produced a graph.
If desired, you can use the type="n" option in the plot() function to set up the axes, titles, and other graph features, and then use the lines() function to add various lines to the plot.
To demonstrate the creation of a more complex line chart, let’s plot the growth of all five orange trees over time.
The code is shown in the next listing and the results in figure 11.19
Listing 11.4 Line chart displaying the growth of five orange trees over time.
Figure 11.19 Line chart displaying the growth of five orange trees.
In listing 11.4, the plot() function is used to set up the graph and specify the axis labels and ranges but plots no actual data.
The lines() function is then used to add a separate line and set of points for each orange tree.
You may want to test your understanding by working through each line of code and visualizing what it’s doing.
If you can, you are on your way to becoming a serious R programmer (and fame and fortune is near at hand)! In the next section, you’ll explore ways of examining a number of correlation coefficients at    once.
Which variables under consideration are strongly related to each other and which aren’t? Are there clusters of variables that relate in specific ways? As the number of variables grow, such questions can be harder to answer.
Correlograms are a relatively recent tool for visualizing the data in correlation matrices.
It’s easier to explain a correlogram once you’ve seen one.
Consider the correlations among the variables in the mtcars data frame.
Which variables are most related? Which variables are relatively independent? Are there any patterns? It isn’t that easy to tell from the correlation matrix without significant time and effort (and probably a set of colored pens to make notations)
You can display that same correlation matrix using the c orrgram() function in the c orrgram package (see figure 11.20)
To interpret this graph, start with the lower triangle of cells (the cells below the principal diagonal)
By default, a blue color and hashing that goes from lower left to upper right represents a positive correlation between the two variables that meet at that cell.
Conversely, a red color and hashing that goes from the upper left to the lower right represents a negative correlation.
The darker and more saturated the color, the greater the magnitude of the correlation.
In the current graph, the rows and columns have been reordered (using principal components analysis) to cluster variables together that have similar correlation patterns.
Figure 11.20 Correlogram of the correlations among the variables in the mtcars data frame.
Rows and columns have been reordered using principal components analysis.
You can see from shaded cells that gear, am, drat, and mpg are positively correlated with one another.
You can also see that wt, disp, cyl, hp, and carb are positively correlated with one another.
But the first group of variables is negatively correlated with the second group of variables.
You can also see that the correlation between carb and am is weak, as is the correlation between vs and gear, vs and am, and drat and qsec.
The upper triangle of cells displays the same information using pies.
Here, color plays the same role, but the strength of the correlation is displayed by the size of the filled pie slice.
Positive correlations fill the pie starting at 12 o’clock and moving in a clockwise direction.
Negative correlations fill the pie by moving in a counterclockwise direction.
When order=TRUE, the variables are reordered using a principal component analysis of the correlation matrix.
Reordering can help make patterns of bivariate relationships more obvious.
The option p anel specifies the type of off-diagonal panels to use.
Alternatively, you can use the options l ower.panel and u pper.panel to choose different options below and above the main diagonal.
The t ext.panel and d iag.panel options refer to the main diagonal.
Off diagonal p anel.pie The filled portion of the pie indicates the magnitude of the correlation.
Main diagonal p anel.minmax The minimum and maximum values of the variable are printed.
Here you’re using smoothed fit lines and confidence ellipses in the lower triangle and scatter plots in the upper triangle.
Figure 11.21 Correlogram of the correlations among the variables in the mtcars data frame.
The lower triangle contains smoothed best fit lines and confidence ellipses, and the upper triangle contains scatter plots.
Rows and columns have been reordered using principal components analysis.
Why do the scatter plots look odd? Several of the variables that are plotted in figure 11.21 have limited allowable values.
This explains the odd-looking scatter plots in the upper diagonal.
Always be careful that the statistical methods you choose are appropriate to the form of the data.
Specifying these variables as ordered or unordered factors can serve as a useful check.
When R knows that a variable is categorical or ordinal, it attempts to apply statistical methods that are appropriate to that level of measurement.
Here we’re using shading in the lower triangle, keeping the original variable order, and leaving the upper triangle blank.
Correlograms can be a useful way to examine large numbers of bivariate relationships.
Because they’re relatively new, the greatest challenge is to educate the recipient on how to interpret them.
Figure 11.22 Correlogram of the correlations among the variables in the mtcars data frame.
The lower triangle is shaded to represent the magnitude and direction of the correlations.
But what if your variables are categorical? When you’re looking at a single categorical variable, you can use a bar or pie chart.
If there are two categorical variables, you can look at a 3D bar chart (which, by the way, is not so easy to do in R)
But what do you do if there are more than two categorical variables?
In a mosaic plot, the frequencies in a multidimensional contingency table are represented by nested rectangular regions that are proportional to their cell frequency.
Color and or shading can be used to represent residuals from a fitted model.
Mosaic plots can be created with the m osaic() function from the v cd library (there’s a m osaicplot() function in the basic installation of R, but I recommend you use the v cd package for its more extensive features)
As an example, consider the Titanic dataset available in the base installation.
Adding the option shade=TRUE will color the figure based on Pearson residuals from.
The formula version gives you greater control over the selection and placement of variables in the graph.
There’s a great deal of information packed into this one picture.
For example, as one moves from crew to first class, the survival rate increases precipitously.
Most females in first class survived, whereas only about half the females in third class survived.
There were few females in the crew, causing the Survived labels (No, Yes at the bottom of the chart) to overlap for this group.
Remember to look at the relative widths and heights of the rectangles.
Figure 11.23 Mosaic plot describing Titanic survivors by class, sex, and age.
Extended mosaic plots add color and shading to represent the residuals from a fitted model.
Be sure to run the example so that you can see the results in color.
The graph indicates that more first-class women survived and more male crew members died than would be expected under an independence model.
Fewer third-class men survived than would be expected if survival was independent of class, gender, and age.
If you would like to explore mosaic plots in greater detail, try running example(mosaic)
In this chapter, we considered a wide range of techniques for displaying relationships among two or more variables.
Some of these methods are standard techniques, while some are relatively new.
In the next chapter, we’ll explore resampling statistics and bootstrapping.
These are computer intensive methods that allow you to analyze data in new and unique ways.
But there will be many cases in which this assumption is unwarranted.
Statistical approaches based on randomization and resampling can be used in cases where the data is sampled from unknown or mixed distributions, where sample sizes are small, where outliers are a problem, or where devising an appropriate test based on a theoretical distribution is too complex and mathematically intractable.
In this chapter, we’ll explore two broad statistical approaches that use randomization: permutation tests and bootstrapping.
Historically, these methods were only available to experienced programmers and expert statisticians.
Contributed packages in R now make them readily available to a wider audience of data analysts.
We’ll also revisit problems that were initially analyzed using traditional methods (for example, t-tests, chi-square tests, ANOVA, regression) and see how they can be approached using these robust, computer-intensive methods.
Permutation tests, also called randomization or re-randomization tests, have been around for decades, but it took the advent of high-speed computers to make them practically available.
To understand the logic of a permutation test, consider the following hypothetical problem.
Ten subjects have been randomly assigned to one of two treatment conditions (A or B) and an outcome variable (score) has been recorded.
The results of the experiment are presented in table 12.1
The data are also displayed in the strip chart in figure 12.1
Is there enough evidence to conclude that the treatments differ in their impact?
In a parametric approach, you might assume that the data are sampled from normal populations with equal variances and apply a two-tailed independent groups t-test.
The null hypothesis is that the population mean for treatment A is equal to the population mean for treatment B.
You’d calculate a t-statistic from the data and compare it to the theoretical distribution.
If the two treatments are truly equivalent, the label (Treatment A or Treatment B) assigned to an observed score is arbitrary.
To test for differences between the two treatments, we could follow these steps:
Calculate the observed t-statistic, as in the parametric approach; call this t0
Randomly assign five scores to Treatment A and five scores to Treatment B.
Notice that the same t-statistic is calculated in both the permutation and parametric approaches.
But instead of comparing the statistic to a theoretical distribution in order to determine if it was extreme enough to reject the null hypothesis, it’s compared to an empirical distribution created from permutations of the observed data.
This logic can be extended to most classical statistical tests and linear models.
In the previous example, the empirical distribution was based on all possible permutations of the data.
In such cases, the permutation test is called an “exact” test.
As the sample sizes increase, the time required to form all possible permutations can become prohibitive.
In such cases, you can use Monte Carlo simulation to sample from all possible permutations.
If you’re uncomfortable assuming that the data is normally distributed, concerned about the impact of outliers, or feel that the dataset is too small for standard parametric approaches, a permutation test provides an excellent alternative.
The remainder of this section focuses on two contributed packages: the coin package and the lmPerm package.
The coin package provides a comprehensive framework for permutation tests applied to independence problems, whereas the lmPerm package provides permutation tests for ANOVA and regression designs.
We’ll consider each in turn, and end the section with a quick review of other permutation packages available in R.
Before moving on, it’s important to remember that permutation tests use pseudorandom numbers to sample from all possible permutations (when performing an approximate test)
Therefore, the results will change each time the test is performed.
Setting the random number seed in R allows you to fix the random numbers generated.
This is particularly useful when you want to share your examples with others, because results will always be the same if the calls are made with the same seed.
The coin package provides a general framework for applying permutation tests to independence problems.
Are two categorical variables independent? Are two numeric variables independent?
Table 12.2 coin functions providing permutation test alternatives to traditional tests.
Two- and K-sample permutation test with a stratification (blocking) factor.
Each of the functions listed in table 12.2 take the form.
Additionally, the data must be stored in a data frame.
In the remainder of this section, we’ll apply several of the permutation tests described in table 12.2 to problems from previous chapters.
This will allow you to compare the results with more traditional parametric and nonparametric approaches.
We’ll end this discussion of the coin package by considering advanced extensions.
To begin, compare an independent samples t-test with a one-way exact test applied to the hypothetical data in table 12.2
Listing 12.1 t-test versus one-way permutation test for the hypothetical data.
With only 10 observations, l’d be more inclined to trust the results of the permutation test and attempt to collect more data before reaching a final conclusion.
In chapter 7, we examined the difference in the probability of imprisonment in Southern versus non-Southern US states using the wilcox.test() function.
Note that in the previous code, the numeric variable So was transformed into a factor.
This is because the coin package requires that all categorical variables be coded as factors.
This is because the wilcox.test() also computes an exact distribution by default.
An approximate k-sample permutation test can be performed instead, using this code:
Here, the reference distribution is based on 9,999 permutations of the data.
The random number seed was set so that your results would be the same as mine.
There’s clearly a difference in response among patients in the various groups.
The latter function is used when the data is stratified on a third categorical variable.
If both variables are ordinal, we can use the lbl_test() function to test for a linear trend.
In chapter 7, we applied a chi-square test to assess the relationship between Arthritis treatment and improvement.
Treatment had two levels (Placebo, Treated), and Improved had three levels (None, Some, Marked)
If you want to perform a permutation version of the chi-square test, you could use the following code:
This gives you an approximate chi-square test based on 9,999 replications.
You might ask why you transformed the variable Improved from an ordered factor to a categorical factor.
If you’d left it an ordered factor, coin() would have generated a linear x linear trend test instead of a chi-square test.
The spearman_test() function provides a permutation test of the independence of two numeric variables.
In chapter 7, we examined the correlation between illiteracy rates and murder rates for US states.
You can test the association via permutation, using the following code:
Based on an approximate permutation test with 9,999 replications, the hypothesis of independence can be rejected.
It had to be converted into a data frame for use in the coin package.
Dependent sample tests are used when observations in different groups have been matched, or when repeated measures are used.
For permutation tests with two paired groups, the wilcoxsign_test() function can be used.
Because the two variables are reported for each of the 50 US states, you have a two-dependent groups design (state is the matching variable)
We can use an exact Wilcoxon Signed Rank Test to see if unemployment rates for the two age groups are equal:
Based on the results, you’d conclude that the unemployment rates differ.
The coin package provides a general framework for testing that one group of variables is independent of a second group of variables (with optional stratification on a blocking variable) against arbitrary alternatives, via approximate permutation tests.
This flexibility comes at a price: a high level of statistical knowledge is required to use the function appropriately.
See the vignettes that accompany the package (accessed via vignette("coin")) for further details.
In the next section, you’ll learn about the lmPerm package.
This package provides a permutation approach to linear models, including regression and analysis of variance.
The lmPerm package provides support for a permutation approach to linear models.
In particular, the lmp() and aovp() functions are the lm() and aov() functions modified to perform permutation tests rather than normal theory tests.
The parameters within the lmp() and aovp() functions are similar to those in the lm() and aov()functions, with the addition of a perm= parameter.
The perm= option can take on the values "Exact", "Prob", or "SPR"
Exact produces an exact test, based on all possible permutations.
Sampling continues until the estimated standard deviation falls below 0.1 of the estimated p-value.
The stopping rule is controlled by an optional Ca parameter.
Finally, SPR uses a sequential probability ratio test to decide when to stop sampling.
Note that if the number of observations is greater than 10, perm="Exact" will automatically default to perm="Prob"; exact tests are only available for small problems.
To see how this works, we’ll apply a permutation approach to simple regression, polynomial regression, multiple regression, one-way analysis of variance, one-way analysis of covariance, and a two-way factorial design.
Using lmp() instead of lm() generates the permutation test results shown in the following listing.
Call: lmp(formula = weight ~ height, data = women, perm = "Prob")
To fit a quadratic equation, you could use the code in this next listing.
As you can see, it’s a simple matter to test these regressions using permutation tests and requires little change in the underlying code.
The output is also similar to that produced by the lm() function.
Note that an Iter column is added indicating how many iterations were required to reach the stopping rule.
Applying the lmp() function to this problem, results in the following output.
Call: lmp(formula = Murder ~ Population + Illiteracy + Income + Frost, data = states, perm = "Prob")
When the two approaches don’t agree, you should look at your data more carefully.
It may be that the assumption of normality is untenable or that outliers are present.
Each of the analysis of variance designs discussed in chapter 9 can be performed via permutation tests.
First, let’s look at the one-way ANOVA problem considered in sections 9.1 on the impact of treatment regimens on cholesterol reduction.
The code and results are given in the next listing.
The results suggest that the treatment effects are not all equal.
This second example in this section applies a permutation test to a one-way analysis.
The problem is from chapter 9, where you investigated the impact of four drug doses on the litter weights of rats, controlling for gestation times.
Based on the p-values, the four drug doses do not equally impact litter weights, controlling for gestation time.
We’ll end this section by applying permutation tests to a factorial design.
In chapter 9, we examined the impact of vitamin C on the tooth growth in guinea pigs.
The two manipulated factors were dose (three levels) and delivery method (two levels)
At the .05 level of significance, all three effects are statistically different from zero.
At the .01 level, only the main effects are significant.
It’s important to note that when aovp() is applied to ANOVA designs, it defaults to unique sums of squares (also called SAS Type III sums of squares)
The default for parametric ANOVA designs in R is sequential sums of squares (SAS Type I sums of squares)
Each effect is adjusted for those that appear earlier in the model.
For balanced designs, the two approaches will agree, but for unbalanced designs with unequal numbers of observations per cell, they won’t.
If desired, specifying seqs=TRUE in the aovp() function will produce sequential sums of squares.
The perm package provides some of the same functionality provided by the coin package and can act as an independent validation of that package.
The corrperm package provides permutation tests of correlations with repeated measures.
The logregperm package offers a permutation test for logistic regression.
Perhaps most importantly, the glmperm package extends permutation tests to generalized linear models.
Permutation tests provide a powerful alternative to tests that rely on a knowledge of the underlying sampling distribution.
In each of the permutation tests described, we were able to test statistical hypotheses without recourse to the normal, t, F, or chisquare distributions.
You may have noticed how closely the results of the tests based on normal theory agreed with the results of the permutation approach in previous sections.
The data in these problems were well behaved and the agreement between methods is a testament to how well normal theory methods work in such cases.
Where permutation tests really shine are in cases where the data is clearly nonnormal (for example, highly skewed), outliers are present, samples sizes are small, or no parametric tests exist.
However, if the original sample is a poor representation of the population of interest, no test, including permutation tests, will improve the inferences generated.
Permutation tests are primarily useful for generating p-values that can be used to test null hypotheses.
They can help answer the question, “Does an effect exist?” It’s more difficult to use permutation methods to obtain confidence intervals and estimates of measurement precision.
Bootstrapping generates an empirical distribution of a test statistic or set of test statistics, by repeated random sampling with replacement, from the original sample.
It allows you to generate confidence intervals and test statistical hypotheses without having to assume a specific underlying theoretical distribution.
But what if you aren’t willing to assume that the sampling distribution of the mean is normally distributed? You could use a bootstrapping approach instead:
Randomly select 10 observations from the sample, with replacement after each selection.
Some observations may be selected more than once, and some may not be selected at all.
In the present case, where the sample mean is likely to be normally distributed, you gain little from the bootstrap approach.
What if you wanted confidence intervals for the sample median, or the difference between two sample medians? There are no simple normal theory formulas here, and bootstrapping is the approach of choice.
If the underlying distributions are unknown, if outliers are a problem, if sample sizes are small, or if parametric approaches don’t exist, bootstrapping can often provide a useful method of generating confidence intervals and testing hypotheses.
The boot package provides extensive facilities for bootstrapping and related resampling methods.
You can bootstrap a single statistic (for example, a median), or a vector of statistics (for example, a set of regression coefficients)
Be sure to download and install the boot package before first use:
The bootstrapping process will seem complicated, but once you review the examples it should make sense.
Write a function that returns the statistic or statistics of interest.
If there is a single statistic (for example, a median), the function should return a number.
If there is a set of statistics (for example, a set of regression coefficients), the function should return a vector.
Process this function through the boot() function in order to generate R bootstrap replications of the statistic(s)
The function should include an indices parameter that the boot() function can use to select cases for each replication (see examples in the text)
Additional parameters to be passed to the function that is used to produce statistic(s) of interest.
Each time, it generates a set of random indices, with replacement, from the integers 1:nrow(data)
These indices are used within the statistic function to select a sample.
The statistics are calculated on the sample and the results are accumulated in the bootobject.
Table 12.4 Elements of the object returned by the boot() function.
Once you generate the bootstrap samples, you can use print() and plot() to.
If the results look reasonable, you can use the boot.ci() function to obtain confidence intervals for the statistic(s)
Possible values are "norm", "basic", "stud", "perc", "bca", and "all" (default: type="all")
The type parameter specifies the method for obtaining the confidence limits.
The perc method (percentile) was demonstrated in the sample mean example.
The bca provides an interval that makes simple adjustments for bias.
See Mooney and Duval (1993) for an introduction to these methods.
In the remaining sections, we’ll look at bootstrapping a single statistic and a vector of statistics.
In addition to the standard regression statistics, you’d like to obtain a 95 percent confidence interval for the R-squared value (the percent of variance in the response variable explained by the predictors)
The first task is to write a function for obtaining the R-squared value:
The d <- data[indices,] statement is required for boot() to be able to select samples.
You can then draw a large number of bootstrap replications (say, 1,000) with the following code:
In figure 12.2, you can see that the distribution of bootstrapped R-squared values.
A 95 percent confidence interval for the R-squared values can be obtained using.
You can see from this example that different approaches to generating the confidence intervals can lead to different intervals.
In this section, we estimated the confidence limits of a single statistic.
In the next section, we’ll estimate confidence intervals for several statistics.
In the previous example, bootstrapping was used to estimate the confidence interval for a single statistic (R-squared)
Continuing the example, let’s obtain the 95 percent confidence intervals for a vector of statistics.
Specifically, let’s get confidence intervals for the three model regression coefficients (intercept, car weight, and engine displacement)
First, create a function that returns the vector of regression coefficients:
When bootstrapping multiple statistics, add an index parameter to the plot() and boot.ci() functions to indicate which column of bootobject$t to analyze.
Figure 12.3 Distribution of bootstrapping regression coefficients for car weight.
If we assume that the predictor variables have fixed levels (typical in planned experiments), we’d do better to only resample residual terms.
Before we leave bootstrapping, it’s worth addressing two questions that come up often:
How many replications are needed? How large does the original sample need to be?
Some say that an original sample size of 20–30 is sufficient for good results, as long as the sample is representative of the population.
Random sampling from the population of interest is the most trusted method for assuring the original sample’s representativeness.
With regard to the second question, I find that 1,000 replications are more than adequate in most cases.
Computer power is cheap and you can always increase the number of replications if desired.
There are many helpful sources of information on permutation tests and bootstrapping.
An excellent starting place is an online article by Yu (2003)
Good (2006) provides a comprehensive overview of resampling in general and includes R code.
A good, accessible introduction to the bootstrap is provided by Mooney and Duval (1993)
The definitive source on bootstrapping is Efron and Tibshirani (1998)
In this chapter, we introduced a set of computer-intensive methods based on randomization and resampling that allow you to test hypotheses and form confidence intervals without reference to a known theoretical distribution.
They’re particularly valuable when your data comes from unknown population distributions, when there are serious.
The methods in this chapter are particularly exciting because they provide an avenue for answering questions when your standard data assumptions are clearly untenable, or when you have no other idea how to approach the problem.
If your original samples aren’t representative of the population of interest, or are too small to accurately reflect it, then these techniques won’t help.
In the next chapter, we’ll consider data models for variables that follow known, but not necessarily normal, distributions.
In this final section, we consider advanced methods of statistical and graphical analysis to round out your data analysis toolkit.
The chapter starts with a discussion of the generalized linear model, and then focuses on cases where we’re trying to predict an outcome variable that’s either categorical (logistic regression) or a count (poisson regression)
Dealing with a large number of variables can be challenging, due to the complexity inherent in multivariate data.
Chapter 14 describes two popular methods for exploring and simplifying multivariate data.
Principal components analysis can be used to transform a large number of correlated variables into a smaller set of composite variables.
Factor analysis consists of a set of techniques for uncovering the latent structure underlying a given set of variables.
More often than not, researchers must deal with incomplete datasets.
Chapter 15 considers modern approaches to the ubiquitous problem of missing data values.
Several of the best approaches are described here, along with guidance around which ones to use, and which ones to avoid.
Chapter 16 completes our discussion of graphics with presentations of some of R’s most advanced and useful approaches to visualizing data.
This includes visual representations of complex data using the lattice package, and an introduction to the new, and increasingly popular, ggplot2 package.
The chapter ends with a review of packages that provide functions for interacting with graphs in real-time.
After completing part 4, you will have the tools to manage a wide range of complex data analysis problems.
This includes modeling non-normal outcome variables, dealing with large numbers of correlated variables, and handling messy and incomplete data.
Additionally, you will have the tools to visualize complex data in useful, innovative and creative ways.
But there are many situations in which it’s unreasonable to assume that the dependent variable is normally distributed (or even continuous)
Such variables take on a limited number of values and are never negative.
Additionally, their mean and variance are often related (which isn’t true for normally distributed variables)
Generalized linear models extend the linear model framework to include dependent variables that are decidedly non-normal.
In this chapter, we’ll start with a brief overview of generalized linear models and the glm() function used to estimate them.
Then we’ll focus on two popular models within this framework: logistic regression (where the dependent variable is categorical) and Poisson regression (where the dependent variable is a count variable)
To motivate the discussion, we’ll apply generalized linear models to two research questions that aren’t easily addressed with standard linear models:
We’ll apply logistic regression to address the first question and Poisson regression to address the second.
A wide range of popular data analytic methods are subsumed within the framework of the generalized linear model.
In this section we’ll briefly explore some of the theory behind this approach.
You can safely skip over this section if you like and come back to it later.
Let’s say that you want to model the relationship between a response variable Y and a set of p predictor variables X1 ...Xp.
In the standard linear model, you assume that Y is normally distributed and that the form of the relationship is.
This equation states that the conditional mean of the response variable is a linear combination of the predictor variables.
You’re saying that you can predict the mean of the Y distribution for observations with a given set of X values by applying the proper weights to the X variables and adding them up.
In generalized linear models, you fit models of the form.
You specify the link function and the probability distribution, and the parameters are derived through an iterative maximum likelihood estimation procedure.
Generalized linear models are typically fit in R through the glm() function (although other specialized functions are available)
The form of the function is similar to lm() but includes additional parameters.
The glm() function allows you to fit a number of popular models, including logistic regression, Poisson regression, and survival analysis (not considered here)
You can demonstrate this for the first two models as follows.
Logistic regression is applied to situations in which the response variable is dichotomous (0,1)
The model assumes that Y follows a binomial distribution, and that you can fit a linear model of the form.
In this case, log(p/1 – p) is the link function, the probability distribution is binomial, and the logistic regression model can be fit using.
Poisson regression is applied to situations in which the response variable is the number of events to occur in a given period of time.
The Poisson regression model assumes that Y follows a Poisson distribution, and that you can fit a linear model of the form.
In this case, the link function is log(l), the probability distribution is Poisson, and the Poisson regression model can be fit using.
It is worth noting that the standard linear model is also a special case of the.
If you let the link function g(mY ) = mY or the identity function and specify that the probability distribution is normal (Gaussian), then.
To summarize, generalized linear models extend the standard linear model by fitting a function of the conditional mean response (rather than the conditional mean response), and assuming that the response variable follows a member of the exponential family of distributions (rather than being limited to the normal distribution)
The parameter estimates are derived via maximum likelihood rather than least squares.
Many of the functions that you used in conjunction with lm() when analyzing standard linear models have corresponding versions for glm()
In the next section, we’ll briefly consider the assessment of model adequacy.
The assessment of model adequacy is as important for generalized linear models as it is for standard (OLS) linear models.
Unfortunately, there’s less agreement in the statistical community regarding appropriate assessment procedures.
In general, you can use the techniques described in chapter 8, with the following caveats.
When assessing model adequacy, you’ll typically want to plot predicted values expressed in the metric of the original response variable against residuals of the deviance type.
The hat values, studentized residuals, and Cook’s D statistics that R provides will.
Additionally, there’s no general consensus on cutoff values for identifying problematic observations.
One approach is to create index plots for each statistic and look for unusually large values.
For example, you could use the following code to create three diagnostic plots:
In the latter graph, the horizontal axis is the leverage, the vertical axis is the studentized residual, and the plotted symbol is proportional to the Cook’s distance.
Diagnostic plots tend to be most helpful when the response variable takes on many values.
When the response variable can only take on a limited number of values (for example, logistic regression), their utility is decreased.
In the remaining portion of this chapter, we’ll consider two of the most popular forms of the generalized linear model in detail: logistic regression and Poisson regression.
Logistic regression is useful when predicting a binary outcome from a set of continuous and/or categorical predictor variables.
To demonstrate this, we’ll explore the data on infidelity contained in the data frame Affairs, provided with the AER package.
Although the number of indiscretions was recorded, our interest here is in the binary outcome (had an affair/didn’t have an affair)
You can transform affairs into a dichotomous factor called ynaffair with the following code.
This dichotomous factor can now be used as the outcome variable in a logistic regression model:
Call: glm(formula = ynaffair ~ gender + age + yearsmarried + children + religiousness + education + occupation + rating, family = binomial(), data = Affairs)
From the p-values for the regression coefficients (last column), you can see that gender, presence of children, education, and occupation may not make a significant contribution to the equation (you can’t reject the hypothesis that the parameters are 0)
Let’s fit a second equation without them, and test whether this reduced model fits the data as well:
Call: glm(formula = ynaffair ~ age + yearsmarried + religiousness + rating, family = binomial(), data = Affairs)
Each regression coefficient in the reduced model is statistically significant (p<.05)
Because the two models are nested (fit.reduced is a subset of fit.full), you can use the anova() function to compare them.
For generalized linear models, you’ll want a chi-square version of this test.
The nonsignificant chi-square value (p=0.21) suggests that the reduced model with four predictors fits as well as the full model with nine predictors, reinforcing your belief that gender, children, education, and occupation don’t add significantly to the prediction above and beyond the other variables in the equation.
Therefore, you can base your interpretations on the simpler model.
In a logistic regression, the response being modeled is the log(odds) that Y=1
The regression coefficients give the change in log(odds) in the response for a unit change in the predictor variable, holding all other predictor variables constant.
Because log(odds) are difficult to interpret, you can exponentiate them to put the results on an odds scale:
Now you can see that the odds of an extramarital encounter are increased by a factor of 1.106 for a one-year increase in years married (holding age, religiousness, and marital rating constant)
Conversely, the odds of an extramarital affair are multiplied by a factor of 0.965 for every year increase in age.
The odds of an extramarital affair increase with years married, and decrease with age, religiousness, and marital rating.
Because the predictor variables can’t equal 0, the intercept isn’t meaningful in this case.
If desired, you can use the confint() function to obtain confidence intervals for the coefficients.
Finally, a one-unit change in a predictor variable may not be inherently interesting.
For binary logistic regression, the change in the odds of the higher value on the response variable for an n unit change in a predictor variable is exp(b j)^n.
For many of us, it’s easier to think in terms of probabilities than odds.
You can use the predict() function to observe the impact of varying the levels of a predictor variable on the probability of the outcome.
The first step is to create an artificial dataset containing the values of the predictor variables that you’re interested in.
Then you can use this artificial dataset with the predict() function to predict the probabilities of the outcome event occurring for these values.
Let’s apply this strategy to assess the impact of marital ratings on the probability of having an extramarital affair.
Next, use the test dataset and prediction equation to obtain probabilities:
From these results you see that the probability of an extramarital affair decreases from 5=very happy (holding age, years married, and religiousness constant)
Using this approach, you can explore the impact of each predictor variable on the outcome.
Overdispersion occurs when the observed variance of the response variable is larger than what would be expected from a binomial distribution.
Overdispersion can lead to distorted test standard errors and inaccurate tests of significance.
When overdispersion is present, you can still fit a logistic regression using the glm() function, but in this case, you should use the quasibinomial distribution rather than the binomial distribution.
One way to detect overdispersion is to compare the residual deviance with the residual degrees of freedom in your binomial model.
To do this, you fit the model twice, but in.
If the glm() object returned in the first case is called fit and the object returned in the second case is called fit.od, then.
We’ll return to the issue of overdispersion when we discuss Poisson regression.
Several logistic regression extensions and variations are available in R:
Robust logistic regression can be helpful when fitting logistic regression models to data containing outliers and influential observations.
The ability to model a response variable with multiple categories (both ordered and unordered) is an important extension, but it comes at the expense of greater interpretive complexity.
Assessing model fit and regression diagnostics in these cases will also be more complex.
In the Affairs example, the number of extramarital contacts was dichotomized into a yes/no response variable because our interest centered on whether respondents had an affair in the past year.
If our interest had been centered on magnitude—the number of encounters in the past year—we would have analyzed the count data directly.
One popular approach to analyzing count data is Poisson regression, the next topic we’ll address.
Poisson regression is useful when you’re predicting an outcome variable representing counts from a set of continuous and/or categorical predictor variables.
A comprehensive yet accessible introduction to Poisson regression is provided by Coxe, West, and Aiken (2009)
To illustrate the fitting of a Poisson regression model, along with some issues that can come up in the analysis, we’ll use the Breslow seizure data (Breslow, 1993) provided in the robust package.
Specifically, we’ll consider the impact of an antiepileptic drug treatment on the number of seizures occurring over an eight-week period following the initiation of therapy.
Data were collected on the age and number of seizures reported by patients suffering from simple or complex partial seizures during an eight-week period before, and eightweek period after, randomization into a drug or placebo condition.
Treatment condition (Trt), age in years (Age), and number of seizures reported in the baseline eight-week period (Base) are the predictor variables.
The baseline number of seizures and age are included because of their potential effect on the response variable.
We are interested in whether or not evidence exists that the drug treatment decreases the number of seizures after accounting for these covariates.
Note that although there are 12 variables in the dataset, we’re limiting our attention to the four described earlier.
Both the baseline and post-randomization number of seizures is highly skewed.
Figure 13.1 Distribution of post-treatment seizure counts (Source: Breslow seizure data)
You can clearly see the skewed nature of the dependent variable and the possible presence of outliers.
At first glance, the number of seizures in the drug condition appears to be smaller and have a smaller variance.
You’d expect a smaller variance to accompany a smaller mean with Poisson distributed data.
Unlike standard OLS regression, this heterogeneity of variance isn’t a problem in Poisson regression.
Note that each of the predictor variables is significant at the p<0.05 level.
In a Poisson regression, the dependent variable being modeled is the log of the conditional mean loge(l)
Because you can’t have a zero age and none of the participants had a zero number of baseline seizures, the intercept isn’t meaningful in this case.
It’s usually much easier to interpret the regression coefficients in the original scale of the dependent variable (number of seizures, rather than log number of seizures)
Now you see that a one-year increase in age multiplies the expected number of seizures by 1.023, holding the other variables constant.
This means that increased age is associated with higher numbers of seizures.
More importantly, a one-unit change in Trt (that is, moving from placebo to progabide) multiplies the expected number of seizures by 0.86
You’d expect a 20 percent decrease in the number of seizures for the drug group compared with the placebo group, holding baseline number of seizures and age constant.
It’s important to remember that, like the exponeniated parameters in logistic.
Also, as with logistic regression, you must evaluate your model for overdispersion.
In a Poisson distribution, the variance and mean are equal.
Overdispersion occurs in Poisson regression when the observed variance of the response variable is larger than would be predicted by the Poisson distribution.
Because overdispersion is often encountered when dealing with count data, and can have a negative impact on the interpretation of the results, we’ll spend some time discussing it.
There are several reasons why overdispersion may occur (Coxe et al., 2009):
Within observations, each event in a count is assumed to be independent.
For the seizure data, this would imply that for any patient, the probability of a seizure is independent of each other seizure.
If overdispersion is present and you don’t account for it in your model, you’ll get standard errors and confidence intervals that are too small, and significance tests that are too liberal (that is, you’ll find effects that aren’t really there)
The qcc package provides a test for overdispersion in the Poisson case.
You can test for overdispersion in the seizure data using the following code:
Not surprisingly, the significance test has a p-value less than 0.05, strongly suggesting the presence of overdispersion.
You can still fit a model to your data using the glm() function, by replacing.
Doing so is analogous to our approach to logistic regression when overdispersion is present.
Notice that the parameter estimates in the quasi-Poisson approach are identical to those produced by the Poisson approach.
In this case, the larger standard errors have led to p-values for Trt (and Age) that are greater than 0.05
When you take overdispersion into account, there’s insufficient evidence to declare that the drug regimen reduces seizure counts more than receiving a placebo, after controlling for baseline seizure rate and age.
Please remember that this example is used for demonstration purposes only.
The results shouldn’t be taken to imply anything about the efficacy of progabide in the real world.
I’m not a doctor—at least not a medical doctor—and I don’t even play one on TV.
We’ll finish this exploration of Poisson regression with a discussion of some important variants and extensions.
Our discussion of Poisson regression has been limited to response variables that measure a count over a fixed length of time (for example, number of seizures in an eight-week period, number of traffic accidents in the past year, number of pro-social behaviors in a day)
But you can fit Poisson regression models that allow the time period to vary for each observation.
To analyze rates, you must include a variable (for example, time) that records the length of time over which the count occurs for each observation.
To fit this new model, you use the offset option in the glm() function.
You could use the rate of seizures as the dependent variable (assuming you had recorded time for each patient in days), and fit the model.
There are times when the number of zero counts in a dataset is larger than would be predicted by the Poisson model.
This can occur when there’s a subgroup of the population that would never engage in the behavior being counted.
For example, in the Affairs dataset described in the section on logistic regression, the original outcome variable (affairs) counted the number of extramarital sexual intercourse experience participants had in the past year.
It’s likely that there’s a subgroup of faithful marital partners who would never have an affair, no matter how long the period of time studied.
These are called structural zeros (primarily by the swingers in the group)
In such cases, you can analyze the data using an approach called zero-inflated Poisson regression.
The approach fits two models simultaneously—one that predicts who would or would not have an affair, and the second that predicts how many affairs a participant.
Think of this as a model that combines a logistic regression (for predicting structural zeros) and a Poisson regression model (that predicts counts for observations that aren’t structural zeros)
Zero-inflated Poisson regression can be fit using the zeroinfl() function in the pscl package.
Finally, the glmRob() function in the robust package can be used to fit a robust generalized linear model, including robust Poisson regression.
As mentioned previously, this can be helpful in the presence of outliers and influential observations.
Generalized linear models are a complex and mathematically sophisticated subject, but many fine resources are available for learning about them.
A good, short introduction to the topic is Dunteman and Ho (2006)
The classic (and advanced) text on generalized linear models is provided by McCullagh and Nelder (1989)
In this chapter, we used generalized linear models to expand the range of approaches available for helping you to understand your data.
In particular, the framework allows you to analyze response variables that are decidedly non-normal, including categorical outcomes and discrete counts.
After briefly describing the general approach, we focused on logistic regression (for analyzing a dichotomous outcome) and Poisson regression (for analyzing outcomes measured as counts or rates)
We also discussed the important topic of overdispersion, including how to detect it and how to adjust for it.
Finally, we looked at some of the extensions and variations that are available in R.
Each of the statistical approaches covered so far has dealt with directly observed and recorded variables.
In particular, you’ll see how you can use factor analytic methods to detect and test hypotheses about these unobserved variables.
One of the most challenging aspects of multivariate data is the sheer complexity of the information.
Two related but distinct methodologies for exploring and simplifying complex multivariate data are principal components and exploratory factor analysis.
For example, you might use PCA to transform 30 correlated (and possibly redundant) environmental variables into five uncorrelated composite variables that retain as much information from the original set of variables as possible.
In contrast, e xploratory factor analysis (EFA) is a collection of methods designed to uncover the latent structure in a given set of variables.
It looks for a smaller set of underlying or latent constructs that can explain the relationships among the observed or manifest variables.
If you apply EFA to this data, the results suggest that the 276 test intercorrelations can be explained by the children’s abilities on four underlying factors (verbal ability, processing speed, deduction, and memory)
The differences between the PCA and EFA models can be seen in figure 14.1
The weights used to form the linear composites are chosen to maximize the variance each principal component accounts for, while keeping the components uncorrelated.
The circles indicate that the factors and errors aren’t directly observable but are inferred from the correlations among the variables.
In this example, the curved arrow between the factors indicates that they’re correlated.
Correlated factors are common, but not required, in the EFA model.
The methods described in this chapter require large samples to derive stable solutions.
Until recently, analysts used rules of thumb like “factor analysis requires 5–10 times as many subjects as variables.” Recent studies suggest that the required sample size depends on the number of factors, the number of variables associated with each factor, and how well the set of factors explains the variance in the variables (Bandalos and Boehm-Kaufman, 2009)
I’ll go out on a limb and say that if you have several hundred observations, you’re probably safe.
In this chapter, we’ll look at artificially small problems in order to keep the output (and page count) manageable.
We’ll start by reviewing the functions in R that can be used to perform PCA or EFA and give a brief overview of the steps involved.
Then we’ll work carefully through two PCA examples, followed by an extended EFA example.
A brief overview of other packages in R that can be used for fitting latent variable models is provided at the end of the chapter.
This discussion includes packages for confirmatory factor analysis, structural equation modeling, correspondence analysis, and latent class analysis.
In the base installation of R, the functions for PCA and EFA are p rincomp() and f actanal(), respectively.
In this chapter, in this chapter, we’ll focus on functions provided in the psych package.
They offer many more useful options than their base counterparts.
Additionally, the results are reported in a metric that will be more familiar to social scientists and more likely to match the output provided by corresponding programs in other statistical packages such as SAS and SPSS.
The p sych package functions that are most relevant here are listed in table 14.1
Be sure to install the package before trying the examples in this chapter.
Table 14.1 Useful factor analytic functions in the psych package.
The reason is that they describe a wide range of approaches, and each approach requires several steps (and decisions) to achieve a final result.
Both PCA and EFA derive their solutions from the correlations among the observed variables.
Users can input either the raw data matrix or the correlation matrix to the p rincipal() and f a() functions.
If raw data is input, the correlation matrix will automatically be calculated.
Be sure to screen the data for missing values before proceeding.
Decide whether PCA (data reduction) or EFA (uncovering latent structure) is a better fit for your research goals.
If you select an EFA approach, you’ll also need to choose a specific factoring method (for example, maximum likelihood)
In the remainder of this chapter, we’ll carefully consider each of the steps, starting with PCA.
At the end of the chapter, you’ll find a detailed flow chart of the possible steps in PCA/EFA (figure 14.7)
The chart will make more sense once you’ve read through the intervening  material.
The goal of PCA is to replace a large number of correlated variables with a smaller number of uncorrelated variables while capturing as much information in the original variables as possible.
These derived variables, called principal components, are linear combinations of the observed variables.
The second principal component is the linear combination that accounts for the most variance in the original variables, under the constraint that it’s orthogonal (uncorrelated) to the first principal component.
Each subsequent component maximizes the variance accounted for, while at the same time remaining uncorrelated with all previous components.
Theoretically, you can extract as many principal components as there are variables.
But from a practical viewpoint, you hope that you can approximate the full set of variables with a much smaller set of components.
The dataset USJudgeRatings contains lawyers’ ratings of state judges in the US Superior Court.
From a practical point of view, can you summarize the 11 evaluative ratings (INTG to RTEN) with a smaller number of composite variables? If so, how many will you need and how will they be defined? Because our goal is to simplify the data, we’ll approach this problem using PCA.
The data are in raw score format and there are no missing values.
Therefore, your next decision is deciding how many principal components you’ll need.
Several    criteria are available for deciding how many components to retain in a PCA.
Each component is associated with an eigenvalue of the correlation matrix.
The first PC is associated with the largest eigenvalue, the second PC with the second-largest eigenvalue, and so on.
Components with eigenvalues less than 1 explain less variance than contained in a single variable.
In the Cattell Scree test, the eigenvalues are plotted against their component numbers.
Such plots will typically demonstrate a bend or elbow, and the components above this sharp break are retained.
Finally, you can run simulations, extracting eigenvalues from random data matrices of the same size as the original matrix.
If an eigenvalue based on real data is larger than the average corresponding eigenvalues from a set of random data matrices, that component is retained.
The approach is called p arallel analysis (see Hayton, Allen, and Scarpello, 2004 for more details)
You can assess all three eigenvalue criteria at the same time via the f a.parallel() function.
For the 11 ratings (dropping the CONT variable), the necessary code is as follows:
All three criteria suggest that a single component is appropriate for summarizing this dataset.
Your next step is to extract the principal component using the principal() function.
Figure 14.2 Assessing the number of principal components to retain for the US Judge Rating example.
As indicated earlier, the p rincipal() function will perform a principal components analysis starting with either a raw data matrix or a correlation matrix.
To extract the first principal component, you can use the code in the following listing.
Here, you’re inputting the raw data without the C ONT variable and specifying that one unrotated component should be extracted.
Because PCA is performed on a correlation matrix, the raw data is automatically converted to a correlation matrix before extracting the components.
The column labeled PC1 contains the component loadings, which are the correlations of the observed variables with the principal component(s)
Component loadings are used to interpret the meaning of components.
You can see that each variable correlates highly with the first component (PC1)
The column labeled h2 contains the component communalities—the amount of variance in each variable explained by the components.
The row labeled SS loadings contains the eigenvalues associated with the components.
Finally, the row labeled P roportion Var represents the amount of variance accounted for by each component.
Let’s consider a second example, one that results in a solution with more than one principal component.
In this case, the dataset consists of the correlations among the variables rather than the original data (see table 14.3)
Again, you wish to replace the original physical measurements with a smaller number of derived variables.
You can determine the number of components to extract using the following code.
In this case, you need to identify the correlation matrix (the cov component of the Harman23.cor object) and specify the sample size (n.obs):
Figure 14.3 Assessing the number of principal components to retain for the Body Measurements example.
You can see from the plot that a two-component solution is suggested.
As in the first example, the Kaiser–Harris criteria, scree test, and parallel analysis agree.
This won’t always be the case, and you may need to extract different numbers of components and select the solution that appears most useful.
The next listing extracts the first two principal components from the correlation matrix.
Together, the two components account for 81 percent of the variance.
The two components together account for 88 percent of the variance in the height variable.
The first component correlates positively with each physical measure and appears to be a general size factor.
Whenever two or more components have been extracted, you can rotate the solution to make it more interpretable.
Rotations    are a set of mathematical techniques for transforming the component loading matrix into one that’s more interpretable.
Rotation methods differ with regard to whether the resulting components remain uncorrelated (o rthogonal rotation) or are allowed to correlate (o blique rotation)
The most popular orthogonal rotation is the v arimax rotation, which attempts to purify the columns of the loading matrix, so that each component is defined by a limited set of variables (that is, each column has a few large loadings and many very small loadings)
Applying a varimax rotation to the body measurement data, you get the results provided in the next listing.
You’ll see an example of an oblique rotation in section 14.4
The column names change from PC to RC to denote rotated components.
Looking at the loadings in column R C1, you see that the first component is primarily defined by the first four variables (length variables)
Note that the two components are still uncorrelated and that together, they still explain the variables equally well.
You can see that the rotated solution explains the variables equally well because the variable communalities haven’t changed.
Additionally, the cumulative variance accounted for by the two-component rotated solution (81 percent) hasn’t changed.
This spreading out of the variance across components is common, and technically you should now call them components rather than principal components (because the variance maximizing properties of individual components has not been retained)
Our ultimate goal is to replace a larger set of correlated variables with a smaller set of derived variables.
To do this, you need to obtain scores for each observation on the components.
In    the US Judge Rating example, you extracted a single principal component from the raw data describing lawyers’ ratings on 11 variables.
The principal() function makes it easy to obtain scores for each participant on this derived variable (see the next listing)
The principal component scores are saved in the s cores element of the object returned by the p rincipal() function when the option s cores=TRUE.
If you wanted, you could now get the correlation between the number of contacts occurring between a lawyer and a judge and their evaluation of the judge using.
Apparently, there’s no relationship between the lawyer’s familiarity and his or her opinions!
When the principal components analysis is based on a correlation matrix and the raw data aren’t available, getting principal component scores for each observation is clearly not possible.
But you can get the coefficients used to calculate the principal components.
In the body measurement data, you have correlations among body measurements, but you don’t have the individual measurements for these 305 girls.
You can get the scoring coefficients using the code in the following listing.
As a practical matter, you could simplify your approach further by taking the first composite variable as the mean of the standardized scores for the first four variables.
Similarly, you could define the second composite variable as the mean of the standardized scores for the second four variables.
Little Jiffy conquers the world There’s quite a bit of confusion among data analysts regarding PCA and EFA.
One reason for this is historical and can be traced back to a program called Little Jiffy (no kidding)
Little Jiffy was one of the most popular early programs for factor analysis, and defaulted to a principal components analysis, extracting components with eigenvalues greater than 1 and rotating them to a varimax solution.
The program was so widely used that many social scientists came to think of this defaults as synonymous with EFA.
Many later statistical packages also incorporated these defaults in their EFA programs.
As I hope you’ll see in the next section, there are important and fundamental differences between PCA and EFA.
If your goal is to look for latent underlying variables that explain your observed variables, you can turn to factor analysis.
The  goal of EFA is to explain the correlations among a set of observed variables by uncovering a smaller set of more fundamental unobserved variables underlying the data.
Each factor is assumed to explain the variance shared among two or more observed variables, so technically, they are called c ommon factors.
Ui is the portion of variable Xi unique to that variable (not explained by the common factors)
The ai can be thought of as the degree to which each factor contributes to the composition of an observed variable.
Although the PCA and EFA models differ, many of the steps will appear similar.
To illustrate the process, we’ll apply EFA to the correlations among six psychological tests.
One hundred twelve individuals were given six tests, including a nonverbal measure of general intelligence (general), a picture-completion test (picture), a block design test (blocks), a maze test (maze), a reading comprehension test (reading), and a vocabulary test (vocab)
Can we explain the participants’ scores on these tests with a smaller number of underlying or latent psychological constructs?
The covariance matrix among the variables is provided in the dataset ability.cov.
You can transform this into a correlation matrix using the c ov2cor() function.
Because you’re looking for hypothetical constructs that explain the data, you’ll use an EFA approach.
As in PCA, the next task is to decide how many factors to extract.
Notice you’ve requested that the function display results for both a principal components and common factor approach, so that you can compare them (fa="both")
Figure 14.4 Assessing the number of factors to retain for the psychological tests example.
If you’d taken a PCA approach, you might have chosen one component (scree test, parallel analysis) or two components (eigenvalues greater than 1)
When in doubt, it’s usually a better idea to overfactor than to underfactor.
Overfactoring tends to lead to less distortion of the “true” solution.
Looking at the EFA results, a two-factor solution is clearly indicated.
The first two eigenvalues (triangles) are above the bend in the scree test and also above the mean eigenvalues based on 100 simulated data matrices.
Most people don’t realize this, so it’s a good way to win bets at parties.
In the present case the Kaiser–Harris criteria also suggest two    factors.
Now    that you’ve decided to extract two factors, you can use the f a() function to obtain your solution.
Unlike PCA, there are many methods of extracting common factors.
Statisticians tend to prefer the maximum likelihood approach because of its well-defined statistical model.
Sometimes, this approach fails to converge, in which case the iterated principal axis option often works well.
For this example, you’ll extract the unrotated factors using the iterated principal axis (fm="pa") approach.
You can see that the two factors account for 60 percent of the variance in the six psychological tests.
When you examine the loadings, though, they aren’t easy to interpret.
You    can rotate the two-factor solution from section 14.3.4 using either an o rthogonal rotation or an o blique rotation.
Let’s try both so you can see how they differ.
Looking at the factor loadings, the factors are certainly easier to interpret.
Reading and vocabulary load on the first factor, and picture completion, block design, and mazes loads on the second factor.
This may indicate a verbal intelligence factor and a nonverbal intelligence factor.
By using an orthogonal rotation, you’ve artificially forced the two factors to be uncorrelated.
What would you find if you allowed the two factors to correlate? You can try an oblique rotation such as promax (see the next listing)
In an orthogonal solution, attention focuses on the f actor structure matrix (the correlations of the variables with the factors)
In an oblique solution, there are three matrices to consider: the factor structure matrix, the f actor pattern matrix, and the f actor intercorrelation matrix.
The factor pattern matrix is a matrix of standardized regression coefficients.
They give the weights for predicting the variables from the factors.
The factor intercorrelation matrix gives the correlations among the factors.
Examination of the columns of this matrix is still used to name the factors (although there’s some controversy here)
The factor intercorrelation matrix indicates that the correlation between the two factors is 0.57
If the factor intercorrelations had been low, you might have gone back to an orthogonal solution to keep things simple.
The factor structure matrix (or factor loading matrix) isn’t provided.
But you can easily calculate it using the formula F = P*Phi, where F is the factor loading matrix, P is the factor pattern matrix, and Phi is the factor intercorrelation matrix.
A simple function for carrying out the multiplication is as follows:
Now you can review the correlations between the variables and the factors.
Comparing them to the factor loading matrix in the orthogonal solution, you see that these columns aren’t as pure.
This is because you’ve allowed the underlying factors to be correlated.
Although the oblique approach is more complicated, it’s often a more realistic model of the data.
You can graph an orthogonal or oblique solution using the f actor.plot() or f a.
If you let simple=TRUE, only the largest loading per item would be displayed.
It shows the largest loadings for each factor, as well as the correlations between the factors.
This type of diagram is helpful when there are several factors.
Figure 14.5 Two factor plot for the psychological tests in ability.cov.
When you’re dealing with data in real life, it’s unlikely that you’d apply factor analysis to a dataset with so few variables.
Figure 14.6 Diagram of the oblique two factor solution for the psychological test data in ability.cov.
Compared    with PCA, the goal of EFA is much less likely to be the calculation of factor scores.
But these scores are easily obtained from the f a() function by including the score=TRUE option (when raw data is available)
Additionally, the scoring coefficients (standardized regression weights) are available in the weights element of the object returned.
For the ability.cov dataset, you can obtain the beta weights for calculating the factor score estimates for the two-factor oblique solution using.
Before moving on, let’s briefly review other R packages that are useful for exploratory factor    analysis.
The F actoMineR package provides methods for PCA and EFA, as well as other latent variable models.
It provides many options that we haven’t considered here, including the use of both numeric and categorical variables.
The F AiR package estimates factor analysis models using a genetic algorithm that permits the ability to impose inequality restrictions on model parameters.
The G PArotation package offers many additional factor rotation methods.
Finally, the n Factors package offers sophisticated techniques for determining the number of factors    underlying  data.
We’ll end this chapter with a brief description of other models that can be fit within R.
These include models that test a priori theories, that can handle mixed data types (numeric and categorical), or that are based solely on categorical multiway tables.
In EFA, you allow the data to determine the number of factors to be extracted and their meaning.
But you could start with a theory about how many factors underlie a set of variables, how the variables load on those factors, and how the factors correlate with one another.
You could then test this theory against a set of collected data.
You can think of SEM as a combination of confirmatory factor analyses (for the variables) and regression analyses (for the factors)
There are several excellent packages for CFA and SEM in R.
The ltm package can be used to fit latent models to the items contained in tests and questionnaires.
The methodology is often used to create large scale standardized tests.
Latent class models (where the underlying factors are assumed to be categorical rather than continuous) can be fit with the F lexMix, l cmm, r andomLCA, and p oLC packages.
The l cda package performs latent class discriminant analysis, and the l sa package performs latent semantic analysis, a methodology used in natural language processing.
The ca package provides functions for simple and multiple correspondence analysis.
These methods allow you to explore the structure of categorical variables in two-way and multiway tables, respectively.
Finally, R contains numerous methods for m ultidimensional scaling (MDS)
The c mdscale() function in the base installation performs a classical MDS, while the i soMDS() function in the M ASS package performs a nonmetric MDS.
The vegan package also contains functions for classical and nonmetric MDS.
In this chapter, we reviewed methods for principal components (PCA) analysis and exploratory factor analysis (EFA)
Whereas the goal of PCA is typically to summarize the data and reduce its dimensionality, EFA can be used as a hypothesis generating tool, useful when you’re trying to understand the relationships between a large number of variables.
It’s often used in the social sciences for theory development.
Although there are many superficial similarities between the two approaches, important differences exist as well.
In this chapter, we considered the models underlying each, methods for selecting the number of components/factors to extract, methods for extracting components/factors and rotating (transforming) them to enhance interpretability, and techniques for obtaining component or factor scores.
The steps in a PCA or EFA are summarized in figure 14.7
We ended the chapter with a brief discussion of other latent variable methods available in R.
Because PCA and EFA are based on correlation matrices, it’s important that any missing data be eliminated before proceeding with the analyses.
Section 4.5 briefly mentioned simple methods for dealing with missing data.
In the next chapter, we’ll consider more sophisticated methods for both understanding and handling missing    values.
In previous chapters, we focused on the analysis of complete datasets (that is, datasets without missing values)
Although doing so has helped simplify the presentation of statistical and graphical methods, in the real world, missing data are ubiquitous.
In some ways, the impact of missing data is a subject that most of us want to avoid.
Statistics books may not mention it or may limit discussion to a few paragraphs.
Statistical packages offer automatic handling of missing data using methods that may not be optimal.
Even though most data analyses (at least in social sciences) involve missing data, this topic is rarely mentioned in the methods and results sections of journal articles.
Given how often missing values occur, and the degree to which their presence can invalidate study results, it’s fair to say that the subject has received insufficient attention outside of specialized books and courses.
Survey participants may forget to answer one or more questions, refuse to answer sensitive questions, or grow fatigued and fail to complete a long questionnaire.
Study participants may miss appointments or drop out of a study prematurely.
Recording equipment may fail, internet connections may be lost, and data may be miscoded.
For example, to increase study efficiency or reduce costs, you may choose not to collect all data from all participants.
Finally, data may be lost for reasons that you’re never able to ascertain.
Unfortunately, most statistical methods assume that you’re working with complete matrices, vectors, and data frames.
In most cases, you have to eliminate missing data before you address the substantive questions that led you to collect the data.
In either case, the end result is a dataset without missing values.
In this chapter, we’ll look at both traditional and modern approaches for dealing with missing data.
We’ll primarily use the V IM and m ice packages.
To motivate the discussion, we’ll look at the mammal sleep dataset (sleep) provided in the VIM package (not to be confused with the sleep dataset describing the impact of drugs on sleep provided in the base installation)
The authors were interested in why animals’ sleep requirements vary from species to species.
The sleep variables served as the dependent variables, whereas the ecological and constitutional variables served as the independent or predictor variables.
Sleep variables included length of dreaming sleep (Dream), nondreaming sleep (NonD), and their sum (Sleep)
The constitutional variables included body weight in kilograms (BodyWgt), brain weight in grams (BrainWgt), life span in years (Span), and gestation time in days (Gest)
The ecological variables included degree to which species were preyed upon (Pred), degree of their exposure while sleeping (Exp), and overall danger (Danger) faced.
In their original article, Allison and Chichetti limited their analyses to the species that had complete data.
We’ll go further, analyzing all 62 cases using a multiple imputation approach.
Readers  new to the study of missing data will find a bewildering array of approaches, critiques, and methodologies.
The classic text in this area is Little and Rubin (2002)
Unfortunately, identifying missing data is usually the only unambiguous step.
Learning why data are missing depends on your understanding of the processes that generated the data.
Deciding how to treat missing values will depend on your estimation of which procedures will produce the most reliable and accurate results.
A classification system for missing data Statisticians typically classify missing data into one of three types.
These types are usually described in probabilistic terms, but the underlying ideas are straightforward.
We’ll use the measurement of dreaming in the sleep study (where 12 animals have missing values) to illustrate each type in turn.
Missing completely at random—If the presence of missing data on a variable is unrelated to any other observed or unobserved variable, then the data are m issing completely at random (MCAR)
If there’s no systematic reason why dream sleep is missing for these 12 animals, the data is said to be MCAR.
Note that if every variable with missing data is MCAR, you can consider the complete cases to be a simple random sample from the larger dataset.
Missing at random—If the presence of missing data on a variable is related to other observed variables but not to its own unobserved value, the data is m issing at random (MAR)
For example, if animals with smaller body weights are more likely to have missing values for dream sleep (perhaps because it’s harder to observe smaller animals), and the “missingness” is unrelated to an animal’s time spent dreaming, the data would be considered MAR.
In this case, the presence or absence of dream sleep data would be random, once you controlled for body weight.
Not missing at random—If the missing data for a variable is neither MCAR nor MAR, it is n ot missing at random (NMAR)
For example, if animals that spend less time dreaming are also more likely to have a missing dream value (perhaps because it’s harder to measure shorter events), the data would be considered NMAR.
Most approaches to missing data assume that the data is either MCAR or MAR.
In this case, you can ignore the mechanism producing the missing data and (after replacing or deleting the missing data) model the relationships of interest directly.
When data is NMAR, you have to model the mechanisms that produced the missing values, as well as the relationships of interest.
Current approaches to analyzing NMAR data include the use of selection models and pattern mixtures.
The analysis of NMAR data can be quite complex and is beyond the scope of this book.
Figure 15.1 Methods for handling incomplete data, along with the R packages that support them.
There are many methods for dealing with missing data—and no guarantee that they’ll produce the same results.
Figure 15.1 describes an array of methods used for handling incomplete data and the R packages that support them.
A complete review of missing data methodologies would require a book in itself.
In this chapter, we’ll review methods for exploring missing values patterns and focus on the three most popular methods for dealing with incomplete data (a rational approach, listwise deletion, and multiple imputation)
We’ll end the chapter with a brief discussion of other methods, including those that are useful in special  circumstances.
In addition, the symbols Inf and -Inf represent positive infinity and negative infinity, respectively.
The functions i s.na(), i s.nan(), and i s.infinite() can be used to identify missing, impossible, and infinite values respectively.
Table 15.1 Examples of return values for the is.na(), is.nan(), and is.infinite() functions.
These functions return an object that’s the same size as its argument, with each element replaced by TRUE if the element is of the type being tested, and FALSE otherwise.
Then is.na(y) will return the vector c(FALSE, FALSE, FALSE, TRUE)
The function c omplete.cases() can be used to identify the rows in a matrix or data frame that don’t contain missing data.
It returns a logical vector with TRUE for every row that contains complete cases and FALSE for every row that has one or more missing values.
The results indicate that there are 12 missing values for the variable D ream.
Nineteen percent of the cases have a missing value on this variable.
In addition, 32 percent of the cases in the dataset contain one or more missing values.
There are two things to keep in mind when identifying missing values.
First, the c omplete.cases() function only identifies NA and NaN as missing.
Infinite values (Inf and –Inf) are treated as valid values.
Second, you must use missing values functions, like those in this section, to identify the missing values in R data objects.
Logical comparisons such as myvar == NA are never true.
Now that you know how to identify missing values programmatically, let’s look at tools that help you explore possible patterns in the occurrence of missing  data.
Before  deciding how to deal with missing data, you’ll find it useful to determine which variables have missing values, in what amounts, and in what combinations.
In this section, we’ll review tabular, graphical, and correlational methods for exploring missing values patterns.
Ultimately, you want to understand why the data is missing.
The answer will have implications for how you proceed with further analyses.
You’ve    already seen a rudimentary approach to identifying missing values.
You can use the c omplete.cases() function from section 15.2 to list cases that are complete, or conversely, list cases that have one or more missing values.
As the size of a dataset grows, though, it becomes a less attractive approach.
In this case, you can turn to other R functions.
The m d.pattern() function in the m ice package will produce a tabulation of the missing data patterns in a matrix or data frame.
Applying this function to the sleep dataset, you get the following:
The first row describes the pattern of “no missing values” (all elements are 1)
The second row describes the pattern “no missing values except for Span.” The first column indicates the number of cases in each missing data pattern, and the last column indicates the number of variables with missing values present in each pattern.
The last row gives the total number of missing values present on each    variable.
Although  the tabular output from the md.pattern() function is compact, I often find it easier to discern patterns visually.
Luckily, the VIM package provides numerous functions for visualizing missing values patterns in datasets.
In this section, we’ll review several, including aggr(), matrixplot(), and scattMiss()
The a ggr() function plots the number of missing values for each variable alone and for each combination of variables.
Figure 15.2 aggr() produced plot of missing values patterns for the sleep dataset.
You can close it; we’ll be using code to accomplish the tasks in this chapter.
The m atrixplot() function produces a plot displaying the data for each case.
A graph created using matrixplot(sleep) is displayed in figure 15.3
Note that in figure 15.3, red has been replaced with crosshatching by hand, so that the missing values are viewable in grayscale.
It will look different when you create the graph yourself.
The graph is interactive: clicking on a column will re-sort the matrix by that variable.
The rows in figure 15.3 are sorted in descending order by BodyWgt.
A matrix plot allows you to see if the presence of missing values on one or more variables is related to the actual values of other variables.
Here, you can see that there are no missing values on sleep variables (Dream, NonD, Sleep) for low values of body or brain weight (BodyWgt, BrainWgt)
Figure 15.3 Matrix plot of actual and missing values by case (row) for the sleep dataset.
The m arginplot() function produces a scatter plot between two variables with information about missing values shown in the plot’s margins.
Consider the relationship between amount of dream sleep and the length of a mammal’s gestation.
The p ch and c ol parameters are optional and provide control over the plotting symbols and colors used.
The body of the graph displays the scatter plot between Gest and Dream (based on complete cases for the two variables)
In the left margin, box plots display the distribution of Dream for mammals with (dark gray) and without (red) Gest values.
Four red dots represent the values of Dream for mammals missing Gest scores.
In the bottom margin, the roles of Gest and Dream are reversed.
You can see that a negative relationship exists between length of gestation and dream sleep and that dream sleep tends to be higher for mammals that are missing a gestation score.
The number of observations with missing values on both variables at the same time is printed in blue at the intersection of both margins (bottom left)
The V IM package has many graphs that can help you understand the role of missing data in a dataset and is well worth exploring.
There are functions to produce scatter plots, box plots, histograms, scatter plot matrices, parallel plots, rug plots, and bubble plots that incorporate information about missing  values.
Figure 15.4 Scatter plot between amount of dream sleep and length of gestation, with information about missing data in the margins.
The resulting matrix is sometimes called a s hadow matrix.
Correlating these indicator variables with each other and with the original (observed) variables can help you to see which variables tend to be missing together, as well as relationships between a variable’s “missingness” and the values of the other variables.
You can see this by viewing the first few rows of each:
Here, you can see that Dream and NonD tend to be missing together (r=0.91)
Finally, you can look at the relationship between the presence of missing values in a variable and the observed values on other variables:
In this correlation matrix, the rows are observed variables, and the columns are indicator variables representing missingness.
You can ignore the warning message and NA values in the correlation matrix; they’re artifacts of our approach.
None of the correlations in this table are particularly large or striking, which suggests that the data deviates minimally from MCAR and may be MAR.
Note that you can never rule out the possibility that the data are NMAR because you don’t know what the actual values would have been for data that are missing.
For example, you don’t know if there’s a relationship between the amount of dreaming a mammal engages in and the probability of obtaining a missing value on this variable.
In the absence of strong external evidence to the contrary, we typically assume that data is either MCAR    or  MAR.
Answers to these questions will help determine which statistical methods are most appropriate for analyzing your data.
For example, if the missing data are concentrated in a few relatively unimportant variables, you may be able to delete these variables and continue your analyses normally.
If there’s a small amount of data (say less than 10 percent) that’s randomly distributed throughout the dataset (MCAR), you may be able to limit your analyses to cases with complete data and still get reliable and valid results.
If you can assume that the data are either MCAR or MAR, you may be able to apply multiple imputation methods to arrive at valid conclusions.
If the data are NMAR, you can turn to specialized methods, collect new data, or go into an easier and more rewarding profession.
In a recent survey employing paper questionnaires, I found that several items tended to be missing together.
It became apparent that these items clustered together because participants didn’t realize that the third page of the questionnaire had a reverse side containing them.
In another study, an education variable was frequently missing in a global survey of leadership styles.
Investigation revealed that European participants were more likely to leave this item blank.
It turned out that the categories didn’t make sense for participants in certain countries.
Finally, I was involved in a study of depression in which older patients were more likely to omit items describing depressed mood when compared with younger patients.
Interviews revealed that older patients were loath to admit to such symptoms because doing so violated their values about keeping a “stiff upper lip.” Unfortunately, it was also determined that severely depressed patients were more likely to omit these items due to a sense of hopelessness and difficulties with concentration.
In this case, the data had to be considered NMAR.
As you can see, the identification of patterns is only the first step.
You need to bring your understanding of the research subject matter and the data collection process to bear in order to determine the source of the missing values.
Now that we’ve considered the source and impact of missing data, let’s see how standard statistical approaches can be altered to accommodate them.
We’ll focus on three approaches that are very popular: a rational approach for recovering data, a traditional approach that involves deleting missing data, and a modern approach that involves the use of simulation.
Along the way, we’ll briefly look at methods for specialized situations, and methods that have become obsolete and should be retired.
Our goal will remain constant: to answer, as accurately as possible, the substantive questions that led us to collect the data, given the absence of complete  information.
In  a rational approach, you use mathematical or logical relationships among variables to attempt to fill in or recover the missing values.
In the sleep dataset, the variable S leep is the sum of the D ream and N onD variables.
If you know a mammal’s scores on any two, you can derive the third.
Thus, if there were some observations that were missing only one of the three variables, you could recover the missing information through addition or subtraction.
As a second example, consider research that focuses on work/ life balance differences between generational cohorts (for example, Silents, Early Boomers, Late Boomers, Xers, Millennials), where cohorts are defined by their birth year.
Participants are asked both their date of birth and their age.
If date of birth is missing, you can recover their birth year (and therefore their generational cohort) by knowing their age and the date they completed the survey.
An example that uses logical relationships to recover missing data comes from a set of leadership studies in which participants were asked if they were a manager (yes/ no) and the number of their direct reports (integer)
If they left the manager question blank but indicated that they had one or more direct reports, it would be reasonable to infer that they were a manager.
As a final example, I frequently engage in gender research that compares the leadership styles and effectiveness of men and women.
Participants complete surveys that include their name (first and last), gender, and a detailed assessment of their leadership approach and impact.
If participants leave the gender question blank, I have to impute the value in order to include them in the research.
To remedy the situation, I employed the following rational process.
Some first names were associated with males, some with females, and some with both.
For example, “William” appeared 417 times and was always a male.
If a first name appeared more than 20 times in the dataset and was always associated with males or with females (but never both), I assumed that the name represented a single gender.
I used this assumption to create a gender lookup table for gender-specific first names.
A rational approach typically requires creativity and thoughtfulness, along with a degree of data management skill.
Data recovery may be exact (as in the sleep example) or approximate (as in the gender example)
In the next section, we’ll explore an approach that creates complete datasets by removing  observations.
Practically, this involves deleting any row containing one or more missing values, and is also known as listwise, or case-wise, deletion.
Most popular statistical packages employ listwise deletion as the default approach for handling missing data.
In fact, it’s so common that many analysts carrying out analyses like regression or ANOVA may not even realize that there’s a “missing values problem” to be dealt with!
The function c omplete.cases() can be used to save the cases (rows) of a matrix or data frame without missing data:
The same result can be accomplished with the n a.omit function:
In both statements, any rows containing missing data are deleted from mydata before the results are saved to newdata.
Suppose you’re interested in the correlations among the variables in the sleep study.
Applying listwise deletion, you’d delete all mammals with missing data prior to calculating the correlations:
The correlations in this table are based solely on the 42 mammals that have complete data on all variables.
If you wanted to study the impact of life span and length of gestation on the amount of dream sleep, you could employ linear regression with listwise deletion:
Call: lm(formula = Dream ~ Span + Gest, data = na.omit(sleep))
Here you see that mammals with shorter gestation periods have more dream sleep (controlling for life span) and that life span is unrelated to dream sleep when controlling for gestation period.
The analysis is based on 42 cases with complete data.
Cases with any missing data on the variables fitted by the function (Dream, Span, and Gest in this case) would have been deleted.
Listwise deletion assumes that the data are MCAR (that is, the complete observations are a random subsample of the full dataset)
To the degree that the MCAR assumption is violated, the resulting regression parameters will be biased.
Deleting all observations with missing data can also reduce statistical power by reducing the available sample size.
In the current example, listwise deletion reduced the sample size by 32 percent.
Next, we’ll consider an approach that employs the entire dataset (including cases with missing    data)
Monte Carlo methods are used to fill in the missing data in each of the simulated datasets.
Standard statistical methods are applied to each of the simulated datasets, and the outcomes are combined to provide estimated results and confidence intervals that take into account the uncertainty introduced by the missing values.
Good implementations are available in R through the A melia, m ice, and m i packages.
In this section we’ll focus on the approach provided by the mice (multivariate imputation by chained equations) package.
Figure 15.5 Steps in applying multiple imputation to missing data via the mice approach.
To understand how the mice package operates, consider the diagram in figure 15.5
The function m ice() starts with a data frame containing missing data and returns an.
Each complete dataset is created by imputing values for the missing data in the original data frame.
There’s a random component to the imputations, so each complete dataset is slightly different.
The w ith() function is then used to apply a statistical model (for example, linear or generalized linear model) to each complete dataset in turn.
Finally, the p ool() function combines the results of these separate analyses into a single set of results.
The standard errors and p-values in this final model correctly reflect the uncertainty produced by both the missing values and the multiple imputations.
How does the m ice() function impute missing values? Missing values are imputed by G ibbs sampling.
By default, each variable containing missing values is predicted from all other variables in the dataset.
These prediction equations are used to impute plausible values for the missing data.
The process iterates until convergence over the missing values is achieved.
For each variable, the user can choose the form of the prediction model (called an elementary imputation method), and the variables entered into it.
By default, predictive mean matching is used to replace missing data on continuous variables, while logistic or polytomous logistic regression is used for target variables that are dichotomous (factors with two levels) or polytomous (factors with more than two levels) respectively.
Other elementary imputation methods include Bayesian linear regression, discriminant function analysis, two-level normal imputation, and random sampling from observed values.
An analysis based on the mice package will typically conform to the following structure:
Examples include lm() for linear regression models, glm() for generalized linear models, gam() for generalized additive models, and nbrm() for negative binomial models.
Formulas within the parentheses give the response variables on the left of the ~ and the predictor variables (separated by + signs) on the right.
Set the seed value for the random number generator to 1234 so that your results will match mine.
By the way, the f mi column reports the fraction of missing information (that is, the proportion of variability that is attributable to the uncertainty introduced by the missing data)
You can access more information about the imputation by examining the objects created in the analysis.
For example, let’s view a summary of the imp object:
From the resulting output, you can see that five synthetic datasets were created, and that the  predictive mean matching (pmm) method was used for each variable with missing data.
No imputation ("") was needed for BodyWgt, BrainWgt, Pred, Exp, or Danger, because they had no missing values.
The Visit Sequence tells you that variables were imputed from right to left, starting with NonD and ending with Gest.
Finally, the Predictor Matrix indicates that each variable with missing data was imputed using all the other variables in the dataset.
You can view the actual imputations by looking at subcomponents of the imp object.
A review of these matrices helps you determine if the imputed values are reasonable.
A negative value for length of sleep might give you pause (or nightmares)
You can view each of the m imputed datasets via the c omplete() function.
Due to space limitations, we’ve only briefly considered the MI implementation provided in the m ice package.
The m i and A melia packages also contain valuable approaches.
If you are interested in the multiple imputation approach to missing data, I recommend the following resources:
Each can help to reinforce and extend your understanding of this important, but underutilized,    methodology.
Although not as broadly applicable as the methods described thus far, the packages described in table 15.2 offer functions that can be quite useful in specialized circumstances.
Finally, there are two methods for dealing with missing data that are still in use, but should now be considered obsolete.
Pairwise    deletion is often considered an alternative to listwise deletion when working with datasets containing missing values.
In pairwise deletion, observations are only deleted if they’re missing data for the variables involved in a specific analysis.
In this example, correlations between any two variables use all available observations for those two variables (ignoring the other variables)
The correlation between BodyWgt and BrainWgt is based on all 62 mammals (the number of mammals with data on both variables)
Although pairwise deletion appears to use all available data, in fact each calculation is based on a different subset of the data.
In    simple imputation, the missing values in a variable are replaced with a single value (for example, mean, median, or mode)
Note that the substitution is nonstochastic, meaning that random error isn’t introduced (unlike multiple imputation)
An advantage to simple imputation is that it solves the “missing values problem” without reducing the sample size available for the analyses.
Simple imputation is, well, simple, but it produces biased results for data that aren’t MCAR.
If there are moderate to large amounts of missing data, simple imputation is likely to underestimate standard errors, distort correlations among variables, and produce incorrect p-values in statistical tests.
Like pairwise deletion, I recommend avoiding this approach for most missing data problems.
Most statistical methods assume that the input data is complete and doesn’t include missing values (for example, NA, NaN, Inf)
Therefore, you must either delete the missing values or replace them with reasonable substitute values before continuing with the desired analyses.
Often, statistical packages will provide default methods for handling missing data, but these approaches may not be optimal.
Therefore, it’s important that you understand the various approaches available, and the ramifications of using each.
In this chapter, we examined methods for identifying missing values and exploring patterns of missing data.
Our goal was to understand the mechanisms that led to the missing data and their possible impact on subsequent analyses.
We then reviewed three popular methods for dealing with missing data: a rational approach, listwise deletion, and the use of multiple imputation.
Rational approaches can be used to recover missing values when there are redundancies in the data, or external information that can be brought to bear on the problem.
The listwise deletion of missing data is useful if the data are MCAR and the subsequent sample size reduction doesn’t seriously impact the power of statistical tests.
Multiple imputation is rapidly becoming the method of choice for complex.
Although many analysts may be unfamiliar with multiple imputation strategies, user-contributed packages (mice, mi, Amelia) make them readily accessible.
I believe that we’ll see a rapid growth in their use over the next few years.
We ended the chapter by briefly mentioning R packages that provide specialized approaches for dealing with missing data, and singled out general approaches for handling missing data (pairwise deletion, simple imputation) that should be avoided.
In the next chapter, we’ll explore advanced graphical methods, including the use of lattice graphs, the ggplot2 system, and    interactive graphical  methods.
In previous chapters, we created a wide variety of both general and specialized graphs (and had lots of fun in the process)
Given the diversity of methods available in R, it may not surprise you to learn that there are actually four separate and complete graphics systems currently available.
In addition to base graphics, we have graphics systems provided by the grid, lattice, and ggplot2 packages.
Each is designed to expand on the capabilities of, and correct for deficiencies in, R’s base graphics system.
The grid graphics system provides low-level access to graphic primitives, giving programmers a great deal of flexibility in the creation of graphic output.
The ggplot2 package provides a method of creating innovative graphs based on a comprehensive graphical “grammar.”
In this chapter, we’ll start with an overview of the four graphic systems.
Then we’ll focus on graphs that can be generated with the lattice and ggplot2 packages.
These packages greatly expand the range and quality of the graphs you can produce in R.
Interacting with graphs in real time can help you understand your data more thoroughly and develop greater insights into the relationships among variables.
Here, we’ll focus on the functionality offered by the iplots, playwith, latticist, and rggobi packages.
As  stated earlier, four primary graphical systems are available in R.
The base graphic system in R, written by Ross Ihaka, is included in every R installation.
Most of the graphs produced in previous chapters rely on base graphics functions.
The grid graphics system, written by Paul Murrell (2006), is implemented through the grid package.
Grid graphics offer a lower-level alternative to the standard graphics system.
The user can create arbitrary rectangular regions on graphics devices, define coordinate systems for each region, and use a rich set of drawing primitives to control the arrangement and appearance of graphic elements.
This flexibility makes grid graphics a valuable tool for software developers.
But the grid package   doesn’t provide functions for producing statistical graphics or complete plots.
Because of this, the package is rarely used directly by data analysts.
Built using the grid package, the lattice package   has grown beyond Cleveland’s original approach to visualizing multivariate data, and now provides a comprehensive alternative system for creating statistical graphics in R.
The intention of the ggplot2 package   is to provide a comprehensive, grammar-based system for generating graphs in a unified and coherent manner, allowing users to create new and innovative data visualizations.
Access to the four systems differs, as outlined in table 16.1
To access grid   and lattice function s, you must load the package explicitly (for example, library(lattice))
Because our attention is primarily focused on practical data analyses, we won’t elaborate on the grid package in this chapter.
Instead, we’ll explore the lattice and ggplot2 packages in some detail.
Each allows you to create unique and useful graphs that aren’t easily created in other  ways.
The    lattice package provides a comprehensive graphical system for visualizing univariate and multivariate data.
In particular, many users turn to the lattice package because of its ability to easily generate trellis graphs.
A trellis graph displays the distribution of a variable or the relationship between variables, separately for each level of one or more other variables.
Consider the following question: How do the heights of singers in the New York Choral Society vary by their vocal parts?
Data on the heights and voice parts of choral members is provided in the singer dataset contained in the lattice package.
It appears that tenors and basses tend to be taller than altos and sopranos.
In trellis graphs, a separate panel is created for each level of the conditioning variable.
If more than one conditioning variable is specified, a panel is created for each combination of factor levels.
The panels are arranged into an array to facilitate comparisons.
A label is provided for each panel in an area called the strip.
As you’ll see, the user has control over the graph displayed in each panel, the format and placement of the strip, the arrangement of the panels, the placement and content of legends, and many other graphic features.
The lattice package provides a wide variety of functions for producing univariate (dot plots, kernel density plots, histograms, bar charts, box plots), bivariate (scatter plots, strip plots, parallel box plots), and multivariate (3D plots, scatter plot matrices) graphs.
Figure 16.1 Trellis graph of singer heights by voice pitch.
Let lowercase letters represent numeric variables and uppercase letters represent categorical variables (factors)
The formula in a high-level graphing function typically takes the form.
Primary variables map variables to the axes in each panel.
Here, y~x describes the variables to place on the vertical and horizontal axes, respectively.
Finally, for multivariate plots (scatter plot matrix or parallel coordinates plot) replace y~x with a data frame.
Following this logic, ~x|A displays numeric variable x for each level of factor A.
A~x displays categorical variable A on the vertical axis and numeric variable x on the horizontal axis.
To gain a quick overview of lattice graphs, try running the code in listing 16.1
Table 16.2 Graph types   and corresponding functions in the lattice package.
Kernel density plot densityplot() ~x|A*B Parallel coordinates plot parallel() dataframe.
Note: In these formulas, lowercase letters represent numeric variables and uppercase letters represent categorical variables.
You may want to vary the formulas and view the results.
High-level plotting functions in the lattice package   produce graphic objects that can be saved and manipulated.
Issuing the statement plot(mygraph) (or simply mygraph) will display the graph.
It’s easy to modify lattice graphs through the use of options.
You’ll see examples of many of these later in the chapter.
If desired, a third element can be added to indicate the number of pages.
Table 16.3 Common options for lattice high-level graphing functions (continued)
You can issue these options in the high-level function calls or within the panel functions discussed in section 16.2.2
You can also use the update() function  to modify a lattice graphic object.
Now that we’ve reviewed the general structure of a highlevel lattice function, let’s look at conditioning variables in more detail.
As    you’ve seen, one of the most powerful features of lattice graphs is the ability to add conditioning variables.
If one conditioning variable is present, a separate panel is created for each level.
If two conditioning variables are present, a separate panel is created for each combination of levels for the two variables.
It’s rarely useful to include more than two conditioning variables.
But what if you want to condition on a continuous variable? One approach would be to transform the continuous variable into a discrete variable using R’s cut() function.
Alternatively, the lattice package provides functions for transforming a continuous variable into a data structure called a shingle.
Specifically, the continuous variable is divided up into a series of (possibly) overlapping ranges.
Printing or plotting this object (for example, plot(myshingle))   will display the shingle’s intervals.
Once a continuous variable has been converted to a shingle, you can use it as a conditioning variable.
For example, let’s use the mtcars dataset to explore the relationship between miles per gallon and car weight conditioned on engine displacement.
Because engine displacement is a continuous variable, first let’s convert it to a shingle variable with three levels:
Note that we’ve also used options to modify the layout of the panels (three columns and one row) and the aspect ratio (height/width) in order to make comparisons among the three groups easier.
The representation in figure 16.2 indicates the continuous nature of the conditioning variable, with the darker color indicating the range of values for the conditioning variable in the given panel.
In the next section, we’ll use panel functions to customize the output    further.
Figure 16.2 Trellis plot of mpg versus car weight conditioned on engine displacement.
Because engine displacement is a continuous variable, it has been converted to three nonoverlapping shingles with equal numbers of observations.
Each    of the high-level plotting functions in table 16.2 employs a default function to draw the panels.
You can incorporate one or more of the 50+ default panel functions in the lattice package   into your customized function as well.
Customized panel functions give you a great deal of flexibility in designing an output that meets your needs.
In the previous section, you plotted gas mileage by automobile weight, conditioned on engine displacement.
What if you wanted to include regression lines, rug plots, and grid lines? You can do this by creating your own panel function (see the following listing)
Here, we’ve wrapped four separate building block functions into our own mypanel() function and applied it within xyplot() through the panel= option q.
The panel.grid() function   adds horizontal and vertical grid lines (using negative numbers forces them to line up with the axis labels)
Each default panel function has its own structure and options.
Figure 16.3 Trellis plot of mpg versus car weight conditioned on engine displacement.
A custom panel function has been used to add regression lines, rug plots, and grid lines.
As a second example, we’ll graph the relationship between gas mileage and engine displacement (considered as a continuous variable), conditioned on type of automobile transmission.
In addition to creating separate panels for automatic and manual transmission engines, we’ll add smoothed fit lines and horizontal mean lines.
Listing 16.3 xyplot with custom panel function and additional options.
The graph produced by this code is provided in figure 16.4
Figure 16.4 Trellis graph of mpg versus engine displacement conditioned on transmission type.
Smoothed lines (loess), grids, and group mean levels have been added.
There are several things to note in this new code.
The panel.xyplot() function plots the individual points, and the panel.loess() function   plots nonparametric fit lines in each panel.
The panel.abline() function   adds horizontal reference lines at the mean mpg value for each level of the conditioning variable.
The scales= option renders scale annotations in red and at 80 percent of their default size.
See help(xyplot) for details on the many scale options available.
In the next section, you’ll learn how to superimpose data from groups of observations, rather than presenting them in separate    panels.
When    you include a conditioning variable in a lattice graph formula, a separate panel is produced for each level of that variable.
If you want to superimpose the results for each level instead, you can specify the variable as a group variable.
Let’s say that you want to display the distribution of gas mileage for cars with manual and automatic transmissions using kernel density plots.
By default, the group=option   superimposes the plots from each level of the grouping variable.
Points are plotted as open circles, lines are solid, and level information is distinguished by color.
As you can see, the colors are difficult to differentiate when printed in grayscale.
The option auto.key=TRUE will create a rudimentary legend and place it above the graph.
You can make limited changes to this automated key by specifying options in a list.
If you want to exert greater control over the legend, you can use the key= option.
Figure 16.5 Kernel density plots for miles per gallon grouped by transmission type.
Listing 16.4 Kernel density plot with a group variable and customized legend.
Here, the plotting symbols, line types, and colors are specified as vectors q.
The first element of each vector will be applied to the first level of the group variable, the second element to the second level, and so forth.
A list object is created to hold the legend options w.
These options place the legend below the graph in two columns, and include the level names, point symbols, line types, and colors.
The legend title is rendered slightly larger than the text for the symbols.
Figure 16.6 Kernel density plots for miles per gallon grouped by transmission type.
Graphical parameters have been modified and a customized legend has been added.
The custom legend specifies color, shape, line type, character size, and title.
The same plot symbols, line types, and colors are specified within the densityplot() function   e.
Additionally, the line width and jitter are increased to improve the appearance of the graph.
Finally, the key is set to use the previously defined list.
This approach to specifying a legend for the grouping variable allows a great deal of flexibility.
In fact, you can create more than one legend and place them in different areas of the graph (not shown here)
Before completing this section, let’s consider an example that includes group and conditioning variables in a single plot.
The CO2 data frame, included with the base R installation, describes a study of cold tolerance of the grass species Echinocholoa crus-galli.
The data describe carbon dioxide uptake rates (uptake) for 12 plants (Plant), at seven ambient carbon dioxide concentrations (conc)
Six plants were from Quebec and six plants were from Mississippi.
Three plants from each location were studied under chilled conditions and three plants were studied under nonchilled conditions.
Listing 16.5 xyplot with group and conditioning variables and customized legend.
Note the use of \n to give you a two-line title and the use of the expression() function   to add mathematical notation to the axis labels.
Here, color is suppressed as a group differentiator by specifying a single color in the col= option.
In this case, adding 12 different colors is overkill and distracts from the goal of easily visualizing the relationships in each panel.
Clearly, there’s something different about the Mississippi grasses in the chilled condition.
Plant is the group variable and Treatment and Type are the conditioning variables.
But such changes are in effect only for the duration of the function call.
In the next section, we’ll review a method for changing graphical parameters that persists for the duration of the interactive session or batch    execution.
In    chapter 3, you learned how to view and set default graphics parameters using the par() function.
Although this works for graphs produced with R’s native graphic system, lattice graphs are unaffected by these settings.
The show.settings() function   can be used to display the current graphic settings visually.
As an example, let’s change the default symbol used for superimposed points (that is, points in a graph that includes a group variable)
First, view the current defaults and save them into a list called mysettings:
Next, look at the defaults that are specific to superimposed symbols:
Here you see that the symbol used for each level of a group variable is an open circle (pch=1)
The changes will remain in effect until all graphic devices are closed.
In    chapter 3 you learned how to place more than one graph on a page using the par() function.
Because lattice functions don’t recognize par() settings, you’ll need a different approach.
The easiest method involves saving your lattice graphs as objects, and using the plot() function   with either the split= or position= option specified.
The split option   divides a page up into a specified number of rows and columns and places graphs into designated cells of the resulting matrix.
Specifically, the first plot() statement divides the page up into one column and two rows and places the graph in the first column and first row (counting top-down and left-right)
The second plot() statement divides the page up in the same way, but places the graph in the first column and second row.
Because the plot() function   starts a new page by default, you suppress this action by including the newpage=FALSE option.
You can gain more control of sizing and placement by using the position= option.
You can also change the order of the panels in a lattice graph.
The index.cond= option in a high-level lattice graph function specifies the order of the conditioning variable levels.
When there are two conditioning variables, include two vectors in the list.
To learn more about lattice graphs, take a look the excellent text by Sarkar (2008) and its supporting website at http://lmdvr.r-forge.r-project.org.
In the next section, we’ll explore a second comprehensive alternative to R’s native graphic system.
The    ggplot2 package implements a system for creating graphics in R based on a comprehensive and coherent grammar.
This provides a consistency to graph creation often lacking in R, and allows the user to create graph types that are innovative and novel.
The simplest approach for creating graphs in ggplot2 is through the qplot() or quick plot function.
Associates the levels of variable with symbol color, shape, or size.
For line plots, color associates levels of a variable with line color.
For density and box plots, fill associates fill colors with a variable.
To create trellis graphs based on a single conditioning variable, use rowvar~
The geom option is expressed as a character vector with one or more entries.
If geom="smooth", a loess fit line and confidence limits are added by default.
When the number of observations is greater than 1,000, a more efficient smoothing algorithm is employed.
Methods include "lm" for regression, "gam" for generalized additive models, and "rlm" for robust regression.
For example, to add simple linear regression lines, you’d specify geom="smooth", method="lm", formula=y~x.
Changing the formula to y~poly(x,2) would produce a quadratic fit.
Note that the formula uses the letters x and y, not the names of the variables.
The following code creates box plots of gas mileage by number of cylinders.
The actual data points are superimposed (and jittered to reduce overlap)
As a second example, let’s create a scatter plot matrix of gas mileage by car weight.
Additionally, we’ll add separate regression lines and confidence bands for each transmission type.
Figure 16.8 Box plots of auto mileage by number of cylinders.
Figure 16.9 Scatter plot between auto mileage and car weight, with separate regression lines and confidence bands by engine transmission type (manual, automatic)
This is a useful type of graph, not easily created using other packages.
As a third example, we’ll create a faceted (trellis) graph.
Each facet (panel) displays the scatter plot between gas mileage and car weight.
Row facets are defined by the transmission type, whereas column facets are defined by the number of cylinders present.
The size of each data point represents the car’s horsepower rating.
Note how simple it is to create a complex graph (actually a bubble chart)
You may want to try adding shape and color options to the function call and see how the resulting graph is affected.
We’ll end this section by revisiting the singer data   with which we began the chapter.
Figure 16.11 Faceted density plots for singer heights by voice part.
Comparing the distribution of heights is easier in this format than in the format presented in figure 16.1
We’ve only scratched the surface of this powerful graphical system.
We’ll end this chapter with a review of interactive graphics and R functions that support    them.
The    base installation of R provides limited interactivity with graphs.
You can modify graphs by issuing additional program statements, but there’s little that you can do to modify them or gather new information from them using the mouse.
However, there are contributed packages that greatly enhance your ability to interact with the graphs you create.
In this section, we’ll focus on functions provided by the playwith , latticist, iplots , and rggobi packages.
Before    getting to the specialize packages, let’s review a function in the base R installation that allows you to identify and label points in scatter plots.
Using the identify() function , you can label selected points in a scatter plot with their row number or row name using your mouse.
Identification continues until you select Stop or right-click on the graph.
Clicking on scatter plot points will label them until you select Stop from the Graphics Device menu or right-click on the graph and select Stop from the context menu.
Many graphic functions in contributed packages (including functions from the car package discussed in chapter 8) employ this method for labeling points.
Unfortunately, the identify() function doesn’t work with lattice or ggplot2    graphs.
The    playwith package provides a GTK+ graphical user interface that allows users to edit and interact with R plots.
On platforms running Mac OS X and Linux, it’s best to also install the JGR graphic user interface (see appendix A), and run playwith from within this GUI.
The playwith()function allows users to identify and label points, view all variable values for an observation, zoom and pan, add annotations (text, arrows, lines, rectangles, titles, labels), change visual elements (colors, text sizes, and so on), apply previously saved styles, and output the resulting graph in a variety of formats.
Try out the buttons on the left, as well as the menu items.
Unlike the identify() function, playwith() works with lattice and ggplot2 graphs as well as base R graphs.
Some options in the Theme menu only work properly with base graphics.
Additionally, some features work with ggplot2 graphs (such as annotating) and some don’t (such as identifying points)
To learn more about the playwith package, visit the project website at http://code.
The user can edit the graph using the mouse with this GTK+ GUI.
The    latticist package lets you explore a data set using lattice displays.
If desired, latticist and can also be integrated with playwith.
A similar interface is available for ggplot2 graphs , through Plot Builder, a plug-in for Deducer, a popular GUI for R (see appendix A)
Because it can’t be run from the R console, we won’t discuss it here.
Whereas    playwith and latticist allow you to interact with a single graph, the iplots package takes interaction in a different direction.
This package provides interactive mosaic plots, bar plots, box plots, parallel plots, scatter plots, and histograms that can be linked together and color brushed.
This means that you can select and identify observations using the mouse, and highlighting observations in one graph will automatically highlight the same observations in all other open graphs.
You can also use the mouse to obtain information about graphic objects such as points, bars, lines, and box plots.
The iplots package is implemented through Java and the primary functions are listed in table 16.5
To understand how iplots works, execute the code provided in listing 16.6
Rearrange them on the desktop so that each is visible (each can be resized if necessary)
A portion of the display is provided in figure 16.14
Click on the three-gear bar in the Barchart (gears) window.
In addition, all cars with three-gear engines will be highlighted in the other graphic windows.
Mouse down and drag to select a rectangular region of points in the Scatter plot (wt vs mpg) window.
These points will be highlighted and the corresponding observations in every other graphics window will also turn red.
Hold down the Ctrl key and move the mouse pointer over a point, bar, box plot, or line in one of the graphs.
Details about that object will appear in a pop-up window.
Right-click on any object and note the options that are offered in the context menu.
For example, you can right-click on the Boxplot (mpg) window and change the graph to a parallel coordinates plot (PCP)
You can drag to select more than one object (point, bar, and so on) or use Shiftclick to select noncontiguous objects.
Try selecting both the three- and five-gear bars in the Barchart (gears) window.
The functions in the iplots package allow you to explore the variable distributions and relationships among variables in subgroups of observations that you select interactively.
This can provide insights that would be difficult and time-consuming to obtain in other ways.
For more information on the iplots package, visit the project website at http://rosuda.org/iplots/
Only four of the six windows are displayed to save room.
In these graphs, the user has clicked on the three-gear bar in the bar chart window.
GGobi is a comprehensive program for the visual and dynamic exploration of high-dimensional data and is freely available for Windows, Mac OS X, and Linux platforms.
Happily, the rggobi package provides a seamless interface between GGobi and R.
Once you’ve installed both, you can use the ggobi() function to run GGobi from within R.
This gives you sophisticated interactive graphics access to all of your R data.
The GGobi interface will open and allow you to explore the mtcars dataset in a highly interactive fashion.
To learn more, review the introduction, tutorial, manual, and video guides available on the GGobi website.
A comprehensive overview is also provided in Cook and Swayne (2008)
In this chapter, we reviewed several packages that provide access to advanced graphical methods.
We started with the lattice package, designed to provide a system for creating trellis graphs, followed by the ggplot2 package, based on a comprehensive grammar of graphics.
Both packages are designed to provide you with a complete and comprehensive alternative to the native graphics provided with R.
Each offers methods of creating attractive and meaningful visualizations of data that are difficult to generate in other ways.
We then explored several packages for dynamically interacting with graphs, including playwith, latticist, iplots, and rggobi.
These packages allow you to interact directly with data in graphs, leading to a greater intimacy with your data and expanded opportunities for developing insights.
You should now have a firm grasp of the many ways that R allows you to create visual representations of data.
If a picture is worth a thousand words, and R provides a thousand ways to create a picture, then R must be worth a million words (or something to that effect)
These resources are a testament to the hard and selfless work of the initial R development team and the thousands of hours of work contributed by    package authors.
We’ve covered a broad range of topics in the book, including major ones like the R development environment, data management, traditional statistical models, and statistical graphics.
We’ve also covered hidden gems like resampling statistics, missing values imputation, and interactive graphics.
The great (or perhaps infuriating) thing about R is that there’s always more to learn.
With so many new packages, frequent updates, and new directions, how can a user stay current? Happily, many websites support this active community and provide coverage of platform and package changes, new methodologies, and a wealth of tutorials.
The R Project (http://www.r-project.org/) The official R website and your first stop for all things R.
The R Journal (http://journal.r-project.org/) A freely accessible refereed journal containing articles on the R project and contributed packages.
Planet R (http://planetr.stderr.org) Another good site-aggregator, including information from a wide range of sources.
CRANberries (http://dirk.eddelbuettel.com/cranberries/) A site that aggregates information about new and updated packages, and contains links to CRAN for each.
Journal of Statistical Software (http://www.jstatsoft.org/) A freely accessible refereed journal containing articles, book reviews, and code snippets on statistical computing.
Revolutions (http://blog.revolution-computing.com/) A popular, well-organized blog, dedicated to news and information about R.
They include a description of the packages and methods available for a given area.
Currently there are 28 task views available (see table below)
It’s stocked with more than 80 brief tutorials on R topics.
The R community is a helpful, vibrant, and exciting lot.
You turned  here first, didn’t you? By default, R provides a simple command-line interface   (CLI)
The user enters statements at a command-line prompt   (> by default) and each command is executed one at a time.
For many data analysts, the CLI is one of R’s most significant limitations.
There have been a number of attempts to create more graphical interfaces, ranging from code editors that interact with R (such as RStudio), to GUIs for specific functions or packages (such as BiplotGUI), to full-blown GUIs that allow the user to construct analyses through interactions with menus and dialog boxes (such as R Commander)
Several of the more useful code editors   are listed in table A.1
Table A.1 Integrated development environments and syntax editors (continued )
The  code editors in table A.1 allow the user to edit and execute R code and include syntax highlighting, statement completion, object exploration, project organization, and online help.
Several promising full-blown GUIs for R are listed in table A.2
The GUIs available for R are less comprehensive and mature than those offered by SAS or IBM SPSS, but they’re developing    rapidly.
My favorite GUI for introductory statistics courses is R Commander   (shown in  figure A.2)
Finally, there are a number of applications that allow the user to create a GUI.
These include the R GUI Generator   (RGG) (http://rgg.r-forge.r-project.org/), and the fgui and twiddler packages   available from CRAN.
For more information, visit the R GUI Projects  page    at http://www.sciviews.org/_rgui/
One of    the first things that programmers like to do is customize their startup environment to conform to their preferred way of working.
Customizing the startup environment allows you to set R options, specify a working directory, load commonly used packages, load user-written functions, set a default CRAN download site , and perform any number of housekeeping tasks.
You can customize the R environment through either a site initialization file (Rprofile.site ) or a directory initialization file   (.Rprofile )
These are text files containing R code to be executed at startup.
It will then look for an .Rprofile file to source in the current working directory.
If R doesn’t find this file, it will look for it in the user’s home directory.
The .First() function   is executed at the start of each R session, and the .Last() function   is executed at the end of each session.
An example of an Rprofile.site file is shown in listing B.1
There are several things you should note about this file:
Setting a .libPaths value   allows you to create a local library for packages outside of the R directory tree.
This can be useful for retaining packages during an upgrade.
The .First() function   is an excellent place to load libraries that you use often, as well as source text files containing user-written functions that you apply frequently.
The .Last() function   is an excellent place for any cleanup activities, including archiving command histories, program output, and data files.
There are other ways to customize the startup environment, including the use of command-line options   and environment variables.
See help(Startup) and appendix  B in the Introduction to R manual (http://cran.r-project.org/doc/manuals/R-intro.pdf) for    more    details.
In chapter 2,    we reviewed a wide range of methods for importing data into R.
But there are times that you’ll want to go the other way—exporting data from R—so that data can be archived or imported into external applications.
In this appendix, you’ll learn how to output an R object to a delimited text file, an Excel spreadsheet, or a statistical application (such as SPSS, SAS, or Stata)
C.1 Delimited text file You    can use the write.table() function   to output an R object to a delimited text file.
Replacing sep="," with sep="\t" would save the data in a tab-delimited file.
By default, strings are enclosed in quotes ("") and missing values are written as    NA.
By default, the variable names in the dataset are used to create column headings in the spreadsheet and row names are placed in the first column of the spreadsheet.
The xlsx package   is a powerful tool for manipulating Excel 2007 workbooks.
C.3 Statistical applications The    write.foreign() function   in the foreign package   can be used to export a data frame to an external statistical application.
Two files are created—a freeformat text file containing the data, and a code file containing instructions for reading the data into the external statistical application.
Other values of package include "SAS " and "Stata "
To learn more about exporting data from R, see the R Data Import/Export documentation,    available    from http://cran.r-project.org/doc/manuals/R-data.pdf.
Research doesn’t    end when the last statistical analysis or graph is completed.
We need to include the results in a report that effectively communicates these findings to a teacher, supervisor, client, government agency, or journal editor.
Although R creates state-of-the-art graphics, its text output is woefully retro—tables of monospaced text with columns lined up using spaces.
There are two common approaches to creating publication quality reports in R: Sweave and odfWeave.
The Sweave package   allows you to embed R code and output in LaTeX documents, in order to produce high-end typeset reports in PDF, PostScript, and DVI formats.
Sweave is an elegant, precise, and highly flexible system, but it requires the author to be conversant with LaTeX coding.
In a similar fashion, the odfWeave package   provides a mechanism for embedding R code and output in documents that follow the Open Documents Format   (ODF)
These reports can be further edited via an ODF word processor, such as OpenOffice Writer, and saved in either ODF or Microsoft Word format.
The process is not as flexible as the Sweave approach, but it eliminates the need to learn LaTeX.
An author creates a text document that includes markup code for formatting the.
The document is then processed through a LaTeX compiler , producing a finished document in PDF, PostScript, or DVI format.
The Sweave package allows you to embed R code and output (including graphs) within the LaTeX document.
A special document called a noweb file   (typically with the extension .Rnw) is created using any text editor.
The file contains the written content, LaTeX markup code , and R code chunks.
The Sweave() function   processes the noweb file   and generates a LaTeX file.
During this step, the R code chunks are processed, and depending on options, replaced with LaTeX-formatted R code and output.
This step can be accomplished from within R or from the command line.
Specifying this syntax option can help avoid some common parsing errors, as well as conflicts with the R2HTML package.
Execution from the command line will depend on the operating system.
For example, on a Linux system, this might look like $ R CMD Sweave infile.Rnw.
The LaTeX file is then run through a LaTeX compiler, creating a PDF, PostScript, or DVI file.
LaTex (TeX) file Text file with LaTex markup and Rcode.
You can add options to each <<>>= delimiter in order to control the processing of the corresponding R code chunk.
Use results=tex when the output is generated by the xtable() function in the xtable package or the latex() function in the Hmisc package.
The xtable() function   in the xtable package   can be used to format data frames and matrices more precisely.
In addition, it can be used to format other R objects, including those produced by lm(), glm(), aov(), table(), ts(), and coxph()
When formatting R output using xtable() , be sure to include the results=tex option   in the code chunk delimiter.
The noweb file was processed through the Sweave() function   in R and the resulting TeX file   was processed through a LaTeX compiler   to produce a PDF document.
D.2 Joining forces with OpenOffice using odfWeave Sweave    provides a means of embedding R code and output in a LaTeX document that’s compiled into a PDF, PostScript, or DVI file.
Additionally, many recipients require reports in a format such as Word.
Once the noweb document is created as an ODT file, you process it through the odfWeave() function in the odfWeave package.
By default, odfWeave will render data frames, matrices, and vectors in an attractive format.
Optionally, the odfTable() function   can be used to format these objects with a high degree of control.
Therefore, the code chunk option result=tex should never be used.
If you look at Figure D.4, you’ll note that the ANOVA table   isn’t attractively formatted (as it was in Sweave)
Rather, the table is in the standard monospaced font produced by R.
This is because odfWeave doesn’t have a formatting function for the objects.
Figure D.4 Initial noweb file (example.odt) to be processed through odfWeave.
To properly format these results, we’d have to pull the components out of the object in question (fit in this case), and arrange them in a matrix or data frame.
Once you have your report in ODF format, you can continue to edit it, tighten up the formatting, and save the results to an ODT, HTML, DOC, or DOCX file format.
D.3 Comments There are several advantages to the Sweave and odfWeave approaches described here.
By embedding the code needed to perform the statistical analyses directly into the final report, you document exactly how the results were calculated.
Six months from now, you can easily see what was done.
You can also modify the statistical analyses or add new data and immediately regenerate the report with minimum effort.
Additionally, you avoid the need to cut and paste and reformat the results.
Unfortunately, you gain these advantages by putting in significantly more work at the front-end.
In the case of LaTeX, you need to learn a typesetting language.
In the case of ODF, you need to use a program like OpenOffice that may not be standard in your work environment.
For good or ill, Microsoft Word and PowerPoint are the current report and presentation standards in the business world.
Many of the functions described in this book operate on matrices.
The manipulation of matrices is built deeply into the R language.
Table E.1 describes operators and functions that are particularly important for solving linear algebra problems.
In the following table, A and B are matrices, x and b are vectors, and k is a scalar.
If R <- chol(A), then chol(A) contains the upper triangular factor, such that R’R = A.
Table E.1 R functions and operators for matrix algebra (continued )
There are several user-contributed packages that are particularly useful for matrix algebra.
The matlab package contains wrapper functions and variables used to replicate MATLAB function calls as closely as possible.
These functions can help port MATLAB applications and code to R.
There’s also a useful cheat sheet for converting MATLAB statements to R statements at http://mathesaurus.sourceforge.net/octave-r.html.
The Matrix package contains functions that extend R in order to support highly dense or sparse matrices.
It provides efficient access to BLAS (Basic Linear Algebra Subroutines), Lapack (dense matrix), TAUCS (sparse matrix), and UMFPACK (sparse matrix) routines.
Finally, the matrixStats package provides methods for operating on the rows and columns of matrices, including functions that calculate counts, sums, products, central tendency, dispersion, and more.
Table F.1 lists the user-contributed packages described in this book, along with the chapter(s) in which they appear.
Functions, data sets, examples, demos, and vignettes from the book Applied Econometrics with R by Christian Kleiber and Achim Zeileis.
Amelia II: A program for missing data via multiple imputation.
Table F.1 Contributed packages used in this book (continued )
Group-wise computations of summary statistics, general linear contrasts and other utilities.
Effect displays for linear, generalized linear, multinomial-logit, and proportional-odds logit models.
Table F.1 Contributed packages used in this book (continued )
Harrell miscellaneous functions for data analysis, high-level graphics, utility operations, and more.
Table F.1 Contributed packages used in this book (continued )
Functions to identify and parse date-time data, extract and modify components of a date-time, perform accurate math on date-times, and handle time zones and Daylight Savings Time.
Table F.1 Contributed packages used in this book (continued )
Simultaneous tests and confidence intervals for general linear hypotheses in parametric models, including linear, generalized linear, linear mixed effects, and survival models.
Table F.1 Contributed packages used in this book (continued )
Regression modeling strategies - about 225 function that assist with and streamline regression modeling, testing, estimations, validation, graphics, prediction, and typesetting.
Table F.1 Contributed packages used in this book (continued )
Ordination methods, diversity analysis, and other functions for community and vegetation ecologists.
Table F.1 Contributed packages used in this book (continued )
For most of us, this design decision has led to a zippy interactive experience, but for analysts working with large datasets, it can lead to slow program execution and memory-related errors.
Error messages starting with cannot allocate vector of size typically indicate a failure to obtain sufficient contiguous memory, while error messages starting with cannot allocate vector of length indicate that an address limit has been exceeded.
When working with large datasets, try to use a 64-bit build if at all possible.
For all builds, the number of elements in a vector is limited to 2,147,483,647 (see ?Memory for more information)
There are three issues to consider when working with large datasets: (a) efficient programming to speed execution, (b) storing data externally to limit memory issues, and (c) using specialized statistical routines designed to efficiently analyze massive amounts of data.
G.1 Efficient programming There    are a number of programming tips that improve performance when working with large datasets.
Use R’s built-in functions for manipulating vectors, matrices, and lists (for example, sapply, lappy, and mapply) and avoid loops (for and while) when feasible.
When using the read.table() family of functions to input external data into data frames, specify the colClasses and nrows options   explicitly, set comment.
This will decrease memory usage and speed up processing considerably.
When reading external data into a matrix, use the scan() function   instead.
Test programs on subsets of the data, in order to optimize code and remove bugs, before attempting a run on the full dataset.
Delete temporary objects and objects that are no longer needed.
The call rm(list=ls()) will remove all objects from memory, providing a clean slate.
This function will help you find and deal with memory hogs.
Profile your programs to see how much time is being spent in each function.
You can accomplish this with the Rprof() and summaryRprof() functions.
The profr and prooftools packages provide functions that can help in analyzing profiling output.
The Rcpp package   can be used to transfer R objects to C++ functions and back when more optimized subroutines are needed.
With large datasets, increasing code efficiency will only get you so far.
When bumping up against memory limits, you can also store our data externally and use specialized analysis    routines.
G.2 Storing data outside of RAM There    are several packages available for storing data outside of R’s main memory.
The strategy involves storing data in external databases or in binary flat files on disk, and then accessing portions as they are needed.
Table G.1 R packages for accessing large datasets (continued  )
The packages above help overcome R’s memory limits on data storage.
However, specialized methods are also needed when attempting to analyze large datasets in a reasonable length of time.
G.3 Analytic packages for large datasets R    provides several packages for the analysis of large datasets:
The biglm and speedglm packages   fit linear and generalized linear models to large datasets in a memory efficient manner.
This offers lm() and glm() type functionality when dealing with massive datasets.
Several packages offer analytic functions for working with the massive matrices produced by the bigmemory package.
The biganalytics package   offers k-means clustering, column statistics, and a wrapper to biglm.
The bigtabulate package   provides table() , split() , and tapply()   functionality and the bigalgebra package provides advanced linear algebra functions.
The biglars package   offers least-angle regression, lasso, and stepwise regression for datasets that are too large to be held in memory, when used in conjunction with the ff package.
The Brobdingnag package   can be used to manipulate large numbers (numbers larger than 2^1024)
Working with datasets in the gigabyte to terabyte range can be challenging in any language.
As consumers,    we take for granted that we can update a piece of software via a “Check for updates…” option.
Unfortunately, there’s no corresponding function for updating the R installation itself.
Downloading and installing the latest version of R from CRAN (http://cran.rproject.org/bin/) is relatively straightforward.
The complicating factor is that customizations (including previously installed contributed packages) will not be included in the new installation.
In my current set-up, I have 248 contributed packages installed.
I really don’t want to have to write their names down and reinstall them by hand the next time I upgrade my R installation.
There has been much discussion on the web concerning the most elegant and efficient way to update an R installation.
The method described below is neither elegant nor efficient, but I find that it works well on a variety of platforms (Windows, Mac, and Linux)
If you have a customized Rprofile.site file   (see  appendix B), save a copy outside of R.
This approach will install only packages that are available from the CRAN.
Luckily, the process will display a list of packages that can’t be installed.
During my last installation, globaltest and Biobase couldn’t be found.
Since I got them from the Bioconductor site, I was able to install them via the code.
Step 6 involves the optional deletion of the old installation.
On a Windows machine, more than one version of R can be installed at a time.
On Mac and Linux platforms, the new version of R will overwrite the older version.
On a Linux platform, it’s probably best to leave well enough alone.
Clearly, updating an existing version of R is more involved than is desirable for such a sophisticated piece of software.
I’m hopeful that someday this appendix will simply say “Select the Check for Updates… option” to update an R    installation.
Package for the Social Sciences datasets, importing data from spss.
Th is makes R a great way to get meaningful information from mountains of raw data.
It presents useful statistics examples and includes elegant methods for handling messy, incomplete, and nonnormal data that are diffi  cult to analyze using traditional methods.
You’ll also master R’s extensive graphical capabilities for exploring and presenting data visually.
What’s Inside Practical data analysis, step by step Interfacing R with other soft ware Using R to visualize data Over 130 graphs Eight reference appendixes.
Rob Kabacoff is a seasoned researcher who specializes in data analysis.
He has taught graduate courses in statistical programming and manages the Quick-R website at statmethods.net.
Front Cover brief contents contents preface acknowledgments about this book The examples Code conventions Author Online About the author Who should read this book Roadmap.
