The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
We live in an increasingly asynchronous world in which we need to interact with a bewildering range of systems, so our software applications need to support a variety of conversation patterns with disparate collaborators.
In the 2000s, Struts, Spring, and Hibernate replaced in-house web MVC, configuration, and persistence code with superior, battle-tested, and well-documented open source code.
Similarly today, integration is core to so many applications that we need quality, generic infrastructure in place of ad hoc solutions.
Spring Integration is a great choice to address these infrastructure requirements.
Too many open source projects reinvent every wheel in sight.
It explicitly builds on established best practices and existing software.
Spring Integration was inspired by one of the classic books on enterprise software: Hohpe and Woolf’s Enterprise Integration Patterns (Addison-Wesley, 2003)
Spring Integration also builds on the powerful and proven Spring Framework.
It extends Spring’s POJO programming model, making it a natural choice for the millions of developers already familiar with Spring.
If you’re a Spring developer, learning Spring Integration won’t feel so much like learning a new framework as like picking up a part of the Spring Framework core that you haven’t yet applied.
You’ll be able to focus on mastering architectural concepts without wrestling with an unfamiliar API.
FOREWORDxvi Spring values such as portability, with the result that it can be used in a wide range of environments.
Spring Integration is set to become an increasingly important project within the Spring Portfolio.
I believe this book will become the definitive work on it.
The authors are uniquely qualified to write about the topic.
Mark Fisher is the creator and lead of Spring Integration, and all the authors are contributors.
Their daily involvement in the open source project has ensured that the book is up to date with Spring Integration 2.0 features and best practices.
Mark, Marius, Iwein, and Jonas have a wealth of worldwide consulting experience helping customers solve integration problems.
This extensive and current hands-on experience underpins their writing and offers great value to the reader.
The authors do an excellent job of putting Spring Integration in context.
Rather than merely explain how to use Spring Integration, they discuss common business problems, the trade-offs between potential solutions, and how Spring Integration can be applied to implement them.
The first few chapters explain how Spring Integration grows naturally out of the Spring Framework.
The examples are well chosen: easy to grasp, yet realistic enough to communicate real-world problems.
A good balance between code examples and explanation develops the reader’s understanding at a steady pace.
There are many highlights, but I particularly like chapter 18, “Testing.” In this, as in many other chapters, you’ll find a wealth of good advice based on experience, which will save you a lot of time debugging.
What kinds of tests will give you the most bang for your buck? How do you test your configuration? How do you figure out what’s going on with an endpoint that has no output? You’ll find thoughtful answers to these and many other questions.
Too often technical books are clumsily and sloppily worded, making them hard work to plow through and potentially confusing.
This book is an exception, being enjoyable to read and always clear and to the point.
I guess, as they say, time flies when you’re having fun, and I would add that it flies even faster (especially the weekends) when you’re writing a book.
For those who have been anxiously awaiting a print copy since this book project was first announced in Manning’s Early Access Program, I hope you find it worth the wait.
One thing is certain: the authors are true subject matter experts.
Each of my coauthors has contributed to the framework, and many of the chapters are written by the one person most intimately familiar with the topic at hand.
In a few cases, new features were even added to the framework based on ideas that originated in the book.
The degree of expertise is most apparent, however, in the frequent discussions about design decisions and the trade-offs involved.
In those sections, I believe the reader will fully recognize that the authors’ perspectives have been shaped by real-world experience building enterprise integration solutions.
The first prototype that eventually led to the official launching of the Spring Integration project was a result of two motivations.
Second, I quite literally had a life-changing experience with the Spring Framework.
I was determined to bring those two forces together in a way that would let them both shine.
Regarding the EIP book, I probably have one of the most well-worn copies, and it’s always within easy reach at my desk.
To this day I refer to it regularly, even for something as mundane as settling upon the right terminology to use in code documentation.
By the time I first read that book, I had experience with several integration frameworks and containers, and yet I felt that none truly captured the essence of.
I wanted to define an API that would be immediately accessible to anyone familiar with the patterns, where not only the functionality but the vocabulary could be easily recognized.
As for my life-changing experience with the Spring Framework, in late 2005, a fortunate series of events led to my joining the core team.
The company behind Spring, Interface21, had recently been established, and as an employee I quickly went from being just a Spring fanatic to being a full-time consultant, trainer, and contributor.
In those early days, I spent a majority of my time on the road.
I worked on site with dozens of organizations, trained hundreds of engineers, and spoke at numerous user groups and conferences.
Throughout those interactions, I noticed that developers became genuinely excited about the Spring Framework once they experienced the proverbial “aha!” moment.
Soon, my primary goal was to elicit a similar reaction from users of a new extension to the Spring programming model that would focus on the enterprise integration patterns.
That pretty much sums up why I started the project.
Much of the groundwork for Spring Integration was coded on the go—in planes, trains, taxis, hotel lobbies, and countless cafes.
Throughout my travels, I demonstrated the early prototypes, and I processed vast amounts of feedback.
My colleagues at Interface21 provided their share of honest feedback, including Rod Johnson, founder of the Spring Framework, who took an early interest in the project.
Interface21 later evolved into SpringSource which was in turn acquired by VMware.
Today, I continue to lead the integration efforts within the Spring team at VMware.
Now it’s much more common for me to write code at a desk, and in fact the majority of new Spring Integration code is currently written by others on the team.
I’ve also been pleased to see the number of community contributors grow, a trend that should lead to many extensions to the core.
Ironically, as the founder of the project, I have a particularly difficult time devising its “elevator pitch.” I suppose it’s always a challenge to avoid verbosity when discussing something that you live and breathe day after day.
That said, based on the background I provided here, such a pitch might go something like this:
Spring Integration provides support for the enterprise integration patterns while building upon the Spring programming model.
It shares the Spring Framework’s goal of simplifying the developer role as much as possible.
This goal is applicable even, and perhaps especially, when the developer is designing and implementing applications that aren’t simple at all.
It’s a fact of life that modern applications are increasingly complex since they tend to require event-driven services interacting with data in nearreal time across a wide variety of distributed systems.
Those are the problems that the enterprise integration patterns address, and applying the Spring programming model to those patterns exposes their full power through a simplified developer experience.
Indeed, Spring Integration enables developers to implement those distributed, eventdriven applications.
It does so in a way that keeps the enterprise integration patterns in clear focus and maintains the associated vocabulary as accurately as possible.
Above all, I hope you find that it does so in a way that lets the developer enjoy the journey and that this book gets you started on the right path.
Writing the book became a journey in itself, and we had many excellent guides along the way.
Mike Stephens, it all began with your patient assistance as we took the first steps and planned our roadmap.
A number of editors and reviewers helped us navigate the terrain, but we would especially like to thank Cynthia Kane for rescuing us from writer’s block on so many occasions.
The last mile felt like a marathon all by itself, but the production team kept us on track.
We apologize if we have inadvertently overlooked any of your suggestions.
To our technical proofreaders, Neale Upstone and Doug Warren: your careful attention to the configuration and code has been greatly appreciated.
Having such experienced technical reviewers gave us the confidence to make the necessary changes, and the reader will surely appreciate that as much as we do.
Last but certainly not least, we would like to thank those who provided the foundations for the Spring Integration framework itself.
Gregor Hohpe and Bobby Woolf: Enterprise Integration Patterns not only sparked the original inspiration, but it has essentially served and continues to serve as our specification.
Arjen Poutsma: not only is your craftsmanship evident across Spring’s REST support, Spring Web Services, and Spring OXM—all of which Spring Integration builds upon—but your direct feedback in the early days of the project influenced some of the most important decisions in defining the core API itself.
Juergen Hoeller and the rest of the core Spring team: the Spring Framework provides much of the underlying functionality that makes Spring Integration possible but, even more importantly, it provides the idioms and principles that keep Spring Integration moving in the right direction.
Rod Johnson: we are truly honored that you wrote this book’s foreword; there could be no better way to start our story here than with the words of the one who started Spring itself.
To my daughter Evelyn, the image of uninterrupted weekends with you has been the inspiration driving me to finish.
To my parents, I am grateful for a lifetime of encouragement.
To the entire Spring team, past and present, thank you for maintaining such a high standard of quality with an even higher level of passion.
I am fortunate to work with the current Spring Integration team—Oleg, Gary, and Gunnar—who make the framework more amazing every day.
To my coauthors, I must say it’s incredible that four people with so many other responsibilities managed to complete a book, even if it took just a little bit longer than expected.
Bob Coates for always having time and enthusiasm to share while I was an undergraduate.
Thanks also go to these individuals: to my coauthors, for the great experience of working together; to all my former and current colleagues, especially from the SpringSource and JBoss teams for helping me understand the real.
I also thank my team members, family, and friends for letting me write during meetings, parties, and other occasions.
Finally, I bow my head in respect to Mark, who has pushed us through the last barrier to get this book to print.
The abstraction provided by that API and model essentially serves as a lightweight messaging framework that can be used in any runtime environment from a full-blown application server to a simple main method within a Java class that’s executed from a command line or within an IDE.
A messaging framework can be quite useful even in standalone applications that don’t require complex system integration.
For example, the core enterprise integration patterns can be used to construct a pipeline of filters, transformers, and routers that all run within a single process.
With the growing interest in asynchronous eventdriven applications, that pipeline design might be a good match for many of your applications.
This book recognizes the value of the core messaging patterns not only as building blocks for system integration but also as a set of components that can facilitate standxxii.
The first two parts of the book, “Background” and “Messaging,” consist of seven chapters that are relevant for either type of application.
The third part, “Integrating systems,” includes six chapters that build upon that core knowledge while demonstrating the most common messaging adapters for assembling distributed applications and integrating various data and messaging systems.
The fourth and final part of the book, “Advanced topics,” provides another five chapters covering practical concerns for those using the framework in real-world applications.
Throughout this book, we hope you’ll find that the depth of content goes well beyond the practical concerns of the framework’s usage as it ventures into the.
It takes a prosand-cons approach to such topics as tight and loose coupling, synchronous and asynchronous communication, and the four integration styles: filesystem, shared database, remote procedure calls, and messaging.
Chapter 3 offers the first jump into the Spring Integration API.
It focuses only on the Message and Message Channel abstractions, since those two must be understood in depth before a meaningful exploration of the rest of the framework.
Chapter 4 takes the next logical step by describing the generic role of Message Endpoints.
Much of what follows in later chapters will focus on specific types of endpoints, but they share the common characteristics described here.
Chapter 5 reveals how you connect the business logic within your application’s service layer to the messaging endpoints.
It emphasizes the importance of maintaining a separation of concerns between that business logic and the integration logic.
Chapter 6 demonstrates how to add conditional logic to messaging flows.
The Message Filter and Message Router patterns are presented within the context of several real-world scenarios.
Chapter 7 explains how to deal with messaging flows that require nonlinear processing.
The Message Splitter, Aggregator, and Resequencer patterns are featured, along with lower-level patterns such as the Correlation Identifier and higher-level patterns like Scatter-Gather.
Unlike many integration frameworks, Spring Integration doesn’t require the use of XML for message structure, yet it’s still a popular format.
This chapter introduces transformers, splitters, and routers that take advantage of XPath, XSLT, and Spring’s own XML marshalling (OXM) libraries.
Chapter 9 is the first to deal with messaging adapters and takes the logical starting point for a Java-based framework: the Java Message Service (JMS)
Along with a detailed discussion of mapping between the JMS and Spring Integration.
Chapter 10 turns to email, perhaps the most widely used form of messaging in the modern world.
Of course, the emphasis is on the various ways that enterprise applications can send and receive email as part of their automated processing.
Chapter 11 gets back to the roots of integration and the role of the shared filesystem.
This modern perspective approaches the topic within the context of messaging, with file directories as endpoints connected to channels either on the reading or writing end of a message flow.
Chapter 12 ventures into the wide world of web services.
Spring Integration supports both REST- and SOAP-based services as either inbound or outbound endpoints.
This chapter shows not only how to use these adapters but also provides guidance on choosing among the options.
Chapter 13 wraps up the discussion of adapters with two more selections from the Spring Integration toolbox.
The chapter begins with the XMPP adapters that enable plugging into instant messaging systems.
The chapter then provides a tour of the Twitter adapters that can be used for updating or reading a timeline, sending or receiving direct messages, detecting mentions, or performing a search.
Chapter 14 reveals how you can monitor and manage a Spring Integration application at runtime.
Chapter 15 provides a thorough discussion of task scheduling in a Spring Integration application.
This is an important topic for any messaging flow that includes the Polling Consumer pattern.
No such discussion would be complete without getting into the thorny details of concurrency, and this chapter doesn’t shy away.
As a result, it’s also quite relevant for anyone relying upon asynchronous execution and parallel processing of messages.
Chapter 16 shows how Spring Integration and Spring Batch can be used together.
First, it provides a basic overview of batch processing applications in general and then a quick introduction to Spring Batch.
Learning how to add messaging capabilities to batch applications might be useful to anyone tasked with modernizing their legacy systems.
Chapter 17 presents the main principles behind the Open Services Gateway initiative (OSGi) and demonstrates how its approach to modularity and its service registry can be utilized in Spring Integration applications.
In the process, this chapter also provides background information about the Eclipse Gemini Blueprint project, which is the successor to Spring Dynamic Modules.
One of the main design goals of Spring Integration is to make enterprise application integration (EAI) accessible in a wide variety of use cases.
Instead of a heavyweight infrastructure requiring the setup of an external integration bus and the use of specialized utilities and tools, Spring Integration builds on top of the Spring Framework and allows the inclusion of its components and concepts directly inside applications, reusing a wide array of skills and tools developers already have, like their knowledge of Java or Spring.
In a similar way, this book intends to be a companion and guide to anyone who needs to incorporate integration aspects in their applications.
Nonspecialists, developers, and architects who need to solve specific integration problems will find a set of useful concepts and patterns described from a pragmatic, example-driven perspective.
Managers will better understand the challenges inherent within the solutions whose development they supervise and, at the same time, the opportunities that the framework offers.
This book addresses an audience with varying degrees of familiarity with EAI, Spring, or even Spring Integration itself.
Prior knowledge of any of these would surely help you connect the dots with greater ease, but isn’t required.
The book introduces basic background concepts before discussing more advanced topics.
Conversely, existing familiarity with any of these topics should detract little from the enjoyment of the book; this deep dive into technical details and best practices will provide a lot to take away.
All source code in listings or in the text is in a fixed-width font like this to separate it from ordinary text.
Code annotations accompany some of the listings, highlighting important concepts.
In most cases, numbered bullets link to explanations that follow in the text.
The latter option helps avoid conflicts with other common uses of the unqualified prefix, as could clearly cause confusion in the case of the prefix http:
In yet other cases, we specify the integration namespace as the base namespace of the XML file itself so that no prefix is necessary for the components defined within that schema (e.g., <channel>)
That’s typically done when it’s the Spring Integration “core” namespace in question, and in those examples, you’ll often see an explicit prefix for the Spring beans namespace (e.g., <beans:bean>)
We wanted to point out these differences here, so that you aren’t confused when you encounter them.
In the real world, you’ll likely stumble upon a wide variety of prefixes, so it actually helps to be accustomed to seeing a reflection of that in the book examples.
Finally, we want to clarify an approach we’ve taken to the occurrence of enterprise integration pattern names within this book.
The first occurrence of a pattern name will typically be capitalized and/or italicized (e.g., Control Bus)
Once the context is established for a given pattern, it will occur in regular text throughout the book.
The pattern names appear so frequently in the book, we decided it was distracting to constantly use capitalization.
Author Online Purchase of Spring Integration in Action includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the authors and other users.
This page provides information on how to get on the forum once you are registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the authors can take place.
It’s not a commitment to any specific amount of participation on the part of the authors, whose contribution to the book’s forum remains voluntary (and unpaid)
We suggest you try asking them some challenging questions, lest their interest stray!
The Author Online forum and the archives of previous discussions will be accessixxvii.
Currently at VMware, he continues to lead the development of Spring Integration while exploring the intersection of big data and messaging.
He has been a committer on a number of Spring projects, including the Spring Framework itself and Spring AMQP, which he cofounded.
Mark speaks regularly at conferences and user groups about messaging, data, integration, and cloud computing.
As part of OpenCredo, Jonas has been a key part of many complex high-performance messaging projects.
Before cofounding OpenCredo, Jonas worked for SpringSource, where he began his involvement with the Spring Integration project as one of the early committers.
He’s the lead for Snowdrop, a utility package for JBossxxviii.
Marius has more than 15 years of experience developing and architecting software systems.
Prior to joining Red Hat, he was a Spring consultant with SpringSource and a contributor to the Spring Integration enterprise integration framework.
Apart from being an expert on TDD, concurrency, and messaging, Iwein especially enjoys building agile teams and lean startups.
The book includes finely colored illustrations of figures from different regions of Croatia, accompanied by descriptions of the costumes and of everyday life.
Split is an ancient port city on the Adriatic coast that is over 1700 years old.
It is the second-largest city in Croatia and an important cultural, academic, and economic center as well as a popular tourist destination.
The figure on the cover is wearing dark blue woolen trousers and an embroidered jacket over a white linen shirt, the typical xxix.
Dress codes and lifestyles have changed, and the diversity by region, so rich only 200 years ago, has faded away.
It is now hard to tell apart the inhabitants of different continents, let alone of different hamlets or towns separated by only a few miles.
Perhaps we have traded cultural diversity for a more varied personal life—certainly for a more varied and fast-paced technological life.
Manning celebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional life of two centuries ago, brought back to life by illustrations from old books and collections like this one.
This book will help you to understand and exploit Spring Integration.
Part 1 of the book presents a high-level overview of Spring Integration and enterprise integration fundamentals.
Spring Integration provides an extension of the Spring programming model to support the well-known enterprise integration patterns.
It enables lightweight messaging within Spring-based applications and supports integration with external systems via declarative adapters.
Those adapters provide a higher level of abstraction over Spring’s support for remoting, messaging, and scheduling.
Spring Integration’s primary goal is to provide a simple model for building enterprise integration solutions while maintaining the separation of concerns that's essential for producing maintainable, testable code.
First, in chapter 1, we provide a high-level overview of what Spring Integration is, and show you how its lightweight intra-application messaging can be incorporated into Spring applications, as well as how it offers interapplication integration adapters for use with many external technologies.
Then, in chapter 2, we explain enterprise integration patterns and how they can be represented and implemented by Spring Integration to add common useful capabilities to your Spring applications.
Our goal is to help you leverage this powerful technology in your own projects.
Throughout this book, we explain all these key concepts, explore important integration features, investigate the extensive configuration options available, and demonstrate how to put all these capabilities to work for you.
Throughout each day, we’re continuously bombarded by phone calls, emails, and instant messages.
As if we’re not distracted enough by all of this, we also subscribe to RSS feeds and sign up for Twitter accounts and other social media sites that add to the overall event noise.
In fact, technological progress seems to drive a steady increase in the number and types of events we’re expected to handle.
In today’s world of hyperconnectivity, it’s a wonder we can ever focus and get any real work done.
What saves us from such eventdriven paralysis is that we can respond to most events and messages at our convenience.
Like a return address on an envelope, events usually carry enough information for us to know how and where to respond.
When we design and build a software application, we strive to provide a foundation that accurately models the application’s domain.
The domain is like a slice of reality that has particular relevance from the perspective of a given business.
Therefore, successful software projects are accurate reflections of the real world, and as such, the event-driven nature plays an increasingly important role.
Whereas many software applications are based on a conversational model between a client and a server, that paradigm doesn’t always provide an adequate reflection.
Sometimes the act of making a request and then waiting for a reply is not only inefficient but artificial when compared with the actual business actions being represented.
When you plan a trip by booking a flight, hotel, and rental car, you don’t typically sit and wait at the computer for all of the trip details.
You may receive a confirmation number for the trip itself and a high-level summary of the various reservations, but the full details will arrive later.
You may receive an email telling you to log in and review the details.
In other words, even though the application may be described as a service, it’s likely implemented as an event-driven system.
Too many different services are involved to wrap the whole thing in a single synchronous action as the traditional client/server conversational model may suggest.
Instead, your request is likely processed by a series of events, or messages, being passed across a range of participating systems.
When designing software, it’s often useful to consider the events and messages that occur within the domain at hand.
Focusing on events and messages rather than being excessively service-oriented may lead to a more natural way to think about the problem.
As a result of all this, it’s increasingly common that enterprise developers must build solutions that respond to a wide variety of events.
In many cases, these solutions are replacements for outdated client/server versions, and in other cases, they’re replacements for scheduled back-office processes.
Sure, those nightly batchprocessing systems that grab a file and process it shortly after midnight still exist, but it’s increasingly common to encounter requirements to refactor those systems to be more timely.
Perhaps a new service-level agreement (SLA) establishes that files must be processed within an hour of their arrival, or maybe the nightly batch option is now insufficient due to 24/7 availability and globalized clientele.
Waiting for the result until the next day is no longer a valid option.
Perhaps the entry point is now a web service invocation, or an email, or even a Twitter message.
And by the way, those legacy systems won’t be completely phased out for several years, so you need to support all of the above.
That means you also need to be sure that the refactoring process can be done in an incremental fashion.
It aims to increase productivity, simplify development, and provide a solid platform from which you can tackle the complexities.
On top of this, it includes a toolbox of commonly required integration components and adapters.
With these tools in hand, developers can build the types of applications that literally change the way their companies do business.
First is the Spring Framework, a nearly ubiquitous and highly influential foundation for enterprise Java applications that has popularized a programming model which is powerful because of its simplicity.
Second is the book Enterprise Integration Patterns (Hohpe and Woolf, Addison-Wesley, 2003), which has standardized the vocabulary and catalogued the patterns of common integration challenges.
The original prototype that eventually gave birth to the Spring Integration project began with the recognition that these two giants could produce ground-breaking offspring.
By the end of this chapter, you should have a good understanding of how the Spring Integration framework extends the Spring programming model into the realm of enterprise integration patterns.
You’ll see that a natural synergy exists between that model and the patterns.
If the patterns are what Spring Integration supports, the Spring programming model is how it supports them.
Ultimately, software patterns exist to describe solutions to common problems, and frameworks are designed to support those solutions.
Let’s begin by zooming out to see what solutions Spring Integration supports at a very high level.
From the 10,000-foot view, Spring Integration consists of two parts.
At its core, it’s a messaging framework that supports lightweight, event-driven interactions within an application.
On top of that core, it provides an adapter-based platform that supports flexible integration of applications across the enterprise.
Figure 1.1 Two areas of focus for Spring Integration: lightweight intra-application messaging and flexible interapplication integration.
Everything depicted in the core messaging area of the figure would exist within the scope of a single Spring application context.
Those components would exchange messages in a lightweight manner because they’re running in the same instance of a Java Virtual Machine (JVM)
There’s no need to worry about serialization, and unless necessary for a particular component, the message content doesn’t need to be represented in XML.
Instead, most messages will contain plain old Java object (POJO) instances as their payloads.
There, adapters are used to map the content from outbound messages into the format that some external system expects to receive and to map inbound content from those external systems into messages.
The way mapping is implemented depends on the particular adapter, but Spring Integration provides a consistent model that’s easy to extend.
The Spring Integration 2.0 distribution includes support for the following adapters:
Most of the protocols and transports listed here can act as either an inbound source or an outbound target for Spring Integration messages.
In Spring Integration, the pattern name Channel Adapter applies to any unidirectional inbound or outbound adapter.
In other words, an inbound channel adapter supports an in-only message exchange, and an outbound channel adapter supports an out-only exchange.
Any bidirectional, or request-reply, adapter is known as a Gateway in Spring Integration.
In part 2 of this book, you’ll learn about channel adapters and gateways in detail.
Figure 1.1 obviously lacks detail, but it captures the core architecture of Spring Integration surprisingly well.
The figure contains several boxes, and those boxes are connected via pipes.
Now substitute “filters” for boxes, and you have the classic pipesand-filters architectural style.1
In this context, it’s probably better to think of filter as meaning processor.
Anyone familiar with a UNIX-based operating system can appreciate the pipes-andfilters style: it provides the foundation of such operating systems.
You can see that it’s literally the pipe symbol being used to connect two commands (the filters)
It’s easy to swap out different processing steps or to extend the chain to accomplish more complex tasks while still using these same building blocks (returns elided):
To avoid digressing into a foofaraw,2 we should turn back to the relevance of this architectural style for Spring Integration.
Those of us using the UNIX pipes-and-filters model on a day-to-day basis may take it for granted, but it provides a great example of two of the most universally applicable characteristics of good software design: low coupling and high cohesion.
Thanks to the pipe, the processing components aren’t connected directly to each other but may be used in various loosely coupled combinations.
Likewise, to provide useful functionality in a wide variety of such combinations, each processing component should be focused on one task with clearly defined input and output requirements so that the implementation itself is as cohesive, and hence reusable, as possible.
These same characteristics also describe the foundation of a well-designed messaging architecture.
Enterprise Integration Patterns introduces Pipes-and-Filters as a general style that promotes modularity and flexibility when designing messaging applications.
Many of the other patterns discussed in that book can be viewed as more specialized versions of the pipes-and-filters style.
At the lowest level, it has simple building blocks based on the pipes-and-filters style.
As you move up the stack to more specialized components, they exhibit the characteristics and perform the roles of other patterns described in Enterprise Integration Patterns.
In other words, if it were representing an actual Spring Integration application, the boxes in figure 1.1 could be labeled with the names of those patterns to depict the actual roles being performed.
All of this makes sense when you recall our description of Spring Integration as essentially the Spring programming model applied to those patterns.
Let’s take a quick tour of the main patterns now.
Then we’ll see how the Spring programming model enters the picture.
Like the diagram in figure 1.1, it’s about messaging and integration in the broadest sense, and the patterns apply to both intra -application and inter application scenarios.
Spring Integration supports the patterns described in the book, so we need to establish a broad understanding of the definitions of these patterns and the relations between them.
From the most general perspective, only three base patterns make up enterprise integration patterns: Message, Message Channel, and Message Endpoint.
Figure 1.2 shows how these components interact with each other in a typical integration application.
There are two main ways to differentiate between these patterns.
First, each pattern has more specific subtypes, and second, some patterns are composite patterns.
This section focuses on the subtypes so you have a clear understanding of the building blocks.
A message is a unit of information that can be passed between different components, called message endpoints.
Messages are typically sent after one endpoint is done with a bit of work, and they trigger another endpoint to do another bit of work.
Messages can contain information in any format that’s convenient for the sending and receiving endpoints.
For example, the message’s payload may be XML, a simple string, or a primary key referencing a record in a database.
The header contains data that’s relevant to the messaging system, such as the Return Address or Correlation ID.
The payload contains the actual data to be accessed or processed by the receiver.
For example, a Command Message tells the receiver to do something, an Event Message notifies the receiver that something has happened, and a Document Message transfers some data from the sender to the receiver.
Figure 1.2 A message is passed through a channel from one endpoint to another endpoint.
Figure 1.3 A message consists of a single payload and zero or more headers, represented here by the square and circle, respectively.
In all of these cases, the message is a representation of the contract between the sender and receiver.
In some applications it might be fine to send a reference to an object over the channel, but in others it might be necessary to use a more interoperable representation like an identifier or a serialized version of the original data.
The channel implementation manages the details of how and where a message is delivered but shouldn’t need to interact with the payload of a message.
Whereas the most important characteristic of any channel is that it logically decouples producers from consumers, there are a number of practical implementation options.
For example, a particular channel implementation might dispatch messages directly to passive consumers within the same thread of control.
On the other hand, a different channel implementation might buffer messages in a queue whose reference is shared by the producer and an active consumer such that the send and receive operations each occur within different threads of control.
As mentioned earlier, regardless of the implementation details, the main goal of any message channel is to decouple the message endpoints on both sides from each other and from any concerns of the underlying transport.
Two endpoints can exchange messages only if they’re connected through a channel.
The details of the delivery process depend on the type of channel being used.
We review many characteristics of the different types of channels later when we discuss their implementations in Spring Integration.
Both the sender and receiver can be completely unaware of each other thanks to the channel between them.
Additional components may be needed to connect services that are completely unaware of messaging to the channels.
We discuss this facet in the next section on message endpoints.
Channels can be categorized based on two dimensions: type of handoff and type of delivery.
The handoff can be either synchronous or asynchronous, and the delivery can be either point-to-point or publish-subscribe.
The former distinction will be discussed in detail in the synchronous versus asynchronous section of the next chapter.
The latter distinction is conceptually simpler, and central to enterprise integration patterns, so we describe it here.
In point-to-point messaging (see figure 1.4), each single message that’s sent by a producer is received by exactly one consumer.
This is conceptually equivalent to a postcard or phone call.
If no consumer receives the message, it should be considered an error.
This is especially true for any system that must support guaranteed delivery.
The former would be like calling each number on a list in turn as new messages are to be delivered, and the latter would be like a home phone that’s configured to fall back to a mobile when nobody is home to answer it.
As these cases imply, which consumer receives the message isn’t necessarily fixed.
For example, in the Competing Consumers (composite) pattern, multiple consumers compete for messages from a single channel.
Once one of the consumers wins the race, no other consumer will receive that message from the channel.
Different consumers may win each time, though, because the main characteristic of that pattern is that it offers a consumer-driven approach to load balancing.
When a consumer can’t handle any more load, it stops competing for another message.
Once it’s able to handle load again, it will resume.
Unlike point-to-point messaging, a Publish-Subscribe Channel (figure 1.5) delivers the same message to zero or more subscribers.
This is conceptually equivalent to a newspaper or the radio.
It provides a gain in flexibility because consumers can tune in to the channel at runtime.
The drawback of publish-subscribe messaging is that the sender isn’t informed about message delivery or failure to the same extent as in pointto-point configurations.
Publish-subscribe scenarios often require failure-handling patterns such as Idempotent Receiver or Compensating Transactions.
Message endpoints are the components that actually do something with the message.
This can be as simple as routing to another channel or as complicated as splitting the message into multiple parts or aggregating the parts back together.
Connections to the application or the outside world are also endpoints, and these connections take the form of channel adapters, messaging gateways, or service activators.
Message endpoints basically provide the connections between functional services and the messaging framework.
From the point of view of the messaging framework, endpoints are at the end of channels.
In other words, a message can leave the channel successfully only by being consumed by an endpoint, and a message can enter the channel only by being produced by an endpoint.
We discuss a few of them here to give you a general idea.
A Channel Adapter (see figure 1.6) connects an application to the messaging system.
In Spring Integration we chose to constrict the definition to include only connections that are unidirectional, so a unidirectional message flow begins and ends in a channel adapter.
Many different kinds of channel adapters exist, ranging from a method-invoking channel adapter to a web service channel adapter.
Spring Integration’s support for enterprise integration patterns details of these different types in the appropriate chapters on different transports.
For now, it’s sufficient to remember that a channel adapter is placed at the beginning and the end of a unidirectional message flow.
In Spring Integration, a Messaging Gateway (see figure 1.7) is a connection that’s specific to bidirectional messaging.
If an incoming request needs to be serviced by multiple threads but the invoker needs to remain unaware of the messaging system, an inbound gateway provides the solution.
On the outbound side, an incoming message can be used in a synchronous invocation, and the result is sent on the reply channel.
For example, outbound gateways can be used for invoking web services and for synchronous request-reply interactions over JMS.
A gateway can also be used midstream in a unidirectional message flow.
A Service Activator (see figure 1.9) is a component that invokes a service based on an incoming message and sends an outbound message based on the return value of this service invocation.
In Spring Integration, the definition is constrained to local method calls, so you can think of a service activator as a method-invoking outbound gateway.
The method that’s being invoked is defined on an object that’s referenced within the same Spring application context.
A Router (see figure 1.10) determines the next channel a message should be sent to based on the incoming message.
This can be useful to send messages with different payloads to different, specialized consumers (Content-Based Router)
The router doesn’t change anything in the message and is aware of channels.
Therefore, it’s the endpoint that’s typically closest to the infrastructure and furthest removed from the business concerns.
A Splitter (see figure 1.11) receives one message and splits it into multiple messages that are sent to its output channel.
This is useful whenever the act of processing message content can be split into multiple steps and executed by different consumers at the same time.
An Aggregator (figure 1.12) waits for a group of correlated messages and merges them together when the group is complete.
The correlation of the messages typically is based on a correlation ID, and the completion is typically related to the size of the group.
A splitter and an aggregator are often used in a symmetric setup, where some work is done in parallel after a splitter, and the aggregated result is sent back to the upstream gateway.
You’ll see many more patterns throughout the book, but what we covered here should be sufficient for this general introduction.
If you paid close attention while reading the first paragraph in section 1.2, you may have noticed that we said Spring Integration supports the enterprise integration patterns, not that it implements the patterns.
In general, software patterns describe proven solutions to common problems.
Enterprise integration patterns meet Inversion of Control patterns rarely have a one-to-one mapping to a single implementation, and contextdependent factors often lead to particular implementation details.
As far as the enterprise integration patterns are concerned, some, such as the message and message channel patterns, are more or less implemented.
Others are only partially implemented because they require the addition of some domain-specific logic; examples are the content-based router in which the content is dependent on the domain model and the service activator in which the service to be activated is part of a specific domain.
Yet other patterns describe individual parts of a larger process; examples are the correlation ID we mentioned when describing splitter and aggregators and the return address that we discuss later.
Finally, there are patterns that simply describe a general style, such as the pipes-and-filters pattern.
With these various pattern categories in mind, let’s now see how the concept of inversion of control applies to Spring Integration’s support for the patterns.
The theme of inversion of control (IoC) is central to this investigation because it’s a significant part of the Spring philosophy.3 For the purpose of this discussion, we consider IoC in broad terms.
The main idea behind IoC and the Spring Framework itself is that code should be as simple as possible while still supporting all of the complex requirements of enterprise applications.
In other words, those complexities can’t be wholly ignored but ideally shouldn’t have a negative impact on developer productivity or software quality.
To accomplish that goal, the responsibility of controlling those complexities should be inverted from the application code to a framework.
The bottom line is that enterprise development isn’t easy, but it can be much easier when a framework handles much of the difficult work.
With that as our definition, we discuss two key techniques that invert control when using Spring: dependency injection and method invocation.
We also briefly explain the role of each technique in the Spring Integration framework.
Dependency injection is the first thing most people think of when they hear inversion of control, and that’s understandable because it’s probably the most common application of the principle and the core functionality of IoC frameworks like Spring.
Entire books are written on this subject,4 but here we provide a quick overview to highlight the benefits of this technique and to see how it applies to Spring Integration.
Spring to the full spectrum of the Spring Integration framework.
If you’re new to Spring, you may want to check out some other books, such as Spring in Action, Third Edition, by Craig Walls (Manning, 2011)
When you design applications, you carefully consider the units of functionality that should be captured within a single component and the proper boundaries between collaborating components.
These decisions lead to contracts in the form of well-defined interfaces that dictate the input and output for a given module.
When one component depends on another, it should make assumptions only about such an interface rather than about a particular implementation.
This promotes the encapsulation of implementation details so that those details can change within the bounds of the interface definition without impacting other code.
What’s the big deal? you may be asking; this is common sense.
But it all breaks down as soon as we do something as seemingly harmless as the following:
Now the code is tightly coupled directly to an implementation type.
That implementation is being assigned to an interface, and hopefully the caller is never required to downcast.
But no matter how you slice it, the code is tied to an implementation because interfaces are only contracts and are separate from the implementations.
An implementation type must be chosen for instantiation, and the simplest means of instantiating objects in Java is by calling a constructor.
There is another option, and it often follows as a seemingly logical conclusion to this problem.
After recognizing that the implementation type leaked into the caller’s code, even though that code really only needs the interface, a factory can be added to provide a level of indirection.
Rather than constructing the object, the caller can ask for it:
This is the first step down the road of IoC.
The factory handles the responsibility that was previously handled directly in the caller’s code.
The caller gladly relinquishes that control in return for having to make fewer assumptions about the implementation it’s using.
This seems to solve the problem at first, but to some degree it’s just pushing the problem one step further away.
It’s the programmatic equivalent of sweeping dirt under the rug.
A better solution would remove all custom infrastructure code, including the factory itself.
That final step to full IoC is surprisingly simple: define a constructor or setter method.
In effect, that declares the dependency without any assumptions about who’s responsible for instantiating or locating it:
In a unit test that focuses on this single component, it’s trivial to provide a stub or mock implementation of the dependency directly.
Then, in a fully integrated system that may have many components sharing dependencies as well as complex transitive dependency chains, a framework such as Spring can handle the responsibility.
All you need to do is provide the implementation type as metadata.
Rather than being hardcoded, as in our first example, there is now a clear separation of code and configuration:5
The Spring Integration framework takes advantage of this same technique to manage dependencies for its components.
In fact, you can use the same syntax to define the individual bean definitions, but for convenience, custom XML schemas are defined so that you can declare a namespace6 and then use elements and attributes whose names match the domain.
The domain in this case is that of the enterprise integration patterns, so the element and attribute names will match those components described in the previous section.
For example, any Spring Integration message endpoint requires a reference to at least one message channel (determined by its role as producer, consumer, or both)
Another common case for dependency injection is when a particular implementation of a strategy interface7 needs to be wired into the component that delegates to that strategy.
For example, here’s a message aggregator with a custom strategy for determining when the processed items received qualify as a complete order that can be released:
Don’t worry about understanding the details of the examples yet.
These components are covered in more detail throughout the book.
The only point we’re trying to make so far is that dependency injection plays a role in connecting the collaborating components while avoiding hardcoded references.
You’ll see examples of the annotation-based style throughout the book, but XML was chosen here because it may be easier to understand initially.
IoC is often described as following the Hollywood principle: Don’t call us, we’ll call you.8 From the preceding description of dependency injection, you can see how well this applies.
Rather than writing code that calls a constructor or even a factory method, you can rely on the framework to provide that dependency by calling a constructor or setter method.
This same principle can also apply to method invocation at runtime.
Let’s first consider the Spring Framework’s support for asynchronous reception of JMS messages.
Prior to Spring 2.0, the only support for receiving JMS messages within Spring was the synchronous (blocking) receive() method on its JmsTemplate.
That works fine when you want to control polling of a JMS destination, but when working with JMS, the code that handles incoming messages can usually be reactive rather than proactive.
In fact, the JMS API defines a simple MessageListener interface, and the Enterprise JavaBeans (EJB) 2.1 specification introduced message-driven beans as a component model for hosting such listeners within an application server.
With version 2.0, Spring introduced its own MessageListener containers as a lightweight alternative.
MessageListeners can be configured and deployed in a Spring application running in any environment instead of requiring an EJB container.
As with message-driven beans, a listener is registered with a certain JMS destination, but with Spring, the listener’s container is a simple object that is itself managed by Spring.
The container manages the subscription and the background processes that are ultimately responsible for receiving the messages.
There are configuration options for controlling the number of concurrent consumers, managing transactions, and more:
It gets even more interesting and more relevant for our lead-up to Spring Integration when we look at Spring’s support for invoking methods on any Spring-managed object.
Sure, the MessageListener interface seems simple enough, but it has a few limitations.
This inhibits testing and also pollutes otherwise pure business logic achieved by relying on the IoC principle.
Second, and more severe, it has a void return type.
That means you can’t easily send a reply message from the listener method’s implementation.
On the slim chance that you get the part, we’ll call you.
Enterprise integration patterns meet Inversion of Control are eliminated if you instead reference a POJO instance that doesn’t implement MessageListener and add the method attribute to the configuration.
For example, let’s assume you want to invoke the following service method:
Whatever client is passing request messages to the quoteRequest destination could also provide a JMSReplyTo property on each request message.
Spring’s listener container uses that property to send the reply message to the destination where the caller is waiting for the response to arrive.
Alternatively, a default reply destination can be provided with another attribute in the XML.
This message-driven support is a good example of IoC because the listener container is handling the background processes.
It’s also a good example of the Hollywood principle because the framework calls the referenced object whenever a message arrives.
Another common requirement in enterprise applications is to perform some task at a certain time or repeatedly on the basis of a configured interval.
For functionality beyond what the core language provides, there are projects such as Quartz9 to support scheduling based on cron expressions, persistence of job data, and more.
Interacting with any of these schedulers normally requires code that’s responsible for defining and registering a task.
For example, imagine you have a method called poll in a custom FilePoller class.
You might wrap that call in a Runnable and schedule it in Java as in the following listing.
The Spring Framework can handle much of that for you.
It provides method-invoking task adapters and support for automatic registration of the tasks.
That means you don’t need to add any extra code.
For example, in Spring 3.0, the configuration might look like this:
As you can see, this provides a literal example of the Hollywood principle.
First, even though the code being invoked should be thoroughly unit tested, you can rest assured that the Spring scheduling mechanism is tested already.
Second, the configuration of the initial delay and fixed-rate period for the task and the thread pool size for the scheduler are all externalized.
By enforcing this separation of configuration from code, you’re much less likely to end up with hardcoded values in the application.
The code is not only easier to test but also more flexible for deploying into different environments.
Now let’s see how this same principle applies to Spring Integration.
The configuration of scheduled tasks follows the same technique as shown previously.
The configuration of a Polling Consumer’s trigger can be provided through declarative configuration.
Spring Integration takes the previous example a step further by actually providing a file-polling channel adapter:
We should also mention that both the core Spring Framework scheduling support and the Spring Integration polling triggers accept cron expressions in place of the intervalbased values.
If you only want to poll during regular business hours, something like the following would do the trick:
The best way to reinforce that knowledge is by diving into a simple but complete hands-on example.
Now that you’ve seen the basic enterprise integration patterns and an overview of how IoC can be applied to those patterns, it’s time to jump in and meet the Spring Integration framework face to face.
In the time-honored tradition of software tutorials, let’s say hello to the Spring Integration world.
Spring Integration aims to provide a clear line between code and configuration.
The components provided by the framework, which often represent the enterprise integration patterns, are typically configured in a declarative way using either XML or Java annotations as metadata.
But many of those components act as stereotypes or templates.
They play a role that’s understood by the framework, but they require a reference to some user-defined, domain-specific behavior in order to fulfill that role.
For our Hello World example, the domain-specific behavior is the following:
The interface that MyHelloService implements is HelloService, defined as follows:
This one is flexible enough to say hello to anyone.
Because this book is about Spring Integration, and we’ve already established that it’s a framework for building messaging applications based on the fundamental enterprise integration patterns, you may be asking, Where are the Message, Channel, and Endpoint? The answer is that you typically don’t have to think about those components when defining the behavior.
What we implemented here is a straightforward POJO with no awareness whatsoever of the Spring Integration API.
This is consistent with the general Spring emphasis on noninvasiveness and separation of concerns.
That said, let’s now tackle those other concerns, but separately, in the configuration:
Spring Integration’s service activator plays the same role, except that this time it’s more generic.
Rather than being tied to the JMS transport, the service activator is connected to a Spring Integration MessageChannel within the ApplicationContext.
Any component could be sending messages to this service activator’s input-channel.
The key point is that the service activator doesn’t require any awareness or make any assumptions about that sending component.
All of the configured elements contribute components to a Spring ApplicationContext.
In this simple case, you can bootstrap that context programmatically by instantiating the Spring context directly.
Then, you can retrieve the MessageChannel from that context and send it a message.
We use Spring Integration’s MessageBuilder to construct the actual message, shown in the following listing.
Running that code produces “Hello World” in the standard output console.
That’s pretty simple, but it would be even nicer if there were no direct dependencies on Spring Integration components even on the caller’s side.
Let’s make a few minor changes to eradicate those dependencies.
First, to provide a more realistic example, let’s modify the HelloService interface so that it returns a value rather than simply printing out the result itself:
Spring Integration handles the return value in a way that’s similar to the Spring JMS support described earlier.
You add one other component to the configuration, a gateway proxy, to simplify the caller’s interaction.
Note that the gateway element refers to a service interface.
This is similar to the way the Spring Framework handles remoting.
The caller should only need to be aware of an interface, while the framework creates a proxy that implements that interface.
The proxy is responsible for handling the underlying concerns such as serialization and.
You may have noticed that the MyHelloService class does implement an interface.
Now the caller only needs to know about the interface.
It can do whatever it wants with the return value.
In the following listing, you just move the console printing to the caller’s side.
The service instance would be reusable in a number of situations.
The key point is that this revised main method now has no direct dependencies on the Spring Integration API.
As with any Hello World example, this one only scratches the surface.
Later you’ll learn how the result value can be sent to another downstream consumer, and you’ll learn about more sophisticated request-reply interactions.
The main goal for now is to provide a basic foundation for applying what you’ve learned in this chapter.
Spring Integration brings the enterprise integration patterns and the Spring programming model together.
Even in this simple example, you can see some of the characteristics of that programming model, such as IoC, separation of concerns, and an emphasis on noninvasiveness of the API.
You learned that Spring Integration addresses both messaging within a single application and integrating across multiple applications.
You learned the basic patterns that also describe those two types of interactions.
Listing 1.3 Hello World revised to use a Gateway proxy.
As you progress through this book, you’ll learn in much greater detail how Spring Integration supports the various enterprise integration patterns.
You’ll also see the many ways in which the framework builds on the declarative Spring programming model.
So far, you’ve seen only a glimpse of these features, but some of the main themes of the book should already be clearly established.
First, with Spring’s support for dependency injection, simple objects can be wired into these patterns.
Second, the framework handles the responsibility of invoking those objects so that the interactions all appear to be event-driven even though some require polling (control is inverted so that the framework handles the polling for you)
Third, when you need to send messages, you can rely on templates or proxies to minimize or eliminate your code’s dependency on the framework’s API.
The bottom line is that you focus on the domain of your particular application while Spring Integration handles the domain of enterprise integration patterns.
From a high-level perspective, Spring Integration provides the foundation that allows your services and domain objects to participate in messaging scenarios that take advantage of all of these patterns.
In chapter 2, we dive a bit deeper into the realm of enterprise integration.
We cover some of the fundamental issues such as loose coupling and asynchronous messaging.
This knowledge will help establish the background necessary to take full advantage of the Spring Integration framework.
Commercial applications are, most of the time, solutions to problems posed by the business units for which they’re developed.
It makes little difference whether the problem under discussion is older, and the solution is automating an existing process, or the problem is new, and the solution is an innovation that allows the organization to do business in a way that wasn’t possible before.
In some cases, the solutions consist of newly developed components that reuse already-existing applications by delegating functionality to them.
This is often the case with legacy applications that implement complex business logic and for which a complete rewrite would be an unjustifiable cost.
Other applications are divided from the beginning into multiple components that run independently to get the most out of the modern hardware and its high concurrency capabilities.
What both these approaches have in common is that they tie together separate components Enterprise integration fundamentals.
Loose coupling and event-driven architecture and applications, sometimes even located on different machines.
Each of these concepts discussed in this chapter plays an important role in designing integrated applications.
They make sure that the integrated components don’t impose needless restrictions on each other and that the system is responsive and can process concurrent requests efficiently.
Applying these principles in practice results in several integration styles, each with its own advantages and disadvantages.
This chapter shows you how to take them into account when implementing a solution.
As a Spring Integration user, you’ll benefit from understanding the foundational principles of the framework.
We already discussed building applications that consist of multiple parts (or that orchestrate collaboration between standalone applications)
It’s important to understand the implications of integration on your design.
One of the most important consequences of decomposing applications into multiple components is that these components can be expected to evolve independently.
The proper design and implementation of these parts (or independent applications) can make this process easy.
Failing to properly separate concerns can make evolution prohibitively hard.
Our criterion to decide whether a particular design strategy fosters independent evolution or stands in the way is coupling.
In this section we argue why loose coupling is preferable over tight coupling in almost every situation.
Because coupling can come in different forms, such as typelevel or system-level, we explore these variants of coupling in more detail.
The last part of the section discusses how to reduce coupling in your application by using dependency injection or adopting an event-driven architecture.
Loose coupling within systems and between systems deserves serious consideration, because it has serious implications for design and maintenance.
Achieving an appropriate degree of loose coupling allows you to spend more time adding new features and delivering business value.
By contrast, tightly coupled systems are expensive to maintain and expand because small changes to the code tend to produce ripple effects, requiring modifications across a large number of interacting systems.
It’s important to note that this increase in cost isn’t the product of a change in conditions but something that could have been avoided at design time.
It’s not so much that loosely coupled systems guarantee quality but that highly coupled systems nearly always guarantee complexity in the code and the paths through the code, making systems hard to maintain and hard to understand.
Highly coupled systems are also generally harder to test, and often it’s nearly impossible to unit test their constituent parts, for example, because their units can’t be constructed without constructing the entire system.
How can you identify a system that’s highly coupled? Measuring coupling has been the focus of various academic attempts based on various forms of code-level connections.
For example, one measure could be how many user types a class references.
Where a class is referencing many user types, as shown in figure 2.1, it’s considered highly coupled, which is usually a bad sign because changes in those referenced types may lead to a requirement to change the class referencing them.
Take, for example, adding a parameter to a method signature: all invokers must provide a value for it.
Using referenced types as a measure of how interconnected classes within a system are will give you an idea whether the system’s coupling is high or low.
This method works well for individual applications, but as new application design strategies are employed, other forms of coupling have become prominent.
For example, in serviceoriented architectures (SOAs), the coupling between the service contract and the service implementation is also taken into account, so that a change in the implementation of the service doesn’t create a need to update all service clients.
In many SOA scenarios, service consumers may not be under the control of the service implementation.
In this case, loose coupling can insulate you from causing regressions that in turn cause unhappy service consumers with broken systems.
A general way of defining coupling is as a measure of how connected the parts of a system are or how many assumptions the components of the system make about each other.
For example, systems that directly reference Remote Method Invocation (RMI) for communication in many places are highly coupled to RMI.
Or a system that directly references a third-party system in a large number of places can be said to be highly coupled to that third-party system.
The connections you create in and between your systems introduce complexity and act as conduits for change, and change generally entails cost and effort.
Figure 2.1 A highly coupled system: components become entangled because of the complex relationships between them.
Loose coupling, in the context of integrating systems, is therefore vital in allowing the enterprise to switch between different variants of a particular component (for example, accounting packages) without incurring prohibitive cost due to required changes to other systems.
Coupling is an abstract concept, and we show over the next few sections a few concrete examples of different types of coupling, along with approaches using Spring and Spring Integration to reduce coupling and improve code quality through increased simplicity, increased testability, and reduced fragility.
Coupling between types is probably the best understood because it’s the form of coupling that’s most often discussed.
The following listing shows a booking service that allows airline passengers to update their meal preferences.
The service first looks up the booking from the database to obtain the internal reference, and then invokes a meal-preference web service.
This type of coupling is known as unambiguous type coupling because it’s coupled to a concrete implementation even though it then assigns the instance to members’ fields typed as the interface.
The BookingService also exhibits unambiguous type coupling to the Spring WebServiceTemplate.
Generally, a system repeating this pattern of coupling to the concrete type can be considered highly coupled because changes in those concrete types will have widespread impact.
The solution to this problem is deferring the creation of concrete instances to the framework, using what we call dependency injection, which is our next topic.
Standard dependency injection allows you to reduce coupling by addressing unambiguous type coupling.
One way to do this is to move the instantiation concern into a single configuration file rather than code.
Removing the code that instantiates the collaborators for your services both reduces coupling and simplifies the code.
Loose coupling and event-driven architecture makes maintenance easier, simplifies testing by allowing tests to inject test-only implementations, and increases the chances of reuse.
The following example shows the new version of the BookingService, which now exposes a constructor that accepts instances of its collaborators:
The Spring configuration to instantiate the BookingService in production looks as follows:
This example reduces coupling in the Java code but introduces a new configuration file that’s coupled to the concrete implementations.
Taking a broader view of the system, you now have that coupling in one place rather than potentially many calls to the constructors, so the system as a whole is less highly coupled because changes to SimpleBookingDao will impact only one place.
With type-level coupling out of the way, it’s still possible for the different collaborators to make excessive assumptions about each other, such as about the data format being exchanged (for example, using a nonportable format such as a serialized Java class) or about whether two collaborating systems are available at the same time.
We group such assumptions under the moniker of system-level coupling.
It’s possible that collaborators might need to change to address new requirements, but it’s almost inevitable that where a large number of systems talk to each other, those systems will evolve at different rates.
Limiting one system’s level of coupling to another is key in being able to cope easily with changes such as these.
Currently, the booking service is coupled to the use of a web service to contact the meal-preference service as well as the XML data format expected by the service.
Things could’ve been worse: the data format could’ve been a serialized Java class instead of XML.
Serialization would be the right tool to use whenever the expectation is that data will be consumed by the same application at a later time or when data is exchanged between components that are expected to be highly connected to each other (like a client/server application)
Exchanging serialized data between independent applications couples them needlessly because it introduces the assumption that both of them have access to the bytecode of the serialized class.
It also requires that they use the same version of the class.
This in turn demands that new versions of the serialized classes be incorporated into the clients as soon as they’re deployed on the server.
It obviously assumes that both applications are using the same platform (Java in our case)
By using a portable data format like XML, we’ve just avoided all of this.
Of course, the applications are still coupled by the XML format, but in the end, both applications must have a basic agreement on what is and what isn’t a correct message.
We believe consumers should be as liberal as possible with that.
We still have another problem to deal with: the BookingService is also temporally coupled to the meal-preference web service in that we make a synchronous call and therefore the call to the BookingService will fail if the meal-preference service is unavailable at that point.
Whether that’s the desired behavior will depend on the requirements, but it would be nice to make the temporal coupling optional.
One option would be to introduce a new class that encapsulates the call to the meal-preference service, removing the web service concern and the data format concern from the BookingService.
This solution eliminates some coupling, but it also introduces coupling on a new meal-preference service type and on the signature of the method declared by that type.
By instead replacing the call to the meal-preference service with a new component and connecting the BookingService with the meal-preference service, you can further reduce coupling in the BookingService without introducing additional type coupling.
This can be achieved by replacing direct method invocations with message passing over channels.
The following example shows the simplified booking service, which now simply enriches the meal preference passed in with the flight reference:1
The functionality of the BookingService is now spread across multiple components.
You can explore this final implementation in the code example repository.
Focusing on message passing as the main integration mechanism leads us toward the adoption of an event-driven architecture, since each component in the message flow simply responds to messages being delivered by the framework.
Event-driven architecture (EDA) is an architectural pattern in which complex applications are broken down into a set of components or services that interact via events.
One of the primary advantages of this approach is that it simplifies the implementation of the component by eliminating the concern of how to communicate with other components.
Where events are communicated via channels that can act as buffers in periods of high throughput, such a system can be described as having a staged eventdriven architecture (SEDA)
SEDA-based systems generally respond better to significant spikes in load than do standard multithreaded applications and are also easier to configure at runtime, for example, by modifying the number of consumers processing a certain event type.
This allows for optimizations based on the actual requirements of the application, which in turn provides a better usage experience.
The question of whether an application built around the Spring Integration framework is inherently an EDA or SEDA application is open to debate.
Certainly Spring Integration provides the building blocks to create both EDA and SEDA applications.
Whether your particular application falls into one category or another in a strict interpretation depends on the messages you pass and your own working definition of what constitutes an event.
In most cases, this is probably not a useful debate to enter into, so Spring Integration makes no distinction between agents as producers of events and producers of messages, nor are event sinks and consumers distinguished.
The one place in which the term event is used in Spring Integration is in the implementation of the Event-Driven Consumer pattern, but here the consumer is consuming messages rather than events—it’s just that the consumption is triggered by the event of a message becoming available.
Reducing coupling is one of the main concerns when integrating applications.
You’ve just finished a section that shows how this can be done by eliminating assumptions about the concrete types used in the application as well as replacing method and web service invocations with message passing.
With respect to the latter, we mentioned synchronous communication as a way to increase the coupling of two systems, which we now address in more detail.
Another possible assumption made when integrating multiple components is that they’re available simultaneously.
Depending on whether this assumption is incorporated in the system’s design, the components may interact synchronously or asynchronously, and in this section we look at the main differences between the two interaction models as well as their advantages and disadvantages.
From the Spring Integration perspective, you’ll see the options that the framework provides in each case and how simple configuration options allow you to switch.
Synchronous and asynchronous communication between synchronous and asynchronous communication without changing the overall logical design of your application.
In synchronous communication (figure 2.2), one component waits until the other provides an answer to its request and proceeds only after a response is provided.
The requests are delivered immediately to the service provider, and the requesting component blocks until it receives a response.
When communicating asynchronously (figure 2.3), the component that issues the request proceeds without waiting for an answer.
The requests aren’t delivered to the service provider but stored in an intermediate buffer and from there will be delivered to their intended recipient.
Looking at how different these two alternatives are, the decision to use synchronous or asynchronous communication should be based on their strengths and weaknesses.
Let’s examine the upsides and downsides of each approach a bit further.
Of the two, synchronous communication is more straightforward: the recipient of the call is known in advance, and the message is received immediately (see figure 2.4)
The invocation, processing, and response occur in the same thread of execution (like a Java thread if the call is local or a logical thread if it’s remote)
This allows you to propagate a wealth of contextual information, the most common being the transactional and security context.
Generally, the infrastructure required to set it up is simpler: a method call or a remote procedure call.
Its main weaknesses are that it’s not scalable and it’s less resilient to failure.
Scaling up is a problem for synchronous communication because if the number of simultaneous requests increases, the target component has few alternatives, for example:
Trying to accommodate all requests as they arrive, which will crash the system.
Throttling some of the requests to bring the load to a bearable level2
Throttling is the process of limiting the number of requests that a system can accommodate by either postponing some of them or dropping them altogether.
Figure 2.2 Synchronous invocation: the requester suspends execution until it receives an answer.
Figure 2.3 Asynchronous invocation: the requester doesn’t block and executes in parallel with the provider.
Figure 2.4 Synchronous message exchange: the message is received immediately by the provider.
When the load increases, the application will eventually fail, and you can do little about it.
The lack of resilience to failure comes from the fundamental assumption that the service provider is working properly all the time.
There’s no contingency, so if the service provider is temporarily disabled, the client features that depend on it won’t work either.
The most obvious situation is a remote call that fails when the service provider is stopped, but this also applies to local calls when an invoked service throws a RuntimeException.
Asynchronous communication offers better opportunities to organize the work on the service provider’s side.
Requests aren’t processed immediately but left in an intermediate storage and from there are delivered to the service provider whenever it can handle them (see figure 2.5)
This means the requests will never be lost, even if the service provider is temporarily down, and also that a pool of concurrent processes or threads can be used to handle multiple requests in parallel.
Asynchronous message exchange provides control for the behavior of the system under heavy load: a larger number of concurrent consumers means a better throughput.
The caveat is that it puts the system under more stress.
The point is that, unlike synchronous calls where the load of the system is controlled by the requesting side, in the asynchronous model, the load of the system is controlled by the service provider’s side.
Furthermore, asynchronous communication has better opportunities for retrying a failed request, which improves the overall resilience to failure of the system.
You saw that asynchronous communication has obvious advantages in terms of scalability and robustness, but there’s a price to be paid for that: complexity.
The messaging middleware that performs the message storage and forwarding function is an additional mechanism that must be integrated with the application, which means that instead of a simple method call, you must deal with supplemental APIs and protocols.
Not only does the code become more complex, but also a performance overhead is introduced.
Figure 2.5 Asynchronous message exchange: the message is stored in an intermediate buffer before being received by the provider.
Also, in a request-reply scenario using the synchronous approach, the result of an operation can be easily retrieved: when the execution of the caller resumes, it already has the result.
In the case of asynchronous communication, the retrieval of the result is a more complex matter.
After issuing an asynchronous request, you have two components that execute independently: the requester that continues doing its work and the service provider whose work is deferred until later.
Concurrency is a powerful tool for increasing the application performance, but it adds complexity to your application too.
You have to consider concerns like thread--safety, proper resource management, deadlock, and starvation avoidance.
Also, at runtime, it’s harder to trace or debug a concurrent application.
Finally, the two concurrent operations will typically end up executing in distinct transactions.
Synchronous and asynchronous communication are compared in more detail in table 2.1
The advantages and disadvantages of the two paradigms should be carefully considered when choosing one over the other.
Traditionally, the integration between enterprise applications is based on message-driven, asynchronous mechanisms.
As the capabilities of hardware systems increase, especially when it comes to execution of concurrent processes and as multicore architectures become more pervasive, you’ll find opportunities to apply the patterns that are specific to integrating multiple applications to different components of an individual application.
The modules of the application may interact, not through a set of interfaces that need to be kept up to date, but through messages.
Various functions might be executed asynchronously, freeing up their calling threads to service other requests.
Definition Request is delivered immediately to provider, and the invoker blocks until it receives a response.
Requests are buffered and will be processed when the provider is ready.
Advantages – Simple interaction model – Immediate response – Allows the propagation of invocation.
Disadvantages – Lack of resilience to failure – Doesn’t scale properly under load.
While synchronous communication can take place using either procedure calls or messages, by its nature, asynchronous communication is message-driven.
We explored the benefits of the messaging paradigm compared to the service-interface approach, and we found the message-driven approach superior, so here’s what happens next: Spring Integration allows you to focus on the most important part of your system, the logical blueprint that describes how messages travel through the system, how they are routed, and what processing is done in the various nodes.
To understand the actions that’ll be taken in response to various messages, what transformations they’ll undergo, or where to send a particular message in order to get certain results, you don’t need to know whether the communication will be synchronous or asynchronous.
Without getting into too much detail, this configuration means the following:
All requests made to the system are sent to the input channel where, depending on their payload (either Order or Notification), they’re routed to the appropriate services.
Services in this context are just POJOs, completely unaware of the existence of the messaging system.
The results of processing the requests are sent to another channel where they’re picked up by another component, and so on.
Figure 2.6 provides a visual representation of the same message flow.
The structure doesn’t seem to tell what kind of interaction takes place (is it synchronous or asynchronous?), and from a logical standpoint, it shouldn’t.
Synchronous and asynchronous communication model is determined by the type of channel that’s used for transferring the messages between the endpoints.
Spring Integration supports both approaches by using two different types of message channels: DirectChannel (synchronous) and QueueChannel (asynchronous)
The interaction model doesn’t affect the logical design of the system (how many channels, what endpoints, how they’re connected)
But to improve the performance, you may want to free the message sending thread (which belongs to the main application) from executing the processing of orders and notifications and do that work asynchronously in the background, as seen in figure 2.8
Both types of channels are configured using the same <channel/> element.
By default, its behavior is synchronous, so the handling of a single message sent to the input-channel, including routing, invocation of the appropriate service, and whatever comes after that, will be done in the context of the thread that sends the message, as shown in figure 2.7
If you want to buffer the messages instead of passing them directly through a channel and to follow an asynchronous approach, you can use a queue to store them, like this:
Figure 2.6 Wiring of the order processing system: channels and endpoints define the logical application structure.
Figure 2.7 Synchronous order processing: the continuous line indicates an uninterrupted thread of control along the entire processing path.
Now the router will poll to see if any messages are available on the channel and will deliver them to the POJO that does the actual processing, as shown in figure 2.8
All this is taken care of transparently by the framework, which will configure the appropriate component.
You don’t need to change anything else in your configuration.
The configuration can also specify how the polling will be done or how many threads will be available to process notifications or orders, enabling the concurrent processing of those messages.
In this case, your application is implemented as a pipes-and-filters architecture.
The behavior of each component encapsulates one specific operation (transformation, business operation, sending a notification, and so on), and the overall design is message-driven so that the coupling between components is kept to a low level.
In some environments, it might be desirable to implement a full traversal of the messaging system as a synchronous operation (for example, if the processing is done as part of an online transaction processing system, where immediate response is a requirement, or for executing everything in a single transaction)
In this case, using synchronous DirectChannels would achieve the goal while keeping the general structure of the messaging system unchanged.
As you can see, Spring Integration allows you to design applications that work both synchronously and asynchronously, doing what it does best: separating concerns.
Logical structure is one concern, but the dynamic of the system is another, and they should be addressed separately.
This overview of synchronous and asynchronous communication wraps up our discussion about coupling.
Over the years, the effort to reduce coupling and to take advantage of the available communication infrastructure has led to the development of four major enterprise integration styles, which are the focus of the next section.
As the adoption of computer systems in enterprise environments grew, the need to enable interaction between applications within the enterprise soon became apparent.
Figure 2.8 Asynchronous order processing: the introduction of a buffered channel creates two threads of control (indicated by a continuous line)—delivery to the router takes place in B and processing takes place in C.
This interaction allowed organizations to both share data and make use of functionality provided by other systems.
In this section, we provide an overview of these four integration styles and their advantages and disadvantages, noting that although Spring Integration is built on the message-based paradigm, the other three forms are supported by the framework.
The most basic approach is for one application to produce a file and for that file to be made available to another system.
This approach is simple and generally interoperable because all that’s required is for interacting applications to be able to read and write files.
Because the basic requirements for using file-based integration are simple, it’s a fairly common solution, but some of the limitations of filesystems mean that additional complexity may be created in applications having to deal with files.
One limitation is that filesystems aren’t transactional and don’t provide metadata about the current state and validity of a file.
As a consequence, it’s hard to tell, for example, if another process is currently updating the file.
In general, to work properly, this type of integration requires some strategies to ensure that the receiver doesn’t read inconsistent data, such as a half-written file.
Also, it requires setting up a process by which corrupt files are moved out of the way to prevent repeated attempts to process them.
Another significant drawback is that applications generally need to poll specific locations to discover if more files are ready to be processed, thus introducing additional application complexity and a potential for unnecessary delay.
Considering these drawbacks, before deciding to use file-based integration, it’s always good to see whether any other integration style would work better.
Nevertheless, in certain situations, it may be the only integration option, and it’s not uncommon to come across applications that use it.
Databases are more advanced data storage mechanisms than files and alleviate some of the limitations of the filesystems.
They provide atomic operations, well-defined data structures, and mechanisms that provide some guarantees on data consistency.
Shared-database integration consists of providing the different applications with access to the same database.
As a smarter form of data transfer, by defining a set of staging tables where the different applications can write data that will be consumed by the receiver applications.
Compared with the filesystem style, this approach has the advantage that metadata, validation, and support for concurrent access are available out of the box for the transferred data.
This has no direct correspondent to the filesystem style and has the advantage that changes made by one application are made available to everyone else in real-time (unlike the data transfer approach, which involves writing data in the file or staging tables and a certain lag until the recipient application makes use of it)
One-size-fits-all is hardly true in software development, and the compromises necessary to implement a domain model (and subsequent database model) based on the needs of multiple business processes may result in a model that fits no one very well.
Sharing the same data model may create unwanted coupling between the different applications in the system.
This may seriously affect their capabilities of evolving in the future because changing it will require all the other applications to change as well (at least in the parts that deal directly with the shared model)
Concurrent systems frequently writing the same set of data might face performance problems because exclusive access will need to be granted from time to time, so they will end up locking each other out.
Caching data in memory might be an issue because applications may not be aware when it becomes stale.
Database sharing works well with transferring data between applications but doesn’t solve the problem of invoking functionality in the remote application.
When it comes to invoking functionality in the remote application, one possible solution is to use Remote Procedure Calls, which we discuss next.
Remote Procedure Calls (RPC) is an integration style that tries to hide the fact that different services are running on different systems.
The method invocation is serialized over the network to the service provider, where the service is invoked.
The return value is then serialized and sent back to the invoker.
This involves proxies and exporters (in Spring) or stubs and skeletons (in EJB)
The problem with this approach is that certain details about remoting can’t.
Assuming that RPC can hide these details will lead to simplistic solutions and leaky abstractions.
Dealing with problems properly will violate the loose coupling we’ve come to enjoy.
From case to case, this can be achieved in different ways, such as through Java serialization or using XML marshalling through mechanisms such as Java Architecture for XML Binding (JAXB)
This not only restricts the types that can be sent, it also requires that the application code deal with serialization or marshalling errors.
We’re probably not the first to tell you, but it’s worth mentioning again that the network isn’t reliable.
You can assume that a local method invocation returns within a certain (reasonable) time with the return value or an exception, but with a network connection in the mix, this can take much longer, and worse, it’s much less predictable.
The trickiest part is that the service that’s invoked can return successfully, but the invoker doesn’t get the response.
Assuming you don’t need to account for this is usually a bad idea.
Sending a representation of some Serializable to an application written in Perl is wishful thinking at best.
The need to know about the method name and argument order is questionable.
Once this is understood, it’s not such a big leap to look for an interoperable representation and a way to decouple the integration concerns from the method signatures on both sides.
This is one of the goals of messaging, our final integration style and the one that’s the general focus of Spring Integration.
Messaging is an integration style based on exchanging encapsulated data packets (messages) between components (endpoints) through connections (channels)
Potentially this resolves many of the problems of encapsulation and error handling associated with the previous three integration styles.
It also provides an easy way to deal with sharing both data and functionality, and overall it’s the most recommended integration style when you have a choice.
This chapter explained the fundamental concepts that stand behind enterprise application integration.
We provided applicable samples from the framework, and in the rest of the book, you’ll see them at work.
First, we discussed coupling as a way to measure how many assumptions two independent systems make about each other.
You saw how important it is to minimize coupling to allow the components to evolve independently.
There are multiple forms of coupling, but the most important ones are type-level and temporal coupling.
Dependency injection helps you deal with type-level coupling, and using a message passing approach instead of direct calls (either local or remote) helps you deal with temporal coupling.
We presented a detailed overview of the differences between synchronous and asynchronous communication and its implications in coupling, system complexity, and performance.
You saw how Spring Integration helps separate the logical design of the system from the dynamic behavior at runtime (whether interaction should be synchronous or asynchronous)
Finally, you got an overview of the four application integration paradigms and their respective strengths and weaknesses.
Spring Integration is generally focused on messaging, but it provides support for using the other three types as well.
We introduced Spring Integration from a high level in the first chapter.
In chapter 3, we look in detail at the parts of Spring Integration that enable messaging, starting with messages and channels.
Part 2 explains how Spring Integration provides extended messaging capabilities to Spring applications.
It covers key concepts such as messages, channels, endpoints, routing, filtering, and splitting and aggregating messages.
With Spring Integration, messages are exchanged through channels between designated endpoints of application components as well as external systems.
Also, based on enterprise integration patterns, it offers features that enable routing and filtering of messages based on message headers or content.
Chapter 3 introduces messages and channels, describes the types of channels available and how they work, and demonstrates channel customizations such as dispatchers for multiple message handlers and interceptors for message monitoring and filtering.
Chapter 4 describes the details of endpoints and how they work as a foundation for higher-level components.
Endpoints contain business logic or integration components such as routers, splitters, or aggregators.
Chapter 5 explores the separation of business and integration concerns and additional features such as transformers, service activators, gateways, and chaining.
Chapter 6 investigates techniques for routing and filtering messages, and implementing more complex nonsequential message flows.
Chapter 7 addresses techniques for splitting messages into parts and aggregating messages into composites, as well as reordering messages and other ways to customize aggregations.
Endpoints are discussed in a chapter of their own (chapter 4), but first you get to read about messages and channels.
In our introduction to messages, we show how the information exchanged by Spring Integration is organized.
When describing channels, we discuss the conduits through which messages travel and how the different components are connected.
Spring Integration provides a variety of options when it comes to channels, so it’s important to know how to choose the one that’s right for the job.
Finally, there are a couple of more advanced components you can use to enhance the functionality of channels: dispatchers and interceptors, which we discuss at the end of the chapter.
Throughout the remaining chapters of this book, many include a section called “Under the hood,” where we discuss the reasoning behind the API and provide details about the internal workings of the concepts discussed.
But in this chapter we focus on the API itself, because messages and channels are fundamental concepts.
Understanding how Spring Integration represents them is an essential foundation for building the rest of your knowledge about the framework.
The best way to get to trust something is to understand how it works.
As you saw in chapter 2, message-based integration is one of the major enterprise integration styles.
Spring Integration is based on it because it’s the most flexible and allows the loosest coupling between components.
We start our journey through Spring Integration with one of its building blocks: the message.
In this section, we discuss the message as a fundamental enterprise integration pattern and provide an in-depth look at how it’s implemented in Spring Integration.
A message is a discrete piece of information sent from one component of the software system to another.
By piece of information, we mean data that can be represented in the system (objects, byte arrays, strings, and so forth) and passed along as part of a message.
Each discrete message contains all the information that’s needed by the receiving component for performing a particular operation.
Besides the application data, which is destined to be consumed by the recipient, a message usually packs meta-information.
The meta-information is of no interest to the application per se, but it’s critical for allowing the infrastructure to handle the information correctly.
A letter contains an individual message destined to the recipient but is also wrapped in an envelope containing information that’s useful to the mailing system but not to the recipient: a delivery address, the stamp indicating that mailing fees are paid, and various stamps applied by the mailing office for routing the message more efficiently.
Based on the role they fulfill, we distinguish three types of messages:
Now that we have an overview of the message concept, we can talk about Spring Integration’s approach to it.
Next we look at it from an API and implementation standpoint and then see how to create a message.
Introducing Spring Integration messages message yourself, but to understand the Spring Integration design, it’s important to see how message creation works.
The nature of a message in Spring Integration is best expressed by the Message interface:
A message is a generic wrapper around data that’s transported between the different components of the system.
The wrapper allows the framework to ignore the type of the content and to treat all messages in the same way when dealing with dispatching and routing.
Having an interface as the top-level abstraction enables the framework to be extensible by allowing extensions to create their own message implementations in addition to those included with the framework.
It’s important to remember that, as a user, creating your own message implementations is hardly necessary because you usually won’t need to deal with the message objects directly.
The content wrapped by the message can be any Java Object, and because the interface is parameterized, you can retrieve the payload in a type-safe fashion.
Besides the payload, a message contains a number of headers, which are metadata associated with that message.
For example, every message has an identifier header that uniquely identifies the message instance.
In other cases, message headers may contain correlation information indicating that various individual messages belong to the same group.
A message’s headers may even contain information about the origin of the message.
For example, if the message was created from data coming from an external source (like a JMS [Java Message Service] queue or the filesystem), the message may contain information specific to that source (like the JMS properties or the path to the file in the filesystem)
Their role is to store the header values, which can be any Java Object, and each value is referenced by a header name:
As you may have noticed from the definition of the Message interface, there is no way to set something on a Message.
You might worry about how these messages are created and why they can’t be modified.
The short answer is that you don’t need to create them yourself and that they’re immutable to avoid issues with concurrency.
In publish-subscribe scenarios, the same instance of a message may be shared among different consumers.
Immutability is a simple and effective way to ensure thread safety.
Imagine what would happen if multiple concurrent consumers were to modify the same Message instance simultaneously by changing its payload or one of its header values.
As mentioned, Spring Integration provides a few implementations of its own for the Message interface, mostly for internal use.
Instead of using those implementations directly or creating implementations of your own, you can use Spring Integration’s MessageBuilder when you need to create a new Message instance.
The MessageBuilder creates messages from either a given payload or another message (a facility necessary for creating message copies)
There are three steps in this process, shown in figure 3.1
The figure shows the builder creating a message from the payload, but the same steps apply for creating a message from another message.
It should be noted that the MessageBuilder initializes a number of headers that are needed by default by the framework, and this is one of the significant advantages of using the MessageBuilder over instantiating messages directly: you don’t have to worry about setting up those headers yourself.
As an example, creating a new message with a String payload can take place as follows:
The MessageBuilder also provides a variety of methods for setting the headers, including options for setting a header value only if that header hasn’t already been set.
A more elaborate variant of creating the helloMessage could be something like this:
You can also create messages by copying the payload and headers of other messages and can change the headers of the resulting instance as necessary:
Now you know what messages are in Spring Integration and how you can create them.
Next, we discuss how messages are sent and received through channels.
Messages don’t achieve anything by sitting there all by themselves.
To do something useful with the information they’re packaging, they need to travel from one component to another, and for this they need channels, which are well-defined conduits for transporting messages across the system.
The sender creates the letter and hands it off to the mailing system by depositing it in a well-known location: the mailbox.
From there on, the letter is completely under the control of the mailing system, which delivers it to various waypoints until it reaches the recipient.
The most that the sender can expect is a reply.
The sender is unaware of who routes the message or, sometimes, even who may be the physical reader of the letter (think about writing to a government agency)
From a logical standpoint, the channel is much like a mailbox: a place where components (producers) deposit messages that are later processed by other components (consumers)
This way, producers and consumers are decoupled from each other and are only concerned about what kinds of messages they can send and receive, respectively.
One distinctive trait of Spring Integration, which differentiates it from other enterprise integration frameworks, is its emphasis on the role of channels in defining the enterprise integration strategy.
Channels aren’t just information transfer components; they play an active role in defining the overall application behavior.
The business processing takes place in the endpoints, but you can alter the channel configuration to completely change the runtime characteristics of the application.
We explain channels from a logical perspective and offer overviews of the various channel implementations provided by the framework: what’s characteristic to each of.
To connect the producers and consumers configured in an application, you use a channel.
All channels in Spring Integration implement the following MessageChannel interface, which defines standard methods for sending messages.
The reason no methods are provided for receiving messages is because Spring Integration differentiates clearly between two mechanisms through which messages are handed over to the next endpoint—polling and subscription—and provides two distinct types of channels accordingly.
This type of channel requires the receiver or the framework acting on behalf of the receiver to periodically check whether messages are available on the channel.
This approach has the advantage that the consumer can choose when to process messages.
The approach can also have its downsides, requiring a trade-off between longer poll periods, which may introduce latency in receiving a message, and computation overhead from more frequent polls that find no messages:
It’s important to understand the characteristics of each message delivery strategy because the decision to use one over the other affects the timeliness and scalability of the system.
From a logical point of view, the responsibility of connecting a consumer to a channel belongs to the framework, thus alleviating the complications of defining the appropriate consumer types.
To put it plainly, your job is to configure the appropriate channel type, and the framework will select the appropriate consumer type (polling or event-driven)
Also, subscription versus polling is the most important criterion for classifying channels, but it’s not the only one.
In choosing the right channels for your application, you must consider a number of other criteria, which we discuss next.
Spring Integration offers a number of channel implementations, and because MessageChannel is an interface, you’re also free to provide your own implementations.
The type of channel you select has significant implications for your application, including transactional boundaries, latency, and overall throughput.
This section walks you through the factors to consider and through a practical scenario for selecting appropriate channels.
In the configuration, we use the namespace, and we also discuss which concrete channel implementation will be instantiated by the framework.
In our flight-booking internet application, a booking confirmation results in a number of actions.
Foremost for many businesses is the need to get paid, so making sure you can charge the provided credit card is a high priority.
You also want to ensure that, as seats are booked, an update occurs to indicate one less seat is available on the flight so you don’t overbook the flight.
The system must also send a confirmation email with details of the booking and additional information on the check-in process.
In addition to a website, the internet booking application exposes a REST interface to allow third-party integration for flight comparison sites and resellers.
Because most of the airline’s premium customers come through the airline’s website, any design should allow you to prioritize bookings originating from its website over third-party integration requests to ensure that the airline’s direct customers experience a responsive website even during high load.
The selection of channels is based on both functional and nonfunctional requirements, and several factors can help you make the right choice.
Table 3.1 provides a brief overview of the technical criteria and the best practices you should consider when selecting the most appropriate channels.
Let’s see how these criteria apply to our flight-booking sample.
Table 3.1 How do you decide what channel to use?
Do you need to propagate context information between the successive steps of a process?
Thread-local variables are used to propagate context when needed in several places where passing via the stack would needlessly increase coupling, such as in the transaction context.
Relying on the thread context is a subtle form of coupling and has an impact when considering the adoption of a highly asynchronous staged event-driven architecture (SEDA) model.
It may prevent splitting the processing into concurrently executing steps, prevent partial failures, or introduce security risks such as leaking permissions to the processing of different messages.
Do you have all-or-nothing scenarios? – Classic example: bank transaction where credit and debit should either both succeed or both fail.
Typically used to decide transaction boundaries, which makes it a specific case of.
Influences the threading model and therefore limits the available options when choosing a channel type.
Those resources are unused when the load decreases, so this approach could be expensive and inefficient.
Moreover, some of the steps may be slow, so resources may be blocked for long durations.
Consider what requires an immediate response and what can be delayed; then use a buffer to store incoming requests at peak rate, and allow the system to process them at its own pace.
Consider mixing the types of processing—for example, an online purchase system that immediately acknowledges receipt of the request, performs some mandatory steps (credit card processing, order number generation), and responds to the client but does the actual handling of the request (assembling the items, shipping, and so on) asynchronously in the background.
How many messages can you buffer? What should you do when you can’t cope with demand?
If your application can’t cope with the number of messages being received and no limits are in place, you may exhaust your capacity for storing the message backlog or breach quality-of-service guarantees in terms of response turnaround.
Recognizing that the system can’t cope with demands is usually a better option than continuing to build up a backlog.
A common approach is to apply a degree of self-limiting behavior to the system by blocking the acceptance of new messages where the system is approaching its maximum capacity.
This limit commonly is a maximum number of messages awaiting processing or a measure of requests received per second.
Where the requester has a finite number of threads for issuing requests, blocking those threads for long periods of time may result in timeouts or quality-of-service breaches.
It may be preferable to accept the message and then discard it later if system capacity is being exceeded or to set a timeout on the blocking operation to avoid indefinite blocking of the requester.
Using the default channel throughout, we have three channels—one accepting requests and the other two connecting the services:
The effect is simple: one thread is responsible for invoking the three services sequentially, as shown in figure 3.2
Because all operations are executing in a single thread, a single transaction encompasses those invocations.
That assumes that the transaction configuration doesn’t require new transactions to be created for any of the services.
How many components are interested in receiving a particular message? – There are two major messaging paradigms: point-to-point and publish-subscribe.
If the processing requirements are that the same message should be handled by multiple consumers, the consumers can work concurrently and a publish-subscribe channel can take care of that.
An example is a mashup application that aggregates results from searching flight bookings.
Requests are broadcast simultaneously to all potential providers, which will respond by indicating whether they can offer a booking.
Conversely, if the request should always be handled by a single component (for example, for processing a payment), you need a point-to-point strategy.
Table 3.1 How do you decide what channel to use? (continued)
Figure 3.2 Diagram of threading model of service invocation in the airline application.
Figure 3.3 shows what you get when you configure an application using the default channels, which are subscribable and synchronous.
But having all service invocations happening in one thread and encompassed by a single transaction is a mixed blessing: it could be a good thing in applications where all three operations must be executed atomically, but it takes a toll on the scalability and robustness of the application.
The basic configuration is good in the sunny-day case where the email server is always up and responsive, and the network is 100% reliable.
Your application needs to work in a world where the email server is sometimes overloaded and the network sometimes fails.
Analyzing your actions in terms of what you need to do now and what you can afford to do later is a good way of deciding what service calls you should block on.
Billing the credit card and updating the seat availability are clearly things you need to do now so you can respond with confidence that the booking has been made.
Sending the confirmation email isn’t time critical, and you don’t want to refuse bookings simply because the mail server is down.
Therefore, introducing a queue between the mainline business logic execution and the confirmation email service will allow you to do just that: charge the card, update availability, and send the email confirmation when you can.
This will also require either an explicit or default poller configuration for the consuming endpoint connected to the queue channel.
Figure 3.3 Diagram of threading model of service invocation when using a default channel.
Let’s recap how the threading model changes by introducing the QueueChannel, shown in figure 3.4
Because a single thread context no longer encompasses all invocations, the transaction boundaries change as well.
Essentially, every operation that’s executing on a separate thread executes in a separate transaction, as shown in figure 3.5
By replacing one of the default channels with a buffered QueueChannel and setting up an asynchronous communication model, you gain some confidence that longrunning operations won’t block the application because some component is down or takes a long time to respond.
But now you have another challenge: what if you need to connect one producer with not just one, but two (or more) consumers?
We’ve looked at scenarios where a number of services are invoked in sequence with the output of one service becoming the input of the next service in the sequence.
This works well when the result of a service invocation needs to be consumed only once, but it’s common that more than one consumer may be interested in receiving certain messages.
In our current version of the channel configuration, successfully billed bookings that have been recorded by the seat availability service pass directly into a queue for email confirmation.
In reality, this information would be of interest to a number of services within the application and systems within the enterprise, such as customer relationship management systems tracking customer purchases to better target promotions and finance systems monitoring the financial health of the enterprise as a whole.
To allow delivery of the same message to more than one consumer, you introduce a publish-subscribe channel after the availability check.
The publish-subscribe channel provides one-to-many semantics rather than the one-to-one semantics provided by most channel implementations.
One-to-many semantics are particularly useful when you want the flexibility to add additional consumers to the configuration; if the name of the publish-subscribe channel is known, that’s all that’s required for the configuration of additional consumers with no changes to the core application configuration.
The publish-subscribe channel doesn’t support queueing, but it does support asynchronous operation if you provide a task executor that delivers messages to each of.
Figure 3.4 Diagram of threading model of service invocation when using a QueueChannel.
Figure 3.5 Diagram of transactional boundaries when a QueueChannel is used.
But this approach may still block the main thread sending the message on the channel where the task executor is configured to use the caller thread or block the caller thread when the underlying thread pool is exhausted.
To ensure that a backlog in sending email confirmations doesn’t block either the sender thread or the entire thread pool for the task executor, you can connect the new publish-subscribe channel to the existing email confirmation queue by means of a bridge.
The bridge is an enterprise integration pattern that supports the connection of two channels, which allows the publish-subscribe channel to deliver to the queue and then have the thread return immediately:
Now it’s possible to connect one producer with multiple consumers by means of a publish-subscribe channel.
Let’s get to the last challenge and emerge victoriously with our dramatically improved application: what if “first come, first served” isn’t always right?
Let’s say you want to ensure that the airline’s direct customers have the best possible user experience.
To do that, you must prioritize the processing of their requests so you can render the response as quickly as possible.
Using a comparator that prioritizes direct customers over indirect, you can modify the first channel to be a priority queue.
This causes the framework to instantiate an instance of PriorityChannel, which results in a queue that can prioritize the order in which the messages are received.
The configuration changes made in this section are an example of applying different types of channels for solving the different requirements.
Starting with the defaults and working through the example, we replaced several channel definitions with the ones most suitable for each particular situation encountered.
What’s most important is that every type of channel has its own justification, and what may be advisable in one use case may not be advisable in another.
We illustrated the decision process with the criteria we find most relevant in each case.
This wraps up the overview of the Spring Integration channels and makes way for some more particular components that can be used for fine tuning the functionality of a channel.
From time to time, creating more advanced applications requires going beyond sending and receiving messages.
For this purpose, Spring Integration provides additional components that act as channel collaborators.
In most of the examples given so far, a channel has a single message handler connected to it.
Though having a single component that takes care of processing the messages that arrive on a channel is a pretty common occurrence, multiple handlers processing messages arriving on the same channel is also a typical configuration.
Depending on how messages are distributed to the different message handlers, we can have either a competing consumers scenario or a broadcasting scenario.
In the broadcasting scenario, multiple (or all) handlers will receive the message.
In the competing consumers scenario, from the multiple handlers that are connected.
Having multiple receivers that are capable of handling the message, even if only one of them will actually be selected to do the processing, is useful for load balancing or failover.
The strategy for determining how the channel implementation dispatches the message to the handlers is defined by the following MessageDispatcher interface:
Providing your own implementations of MessageDispatcher is uncommon and should be resorted to only where other options don’t provide the level of control desired over the process of delivering messages to handlers.
An example where a custom MessageDispatcher could be appropriate is where different handlers are being used to deal with varying service levels.
In this scenario, a custom dispatcher could be used to prioritize messages according to the defined service-level agreements.
The dispatcher implementation could inspect the message and attempt to dispatch to the full set of handlers only where a message is determined to be high priority:
So far, we’ve seen how to control the way messages from a channel are distributed to the message handlers that are listening on that particular channel.
Now let’s see how to intervene in the process of sending and receiving messages.
Another important requirement for an integration system is that it can be notified as messages are traveling through the system.
This functionality can be used for several purposes, ranging from monitoring of messages as they pass through the system to vetoing send and receive operations for security reasons.
For supporting this, Spring Integration provides a special type of component called a channel interceptor.
The channel implementations provided by Spring Integration all allow the registration of one or more ChannelInterceptor instances.
Channel interceptors can be registered for individual channels or globally.
The ChannelInterceptor interface allows implementing classes to hook into the sending and receiving of messages by the channel:
Just by looking at the names of the methods, it’s easy to get an idea when these methods are invoked, but there’s more to this component than simple notification.
This allows the implementation to control what gets sent to the channel, effectively filtering the messages.
It’s invoked when a component calls receive() on the channel, but before a Message is actually read from that channel.
It allows implementers to decide whether the channel can return a message to the caller.
It’s invoked after a message is read from a channel but before it’s returned to the component that called receive()
This allows the implementer to control what, if anything, is actually received by the poller.
Creating a new type of interceptor is typically done by implementing the ChannelInterceptor interface:2
In this case, the interceptor intercepts the messages and sends their payloads to an audit service (injected in the interceptor itself)
Before an email request is sent out, the audit service logs the charged bookings that were created by the application.
Setting up the interceptor on a channel is also straightforward.
An interceptor is defined as a regular bean, and a special namespace element takes care of adding it to the channel, as follows:
Some of the best examples for understanding the use of the ChannelInterceptor come from Spring Integration, which supports two different types of interception: monitoring and filtering messages before they are sent on a channel.
For the monitoring scenario, Spring Integration provides WireTap, an implementation of the more general Wire Tap enterprise integration pattern.
As you saw earlier, it’s easy to audit messages that arrive on a channel using a custom interceptor, but WireTap enables you to do this in an even less invasive fashion by defining an interceptor that sends copies of messages on a distinct channel.
This means that monitoring is completely separated from the actual business flow, from a logical standpoint, but also that it can take place asynchronously.
For each booking you charge, you send a copy of the message on the monitoringChannel, where it can be analyzed by a monitoring handler independently of the main application flow.
The filtering scenario is based on the idea that only certain types of messages can be sent to a given channel.
The MessageSelector is used in several other places in the framework, and its role is to encapsulate the decision whether a message is acceptable, taking into consideration a given set of criteria, as shown here:
The Datatype Channel pattern is also supported by simply adding a datatype attribute to any channel element.
With this setup, only messages with a ChargedBooking payload are allowed to be sent on the chargedBookings channel.
The difference is subtle, as the role of filters is mostly to prevent messages from reaching other endpoints (especially in a chain setup), while selectors prevent messages from being sent on channels; they do pretty much the same thing but in different scenarios.
The concepts of messages and channels are essential to the flexibility inherent in applications built on Spring Integration.
The ease of swapping channel implementations provides a high degree of flexibility in controlling threading models, thread utilization, and latency.
In choosing the correct channel, it’s vital to understand the behavior of the provided implementations because choosing incorrectly can have serious performance implications or can invalidate the correctness of the application by altering the transactional boundaries.
In this chapter, you learned exactly what a message is to Spring Integration and how you can create messages of your own.
You also learned what channels are, and we gave you examples to help you choose the right channel for the job.
In the next chapter, we dive into the components that are connected by channels.
The endpoints in Spring Integration contain your business logic but can also be infrastructural components like routers or splitters.
The latter offer the flexibility of even more loosely coupled interaction, but break that transaction boundary across separate threads for the producer and sender.
From the programming perspective, the event-driven model is easier to grasp.
For example, the MessageHandler interface that’s central to Spring Integration is as simple as it can be:
In the previous chapter, we covered message channels in detail.
You now know that some channels accept subscribers to be called in an event-driven way, whereas others require polling consumers.
All message-handling components in Spring Integration, such as transformers, splitters, and routers, implement that interface.
Therefore those implementations are relatively straightforward, always reacting to a received message, much in the same way as a Java Message Service (JMS) MessageListener would.
These components can be connected to any type of channel, but some of the channel types accept event-driven subscribers, whereas others must be polled.
Clearly, to connect components to a channel, a certain amount of glue is necessary.
In Spring Integration, that glue comes in the form of an adapter that understands how to interact with a given channel while delegating to a MessageHandler.
The generic term for that adapter is a message endpoint.
A few different kinds of adaptation are required depending on the type of channel and the role of the handler component being connected to that channel.
If the channel is subscribable, the handler is invoked within the thread of the sender or by the channel’s TaskExecutor if one is configured.
If on the other hand the channel is one that buffers messages in a queue, such as a QueueChannel, polling is necessary.
In addition to the distinction between subscription and polling, you must consider whether a reply message is expected.
If a reply is expected, would that reply be sent back to a caller or passed to the next component within a linear pipeline? Likewise, when interacting with an external system, you must consider whether a component is unidirectional (a channel adapter) or bidirectional (a gateway)
One of the key benefits of a messaging system is the ability to support these different interaction models and to even switch among them by changing configuration rather than directly impacting the underlying components.
In other words, it should be trivial to change from a request-reply interaction to a pipeline if you determine that an additional transformation step is necessary before sending a reply.
In this chapter, we explore the different endpoints available in Spring Integration and see how they can be used to decouple message-producing and message-consuming components from the channel type and response strategy.
First we describe the difference between polling and event-driven components in detail.
Then we look at those differences in the context of inbound and outbound endpoints.
Finally we discuss the different response strategies to clarify the distinction between channel adapters and gateways.
Not all patterns have their direct counterpart in the API—not every pattern maps directly to an implementation.
Some patterns are more conceptual, and others describe a broad category within which implementations may be classified.
The message endpoint is an example of such a pattern.
The endpoint is too generic to be covered by a single concrete implementation.
Instead, we opted for different endpoint implementations in a class hierarchy and explicitly named patterns like transformer, splitter, and router defined in an XML.
Likewise, taking into account the various external endpoints, the named patterns gateway and channel adapter are defined in an XML schema for each supported type of system (file, JMS, and so on)
Other patterns that are supported but not literally implemented include Request-Reply, Selective Consumers, and Return Address.
The first two are composite configurations of other components, and the return address pattern is supported by a message header that’s recognized by any replyproducing message handler that doesn’t have an explicit output channel configured.
Before we can talk about endpoints and decide which one is best applied to a given problem, it’s important to understand the different properties of endpoints a little better.
The next section goes over different characteristics of endpoints that will keep popping up in the rest of the book.
We need to be precise in our understanding of the properties of a certain type of endpoint before we can have a meaningful discussion about it.
One advantage of Spring Integration is the consistent naming in the namespace support.
This section explores what makes endpoints different enough to give them different names.
Considering all the possible ways we can combine the switches, we end up with 16 candidates for unique names.
Multiply this by the number of supported protocols in case of an external endpoint, and the numbers become dizzying.
Luckily, only certain combinations make sense for a given protocol, which greatly reduces the number of options.
Also, Spring Integration can implicitly take care of the polling concerns.
You still have plenty of options to consider—polling or eventdriven, inbound or outbound (from the perspective of the Spring Integration application), unidirectional or bidirectional (one-way or request-reply respectively), internal or external (with respect to the application context)—so let’s look at the most important examples in table 4.1
As you might guess from the table, there are some naming conventions for endpoints.
Wherever possible, the names were chosen to  closely mirror a corresponding pattern name.
In addition, the properties are signified by consistently applied tokens.
For example, a gateway is always capable of bidirectional communication.
This doesn’t change between HTTP and JMS or between inbound and outbound.
In contrast, a channel adapter is always unidirectional: it’s either the beginning (inbound)
Now that you know the choices you can make before selecting an endpoint, it’s time to look into the implications of those choices.
First we ask what may be the most important question when designing a messaging system.
Those of you who’ve read Hamlet know that Shakespeare got some of it right: the important decision is whether you should be a slave to your invokers or take matters into your own hands.
We shouldn’t stretch the analogy to suicide, so let’s get practical.
When a user of your application is entering a lot of information, you’d like to save that input somewhere as soon as possible.
Modern browsers support client-side storage facilities, but in many applications that need to be compatible with lesser browsers, this isn’t an option.
Even if client-side storage is an option technically, in many cases it isn’t an option functionally.
Imagine a web application that allows users to collaborate, as they would with an online word processor.
This would require changes to flow from one user to another in two directions with the server as a referee in the middle.
Although it’s important to get the changes from one user to another quickly, it’s much more important to get the changes from a single user to their client quickly.
In other words, if a sentence reaches another user a second after one user typed it, it would be fine.
If the cursor were trailing a sentence behind the current user’s typing in the client, nobody would use the application.
In most cases when you’re typing, an application should respond to a key press within 0.1 seconds; otherwise the application would be unusable.
Changes from somebody else may take up to a second before you would notice that they’re delayed.
You’ve probably seen a few applications that allow you to collaborate within these boundaries; the increased use of this type of collaborative editor came with the rise of Ajax.
With the boundaries of network latency, the only option to make an application like this work is to asynchronously send one user’s changes to another user’s client without making the first user wait for confirmation.
As discussed in chapter 3, we use messages as the central concept to transfer data from one place to another.
What can you expect of an endpoint? If messages are handed off asynchronously, the assumption is that they should eventually be received by a polling consumer.
Because the poller is an active process, it requires a separate worker thread.
For this purpose, Spring Integration integrates with any implementation of the core Spring TaskExecutor abstraction.
Thread management is then a responsibility of that implementation and can even be delegated to a WorkManager when running in an application server environment that requires threads to be managed by the container.
The problem at hand is to make sure the separate worker thread is available and used properly to continue processing messages that have been handed off asynchronously.
Even more important, we need to understand how to switch between synchronous and asynchronous invocation.
A polling endpoint will actively request new data to process.
This data may be messages in a QueueChannel, files in a directory, messages in a JMS destination, and so on.
The endpoint needs at least a single thread to perform this polling.
You could handcode an endpoint like this by creating an infinite loop and invoking receive periodically.
Although it may be easier to understand the inner workings of an active endpoint because the behavior is self-contained, coding components like this yourself has some serious downsides.
First of all, it requires you to write threading code, which is notoriously hard to do right.
Second, it makes the component much harder to test in a unit test.
Third, it becomes troublesome to integrate the component where a passive component is needed.
In Spring Integration, you don’t have to write different code depending on whether you want your component to be active.
Instead, whether the component should be active or passive is inferred from the configuration and handled by the framework.
Configuring the components that will take the responsibility for the polling concerns is still up to you.
Asynchronous handoff usually isn’t required, and in those cases, an event-driven model should be used.
This can be as simple as wrapping a plain old Java object (POJO)
The essential thing about passive, or event-driven, components is that they don’t take responsibility for thread management.
They’re still responsible for proper thread safety, but this could be as simple as not maintaining any mutable state.
Just as important as getting information out of the messaging system is getting information into it.
This is done through inbound endpoints, typically receiving information in a format that’s not native to Spring Integration.
Examples are a web service invocation, an inbound email, or a file written to a directory.
A Java method invocation is also a plausible inbound integration point.
The generic algorithm for an inbound endpoint is shown in figure 4.1
Whether an inbound endpoint needs to poll or can be event-driven depends on the architecture external to the message system.
Some technical constraints, such as the lack of filesystem events in Java, affect this decision.
But in most cases it depends on the system with which you’re integrating.
If the external integration point is passive, the inbound endpoint becomes responsible for actively polling for updates.
In Spring Integration, several polling endpoints are provided: inbound channel adapters for files, JMS, email, and a generic method-invoking variant.
The latter can be used to create custom channel adapters, such as for querying Twitter or a Really Simple Syndication (RSS) feed.
Twitter and RSS adapters are both available as of Spring Integration 2.0, but before they were added, users could simply configure the generic method-invoking adapter to call their own implementations.
Often, an external system actively invokes a service on the messaging system.
When that happens, the responsibility of thread management can be left out of the messaging solution and left to the invoker (in the case of a local Java method call) or to the container (in the case of a web service invocation)
But as soon as QueueChannels are to be used internally, thread management becomes a concern of the messaging system again.
Examples of event-driven endpoint types supported in Spring Integration include web services (through Spring WS), Remote Method Invocation (RMI), and JMS (where Spring’s listener container takes care of the polling)
Again, you can generically create a custom event-driven endpoint using the @Gateway annotation.
At some point in a message flow, you’ll likely need to invoke a service that’s external to the messaging system.
This may be a local method call, a web service invocation, or a message sent on another messaging system such as JMS or Extensible Messaging and Presence Protocol (XMPP)
The general responsibilities of an outbound endpoint are depicted in figure 4.2
Input is taken from an external source and then converted into a message, which is sent to a channel.
What can you expect of an endpoint? The algorithm shown in this figure usually must implement details such as transactions and security, which we cover later.
The details of conversion and the invocation of external services are also assumed to be implementation details of the specific endpoint.
But the reception of the message and whether a result is generated are concepts that belong to the base API.
If an outbound endpoint is connected to a PollableChannel, it must invoke the receive method on its input channel to receive messages.
That requires scheduling an active process to do the polling periodically.
In other words, step 1 in the algorithm is triggered by the endpoint.
Spring Integration automatically wraps any passive components in a polling endpoint if they’re configured to be on the receiving end of a PollableChannel.
The channel ensures that a thread exists to invoke the endpoint when a message arrives.
In many cases, the thread that invoked the send method on the channel is used, but it could also be handled by a thread pool managed at the channel level.
The advantage of using a thread pool is that the sender doesn’t have to wait, but the disadvantage is that a transactional boundary would be broken because the transaction context is associated with the thread.
Endpoints using this style of communication are called channel adapters.
Unidirectional communication is often enough to establish a working solution, but bidirectional requirements are common.
The EIP definition of gateway isn’t as clear as we would’ve liked, so we did a bit of interpretation.
Mainly, the lack of clear distinction between synchronous invocation and asynchronous handoff makes the gateway concept too broad and widely overlapping with other concepts, such as channel adapter.
In Spring Integration, a gateway is synonymous with synchronous two-way communication.
If you need an asynchronous gateway, you should compose it.
First a message is received from a channel; then the message is converted into something the external component understands.
When a message reaches an endpoint, through polling or otherwise, two things can happen:
The first case is where you’d use a channel adapter, as shown in the previous sections.
The second case becomes more complicated, but luckily Spring Integration has gateways to help you support it.
The complexity lies in that you must do something with the response.
Usually you’ll want to either send it further along its way to the next endpoint or send a confirmation back to the original sender when all processing for this message is done.
For the first option, you set an output channel, and for the second, you omit the output channel and Spring Integration uses the REPLY_CHANNEL header to find the channel on which the original sender wants the confirmation to go.
At this point you should have a good understanding of the different high-level categories of endpoints.
We explored the distinctions between polling and event-driven consumers, and we discussed both unidirectional and bidirectional behavior.
In the next section, we add one of the most important pieces to the puzzle: transactions.
Unfortunately, although their importance is undeniable, transactions can be confusing for many developers.
We’ll do our best to remedy that situation, at least in the context of Spring Integration applications.
If you’re an expert on transaction management, you might like to skip parts of the next section and just skim for Spring Integration particulars.
A common mistake in moving from thread per request to asynchronous handoff is to make incorrect assumptions about transaction boundaries.
Another common mistake is to incorrectly assume that the security context is set in the thread that’s receiving the handoff.
Both incorrect assumptions occur because security and transaction contexts are traditionally stored in a ThreadLocal.
This makes sense for most web applications, and you should practice caution if you want to break this convention.
Security contexts shouldn’t be shared lightly, and transactions should be kept short.
This section goes into the details of transaction management around Spring Integration endpoints.
This isn’t a general work on transactions, so we focus on endpointrelated concerns here.
Before we explore the technicalities of transaction management and draw parallels to security, we must establish the rationale of thread-bound contexts.
Have you ever been working on a shared file and found out that someone else just fixed the same problem you worked on all morning? We bet you have.
Transaction boundaries around endpoints a situation like that is usually to request immediate notification of any changes that others plan to make.
Or your team decides to plan better or to use a new tool.
Have you ever tried to work on something and been constantly distracted by team members who were trying to work on the same thing and tried to align with you? Have you ever been stuck in planning meetings all day? We bet you have.
You must carefully consider the consequences of living with the overhead of either sharing or merging changes.
This is exactly the choice you must make when establishing the extent of a transaction.
From a traditional web application point of view, you want to give your user a consistent view on reality (open transaction in view or even a conversation), but you also want to ensure the smallest possible chance of your user running into a conflict with another user’s changes.
Good practice is to keep transactions small and to use compensation instead of rollback to recover from mistakes.
For this reason, it makes sense to have a transaction context confined to the thread that’s servicing a single HTTP request.
One commonly suggested approach is to store transaction contexts in a message instead, but that might considerably increase the scope and duration of the transaction, so it’s not natively supported by Spring Integration.
We talked about sharing a view of the world from the perspective of an editor, but how about sharing only with people you trust?
Security contexts don’t become stale over time or open the door to merge conflicts.
Once properly authorized and kept in sight at all times, a user doesn’t need to reenter a password.
The problem is that it’s easy to let someone slip out of sight for a second.
If you share a security context with multiple threads, you need to be careful to ensure that the security details are visible only from the context of the user.
If multiple users are using the application concurrently, the least error-prone isolation mechanism is to allow access only from the thread currently processing the request.
Usually, the authentication is no longer needed; just a username will do.
Classic web applications usually don’t need to worry about asynchronous handoff to work.
Spring Integration comes in when asynchronous handoff becomes relevant, and at that point, you must jump through some hoops.
Of course, the first thing a crafty engineer asks before jumping through hoops is whether they can get away with not doing it.
Go to the most senior database administrator in your organization and ask this question.
After you’ve listened carefully to his lecture, remember at least one thing: there’s no excuse for being clueless about transactions.
Transactions aren’t the answer to all your problems, and overusing them can be detrimental to performance and scalability.
ACID, as we all know, stands for atomic, consistent, isolated, and durable.
A transaction is supposed to give you all of this.
Don’t be fooled: with a lot of ifs and buts, a transaction will give you something quite close to ACID, or an exception.
The tricky part of ACID is isolation in combination with global consistency.
Let’s look at two examples of multiple reasonable users in a standard web application.
Jill and Jack must check in for their afternoon flight to New York.
They’re on the same flight, and they have independently decided to try to find a seat next to each other.
She tags that she wants to travel with a companion and fills in Jack’s details.
The application renders a page to Jill that shows Jack’s position on the plane.
Jill stops to check the make of the plane and the seat arrangement to decide whether she wants the aisle seat opposite to Jack or whether she wants to take the middle seat next to him.
It won’t be as comfortable as the other seat, but at least they’ll sit together.
Jack’s transaction completes fine, and he runs for his cab.
Jill is done as well, hits Submit, sees that her seat number has changed, and continues to synchronize her mail before she goes offline.
It doesn’t take a lot of mental effort to predict that Jack and Jill are in for a surprise.
The most important thing to take away from this example is that the only way to prevent this scenario is to stop the world as soon as a passenger starts seat selection.
But how long are you prepared to wait for that passenger to check the seating arrangement before you allow other passengers to get on? Having a globally consistent view of the world is commonly too impractical to work.
Are use cases like the story of Jill and Jack impossible to accommodate? Surely not! Anything is possible; you just need to find an effective way to deal with conflicting changes.
If the program could figure out that Jill and Jack want to sit next to each other, it could warn them if their effort failed.
For example, after Jill submits her change, the application could show an alert warning her that she’s moving away from her companion.
This will come as a surprise to Jill; in her reality, she was moving toward Bill, not away from him.
A properly chosen alert message would make sense to Jill.
The scenario just described is a form of eventual consistency achieved by compensating transactions.
After the transaction is committed, stakeholders are invited to make compensating changes based on the new reality.
What if Jill and Jack didn’t change their seats but asked to be seated together? Then the scenario would play out in one go.
Jack asks to be seated next to Jill first and is moved.
Then Jill asks to be seated next to Jack, and the server knows that no move is needed.
The concession here is that Jill and Jack no longer control their locations.
Allowing them to do so would require a much more complex application and, most important, would make the application more difficult to use.
If you take a trivially simple implementation of this system, you can make it work as long as you’re prepared to send many requests.
Let’s say the server understands “Please move me next to my companion,” and it blindly executes this directive without any transaction.
In this case, conflicting changes such as in the example might occur, but if you keep sending the message until you see the right result as a client, the system also implements eventual consistency.
Eventual consistency is guaranteed through a redelivery policy combined with an idempotent receiver.
The two flavors of eventual consistency are both ultimately based on their not having to support a rollback.
If you can get by without them, you can loosen the transactional requirements to allow massive improvements in scalability.
If you understand the reasoning behind relaxing ACID constraints, it becomes even more important to understand where you need to replace transactional boundaries with eventual consistency strategies.
If you miss a beat here, you’ll end up with lost updates, dirty reads, and all their two-faced friends and relatives.
There’s no excuse for being clueless about transactions, remember? Let’s look at the details of transactions around endpoints.
Remember that the transaction boundary is broken as soon as you add a task executor or a queue to a channel.
In this section, we look at the beginning and end of the transaction from within the endpoint.
As we learned in the first section of this chapter, there are many different types of endpoints.
As much as possible, we’ve aimed to maintain transactions within the endpoint as a rule.
This makes it easier for you, as the user, to identify transaction boundaries on channels.
It also allows you to extend a transaction over multiple endpoints by using the right channel in between.
A transaction is started by a poller just before it pulls a message from a MessageSource.
To be precise, the poller starts a transaction only if it was configured with a transaction manager.
The transaction is then committed as soon as the send method of the channel the poller is sending to returns.
If the MessageSource is transactional, it participates in the same transaction as the downstream endpoints as long as the boundary isn’t broken by an asynchronous handoff.
Similarly, a poller equipped with a transaction manager starts a transaction before pulling a message from a QueueChannel.
The business transaction in the downstream endpoint is wrapped by a transaction that includes the reception of the message from the channel.
It also includes sending the message to the output channel.
Figure 4.3 shows the two standard transaction scopes to keep in mind when designing a system.
When letting a poller take care of the transaction, as shown in the figure, transaction management is simple.
The only thing left to keep an eye on is endpoints that can break transactional boundaries around your message.
For example, when you use an aggregator, the messages are stored in the endpoint until they’re complete or a timeout occurs.
This means the message going in doesn’t have to continue on the same thread, so it usually doesn’t participate in the same transaction.
Transaction management around an endpoint isn’t always related to the poller that invokes it.
If a different thread is involved, the transactional context doesn’t typically propagate to that thread.
In general, this is the case whenever an endpoint can be configured with a task executor.
Although this chapter is relatively technical in nature, much of the discussion has been theoretical.
Even when discussing concerns specific to Spring Integration, the examples have been mostly at the namespace-based configuration level.
In the next section, we again take a look under the hood so that you can learn more about the implementation details of message endpoints, polling consumers, and event-driven consumers.
You’ll have a better understanding of the types of components that are created by the namespace parser.
If you’re not interested in that level of detail, feel free to skip ahead to the summary that follows.
Early in this chapter, you read that Spring Integration can take care of polling concerns for you.
We haven’t explained in detail how this works, and in daily life you.
Figure 4.3 Transaction boundaries are determined by the scope of a given thread’s responsibility, so the transactional context doesn’t propagate across an asynchronous channel.
If ever you find yourself debugging a Spring Integration application, though, you might benefit from knowing how this works under the hood.
Things will get technical now, and if you feel you’ve seen enough at any point, feel free to skip this section.
If you’re not afraid to look inside, this section is for you.
The base class for all endpoints is AbstractEndpoint, and several subtypes exist to take care of the differences between polling and event-driven behavior.
PollingConsumer wraps a MessageHandler and decorates it with a poller so that it can be connected to any PollableChannel.
Albeit more complex, this is still nothing more than a wrapper around a MessageHandler.
Things become more interesting when we start looking into what happens during the parsing of the application context where the decisions between polling and eventdriven behavior are made.
Each XML namespace supported by Spring Integration and the other Spring projects is typically described as a domain-specific language (DSL)
The domain model consists of messages, channels, transformers, routers, and so on.
The advantage is that the elements in the namespace are closer to the concepts that we’re working with than is, say, a simple Spring bean element.
You can always drop down to a lower level and configure everything as simple beans as long as you’re familiar with the API.
But considering most Spring Integration users rely largely on the schemas defined in the various namespaces, we include the role of parsers in our discussion.
Let’s begin with the following configuration of two transformers and two channels:
The first thing to notice is that channel1 has no queue.
The channel2 element leads to the creation of an instance that.
Those messages must then be explicitly received by some active poller.
Let’s now walk through the result of parsing the associated endpoints.
Each has a trivial expression to evaluate, and for purposes of this example, we can assume that the messages will all have a string typed payload.
Its primary role is to connect MessageHandler objects to the correct input channels, as defined in the configuration.
At parsing time, a limited amount of information is known.
The endpoint parser doesn’t know yet whether that’s a pollable or subscribable channel.
As a result, the parser can’t know whether to create a polling or event-driven consumer.
To handle that limitation, Spring Integration’s parsers rely on another common Spring feature: the FactoryBean interface.
Any object defined in a Spring context which implements that interface is treated differently than other objects.
Instead of instantiating the object and adding it to the context, when Spring encounters an object that implements the FactoryBean interface, it creates that factory and then invokes its getObject() method to create the object that should be added directly to the context.
One advantage of that technique is that much more information is available when getObject() is called than is available at parsing time.
This is how Spring Integration’s endpoint parsers avoid the limitation mentioned previously.
As its name suggests, it’s a generic creator of consumer endpoints.
Because the input channel instance referenced by name in the configuration is available at the time the factory is invoked, the decision can be made on demand.
This distinction becomes clear when we investigate the constructors for each of these objects.
Both take a MessageHandler that handles the messages sent by a producer to.
Under the hood the input channel, but the interface of the expected channel is different.
We’ve mentioned several times that the primary responsibility of these consumer endpoints is to connect the MessageHandler to an input channel, but what does that mean? It has to do with the lifecycle management of these components.
We end our under-thehood exploration with a quick investigation of what lifecycle management means in the context of these two consumer types.
The foundation for managing the lifecycle of any components in Spring Integration is the Spring Framework.
As you can see, methods are available for starting and stopping a component.
Likewise, the unsubscribe method is called from within the stop operation.
The start operation of a PollingConsumer schedules the poller task with the trigger that’s set on that consumer.
The trigger implementation may be either a PeriodicTrigger (for fixed delay or fixed rate) or a CronTrigger.
The stop operation of a PollingConsumer cancels the poller task so that it stops running.
You may wonder who’s responsible for calling these lifecycle methods.
Stopping is more straightforward than starting, because when a Spring ApplicationContext is closing, any lifecycle component whose isRunning method returns true is stopped automatically.
When it comes to starting, Spring 3.0 added an extension called SmartLifecycle to the Lifecycle interface.
SmartLifecycle adds a boolean method, isAutoStartup(), to indicate whether startup should be automatic.
It also extends the new Phased interface with its getPhase() method to provide the concept of lifecycle phases so that starting and stopping can be ordered.
Spring Integration consumer endpoints make use of this new interface.
By default, they all return true from the isAutoStartup method.
This chapter dove deeply into Spring Integration internals that you likely won’t need to think about on a regular basis.
The higher-level components such as transformer, router, and channel adapter are the typical focal points.
But the information you learned here provides a strong foundation for understanding how those higher-level components work.
You now know that message endpoints break down into either polling consumers or event-driven consumers.
You also know that regardless of the type of a given consumer, it delegates to a MessageHandler.
The lifecycle of an endpoint consists of managing the connection between the MessageHandler and the MessageChannel where messages are received.
The way that connection is managed (either by a poller or a simple subscription) is the distinguishing factor for the types of consumer implementation.
Now that we’ve explored the low-level details of endpoints in this chapter and of messages and channels in the previous chapter, we’re ready to move to a higher level.
In the chapters that follow, you’ll learn about several components that map to the enterprise integration patterns.
These include routers, splitters, aggregators, and a wide variety of channel adapters.
We begin this journey by considering the role of Spring Integration in the larger context of a real-world application.
Business concerns are usually the most critical part of an application, and that fact motivates Spring Integration and the Spring Framework to promote a clean separation between integration and business concerns.
The goal of these frameworks is to allow developers to focus on implementing the business functionality in their application’s domain without spending excessive time on infrastructure.
The next chapter provides several examples in the flight management domain of our sample application.
You learned about the three key components of the framework: message, message channel, and message endpoint.
Hopefully, you now have a greater appreciation of how those three components promote loose coupling.
Now it’s time to focus on a related concept: separation of concerns.
In this chapter we explore how Spring Integration enables a clean separation of integration concerns from the core business logic of an application.
Separation of concerns is a well-established principle of object-oriented programming and underlies many approaches to software design.
Programming to interfaces enforces a separation of the contract from the implementation.
Building layered architectures enforces a separation based on roles such as presentation and persistence.
Aspect-oriented programming (AOP) enforces a separation of the Getting down to business.
The Spring Framework helps developers achieve a separation from the underlying infrastructure so their applications are portable across different environments without requiring code changes.
Spring Integration follows this trend by supporting a clean separation of integration concerns from the particular business domain of an application.
It provides a noninvasive framework for supporting message-driven interactions on top of a business service layer.
The book so far has focused on the integration concerns.
You explored the construction of messages, different types of message channels, and the behavior of both polling and event-driven consumers.
You saw a few examples of how these channels are connected to services.
By the end of this chapter, you should have a thorough understanding of how to transform data between the representations used by external systems and the core domain model of an application.
In many cases, such transformation is applied to the format or structure of the data, but in some cases, the content needs to be enriched or headers need to be added to a message.
You’ll learn how to connect the business services of an application to the messaging components provided by the Spring Integration framework.
This is where the Spring programming model is most apparent.
There is a clear emphasis on plain old Java objects (POJOs) and Inversion of Control (IoC)
You’ll also learn how to chain endpoints together to invoke several different business services within a messagedriven flow.
Finally, you’ll learn about some AOP capabilities provided by Spring Integration.
One such capability is the noninvasive interception of application methods to send messages.
Another is the framework’s ability to dynamically generate proxy implementations of an application interface so that invoking a method will send a message.
When building an object-oriented application, one of the most important steps is to design a data model that accurately reflects the business domain of that application.
But when integrating enterprise systems, data may be represented in a wide variety of formats.
This section covers the ways such data representations can be transformed to and from the domain-driven object model used in the application.
A common requirement for enterprise integration is to transform inbound data into the format expected by the consumer of that data.
Syntactic transformations like converting a string representation of XML into a Document Object Model (DOM) instance can be handled by a generic transformer that builds on existing libraries.
Many structural transformations, such as those that rely on XSLT or XPath expressions, can even be handled by generic transformers.
Semantic transformations require focusing on content within the business domain of an application.
Objects from a domain model are often represented to the outside world by the simplest means possible.
Typically that means the external representation includes the least amount of data sufficient to uniquely identify the object.
For example, consider a domain object that maps to a flight.
The flight can be uniquely identified by the flight number and its date.
That content is simple enough to be represented in text or in a more structured form, such as comma-separated value (CSV) or XML.
Figure 5.1 depicts schematically how different transformations can morph the Flight domain object into different interoperable formats.
Any of those formats are portable across any number of environments or programming languages.
That’s a good thing because interoperability is a fundamental concern for enterprise integration.
Another benefit of this approach is that it minimizes the amount of data sent over the wire, which means exchanging this data won’t have an unnecessary impact on bandwidth or performance.
So far we’ve talked about transforming to a portable format, a process often called marshalling.
We also need to transform the portable format back to a domain object, which is fittingly called unmarshalling.
Imagine that the ultimate consumer of the flight data is a service that expects a fullfledged Java representation of the flight as defined within the domain model of the application:
Not only do you need the ability to transform domain objects into intermediate formats, you also need to transform in the other direction.
Typically transformations such as the ones shown in figure 5.1 implement both marshalling and unmarshalling functionality.
The definition of a Flight class provides the various properties you’d expect: the flight number, the origin and destination, the scheduled departure time, the passenger seat assignments, and even a reference to the crew.
Everything you see in the code example is related directly to the business needs of the application.
There are no integration concerns mixed in with the domain model, or vice versa.
As emphasized in the introduction to this chapter, the main goal is to enforce a clean separation between the business domain and the integration details.
One way that such separation of concerns is often violated in real-world applications is by combining the details of some particular representation or format of the data directly within the class that defines that data.
For example, consider the following seemingly innocent addition to the Flight class:
That one minor change may seem well justified because, after all, XML is the language of interoperability.
Several different users of the flight domain could benefit from the convenience of this addition.
Now they can easily create an XML representation of the flight data and send it to another system, write it to a file, and so on.
The problem with that approach is that it’s the first step down a slippery slope that leads to tight coupling of the integration concerns within the business domain.
Consider that a fromXml method is an immediately obvious next step, and then a pair of toJson and fromJson methods would be justified on the basis of the XML precedent.
The next thing you know, your domain model is polluted with so many integration details that it’s hard to focus on the business domain.
A much cleaner solution is to isolate the functionality into modular code that is itself easy to test and easy to plug in on an as-needed basis.
Yet others might need different, more specialized formats, such as a file importer that reads and tokenizes flight information from each line of plain text.
By modularizing the XML transformation logic into a component, you can classify it as an integration component providing functionality that is orthogonal to the business domain.
Because transformation is a common need in such applications, it’s covered by the enterprise integration patterns, specifically the Message Translator.
In Spring Integration, the general term for this functionality is a message transformer.
Returning to the XML example with this goal in mind, you could isolate that code to a single module with that XML-to-object transformation being its sole function.
It might be pragmatic to create a single implementation that handles both marshalling to XML and unmarshalling from XML, but within any particular message flow, the transformation in one direction may be depicted by a single component.
For example, figure 5.2 represents the transformation from XML to our Flight object as used in an integration pipeline that begins from some external system, such as a Web Service invocation or Java Message Service (JMS) message listener.
As you can see in figure 5.2, a transformer might be used to unmarshal the object.
This way both the sending application and the receiving service can remain oblivious to the other’s different representation or interpretation of the domain.
This is yet another excellent example of the power of loose coupling.
The XML example captures what is probably the most common approach for representing data in an interoperable way.
But even the XML representation might be overkill for particular use cases.
The external system that sends flight data to your application might not be concerned with or aware of much of that data, and in such cases, it might be possible to use a far simpler representation of the data involved.
As developers, we should always find the simplest solution that effectively and efficiently solves the problem at hand.
Besides, when building enterprise integration systems, it’s generally a good idea to keep the footprint as small as possible to minimize the bandwidth requirements.
Figure 5.2 A transformer that unmarshals a flight from XML to a domain object.
All it knows is that a flight with a certain number is going to be delayed by a certain amount of time.
In such a system, the data representation might be as simple as this:
The event that’s processed by the service might look like this:
You may be tempted to add another method to the interface that accepts this simple format from external systems.
But, as described earlier with regard to the XML transformation, that type of approach leads quickly to bloated and brittle software by violating the principle of loose coupling and becoming difficult to maintain as a result.
An integration framework addresses such concerns so that a clear separation can be maintained between the external representation of the data and the internal domain model.
For one thing, external systems may change frequently and new systems may be added that use different formats.
What you need is a dedicated component that effectively normalizes the data into the canonical format expected by the business service layer:
Figure 5.3 shows how the content enricher collaborates with a repository to convert a simple string to a domain object.
It depends on a FlightScheduler service to look up the Flight domain object for the given flight number.
Assuming you have a well-designed FlightScheduler interface to avoid tightly coupling the caller’s code to the particular implementation, this transformer would benefit from dependency injection.
First, dependency injection makes the transformer code easier to test because you can rely on a stub or mock implementation.
Second, it makes it easy to share the same FlightScheduler instance that needs to be used by other callers, such as those in the business service layer.
You’ll see each of these benefits as you continue to develop the example.
Now that you understand the code, let’s turn to the configuration.
You may have noticed that the constructor is annotated with @Autowired.
That annotation was introduced in version 2.5 of the Spring Framework and provides a directive to the container to inject a dependency that matches the expected type of that argument.
Assuming your application will have only a single instance that implements the.
Figure 5.3 A transformer that uses a repository to enrich a message.
The incoming message is just a string containing the flight number and the delay.
The enricher retrieves the flight from the repository and calculates a date that represents the delay.
FlightScheduler interface at any one time, @Autowired is sufficient for providing that dependency to the transformer.
The @MessageEndpoint annotation at the class level plays a related but slightly different role.
It’s defined by Spring Integration, but it builds on annotation support of the core Spring Framework.
Component’s presence in a class allows that class to be automatically recognized by Spring.
The result is a bean definition being registered with the container just as if its metadata had been provided in XML.
The answer is that Spring extends the role of @Component to other annotations, which are collectively described as stereotypes.
The mechanism that drives this is known as meta-annotations, and it’s a simple technique.
Rather than annotating all such classes with the generic @Component annotation, a more specific one can be used.
Specific annotations, like @MessageEndpoint, can better describe the role of the annotated component, which is why they’re called stereotypes.
In the spirit of test-driven design, let’s quickly build an integration test for the transformer.
In this case, the integration will include an input and output channel and a simple stub implementation of the FlightScheduler.
You don’t need to worry about the actual production version of the FlightScheduler yet; you just want to make sure the transformer is behaving as expected.
Since you’re concerned with calculating flight delays, the most important thing to recognize in this stub implementation is that the scheduledDeparture property is set to the current time.
You need to define a bean for the preceding stub implementation as well as two channels.
You also need to enable the component scanning so the @MessageEndpoint annotation will be recognized on the transformer class:
Now you can write a JUnit test class that sends a Message to the flightDelayInput channel.
That message’s payload should be a string with the expected format.
If all goes well, the delayEvents channel should have a message enqueued whose payload is actually a FlightDelayEvent instance.
The real test is whether that event has an estimatedDeparture property that correctly reflects the delay.
As you can see, the test confirms that the estimated departure time has been adjusted according to the delay event.
The test even provides a few null checks and verifies that the payload type is correct.
Those assertions can be helpful because the exact line of the test failure will indicate what the problem was.
You don’t need to worry too much about the details of this test logic.
The main thing to learn from all of this is how to create a simple integration test for a message endpoint that involves sending and receiving messages across message channels.
In case you’re not familiar with Spring’s integration testing support, we provide a quick overview by describing the test class in listing 5.1
If you want to delve into more detail, refer to the Spring Framework reference manual’s testing chapter.
This is a nice improvement over the JUnit 3 style, which required methods to begin with test.
Another nice change in JUnit 4 is that it’s no longer necessary to extend a TestCase superclass as provided by the framework.
Instead, the default test runner strategy knows how to detect the @Test annotations.
Luckily, it’s still possible to extend the testing framework by providing a customized implementation of the test runner.
This is how Spring provides its integration testing support for a JUnit 4 environment.
That pretty much covers the testing support for this example.
The @Autowired annotation is standard Spring, and the field name is used to drive resolution based on bean names because a resolution based on type alone would lead to ambiguities in this case.
The previous sections explained how to marshal and unmarshal objects to and from different formats.
You also learned how to test these applications using Spring’s test context support.
The requirement to convert a simple unique identifier to a fully populated instance of a domain object is a common one, but message transformers can address many other common requirements.
Let’s take a quick look at a few other types of transformation that rely closely on the code from the particular domain model of an application.
Another common requirement is to add content to an existing domain object by invoking a business service.
For example, for the flight booking use case, you might need to add the email address of a given passenger.
Imagine that you could uniquely identify the passenger on the basis of a frequent flyer program number and also look up the email address from the frequent flyer information service.
An implementation of this logic in a transformer could be considered a content enricher.
The email address is the enriched content being added to the payload of a message that didn’t yet include that information.
In fact, if a user is enrolled in the frequent flyer program, you may be able to add much more information beyond the email address.
The user’s profile may contain preferences, such as meal type, and whether the passenger prefers aisle or window seating.
The information would probably also include mileage credits and status.
The main point is that a Passenger instance may or may not contain all of this data.
The airline doesn’t require enrollment in the frequent flyer program, so a minimal Passenger instance may contain only the person’s name and some form of official identification, such as a passport number, driver’s license number, or tax ID.
The content enricher in the following listing adds the extra information if available but otherwise passes on unaffected Passenger instances.
As with the previous examples, it’d also be easy to unit test without any reliance on the integration framework.
The only dependencies are on the domain model objects and services, so test code wouldn’t even need to include messages, channels, or endpoints.
As time goes on, having such cohesive, dedicated units of functionality proves beneficial to the maintenance effort as well.
If something else needs to be added to the passenger profile, it’ll be clear where the corresponding code change needs to occur.
Likewise, if a new service for profile information is released in the future, it’ll be simple to change this single point of access to such information.
Once again, this reveals the true value of a loosely coupled solution.
The rest of the system would likely adapt to such a change without any rippling side effects.
Business services deal with the payload of a message, and often that payload can be considered a document.
Channel adapters and messaging gateways that connect external systems, by contrast, might be interested in information carried in a message header.
As an example, consider an adapter that sends flight delay notifications to passengers who sign up for such notifications.
Continuing with the example from the previous section, let’s assume the email addresses are available on the payloads, which are instances of the Passenger domain object, because they’ve been added by the content enricher shown in listing 5.2
Basically, passengers enrolled in the frequent flyer program would have the option of providing an email address as part of their profile as well as enabling notifications for a flight delay.
For now, it’s sufficient to know that the email-sending adapter expects the target email address to be provided in a header.
The email adapter could advertise that the header name should be, for instance, mail_to.
It’s then the responsibility of an upstream component to populate that header with the correct value.
Of course, you could implement this header-enriching transformer in code.
All you need to do is receive the payload, which is a Passenger instance in this case, and then grab the email address, if available, so it can be added to the map of headers.
The intention of the code in listing 5.3 should be clear nonetheless.
If the passenger represented by the current message’s payload has an email address on file, it is added to the headers.
That way, some generic email-sending adapter that will eventually receive this message doesn’t need to know how to extract the email address from every possible type of message payload it may receive.
In other words, the adapter specifies a contract: “I will send a mail IF the MailHeaders.TO header contains a valid email address.” If the fulfillment of that contract is the responsibility of a separate component, the mail adapter is highly reusable.
For example, the same adapter could be used for sending email to banking customers in an application designed for that domain.
You might think this is a lot of code to write merely for the benefit of a reusable mail-sending adapter.
That’s a valid point, and it highlights one of the key trade-offs involved in enterprise integration and messaging application design.
We discussed the benefits of loose coupling at length, but what you see here is the initial indication that loose coupling may have some negative consequences.
As with most principles, if taken to extremes, the costs may outweigh the benefits.
If every single line of code were modularized into its own class and implemented its own interface, the result would be the ultimate loosely coupled system.
But testing and maintaining such a system would be prohibitively difficult.
It would probably be much more challenging than testing a tightly coupled implementation where all of the code is in a single class.
Listing 5.3 Header enricher to associate passengers with email addresses.
With that perspective, consider the code in listing 5.3 once more.
And to add fuel to the fire, consider the verbosity of that code in relation to the relatively straightforward goal of the implementation.
These days, there seems to be a growing recognition of the benefits of scripting languages and specifically dynamic or functional variants.
Despite what some may claim, it doesn’t necessarily mean that Java should be avoided altogether.
On the contrary, there seems to be a trend toward using multiple languages together in an application, and the Java Virtual Machine (JVM) provides a great environment for exactly that.
Deciding which language to use for a particular task should be based on the nature of that task.
It so happens that many of these integration tasks are a good fit for dynamic languages.
Spring Integration supports Groovy and other scripting languages alongside Java for the implementation of routers, transformers, splitters, filters, and other components.
For now, let’s consider a related but even simpler option: Spring’s own expression language.
It’s a perfect match for situations like this header enricher.
Believe it or not, this code accomplishes the same thing as the earlier Java code.
Knowing that, you should be able to compare the two implementations and understand exactly how the expression language version works.
At this point, you might be thinking, “Wow! I’ll just use these expressions for everything.” This is a common first reaction, but using the expression language is not always the most sensible thing to do.
Because expressions are stored in strings, and in this case inside the XML configuration, it’s all too easy to “program” in XML.
Logic stored in this manner is much harder to test and refactor.
You should therefore consider using the expression language only for simple, to-the-point expressions.
If you’re a Java programmer, you’re used to type safety and relying on good tools and the compiler to warn you of mistakes.
These warnings won’t be raised when you use the expression language.
Now that you can consider yourself armed (and warned) with knowledge of various types of transformers, including the expression language support, it’s time to really get down to business.
Although transformers may be viewed as an extension to the messaging infrastructure you are building, the main goal of an integration application is most likely to invoke the real business logic.
The next section dives into the details of how to reach that goal with Spring Integration.
In the previous section, you converted data from a simple text representation to a FlightDelayEvent object that’s part of the domain model.
Now let’s look at the implementation of that service and see how to invoke it from the integration layer.
The component responsible for that invocation is a service activator.
Recall that you want to invoke the updateStatus method and that it expects a FlightDelayEvent object.
The return value from that method invocation would be a FlightStatus object.
Therefore, the service activator configuration would look something like this:
If you’ve ever worked with the Spring Framework’s JMS support, and specifically with the message-driven POJO feature, this configuration should look familiar.
One of the main motivations behind Spring Integration was to provide a more generic model for building message-driven systems with the Spring JMS support serving as a precedent.
But instead of a JMS message, it’s a simple Spring Integration message being mapped to the domain object expected by that service.
The source of that message could be any code that sends to that channel or any adapter that’s connected to that channel.
You may also have noticed that the service defines a return value, so a corresponding output-channel is declared on the service activator configuration element.
That output channel might be connected to some external system by a channel adapter, or it might refer to the input channel of another component, such as a router, a splitter, or another service activator.
It’s common for the output of one service activator to refer to the input channel of another service activator, and that approach can be used to connect multiple services into a pipeline or chain.
Figure 5.4 shows three services that are connected in such a chain.
As we discuss later in this chapter, chaining services like this is such a common technique that Spring Integration provides an even simpler way to define such chains without having to explicitly define each channel.
The chained services model works well for linear use cases where a sender on one end triggers the message flow but doesn’t expect a return value.
Message-driven services ultimate receiver at the other end of the chain.
Either the final service to be invoked doesn’t return a value or the chain ends with a channel adapter such as a file writer.
In other use cases, the original sender does expect a reply.
In those cases, a header can be provided on the original message with a reference to a reply channel.
In enterprise integration pattern (EIP) terminology, that header represents the return address.
Here’s an example of a sender that provides such a header and then waits for the response:
Before going any further with this example, we should point out that you’ll probably never write code like this because Spring Integration provides support for the reply channel configuration behind the scenes.
You’ll see an example of this in the upcoming section on the messaging gateway support.
For now, we want to consider the implications of the reply channel header for a service activator.
The key difference would be the absence of an output-channel.
By leaving that out, you force the service activator to store the service’s return value in the payload of a message that it then sends to the channel referenced by the replyChannel header.
A message is deconstructed and reconstructed at each service activator.
The payload is fed into the service, and the return value is used to construct a new message.
The headers of the message remain the same, so the overall context is preserved throughout the pipeline.
The configuration is almost identical to that in the previous version, and more important, no changes are required to the service code.
Switching between a chainedservices and a request-reply interaction model is as simple as including or omitting the output channel from the configuration, respectively.
Keep in mind that if a service does return a value, and neither the output channel has been configured nor the reply channel header provided, an exception would be thrown at runtime because the service activator component wouldn’t know where to send the return value.
If a service does return a value, yet you intentionally want to ignore it, you can provide nullChannel as the output-channel attribute’s value.
Thus far in this chapter, we’ve explored the components that consume messages or, more commonly, just the content of message payloads.
In the context of a business service, these consumers may also return values that are then published as response messages.
The previous section covered the details of how these response messages can be sent to either the endpoint’s output channel or a return address header that’s provided with the incoming message for that purpose.
But integrating a messaging system with an application’s business layer shouldn’t be limited to components that react to messages.
It’s useful to have business services in the application publish messages based on certain events that occur within the business services themselves.
A typical example is sending a message each time a particular service is invoked.
The payload of that message might be the return value of the invocation, it might be an argument that was passed with the invocation, or it might be some combination.
In any case, Spring Integration provides support for this by applying Spring AOP.
A proxy can be generated to intercept method invocations on services that have no direct awareness of the messaging system.
The proxy handles the responsibility of creating the message from the invocation context and publishing that message to a channel.
Imagine that your system needs to perform some auditing every time a flight’s status is updated.
Perhaps you’re responsible for gathering these statistics to provide data about the percentage of flights that actually depart on schedule.
By now, if we’ve accomplished our goals in the book so far, you’re probably having a Pavlovian response already.
Clearly, this is another violation of the principles of loose coupling and separation of concerns.
With listing 5.4, even testing the core business functionality of the status update service would require awareness of the message publishing behavior as well.
Fortunately, Spring Integration provides a cleaner way to handle such requirements.
As with any application of AOP, the goal is to modularize the functionality so you don’t mix cross-cutting concerns, such as messaging, with the core business logic.
The key is to apply the noninvasive technique of interception.
Also, as with many of the other Spring solutions that rely on AOP, such as transactions, this behavior can be enabled by using a simple annotation:
As you can see, this is much better because the code is focused only on the business logic.
There’s no need to worry about the cross-cutting messaging concerns when testing or maintaining the code.
Likewise, the message publishing code can be tested independently of the business logic.
In other words, it’s a cleaner solution all around because it separates the integration and business concerns.
The noninvasive interceptor-based approach described in the previous section is a powerful yet simple way to send messages based on actions that occur within the business service layer.
But interception is only really useful for reactive use cases in which the publication of a message is considered a by-product of some other primary action.
There are other equally valid use cases in which publishing a message is itself the primary action, and for those cases, a more proactive technique fits well.
Such use cases correspond to the traditional event-driven programming model.
Suppose you want to send a general notification that a flight status has been updated.
One way to accomplish this is to publish the event to each interested party.
The following listing demonstrates a class that accomplishes this in an intuitive but less-than-ideal way.
Why is this example less than ideal? There are a number of reasons.
First, consider what you’d need to do if you encountered a new requirement to add another destination for these events, such as the local filesystem.
The revised implementation might look something like the following listing.
Listing 5.6 demonstrates the increased complexity as you try to meet the demands of too many requirements within a single component.
With three different notification targets being handled by a single method invocation, it would be difficult to completely isolate each one in a test.
That’s a pretty good indicator that you’re violating the separation of concerns principle.
We left out the full implementation details of the writeFile method but purposefully provided three comments to describe what such an implementation entails.
As you’ll see in chapter 11, each step can even be addressed by separate components or strategies when using Spring Integration’s file adapters.
As for mail sending and JMS publishing, the preceding examples assume use of the corresponding support in Spring.
In both cases, there is still a dependency on the mailSender and jmsTemplate objects in the code.
As you’ll learn later in the book, Spring Integration builds on such support classes provided by the underlying Spring Framework, but with its more generic messaging model, it allows for an even greater degree of separation of concerns.
For now, don’t worry about the details of these channel adapters.
As we said, you’ll have plenty of exposure in upcoming chapters.
Even without knowing any details about the mail, JMS, and file adapters, this configuration should be self-explanatory.
The main idea is that multiple adapters are subscribed to a channel, and as a result, the producer of flight status notifications doesn’t need to know anything about the individual subscribers.
If you need to add or remove subscribers, it’s a matter of inserting or deleting the corresponding element.
As far as transformation is concerned, it’s also treated as a separate concern.
If you need to transform to different types for each of the adapters, you can add transformers after the common channel instead.
And, of course, if you have custom transformation requirements, it’s just as easy to provide a reference to your own transformer implementation.
You’ll learn more about each of these adapters and their associated transformers in upcoming chapters.
The one element we do want to discuss in some detail here is the gateway.
As you can see, it declares the fully qualified name of an interface.
If you’ve ever worked with Spring’s support for remoting via Remote Method Invocation (RMI) or Spring’s own HTTP invoker mechanism, this concept will be familiar.
The element triggers the registration of a Spring FactoryBean that produces a dynamic implementation of the declared interface.
In this case, the proxy is backed by the supporting code in Spring Integration that maps the arguments to a message and then publishes that message to the referenced channel.
Earlier we discussed the testing aspects of the non-ideal implementation as an indicator that the code was fragile.
With that implementation, it was difficult to isolate the various notification subscribers’ code for testing purposes.
For one thing, relying on the framework considerably reduces the amount of in-house code to be tested.
In fact, the only code from the previous example that isn’t part of the framework is the interface.
This means that all responsibility for testing the internal functionality of the subscription side belongs to the Spring Integration development team.
You’d likely want to have some level of integration testing to verify that the mail sender, JMS queue, and filesystem directory are configured properly.
In terms of unit testing, though, the focus can shift to the publishing side.
Because that’s now reduced to the invocation of a method on an interface, it should be easy to build a battery of tests using traditional mock or stub techniques.
In many cases, messages flow linearly through the system from endpoint to endpoint, separated by direct channels.
In these cases, configuration can be unnecessarily verbose if users are required to create channels between each of the endpoints explicitly.
Figure 5.5 Endpoints can be chained together with channels implicitly created using a chain element.
As discussed earlier, synchronous channels are used by default to preserve transactional boundaries.
In a chain, the synchronous channels needn’t be specified, but there’s no way to override the type of channel used.
The argument to do things this way is to improve the readability of the configuration files by reducing repetition but not sacrificing the option of a more explicit configuration where needed.
You’ve seen several examples in this chapter of a single component performing one focused, well-defined task.
This is a good thing from the perspective of modular design, but even loose coupling and separation of concerns can be taken to extremes.
It has many channels that are used only once and many endpoints configured to connect to those channels.
Even if you rely on Spring Integration’s creation of default channels for any single component’s input-channel attribute, or for the id attribute of a channel adapter, the configuration is still noisy:
This configuration allows you to remove the channel elements, but it might be even harder to follow when reading the configuration directly.
Now you have to look at the various input-channel values that match with output-channel values.
If the endpoints were listed out of order, it would be confusing.
Whenever default channels are being referenced by only one subscribing endpoint, and the flow of messages across multiple endpoints is a simple linear arrangement, consider the simplification provided by the chain element (see figure 5.6).1
When creating a chain, there are a few things to consider regarding the position of endpoints.
If an endpoint isn’t the last entry in a chain, then it must accept an output-channel.
Those endpoints that would be playing the role of a terminating endpoint must be placed in the last position.
It couldn’t be placed in the middle of the chain because any endpoint added after it would never be invoked.
Likewise, if adding a router to a chain, it must be in the final position.
Considering that the role of a router is to determine the next channel to which a message should be sent, it wouldn’t fit in any other position within a chain because all endpoints before the last one essentially have a fixed output channel.
In this chapter, we explored a number of ways Spring Integration may interact with components that are part of a business domain.
This configuration can be used with Spring Integration 2.2 and higher.
If using an earlier version, the outbound channel adapter must be declared outside of the chain.
On the contrary, the service objects can be POJOs because Spring Integration provides the necessary adapters to connect those POJOs to message channels.
Those adapters also assume the responsibility of mapping between messages and domain objects.
This is why we describe it as a noninvasive programming model: the methods to be adapted on the POJOs don’t need to operate with messages.
Instead, those methods can expect domain objects as method parameters and can likewise return domain objects when invoked.
One of the benefits of this model is that preexisting business services may be connected to the messaging system with little effort.
As a result, Spring Integration is easy to adopt incrementally.
Even more important, whether connecting to preexisting services or implementing a completely greenfield application, this model makes it easy to enforce a clean separation of concerns between the business logic and the integration components.
The messaging system forms a layer above the business services, and those services are easy to implement, test, and maintain without having to take any messaging concerns into account.
The message-driven interactions with a business domain featured in this chapter are all relatively simple.
Even when multiple steps are necessary, such as transforming content before invoking a service, they’re in the form of linear pipelines.
You learned how Spring Integration provides support for such a pipeline with the XML configuration of a handler chain.
But sometimes the interaction with the business layer isn’t so straightforward.
One of the most common requirements is to add some decision logic to determine where a particular message should go.
That decision may be based on some content within the message’s payload, or it may be based on a header value that’s been added to the message.
In the next chapter, we explore Spring Integration’s support for addressing such requirements with components responsible for routing and filtering messages.
Continuing the theme of this chapter, we’ll see how Spring Integration promotes the separation of concerns principle.
Business concerns about delivering work to certain services are decoupled from the processing that happens in those services.
Earlier chapters showed you how to use Spring Integration to create an application from a group of processing units.
You learned to choose the format of the messages that are exchanged between components, and to define channels to which messages are published and thus propagated through endpoints to the message handlers that process them.
But there’s more to Spring Integration than this basic model.
Sure, building an application like this is a simple and efficient (and therefore great) way to reduce coupling.
You can also control the responsiveness of the system by adopting an asynchronous model.
But this chapter introduces another form of control that goes beyond the Go beyond sequential processing: routing and filtering.
Do you want to get this message? conveyor-belt model of sequential processing: it explains how to selectively process messages and define alternative routes within the system.
First, you’ll see how you can limit the scope of what your components will handle by using filters that allow only certain messages to pass through to be processed.
When your application provides alternative (or complementary) processing options for your messages, you can use routers to let the system choose the actual destination of a message, thus freeing message producers from the decision process.
This technique allows more flexibility in the system because your components become more reusable.
This means the service is invoked for every message that arrives on the inbound channel.
It’s possible that a given consumer isn’t interested in all the messages it receives.
For example, in a publish-subscribe scenario, different consumers may have different interests in the incoming messages even if they’re all potential recipients.
To enable the selective consumption of messages, you can use a special type of message handler, a message filter.
As you can see in figure 6.1, the filter is a message handler that evaluates messages without transforming them and publishes back to an output channel only the ones that satisfy a given rule.
Of course, the decision of whether a certain component can process certain types of messages can be made by the component itself.
For example, the plain old Java object (POJO) implementation of a service activator can decide whether messages are valid.
Embedding this decision into the component works well if the decision is based on a broad general principle that’s applicable everywhere that component is used.
The requests are forwarded to a cancellation processing service, and further to a notification service, which sends out confirmation emails.
Cancellations for different types of reservations may need to be processed differently because the conditions may be different (refund policies, advance notice requirements, and so on)
The example cancellations processor knows only how to process Gold cancellations, so you can discard everything else, as in the following example:
Only A’s are published to the next channel, whereas B’s and C’s are discarded.
Note the <filter/> element used for inserting the filter between the gateway and the service activator that processes the cancellation requests.
From a syntax perspective, its definition isn’t very different from that of other message handlers discussed so far, such as the transformer and the service activator.
It has an input channel for handling incoming messages, an output channel for forwarding messages, and delegates to a Spring bean for the actual processing logic.
Messages don’t suffer any transformation when they pass through this component; the same message instance that arrives on the input channel is forwarded to the validated channel if it passes the filtering test.
Do you want to get this message? In our application, a cancellation request that can be processed by the service must contain a reservation code that corresponds to a Gold reservation.
Of course, this tells nothing about whether a reservation with that code exists, and we won’t try to do all the validation at this point (many things can be wrong with the request itself, and dealing with such errors is the responsibility of the cancellation service)
But, at a minimum, you know that reservation codes have to conform to a certain standard pattern, and you can do a quick test for that.
As with the service activator and the transformer, you can implement the logic in a POJO (and we strongly recommend you do so)
The filtering logic consists of a method that takes as argument the payload of a message (as in the previous example), one or more message headers (using the @Header/@Headers annotation), or even a full-fledged message, and returns a boolean indicating whether the message will pass through.
In the simplest case, which is also the default, messages are just discarded (or, for a UNIX-based analogy, /dev/null)
If you don’t want them to be discarded, you have two other options:
In this case, rejection is more a form of redirection, allowing the application to handle them further as regular messages.
From the point of view of the framework, they’re still messages and therefore subject to any handling a message on a channel can undergo.
You can instruct the framework to throw an exception whenever a message is rejected.
The framework allows both options to be enabled at the same time, but from a practical perspective, they’re mutually exclusive.
Nevertheless, when both options are active, the message is sent on the discarded messages channel before the exception is actually thrown (an important detail when a synchronous channel strategy is in place)
To illustrate, here’s a more elaborate variant of the previous example, where rejected cancellation requests are redirected on a specific channel and from there are forwarded to an outbound notification system.
Assuming the cancellation request contains enough information about the requester itself, so that a reply message can be sent, you can implement the filter this way:
Or if you want to throw an exception instead, you can write this:
As you’ve seen, it’s simple enough to implement the filtering logic as a POJO.
But in a lot of common cases, you don’t even need to do that.
If all the information is to be found in the message itself, and all you need is to write a logical expression that’s computed against the payload or the header values, you can use the Spring 3.0 Expression Language (SpEL) directly.
This way, there’s no need for a distinct bean to implement the decision logic.
The advantage of using this filter definition is that the filtering logic can quickly be viewed in the context of the message flow.
The disadvantage of using SpEL expressions directly is that they’re harder to test in isolation from the filter itself, so you should take care to call out to the proper abstractions if the logic gets complicated.
Our recommendation for making a decision in this case is to use expressions whenever the condition is based on the attributes of the message itself.
In more complicated cases, when the decision involves a sophisticated algorithm, or the sender must consult with other collaborating components, you may want to create a standalone implementation and take advantage of Spring’s dependency injection capabilities.
If you really want to have your cake and eat it too, there’s a hybrid solution: use a SpEL expression, but delegate (part of) the logic to a Java object.
One powerful and simple way to do this is to delegate the decision to the message payload or a header.
Weighing the pros and cons of each option is something you’ll have to do again for each situation.
The framework will support whatever decision you make in the end.
Now that you have some tools, let’s look at some uses for them.
Let’s get a bit of perspective here: how do messages that don’t satisfy the criteria to be processed get on the inbound channel, anyway? Wouldn’t it be simpler if their producers didn’t bother to send messages that won’t be processed?
Often, filters are used in combination with publish-subscribe channels, allowing multiple consumers with different interests to subscribe to a single source of information and to process only the items they’re really interested in.
Such a solution doesn’t preclude multiple components receiving a message at the same time, but the components have complete control over what they can and can’t process.
Three different components with different interests subscribe to the same channel.
It’s much easier if a producer knows only about a single destination channel for its messages.
Figure 6.2 A publish-subscribe channel and filters combination for selective processing.
Message filters can decide whether a message will be forwarded to a next channel.
If you decide to configure them with a channel for discarding messages, message  filters will act as switches, choosing one channel or another depending on whether the message is accepted or discarded.
In that case, they’re a simplified form of a more general type of component that can decide from among multiple destinations where a message should go next.
This component is the message router, which is the focus of the second part of the chapter.
Channels are also, by design, the only components of the framework that are directly referenced either by other components or by external modules of the application.
Decoupling producers from consumers means, in this context, that the only information available to a message producer is a reference (or name) to the channel on which it should send a message, whereas the actual destination of the message and its processing sequence are determined by the configuration.
For a message to be processed correctly, it should be sent on the correct channel.
As systems become more complex, determining which is the appropriate next channel becomes significantly more complicated.
This knowledge is application specific, so it’s a good idea to isolate it from the rest of the application.
The solution is a new component—the router—whose role is to choose a next target channel for a message and publish it there.
Why wouldn’t components just send each message directly to the appropriate channel, instead of publishing them to an intermediate channel and deferring the routing decision to yet another component? The main benefit of using a message router is that the decision logic is encapsulated in a single component.
The message publishing components needn’t know anything about what follows downstream; the only information they require is which channel they should publish to.
Spring Integration provides an infrastructure for configuring message routing in your application.
You can use either one of the routers provided out of the box and use the namespace configuration feature, or you can implement your own routers.
The aphorism we just quoted has a second part, which is usually forgotten: “but that usually will create another problem.” When using a router, its configuration must be aware of all the possible destination channels so it can make correct decisions.
This may create another problem, as the saying goes, because the configuration of the router must be updated every time the routing logic changes.
In practice, centralizing the configuration in a single place is generally a better choice than spreading the knowledge across the system.
If a more dynamic approach is necessary, you can fall back on the alternative of using publish-subscribe channels and filters.
The quote belongs to Butler Lampson, who in turn attributed it to David Wheeler.
But many payment methods are available to a customer: online payment by credit card, popular services like PayPal, interbank networks, and so on.
Even separate invoicing may be an option to trusted customers.
Each payment method requires some specific information (such as an account number, a billing address) and performs a different operation.
Before a reservation is secured, the system must process a payment, which is initiated by the user.
The payment process by itself is relatively straightforward: after the user decides on the form of payment, a message is sent to a payment channel, where it is picked up for processing by the system, which completes the booking process.
The scenario works equally well if, for example, the user decides to save the reservation and pay for it through online banking.
In this case, the payment notification isn’t sent by the web application but by an external process (such as a nightly batch processing transaction)
The individual processing strategies are mutually exclusive alternatives, and it makes sense to provide an individual channel for each of them to ensure that each payment notification is forwarded to the appropriate payment processor.
Individual channels make it possible for any module (like the web UI) or a batch process to trigger payments of different types by creating messages with appropriate payloads and sending them on the appropriate channels.
In this case, rather than giving each component access to the complete list of channels and having them decide where to post the next message, it makes more sense to introduce a router.
With a router, all payments are sent to a single channel and from there are processed by the router, which then forwards them to the appropriate target destinations.
The method returns the name of the channel on which the message will be forwarded next, and if it returns null, the message won’t be forwarded further.
A service activator can provide a number of overloaded methods that take the different types as arguments.
Using this class, you can configure a service activator to process the messages:
Whose message is this, anyway? Spring Integration dynamically invokes the overloaded method that accepts an argument of the type of the payload of the message.
You don’t need a router as long as the right methods are compiled on the service activator class.
How does this compare with introducing a router? To answer this question, we must remember our primary goals: low coupling and easy extension of the application.
The PaymentManager is a passable solution, but it works well only if the number of options is fixed and known in advance.
If you need to add another payment option, you introduce a new payment type and a new method to handle it.
As an example, let’s assume you want to handle DirectDebitPayment as well.
In this case, you have to add a new method to the PaymentManager:
By comparison, the router-based method allows you to expand the application without modifying anything that exists already (besides the application configuration)
And, if you’re using the most basic router, shown earlier, you have to modify the routing logic accordingly:
If all you do is trade one class change for another, what’s the gain here? For one thing, the routing logic is just a thin layer that has no dependency or direct interaction with.
One of the advantages of using routing is that you can get away without implementing a new router class, as you can see from the first example in this section.
Spring Integration comes with a wide variety of routers out of the box, the topic of the next section.
The router in the example decided where to send messages according to the payment option selected by the user, which constitutes the payload type of the payment message.
Using the terminology of Enterprise Integration Patterns, all the routers provided by the framework are content-based routers.
This means the decision on where to send the message next is based solely on the contents of the message.
The PaymentSettlement router you saw earlier is an example of using a default router.
The simplest router is created by defining a <router/> element.
When you define a router, bear in mind the following:
It must describe how the routing decision is made—it must indicate who is responsible for evaluating the message and determining the next channel.
The decision may come in the form of a channel or channel name; in the latter case, the channel names must be converted to channel instances.
When it comes to making a decision, there are two possible variants: either delegate to a method defined on a POJO, or use a SpEL expression.
The previous router example showed how to implement a router using a POJO.
To be usable for the routing logic, a method definition’s arguments must comply with the same general requirements as business services and service activators: either take as argument a message or take as argument an object representing the payload and/or a number of @Header-annotated parameters.
The return type of the method must be a message channel, a string representing a message channel name, a collection of strings or message channels, or an array of strings or message channels.
The latter are supported because routers can return multiple values, and the next section provides an example of how that works.
If the method returns channel names, the names must be converted into channel instances.
If you don’t want to provide one, the framework, by default, will provide one that looks up channels by their IDs in the application context.
Deciding whether or not to use an explicitly configured channel resolver largely depends on how keen you are on using channel names inside your routing logic.
Whose message is this, anyway? You can always resort to this option for configuring a router, but there are other options that don’t require creating a new implementation every time you want to configure a router.
The framework provides implementations that cover certain common use cases.
The router you implemented for PaymentSettlement instances takes into account the type of the object when deciding the next target channel.
In your router implementation, you checked that yourself, but instead of doing that, you could’ve used an implementation provided by the framework.
If the routing information (for example, the target channel) can be found in one of the headers of the message, you can use a header value router to simplify the configuration:
The general idea behind routing based on header values is that the routing information can’t be easily found in the message itself, but is added to the message earlier in the message flow.
A header value router pairs well with a header enricher to populate the relevant header of the message:
Payload-type routers and header value routers are simplifications that cover particular use cases.
Another way to set up a router without writing a new class is to use SpEL expressions.
If the routing decision can be made through a simple evaluation of the message instead of by creating a separate POJO implementation and delegating to it, you can embed a SpEL expression into the router.
This works well when the outcome of the expression evaluation matches the names of the target channels.
But different credit cards may be processed differently because they’re handled by different issuing organizations.
You can extend the routing logic by adding another router to deal with credit card payments specifically, as follows:
This example doesn’t use a separate class to implement the routing logic.
In this case, you evaluate the creditCardType property of the payload (which is of the type CreditCardPayment)
The outcome of the evaluation may be VISA, AMERICAN_EXPRESS, or MASTERCARD, which are also the names of the potential channels.
In all the examples so far, the routing logic returns the name of the next destination channel.
In most situations, this works well, but relying on the channel names to be fixed may prove to be a problem in the long run.
You can introduce another level of indirection by allowing mapping the string values returned by the routing logic to channels from the configuration.
A strategy for developing more reusable application components is not to rely on your routing logic to return channel names but instead to use placeholder values that convey the logical significance of the routing process (“Whenever the payment will be settled through VISA credit card payment, send it to the channel for VISA payments, whichever name it has.”)
The rationale for its removal is that the ability to specify a SpEL expression in a router provides more than enough flexibility.
For example, it would be trivial to provide an expression that appends a suffix.
Whose message is this, anyway? In this case, the router can be supplied with a ChannelResolver that will take care of translating the string value provided by the routing logic into an actual channel instance:
In the previous example, the names of the channels had to match the potential results of evaluating the routing expression, but introducing the ChannelResolver gives you more liberty in defining your configuration.
Adding a suffix may not seem a significant change to the previous case, but the destination channels have different names than the result of evaluating the expression, and it is the role of the ChannelResolver to bridge the gap.
All the examples so far are based on the assumption that the routing scenario implies a single next destination channel.
But routing can also enable a message to be resent to a number of other channels, as discussed next.
Our payment example showed how to use a router when you have one possible destination out of a few.
This is the most common situation you’ll encounter, but it’s also possible to have more than one next destination channel for a message.
Let’s consider a notification system that has different ways of notifying customers and different notification types.
A simple weather update may be sent through email, but more urgent notifications, such as a cancellation notice, may require sending a Short Message Service (SMS) text, an email, and placing an automated phone call at the same time to make sure the customer is reached by all means possible.
Because there is a single source of notifications, the notifications are sent on a single channel.
From there, a router distributes them to the channels that correspond to the individual notification strategies: SMS, email, phone.
The difference between this notification scenario and the payment scenario is that a notification can be sent by multiple channels simultaneously.
The intent is to contact the client in as many ways as the business logic requires.
Routing logic must take into account both the notification settings of the user and the urgency of the notification.
Receiving an automated phone call in the event of a cancellation is pretty sensible, but being called every time there’s a sale doesn’t fall into the same category.
Both are notifications, and will be delivered initially on the notifications channel, where they’ll be distributed by the router:
In this case, the router sends messages to all the channels returned by the method, and please note that the resulting combination can vary from case to case (both the priority and the user settings are factors to be considered)
If the only job for the router is to dispatch messages to a set of fixed channels, the application can use a Recipient List Router.
A recipient list router is slightly different from the routers you’ve seen so far that make decisions about the next destination channel.
Recipient list routers are configured with a group of destination channels and will forward incoming messages to all of them.
You may wonder what such a router is good for.
After all, publishing to multiple destinations is the job of a publish-subscribe channel.
This may be true when you’re designing a system from scratch, but the recipient list router works well when the destination.
Under the hood channels are already defined and the target components are already listening to them.
Another possible use is for broadcasting a message to multiple channel adapters:
With such a configuration, any message sent to the notifications channel is automatically forwarded to all the channels defined on the recipient list of the recipient list router.
This overview of the message routers provided out of the box by the framework wraps up the second part of this chapter.
Now you know pretty much all that you can do for filtering and routing by using the namespace configuration.
If you’re interested in learning more about the classes used to implement these features and how they interact, you’ll find more information in the next section, where we lift the hood and take a peek at the internal parts of the framework.
The most common way to use filters and routers is through the namespace configuration.
This section dives deeper into details and explains what happens underneath.
It’s optional reading, because knowing the internals of the framework isn’t a prerequisite for using it efficiently.
But there are two valid reasons to investigate what happens inside the framework and what main actors are at play.
Second, the framework provides a few out-of-the-box components that treat the most common use cases and allow you to create problem-specific implementations.
Expanding the framework in such a way requires some detailed knowledge about the collaborating classes and the APIs provided by the framework, and this is what we focus on next.
The extension point of the message filter API is the MessageSelector.
It’s a method object that computes a boolean value for a given message.
The main actor is the MessageFilter class, which implements all the filtering process except for decision making, which is left to the MessageSelector that’s injected into the MessageFilter.
The router invokes the method and retrieves a collection of MessageChannels.
The list is iterated and the message is sent to every channel from the collection.
A flag that forces the router to throw an exception if no next destination channel can be determined (that is, if a resolution is required)3
Figure 6.4 Sequence diagram of the MessageFilter: a MessageSelector’s accept() method evaluates the message to decide if it should be sent further.
All the namespace elements used to define a router allow for setting these options through XML attributes.
In this chapter we moved past the simple sequential model of chaining message handlers and started to deal with more complex configuration problems.
You saw how to set up your application to process messages selectively by filtering out the ones that a given component is not supposed to handle.
Filters can be used to perform message validation (especially when validity isn’t a domain-inherent characteristic but can vary among applications)
More than simple validation, this functionality is a powerful complement to publish-subscribe channels, allowing consumers with different interests to subscribe to the same channel but guaranteeing that they’ll receive only the messages they’re supposed to handle.
To handle a message correctly, a producer must send it to the appropriate channel—a channel that has the intended recipient as a consumer.
It’s possible to have a number of alternatives to choose from, and in order to have a flexible configuration, and to keep components reusable, such decisions are best left out of the processing components, and set up through the application configuration.
You saw two message distribution alternatives: one using publish-subscribe channels and filters, and one using routers.
A router-based solution is both centralized and closed, and by that we mean all the routing configuration is in a single place (the router)
You can’t add new potential recipients of the message without modifying it.
The publish-subscribe channel solution, on the other hand, is decentralized and open; the routing configuration is emerging as a sum of all the conditions defined on the individual filters, and new subscribers can be added without modifying any existing component.
If your scenario calls for adding and removing consumers dynamically, the publish-subscribe channel solution is the best option.
Otherwise, the centralized configuration provided by the router allows for easier maintenance.
Ideally, message consumers such as transformers and service activators should be compact, encapsulated, and highly cohesive units.
This allows them to be flexible so they can be redeployed in a large variety of scenarios.
At the same time, the application may be required to handle more complex messages.
The next chapter is dedicated to components such as splitters and aggregators that allow the framework to split messages into groups and deal with the correlated messages.
Previous chapters explained how a single message is processed as a unit.
You saw channels, endpoints such as service activators and transformers, and routing.
All these components have one thing in common: they don’t break the unit of the message.
If one message goes in, either one message comes out the other end or it is gone forever.
This chapter looks at situations in which this rule no longer holds.
In some situations, one message goes in and several messages come out (splitter), and in others, several messages go in before messages start coming out (aggregator, resequencer)
Examples of endpoints illustrating the various possible scenarios are shown in figure 7.1
The resequencer and the aggregator must maintain state because the outcome of handling a given message Splitting and aggregating messages.
Introducing correlation depends on the previous messages, which isn’t the case for the endpoints described in earlier chapters.
The fact that these endpoints maintain state for functional reasons differentiates them also from other stateful endpoints that maintain state for improving performance, such as file adapters, which hold a queue of files in memory to prevent costly file listings (see chapter 11)
Not all the endpoints in this chapter are stateful: for example, the splitter, which we introduce for symmetry with the aggregator, is a stateless component.
Generally, the fact that components are stateful or stateless plays an important role in the concurrent and transactional behavior of your application, so it’s important to pay close attention to this aspect.
As we discussed in chapter 1, correlating messages is sometimes essential to implementing a certain business requirement.
In all these cases, the correlation of the messages is key, so it makes sense to discuss how Spring Integration stores and identifies the correlation of messages and how you can extend this functionality.
It’s explained in detail in the “Under the hood” section later in the chapter; for now, it’s enough to understand the functional concept of correlating messages in a group.
This section focuses on correlation from a functional perspective, introducing the main concepts behind it.
An aggregator, such as the one in figure 7.2, waits for messages in a certain group to arrive and then sends out the aggregate.
Figure 7.1 Examples of endpoints processing a message in one-to-one, one-to-many, and many-to-one scenarios.
The part we’re interested in first is how it determines what messages belong together.
Spring Integration uses the concept of a message group which holds all the messages that belong together in the context of an aggregator.
These groups are stored with their correlation key in a message store.
The correlation key is determined by looking at the message, and it may differ between endpoints.
Now is a good time to look at an analogy to help anchor the important terms used in this chapter.
Maybe you’re not the cooking kind, but you’re probably familiar with the concept of home cooking, and that’s more than enough.
We’ll look at the whole process of preparing and serving a meal to see what’s involved in automating this process and how it applies in terms of messaging.
For the sake of argument, we’ll ignore the possibility of ordering take-out (which would greatly decrease the complexity of the setup but would ruin the analogy)
Figure 7.3 illustrates the scenario described in the rest of this section.
A recipe is split into ingredients that are aggregated to a shopping list.
This shopping list is converted into bags filled with products from the supermarket.
The bags are then split into products, which are aggregated to a mise en place, which is finally transformed into a meal.
The channel configurations are considered trivial and are omitted from the diagram.
Involved in the dinner are the host (that’s you), the kitchen, the guests, and the shops.
You orchestrate the whole event; the guests consume the end product and turn it into entertaining small talk and other things irrelevant to our story.
The kitchen is the framework you use to transform the ingredients you get from the shop into the dinner.
Aggregator Figure 7.2 An aggregator is an endpoint that combines several related messages into one message.
Figure 7.3 The flow of messages in the home cooking example.
We’re interested in the sequence of events that take place after a date is set.
It starts with your selecting a menu and gathering the relevant recipes from your.
The ingredients for the recipes must be bought at various shops, but to buy them one at a time, making a round trip to the shop for each product, is unreasonably inefficient, so you must find a smarter way to handle this process.
The trick is to create a shopping list for each shop you must visit.
You pick the ingredients from each recipe, one by one, and put them on the appropriate shopping list.
You then make a trip to each shop, select the ingredients, and deliver the ingredients back to your kitchen.
With the ingredients now in a central location, each shopping bag must be unpacked and the ingredients sorted according to recipe.
Having all the right ingredients (and implements) gathered together is what professional chefs call the mise en place.
With all the necessary elements at hand, each dish can be prepared, which usually involves putting the ingredients together in some way in a large container.
When the dish is done, it’s divided among plates to be served.
But what does this have to do with messaging? Perhaps more than you think.
This message is split into ingredients, which can be sent by themselves as messages.
The ingredients (messages) reach an aggregator that aggregates them to the appropriate shopping lists.
The shopping lists (messages) then travel to the supermarket where the shopper turns them into bags filled with groceries (messages), which travel back to the kitchen (endpoint)
The shopping bags are unpacked (split) into products that are put together in different configurations on the counter during the mise en place.
These groups of ingredients are then transformed into a course in a pan.
The dish in the pan is split onto different plates (messages), which then travel to the endpoints that consume them at the table.
What we see here is a lot of breaking apart and putting together of payloads.
The recipes are split and the ingredients aggregated to grocery lists.
The bags are unpacked and the products regrouped for the different courses.
Some observations can be made from this analogy that will be helpful to think back on later in the chapter.
Splitting is a relatively easy job, but it’s important to keep track of which ingredients belong to which recipe.
Messages (ingredients in this example) are given a Correlation ID to help track them.
Most easy examples of splitting and aggregating use a splitter that takes a thing apart and an aggregator that turns all the split parts into a thing again.
This is a simplistic example, so it’s good to keep a simple analogy in mind that does things differently.
In a real enterprise, an aggregator often has no symmetric splitter.
It now becomes important to think about how we’ll know when the aggregate is done.
Or to stay with the example, when is the shopping list done? The answer is that it’s only done when all the recipes have been split and all ingredients are on their.
This is still relatively simple but more interesting than to say that a list is done when all ingredients of one particular recipe are on it.
When aggregating is done without a previous symmetric splitting, it becomes harder to figure out which messages belong together, or in this example, which ingredients go on which list.
Usually aggregation relies not on a message’s native key but on a key generated by a business rule.
For example, it could be that all vegetable ingredients go on the greenery list and all meat ingredients go on the butcher list.
This assumes that you’re not just buying everything from the supermarket, but even if you do, it makes sense to organize your shopping list by type to avoid having to backtrack through the supermarket.
The next sections explain how the components introduced here are used.
It’s common for domain models to contain high-level aggregates that consist of many smaller parts.
An order, for example, consists of different order items; an itinerary consists of multiple legs.
When in a messaging solution one service deals with the smaller parts and another deals with the larger parts, it’s common to tie these services together with endpoints that can pull the parts out of a whole (splitter) and endpoints that can (re)assemble the aggregate root from its parts (aggregator)
The next few sections show typical examples and the related configuration of splitters, aggregators, and resequencers.
The basic functionality of a splitter is to send multiple messages as a response to receiving a single message.
Usually these messages are similar and are based on a collection that was in the original payload, but this model isn’t required in order to use the splitter.
Let’s look at an example of splitting in our sample application.
When flight notifications come in, you want to turn them into notifications about trips and send them to impacted users.
To do so, you enrich the header of a flight notification with a list of impacted trips.
At the end of the chain, you can then use a splitter that creates a TripNotification for each trip related to the flight:
Interesting to note here is that the payload isn’t chopped up; the splitting is based instead on a list in a header.
Spring Integration is indifferent to the splitting strategy as long as it gets a collection of things to send on as separate messages.
The home cooking example also contains a splitter, which chops up a recipe into its ingredients:
As you can see, a chain is used around the splitter to pop references to the original recipe on a header, so you can use it as a correlation key later when you aggregate the products.
The splitter is a simple expression that gets the ingredients (a list) from the recipe payload.
You can use a plain old Java object (POJO) or a simple Spring Expression Language (SpEL) expression to retrieve the desired information from the message in the form of a List.
Before we think about doing the reverse, we need to think backwards through the splitting process.
The splitter outputs sets of messages, each generated by a single message received by the splitter.
It’s the original message that correlates them (see section 7.2)
In Spring Integration, messages that are correlated through a correlation key can be grouped in certain types of endpoints.
These endpoints keep the notion of a MessageGroup, discussed in the section “Under the hood” later in this chapter.
Messages can belong to the same group for many reasons.
They may have originated from the same splitter or publish-subscribe channel, or they may have common business concerns that correlate them.
For example, the flight notification application could have a feature that allows users to have the system batch the notifications they receive per email on the basis of certain timing constraints.
In this case, there’s no concept of an original message; you’ll often see examples where aggregating isn’t used in relation with any splitting logic.
Remember popping the recipe on a header? Now when you aggregate the products from the stores back together, you can use this recipe as a correlation key:
The aggregator called kitchen refers to a cook for the assembly of the meal.
This snippet shows how a group of related products are assembled into a meal, but it doesn’t show how these messages are related and when they’re released.
More details on this are found in the “Under the hood” section.
For now, we just show the implementations of correlation strategy (to determine which message belongs to which group) and release strategy (to determine when a group is offered to the cook)
The release strategy delegates to the recipe to determine if all the ingredient requirements are met by products:
But gathering related messages and processing them as a group isn’t the only use case for correlation.
Another component works similarly to the aggregator and can be used to make sure messages from the same group flow in the correct order: the resequencer.
When different messages belonging to the same group are processed by different workers, they may arrive at the end of a message flow in the wrong order.
As we saw in chapter 3, there is a priority channel that can order messages internally, but this channel doesn’t consider the whole group.
Splitting, aggregating, and resequencing not caring about gaps in the sequence caused by messages that haven’t yet arrived at the channel.
It can guarantee that all messages in the group arrive in exactly the right order on the resequencer’s output channel.
This pattern is more like an aggregator than you might realize at first glance.
Like the aggregator, the resequencer has to wait for several members of a group of correlated messages to arrive before it can make a decision to send a message to its output channel.
Good recipes give the ingredients in an order that makes sense for the preparation.
But as you split the ingredients lists and spread the items out over multiple shopping lists, you end up with the ingredients in a different order than they should be.
When groups of messages are processed concurrently, say, when you start splitting recipes with multiple people or when you let multiple people shop at the same time, it’s obvious that you introduce race conditions.
As long as no checking is done later and no ordering requirements are presented, you won’t see any negative side effects.
Remember this rule of thumb: adding concurrency increases the random reordering of messages.
On the shopping list, the ingredients are in semi-random order determined by the order in which the recipes are split.
Because stores generally arrange their products by type, it's most efficient for you to organize your list in the same way.
This is a traveling salesman problem1 that can be simplified by assuming the shop has only one possible walking route and you only have to avoid backtracking.
A traveling salesman problem is the problem of finding the shortest route that visits all destinations from a given set exactly once.
Why ordering should be avoided if possible Before looking at examples of where a resequencer might be useful, we offer a word of warning.
From an architectural perspective, depending on the ordering of messages is almost without exception a problem when scaling and performance are at stake.
The problem arises because the resequencer is a stateful component, and to guarantee that all messages of the sequence arrive in the right order, the only way to clean up the state from a resequencer is to ensure that all messages of the sequence have been sent to it.
As a rule, you should only depend on resequencing within a single node and only if the whole sequence can reasonably be expected within a short time.
What is reasonable and short wholly depends on the characteristics of your application and target environment.
Recovering from message loss or timeouts is far from trivial when you have sequence dependencies.
If you can design the system in such a way that messages which are older than the last message processed are simply dropped, this is a fundamentally more robust solution.
That said, in some cases resequencing is convenient, so you should understand the concept.
It’s important to make sure the shopping list is reordered when it’s completed (not before)
This is done by the resequencer, which is much like the aggregator, but instead of releasing a single message, it releases all messages ordered according to their sequence number or a custom comparator.
In the shopping example, the sequence number isn’t used, but instead, the ingredients are compared in shoppinglist order.
Using a custom comparator, it’s not possible to release partial sequences because you can’t know in advance whether another ingredient might need to be inserted in the middle of the list.
When doing the mise en place, it’s a good practice to arrange the ingredients in the order they appear in the ingredient list.
The mise en place is essentially the resequencing of the correlated ingredients according to the order in which they appear in the recipe.
This happens just before they’re aggregated into the pan (or bowl or what have you)
For this example of resequencing, you can depend on the sequence number and size set by the recipe splitter.
If your ingredients list starts with onions and lists garlic as a second item, you can be sure that if you pull the onions and the garlic out of the shopping bag first, you can prepare them before pulling the next items out.
This should give you some idea of how to think about resequencing in an everyday life scenario.
In most complex enterprise applications, it’s possible to find an analogy, using your favorite subject, that fits well with what should happen.
Car and kitchen metaphors will carry you a long way as an architect.
After reading this section, you should have a good general idea of what splitting, aggregating, and resequencing are and how you can use them in an architecture.
The next section elaborates a bit more on some common but nonstandard configurations.
Our example cases so far have shown the most obvious correlations between messages: a well-defined group of payloads that are released as a group.
This isn’t the only possible use for correlation, though, and this section elaborates on two other use cases: timing-based aggregation and the scatter-gather pattern.
Both demonstrate that aggregation involves much more diverse scenarios than you might first think.
The way the different payloads and headers are aggregated is important, and so are the ways of determining what messages belong together and how strong this correlation is from a business point of view.
Many scenarios don’t operate with groups of individual messages that can be aggregated together like the order items that belong to a specific order.
They operate in a looser fashion: the groups have certain requirements concerning the numbers or kinds of payloads that must be present in the group before release, but, for example, payload instances of the same kind may be interchangeable.
Consider an order trading system: the condition for making a trade and releasing a group of messages is to find a match between the buy (long) and sell (short) orders.
These three orders can be fulfilled against each other, but if a customer D then placed a long order of 500 Glorp shares, its order could also be fulfilled against customer B’s short order.
The outcome depends greatly on the sequence in which the orders arrive, including timing.
Race conditions like this one are often inevitable because being completely fair is impractical, if not impossible, given the performance requirements.
A heuristic approach is a better fit, and various patterns have emerged in an attempt to offer a satisfactory, albeit not ideal, solution.
The next two sections focus on two common patterns in aggregation that don’t immediately fall under the straightforward example of taking something apart and putting it back together.
The first section explores aggregation based on nothing but timing, and the second section deals with scatter-gather.
In many aggregator use cases, completion is based not only on the group of messages but also on external factors such as time.
Let’s look into such a scenario and see how it’s supported by Spring Integration.
When, as the host, you’re aggregating ingredients on the shopping lists, you can of course wait until all recipes have been split before going to the store, but that might make for a very long list.
It might also take a lot of time; say, for example, 10 minutes longer than if you were to give your spouse a partial list so they could leave for a particular shop while you wait for the splitting to finish.
Then you can give the next part of your unfinished but long list to a friend, who can also start shopping before the splitting is complete.
When the splitting is done, you have three separate lists, two of which are already being worked on.
This early completion strategy is useful to ensure all workers are busy in a complex system.
Big lists are good for optimization, but making a list infinitely big doesn’t help effectiveness.
In terms of Spring Integration’s aggregator support, what should be happening here? First of all, this scenario has a time-based constraint.
At a particular time, the aggregated list is sent regardless of whether or not it’s complete.
Then, of course, the newly arriving messages must still be aggregated, so multiple aggregates, not just one, are sent.
In figure 7.4, you can see how this might work in practice.
Figure 7.4 Before the timeout, just two out of three messages have arrived.
A bit later, it receives the missing 2, which it must send out without the rest of the aggregate.
This means you need to do something else to ensure a partial timeout at some point.
It’s sometimes possible to modify the release strategy to always release the group when it finds a certain time has elapsed, but the problem with this approach is that the release strategy is only interrogated when a new message arrives.
If it takes a while for messages to arrive, the timeout might pass without a release happening.
When an incomplete group is sent on expiry, the remaining group is usually also incomplete.
For example, the default strategy of counting the messages and comparing their number with the sequence size will no longer work.
Usually in this case, there’s a business rule that can tell you whether you received all the messages.
In the shopping list example, you can check whether all recipes are split already, and because a direct channel is used for sending ingredients, the last group can be completed without checking the size.
As you can see, aggregation can be based on more than just business keys and even on the same key repeatedly.
Next we look at a situation in which the different messages are the result of work done on a different collaborating node: scatter-gather.
In the most typical cases, aggregating is based on a list of similar messages and splitting is about cutting up the payload of a message.
In this section, we look at a common use case that doesn’t follow this pattern.
The next few paragraphs are about the definition of scatter-gather and how it’s different from MapReduce.
Even if you don’t know what MapReduce is, you should be fine with the rest of the chapter.
Scatter-gather is a name commonly used to refer to a system that scatters a piece of information over nodes that all perform a certain operation on it; then another node gathers the results and aggregates them into the end result.
The major difference between it and MapReduce is that, in scatter-gather, the different nodes might have different functions.
You can learn about MapReduce from many other resources, and because Spring Integration isn’t a MapReduce framework, we don’t cover it here.
It’s important to note that scatter-gather and MapReduce are by no means mutually.
Note on timeouts Timing out means a separate trigger is fired when the timeout point is reached.
This is fundamentally different from normal release because the timeout event is not based on reception of a message.
In Spring Integration 2.0 this functionality has been pushed down from the aggregating message handler into the message store itself.
Because timeout is important to the end user, it’s still exposed as a flag on the <aggregator/> element.
Useful patterns exclusive; they are complementary, and a good architect should be able to weigh the applicability of both or either of them against the complexity they inevitably add to the system.
This is a broad definition, and it could even be said that MapReduce is a subtype of scatter-gather.
We look at an example where the different nodes have different functions so that we’re forced to stay clear of MapReduce concerns.
The home cooking example contains a good candidate for scatter-gather.
When you split the ingredients over multiple shopping lists, you might find that certain shops offer the same products.
You can implement several behaviors that take this into account.
For example, a certain product might often be out of stock in shops.
If so, then it’s no problem to stock more than you need (it’s conservable), so you can try to buy it at all the shops.
If you serialize the shopping or allow communication between the shoppers, you could decrease the risk of overbuying.
If you’re looking at an expensive product, you can allow shoppers to compare prices with each other when they’re shopping in parallel.
The amount of synchronization needed here depends on how bad it would be if you bought too much or if you bought it at a higher price.
Our example assumes that no synchronization is done and you’ll try to buy the ingredient at all shops.
The one that ends up on the mise en place is closest to the bestbefore date.
Figure 7.5 presents a schematic overview of a this scatter-gather scenario.
To scatter an ingredient after splitting, you need to route it to multiple nodes.
You can do this by configuring a router that does its best to route to a single shopping list, but if that fails, it routes to a publish-subscribe channel that all the shopping lists are connected to (so it ends up on all lists instead of one)
Another option is to get rid of the router altogether and use a filter in front of each shop that drops all ingredients which can’t be found at the shop.
Yet another option is to let the filtering occur naturally by asking each shop for each product and taking all that are available.
It depends on the situation which option is more efficient, and we won’t spend time tuning it further here.
Figure 7.5 The needed ingredient is scattered over all shopping lists and sent to each shop (A and B)
The gatherer decides, on the basis of the bestbefore date (or some other criterion), what is the best product to use and sends that to the mise en place (not in this picture)
Gathering happens when the products come back from the shops.
The easiest way is to use whatever comes first out of the shopping bags and to not use unneeded items.
The other option is to compare the duplicate products’ best-before date when all products for a mise en place are complete and store the one that can be conserved longest.
In this section, you saw two examples of aggregator that differ from the standard usage of reassembling some collection.
There’s only one thing left to do, and that’s to open the black box and look at the machinery of Spring Integration that makes all this tick.
When a message group might go out, it’s released, processed, and finally marked as completed.
Let’s look into the details of each of those steps—correlate, store, release, process, complete—as shown in figure 7.6, and introduce collaborators as we go along.
Many of the details discussed here changed as well, but the default strategies are the same.
The message group is defined by its correlation key (not to be confused with correlation ID)
The default strategy picks the correlation ID from the message headers, but this doesn’t have to be your strategy.
After the correlation key is found, the message can be stored with its group.
For this it uses a MessageGroupStore, which defaults to an in-memory implementation.
A JdbcMessageStore is available in the framework, but it stands to reason that a NoSQL store is more fitting in many cases.3 The storage will hold all incomplete message groups, so it’s important to consider memory consumption and performance in case of large groups or large numbers of incomplete aggregates.
After the message is stored, that message’s group is considered for release.
For example, a completed aggregation will be released, or a partially completed group that may contain the first few elements of a sequence may be released.
The release strategy says nothing about the completeness of the group.
Its only responsibility is to decide whether the message processor may process this particular group.
This is where the actual operations on the messages are performed.
The processor is handed a template to send messages with and is expected to make all decisions relevant to sending output messages.
It’s also responsible for marking the messages it has processed in the message group.
The marked messages are then recognizable as processed if the same group hits the processor later.
There is no restriction on the contract that the processor has to fulfill that disallows it from reprocessing marked messages.
In the next few paragraphs, you’ll see the implementation of aggregator and resequencer as examples of the mentioned strategies.
In both aggregator and resequencer, correlation and storage are the same (and trivial), so we go into the details of release and processing only.
The aggregator, as discussed earlier, takes a group as a whole and forges a new message out of it.
We look at the release and processor strategies in detail in the next few paragraphs.
The release strategy of an aggregator should be to release the group only when the processing is complete (or if it times out)
For this common case, the MessageGroup has an isComplete() method, the default implementation of which compares the sequence size header to the size of the group.
This is convenient if you’re implementing a custom release strategy but still are interested in the default completeness of the group.
A few NoSQL implementations are available in Spring Integration version 2.1
The message group processor of an aggregator should turn all the messages of a group into a single aggregated message and send it off to the output channel.
The method should have the following signature (pointcut expression language):
Similar to other implicit conversions to and from messages in the framework, Spring Integration automatically unwraps the elements in the list if they’re not messages.
The return value is wrapped in a message if needed and sent to the output channel of the aggregator.
The resequencer example follows the same lines as the aggregator with two main differences.
First, the messages from an incomplete group may already be released.
Second, the processor is expected to return the same messages that came in.
This flag allows the release strategy to release parts of an incomplete sequence that are in the right order to allow for a smoother message flow.
The message group processor of a resequencer takes all the messages in the group, orders them, and then sends all the messages that form a sequence to the output channel.
The main customization is to supply a different comparator for the ordering so sequence numbers can be avoided.
To decide when to release the group for processing, a ReleaseStrategy is used.
Implementations of these strategies together form the different correlating endpoints.
In this chapter you learned to deal with splitters, aggregators, and resequencers.
You also saw examples of some nontrivial aggregator use cases and finally looked at the design that’s at the core of Spring Integration.
The chapter also discussed endpoints that group messages together before sending reply messages.
Correlation, the basis for both aggregating and resequencing, was examined in detail.
By default, its correlation and release strategies are complementary to the splitter.
Its correlation and release strategies are similar to those of the aggregator with the exception of releasing partial sequences.
When partial sequences are released, multiple releases for the same group may happen.
Now that you’ve read this chapter, you should have a clear idea how Spring Integration can help you when you need to split up some work or aggregate the results of some operations back together.
Aggregation is a particularly complex use case that often differs subtly from the examples found in this book or online.
In some cases it pays to write a custom solution.
It’s particularly important here to consider carefully the pros and cons of extending the framework versus inventing your own.
You now know how the core of Spring Integration works.
We reviewed all the main components in the core and showed you several examples of messaging applications using the components.
The interesting part comes when you start integrating with remote systems and look beyond the walls of the JVM.
In the next chapter, you’ll work with XML, because it’s the ubiquitous language of system integration.
Chapter 8 shows you concepts from this and previous chapters reused in the context of XML payloads, such as the XPath splitter and the XPath router.
In part 3, we'll focus on how to use Spring Integration to enable communication and data exchange between your Spring applications and other systems.
Chapter 12 explains how to exploit web services with Spring Integration.
Finally, chapter 13  covers adapters for XMPP instant messaging to enable chat services and presence notifications, as well as support for Twitter search, timelines, status updates, mentions, and direct messages.
Now let's see how we can put all these pieces together to extend the breadth and scope of your Spring applications using important technologies such as XML, JMS, mail, filesystems, web services, and more.
Chapter 6 discussed routing, and you saw that the flow of a message through the system, or the chain of events caused by the arrival of a message, is usually dependent on its payload.
We discussed in chapter 5 that transforming payloads to and from an intermediate format, such as XML, is essential to integrating different systems with each other.
There are many exchange formats in the wild, but none is as widespread as XML.
Virtually all programming languages have some sort of XML support.
Integration components generally don’t need detailed knowledge of the domain, but they do need to react to the contents of a message.
If both input and output are XML messages, and simple message processing is all that’s required, marshalling the message (as we discussed in chapter 5) will cause unnecessary complexity and overhead.
In such cases it’s better to work on the XML directly, using, for example, XPath or Extensible Stylesheet Language Transformation (XSLT)
Where more complex processing is required, it’s often preferable to convert the XML message to a domain object, so this chapter also looks at the object-to-XML mapping (OXM) support provided by the frameworks.
Because XML is the most commonly used interoperable format, it makes sense to equip an integration framework with support for this language.
We discuss the details of Spring Integration’s XML support, including the different payload types supported by the provided XML endpoints.
In common with the rest of the framework, the XML module is rooted in the enterprise integration patterns and provides support for those patterns to make working with XML simple.
This chapter introduces the XML capabilities of the Spring Integration framework using an example of how to implement a message flow using the provided XML support.
The chapter finishes with a detailed explanation of the approach taken in the implementation of these endpoints along with an overview of extension points for advanced usage.
In our Spring Integration travel example application, a quotation request for a trip may contain multiple legs.
Once the trip is split into legs, the message flow needs to generate quotation requests for the different elements of the trip, such as the hotel accommodation and car rental.
To do this, you must separate the constituent parts of the trip relating to hotel, flight, and car rental before transforming the internal canonical representation to one that is understood by the third-party systems.
One approach is to maintain a number of Java payload types representing the request formats of each individual system and a set of transformations between the canonical form and the third-party form.
This task can become unwieldy as the number of systems with which you integrate grows.
The request format of the third-party system is likely to be some form of XML, so the process of generating a quote for each leg of the trip should be implemented as an XML-based message flow.
It contains the requirements for the start and end locations of the leg, the desired dates for travel, and additional specifications for hotel and car rental if required.
On receipt of this message, the system first converts it into the canonical XML form of a trip leg.
Having duplicated the leg information to each criteria section, you can then split the document into separate requests.
Since the requests are now selfcontained, they can then be processed in parallel, a common strategy for speeding up.
In this case the splitting is achieved by the use of XPath, which splits the document into one document per child of the legQuote element.
The leg-quote flow can then process each request in parallel and route it to the appropriate third-party system.
Again, this is done using an XPath expression, which routes the message according to the route element name.
Finally, before making a request to the third-party system, you validate that each request conforms to the schema that defines the third-party request format.
This is achieved by using the out-of-the-box support for validating messages against an XML schema definition.
The next section discusses the support for marshalling, which allows you to convert the payload of the message into an XML form ready for subsequent processing and dispatching to the external systems.
To convert your message between XML and objects, you use OXM.
Spring OXM provides a common interface across a number of leading Java OXM solutions.
By providing a common abstraction, the OXM module decouples code from the implementation details and provides a consistent exception hierarchy regardless of the OXM technology being used.
This decoupling is achieved by encapsulating the mapping process behind implementations of the Marshaller interface, which maps from an Object to the XML Result, and the Unmarshaller interface, which maps from a Source to the Object representation.
Both of these interfaces are shown in the following code snippets, and the unmarshaller is visualized in figure 8.1:1
Spring OXM was developed as part of the Spring WS project.
As of Spring 3.0, the OXM framework is part of the core Spring framework.
For the leg-quote flow, you can use the Java Architecture for XML Binding (JAXB) v2 OXM technology in conjunction with Spring OXM.
Because the sample already contains a set of existing rich domain classes, you can annotate the domain classes with JAXB annotations rather than generate the classes from an XML schema.
Following is the annotated LegQuoteCommand, which maps to the root of the canonical XML legquote document.
To indicate that this maps to the root of an XML document, you annotate it with the @XmlRootElement annotation.
You also specify the name of the XML element with the name attribute of the annotation.
This contains fields representing the criteria for each of the constituent parts of the leg quote:
By default, JAXB outputs all fields of the annotated type, although it can only convert standard Java types out of the box.
This functionality can be extended by providing XmlAdapter implementations capable of mapping types unsupported by JAXB to types supported by JAXB.
An example is the Leg class, which contains start and end dates and locations.
This class uses the Joda Time project (http://joda-time.sourceforge.net) to represent dates.
This adapter converts between the date types using coordinated universal time (UTC):
The Spring Integration marshalling and unmarshalling support is a fairly thin adapter layer on top of Spring OXM, so you first configure an instance of a marshaller or unmarshaller, as appropriate.
The out-of-the-box implementations all implement both the Marshaller and Unmarshaller interfaces.
The following code example shows how to configure an instance of Jaxb2Marshaller, providing a list of the annotated types, in this case just the LegQuoteCommand:
On top of this, you can configure a marshalling endpoint to convert from the LegQuoteCommand to a canonical XML form using the Spring Integration XML namespace, as in the next example:
Where it’s used outside the scope of a chain, it also requires an input channel and output channel.
Looking at the standard Marshaller interface, you can see that the marshaller converts the Object graph to an instance of Result.
Because the Spring Integration marshaller’s primary purpose is to be a thin adapter layer on top of the OXM support, this behavior is the same behavior as you get with Spring Integration by default.
If you were to configure the marshaller with nothing else but an instance of Marshaller and references to the input and output channels, the output of the marshaller would be a message containing a payload of type Result.
In fact, without casting the Result, the only thing you can do is get or set an optional system ID, which may be used for error messages or to relate to a file on the local filesystem.
To avoid the proliferation of tests to determine the type of Result and casts to allow something useful to be done with it, most of the provided XML endpoints attempt some form of type negotiation whereby the payload output type is determined by the input.
For example, if you pass in a String, most provided XML endpoints will pass back a String.
The supported payload types and strategies for determining output type are discussed later in this chapter in the “Under the hood” section.
The marshaller is different: because you’re passing in an Object and asking for the Object as XML, the marshalling endpoint has no clue how you want the XML.
To make this payload more useful in this case, you provide an optional collaborator of type ResultTransformer to convert the Result to something more useful.
Out of the box, two implementations of ResultTransformer are provided, one to convert to a Document and the other to convert to a String.
Following is an example of the XML output you’d expect to see for a sample message passed into the marshaller:
Now that you have the message with an XML payload, you’re ready to transform this output into something suitable for splitting and dispatching.
The following section explains how the transformation can be achieved using XSLT.
Now that the leg quote is in the canonical form, the next step is to transform the XML document to ensure that it can then be split into three separate requests ready to send to the external systems.
These external systems then provide the quotes for car rentals, hotels, and flights.
Currently, the elements containing information about start and end locations and dates are child elements of the root legQuote element.
Therefore, the next step in the message flow is to use an XSLT to add the leg information to the criteria elements and rename them.
Using the Spring Integration XML namespace, you configure an XSLT endpoint, as shown in the next code example.
The alternative is to instantiate the Templates instance directly and pass a reference into the transformer where customization of the template creation is required.
Passing the example message through this transformation produces the following XML, which is now suitable for splitting into the three separate parts of the leg to generate the three quotes (for car rental, hotels, and flights):
Having now enriched the message, you’re ready to split it into parts representing each of the quotes you need.
The next section demonstrates Spring Integration’s support for the splitter pattern using XPath to express how you want to split your XML message.
Much of the support for processing XML payloads utilizes XPath expressions within the configuration.
Spring Integration builds on the XPath support provided by the Spring Web Services project and adds additional namespace support to make working with XPath-based components easier in the context of Spring Integration.
XPath expressions can be defined either within the definition of the endpoint using the expression or, as in the following example, as top-level beans that can then be referenced in multiple places:
Because XML namespaces are commonly used to avoid element name clashes, XPath expressions generally must incorporate a namespace.
In the simple case of only one namespace, the namespace can be the URI and prefix, or alternatively, it can be referenced as a map of namespaces:
Given this support for XPath, you can use XPath expressions to split a message into a number of parts for separate processing, as shown in the following section.
Now you have a quote in a form that allows you to split it into three self-contained requests using an XPath expression that defines how you want to split the document.
The XPath splitter implementation evaluates the provided XPath expression to create a node set.
Each node within the node set then becomes the payload of a new message.
To split the leg quote, an XPath expression evaluates to give the direct children of the legQuote element, which is simply /legQuote/*
Figure 8.2 illustrates the configuration for the XPath splitter that separates the different types of quote request:
By default, the XPath splitter creates messages, each containing one of the nodes returned by the XPath evaluation.
To isolate the message payloads from the split document, the create-documents flag can be set.
This creates new documents for each of the nodes returned by the XPath evaluation and imports the node to the new document.
Importing the node isolates the new document from the source document.
In this case, the carQuote, flightQuote, and hotelQuote elements in the postsplit messages are no longer children of the legQuote element.
This implies that changes made to any of the documents created by the XPath splitter won’t be seen in the original document.
Now you have your three separate requests; the next thing you need to do is route the requests to the different third-party systems for processing.
Since each request type has a well-known root element, you can use the provided XPath router to route according to the root element name, as illustrated in figure 8.3
The configuration for the XPath router is relatively simple and requires, as a minimum, an input channel and an XPath expression.
In the following configuration, you also provide a series of channel mappings.
As you learned in chapter 6, if no such mapping is provided, the default strategy is to resolve the value returned by the expression as a bean ID of a channel defined within the application context.
It may be possible to use this strategy, but the additional level of indirection provided by the mappings allows the use of meaningful channel names rather than channel names that reflect the structure of the XML messages being processed.
Please note that as an alternative to specifying the mappings in the router configuration, you can inject a channel resolver into the router in a similar way to the router element from the core namespace:
To ensure the correct operation of your system, you need to apply a somewhat defensive approach to the messages you receive.
The next section shows you how to validate an XML message to ensure you can successfully process it.
It’s important to ensure that only valid requests are sent to third-party systems.
As a final step before transmitting the XML, you validate the quote requests produced by the leg-quote message flow.
To carry out XML validation, you use the provided validating filter implementation, which can validate XML payloads against a schema definition provided in either Regular Language for XML Next Generation (RELAX NG) or XML schema format.
The validation support in the XML module takes on the form of a filter implementation that routes to either on output or discard channel according to the outcome of the schema validation.
Following is the XML schema definition for validating flight quote requests:
Plugging that code into the pipeline is simple, as shown in the following configuration.
The schema is loaded as a resource, and references to the input, output, and discard channels are configured:
Let’s look under the hood at the different forms of XML messages that Spring Integration supports and learn how to make informed choices.
It also takes care of conversion between different representations of XML, such as String, Document, Result, and Source.
This generally makes the life of the developer easier, but knowing what the framework is doing and how to change the default strategies is useful too.
For example, passing around XML as a String can be convenient because strings are easy to create and easy to log and debug.
In a high-performance application, though, it’s important to understand that extensively using XPath evaluation on strings involves a considerable amount of on-the-fly conversion between a String and a Document Object Model (DOM) representation.
When dealing with large XML payloads, tree-based parsers (DOM parsers) tend to induce poor performance characteristics in an application.
The first thing people do to improve this is to move to a streaming parser (Simple API for XML [SAX], Streaming API for XML [StAX])
A stream is a leaky abstraction under messaging, and it’ll break or cripple many serialization options.
To say either streaming or messaging is the way to go would be misleading, so when you need streams, design to use them without the need for stream references to travel through the messaging system.
Spring Integration’s XML support steers clear of streaming for this reason.
The following section exposes the internal workings of the Spring Integration XML module with regard to XML payload types.
It explains the implications of using different payload types and provides background for some of the choices made during the implementation of the module.
One of the main challenges when implementing the XML module for Spring Integration was deciding which of the many APIs for working with XML in Java to support.
It’s possible to interact with data expressed as XML from Java in many different ways, for example, by using SAX events, parsing into a tree representation such as org.w3c.dom .Document, or using one of the additional projects (for example, Dom4j or JDom) that aim to make working with XML from Java simpler and more intuitive.
The development team had a number of reasons for focusing primarily on string representations that could easily be converted into DOM on the fly and on DOM itself.
Using JAXP and org.w3c.dom packaged classes minimizes the need for third-party libraries and also keeps the implementation relatively simple.
In addition, many of the patterns supported, such as splitter and router, rely on XPath evaluation, and the fact that the standard implementations of XPath require a fully built document to evaluate against was another reason to initially focus on DOM rather than event-based processing of XML payloads.
The downside of focusing on DOM and String representations exclusively is that processing very large XML documents isn’t currently well supported because they result in large in-memory representations.
This has clear implications for concurrency and the maximum size of document Spring Integration can support.
In the future, support for streaming through StAX and/or SAX may be added to Spring Integration.
This chapter introduced the capabilities that the Spring Integration XML module provides for working with XML payloads.
We used a style that complements the POJO and Spring namespace approach of the framework as a whole.
Using XML payloads shouldn’t be the default for most projects, but it’s a valuable tool for projects requiring integration with external systems that produce or consume XML.
This chapter also showed you how to use XPath support to implement contentbased routing, and you learned how to split XML messages using XPath expressions.
We also demonstrated Spring Integration’s support for XSLT transformations by duplicating information in the XML message before splitting it.
In chapter 9, you’ll see how Spring Integration’s support for Java Message Service (JMS) makes it trivial to implement request/reply functionality between systems using JMS.
Later, in chapter 12, we return to XML, showing support for Web Services that may be more suitable for integration with systems written in other languages.
For many Java developers, the first thing that comes to mind when they hear “messaging” is the Java Message Service (JMS)
That’s understandable considering it’s the predominant Java-based API for messaging and sits among the standards of the Java Enterprise Edition (JEE)
The JMS specification was designed to provide a general abstraction over message-oriented middleware (MOM)
Most of the well-known vendor products for messaging can be accessed and used through the JMS API.
A number of open source JMS implementations are also available, one of which is ActiveMQ, a pure Java implementation of the JMS API.
We use ActiveMQ in some of the examples in this chapter because it’s easy to configure as an embedded broker.
If you want to learn more about it, please refer to ActiveMQ in Action by Bruce Snyder, Dejan Bosanac, and Rob Davies (Manning, 2011)
Hopefully, by this point in the book, you realize that messaging and event-driven architectures don’t necessarily require the use of such systems.
We’ve discussed messaging in several chapters thus far without having to dive into the details of JMS, which reveals that Spring Integration can stand alone as a framework for building messaging solutions.
In a simple application with no external integration requirements, producer and consumer components may be decoupled by message channels so that they communicate only with messages rather than direct invocation of methods with arguments.
Messaging is really a paradigm; the same underlying principles apply whether messaging occurs between components running within the same process or between components running under different processes on disparate systems.
Nevertheless, by supporting JMS, Spring Integration provides a bridge between its simple, lightweight intraprocess messaging and the interprocess messaging that JMS enables across many MOM providers.
In this chapter, you learn how to map between Spring Integration messages and JMS messages.
You also learn about several options for integrating with JMS messaging destinations.
Spring Integration provides channel adapters and gateways as well as message channel implementations that are backed by JMS destinations.
In many cases, the configuration of these elements is straightforward.
But, to get the most benefit from the available features, such as transactions, requires a thorough understanding of the underlying JMS behavior as dictated by the specification.
Therefore, in this chapter, we alternate between the Spring Integration role and the specific JMS details as necessary.
Spring Integration provides a consistent model for intraprocess and interprocess messaging.
The primary role of channel adapters and messaging gateways is to connect a local channel to some external system without impacting the producer or consumer components’ code.
Another benefit the adapters provide is the separation of the messaging concerns from the underlying transports and protocols.
They enable true document-style messaging whether the particular adapter implementation is sending requests over HTTP, interacting with a filesystem, or mapping to another messaging API.
The JMS-based channel adapters and messaging gateways fall into that last category and are therefore excellent choices when external system integration is required.
Given that the same general messaging paradigm is followed by Spring Integration and JMS, we can conceptualize the intraprocess and interprocess components as belonging to two layers but with a consistent model.
Even though we tend to focus on external system integration when discussing the roles of JMS, there are also benefits to using JMS internally within an application.
For this reason, Spring Integration provides a message channel implementation that delegates to JMS behind the scenes.
That channel looks like any other channel as far as the message-producing and message-consuming components are concerned, so it can be used as an alternative at any point within a message flow as shown in figure 9.2
Even for messaging within a single process, the use of a JMS-backed channel provides several benefits.
By default, such a channel doesn’t persist messages to a transactional resource.
Instead, the messages are only stored in a volatile queue such that they can be lost in the case of a system failure.
They’ll even be lost if the process is shut down intentionally before those messages are drained from the queue by a consumer.
In certain cases, when dealing with real-time event data that doesn’t require persistence, the loss of those event messages upon process termination might not be a problem.
It may be well worth the trade-off for asynchronous delivery that allows the producer and consumer to operate within their own threads, on their own schedules.
With these message channels backed by a JMS Destination, though, you can have the best of both worlds.
If persistence and transactions are important, but asynchronous delivery is also a desired feature, then these channels offer a good choice even if they’re only being used by producers and consumers in the same process space.
The main point here is that even though we often refer to JMS as an option for messaging between a number of individual processes, that’s not the only time to consider JMS or other interprocess broker-based messaging solutions, such as Advanced Message Queuing Protocol (AMQP), as an option.
When multiple processes are involved, the other advantages become more evident.
First among these is the natural load balancing that occurs when multiple consuming processes are pulling messages from a shared destination.
Figure 9.1  The top configuration shows interprocess integration using JMS.
Which type of integration is appropriate depends on the architecture of the application.
Figure 9.2 Design of the destination-backed channel of Spring Integration.
It benefits from the guarantees supported by a JMS implementation, but it hides the JMS API behind a channel abstraction.
For example, some processes may be running on slower machines or the processing of certain messages may require more resources, but the consumers only ask for more messages when they can handle them rather than forcing some upstream dispatcher to make the decisions.
Message-producing processes might be sending more messages than a single consuming process can handle without creating a backlog, resulting in a constantly increasing latency.
By adding enough consuming processes to handle the load, the throughput can increase to the point that a backlog no longer exists, or exists only within acceptable limits in rarely achieved high-load situations that occur during bursts of activity from producers.
If a consuming process crashes, messages can still be processed as long as one or more other processes are still running.
Even if all processes crash, the mediating broker can store messages until those processes come back online.
Likewise, on the producing side, processes may come and go without directly affecting any processes on the consuming side.
This is nothing more than the benefit of loose coupling inherent in any messaging system, applied not only across producers and consumers themselves but the processes in which they run.
Keep in mind when we discuss these scenarios where processes come and go, we’re not merely talking about unforeseen system outages.
It’s increasingly common for modern applications to have “zero downtime” requirements.
Such an application must have a distributed architecture with no tight coupling between components in order to accommodate planned downtime  of individual processes, one at a time, for system migrations and rolling upgrades.
One last topic we should address briefly here is transactions.
We revisit transactions in greater detail near the end of this chapter, but one quick point is relevant to the discussion at hand.
In the scenario described previously, where a consuming process crashes or is taken offline while responsible for an in-flight message, transactions play an important role.
If the consumer reads a message from a destination but then fails to process it, such as might occur when its host process crashes, then the message might be lost depending on how the system is configured.
In JMS, a variety of options correspond to different points on the spectrum of guaranteed delivery.
One configuration option is to require an explicit acknowledgment from the consumer.
It might be that a consumer acknowledges each message after it successfully stores it on disk.
The consumer would commit the transaction only upon successful processing of the message, and it would roll back the transaction in case of a known failure.
When this functionality is enabled, not only do the multiple consuming processes share the load, they can even cover for each other in the event of failures.
One consumer may fail while a message is in flight, but its transaction rolls back.
The message is then made available to another consuming process rather than being lost.
Table 9.1 provides a quick overview of the benefits of using JMS with Spring Integration.
It’s worth pointing out that the benefits listed in table 9.1 aren’t limited to JMS.
Any broker that provides support for reliable messaging across distributed producer and consumer processes would provide the same benefits.
For example, Spring Integration 2.1 adds support for RabbitMQ, which implements the AMQP protocol.
Likewise, although not as sophisticated, even using Spring Integration’s queue-backed channels along with a MessageStore can provide the same benefits because that too enables multiple processes to share the work.
For now, let’s get back to the discussion at hand and explore the mapping of Spring Integration message payloads and headers to and from JMS message instances.
When considering interprocess messaging from the perspective of Spring Integration, the primary role of channel adapters is to handle all of the communication details so that the component on the other side of the message channel has no idea that an external system is involved.
That means the channel adapter not only handles the communication via the particular transport and protocol being used but also must provide a Messaging Mapper (http://mng.bz/Fl0P) so that whatever data representation is used by the external system is converted to and from simple Spring Integration messages.
Some of that data might map to the payload of such a message, whereas other parts of the data might map to the headers.
That decision should be based on the role of the particular pieces of data, keeping in mind that the headers are typically used by the messaging infrastructure, and the payload is usually the business data that has some meaning within the domain of the application.
Thinking of a message as fulfilling the document message pattern from Hohpe and Woolf’s Enterprise Integration Patterns (Addison-Wesley, 2003), the payload represents the document, and the headers contain additional metadata, such as a timestamp or some information about the originating system.
It so happens that the construction of a JMS message, according to the JMS specification, is similar to the construction of a Spring Integration message.
This shouldn’t surprise you given that the function of the message is the same in both cases.
It does mean that the messaging mapper implementation used by the JMS adapters has a simple role.
We’ll go into the details in a later section, but for now it’s sufficient to point.
Load balancing Multiple consumers in separate virtual machine processes pull messages from a shared destination at a rate determined by their own capabilities.
Scalability Adding enough consumer processes to avoid a backlog increases throughput and decreases response time.
Availability With multiple consumer processes, the overall system can remain operational even if one or more individual processes fail.
Likewise, consumer processes can be redeployed one at a time to support a rolling upgrade.
In JMS, the message has a body, which is the counterpart of a payload in Spring Integration.
Likewise, a JMS message’s properties correspond to a Spring Integration message’s headers.
By now you’re familiar with the various message channel types available in Spring Integration.
One of the most important distinctions we covered is the difference between point-to-point channels and publish-subscribe channels.
You saw that when it comes to configuration, the default type for a channel element in XML is point-topoint, and the publish-subscribe channel is clearly labeled as such.
The JMS specification uses destination instead of message channel, but it makes a similar distinction.
A JMS queue provides point-topoint semantics, and a topic supports publish-subscribe interaction.
When you use a queue, each message is received by a single consumer, but when you use a topic, the same message can be received by multiple consumers.
Now that we’ve discussed the relationship between Spring Integration and JMS at a high level, we’re almost ready to jump into the details of Spring Integration’s JMS adapters.
First, it’s probably a good idea to take a brief detour through the JMS support in the core Spring Framework.
For one thing, the Spring Integration support for JMS builds directly on top of Spring Framework components such as the JmsTemplate and the MessageListener container.
Additionally, the general design of Spring Integration messaging endpoints is largely modeled after the Spring JMS support.
You should be able to see the similarities as we quickly walk through the main components and some configuration examples in the next section.
Figure 9.3 Spring Integration and JMS messages in a side-byside comparison.
The terminology is different, but the structure is the same.
The logical starting point for any discussion of the Spring Framework’s JMS support is the JmsTemplate.
This is a convenience class for interacting with the JMS API at a high level.
Each of these Spring-provided templates satisfies the common goal of simplifying usage of a particular API.
One quick example should be sufficient to express this idea.
First, we look at code that doesn’t use the JmsTemplate but instead performs all actions directly with the JMS API.
Note that even a simple operation such as sending a text-based message involves a considerable amount of boilerplate code.
This code is about as simple as it can get when using the JMS API directly.
ActiveMQ enables running an embedded broker (as you can see from the "vm://localhost" URL provided to the ConnectionFactory)
Many JMS providers would be configured within the Java Naming and Directory Interface (JNDI) registry, and that would require additional code to look up the ConnectionFactory and Queue.
Now, let’s see how the same task may be performed using Spring’s JmsTemplate:
The code is much simpler, and it also provides fewer chances for developer errors.
The JMS resources, such as Connection and Session, are also acquired and released as appropriate.
In fact, if a transaction is active when this send operation is invoked, and some upstream process has already acquired a JMS Session, this send operation is executed in the same transactional context.
If you’ve ever worked with Spring’s transaction management for data access, this concept should be familiar to you.
If one particular operation in the transaction throws an uncaught RuntimeException, all operations that occurred in that same transactional context are rolled back.
There are also send() methods that accept a JMS Message you’ve created, but by using the convertAndSend versions, you can rely on the JmsTemplate to construct the Messages.
The receiveAndConvert method performs symmetrical conversion from a JMS Message.
Sometimes the default conversion options aren’t a good fit for a particular application.
That’s why the MessageConverter is a strategy interface that can be configured.
Asynchronous JMS message reception with Spring directly on the JmsTemplate.
Spring provides an object-to-XML (OXM) marshalling version of the MessageConverter that supports any of the implementations of Spring’s Marshaller and Unmarshaller interfaces within its toMessage() and fromMessage() methods respectively.
For example, an application might be responsible for sending and receiving XML-based text messages over a JMS queue, but the application’s developers prefer to hide the XML marshalling and unmarshalling logic in the template itself.
It’s also possible to provide a custom implementation of the Marshaller and Unmarshaller interfaces or even a custom implementation of the MessageConverter.
For example, you could implement the MessageConverter interface to create a BytesMessage directly from an object using some custom serialization.
That same implementation could then use symmetrical deserialization to map back into objects on the receiving side.
Likewise, you might implement the MessageConverter interface to map directly between objects and text messages where the actual text content is formatted using JavaScript Object Notation (JSON)
In this section, you learned how the JmsTemplate can greatly simplify the code required to do some basic messaging when compared with using the JMS API directly.
The examples covered both sending and receiving, but the receive operations were synchronous.
Before we discuss how Spring Integration can simplify things even further, we cover the support for asynchronous message reception in the Spring Framework, which provides the foundation upon which the most commonly used JMS adapters in Spring Integration are built.
The polling consumer and event-driven consumer patterns both make appearances throughout this book.
You saw in chapter 4 that even with simple intraprocess messaging in a Spring Integration–based application, each pattern has a role in accommodating various message channel options and the use cases that arise from those choices.
When dealing with external systems, some transports and protocols are limited to the polling consumer pattern.
The JMS model enables both polling and event-driven message reception.
This section covers the reasons to consider the event-driven approach and how the Spring Framework supports it, ultimately with message-driven plain old Java objects (POJOs) that keep your code simple and unaware of the JMS API.
Earlier chapters made it clear that receiving messages is usually more complicated than sending them.
Even though the receiving part of the JmsTemplate example looks as simple as the sending part, it’s important to recognize that in that example, the receive operation is synchronous.
The JmsTemplate receive operations return as soon as a message is available at the JMS.
That means that if no message is available immediately, the operations may block for as long as indicated by the receiveTimeout value.
When relying on blocking receive operations, such JmsTemplate usage is an example of the polling consumer pattern.
In an application in which an extremely low volume of messages is expected, polling in a dedicated background thread might be okay.
But, as we mentioned earlier in the book, most applications using messaging would prefer to have event-driven consumers.
Support for event-driven consumers could be implemented on top of the simple polling receive calls, though all but the most naive implementations would quickly become complex.
A proper and efficient solution requires support for concurrent processing of the received messages.
Such a solution would also support transactions, and ideally, it would accommodate a thread pool that adjusts dynamically according to the volume of messages being received.
Those same requirements apply to any attempt to adapt an inherently polling-based source of data to an event-driven one.
Obviously, that’s a common concern for many components in Spring Integration.
As far as the JMS adapters are concerned, the crux of the problem is that the invocation of the JMS receive operation must be performed within the context of the transaction.
Then, if the subsequent handling of the message causes a failure, that’s most likely a reason to roll back the transaction.
Some other consumer may be able to process the message, or perhaps this same handler could handle the message successfully if retried after a brief delay.
For example, some system that it relies on might be down at the moment but will be available again shortly.
If a JMS consumer rolls back a transaction, then the message won’t be removed from the destination; that’s what enables redelivery.
But if the Exception is thrown by the handler in a different thread, it’s too late: the JMS consumer has already performed its role, and by passing off the responsibility to an executor that invokes the handler in a different thread, it would’ve returned successfully after that handoff.
It would be unable to react to a rollback based on something that happens later, downstream in the handler’s processing of the message content.
You’re probably convinced that implementing an asynchronous message-driven solution isn’t trivial.
It’s the type of generic, foundational code that should be provided by a framework so developers don’t have to spend time dealing with the low-level threading and transaction management concerns.
Spring provides this support for JMS with its MessageListener containers.
Let’s look at a modified version of the earlier Hello World example.
Nevertheless, you might be thinking we added a lot more code to the example, and we’re back to dealing with some JMS API code directly.
For example, we provided an implementation of the JMS MessageListener interface, and we’re catching the JMSExceptions to convert into RuntimeExceptions ourselves.
In the next section, we take things two steps further.
Second, we refactor the example to use declarative configuration and a dedicated Spring XML schema.
The code and configuration for asynchronous reception can be much simpler.
One goal for simplification should be to reduce the dependency on the JMS API.
It’s straightforward: the adapter implements the MessageListener interface, but it invokes operations on a delegate when a message arrives.
That instance to which it delegates can be any object.
The previous code could be updated with the following replacement for the registration of the listener:
An even better option is to use configuration rather than code.
Spring provides a jms namespace that supports the configuration of a container and adapter in a few lines of XML:
Many configuration options are available on both the listener-container and listener elements, but the preceding example provides a glimpse of the simplest possible working case.
The XML schema is well documented if you’d like to explore the other options.
Our goal here is to provide a sufficient level of background information so you can appreciate that Spring Integration builds directly on top of the JMS support within the base Spring Framework.
At this point, you should have a fairly good understanding of that support.
We now turn our focus back to Spring Integration to see how it offers an even higher-level approach.
As with most adapters in Spring Integration, a unidirectional channel adapter and a request/reply gateway are available.
Because it’s considerably simpler, we begin the discussion with the unidirectional channel adapter.
Spring Integration’s outbound JMS channel adapter is a JMS message publisher encapsulated in an implementation of Spring Integration’s MessageHandler interface.
That means it can be connected to any MessageChannel so that any Spring Integration Messages sent to that channel are converted into JMS Messages and then sent to a JMS Destination.
The JMS Destination may be a Queue or a Topic, but from the perspective of this adapter implementation, that’s a configuration detail.
If you look at its implementation, you’ll see that it builds completely on the JMS support of the underlying Spring Framework.
The most important responsibilities are handled internally by an instance of Spring’s JmsTemplate.
Most of the code in Spring Integration’s adapter handles the various configuration options, of.
Sending JMS messages from a Spring Integration application which there are many.
As far as most users are concerned, even those configuration options are handled by the XML namespace support.
In most cases, only a small subset of those options would be explicitly configured, but there are many options for handling more nuanced usage requirements.
We walk through several of these in a moment, but first let’s look at a typical configuration for one of these adapters:
That looks simple enough, right? Hopefully so, and if you can rely on the defaults, then that’s all you need to configure.
If you want to use a Topic instead of a Queue, be sure to provide the pub-sub-domain attribute with a value of true, as in the following example:
Sometimes it’s not practical to rely on just the name of the JMS destination.
In fact, it’s common that the queues and topics are administered objects that developers should always access via JNDI lookups.
Fortunately, you can rely on the Spring Framework’s ability to handle that.
Instead of using the destination-name attribute, you could provide a destination attribute whose name is a reference to another object being managed by Spring.
That other object could then be a result of a JNDI lookup.
Although it’s perfectly acceptable to define that FactoryBean instance as a low-level bean element, there’s XML namespace support for much more concise configuration options, as shown here:
Access by name is often sufficient in development and testing environments, but JNDI lookups might be required for a production system.
In those cases, you can manage the configuration excerpts appropriately by using import elements in the configuration or other similar techniques.
The important factor is that you don’t need to modify any code to handle those different approaches for resolving JMS destinations.
Fortunately, the configuration of the ConnectionFactory and Destinations can be shared across both the sending and receiving sides.
Likewise, for commonly configured references, such as these destinations, there is consistency between the inbound and outbound adapters.
In the next section, we focus on the receiving side.
We begin with the inbound channel adapter that serves as a polling consumer.
You can define an inbound channel adapter that acts a polling consumer or one that acts as an event-driven consumer.
It accepts a destination-name attribute for the JMS Queue or Topic.
Here’s a simple example of an inbound channel adapter that polls for a JMS message every three seconds:
Like the outbound version, if you’re specifying a topic name rather than a queue name, you should also provide the pub-sub-domain attribute with a value of true.
If instead you want to reference a Queue or Topic instance, you can use the destination attribute in place of destination-name.
This is a common practice when defining this adapter alongside Spring’s JNDI support, as shown previously in the outbound channel adapter examples.
The following is an example of the corresponding inbound configuration:
As mentioned earlier, polling is rarely the best choice when building a JMS-based solution.
Considering that the underlying JMS support in the Spring Framework enables asynchronous invocation of a MessageListener as soon as a JMS message arrives, that’s almost always the better option.
The only exceptions might be when you want to configure a poller to run infrequently or only at certain times of the day.
If the poller is limited to run at certain times of the day, you’d most likely use the cron attribute on a poller element.
The basic configuration will look the same, but there’s no longer a need to define a poller:
It may seem odd that, unlike most adapters you’ve seen, the element doesn’t include inbound in its name.
Request-reply messaging that this channel adapter is reacting to inbound JMS messages that arrive at the given queue or topic.
It sends those messages to the channel referenced by the channel attribute.
The discussion and examples in this chapter have thus far focused on unidirectional channel adapters.
On the sending side, we haven’t yet discussed the case where we might be expecting a reply, and on the receiving side, we haven’t yet discussed the case where we might be expected to send a reply.
We saw that Spring Integration’s inbound JMS channel adapters can receive messages with either polling or messagedriven behavior.
On the other hand, the outbound channel adapter can be used to send messages to a JMS destination, be it a queue or a topic.
In both cases, the Spring Integration message is mapped to or from the JMS message so that the payload as well as the headers can correspond to the JMS message’s body and properties respectively.
This section introduces Spring Integration’s bidirectional gateways for utilizing JMS in a request-reply model of interaction.
Much of the functionality, such as mapping between the JMS and Spring Integration message representations, is the same.
The difference is that these request-reply gateways are responsible for mapping in both directions.
As with the unidirectional channel adapter discussions, we begin with the outbound side.
Whereas earlier we could describe the outbound behavior as solely responsible for sending messages, in the gateway case, there is a receive as well, assuming that the JMS reply message arrives as expected.
The simplest way to think of the outbound gateway is as a send-and-receive adapter.
The request-channel and reply-channel attributes refer to Spring Integration MessageChannel instances.
Any Spring Integration message that’s sent to the request channel will be converted into a JMS message and sent to the gateway’s request destination (in this context, destination always refers to a JMS component, and channel is the Spring Integration message channel)
Because the gateway must manage sending and receiving separately, many of its attributes are qualified as affiliated with either the request or the reply.
The reply-channel is where any JMS reply messages are sent after they’re converted to Spring Integration messages.
That attribute is optional, but it’s common to leave it out.
If you don’t provide a specific destination for that, then it’ll handle creation of a temporary queue for that purpose.
This assumes that wherever these JMS messages are being sent, a process is in place to check for the JMSReplyTo property so it knows where to return a reply message.
We discuss the server-side behavior in the next section when we cover the inbound gateway.
For the time being, we discuss this interaction with a hypothetical server side where we assume such behavior is in place.
The JMSReplyTo property is a standard part of the message contract and is defined in the JMS specification.
Therefore, it’s commonly supported functionality for server-side JMS implementations that accept request messages from a sender who is also expecting reply messages.
You must be sure that you’re sending to a destination that’s backed by a listener implementation with that behavior.
The inbound channel adapter we discussed earlier would not be a good choice because, as we emphasized, it’s intended for unidirectional behavior only.
On the other hand, the inbound gateway we discuss in the next section would be a valid option for such server-side request-reply behavior.
The core Spring Framework support for message-driven POJOs also supports the JMSReplyTo properties of incoming messages as long as the POJO method being invoked has a nonvoid and non-null return value.
Like the outbound gateway, Spring Integration’s inbound gateway for JMS is an alternative to the inbound channel adapters when request-reply capabilities are required.
Perhaps the quickest way to get a sense of what this means is to consider that this adapter covers the functionality we described in abstract terms as the server side in the previous section.
The outbound gateway would be the client side as far as that discussion is concerned.
The inbound gateway listens for JMS messages, maps each one it receives to a Spring Integration message, and sends that to a message channel.
Thus far, that’s no different than the role of an inbound channel adapter.
The difference is that the message channel in this case would be the initiating end of some pipeline that’s expected to produce a reply at some point downstream.
When such a reply message is eventually returned to the inbound gateway, it’s mapped to a JMS message.
The gateway then sends that JMS message to the reply destination.
That particular JMS message fulfills the role of the reply message from the client’s perspective.
The reply destination is an example of the return address pattern.
It may have been provided in the original message’s JMSReplyTo property, and if so, that takes precedence.
If no JMSReplyTo property was sent, the inbound gateway falls back to a default reply destination, if configured.
As with the request destination, that can either be configured by direct reference or by name.
Request-reply messaging request message nor a configured reply destination, then an exception will be thrown by the gateway because it would have no way of determining where to send the reply.
That description of the inbound gateway’s role probably sounds like it involves a complex implementation.
Keep in mind that it builds directly on top of the underlying Spring JMS support that we described earlier.
Now you can probably appreciate why we went into considerable detail about that underlying support.
As a result, you already have a basic understanding of how the inbound gateway handles the serverside request-reply interaction.
As with any Spring Integration inbound gateway, once it maps to a Spring Integration message, it sends that message to a message channel.
What makes each gateway unique is what it receives and how it maps what it receives into a Spring Integration message.
Now that you understand the role of the inbound gateway for JMS, let’s look at an example.
You may have noticed that no reply-channel attribute is present.
This attribute is an optional value for inbound gateways in general.
If it’s not provided, then the gateway creates a temporary, anonymous channel and sets it as the replyChannel header for the message that it sends downstream.
Add that attribute if for some reason the JMS ConnectionFactory you need to reference has a different bean name.
As you might expect, knowing that we’re building on top of Spring’s message listener container, a number of other attributes are available.
Many of them are passed along to that underlying container.
For example, you might want to control the concurrency settings.
At that point, each of those extra consumers can have up to three idle tasks—those where no message is received within the receive timeout of 5 seconds, at which point the consumer will be cleared.
This example shows that various settings of the underlying message listener container can be configured directly on the XML element that represents the Spring Integration gateway.
The preceding attributes are a small subset of all the configurable properties of the container.
When defining your elements in an IDE with good support for XML, such as the SpringSource Tool Suite, you can easily explore the entire set of available attributes.
You saw how such adapters can be used on both the sending side and the receiving side.
You saw the unidirectional channel adapters as well as the bidirectional gateways that enable requestreply messaging.
Next, we consider the scenario in which the JMS messaging occurs between two applications that are both using Spring Integration.
In the previous sections, you saw the inbound gateway and the outbound gateway.
Both play a role in supporting request-reply messaging, but they were discussed separately thus far.
That’s because each can be used when you’re limited in the assumptions you can make about the application on the other side.
As you might expect, the two gateways can work well together when you have Spring Integration applications on both sides.
Figure 9.4 captures this situation by reusing one of the diagrams from the introductory chapter, this time labeled specifically for JMS.
In the scenario depicted in the figure, it will obviously be necessary to map between the Spring Integration messages used in each application and the JMS messages that are being passed between the applications.
We saw several examples of how the adapters use Spring’s JMS MessageConverter strategy to convert to and from JMS messages.
So far, the examples have mapped between the JMS message body and the Spring Integration message payload.
Likewise, the JMS properties have mapped to and from the Spring Integration message headers.
These are by far the most common usage patterns for message mapping with JMS, and they make minimal assumptions about the system on the other side of the message exchange.
In a particular deployment environment, though, it might be well known that Spring Integration–based applications exist on both sides of the JMS destination.
One Spring Integration application would act as a producer, and the other would act as a consumer.
Request Destination Figure 9.4 A pair of gateways, one outbound and the other inbound.
Those two applications share access to a common JMS broker and a pair of destinations.
If that’s the nature of the deployment model, it may or may not be desirable to pass the entire Spring Integration Message instance as the JMS message body.
The default mapping behavior would obviously work in such an environment, but if you want to “tunnel” through JMS instead, for some reason, then you can override the default configuration.
When passing the Spring Integration Message as the JMS message body, it’s necessary to have a serialization strategy.
The standard Java serialization mechanism is one option, because Spring Integration Messages implement the Serializable interface.
One thing to keep in mind when choosing that option is that nonserializable values that are stored in the message headers won’t be passed along because they can’t be serialized with that approach.
An even more important factor to keep in mind is that Java serialization requires that the same class be defined on both the producer and the consumer sides.
Not only must it be the same class, but the version of the class must be the same.
At first, it seems convenient to pass your domain objects around without any need to think about conversion or serialization, but it’s almost always a bad idea in reality.
By requiring exactly the same classes to be available to both the producer and the consumer, this approach violates the primary goal of messaging: loose coupling.
Even if you control both sides of the messaging exchange, the fewer assumptions one side makes about the other, the more flexible the application will be.
As any experienced developer knows, some of the most regrettable assumptions are those made about the future of an application.
For example, if an application needs to evolve to support multiple versions of a certain payload type, reliance on default serialization to and from a single version of a class will be a sure source of regret.
With these twin goals of reducing assumptions and increasing flexibility in mind, let’s consider some other options for serializing data.
Probably the most common approach in enterprise integration is to rely on XML representations.
Spring Integration provides full support for that option with the object-to-XML marshaller and XMLto-object unmarshaller implementations from the Spring Framework’s oxm module (see chapter 8 for more detail)
Another increasingly popular option for serializing and deserializing the payload is to map to and from a portable JSON representation.
The advantage of building a solution based on either XML or JSON instead of Java serialization is that the system can be much more flexible.
It’s not necessary to have the same version on both sides.
In fact, as long as the marshaller and unmarshaller implementations account for it, the producer and consumer sides may even convert to and from completely different object types.
Regardless of the chosen serialization mechanism, you must configure a MessageConverter on the gateway any time you don’t want to rely on the default, which uses Java serialization.
Generally, we recommend avoiding the tunneling approach because having the mapping behavior on both sides promotes loose coupling.
If a Spring Integration payload is a simple string or byte array, then it’ll map to a JMS TextMessage or BytesMessage respectively when relying on the default MessageConverter implementation.
The default conversion strategy also provides symmetric behavior when mapping from JMS.
A TextMessage maps to a string payload, and a BytesMessage maps to a byte array payload.
But if your Spring Integration payload or JMS body is a domain object, then it’s definitely important to consider the degree of coupling because the default MessageConverter will rely on Java serialization at that point.
Spring Integration provides bidirectional XML transformers in its XML module (again, see chapter 8 for more details), and it provides bidirectional JSON transformers in the core module.
Both the XML and JSON transformers can be configured using simple namespace-defined elements in XML.
You can provide the object-to-XML or object-to-JSON transformer upstream from an outbound JMS channel adapter or gateway, and you can provide the XML-to-object or JSON-to-object transformer downstream from an inbound JMS channel adapter or gateway.
One advantage of relying on the transformer instances is that you can reuse them in multiple messaging flows.
For example, you might also be receiving XML or JSON from an inbound file adapter, and you might be sending XML or JSON to an outbound HTTP gateway.
If such opportunities for reuse aren’t relevant in your particular application, you may prefer to encapsulate the serialization behavior.
You can rely on any implementation of Spring’s MessageConverter strategy interface.
A similar JSONbased implementation was introduced in Spring 3.1, but if you're using an earlier version, it  wouldn’t be difficult to implement in the meantime.
You can provide any other custom logic or delegate to any other serialization library that you choose.
If going down that path, you would define the chosen MessageConverter implementation as a bean and then reference it from the channel adapter or gateway’s messageconverter attribute.
Those requirements usually support one key concern: messages can’t be lost even if errors occur while processing them.
In the worst case, after a number of attempts, a message should be delivered to a dead letter queue.
The primary goal when managing transactions in general is to ensure that data is always accounted for.
No party should relinquish responsibility until another party has assumed responsibility.
In a messaging interaction, the two parties that may assume responsibility at any given time are the producer and the consumer.
Therefore, when working with JMS, it’s important to understand exactly how and where to apply transactions on the producer side as well as on the consumer side.
It’s also important to consider whether the unit of work that should be transactional includes not only interaction with the JMS message broker but also some other transactional resource, such as a database.
Finally, it’s important to understand the advanced options (those that may be outside of the JMS specification) that are available with a given broker, such as the retry policies.
Such options are beyond the scope of this book, but the JMS provider’s documentation is a good place to start.
If you’re using ActiveMQ, its options are covered in detail in ActiveMQ in Action.
Here, we cover the general aspects of JMS transactions and specifically how they can be configured on Spring Integration’s JMS channel adapters and gateways.
Let’s revisit the creation of a JMS Session, the object that represents a unit of work in the JMS API.
As you saw at the beginning of this chapter, the Session acts as a factory for creating the messages as well as the producer and consumer instances.
When creating a Session from a Connection, you must decide whether that Session should be transactional.
Here’s a simple example using the JMS API directly (without Spring):
That boolean value (true in this example) indicates that the Session should be transactional.
Note that the second argument indicates the acknowledge mode for the Session.
If you provide a value true to indicate that you want a transactional Session, the acknowledge mode value will be ignored.
On the other hand, if you provide false to indicate that you don’t want a transactional Session, then the value will play a role.
We briefly return to the acknowledge modes in a moment, but first let’s discuss the publishing side where things are straightforward.
When publishing JMS messages, the transactional boolean value passed when creating the Session plays an important role.
The Session is used as a factory for creating any MessageProducer instances, and the transactional characteristics of those producers would reflect that boolean flag.
If the Session isn’t transactional, then publishing a message will truly be a fire-and-forget operation, and there will be no way to undo the publish operation or to group multiple operations into a single unit of work.
In contrast, when the transactional setting is enabled on a Session, any messages published with that Session are only made available within the broker upon committing the transaction.
That means that if something goes wrong, any messages published with that Session in the scope of that transaction can be rolled back instead.
The commit() and rollback() methods are available directly on the Session.
When using a transactional Session, similar behavior is available on the receiving side.
A MessageConsumer created from a transactional Session won’t permanently take its messages from the broker until the Session’s transaction is committed.
If something goes wrong after receiving a message, calling rollback() on the Session will undo the reception.
That means any messages received within the scope of the rolled-back transaction may be eligible for redelivery, potentially to a different consumer.
As mentioned previously, when the transactional boolean value is false, the acknowledge mode plays a role in the message reception.
The valid integer values for acknowledge modes are defined as constants on the Session class.
You can set the value to CLIENT_ACKNOWLEDGE instead, which indicates that you expect the client consuming the JMS messages to explicitly invoke the acknowledge() method on those messages.
An acknowledge() call on a single Message instance will acknowledge all previously unacknowledged Messages that have been consumed on that same Session.
It differs in that the acknowledgments may be sent lazily, allowing for the chance that duplicate messages might be received in the case of certain failures within the window of vulnerability.
The details of acknowledge modes are covered in the JMS specification.
You can also find some information in Spring’s Javadoc and XML schema documentation.
After our detailed discussion earlier in this chapter, you know that the Spring Framework’s JMS support includes a component that serves as a MessageListener container.
Session management for message consumption is one of the responsibilities of such a container.
In consideration of the mutual exclusivity between the boolean value to indicate a transactional Session and the acknowledge mode to be used otherwise, the schema-based configuration of the listener container rolls these options into a single acknowledge property.
It can take a value that corresponds to any of the three acknowledge modes defined by the JMS specification, or it can be set to transacted.
When configuring Spring Integration’s inbound channel adapter or gateway for JMS, you’ll see the same attribute.
It accepts any value from the same enumeration: auto, client, dups-ok, or transacted.
One important final note: if no value is provided, the default will be auto acknowledge mode, and hence Sessions won’t be transactional.
Transactions add overhead, so if you’re sending nonessential event data where occasional message loss isn’t a problem, the default might be fine.
But if your messages are carrying document data to be processed, be sure to consider this setting carefully.
The Spring Framework provides an abstraction for transaction management that makes it easy to switch between different strategies for handling transactions within an application.
Most likely, that change is limited to replacing a single bean because the code depends on the abstraction.
Furthermore, because transaction support is typically handled by interception within a proxy, end user code is almost never affected directly.
It defines the methods you’d expect for managing transactions regardless of the details of the underlying system:
As far as transaction management strategies are concerned, broadly speaking, there are two main choices: local or global transaction management.
What we mean by local is that transactions can be managed directly against the transactional resource without.
In the case of JMS, the local transactional resource would be the JMS Session instance.
As you saw earlier, a Session can be created with a simple boolean flag to indicate whether it should be transactional.
The JMS Session class also defines commit and rollback methods.
Another example that might clarify this terminology is the JDBC Connection class.
It’s also a local transactional resource because it provides commit and rollback methods.
Whereas the JMS Session is transactional upon creation (assuming the flag is true), the JDBC Connection has a setAutoCommit() method such that passing a value of false will disable autocommit mode.
Explicit commit (or rollback) is then required, so the Connection is then transactional in that multiple operations can be handled within the scope of a single unit of work.
A JMS Session and a JDBC Connection are both good examples of resources that enable local transaction management.
Earlier we mentioned a common case where a unit of work spans a JMS message exchange and one or more database operations.
In those cases, it might be tempting to move toward a global transaction solution.
You could rely on XA-capable resources and a Java Transaction API (JTA) implementation that supports two-phase commit as the transaction manager, but it’s not always necessary to go that far.
A distributed transaction in the more general sense means more than one resource is involved.
Rather than assuming you need to use XA and two-phase commit along with the extra overhead they bring, you should consider all of the options.
It might be possible to find some middle ground such that you can maintain the simplicity of the local transaction management but chain multiple local resources together.
The trade-off is a requirement to carefully consider the order of operations and how to manage certain rare failure scenarios.
One of the most common patterns when dealing with messaging is to have a process that begins with a received JMS message but then invokes a database operation based on that message’s content.
For example, imagine an order being received and then inserted into the database.
The JMS Session is one local transactional resource, and the JDBC Connection is another.
If a failure occurs while processing the message content or inserting the database record, then an exception could trigger a rollback of both the JDBC and JMS transactions.
The JMS message would be eligible for redelivery, and the database would remain unchanged.
If everything goes smoothly, the database insert is committed, and the JMS message is permanently removed from its destination.
The rare situation that could lead to a problematic result is if the database transaction commits, but for some reason, such as a system-level failure, the JMS transaction rolls back afterward.
In that case, the JMS message could potentially be redelivered, and so the application must be able to handle such duplicates.
If the database operation is idempotent, then there might not be any problem at all.
Summary idempotent operation, a record would be ignored if the same data already exists, likely based on a where clause condition.
If the operation isn’t inherently idempotent, there might be more work involved to add similar logic at a higher level of the application.
Perhaps any message where the JMSRedelivered property is true can be checked against the database prior to invoking the normal insert operation.
If it’s recognized as a duplicate that has already been handled successfully, the message could be ignored, and its JMS transaction could be committed so it won’t be redelivered anymore.
Keep in mind that while that sounds like added overhead, such redeliveries would likely be extremely rare.
Depending on the particular application structure, these types of solutions might be simpler and add less overhead than a full XA twophase commit option.
For much more detail on these types of distributed transaction patterns and the trade-offs involved, refer to “Distributed Transactions in Spring, with and without XA,” by Dr.
David is the lead of the Spring Batch project and a committer on several other Spring projects, including Spring Integration.
Considering the central role that JMS plays in many enterprise Java applications, we wanted to make sure it was clear where Spring Integration overlaps with JMS and where the two can complement each other.
You saw the relationship between the two message structures and how to map between them.
You also learned how the underlying Spring Framework provides base functionality that greatly simplifies the use of JMS and how Spring Integration takes that even further with its declarative configuration and higher level of abstraction.
After walking through both the unidirectional channel adapters and the request-reply gateways, we dove into a bit of depth regarding transactions.
At this point, you should be well prepared to build Spring Integration applications where none of the code is directly tied to the JMS API while still benefiting from the full power of whatever underlying JMS provider you choose.
In chapter 10, we cover Spring Integration’s support for yet another type of messaging system, one that you most likely use on a daily basis: email.
But this isn’t a book about email etiquette, nor do we want to settle the age-old question of top-posting versus bottom-posting; we’re interested only in the implications of using email as an enterprise application integration tool.
Besides its primary role as a means of communication between people, email is also a useful interaction medium for applications, either with users—by sending notifications or receiving requests—or with other applications.
Email is a complex messaging mechanism, supporting broadcast, data delivery as attachments, and store-and-forward transmission with message relaying, and allowing a choice of message receiving protocols, Email-based integration.
If JMS is the first thing that comes to your mind when associating Java and messaging, then chances are that associating internet and messaging will make you think of email.
Sending email either event-driven (IMAP push) or polling (POP, IMAP pull)
Also, email is continuously evolving in response to changes in technology and challenges such as spam and fraud, incorporating more sophisticated mechanisms for secure data transmission, authentication, and authorization, to name just a few areas of interest.
In this chapter, we provide an overview of the most significant use cases that require email support, and you’ll learn how you can use Spring Integration to add it to your application.
We show you how to send and receive email, the various options for doing so, and how to choose among them.
Let’s start with sending emails, the more typically encountered use case.
Event-driven interaction isn’t something that happens only between applications and systems; humans have been exchanging messages since the dawn of history, and electronic mail is a way of using modern technology for implementing an ancient communication pattern.
But incorporating it in working applications opens the way for interaction workflows that improve and transform the user experience.
For example, you may be waiting for a shipment (maybe your print copy of Spring Integration in Action), but you don’t know the exact date and time when you have to be home to receive it.
You may try to find out when you’re supposed to be home by repeatedly checking the shipper’s site, but this can become a tedious exercise, and often a futile one, because the information may change frequently.
Now you may carry on with your own business, assured that you’ll be informed without having to repeatedly check the shipper’s site for a status report.
You can find many other examples where a notification system is a valuable add-on to an application, and for a notification system to be efficient, it must use a mechanism that allows easy access on the recipient’s side.
This assumption holds true for email; it’s widely available, either as a public service or as part of a corporate communication infrastructure, and because mobile devices have become so popular, email is accessible even when you’re on the go.
Although email is a widely used notification method, there are other ways of sending short notifications to users, such as Short Message Service (SMS) and social networking media, such as tweets and chat.
Email differentiates itself from these other personal messaging systems through features such as the ability of carrying attachments, broadcast, store-and-forward delivery, delivery status availability, and failure notification, which make it interesting for use as an intersystem message exchange transport.
Email isn’t a typical first choice for integrating applications hosted on the same machine or deployed in the same local network, where specifically designed messaging middleware, accessible via Java Message Service (JMS) or Advanced Message Queuing Protocol (AMQP), is a far better fit when considering any relevant criterion (performance, throughput, transaction capabilities, delivery guarantees)
But emails can be used as an integration mechanism in other situations, for example.
They also don’t require as much special setup for remote access, like exposing ports and addresses, as a message broker may normally require.
A complete solution involves a chain of Spring Integration endpoints with specific roles, typically similar to what you can see in figure 10.1
Because the key component of this chain (and the only one whose presence is mandatory) is the channel adapter, we look at that first.
The operation of sending email messages from your application is fairly straightforward and intuitive, considering that it’s based on a pattern you’re already familiar with: the outbound channel adapter.
As a first step, you must configure such an adapter, which can be as simple as in the following code snippet, where the username and password attributes describe the login credentials for using the Simple Mail Transfer Protocol (SMTP) services provided by the host:
What you send in a message can be, in the simplest case, equivalent to the following snippet; the outbound channel adapter composes a message with the payload as the body and the recipients lists (to, cc, bcc) and subject as header values:
Figure 10.1 Sending email requires a channel adapter and includes optional transformers for preparing outbound messages.
We said equivalent because sending mail in a programmatic fashion isn’t the only way to send it and perhaps isn’t even the most common: email messages are usually produced by upstream endpoints or by a publisher interceptor.
The end result, regardless of what led to it, is that a Spring Integration message with a mail-specific payload and mail-specific headers is sent to a channel to which an outbound email channel adapter listens, and then composes an email message based on payload and header content and sends it.
One aspect to consider when composing messages for the email outbound channel adapter is that it supports only a limited number of payload choices.
A string is the most straightforward alternative and can be either a predefined message or content generated from a template.
Another type of payload accepted by the channel adapter is a byte array, in which case the resulting email message adds the binary content as an attachment.
You can use additional headers for describing the attachment (filename, content type, and so on)
A complete list of the message headers and how they’re used for constructing the outgoing message is found in table 10.1
The header name refers to string constants defined in the MailHeaders helper class.
To understand more advanced ways of composing email messages and configuring outbound email channel adapters, we need to look closer at the innards of the mailsending process.
Under the hood, the Spring Integration email outbound channel adapter is based on the JavaMail API and on additional utilities provided by the Spring Framework.
This mechanism is used for finely tuning the configuration parameters, adding connectivity features such as Secure Sockets Layer (SSL) support for sending messages, as shown in the following snippet.
For a complete list of the configuration options and their meanings, please consult the JavaMail documentation:
For applications that need to send email (especially the ones that don’t use Spring Integration), using the JavaMail API directly can become a cumbersome exercise, which involves operating directly with the low-level components of the API and direct manipulation of resources.
To simplify the general process of sending and receiving email, the Spring Framework provides a higher-level API for sending emails, the JavaMailSender.
When receiving messages, dealing directly with such objects is fairly simple because they provide access to all the properties of the email (as explained in the following section)
Spring therefore provides the JavaMailSender abstraction, an intermediate layer that uses a Spring-specific object, the MailMessage, as an input and interacts with the infrastructure for sending messages, as shown in figure 10.2
For more details, we encourage you to read the email section of Spring in Action by Craig Walls (Manning, 2011)
First, users can inject a JavaMailSender object directly in the outbound channel adapter, which can also act as a general configuration for sending emails (rather than, for example, individually configuring each channel adapter)
Second, the outbound channel adapter accepts two other types of object as payload: the raw JavaMail MimeMessage and the Spring-specific.
MailMessage, which allows developers to compose messages using the Spring Framework utilities and send them through the channel adapter.
Sending email is usually part of a larger chain of events, and to illustrate that, let’s consider a business example.
You saw in chapter 6 that as soon as the application is notified of a flight schedule change, it must inform all the customers who are booked on that flight.
The upstream service activator takes care of searching for accounts and producing one or more Notification objects.
One of the strengths of Spring Integration is decoupling between components, so it may not seem too far a stretch that a service activator won’t want to produce dedicated MailMessage instances, but domain-specific notifications, leaving it to the downstream components to translate them into email-ready instances.
You may also need this functionality when supporting more than one kind of notification type by the application; besides email, you may have SMS messages or even direct phone calls.
In such cases, an upstream component will create a generic Notification instance, which is sent to a publish-subscribe channel, with the email-sending component being just one of the subscribers.
The pipes-and-filters architecture of Spring Integration has a simple solution for this: all you need to do is include a message transformer that converts the message with a plain old Java object (POJO) payload into one of the four compatible payloads, as shown in figure 10.1
If the transformer logic is only dealing with the payload (for example, creating a string message based on the content of the Notification object), you can use a dedicated header enricher component for populating the message headers with appropriate values.
A header enricher can use static values or Spring Expression Language (SpEL) expressions evaluated against the message to be enriched.
In general, the best practice is to insert the header enricher first because the transformer may remove information that’s required for populating the headers; for example, the recipient of the message may be one of the properties.
Figure 10.2 Spring Integration uses the mail sending layer provided by the Spring Framework proper.
The components from each layer, as well as the mail message abstractions, are shown on the right side.
A complete example, including the transformer and the optional header enricher, is shown in the following snippet.
It’s time now to discuss how to handle inbound email messages.
Sending notification emails is the most common use case, but a significant number of applications also support an email-driven application flow.
Sending documents as email attachments can be an alternative to uploading them on a web page, especially when you don’t want to put up with creating a dedicated website.
Also, emails carry the identity of the sender, which works pretty well in request-reply scenarios, because the identity of the sender and the reply-to information is communicated as part of the email.
And, as we mentioned in the previous section, email can serve as transport for carrying data over the web when using a message broker would create too many logistical and infrastructural issues.
For example, your application may offer users the opportunity to perform flight status checks through email.
The company provides an email address to which customers send an email with the keyword STATUS and the flight number in the subject, and the application replies with the flight information for today.
Our application acts as an email client, reading the contents of the mailbox and processing every incoming message as a request.
The general structure of an emailreceiving application is shown in figure 10.3
The inbound channel adapter creates messages with a JavaMail MimeMessage payload, and the messages are sent on the adapted channel for further processing downstream.
For separating concerns in the application, the MimeMessages are converted to domain objects downstream, typically using a transformer.
The first step for creating such an application is deciding how to receive emailswhat kind of a channel adapter you’d like to include.
You can either poll the mailbox or be notified when new messages arrive, so let’s examine these two options before we discuss how to process inbound messages.
Polling is the most basic email-receiving strategy, and it’s supported by all mail servers.
A client connects to the mailbox and downloads any unread messages that have arrived since the last polling cycle.
A polling channel adapter can be configured as in the following example:
The protocol that will be used in a particular case depends on the setup of the mail server where the incoming mailbox is located, and it’s not typically left to you as an option.
Mailboxes are set up as a temporary storage, and the assumed permanent destination of the messages is the local store provided by the email client.
This means the server is somewhat aware that a message has been read, but only for the duration of a session.
In Spring Integration, an email client session is started when the application starts and is closed on shutdown, so any messages that aren’t deleted from the server before shutdown may.
Figure 10.3 Receiving emails requires a channel adapter and includes optional transformers for extracting message content.
Because there’s no such concept as a transaction, a failure that occurs between the moment the email has been read and the moment it has been processed may result in the message being lost.
Moreover, using the mailbox in a multiple-client, publishsubscribe scenario would be problematic if one of the subscribers started deleting emails.
Because there’s no single best choice here, Spring Integration forces you to state explicitly which one of the two values you want to set, essentially deciding between providing a contingency on loss or duplication.
Mailboxes are the permanent storage for emails, and email clients, by default, provide a view of the remote mailbox state rather than retrieving content locally.
Because the remote mailbox maintains state, deleting messages isn’t necessary to avoid duplicate reception.
In the case of IMAP, you can also decide whether you want to mark processed messages as read.
Although polling is available for any email server, for particular server configurations it’s also possible to set up an event-driven inbound channel adapter for receiving emails.
This functionality is available when connecting to IMAP servers that support IMAP-Idle, a feature that allows clients to request real-time notifications on mailbox changes.
As a result, clients don’t have to poll the server for new messages; they’re notified when new messages are added to the mailbox.
This approach is lighter on resource consumption and traffic because the application doesn’t have to actively check the server for new messages.
It also improves responsiveness because applications are notified as soon as a message arrives, without the lag introduced by a polling cycle.
But it’s important to note that the IMAP-Idle command has a timeout of, at most, 30 minutes, which means that clients have to reissue it periodically.
For using the IMAP-Idle message retrieval strategy, all you need to do is use a different type of channel adapter:
Whether it’s polling or event-driven, the inbound email channel adapter will create a message with a payload of the JavaMail’s Message type.
To handle these messages in the application, you must transform them to a more neutral format, such as a string or a POJO.
Once you configure your inbound channel adapter, the next step is to process the received messages downstream.
For handling the lower-level JavaMail Message objects in the domain-aware services, it’s typical to include a transformer before the objects enter the application flow.
The simplest solution is to use a SpEL-based transformer that’s evaluated against the JavaMail Message payload of the inbound message.
For example, the transformer in the following snippet extracts the message body and creates a new Spring Integration message, using it as a payload.
You can write more complex SpEL expressions to include other properties of the message, such as the subject, sender, or date:
Spring Integration provides an out-of-the box transformer you can use for extracting the email body or the binary content into a string payload.
One of the benefits of using it is that it also extracts the other email properties as message headers, mirroring what happens when you send a message.
Review table 10.1 for a reminder of the mail property/message headers mapping.
Here’s an example where it replaces the previous SpEL-based transformer:
Then you can add your implementation to the chain using a regular custom transformer setup:
The resulting transformer will also extract the mail properties as header values, as in the case of the string transformer.
One of the advantages of the latter two approaches over the SpEL transformer is that they automatically provide access to entries, such as the originator of the message and the reply address, which may be useful when handling the message and are important if you want to implement an ad hoc gateway that sends back an email response to an inbound email request.
For implementing such a round-trip solution, all you need to do is create a sequence of endpoints that starts with an inbound channel adapter and ends with an outbound channel adapter.
Email is a ubiquitous modern communication technology that can be used for human interaction, for allowing applications to send notifications or receive requests via email from users, and as a transport mechanism, especially between remote applications.
On criteria such as performance and reliability, email can’t compete with dedicated messaging middleware, but it has some qualities that make it attractive in internet communication: it’s widely available and accessible to users, it can transfer information over the internet without requiring custom firewall access, and it supports small to mid-sized attachments.
Spring Integration provides support for sending and receiving emails through dedicated channel adapters, which cover all the major connectivity options and email transfer protocols (POP3, IMAP, SMTP)
It provides out-of-the box support for handling the most typical use cases, such as text messages (including HTML) and file attachments; and special situations, such as complex email structures and conversion to POJOs, can be easily addressed by including customized transformers in the processing chain.
This wraps up our introduction to email support in Spring Integration, and it’s time to look at our next topic: file support.
No different from the other integration options, file-based integration is all about getting information from one system into another.
In file-based integration, this is accomplished by one system writing to disk and another reading from it.
The particular details of the implementation (for example, whether both systems stream to the same large file, or multiple files are being moved around) aren’t part of the definition.
After reading this chapter, you’ll have a solid understanding of the details of file-based integration and how to implement it cleanly using Spring Integration’s file handling support.
In the previous two chapters, we looked at messaging through Java Message Service (JMS) and email.
In many ways, messaging-based integration is desirable over file-based integration.
Unfortunately, many legacy systems still offer file transfer as their only integration option, but when comparing the simplicity of sharing a file to Filesystem integration.
The book Enterprise Integration Patterns defines four basic integration styles: file transfer, shared database, Remote Procedure Call (RPC), and messaging.
Having one process write a file in a directory and another read from that directory is often the simplest thing that might possibly work.
If a simple solution just works, it has earned a right to stay in business.
Enterprises have been trying hard to move away from file-based integration in favor of service-oriented architecture, or SOA (with various definitions), but most of them have ended up keeping some of the legacy file-based integration points around.
Building new applications that have to be integrated into an architecture that has grown under these circumstances necessitates interaction with the filesystem.
In this chapter, you learn how to do this with Spring Integration, and we provide some generic pointers on designing interaction with the filesystem.
The chapter explores reading and writing, different ways of dealing with files, how the different components work under the hood, and how to handle several advanced problems you might encounter.
Before exploring how to interact with the filesystem, let’s carefully examine why it makes sense to do so in the first place.
If you consider yourself a good developer and you really like those things, you might have noticed that applying yourself to using them doesn’t make you friends in your team.
In the introduction, we associated file-based integration with the simplest thing that might possibly work, but in many cases, dealing with files properly is far from simple.
Simplicity is the best friend you can have in your programming career.
More often than you’d expect, things like the filesystem can be safely ignored in favor of a memory solution.
A component as common as the filesystem can’t be all bad.
In UNIX, for example, everything is a file, and working with intermediary files is often the simplest solution.
In an object-oriented realm such as the Java Virtual Machine (JVM), dealing with the filesystem requires an extra abstraction, which adds complexity.
Table 11.1 lists the advantages and disadvantages of using files.
Table 11.1 Advantages and disadvantages of interacting with the filesystem.
The disk has much more room than memory, so running out of room is less likely.
Files are easier than more advanced data stores to integrate with other applications.
Filesystem has no atomicity, consistency, isolation, durability (ACID) or Representational State Transfer (REST) semantics, and scalability is more complicated.
Can you be friends with the filesystem? If you can get away with the simple solution of using objects on the heap, go for it.
But if it just doesn’t work, look at the filesystem seriously before you jump into even more complex solutions.
We look at an example that can best be implemented using file sharing.
The rest of the chapter teaches you how you can implement it using Spring Integration’s file support.
One of the best examples of filesystem integration is an editor.
In this chapter, we talk about a web-based editor for trip details.
It would be great fun to elaborate on a vector graphics editor or something like that, but that would be over the top.
To keep things simple, let’s discuss a plain text editor.
The editor uses a plain text file as its output format, and that would be that if we didn’t have to worry about undo history and live collaboration.
For this example, we make it a point to worry about those things.
In the flight-booking application, users can connect with friends and keep a diary of the trip online.
You expect heavy usage, you don’t want to bombard the database with updates, and you don’t want to keep everything in memory either.
Put this way, the filesystem seems like a decent middle road.
To keep the undo history, you create small files with changes and store them alongside the base file.
Whenever a change is made in the editor, a file containing that change is created.
Now when the editor is closed, you can reload the file and the full undo history at startup, but the organization used here also allows you to do something even better.
You can have multiple editors open, perhaps at different terminals, that all edit the same piece of text.
If one editor writes a change to the directory, the other editors pick up the file and update their screen.
The examples discussed here can easily be extended to a rich text editor, a spreadsheet, or something else.
The only restriction is that you can define a base file format and a file format for the changes.
In figure 11.1 you can see the high-level design of the application.
Let’s explore the responsibilities of the components in figure 11.1 in more detail.
First, a single client creates a new document to work on, which results in a directory for the document and an empty file as a starting point.
After each change, a file with the change is created.
Figure 11.1 A file is picked up by the inbound adapter; it then flows through the transformer.
The UI interprets the change and sends changes made by its user back to the working directory.
You might want more fine-grained modifications later on, perhaps to update other clients on a key-by-key basis, but for this example, the solution we outlined in the previous paragraphs will suffice.
If both clients watch the directory with the base file for changes, they’ll see new files appearing in the directory as the other client edits the file.
Each client can then apply these changes to the inmemory model and refresh the screen so that the user is aware of the change.
With an appropriate scan rate and change size, a seamless collaboration between different users can be achieved.
We chose this example because it requires the right interactions with the filesystem to be suitable as an illustration of Spring Integration’s file support.
The first thing you should remember about file support in Spring Integration is the namespace.
You don’t have to use the namespace to be able to configure the various elements, but typically the namespace is an excellent tool to hide unwanted complexity.
Only in advanced scenarios does it become important to consider the underlying plumbing in detail.
The file: namespace To ease the configuration of file-related components, Spring Integration comes with a dedicated file: namespace.
To use this namespace, add the following to the root element of your configuration file:
The file: namespace allows you to use the shorthand for file-specific inbound and outbound channel adapters and various file-oriented message transformers.
All the different components can also be configured using plain old <beans/>
If you need to do more advanced customizations to the configuration, it could be useful to understand the bean configuration.
Look at the “Under the hood” section at the end of this chapter.
In the next section, we use the namespace to configure the file-writing leg of the application.
Writing a file to the local filesystem is a simple job, so let’s start there.
To be able to write a file, your application needs several things: write access to the directory where the file should be written, a byte array or a string to write to the file, and a filename to write this data into.
If you tried to do this using the raw File API of the JDK, you’d have to write some less-than-elegant boilerplate code that you’ve probably reproduced dutifully on countless occasions:
Spring Integration contains a component that does all these things (see figure 11.2), as detailed in the next few sections.
The only thing left to do is wire that up in your application context.
Some concerns related to the intended reader of the file must be taken into account.
What do you do about encoding? If writing a file takes a long time, how do you prevent readers from picking up incomplete files? This section focuses on the standard solution to this problem, and at the end of the chapter, we go into more complex scenarios.
The easiest way out of a concurrent read/ write bug is to write to a file until it’s done and only then move it to a place where the reader can pick it up.
This is exactly what the outbound adapter described in the next section does by default.
This element creates a file-writing component that writes the payloads of messages to a directory of your choosing.
Table 11.2 lists the attributes you can use in this configuration.
Figure 11.2 Schematic design of the Spring Integration file outbound channel adapter.
The outbound adapter is a passive component that responds to incoming messages by writing their payloads to a file.
Of these attributes, only id (and/or channel) and directory are required.
In keeping with good practices in Spring Integration components, reasonable defaults are included for the other properties.
When a file is received from the input channel, the file channel adapter opens it and looks at the payload.
Several payloads, such as byte[], File, and String, are supported.
If the payload can be written to a new file, a temporary file is opened.
This file has a suffix so that readers can prevent opening files that aren’t ready yet.
Then the adapter writes the contents to the file, and if all goes well, it moves the file to its final name.
This name is provided by the FileNameGenerator that you can wire up as a bean and inject using the filename-generator attribute.
In many cases, you must handle an original file after its contents are written to a directory.
This component behaves similarly to the channel adapter, but it allows you to send the created file to another endpoint immediately.
This option is an excellent choice if you must notify another service when the file is written.
Several potential problems must be considered when you write files.
You might encounter an IOException, for example, because you suddenly lost the ability to write to the disk.
There is nothing special about this failure, but Spring Integration allows you to let the exception bubble up as a RuntimeException.
Be careful to implement a custom FileNameGenerator in such a way that it doesn’t generate the same.
If no channel is specified, a channel is created automatically as with any other channel adapter.
This directory is a resource, but the default loading rule is always from the filesystem, no matter in what type of ApplicationContext this bean is loaded.
Writing files filename for files that should be named differently.
The default name generator ensures that within the same application context the names are unique, but you’ll probably need more control over the filenames.
It’s also important to avoid overwrites between multiple runs of the application, whether in parallel or in sequence.
With the outbound adapter, you can implement the save feature of the collaborative editor.
In the sample application introduced earlier, we defined the requirement to track changes incrementally.
To make this work, the files are named after the running application key (which is unique for each time the application context is loaded) and a timestamp.
Later in this chapter, you learn how to use this information to make sure changes are applied to the other applications in the right order.
Each file will contain a change in a format which allows an endpoint that’s processing it to apply it to a string.
The details of applying the changes are tricky, but lucky for us they’re not relevant to this book.
It’s not the responsibility of the channel adapter to ensure that the content of the files is in an appropriate format for the reader.
See figure 11.3 for a graphical representation of the writing leg of the application.
At this point it’s good to inspect the configuration related to file writing:
Now you can write files to the work directory, but you must also learn to read files.
The String payload is then converted to a file in the work directory.
Reading files is more complex than writing them, but once it’s clear which file should be read, it’s as trivial as writing to a file:
This snippet contains some boilerplate, and judging from countless examples of where people have messed this up, it’s quite error prone.
Although it’s interesting to discuss the low-level correctness of filesystem access, the primary goal of this chapter is to address the functional complexities of dealing with files.
The Trip Diary module must read files from a directory to update the displayed diary with changes from other editors.
The complexity lies in determining which file to open and when.
When a reading process is watching a directory, it has to periodically list all the files in a directory and decide which ones are new.
Imagine you’re looking at a directory that someone else is dropping files in.
How can you know which files to pick up and which files you’ve already processed? How do you ensure that no one is still writing to that file?
By default, Spring Integration’s file inbound channel adapter (depicted in figure 11.4) reads all files in a certain directory, but once it’s read, a file isn’t picked up again.
As a user, you can customize the behavior by specifying which files should be read using FileListFilters.
First let’s make sure you understand Java’s file API a little better.
Central to the filesystem interaction in Java is the File class, which is immutable.
This is a good thing, because as mentioned earlier, immutable payloads make your life much easier once you start.
Figure 11.4 Design of the Spring Integration inbound channel adapter.
Having a File object as a message payload is perfectly safe from a concurrency point of view.
We all know that files change all the time, so there must be something more to it.
Looking at the File source code reveals where the problem is hidden.
The File’s path is effectively immutable, but most operations on the File don’t access immutable fields; instead, they access the filesystem.
You can understand that getting the last modification date requires accessing the filesystem to check whether the file was changed lately.
This means you have to be careful to distinguish between operations that access the filesystem and operations that don’t.
If you need to look at the modification date or the file size to determine if a file should be read, you assume the responsibility to account for concurrent filesystem access.
A common tactic in dealing with this problem is to make sure that while files are being written, reading processes don’t see them yet—for example, by writing in a directory that isn’t visible to the reading process, or, as we do automatically in Spring Integration, writing to files with an agreed suffix.
Then, when you’ve finished writing, move the file to where the reading process can see it.
By holding the writing process partly responsible, the readers can be much simpler.
Exclude in-progress files using a FileListFilter and let Spring Integration deal with the visible files with the default behavior.
But how can you make Spring Integration do all that? The next section shows you the details of the configuration.
The attributes for this element are listed in table 11.3
If no channel is specified, a channel is created automatically as with any other channel adapter.
This directory is a resource, but the default loading rule is always from the filesystem, no matter what type of ApplicationContext this bean is loaded in.
Of these attributes, only id (and/or channel) and directory are required.
You have quite a few options here, but because only a couple of attributes are required, an inbound channel adapter configuration can be as simple as this:
This configuration would pick up files written to the work directory and send them to a channel called fileGuzzler wrapped in a message.
It would also make sure they’re picked up only once during the lifetime of the application.
This filter remembers all files it has seen before and doesn’t let them pass again.
The internal queue is prioritized according to the Comparator, if provided.
In the next section, we again apply the tools to the example application.
The tricks you just learned will help you implement the reading leg of the application.
Because the filter is embedded in the default scanner, this attribute is mutually exclusive with a filter, although a custom scanner can easily delegate to a filter that’s injected into it.
This option should not be used with the filter attribute.
Now that you know how to read files, let’s look at reading the incremental updates from the directory.
Given two running editors on the same document, editor A and editor B, files will be written into the directory by both A and B.
Editor A wants to read the files from B, or more precisely, the files not written by A itself.
If you distinguish each editor uniquely by a key generated on startup, restarting A will result in a running editor C, where C != A.
This is convenient because a newly started editor can read all the changes to the file, and even a restarted editor can read all the files written by the previous editor run by the same user.
As mentioned before, you could use a combination of a timestamp and process ID to prevent file duplication:
You learned in this section how to configure a file-reading component, and we discussed the more elaborate details of the possible configuration options.
In the examples, you can see that the typical boilerplate code is replaced by somewhat more readable declarative XML.
The collaborative editor now has both legs to stand on.
But what are you going to do with them inside the application?
When a file is read and wrapped into a message, you need to do something with it that makes sense within the context of your application.
Figure 11.5 Inbound components of the collaborative editor: a file is picked up from the working directory and sent on the incomingFiles channel wrapped in a message.
It’s then transformed into a String and passed along to the editor.
If it isn’t, it must be converted into something that makes sense, typically an object from the domain.
The process consists of two parts: first, the contents of the file are read into memory, and second, the raw data is converted into domain objects.
The conversion process is called unmarshalling, and plenty of frameworks, such as Spring OXM (discussed in chapter 8), can do it.
Spring Integration file support doesn’t care how you do your unmarshalling, so even though it’s an important concern, it’s not discussed in detail in this chapter.
The first part of the process, reading the files into memory, is an important responsibility of the file support.
Many different types of content can reside in a file, but from a high-level view, it’s all just bytes.
The built-in transformers of Spring Integration can transform a File object into either a byte[] or a String.
Transforming a file means your system is now going to deal with it.
At this point, you might choose to delete the file, but if the system fails to deal with the file’s content successfully and you’ve already deleted the file, you may lose the data.
Imagine you’re writing new products in a directory for your online shop.
These products will be dropped in a directory as XML messages and picked up by a process that inserts them into the database backing your web application.
First, the XML must be transformed into a String, and then an unmarshalling step will prepare the objects destined for persistence through a repository.
If the transformer deletes the file, it does so after it finishes the transformation, but it’s not unthinkable that subsequent requests are separated by asynchronous handoff.
In this case, obviously, the file will be deleted before the objects are persisted, but even if there is no asynchronous handoff, problems can occur because the transaction boundaries are broken.
If the process crashes before the file is deleted, but after the inserts are committed, restarting the process might cause duplication.
The next section outlines some common file-handling scenarios and discusses the pros and cons of each approach.
Managing files on disk from several applications can be done many different ways.
A lot of passionate discussion takes place on how this file management should work.
If no harm is caused by picking up the file again when the application is restarted, or if you have a clear way to filter out old messages, it’s often best to leave the files in the input directory.
If dealing with a file is a nondestructive operation, it’s much easier to.
Handling file-based messages recover from a bug or failure that causes the file to be dealt with improperly.
The downside of this approach is that from the outside it’s impossible to determine, on the basis of its name or location, whether a file was processed.
If a new file is created that can be correlated to the input, or if the application can be queried to determine whether a file was processed, this problem can be worked around.
If the files are left on the filesystem, it’s usually easy to devise a cleanup strategy after the application is operating for a while and you start running out of space.
For example, a script that deletes all files older than a week is not hard to write.
It’s important, though, to think of how much space you need for the files in advance and also to think about purging strategies.
If these strategies are complicated, you shouldn’t postpone implementing them.
Sometimes incoming files aren’t valuable, and dropping a file or two is no big deal.
For example, if a weather service gets forecast updates every minute, dropping one when it crashes won’t be noticed when the service restarts, so deleting the file from the transformer makes perfect sense.
But of the three options listed, this one is the most likely to delete a file prematurely and never deliver the contents to a downstream component.
Avoid this solution if you can’t afford to lose some data.
Some files are too big to just leave them, but you can’t afford to lose a single one.
In this case, you can’t delete the file from the transformer because you could lose the data if the JVM is killed, but you have to delete the file in process.
A file-reading endpoint will set the original file on the message as a header.
This header can be used to delete a file after it’s fully processed.
Usually, the process should write a file to a processed directory when processing is complete.
If the filesystem is full or the process crashes midstream, input files could sit in the input directory even though they’re already processed.
To mitigate this risk, you can implement an idempotent receiver in the endpoint that processes the file.
Then again, it might be easier to just leave the file and write a separate cleanup routine.
Each supports the same attributes (other than a charset attribute that's only present on the string transformer), so the primary difference is the type of the payload of the outgoing message.
Table 11.4 shows a short description of the allowed attributes.
Only the input-channel is required; in a chain, the element can even be used without any attributes.
Setting the delete-files attribute to true is equivalent to deleting on consumption.
This attribute defaults to false for a good reason: you wouldn’t be happy if your application deleted a file even though its content wasn’t processed correctly.
By now you’re probably ready to see the application work, so we put the ends together in the next section.
In the example collaborative editor, the incoming changes contain snippets of text and a position at which they should be inserted in the overall text.
We don’t dive deeply into the tricky domain at this point, but if you’re interested in the details, check out the sources and play around with them for a while.
As far as Spring Integration support is concerned, applying changes is only about reading the files and passing the resulting strings along to the editor.
The transformer is blissfully unaware of whatever tricks the editor needs to do with the supplied strings.
You can refer back to figure 11.5 to see how the transformer fits in the overall flow of the application.
There is little else to the API for reading files.
On the surface, it looks simple, and if the file interaction you’re planning for your application is as simple as this, there’s no more to it than configuring inbound and outbound adapters and transformers, as necessary.
As mentioned in the introduction of this chapter, though, the devil is in the details.
If the details become important, the next section will help you find your footing when dealing with issues such as locking or files being picked up multiple times or before they’re ready.
This section deals with the nitty-gritty details of filesystem integration.
If you need to implement more elaborate file-related use cases, you can avoid painful debugging by understanding the underlying concepts and rules.
Reading this section in advance is highly recommended in such cases.
Here we discuss the details of ordering and locking when reading files.
Can be omitted only if the transformer is part of a chain.
When omitted, the result is sent to the reply channel set as a message header.
Under the hood writing files but surface on the reading end in particular.
It’s a completely passive component that should be polled on its receive() method.
The endpoint that the component is wrapped in by Spring Integration takes care of the polling transparently.
When a poll occurs for the first time, it follows the steps depicted in figure 11.6
On subsequent receive() invocations, the internal queue is synchronized with the.
This can have implications on the internal ordering, as discussed next.
In some cases, the order in which files are read is important.
The collaborative editor, for example, is highly dependent on the order in which files are processed.
Before we dive into the code, first we need to understand the problem better.
The file listing returned by the File’s listFiles() method is a File[], so it’s ordered, but the order isn’t well defined on different operating systems and under different configurations.
To keep a long story short: it isn’t a good idea to rely on the ordering of this array.
As usual, a Comparator can be used to control the ordering.
The problem now arises that when files aren’t written in order, the queue might have to be reordered after a file was wrapped in a message and sent to a channel by a.
We wouldn’t be telling you this if you couldn’t fix it, though.
If you write the files into the directory in the order in which you want them to be received and you provide a comparator that puts the files in the desired order, the files will be received in that order.
If you do something else, like writing the files in the right order without a comparator, the files might be received in the right order if you’re lucky.
It’s never a good idea to depend on message order implicitly.
If you can’t avoid depending on order, you should use a resequencer (see chapter 7)
This makes the system more robust and easier to maintain.
Another problem that we already touched on in the introduction of this chapter is the risk of reading unfinished files.
As mentioned in section 11.3, it’s usually important to pick up files only after the writing process is done writing to them.
For example, the built-in transformers assume that a file is finished and return a String or byte[] containing whatever was in the file at the moment they reached the end of it.
As long as the file isn’t opened, there’s no problem passing around a reference while other processes are writing to it, but once a transformer (or another endpoint) opens the file for reading, all bets are off.
The best way to avoid premature reading of a file is to have the writing process decide when the file can be picked up.
This can be done through a move at the moment the writing is finished or through a barrier that holds file messages until the writer says they’re done.
For example, an application using this strategy might start failing unpredictably when a system administrator configures a different device for the input directory than the directory where the file is moved.
This is typically configured from the outside in a properties file, so the type of directories that should be used now must be part of the documentation of the application.
If the writing process is out of the control of the reader, it might still be possible to determine whether a file is ready.
For example, if an XML file is being written linearly, the only thing the reader has to do is postpone files that don’t have the last end tag.
Many formats have similar terminator sequences that can help prevent reading unfinished files.
The problem with reading terminator sequences is that they’re located at the end of the file, so you must open the file and read the last part of it.
This approach obviously performs a lot worse than the move-when-ready strategy.
Another tricky problem arises when the writing process is writing in multiple parts of the file concurrently.
The last part could be done while the middle part is still under construction.
BitTorrent uses this strategy, as do other fields, such as image processing.
In cases where a file isn’t moved when it’s done, and looking for terminators is either too inefficient or not possible, you have yet another option: file locking.
File locking works differently on different operating systems, but Java has an abstraction over file locking in the java.nio libraries.
Spring Integration’s file support was expanded to include support for filtering based on java.nio in version 2.0
You inject a NioFileLocker and make sure the writing process respects locks too.
You can also work with lock files that are created, moved, and deleted according to a protocol agreed upon by both writer and reader.
Locking might seem like a great idea at first, but it’s more complex than you might think.
If you can find a way to solve your problem without locking, you’re doing yourself a huge favor.
And with that, we’ve told you enough about dealing with files.
It’s time to round up and move on to the next challenge.
In this chapter you learned how to work with files using Spring Integration.
This adapter provides the most common options needed to create output files in a configurable directory.
Using the file-writing components of Spring Integration frees you from the responsibility of opening FileOutputStreams or FileWriters and closing them again.
We also discussed the outbound gateway, which is useful if you need to postprocess the written file.
You saw how to read files from the filesystem, prevent duplication, and deal with pattern-matching filters.
We discussed the FileListFilter extension point that allows you to filter incoming files on any criterion of your choosing.
You can go even further than that and implement your own scanner.
When files are read from the system, there are several things you might need to do with them; the most common tasks are implemented in the Spring Integration file transformers.
You can easily apply what you learned to implement a file transformer that better suits the needs of your business case.
Finally, we looked under the hood of the components we discussed.
In the final section, we explored reading unfinished files in more detail, so you have a better idea of the advanced problems you may encounter when implementing file-based integration with or without Spring Integration.
After reading this chapter, you should have a good understanding of how Spring Integration helps you tie your messaging infrastructure into the filesystem.
With the adapters described here, it’s simple to convert between messages and files.
As a final note, remember that the file support in Spring Integration, though convenient, doesn’t compete with frameworks that focus on the interpretation and processing of files, such as Spring Batch and other extract, transform, and load tools.
It’s a healthy idea to use Spring Integration to create the events that drive the specialized process that deals with the files.
In chapter 12, we discuss integration through web services, which in many cases is a more robust alternative to file-based integration.
Most systems, for example, don’t share a filesystem because they’re likely to be on separate hardware, potentially in different parts of the world.
With separate hardware in remote locations, file transfers over the network would have to facilitate this form of communication.
Differences in file format and character encoding may create additional challenges.
Using the network directly for message exchange is an appealing alternative.
Because of this capability, web service use has grown to the point where it’s now the default approach for many forms of intersystem communication.
This chapter discusses the support offered by Spring Integration for both exposing and consuming web services and the different flavors of web services supported (see figure 12.1)
Let’s first make clear what we mean when we talk about web services.
Furthermore, even SOAP isn’t exclusively tied to HTTP as the underlying transport.
Before Spring Integration was released, there was support for web services over Java Message Services (JMS), email, and Extensible Messaging and Presence Protocol (XMPP) built on top of Spring WS.
In this book, separate transport mechanisms are addressed in separate chapters.
Most people think of web services as communication using XML messages sent with HTTP.
Even though the definition is much broader, this is the form of web service that we focus on in this chapter.
Attempts to standardize web services gained great popularity in the 1990s.
Most large software vendors were happily selling a broad range of tools and generating an increasing number of add-ons to the specs.
Known as the WS-* specifications, these add-ons covered everything from security to business activities.
The relatively heavyweight and complex approach to web services dictated by SOAP left many people looking for alternatives.
One popular alternative is to resort to plain XML, commonly called plain old XML or POX.
An increasingly popular option today is to use simpler formats that can be customized for the task at hand.
Simpler forms of web services can use HTTP as more than just a transport.
The biggest growth in this form of web service has come through.
Figure 12.1 Integration over HTTP is simple because the network exists and HTTP traffic passes relatively easily in and out of enterprise networks.
Spring Integration provides support for exchanging XML-based web service messages over HTTP by building on the Spring WS project for both SOAP-based and POXbased web services.
We look at POX support in the first section of this chapter.
Spring Integration provides adapters for working with HTTP directly as well, which can be useful for non-XML or RESTful services.
We look at the HTTP adapters in the second section of this chapter.
Let’s first look at the most common case using XML.
Spring WS is a Spring portfolio project that focuses on contract-first web services with XML payloads.
As mentioned before, not all XML web services use SOAP.
Spring WS provides support for POX-based web services, among others.
One of the strongest points of Spring WS is that it decouples application code from the relatively complex requirements of SOAP.
Developers can code agnostic to the fact that SOAP is being used except where the application requires control over those details.
Without the use of an abstraction like Spring WS, the creation of even a simple SOAP message, as in the following example, is relatively complex; it requires a good understanding of XML APIs and namespace usage:
We already need to define a minimum of two elements in the SOAP namespace before we get to the message we want to pass.
It’s also worth noting that SOAP always uses the POST HTTP method to make requests even in the case of a read-only request for data that arguably is better suited to the use of the HTTP GET method.
This is because SOAP uses HTTP as a mechanism for passing data and makes no real use of the HTTP protocol.
You’ll see later in this chapter that this contrasts with the more HTTP-centric alternatives to SOAP.
The Spring WS project provides a simple programming model that insulates the developer from much of the complexity of web service development.
This is achieved by combining Spring configuration with simple interface-based or annotated endpoints.
Spring Integration extends the simple programming model offered by Spring WS.
It adds the power and simplicity of messaging combined with the established enterprise integration patterns (EIP) implementations.
For example, an insurance quote comparison site might need to split the quote request into a number of requests in different formats, invoke a number of different quote providers using different technologies, and then process a number of responses into a single web service response.
This can be achieved by using Spring WS alone, but it’s much less work to implement using a combination of the two, and that’s what the web service support in Spring Integration is intended to do.
To get started, you need to make sure the application can send and receive external messages through Spring WS.
In the next section, we look into wiring the beans needed to expose an inbound gateway to do this.
The inbound web service gateway allows for a POX- or SOAP-based endpoint to be exposed for handling requests, resulting in the creation of Spring Integration messages that are then published to a channel.
When receiving web service requests over HTTP, a front controller servlet must be deployed and configured to pass requests to the Spring Integration inbound gateway.
This strategy interface provides support for mapping a given request received by the servlet to a particular endpoint.
Spring WS provides built-in support for mapping on a variety of characteristics of the request from the URI invoked by the caller to the name of the root payload element in the request payload.
When using the Spring Integration inbound gateway, it’s more common to delegate all received requests to one gateway instance with additional routing being carried out by using the built-in Spring Integration support, for example, the XPath routing capabilities.
The following configuration delegates all received requests to a channel named ws-requests:
In some cases, it may be desirable to publish the whole web service message rather than just the payload.
This is achieved by setting the extract-payload flag to false, as follows:
Invoking a web service from Spring Integration is also extremely simple.
The payload of the web service response is then published to the configured reply channel:
Marshalling versions of both the inbound and outbound gateways are provided and are enabled by configuring the inbound or outbound gateway with a marshaller and/ or unmarshaller to allow automatic conversion between Java and XML for both the request and reply messages.
SOAP- and POX-based endpoints aren’t the only form of web service supported by Spring Integration.
It has become increasingly popular to provide web services that don’t conform to web service standards such as SOAP and the plethora of additional WS-* specifications.
Of particular interest to many is the use of the architectural style known as REST, which stands for Representational State Transfer.
In a RESTful approach, HTTP is used as the service protocol.
But receiving a GET request as follows, where only the HTTP method is different, would result in retrieval of a representation of book 123:
In HTTP-based communication, it’s common not to make use of all the HTTP methods and instead to use GET or POST for a multitude of purposes, including requests which are really deletions of some sort.
This is technically not REST, but sometimes tricks are used to stay as close as possible to the intent of REST without blocking browsers that only support GET and POST.
This support focused on mapping controller methods to requests using both the URI and the HTTP methods while also making it easy to map parts of the URI to controller method parameters.
The following code shows an example controller configured to process requests for the retrieval of books:
Because REST is a style rather than a specification, HTTP-based services may or may not conform to the REST style.
Therefore, although it’s possible to use Spring Integration to expose services in a RESTful fashion, HTTP can also be used to expose services that aren’t considered RESTful.
Support for all HTTP methods is something that may not exist in some technologies, such as Asynchronous JavaScript + XML (Ajax), that use, almost exclusively, GET and POST HTTP methods and rarely use URI schemes that reference resources.
The HTTP inbound gateway and channel adapter endpoints allow for the consumption of an HTTP message, which is then used to construct a Spring Integration message for processing.
The inbound endpoints, by default, attempt to convert the received message according to a strategy.
The default strategy for conversion is based on the content type of the request and converts the received payload to an appropriate Java message.
To start processing inbound HTTP requests, you first need a servlet to act as the entry point for requests.
You configure it with a Spring application context containing the Spring Integration inbound gateway:
In this example, a reply channel is provided, but it isn’t required, and when it’s not provided, a temporary channel is used to receive responses:
Which one is instantiated at runtime depends on whether the Spring MVC view technology is being used to render the response.
If an attribute specifying a view name is provided, then the controller version is created; otherwise, the gateway implementation with its own response-rendering logic is used.
When the controller-based gateway is used, the HTTP response can be rendered by one of the large number of view technologies supported by Spring MVC, including Java Server Pages, FreeMarker, or a custom implementation of the View interface.
Apart from specifying the view, several other attributes can be set.
Table 12.1 shows an overview of the attributes and their functions.
The ability to provide a view gives a great deal of flexibility in generating a response.
Table 12.1 Additional configuration attributes for the HTTP inbound gateway element.
If view-name is provided, a controller class is used to provide the inbound request processing functionality.
For processing HTTP requests which require only confirmation that the request was successfully accepted, Spring Integration provides an inbound channel adapter.
The view-name attribute is specified, so the resolution of the view and the creation of the response is handled as usual in Spring MVC:
Note that in this example, the name of the bean is assigned as a path.
This allows the bean to be used with the default behavior of Spring’s DispatcherServlet, as configured in the following snippet:
This strategy converts the HTTP headers to and from Message headers.
Table 12.1 Additional configuration attributes for the HTTP inbound gateway element (continued)
Now you know how to receive incoming messages over HTTP, but that’s only one side of the story.
In addition to processing inbound HTTP requests, Spring Integration provides an outbound HTTP gateway capable of transforming a Spring Integration message and optionally publishing the HTTP response as a Spring Integration message.
Following is a simple example using the HTTP namespace support.
This example uses the URI variable capabilities of the REST template that performs the HTTP request, allowing the evaluation of an expression on the message to provide a value for insertion into the URI used for the HTTP request.
In other words, the URL can contain a variable, which is filled at runtime.
In this case, we expect a location to be contained within the message payload, and it’s used to request the weather for that location from Google:
The HTTP response is provided as a String even though the content type of the response is text/xml.
Changing the gateway to specify the response type as a Source will facilitate processing as XML:
In the previous two sections, we’ve talked about synchronous messaging.
This is natural because HTTP is an inherently synchronous protocol.
But there’s no reason why we can’t use a synchronous protocol for asynchronous communication architectures.
The next section discusses the outbound channel adapter that can be used to do this.
An outbound channel adapter is much like an outbound gateway except that it doesn’t send a response on a reply channel.
Behind the scenes, the HTTP outbound channel adapter uses the RestTemplate from the Spring Framework.
The error handler can be provided by setting an attribute on the outbound channel adapter element.
This can lead to problems, for example, with a POST request for which the response is 301 (moved permanently)
Such a response won’t be treated as exceptional, and messages will be silently dropped as the HTTP server discards them without any indication of a problem at the Spring Integration end.
Following is an example configuration that posts messages to the configured URL:
Integration over the internet is becoming increasingly important, allowing use of content and services from a wide variety of sources and supporting a large number of different consumers of that content with different format requirements.
Although standards such as SOAP are still favored in large corporations, many people are moving to more flexible approaches to web services.
The de facto standard on the web is REST because it fits so well with the HTTP paradigm.
To integrate systems that take different approaches to web services, you need flexibility in your integration solution.
In this chapter, you learned that Spring Integration provides a number of options for web service integration.
Spring Integration covers common forms of web service, such as SOAP, REST, and POX, as well as other HTTP-based forms of integration.
The namespaces simplify the development of applications that expose or consume web services.
Combining Spring Integration with Spring WS and the REST support provided by Spring MVC helps the developer to use EIP patterns within those applications.
Now that we’ve covered the mainstream enterprise types of service integration, we can spend the next chapter looking at emerging integration options.
As with email, Spring Integration supports other messaging systems suitable for human interaction, such as Twitter and XMPP.
You saw how to send and receive messages using the Java Message Service (JMS) API.
Those adapters, and most of the others that we covered up to this point, are focused on well-established technologies that are commonly encountered in the enterprise development environment.
In this chapter, we shift our focus to two different technologies: XMPP and Twitter.
Both are well on their way to reaching mainstream status, if they aren’t already there.
Many enterprise Java developers probably use both technologies daily, but XMPP and Twitter have relative newcomer status in the realm of enterprise Java applications.
That trend is likely to accelerate, and in this chapter, you learn how easy it can be to introduce these technologies into your applications via Spring Integration’s corresponding adapters.
We covered a lot of Spring Integration adapters in previous chapters.
It defines a fairly generic model for packets containing content and metadata which can be passed along with that content to describe it.
As indicated by the name, the two categories of content are messages and presence notifications.
Even though XMPP can be applied to many different domains, its most widely recognized role is in the implementation of instant messaging (IM) services.
Even if you’ve never heard of XMPP, you might use it on a daily basis.
Several popular IM services are built on it, including Google’s chat, and Apple’s iChat supports XMPP as well.
That’s a fancy way of saying that the messaging can occur in two directions at the same time.
This concept is easy to understand by considering a common case of messages crossing in a real-time chat scenario.
Figure 13.1 demonstrates a chat session in which both parties are busy typing a message at the same time rather than taking turns.
Spring Integration provides adapters for sending and receiving both message content and presence notifications with XMPP.
The potential use cases for exchanging message content are relatively obvious.
The applicability of presence notifications is less obvious, but it’s a distinctly powerful feature of the protocol.
When you use an XMPP-based IM client and you see the members of your friends list who are online, that functionality is driven by presence notifications.
Likewise, when someone updates their status, your client receives that update in your roster.
Those presence notifications are separate from but parallel to the message content.
Let’s look at the adapters, beginning with the sending of simple message content.
We start with what’s probably the most common use case for XMPP in a Spring Integration application: sending messages.
The concept of chat has many practical uses beyond the simple IM scenarios between friends.
Two machines can exchange messages as a way to pass data or notify each other about important events.
Figure 13.1 XMPP supports full-duplex messaging, meaning that communication occurs bidirectionally, even simultaneously.
The clients reading messages in that chat room might be a combination of other machines and human users.
The topic of such messages might be system-related notifications, such as “server X has encountered a problem,” or they might be events within the business domain, such as “order X has just been processed and all items are available in the warehouse.” Presence notifications can play a role as well, but we explore those adapters later.
The first step to using any of the XMPP adapters is to set up an account and its associated credentials.
Once the account is activated, it can be used to create a connection to an XMPP server.
Rather than configuring the account information every time you define an adapter, Spring Integration allows you to configure the connection separately so that it can be reused across multiple inbound and outbound adapters.
It’s generally recommended that you externalize the connection settings in a simple text file as key-value pairs.
Using the same example, you could accomplish that by creating a properties file, such as xmpp.properties:
Then, you could associate placeholder variables with the keys from that properties file.
Once the connection is defined, you can set up the adapter.
We already provided all of the connection details with our XMPP connection, so this configuration is trivial.
The only thing that’s required is the name of the Spring Integration channel where the application will send messages intended for the XMPP chat:
Now you can turn to the use of these adapters from application code.
As with any other channel adapter, the point of the abstraction is to enable the application to operate only in terms of the simple generic messages.
Nevertheless, for the adapter to construct an XMPP message from a Spring Integration message requires one additional piece of information beyond the chat content.
The adapter needs to know to whom the message should be sent.
The way to pass that information is through a Spring Integration message header.
The key of that header is provided as a constant value: XmppHeaders.TO.
The following code provides an example of sending a message along with that header:
As you know by now, it’s not necessary to work directly at the API level.
One of those alternatives is to use the aspect-oriented programming (AOP)-based technique that’s driven by the @Publisher annotation.
Let’s look at an example of sending a chat message every time an order is processed:
This example would work perfectly if you intend to send the chat messages as a byproduct of the service method invocation.
That method is already being called as part of your application logic.
You’re intercepting that normal call in order to also send a chat message each time.
Under the covers, Spring Integration is using AOP, where a proxy is created to decorate the service.
The only thing you need to add to your configuration to enable such proxies is a one-line directive with an element defined in the core Spring Integration namespace:
You can even add the default channel to use for any @Publisher annotation that doesn’t include one explicitly.
That works well if you have a simple application and can rely on a single channel for all interceptor-driven messages:
Even though this technique also relies on a proxy, it’s best suited for a different type of use case.
We already saw that the intercepted application service method treats messaging as a by-product, and the primary responsibility of the code is still handled by the application service.
With the gateway proxy, there’s no underlying service implementation to intercept.
Rather than having messages as a by-product, the primary purpose of the gateway is to send a message.
For that reason, you only need to provide an interface, not an implementation.
Then, the configuration points to the fully qualified name of that interface:
In this case, Spring Integration creates a proxy that implements the interface.
You can then reference that instance, typically using dependency injection to make it available from client code.
You might wonder why you would go through this hassle when you could just as easily inject the channel directly or, for more control, use a MessagingTemplate instance.
The advantage of using the gateway proxy is that it’s noninvasive.
It’s the same justification for using the @Publisher annotation rather than requiring the calling code to have a direct dependency on the Spring Integration API.
In both cases, the underlying implementation provided by the proxy will handle the API-level concerns so that you don’t have to write that code.
Also, in both cases, you can easily leave this configuration out of the picture for simple unit-level testing.
In the gateway case, you can test the calling code by swapping a mock-driven implementation rather than the gateway proxy.
You may have noticed that we’re missing one important detail in the recent examples.
We’re passing the chat message payload text, but we left out the recipient’s username.
Obviously, that information must be provided so that the messages can be sent to someone.
Probably the simplest option, if it’ll work from the perspective of the calling code, is to rely on a method argument.
Here’s a slightly modified version of the ChatGateway interface to demonstrate such an argument:
The unannotated argument is used as the payload of a newly created Spring Integration Message, and the argument annotated with @Header is added to that message with the annotation’s value as the header name.
This example relies on the XmppHeaders.TO constant because that’s what the downstream XMPP adapter expects.
If you’re not willing or able to include the username in the method invocation, another option that’s even less invasive is to add a header-enricher to the message flow.
Assuming the gateway is sending to the channel named chatChannel, the following configuration would work nicely:
One obvious limitation of that particular configuration is that the header value is statically defined.
If you need to determine the username dynamically, you can add an expression instead.
Imagine a situation where the customer Account object is stored in a header already.
Perhaps the Account instance was passed as an annotated method argument instead of just the username.
Here we show just the header enricher’s chat-to subelement in isolation to focus on this option:
A slightly more advanced option would use an expression that relies on some other bean that’s defined in the application context.
For example, you might have the Account object, but it doesn’t provide the user’s IM username.
At the same time, imagine you have a simple lookup service that can be used to find the chat username from the account number.
The following would connect that service to the header enrichment step:
This expression is one example, but the point is that combining the Spring Expression Language (SpEL) with the header enricher functionality provides considerable flexibility in how you derive the necessary information from the message context at runtime.
That in turn enables the dynamic behavior often needed when sending messages in a chat application.
In the previous section, we covered all the details of sending XMPP messages via a Spring Integration channel adapter.
Considering that XMPP is a protocol for chatting, there must be a receiver as well as a sender.
As we described earlier, XMPP is designed for full-duplex communication, meaning that both parties involved in a chat can play the role of sender and receiver simultaneously.
In Spring Integration, each direction of communication is handled by a distinct channel adapter.
The outbound channel adapter represents the role of sending, because a chat message is sent out from the perspective of the Spring Integration application.
Its responsibility is to receive messages that are coming into the Spring Integration application.
The inbound adapter has much in common with the outbound adapter.
Not only do they rely on the same protocol and the same underlying API for negotiating that protocol, but they both rely on the same connection configuration to use that API.
That means that any of the xmpp-connection element examples displayed in the previous section would be equally valid on the receiving side.
Otherwise, a simple inbound adapter looks like a mirror image of the outbound adapter:
This example assumes the XmppConnection instance is defined as a bean named xmppConnection.
Each Spring Integration message sent to chatChannel would contain the body of the received XMPP message as its payload.
That body contains text content and is therefore represented as a String instance.
If you have a reason to use the entire XMPP message instance as the Spring Integration message payload, then the extract-payload attribute can be provided on the channel adapter XML element with a value of false.
That pretty much covers the essentials of sending and receiving basic chat messages.
In the next section, we explore the Spring Integration support for the other major feature of XMPP: presence messages.
You now know how to send and receive chat messages, and obviously those are the most important features of a chat service implementation.
Nevertheless, when you consider your typical usage of an IM application, you may realize that you also depend heavily on the ability to know which of your friends are available.
In fact, most IM applications support status messages that provide more detail than whether someone is online or offline.
Your friends are listed on the roster, or buddy list, and some may have status messages that indicate, for instance, “do not disturb,” or they may have a custom message such as “in San Fran until Tuesday.” XMPP supports these types of messages as well, and they’re known as presence notifications.
If you want to know when your friends become available online, go offline, or change their status message, you can listen to their presence notifications in addition to their chat messages.
As it has for the chat message support, Spring Integration has a pair of inbound and outbound adapters for presence.
Here’s an example of a simple inbound adapter for receiving presence notifications:
Note that the presence adapter can be defined with an element from the same xmpp namespace.
The rationale is that the more common feature is the chat messaging.
As with the chat adapters, the xmpp-connection attribute is optional.
As long as the name of the XmppConnection bean is xmppConnection, the attribute is unnecessary.
To send presence status updates, you can define the mirror image outbound adapter as shown here:
You may decide to use the same channel for chat and presence messages, but by providing distinct adapters for each, the Spring Integration support allows you to make that decision on a case-by-case basis.
If you were to implement a chat service, you’d most likely maintain the separation of concerns.
One channel can be dedicated to inbound chat messages, and another can be dedicated to outbound chat messages.
Likewise, you can provide one inbound and one outbound channel for transferring the presence notifications.
Here again, the xmpp-connection attribute is unnecessary if the referenced bean is named xmppConnection.
As you’ve seen thus far in this chapter, XMPP is a useful protocol that spans a variety of interesting use cases.
It caters well to building a chat server, but that only scratches the surface.
Considering the trend toward highly distributed applications and event-driven architectures, XMPP can fill an important gap where applications, not just people, need to communicate with each other and notify each other when they come online or go offline.
The channel adapters provided by Spring Integration make it easy to add such behavior to your own application without having to dive into the depths of XMPP or code directly against any XMPP client library.
Spring Integration again provides corresponding channel adapters so that you can easily integrate the entire spectrum of Twitter features into your application.
The first thing most people think of in relation to Twitter is the timeline.
When someone tweets, they’re posting a status update or, more generally, some comment that will then appear in that user’s timeline.
Twitter your standard view is most likely the combined timeline that displays the tweets of all the users you follow as well as your own tweets.
You can perform a search for all tweets that include some interesting text, regardless of whether those tweets were posted by users you follow.
Similarly, you can find all tweets that mention your own username, and those results also may include tweets from users whom you don’t yet follow.
Finally, Twitter provides support for sending and receiving direct messages, which are private alternatives to the more common broadcasting that’s visible to everyone.
Because Twitter provides so many different options, Spring Integration provides a wide variety of Twitter adapters.
Table 13.1 shows the name of each Twitter adapter element that can be configured in XML and the corresponding role of that adapter in terms of how it uses the underlying Twitter API.
The remainder of this chapter focuses on the adapters listed in the table.
You’ll see that Spring Integration covers the whole spectrum of Twitter functionality.
With minimal configuration and no direct dependency on any Twitter API, you can easily add a Twitter search feature, or direct messaging, or plain old tweeting of status updates.
The reason search is simpler than most of the other adapter types is because the underlying API enables search capabilities without requiring authentication.
Let’s begin this tour of Twitter adapters with the ability to convert search results into inbound Spring Integration messages.
One of the most common uses of Twitter in a Spring Integration application is to perform a search periodically and send the results downstream in messages.
It also happens to be simple in its configuration because a Twitter search doesn’t require authentication.
We explore other Twitter adapters that do require authentication because they’re focused on a particular user, but first let’s take a look at an example with the search adapter:
As you can see, the poller subelement dictates that the Twitter search should be performed every minute.
Each message passed to that channel will contain a single tweet from those search results as its payload.
The actual instance type is a Tweet class defined by Spring Social.
This Tweet type is common to all of the adapters you see in this chapter.
The following excerpt shows all of the instance variables defined on the Tweet object:
Knowing those properties, you can control the content of downstream messages by using SpEL.
You could add the expression attribute to have more control of the message to be logged:
A more general approach would be to define a transformer element.
Then you can reuse that transformer in different message flows:
At this point, you know everything you need to know to periodically retrieve Twitter search results as Spring Integration messages.
Another popular use for Twitter adapters on the inbound side is to read your Twitter timeline.
It requires more configuration because you must provide the information necessary for configuration of authentication details.
Whereas the search operation can be handled with an anonymous Twitter template instance, many other operations, including reading your timeline, require authentication.
We cover the timeline reading adapter shortly, but first we discuss how you should provide the configuration necessary for creating a Twitter template instance that can pass OAuth credentials.
It implements a Twitter interface, and that interface provides access to a variety of operations instances which define the methods that all of these Spring Integration Twitter adapters depend on to interact with the Twitter API.
As with other Spring templates, it provides a higher level of abstraction to simplify invocation of the underlying operations.
In the case of Twitter, it means you can configure it once, share it across your application, and invoke those Java methods rather than performing REST requests directly against the Twitter API.
But if you were to look into the search adapter’s code, you’d see that it does delegate to the TwitterTemplate.
The reason it doesn’t need to be created explicitly is because the search adapter doesn’t require authentication.
To perform any Twitter operation that does require authentication, a TwitterTemplate must be configured in the Spring configuration so that the authentication credentials can be provided.
The authentication mechanism used by this template is OAuth, and its credentials consist of a consumer key and an access token as well as their associated secrets.
To create a TwitterTemplate with OAuth capabilities, you must first register your application at http://dev.twitter.com.
Upon registration, you receive a consumer key and an associated secret.
On the developer site, you should see a link to register your application.
When filling out the form, be sure to specify the Application Type as Client.
You must also click on the Access Token button to receive the access token and its associated secret.
Be careful to store all of the keys and secrets in a safe place.
In the examples throughout this chapter, the OAuth configuration properties are stored in a file.
Such a file can be protected so that your application has read access but other applications don’t.
Then, the TwitterTemplate can be configured as a bean definition using property placeholders:
As of that version, Spring Integration builds on the TwitterTemplate included in the Spring Social library.
Both take the OAuth configuration values as constructor arguments, and they even appear in the same order.
That means the only necessary change is in the fully qualified classname in the template’s bean definition.
Now that you know how to configure a TwitterTemplate instance that uses OAuth configuration properties, let’s discuss some of the adapters that require authentication.
We begin with the inbound Twitter adapter for reading your timeline.
Along with the search functionality you saw earlier, reading the timeline is the other major use case for an inbound adapter accessing Twitter.
You have to be logged in because the timeline is associated with your user account.
As you saw in the preceding section, OAuth is the mechanism used to establish an authenticated connection.
If you’ve created a TwitterTemplate instance that has the required OAuth properties, as shown in the earlier excerpt, then you can provide a reference to it when creating a timeline-reading Twitter adapter.
That adapter has a poller, and it’s configured to poll every 10 seconds.
With each poll, it attempts to read up to 25 tweets from the timeline of the user whose OAuth credentials are provided to the template.
The rationale is that reading the timeline is the most obvious thing you can do with Twitter.
If you have a Twitter client on your desktop or phone, when you start that client application, the default view is most likely the timeline.
In the next section, we explore what’s probably the most popular use of Twitter for outbound adapters: updating your Twitter status.
In the previous section, you saw how to read the Twitter timeline with an inbound adapter.
If you hadn’t already known, you learned that the Twitter timeline is a collection of status updates, typically called tweets, from all of the Twitter users whom you follow.
In this section, we explore the other side of that relationship.
When you update your status by posting a tweet, all of your followers should soon see the update in their timelines.
As with reading the timeline, you must be authenticated with valid Twitter account information before you can send status updates.
The configuration for the OAuth credentials is the same for all adapters that require authentication.
In fact, each of these adapters delegates to the underlying TwitterTemplate instance, and your job is to ensure that the TwitterTemplate is configured properly, including the authentication credentials if needed.
Only the Twitter search adapter can be used without authenticating, so in all other cases, the TwitterTemplate should be configured with the full OAuth credentials.
Because the TwitterTemplate can be used concurrently, you’d most likely define only a single TwitterTemplate instance for a given Twitter user account.
Then you can reference that template from each of the adapters that you might be using in an application as long as those adapters are intended to represent that account.
Once the TwitterTemplate is defined, the status updating adapter is trivial to define.
It’s the mirror image of the timeline-reading adapter on the inbound side:
Also like Twitter, your message text should be within the limit of 140 characters.
In the next section, we look at another interesting use case that’s common with Twitter: the ability to receive any tweets that reference your own username.
Status updates can be retweeted or replied to by those who follow you and see the status update in their timeline.
Perhaps more interesting is that someone who doesn’t follow you but happens to notice your tweet, perhaps in a search result, might decide.
Likewise, they might decide to start following you and reply to the tweet.
In some respects, a tweet is similar to a message sent to a chat room when using an IM application or XMPP, except for two key differences.
First, the chat room is much larger and less restrictive, basically the internet itself.
Second, you may or may not be posting a tweet in an attempt to solicit responses.
Even if you don’t expect or intend to provoke responses, others might be inspired to extend your isolated tweet into a dialog.
You might be tweeting about something you find interesting without any intention of starting a discussion, but it could grow into an interesting discussion that pulls in people whom you had never known.
On the other hand, you might initiate such a discussion more intentionally by posting an interrogative or poll message, such as “Which one of these laptops should I buy?” Compared to an IM or a chat room, a dialog via Twitter might be an unintended but welcome consequence and may include people who happened to stumble upon one of the tweets in that dialog.
Twitter doesn’t organize tweets into threaded discussions: any tweet may have an in-reply-to value.
It’s less structured than a chat room, but that lack of structure is what differentiates Twitter as an open-ended form of messaging among a massive community.
In this section, we walk through the process of monitoring Twitter for any mentions of your username in retweets or replies.
When it comes to messaging styles, you can think of Twitter as a publish-subscribe scenario in which the person posting a tweet is the publisher and all of that person’s followers are subscribers.
Considering that all of the tweets are visible to anyone who performs a search or visits the person’s public timeline, the pool of subscribers extends far beyond just the followers.
Once a tweet is posted, those subscribers may decide to retweet if they want to reiterate the point of the original tweet and make sure that their followers see the tweet.
If they want to provide some commentary of their own or an answer to a question, then they can reply instead.
The difference is subtle, but all retweets and replies share one thing in common: they contain the original publisher’s username, including the @ symbol that precedes all Twitter usernames.
In addition to retweets and replies, someone may refer to someone else’s username in a Twitter message, such as “I just had dinner with @somebody.” If you want to find all mentions of yourself, you could perform a search on your username, but because it’s such a common operation, the Twitter API provides explicit support for reading all mentions of the logged-in user’s username.
Likewise, Spring Integration provides an inbound channel adapter for reading those mentions.
It’s configured almost identically to the basic timeline-reading adapter, but with a different element name.
Like the timeline-reading adapter, it requires a template instance injected with the user’s OAuth configuration properties:
Before taking this discussion too far, we should also consider that Twitter provides an alternative for direct messaging.
This goes to show that Twitter provides both publish-subscribe and point-to-point messaging semantics.
The difference between the two in Twitter terms is extreme: either broadcast to the internet or send a message to exactly one person.
We mentioned that Twitter, in general, supports a broad publish-subscribe style of messaging.
Although it’s probably not as popular, Twitter also supports a point-topoint style via direct messages.
If you want to send a message to someone who follows you, and you want only that person to receive it, then use direct messages instead of updating your status.
When considering the classification of communication options, direct messaging with Twitter seems to sit between email and chat.
Just as some people refer to Twitter public timeline posting as microblogging, the direct message functionality may be thought of as micromailing.
In both cases, the essential characteristic of the Twitter alternative is its enforced brevity.
From a cultural perspective, it’s interesting to see the wild success of a technology that seems to be defined by a limitation.
Even though the content and size of messages may have more in common with those of an IM chat session, the request-and-reply interaction typically feels more like email.
To some degree, those who frequently use direct messaging with Twitter embrace the asynchronous nature of communication.
They accept the reality that even if other parties are currently available, they may prefer to respond when it’s more convenient for them.
Using a phone call analogy, it’s the same reason voice mail is so commonly used even on a mobile phone that’s most likely in the presence of its owner most of the time.
When you send a colleague a message like “Want to grab a beer after work?” you probably don’t need to know the answer immediately.
Likewise, the receiver will likely perceive it as more polite that the sender acknowledges the question as a lower priority than one that would necessitate a synchronous phone call.
It’s as if Twitter gives the sender the option to go straight to voice mail.
Let’s take a look at the Spring Integration channel adapters that support direct messages.
The outbound channel adapter for sending direct messages via Twitter looks similar to the status updating channel adapter.
The key difference is that its element name is qualified with the dm- prefix.
Because sending direct messages requires authentication, you also need to provide a reference to a fully configured TwitterTemplate instance.
There’s one more requirement when sending a direct message via Twitter.
You must somehow specify the user to whom you’d like to send the message.
As you’d likely expect by now, the target user information should be specified in a message header.
Here’s an example of programmatically sending a Spring Integration message that ultimately triggers the sending of a Twitter direct message.
This code would send a direct message whose content is hello to the user whose name is someuser.
The receiving side is simpler because the target user ID is a responsibility of the sender.
All you need to do on the receiving side is make sure you’ve configured a TwitterTemplate with the proper user information and OAuth credentials.
Any direct messages sent to that user will be received by that channel adapter.
The inbound channel adapter that receives direct messages is practically identical to all of the others that Spring Integration provides for receiving tweets.
The element names are different, but the template reference configuration looks the same across all of them, and nothing else is required.
With the preceding information and examples provided, you’ll be able to work with not only the status update Tweets that appear in the timeline but also with search results, mentions, and direct messages.
In this chapter, we covered Spring Integration’s support for any XMPP-based chat service and support for the full range of Twitter options: search, timeline status updates, mentions, and direct messages.
Spring Integration will continue to evolve to support additional chat and IM protocols, such as Internet Relay Chat (IRC) and Short Message Peer-to-Peer Protocol (SMPP)
In fact, some prototypes have already been created in the Spring Integration sandbox repository for both of those protocols.
Likewise, Spring Integration will evolve to provide many more channel adapters in the social media realm.
The relatively new Spring Social project provides support for a number of different social media technologies, and it’s easy for Spring Integration to build adapters on top of those underlying APIs.
The supporting classes provided in the Spring Social project take the form of templates, strategies, and other common.
Twitter is one of the supported technologies, and as of version 2.1, Spring Integration relies completely on that library’s TwitterTemplate.
Other technologies supported by the Spring Social project will be represented by new channel adapters added in future versions of Spring Integration.
In most cases, the template implementations for the various social media sites will be built on top of Spring’s RestTemplate, and many of them will depend on common OAuth configuration just as you saw with Spring Integration’s current Twitter support.
You can expect to see support for Facebook, LinkedIn, GitHub, and more.
In the meantime, you can rely on any of the Spring Social project’s template implementations from within simple service-activator elements defined in a Spring Integration configuration.
We highly encourage such usage now because even when new channel adapters are added and the corresponding namespace support is provided, the underlying functionality will be the same.
The new Spring Integration adapters will be invoking the same templates that you can go ahead and start using today.
One distinction is that XMPP focuses on bidirectional messaging in real time.
That presence earns one of the letters in the protocol acronym shows that being online and hence available for chatting is an important aspect of the intended usage.
With Twitter, on the other hand, people tend to “catch up” when they have free time.
When updating your status, you don’t expect people to respond immediately and likely don’t sit there waiting for that to happen (at least we hope you don’t)
In terms of expected response time, even direct messages in Twitter are typically considered more like miniature email messages than chat messages.
Both require that you register with a username, and both provide ways to keep track of your friends.
As you saw throughout this chapter, Spring Integration also treats them as consistently as possible and provides header enricher components for both so that you can dynamically determine the intended recipient of a message.
Even more important, as with all of the adapters you’ve seen so far in the book, those described in this chapter enable you to easily interact with XMPP and/or Twitter without having any direct dependency on the underlying APIs.
You can configure the channel adapters and then connect the associated message channel to other components such that you’re dealing only with the text content of the messages.
Your code can be extremely simple, depending only on strings—a prime example of that clean separation of concerns we emphasized so early in the book, which is the essence of the Spring Integration framework.
You have been presented with the most typical integration options.
Other options can be implemented as extensions of Spring Integration or integrated as generic service activators, gateways, or channel adapters.
In part 4, we discuss advanced topics such as concurrency, performance, and monitoring.
The upcoming chapters build on the knowledge you’ve gained thus far and give you the tools to help you maintain solid integration solutions.
In part 4, the final part of the book, we'll walk through a number of topics that will likely cross your path as you take what you’ve learned thus far and begin applying that knowledge while building applications for the real world.
Chapter 14 answers the inevitable question: how does one monitor and manage an application built using this framework? In the answer to that question, you’ll encounter yet more enterprise integration patterns, including Message History, Wire Tap, and Control Bus.
Chapter 15 offers a deeper view into how Spring Integration manages task execution and scheduling, topics that are important to understand when building solutions that may contain several concurrent and asynchronous message consumers.
Chapter 16 provides a quick glimpse into Spring Batch, a sibling project of Spring Integration.
As you'll see, combining the two allows you to incorporate the messaging patterns you've learned in this book into applications that otherwise concentrate on batch processing and its own set of patterns, such as jobs and steps.
Chapter 17 caters to those interested in deploying applications to a container based upon the Open Services Gateway initiative (OSGi)
It provides a high-level overview of the Eclipse Gemini Blueprint project—the successor to Spring Dynamic Modules—and demonstrates how to use Spring Integration within such an environment.
It offers guidance for one of the most important challenges in software development: testing.
No doubt when dealing with asynchronous events and integrated systems, even the most ardent.
Here we share some tools and tactics that we've used while testing the framework itself.
When it comes to monitoring message-driven applications, meaningful statistics can be gathered from two perspectives: (1) component-centric data can be acquired by monitoring the general message traffic at the level of a message channel or message endpoint; (2) message-centric data can be acquired by recording every message channel or message endpoint that a particular message passes.
The latter is useful for tracking the detailed history of a given message even though it traverses a pipeline of loosely coupled components, possibly spanning many different threads of control along the way.
To a large degree, software architecture is the art of evaluating trade-offs.
Almost every advantageous characteristic of a given design has a cost.
For example, a higher level of abstraction may offer the advantage of a simpler developer experience, but its cost might be extra overhead.
One of the most important roles for an architect is to navigate such trade-offs and determine if the advantage gained is worth the cost.
When it comes to a message-driven architecture, the most significant advantageous characteristic is the loose coupling of components.
That brings a number of benefits, as we’ve hopefully made clear throughout this book.
Channel adapters can be swapped across a wide variety of transports and protocols.
Service activators can be reconfigured to point to different objects providing business functionality.
If loose coupling is the advantage, what is the trade-off? In the course of its flow, each message may traverse a number of different components.
The fact that those components are decoupled means that no single component plays a centralized role of tracking where the message has been.
As any developer knows, it won’t be long before monitoring requirements make their way into the high-priority list for any application.
With that in mind, Spring Integration provides a mechanism for tracking the flow of each message without compromising loose coupling or resorting to a single, centralized resource.
This mechanism records not only where the message has been but also when it was there.
The key to this strategy is that the historical data is written directly to the message.
To enable such tracking, you add an element from the core namespace anywhere within the configuration:
The following example begins with an inbound adapter, passes through a transformer, and ends with an outbound adapter.
The configuration could be more concise if you relied on implicit channel creation based on the adapters’ id attributes or the transformer’s input-channel attribute, but the goal here is to make the example as clear as possible.
For that reason, the configuration defines all channels and endpoints explicitly.
Because the inbound channel adapter is a polling consumer, its task is scheduled automatically.
After running for about 30 seconds, the output looks like this:
Earlier, we mentioned that the history-tracking mechanism informs you not only of where the message has been but also when it was at each component.
From the output, you can see that it records the name of each component it passes, but the time tracking seems to be lacking.
What’s happening here is that only the result of the history header value’s toString() method is shown.
To avoid a lot of extra noise, in typical logging scenarios, that method is implemented in a purposefully simple way.
The actual value of the history header is essentially a list of key-value pairs.
In other words, in addition to the name of the component, it holds the type of the component and the exact time in milliseconds at which it was tracked at that component.
After running this revised example for 30 seconds, you see output similar to this:
As you can see, when logging the individual entries of the history header value, both the component type and timestamp information are available in addition to the component name.
Let’s take a quick look at these timestamps, which are recorded in milliseconds, to learn how the system is performing.
First, we compare the timestamp of each subsequent invocation of the logger component.
Only the last five digits are significant because the rest of the digits are exactly the same.
That’s to be expected because the poller has a fixed delay of 10 seconds.
The differences between those values would likely be closer to 10 seconds if the fixed-delay attribute were changed to a fixed-rate attribute on the poller element.
Another thing you see in this example is that not much time is spent in the flow.
The middle of the three flow executions shows no quantifiable elapsed time at the millisecond level of granularity, and the difference between start and end times in the last execution is only 1 millisecond.
The first execution shows 6 milliseconds, but that’s most likely a result of priming the system.
After running the example several times, a similar difference is consistently recorded for the first execution, and the subsequent executions always show negligible overall times.
If some CPU-intensive processing occurred within the flow, for example, on methods being invoked from service activators, these history timestamps would be helpful for determining where in a flow the time is being spent.
Most important, because the history data is recorded and stored with the message, those timestamps are available regardless of how many thread context boundaries may have been crossed during the message flow.
In chapter 3, we covered the ChannelInterceptor interface and the methods it provides for receiving callbacks before and after sending messages on a message channel as well as methods that are called before and after receiving if that message channel is pollable.
The ability to add interceptors to a channel is one of the key advantages of a.
It goes hand in hand with the loose coupling and further promotes separation of concerns in a way similar to using aspect-oriented programming (AOP)
As with AOP advice, the interceptors enable the addition of cross-cutting concerns, such as logging or security, to be added to any channel in a reusable way without requiring direct modification to that channel.
Another important crosscutting concern is monitoring, and because that’s the topic of this chapter, we now revisit the role of a ChannelInterceptor that’s specifically dedicated to that concern.
But sometimes you might want to reuse functionality available from the integration framework for the monitoring.
For example, you might only be interested in gathering information about messages whose payload contains certain data, and obviously a message filter could be useful in such a situation.
Likewise, a message router might be a convenient way to send messages to different monitoring services dedicated to certain types of message content.
For these reasons, Spring Integration provides an out-of-the-box interceptor that supports the Wire Tap pattern.
Imagine a banking application in which messages are published every time a debit is requested.
Then consider the requirement to monitor all debits whose transaction amount exceeds a certain threshold, such as $10,000
This could be implemented in the main message flow by inserting either a publish-subscribe channel or recipient list router that passes the messages along to a logger, which is preceded by a filter that checks the balance of the transaction represented by the message payload.
The flow would look something like that shown in figure 14.1
Even though it gets the job done, the flow pictured in the figure may not be the best way to achieve the goal.
It’s somewhat subjective and needs to be decided on a case-by-case basis, but the key factor in making this decision is whether the role of that logging component is really considered part of the main flow.
Perhaps this is a mandatory compliance-driven auditing function, and the logging component is a first-class component of the application.
On the other hand, it might be a cross-cutting concern, or what some may call a nonfunctional requirement.
For example, if the main purpose of the logging is to monitor the message flow for data that’s covered by some.
Figure 14.1 A publish-subscribe channel enables multiple downstream message flows but doesn’t make any clear distinction between secondary and primary flows.
In such a case, the wire tap pattern may be more appropriate.
The choices are without limit when it comes to the downstream flow initiated by such a wire tap.
That’s obvious when you consider that it’s just like any other message flow that happens to run perpendicular to the main flow in a logical sense.
All of the components that Spring Integration provides, and all of the options for invoking methods on Spring-managed objects or evaluating expressions, are available to that tangential flow as well.
Here’s the configuration that corresponds to the wire tap diagram:
For this example, the Debit domain object contains just two properties: amount and accountId.
Along with a constructor to set those properties and some methods to retrieve them, it provides a toString() method so that the logging channel adapter’s output provides useful information.
Figure 14.2 A Wire Tap connected to a channel enables a flow with crosscutting behavior to be added while maintaining a clean separation from the primary message flow.
Then there’s the DebitService code, and for purposes of this example, we just print a message to the standard output:
Finally, we need to send a few messages through the debitChannel to demonstrate the wire tap functionality.
The following code shows a main method that sends two different debit messages.
The first one is for an amount under the $10,000 threshold we set for the required auditing.
When executing that main() method, the output would look like the following.
As you can see, both debits were processed by the DebitService, but only the one whose amount exceeded the $10,000 threshold was logged by the channel adapter.
The other message, with the $5,000 debit amount, didn’t make it past the filter’s SpEL expression and was therefore prevented from continuing in the audit flow:
The key point is that the wire tap can initiate a message flow that uses the same Spring Integration endpoint types that are available to the primary flow.
At the same time, the wire tap enables a clear separation for that secondary flow.
It’s easy to disable that flow by removing the wire tap interceptor from the message channel.
Likewise, you can move that wire tap to another channel or apply it to more than one.
You can even configure the wire tap as a global channel interceptor by defining it as a top-level element.
If you do that, be sure to provide a channel name pattern to restrict the wire tap to the set of channels that should be intercepted.
It might be a good idea to rely on naming conventions within your application to facilitate that.
The following configuration demonstrates such a naming convention along with a top-level wire tap element applied globally based on pattern matching against channel names:
One other thing you may have noticed in the configuration is that the auditChannel contains a dispatcher subelement that provides a reference to an Executor instance.
That Executor-driven dispatcher on a message channel enables a nonblocking handoff to a thread within a pool.
In other words, the flow initiated by this wire tap can be asynchronous.
That means the primary flow isn’t held up by this secondary flow.
The only disadvantage of such a configuration is that the wire tap’s flow won’t participate in any transaction that may have been in progress because the thread boundary is crossed and the transactional context doesn’t propagate to the new thread.
In many cases, that’s the desired behavior, especially if a failure in the wire tap flow isn’t important enough to worry about rolling back the primary flow.
You might sometimes need the wire tap flow to be transactional.
Perhaps the auditing is required to meet compliance regulations, and any transaction that isn’t audited is in violation of those regulations.
In those cases, a nice middle ground solution that offers the best of both worlds is to use a channel with a queue subelement where the referenced queue is backed by a transactional message store implementation, such as the JDBC implementation that Spring Integration provides.
That way, the message is still handed off without blocking the primary flow, but it’s persisted within a transaction so that failures in the wire tap flow don’t result in lost data.
Of course, if the wire tap flow is important, you might consider whether it’s better to think of it as a parallel primary flow after all.
In other words, it might be better to use a publish-subscribe channel or recipient list router, as discussed earlier.
It’s worth recognizing the flexibility that the choice of message channel types provides in these various wire tap scenarios.
It may at first seem like every wire tap flow should be asynchronous by default, but when you encounter a scenario such as compliance-driven auditing, it’s nice to know the choice of channel types exists.
It might be common to have asynchronous wire tap flows, but doing so is as simple as adding the dispatcher subelement demonstrated earlier.
Once again, the same capabilities of the framework are available to wire tap flows as are available to the primary flow, even down to the choice of message channel type.
Now that you know how to add a wire tap where you have the full power of the framework available to build some custom flow for monitoring purposes, we turn to another powerful tool in the monitoring toolbox.
In the next section, we explore the ways you can take advantage of Java Management Extensions (JMX) in your message flows.
You’ll learn how to monitor certain attributes of the message channels as well as the message endpoints within the application context.
Attributes exposed via JMX can either be read-only, such as the number of messages on a queue, or read-write, such as the maximum number of database connections allowed in a connection pool.
As well as exposing attributes, JMX allows operations to be exposed.
Each operation relates directly to a Java method invocation with zero or more parameters.
In addition to operations and attributes, JMX has a concept of notifications, which are essentially events emitted by a component.
These are typically used to notify listeners of some sort of problem rather than a more general form of interprocess or intercomponent communication.
One of the advantages of using JMX is the tooling support: many monitoring systems support JMX, allowing operations teams to use the tooling they may already use to monitor hardware to also monitor Java applications that expose information via JMX.
Where JMX isn’t directly supported, it’s also possible to map some of the JMX concepts to other monitoring technologies; for example, JMX notifications can be mapped to SNMP (Simple Network Management Protocol) traps.
First, we look at the out-of-thebox support for exposing information about message channels, message sources, and message handlers.
We then look at Spring Integration’s support for adapting JMX concepts to Spring Integration messages, and vice versa.
The out-of-the-box JMX support means that if you add the following configuration, a wide range of information about the runtime behavior of your application is made available.
In the case of channels, the implementation type of the channel determines the amount of data to be monitored; a default set of information is made available for all channels, additional information is made available for pollable channels, and yet more information is made available for channels that are both pollable and backed by a queue.
The default message channel implementation exposes the attributes shown in table 14.1
Where an attribute relating to rate or ratio over time is exposed, it’s calculated using an exponential moving average.
This allows for the calculation of rates and ratios that give a higher weight to more recent data items yet precludes the need to constantly recalculate or to maintain a long list of data points.
In applications requiring high performance or throughput, it’s worth considering that there will be some effect from observation.
The “Under the hood” section explores this overhead in more detail.
Table 14.1 Default set of metrics exposed for message channels.
Send count int Number of messages sent to this channel.
Resending the same message a number of times will increment this value repeatedly even where it isn’t successfully processed.
Send rate Statistics Provides statistics relating to the rate of messages being sent on this channel.
Includes a mean number of messages per second using the previously mentioned exponential moving average.
Retrying failed message sends results in multiple entries for the purposes of this statistic.
Time since last send double Time in seconds since last successful or unsuccessful send.
Mean send rate double Calculated mean number of messages sent per second.
The attributes listed in the table relate only to sending because the MessageChannel interface doesn’t define any methods related to receiving messages.
Channel implementations that cater to asynchronous receive operations where the send doesn’t result in a direct pass to the receiver additionally implement the PollableChannel interface.
Where this interface is detected, additional details related to receive operations are exposed, as detailed in table 14.2
Where a channel is backed by a queue, it’s often useful to know how many messages are currently queued and what the remaining capacity of the queue is.
Classes that extend from QueueChannel additionally expose the metrics shown in table 14.3
Send duration Statistics Statistics related to successful send durations in seconds.
Min send duration double Minimum recorded time for a successful send in milliseconds.
Max send duration double Maximum recorded time for a successful send in milliseconds.
Send error count int Number of sends that resulted in an error.
Mean error rate double Calculated mean per second of messages sent that resulted in an error.
Error rate Statistics Statistics relating to message sends resulting in an error.
Receive count int Calls to receive that returned a non-null result and did not result in an error.
Receive error count int Calls to receive that resulted in an error.
Table 14.3 Additional metrics exposed for channels backed by a queue.
Queue size int Current number of messages queued and waiting to be received.
Queue remaining capacity int Number of messages that can be queued before queue is full.
Table 14.1 Default set of metrics exposed for message channels (continued)
Monitoring channels is useful for checking throughput and error rates, but other important statistics can be tracked by monitoring Spring Integration components such as routers, transformers, and adapters that act as message sources and handlers.
Components that act as handlers implement the MessageHandler interface in some form.
It may not be obvious when writing a plain old Java object (POJO) router that it will effectively implement this interface, but there will always be either an adapter or a superclass that implements this interface somewhere at runtime.
Table 14.4 details the MessageHandler metrics exposed for monitoring purposes.
In addition to channels and handlers, the built-in support also exposes message sources.
These are typically pollable sources of messages, such as inbound file channel adapters, whereby a scheduled task periodically calls receive on the message source and any non-null result is then published to a channel as a message payload.
In the case of the message sources, a count of messages received in response to calls to the receive method is maintained and exposed as a JMX attribute.
In addition to exposing attributes, the mbean-export element in a Spring Integration configuration triggers the exposure of a number of operations.
All of the monitored components expose a reset operation that allows all metrics, such as counts and rates, to be reset to zero.
In addition, active components (components having some scheduled periodic behavior, such as polling a channel) generally implement the Spring Lifecycle interface, which defines stop, start, and isRunning methods.
These methods allow active components to be stopped individually without the need to stop the whole application or application context.
For convenience, MessageHandler and MessageSource instances that implement the Lifecycle interface have these methods exposed as JMX operations.
Error count int The number of calls to the handle message method that resulted in an error.
Mean duration double Mean duration of handle calls in milliseconds.
Min duration double Min duration of handle calls in milliseconds.
Max duration double Max duration of handle calls in milliseconds.
Active count int The number of calls to handle messages currently in process.
In this section, we look at the support offered by Spring Integration for JMX notifications, operations, and attributes in turn.
The outbound adapters’ primary role is to make it easy to manage and monitor Spring Integration applications.
The inbound adapters allow the use of Spring Integration to carry out the management and monitoring of applications via JMX, whether those applications use Spring Integration or not.
It’s also common to use Spring Integration both for the core application functionality and the management and monitoring of the core application.
One of the most common monitoring requirements for an application is that it should produce notifications when things go wrong so problems can be addressed early rather than waiting until a small unnoticed problem escalates to a show stopper.
Using JMX, the best way to achieve this requirement is through notifications that allow an application to send data that may indicate a problem to subscribed listeners.
The concept of a notification maps well to a message, so the support for JMX notifications takes the form of channel adapters that map between notifications and Spring Integration messages.
The inbound channel adapter is simple to set up and requires a channel name and a JMX object name.
The following configuration assumes a Spring bean named mbeanServer is available:
The following example demonstrates that and the use of an MBean server bean with a nonstandard name:
When it comes to JMX operations, Spring Integration provides support for operation invocations where no result is expected through a channel adapter implementation.
In both cases, the invocation of the JMX operation is triggered by the receipt of a Spring Integration message that’s used to determine the parameters to pass the operation if any are required.
Both the gateway and channel adapter implementations use the same strategy for mapping the inbound trigger message to operation invocation parameters.
Where the payload of the message is a Map, each key-value pair within that map is considered to be a parameter.
Where only a single parameter is expected, the payload itself is assumed to be that parameter.
Where the JMX invocation doesn’t expect any parameters, the payload is ignored.
Given that the adapter carries out the parameter mapping, all that’s required is the JMX object name, the operation name, the request channel name, and the name of the MBean server if it’s not the default of mbeanServer:
The operation invoking gateway looks almost exactly the same except it allows for the configuration of a reply channel:
The final JMX adapter allows for periodically polling an attribute exposed over JMX.
The attribute value then becomes the payload of the resulting message:
As described in Enterprise Integration Patterns, the idea of a Control Bus is to use the same messaging components that are used for the application to manage the messaging system itself.
In other words, you should be able to invoke management operations on endpoints and other manageable resources in the system by sending control messages to a message channel.
That channel essentially plays the role of an operation channel.
Spring Integration supports this notion of a control bus by allowing you to send messages whose payload represents a command to be executed.
The target of that execution can be any Spring-managed object that happens to be manageable.
Two alternative syntaxes are supported for the command: SpEL and Groovy.
First, let’s explore Spring’s support for annotations that label managed resources.
This is a good starting point because Spring Integration relies on that same mechanism to determine valid targets of command execution.
The Spring Framework defines a handful of annotations in its JMX support to facilitate building applications whose components may be monitored and managed.
This is yet another example where having a container that’s aware of the objects defined within its context, such as Spring being aware of its beans, allows other behavior to be added to those objects.
It’s the same general idea that we’ve described elsewhere in this book of using AOP to add cross-cutting concerns noninvasively.
In this case, the cross-cutting concerns are monitoring and management.
The class-level annotation that Spring defines is @ManagedResource, and its role is to indicate a candidate for monitoring and management.
Here’s a simple example of a class annotated with @ManagedResource that indicates a single property that can itself be managed.
The annotation for that property can be applied at method level to the getter and/or setter, and its name is @ManagedAttribute:
You can see that the numeric value should be managed in terms of both read and write operations.
If you’d like to expose other operations that don’t map to simple JavaBean-style getters and setters, then you can add @ManagedOperation annotations to such methods.
Now, an instance of NumberHolder configured within a Spring application can easily be exposed for JMX monitoring and management.
To export those attributes and operations from the Spring context requires only a single configuration element from Spring’s context namespace:
As you’ll learn in the next section, those same annotations are used in Spring Integration to determine what operations may be invoked on components through the control bus.
The control bus’s awareness of invocable operations isn’t limited to the annotated classes in Spring Integration’s own API.
You can use those annotations anywhere in your code and then have the annotated methods available as operations on the control bus.
The next two sections present a few examples of invoking such operations.
We cover two different options for the syntax of the command messages sent to the control bus channel: SpEL and Groovy.
The control bus is a Spring Integration component that accepts messages on its input channel much like a service activator, transformer, or other type of message endpoint.
The key difference with the control bus is that the payload of a message it receives is expected to indicate an invocable operation.
The Spring Integration core namespace provides an implementation that evaluates SpEL expressions for those operations.
To enable such a control bus in your application, add the following element to its configuration:
The message payloads sent to the control bus should contain SpEL expressions referencing a bean that’s the intended target of the control operation.
The following example shows how to send a control message that increments the value of the NumberHolder instance discussed in the previous section:
Knowing that, you can provide any number of beans within your application context that will be valid targets for control bus operation messages.
A major use of the control bus is to manage the Spring Integration components.
Probably the most common use case is to start and stop endpoints.
Polling consumer endpoints can start and stop polling on demand.
Even event-driven consumer endpoints can be controlled this way because the start operation will correspond to subscribing, listening, or simply activating such an endpoint.
To accommodate these and similar use cases for a large number of other Spring components, the valid operations also include those methods defined on the Lifecycle interface, even if the @ManagedOperation annotation isn’t present.
Here’s an example of stopping an endpoint by sending a message to the control bus:
Obviously, those lifecycle operations can be exposed on your own Spring-managed objects as well if their classes implement Spring’s Lifecycle interface.
Finally, you can also invoke certain operations on the configurable thread pools that Spring defines.
For example, imagine that you have the following configuration, utilizing Spring’s task namespace, to create a thread pool Executor instance within your application context:
Certain properties of that thread pool are configurable, such as the core and max pool size values.
The following example shows a control bus operation that sets the max size of that thread pool:
Now that you’ve seen a number of possibilities enabled by the control bus, let’s take a quick look at an alternative implementation, one that accepts Groovy syntax rather than SpEL in the control message payloads.
The Groovy control bus is basically the same as the SpEL implementation included in the core module.
This implementation compiles each control message’s payload into a Groovy script.
It then makes any managed bean in the Spring context available as a variable to that script.
The beans that are eligible to be managed in this way are chosen according to the same criteria described in the previous section.
Any instance of a class with @ManagedResource, any Lifecycle implementation, and Spring’s thread pool classes are all eligible to be accessed as variables in the Groovy script payload of a message sent to the Groovy control bus.
We mentioned earlier that monitoring channels and endpoints with JMX may cause some small performance overhead.
As with all potential performance issues, don’t panic: wait until you know for sure that you have a problem because premature optimization can introduce as many problems as it solves, or more.
If you’re interested in what happens under the hood, this section provides a detailed view of the implementation of statistics gathering for channels and endpoints.
To determine the overhead, it’s worth running a small benchmark on your system and keeping in mind that the overhead relative to the request processing is probably more important than the absolute figure for the overhead.
For example, where you’re hitting a database an overhead of a few hundredths of a millisecond will be insignificant.
This was run on an early 2011 quad-core MacBook Pro.
In almost all cases, such overhead is insignificant in the context of the processing that’s occurring.
It’s helpful to understand what the Spring Integration framework is doing under the hood when you include the mbean-export tag from the Spring Integration JMX namespace.
The first thing that needs to happen is that the framework must start monitoring when certain methods are called on beans implementing the key Spring Integration interfaces.
This is achieved by using the standard Spring strategy of a BeanPostProcessor.
The BeanPostProcessor receives a callback for each bean at two points in the bean’s lifecycle: before any initialization has occurred and after initialization.
The interesting thing about the method signature is that it has a nonvoid return to allow something other than the bean passed as an argument to be returned, effectively changing the instance that’s available in the application context under this bean name.
The most common use of this approach is to return a wrapper of some sort.
That’s how Spring enables AOP, which allows you to apply cross-cutting concerns such as security and transaction management by wrapping the bean and intercepting calls to its methods:
Spring supports two implementations of AOP: one using the AOP Alliance project (essentially wrapping objects as just described) and the other using AspectJ.
Many people prefer AspectJ because they regard it as more powerful, but it comes with some additional setup complexity.
Its use requires some form of weaving, either postcompilation or at class load time.
Although that complexity has decreased in recent years, in part with help from Spring, most Spring-based AOP still relies on the AOP Alliance library, which uses built-in JDK dynamic proxies to create a wrapper instance of a specified interface at runtime.
This bean implements BeanPostProcessor, which means it gets a callback for each bean defined in the application context.
This callback tests to see if the bean passed in implements MessageChannel, MessageSource, or MessageHandler.
This is where the overhead enters the picture; for example, each call to send on a message channel passes through an interceptor, which decides if the method call is of interest and then updates the associated metrics as appropriate.
In this chapter, we looked into the monitoring and management aspects of a messaging application and how they can be implemented using Spring Integration.
We distinguished between monitoring the messages and monitoring the channels and endpoints.
We used MessageHistory support for the former and wire tap and JMX support for the latter.
It can be invoked through control messages using either SpEL or Groovy to specify the operation and its target bean.
In the previous chapters, you learned about the core and specialized components and how to put them together to build a successful application.
But there’s more to building an application than finding all the parts you need and putting them together into a structure.
This chapter takes a turn to discuss application building from a different perspective: dynamic configuration.
Dynamic configuration requires shifting focus from the logic of the application to the expected behavior at runtime.
Configuring runtime behavior involves controlling the schedule of various timed events in the application, such as the frequency with which external message sources are polled, and also configuring various parts of the application to run concurrently, making optimal use of system resources, and increasing the rate at which messages are being processed.
You learn to manage scheduling and concurrency in this chapter.
A final word before we start: sometimes, fine-tuning the dynamic performance of an application is more art than science because no two applications are exactly the same.
No prescription can tell you precisely what to do and when, and often you must experiment to find what works best in your case.
But engineering is an art too, so we provide you with the options and guidelines on best practices, leaving it to you to determine what works best for your application.
As the previous chapters made abundantly clear, communication is the predominant point of interest of enterprise integration applications.
Factors such as the format of data that’s being exchanged, the processing services, ordering, and most important, timing play critical roles in achieving this goal.
Integration applications often must satisfy timeliness requirements, which can be of various types.
You might have responsiveness requirements, defining how quickly the system should respond when a new message becomes available, or scheduling requirements, prescribing how often or at what time the application should interact with its counterparts.
This section focuses on the framework features that control the timed behavior of the application.
Pollers are active components that pull new data from external message sources or trigger the handling of the existing messages stored in the system’s queues.
Such an asynchronous mode of operation, in which the responsibility of acknowledging the existence of new data falls on the service provider, increases the overall robustness of the system, but it also introduces a lag that’s ultimately related to how aspects such as frequency, timeout, or number of messages processed per polling cycle are configured.
Timed events are not just about polling, though, so we show you how to set up a scheduled producer for invoking existing beans and sending out messages at specified times.
An analysis from chapter 4 explained that the main distinction between event-driven and polling components is in how they receive messages that they have to process.
Event-driven components receive messages directly from their invokers (see figure 15.1), and polling components must reach out and consult a message store to find out whether any new data is available for them.
In Spring Integration, polling is used internally by endpoints for receiving messages transmitted asynchronously through queue channels (see figure 15.2) and also by channel adapters for reading data from a message source, which is typically used for retrieving external data.
Figure 15.1 Event-driven components receive messages directly from their invokers.
Whether an application needs to use polling at all is often a matter of choice; for example, you might deliberately introduce a queue channel for handing off messages asynchronously between components.
In other cases, it’s a matter of necessity: not all external systems can integrate in an event-driven fashion by pushing messages into an application; instead, an application often has to reach out and grab the data.
For example, receiving up-to-date weather information about airports may require accessing an external web service that is exposed publicly and can be invoked by the interested parties, as opposed to receiving notifications from a subscriber service.
In the former case, polling the web service is the only option available.
If you followed the chapters in order, you saw <poller/> elements whenever we configured a polling channel adapter or an endpoint consuming from a queue-backed channel.
Because polling behavior varies across message source types, and its configuration has a strong impact on performance, no default is provided by the framework.
Any polling component in your application must have a polling configuration, describing the polling frequency, timeouts, maximum number of messages to process per polling cycle, and whether the polling task should be deferred to a task executor.
Polling component definitions don’t have to include an explicit poller configuration; they can instead rely on the existence of an application-wide default configuration, which has to be provided by the developer.
This makes it possible to simplify component definitions in general (because the default is usually a configuration that applies to the majority of the polling components)
At the same time, this scheme still allows you to provide a custom configuration wherever the default doesn’t fit the needs.
In both cases, a <poller> element is used to define a set of polling configuration parameters.
The following listing shows you how to configure two different polling endpoints using the default and individual configuration.
The picture shows a typical scenario for a polling endpoint: the invoker sends the message to a buffered channel B, and the polling endpoint receives it by polling the channel C.
The definition of the default poller configuration is shown at B.
The channel adapter, defined at D, defines a different policy because it needs to consult the message source much less frequently.
In general, it’s good practice to create a default configuration to be used for consumers that poll queue channels; it simplifies the overall configuration, and channel adapters are likely to need customized configurations anyway, so they rarely rely on the default.
Note in the listing that the numeric values are configured in the fixed-delay and fixed-rate attributes of the poller elements.
They’re used for configuring the polling frequency, and we look at them next.
The frequency at which pollers operate has a strong impact on the speed with which messages travel across the system and on the responsiveness of the system.
The more frequently the application polls a message source or a queue channel, the more likely the polling event is to occur close to the moment when a message becomes available, and so the application will be able to process it and respond faster.
Over time, this leads to a shorter lag on average.
But not all types of messages are equally important, and not all messages require immediate attention, so every application should set up a polling scheme that’s consistent with the timeliness requirements of the application, which often define what is an acceptable delay.
As a consequence, the polling frequency should be configured to balance performance with resource usage.
Because an application must be highly responsive, you might consider setting the frequency to have the shortest possible delay.
But every polling operation consumes computing resources that may be needed elsewhere.
High-frequency polling is appropriate in terms of resource usage if the effort of checking for new messages is minimal and if the messages are likely to arrive at a relatively high rate.
In general, if polling operations are expensive or blocking for a relatively long time (such as occurs with remote accesses), it’s a good idea to perform them less frequently; otherwise, the system will be busy all the time performing checks that may not produce any result.
The default polling interval is set to a fixed delay of 0.5 seconds (all numeric values are in milliseconds)
This means that after each polling operation, the system waits half a second before trying again.
Note how fixed-delay polling causes the timing of the polling event to drift over time, but fixed-rate polling takes place at predictable moments.
The two approaches are similar, and the differences are barely noticeable when the polling operation is extremely short.
A fixed-rate setup guarantees that polling will happen at definite moments, which in turn makes the application behave in a more predictable way.
A fixed-delay strategy allows a consistent period of inactivity between two successive polling operations.
Table 15.1 shows you a few points to consider when deciding between the two.
Fixed-rate scheduling produces a predictable sequence of polling events, but they aren’t entirely deterministic.
Let’s consider an email client that synchronizes with the server every 10 minutes.
In this case, it doesn’t matter when the 10-minute period starts as long as the interval is respected.
But there are other operations for which the start time is important.
For example, batch operations such as payment transaction processing and bulk reference-data changes are usually processed outside the normal application operation hours.
Batch operations are resource intensive too, and running them while other users are accessing an application may impact overall performance.
Table 15.1 How to decide between fixed-rate and fixed-delay strategies.
Fixed-rate – Ensures that messages are read at well-defined moments – Works best when the duration of the polling operation is fixed and short relative to the polling period; otherwise it may force the system into a busy loop.
Fixed-delay – Guarantees a minimum silent interval between successive reads – Works best when the duration of the polling operation is variable or when the.
A fixed-rate scenario guarantees that polling will happen at the same preset interval, but the gap between poll operations completing and starting may vary.
A fixed-delay scenario guarantees that the interval between polls is constant, but the timing of the polling event may drift over time.
For batch operations, instructions such as execute this task every 24 hours aren’t precise enough.
You must specify, for example, execute this job every day at midnight.
Neither fixed-rate scheduling nor fixed-delay scheduling have the capacity to describe this type of configuration.
The frequency and polling strategies—fixed rate, fixed delay, and cron expressionsare the most frequently used settings for customizing the polling behavior.
Polling performance is also affected by other parameters, such as timeouts and the number of messages processed per cycle.
Optimizing the polling strategy doesn’t stop with finding the best frequency for the polling cycle.
Consider a case where you need to poll a message source for which a read operation is time consuming, regardless of whether data is available, which happens often when doing invocations across the network.
The polling operation will be blocked until the read operation completes, but it can’t wait there indefinitely; that would prevent the application from performing other useful jobs.
If you must place a limit on the time a poller can spend waiting for a message to arrive, you can configure the value of the timeout as follows:
In the example above, the timeout is set to 2 seconds.
If a message arrives after 2 seconds, it’s picked up by the next polling operation.
Setting a reasonable timeout for polling operations is important for the performance of your application.
Setting a large timeout value without good reason won’t help performance.
Not getting a response beyond the acceptable waiting time can mean a lot of different things but should be interpreted as a sign that either a message isn’t available or the resource from which data is read is unavailable, so there’s little chance of reading any data from it.
But the timeout value shouldn’t be set too low either, because long-running read operations must be allowed to complete.
Generally speaking, timeouts should be set as low as possible but not lower than the longest duration of a normal read operation plus some contingency added in.
For example, a timeout in the range of a few tens of seconds for a remote web service call could be reasonable, whereas reading from a queue channel.
Cron expressions Cron expressions are schedules tied to the system clock and calendar (say, “every day at 5:00 p.m.”) that were originally used in UNIX systems.
The cron expression variant used by Spring is a string comprised of six fields separated by space, representing (in order) second, minute, hour, day, month, weekday.
Wildcards and special expressions can be used for describing a wider range of values.
This is just an example; your settings should be based on how your application works.
Let’s consider the case of a poller that runs every second.
Let’s also assume that not one but two messages become available on the message source.
With what we configured so far, here’s what will happen: when the first polling cycle begins, the poller reads the first message and starts processing by delivering it to an endpoint.
The second message will be left unprocessed until the next polling cycle.
You can immediately see how this configuration can become a limitation; the rate at which messages can be consumed is limited by the frequency of the polling cycle.
Besides polling the message source more frequently, another option can help in this case: receiving more than one message per polling cycle.
With this setting in place, both messages will be received in the same polling cycle without the second message having to wait.
You can choose any maximum value for the number of messages that you can receive in a cycle, but setting the value too high isn’t a good idea either because it may increase the time that the poller spends reading messages beyond an acceptable threshold.
With a value of 2, the messages are processed as part of the same polling cycle (bottom)
This concludes our overview of poller configuration as a way of controlling the timerelated aspects of the application.
But there’s another mechanism you can use for triggering timed events: the scheduled publisher.
Besides polling a message source or queue channel, another type of timed event can take place in your application.
Spring Integration allows you to configure a generic channel adapter that publishes messages regularly using the same polling configuration attributes.
Suppose you want to execute a periodic cleanup task on your application’s database.
Every 10 minutes, a message is sent to the clean-input channel, triggering the invocation of the cleanup service (and in turn cleaning up the database)
The power of this setup is that the service activator can be triggered by multiple channel adapters sending messages to the clean-input channel.
The payload of each message may contain configuration data, so the specific values for each adapter triggering execution of the target service can be parameterized.
This technique can essentially be used to turn a service activator into a channel adapter.
Consider the case when an application needs to send out reminder or notification emails.
Such emails may be necessary for notifying customers that they have a scheduled flight on the same day.
This is usually a multistep process, which includes searching the database for such reservations, generating notification messages, and sending them as emails.
The scheduled channel adapter simplifies the implementation of such a solution because the payload of the published messages can be set up in a SpEL expression, which in turn can invoke a search Data Access Object (DAO)
The process consists of a poller triggering a channel adapter that generates a list of reservations which need notification, a transformer that creates messages out of the reservations, and an email outbound channel adapter that sends the messages created by the transformer.
You now know how to incorporate time-related concerns in your application’s design.
Polling and timeliness are also closely related to concurrency management, because poller tasks and asynchronous operations in general are concurrent operations.
Enterprise applications can increase their performance and responsiveness by executing concurrent operations.
For example, the application may be required to be able to handle multiple incoming requests at once.
It could also be a way of taking advantage of the capabilities of the hardware, especially since multicore and multiprocessor systems have become mainstream.
In either case, the problem of creating an application that executes tasks in parallel can be split into two parts, with orthogonal concerns.
First, you must establish an application design that allows operations to execute in parallel, mainly by splitting multistep operations into finer-grained parts that get executed asynchronously.
One important counterpoint is that single-threaded operations can propagate context information, such as the transactional and security context, across endpoints, so losing that behavior is a significant trade-off.
The first subsection walks you through the mechanics of breaking down single-threaded processes into multiple asynchronous tasks.
Second, after establishing the right application design, you must allocate resources (threads) to execute the parallel operations.
Again, this involves a number of tradeoffs between maximizing the number of concurrent operations and making sure the system’s resources are used efficiently.
In the second subsection, we describe the thread management infrastructure of Spring Integration and how it allows changing the configuration parameters (and implicitly the concurrency degree of the application) in a transparent fashion and without affecting the application design.
An enterprise integration application is an assembly of message processing endpoints connected by channels.
A message sent to a channel undergoes a series of transformations while it travels through this structure.
From a purely logical standpoint, you can start by ignoring whether this series of transformations happens within the scope of a single thread or spans multiple thread contexts; the processing that takes place within.
Managing concurrency each individual endpoint and the end result will be the same.
In fact, Spring Integration applications may work just fine by adopting a single-threaded approach, which is also the default.
Without any additional configuration options, processing a message from end to end takes place within the scope of a single thread.
Single-threaded processing may work well when the goal of using Spring Integration is to encapsulate a multistep process that includes invocations across multiple services and systems.
In these situations, you don’t need to worry about concurrency.
Multistaged processing pipelines are good candidates for parallelization if it’s acceptable for the various stages to execute in separate thread contexts.
To illustrate the concept, let’s begin with a simple example: a car assembly operation.
Building a car goes through two stages: assembling and painting.
The simplest possible setup can be seen in the following listing.
A PieceKit payload is delivered to the AssemblyLine where it’s assembled into a car.
The car is painted in the PaintShop and sent to the final counter, which records how many cars were built in a given time frame.
An overview of the whole process is shown in figure 15.5
PieceKit AssembledCar FinishedCar Figure 15.5 The car assembly operation consists of assembling and painting.
The two stages can be executed in the same thread or asynchronously.
In the default case, because direct channels are used, the whole process of building a car is single-threaded.
You can save time if you allow for creating cars in parallel by keeping the current configuration and sending messages in separate threads.
This type of scenario would happen, for example, in web applications, where different HTTP requests are processed in separate threads.
What we want to focus on, though, is how to use the framework for handling parallelization internally.
Instead of executing both stages in the same thread, let’s execute them in parallel; after being assembled, the car is handed over asynchronously to the PaintShop, and the invoking thread returns, making room for another PieceKit to be sent to AssemblyLine.
The difference between the two scenarios is shown in figure 15.6
With Spring Integration, switching between scenarios is a matter of choosing different types of channels without modifying the logical structure of the application.
You can hand over messages asynchronously by using a queue channel or by using an executor channel.
When you change the input to a queue channel rather than a direct channel, the processing of the message by the endpoint doesn’t execute in the original messagesending thread, which loses control over the message once it’s enqueued.
Instead, the message is processed in the scope of a new task initiated by the poller of the endpoint that’s connected to the channel.
The message-sending thread returns to the loop and is now free to send another new message to the channel, and so on, blocking only when the queue of the channel is full.
Meanwhile, the poller repeatedly removes elements from the queue and processes them.
To enable such behavior, you only need to add a queue subelement to the channel:
You can get a similar result by using an executor channel.
This is a subscribable channel connected to a task executor, which provides a thread in the context of which the application will process the new message.
The task executor is the main backing abstraction for thread allocation in Spring.
Instead of creating a new thread directly for executing a concurrent task, the application components pass the task that needs.
Figure 15.6 The difference between the singlethreaded and concurrent approach.
Scenario C is concurrent: the assembly and paint take place in parallel, which increases the processing speed.
Managing concurrency to be executed asynchronously to a task executor, which encapsulates the thread allocation strategy.
For that behavior, add a dispatcher subelement with a reference to a task executor:
There are differences between them that make each one more suitable for different scenarios, as you can see in table 15.2
In general, asynchronous handoff is a solution for processing multiple messages concurrently, which is a common occurrence in scatter-gather and publish-subscribe scenarios.
As in the previous example, you can set the output of the splitter to be either a queue channel or an executor channel, and every resulting message will be processed independently.
If you’re using a publish-subscribe channel, you can define a task executor at the channel level, allowing each subscriber to process the message in a separate thread:
Now that we’ve clarified the design decisions that allow for the concurrent execution of tasks, we can discuss how to set up the backing infrastructure for asynchronous execution and review in more detail the configuration of a task executor.
There’s a distinction between designing your application so that it has parallel execution capabilities and whether its tasks actually execute concurrently.
The distinction comes from the fact that no matter how much you desire to run things in parallel, the.
Table 15.2 Queue channel versus executor channel: what you need to consider.
Queue channel – Wide choice of queuing and storage strategies – Allows advanced features such as prioritization or persistence – Need for polling introduces a lag, influenced by the polling strategy – Support for distributed message processing if the queues or backing message stores are accessible from different virtual machines.
Executor channel – No polling lag: messages are processed as soon as thread is available – Less robust in face of an increased load – Unprocessed messages may be lost (depending on rejection strategy) – Lacks support for distributed message processing; dispatching task must be.
In the previous section, we talked about the task executor, which is the main component that enables the asynchronous execution of tasks.
Now, we provide an example of how to configure such a task executor.
To clarify the motivations behind it, though, we first look at the main concerns regarding thread allocation.
Thread allocation is an infrastructure concern best kept separate from the business logic to allow an easy switch between environments—for example, scaling up and down or even running on different platforms.
It addresses the main concerns regarding thread management, which can be summarized as follows.
Only a limited number of tasks may execute at once.
Concurrent applications make better use of the system’s resources, but too much of a good thing may be bad, and executing too many tasks at once may backfire.
Each thread consumes its own share of memory, CPU, and other resources, and as consumption increases, a large number of highly concurrent activities may perform worse than the same number of activities executing with moderate concurrency (for example, by limiting the number of threads that can execute concurrently and forcing some of those activities to wait until their predecessors have completed)
Even if you limit the number of possible concurrent activities in the system, creating new threads is an expensive process, so destroying a thread after you’re done with it would be a waste.
As with any expensive resource, recycling is a better solution; instead of creating a new thread every time you need to launch a new concurrent process, you can use a pool of precreated threads, which can be borrowed as necessary and returned upon completion.
The number of concurrent tasks executing at once can be only as large as the number of threads in the pool.
One impulse could be to improve performance by increasing the number of threads in the pool.
Setting the size of the pool too low can force concurrent activities to execute sequentially, but setting it too high can make the application reserve a large amount of resources that it doesn’t even use.
Threads in the pool consume resources such as memory even when they’re not running, and this may affect other processes executing on the same machine.
Often, the solution is to allow the number of threads in the pool to fluctuate between a lower and an upper bound.
This elasticity allows for the concurrency to increase when the system is under heavy load and to decrease when things return to normal.
Managed environments don’t allow the direct creation of new threads.
Managed environments, such as Java EE containers, are even more restrictive than other runtimes; application developers are explicitly forbidden to create new threads in the system.
In such cases, the system exposes an API that application developers can use to launch concurrent activities.
The preceding concerns are mainly addressed by the Spring task executor abstraction.
Whenever a component needs to execute a task asynchronously (a task in this case being to send a message or to poll a message source), it submits it to the task executor, which allocates a thread for its execution.
Its most important characteristic is that the thread allocation mechanism is completely isolated from the application itself.
The most common implementation of the task executor implementation is backed by a pool of threads, and the executor’s job is to allocate a free thread or keep track of the tasks that must be executed and to launch them when a thread becomes available.
Adding such a component to your application is easy and is supported by a Spring XML namespace.
In its simplest form, it can be defined as follows:
The first example of using a task executor in a concurrent scenario is polling.
Poller tasks are launched automatically by the framework and execute concurrently, using a scheduling mechanism configured transparently by the framework.
A polling operation includes reading a message from a message source or queue channel and sending it to the adapted channel or message handler for further processing, a process which may include traversing other channels and endpoints as long as those handoffs are synchronous.
This means that a polling operation can take a long time to execute, keeping the internal scheduler thread.
Depending on how many polling operations need to be executed at the same time, the internal scheduler may run out of threads, and other polling operations may have to wait.
This configuration option applies to pollers configured inside endpoints or channel adapters as well:
With this setup, the internal scheduler thread is kept busy for only a short time, since it immediately delegates the actual execution of the polling operation to the task executor.
This approach doesn’t necessarily increase the degree of concurrency in the application, but it introduces a more sensible way of managing threads for the existing concurrent tasks.
In your application, you can use any of the executor implementations, of which the most common is the one based on the Java 5.0 concurrency API.
It’s intended to be used by default and is supported by a Spring namespace.
You previously saw a definition for it, and here’s how you can also adjust the pool size:
A more sophisticated configuration can involve setting up a task executor with a variable pool size and a limit of, for example, 100 waiting tasks:
A more detailed description of the API as well as other task executor implementations can be found in the next section, which takes a look under the hood.
In the previous sections, we provided a higher-level view of how to configure the task executors and schedulers that back up polling and concurrent task execution in Spring Integration to adjust performance and resource usage and to ensure that the application meets its timing requirements.
For a typical user, this perspective is enough, because the namespace support and abstraction implementations for task execution and scheduling cover the typical use cases.
This section focuses on the needs of more specialized cases that aren’t covered by the defaults.
It introduces you to the Spring API for task scheduling and execution and shows you how to provide your own implementations that customize thread allocation or timed event triggering.
As you saw earlier, the Spring Framework provides its own abstraction for executing concurrent activities:
Such environments include WebSphere and WebLogic, and, in general, any Java EE application server discourages the creation of such threads.
In such cases, the application server exposes its own API in the form of a Java Naming and Directory Interface (JNDI)-bound WorkManager.
Although not configurable directly by the application developer, its purpose is similar to that of a Java 5.0 Executor: an application can execute a new concurrent task through it.
Sping’s abstraction has various implementations, so depending on the environment, you can choose between one that leverages Java 5.0, an adapter around the CommonJ WorkManager, or you can even implement this interface on your own, if needed.
From a strictly logical point of view, this and the namespace-defined version shown earlier produce similar results: a TaskExecutor that can be used to execute concurrent activities.
Their behavior is different, though, reflecting the fact that you’re using different implementations.
The TaskExecutor allows the launch of concurrent activities, and, generally speaking, the assumption is that it launches each activity as soon as possible (as soon as a thread is available to execute it)
Also, it’s assumed that the Runnable encapsulates a one-off activity that executes and finishes within a reasonable time frame (so that the thread on which it executes is freed up and another Runnable can be accepted for execution)
But an application may need to schedule periodic activities as well as tasks that execute at specific moments in time, and the TaskExecutor isn’t sufficient for that.
For covering those cases, the Spring Framework provides a richer abstraction, the TaskScheduler.
We talked about task executors and how to delegate the execution of an operation to them.
But to understand the behavior of polling consumers, we need to look at a different mechanism: task scheduling.
The work of a task executor is relatively simple: once a task (Runnable) is received, it tries to acquire the first available thread for it and, as soon as a thread becomes available, it executes it.
But as soon as possible isn’t sufficient when tasks need to be executed at precise moments.
As you see, the methods provided by the TaskScheduler allow you run a task at a specific moment or periodically (at either a fixed rate or a fixed delay)
The role of the ScheduledFuture is to give the invoker a handle for inquiring about the scheduled task, including whether it’s done, if it’s cancelled, or how much delay remains.
You can even use that handle to attempt to cancel its execution.
You can also implement your own schedule by using the Trigger interface, shown next, along with the associated TriggerContext that allows an implementer to calculate the next scheduled time based on the previous history:
A custom implementation of the Trigger interface is useful when the out-of-thebox configuration options (fixed rate, fixed delay, and cron expression) don’t cover the needs of your use case—a rare occurrence, but a possible one.
None of the out-of-the-box triggers can provide this functionality (although two cron-based pollers can), so you need a custom trigger, as in the following example:
This custom trigger can be defined as a bean and then referenced by a poller as follows:
As with the TaskExecutor, the Spring Framework provides special implementations of the TaskScheduler interface for environments in which the Java 5 abstractions aren’t applicable.
This wraps up the overview of the internal task execution and scheduling API.
Although not a common need in day-to-day usage, providing your own implementations of those abstractions may come in handy in special cases.
Also, now that you have a better understanding of how the abstractions work, you can make better decisions on what to use and when.
It’s time to wrap up the chapter, so let’s take a quick look at what we covered.
In this chapter, you learned about the dynamic configuration options of Spring Integration.
They’re complementary to the component definitions you learned about in previous chapters.
You didn’t learn about a new type of component but about how to configure the existing ones.
Pollers allow you to control the timing aspects of your application.
By adjusting the frequency rates, you can adjust their responsiveness to changes, such as data becoming available for processing in an external source or an internal message handoff that takes place over a message queue.
Polling strategy, though, encompasses not only the frequency of reading data but also other parameters such as the timeout and number of.
You also learned how to use inbound channel adapters to trigger operations that must take place at a given frequency or at specific times.
Applications must be able to do multiple things at once.
But the degree of concurrency in an application, a factor that plays an important role in determining its throughput, is controlled in two ways.
First, you must design your application to allow for parallelizing a large number of its processing operations by breaking singlethreaded flows that span multiple processing endpoints into smaller sequences using asynchronous handoffs to allow their execution on multiple threads.
Second, you need to allocate the resources (threads) so that these operations can run in parallel.
Both aspects can be addressed by using the configuration options of the Spring Integration framework.
There’s a trade-off too, because important information, such as transactional context, is lost in the process of handing off messages asynchronously.
Now that we’ve wrapped up the handling of timeliness and concurrency, we can move ahead and look at another type of enterprise application integration: massive dataset processing and batch operations.
Akin to living fossils, they’re a glimpse into the history of computing.
At the beginning of the computing era, collecting data and feeding it to programs that ran on mainframes was the only efficient option.
Access to the actual system, let alone the idea of a user interface, was restricted to a limited number of operators.
Technology has evolved since then, and progress in user interface and communications technology has led to their gradual replacement with more interactive solutions.
But, resorting again to an analogy with evolutionary biology, the secret of the survival of batch applications is the specific set of traits that ensured that they remain a superior solution in specific use cases.
By the end of this chapter, you’ll be familiar with the most significant features of batch applications, recognizing the main differences between them and other paradigms, such as online transaction processing.
In the context of this book, batch applications are important for their role in enterprise integration.
Because this book focuses on Java enterprise software development, we highly recommend one of Spring Integration’s siblings, Spring Batch, as an implementation solution (and if you take a strong interest in batch job implementation, we also recommend you read Spring Batch in Action to learn about it in detail)
In this chapter, you get a quick glimpse of how to use it for implementing a typical batch job example.
Apart from that, you’ll find no sibling rivalry here; the two frameworks complement each other well to produce sophisticated, event-driven, and highly scalable applications, and learning about the various integration opportunities will conclude our foray into batch application development.
Let’s begin by looking at what makes batch jobs so special.
Before introducing you to Spring Batch, we provide some background on the typical features of batch applications.
This section is a must-read if batch applications are new or unfamiliar to you, and it’s a recommended read for everyone else.
It focuses on the main concepts and concerns that drive the design and implementation of batch jobs in general, laying the foundation for the next section’s insight into the specifics of Spring Batch.
A quick definition of batch processing would describe it as the execution of a set of programs (called jobs) with little to no manual intervention.
Their working model is that the input data is collected in advance and provided up front in a machinereadable format, such as files or database entries, and the output is, in counterpoint, machine consumable as well.
As a result, these applications aren’t user-driven, and the set of operations that can be performed manually is limited to starting a job and introducing its input parameters (such as the location of the input file or other values relevant to a particular execution of the program) as well as checking the results and perhaps restarting failed jobs if necessary.
Currently, this application model faces strong competition from online transaction processing, which was made possible by advances in user interface technology and communications infrastructure.
To understand the differences between the two approaches, as shown in figure 16.1, consider a payment processing application.
The online transaction processing approach would take the payments as soon as they’re submitted, contact the buyer’s and seller’s financial institutions, execute the debit and credit operations, and return a confirmation immediately.
The batch processing approach bundles the payments and processes them later.
We stress the importance of bundling as opposed to simple deferral.
What’s characteristic for batch applications is that multiple requests (potentially of an order of magnitude of millions of items) are provided as input at once.
In some use cases, such as airline reservations, using a batch processing approach makes little sense.
You could collect the travel information and process requests overnight, returning an answer with the available options on the next day.
It’s an absurd example, but it serves to explain why highly interactive and online transaction processing systems gained so much traction.
In such situations, users want to receive answers immediately and be able to act on them: what if there’s limited seating on the flight of their choice? But there are specific cases when processing items in a batch is necessary or at least preferable.
For one thing, some business processes are based on the concept of gathering data during the day and processing them overnight.
For example, processing credit card transactions in batches as opposed to in real time is preferred because of the increased security (when the list of transactions is sent once a day, it’s easier to monitor and authorize than if interactions take place randomly, and this approach also simplifies logistics)
In other cases, batch processing is used for importing data outside the normal usage hours of the application.
Consider an online store that needs to update its products catalog every month; the store might need to be offline while the catalog is updated.
Likewise, processing during off-peak times may be preferred for performance reasons—for example, a large import that involves writing a massive amount of data to the database can seriously impair the performance of a running application that uses the same storage system.
Figure 16.1 Online versus batch processing: requests are processed individually or as a group respectively.
Introducing batch jobs because of the high resource consumption or frequent table locking that may need to take place during the import.
Extract, transform, and load (ETL) processes (see figure 16.2) are frequently used in enterprise integration for transferring data between applications.
This is a fairly generic description of a batch job, and at a first glance, it looks simple—a lot of batch jobs are implemented as scripts or as simple stored procedures that are executed periodically.
A closer look reveals that they have a complex set of requirements, which in certain cases require a significantly more sophisticated approach.
Let’s look at an example to guide us through the intricacies of a batch job implementation.
At the end of each day, the day’s transactions are submitted in a file and, overnight, the accounts in the system must be updated.
The transactions are transmitted as a text file, each line containing the payer and payee accounts and the amount that’s transferred between them, as well as the transfer date, as in the following listing.
This application must record the payments and update the payer and payee accounts accordingly.
The general outline of the batch job is shown in figure 16.3
Each row of the input file represents a payment, and this application must parse this data and convert it into data structures.
Because of the data volume, it’s impractical to assume that the whole file will be loaded in memory and parsed from there.
The transListing 16.1 An input file for a batch job; each row represents an individual item.
Figure 16.3 A simple batch job definition: the transactions are loaded from the input file B, parsed C, processed D, and written to the database E.
Figure 16.2 A typical ETL job: data is extracted from an external source, transformed, and loaded into the target.
Instead of doing that, the application must process the input data gradually, row by row—in other words, it must be capable of streaming the data.
Streaming may need to be performed for a wide variety of input sources ranging from input files, for which the application must implement its own strategy of partitioning the content into records, to database access, where the records are already well defined, but the application may still need to maintain an open cursor so it can deal with a large amount of input data (consider a SELECT statement returning millions of data records)
Gradually reading and processing input data implies gradually writing output data as well.
More often than not, the goal of processing is to transform each input item into one or more output items (the alternative being a simple aggregate calculation)
The simplest approach, which isn’t necessarily the right one, would be to process each item in its own transaction.
But that may be inefficient because every transaction start and subsequent commit introduces overhead.
The impact of the overhead depends on how many transactions need to execute within a given time period, which is why it can be neglected in an online system, where items are processed as soon as they arrive, but the impact may be severe when a huge number of items are submitted for processing as a batch.
The solution in this case is to span a transaction over a chunk of multiple items.
The size of a chunk varies from case to case; large chunks will buffer the items in memory and drain resources, at the same time risking the loss of data if an error occurs, whereas extremely small chunks limit the benefit of this strategy.
If the application fails while processing an item, it has a few options regarding what to do next: it can ignore the current item and pass to the next; it can retry processing the item; or it can abort the process altogether.
A robust implementation should provide an automatic mechanism that can decide, based on the nature of the error, whether the application should retry processing the item (if the error is the result of a temporary condition), skip it (if the error is nonrecoverable but losing this piece of data is noncritical), or completely abort the execution.
When a batch process stops because of an irrecoverable error, it usually needs to be restarted after the errors are fixed.
But usually this doesn’t mean that everything has to be started again from the beginning.
Restarting the process after half of the data has been imported can mean, in the best case, that the items previously imported will be overwritten, and in the worst case, that they’ll be imported twice.
Ideally, the state of the batch process must be persistent, and the application must provide the necessary infrastructure for recovering after a crash by resuming from where it left off.
As you can see, even if the business logic isn’t too complicated, implementing a batch application can become a complex task because of the additional requirements for streaming, chunking, and recoverable execution.
As is always the case with enterprise middleware, most of the effort may be spent on the infrastructure instead of the business logic.
Fortunately, as sophisticated as they are, the fact that almost all batch applications share a common set of requirements presents a good opportunity to provide a generic solution: a framework.
Spring Batch is a batch application development framework built on top of Spring.
As a batch-oriented framework, it provides the generic constructs and infrastructure that implement the main components of a batch job, allowing you to fill in the details with your own implementation.
Being built on top of Spring, it can take advantage of the container’s features, such as dependency injection, handling of cross-cutting concerns, and the extended set of modules that simplify messaging and database access, including Object-Relational Mapping (ORM) integration.
Fundamentally, Spring Batch covers the implementation of a batch process by providing the following essential capabilities:
A batch job skeleton and a toolkit of components for building the definition of a batch process as a collection of Spring beans.
We illustrate how Spring Batch provides these facilities through a simple batch process.
To complete a reservation, our travel agency must receive the payment for it.
The transactions are carried out through an external payment system.
Overnight, the application must be updated with the transactions that took place during the day.
The transactions are transmitted as a text file, each line containing the payer and payee accounts and the amount that’s transferred between them.
The application must record the payments and update the payer and payee accounts accordingly.
The batch job definition for this example is shown in the following listing.
The example uses the dedicated Spring Batch namespace as a domain-specific language to define batch jobs.
More sophisticated applications can have multiple steps, but this example has just one: importing the batch file.
The most common way to implement a step is delegating to a tasklet, which defines the activity that must be performed, leaving the proper step implementation to deal with the boilerplate aspects of the execution (maintaining state, sending events, and so on)
The work performed in a tasklet can be a simple service call or a complex operation.
The most typically used tasklet implementation is the chunk-based tasklet (see figure 16.4), which processes a stream of items and writes back the results in bulk.
The streaming and chunking aspects of batch processing can be easily identified: the.
Figure 16.4 Chunk-oriented step: items are read individually by the ItemReader, optionally processed by the ItemProcessor, then written in bulk by the ItemWriter.
The framework allows for constructing batch jobs that consist of multiple steps, so, for example, the job may need to execute some additional tasks after importing the data, and a multistep job is the right way to do so.
Also, steps may specify various levels of fault tolerance, allowing the process to retry or skip items for certain types of exceptions rather than stop a job if an error occurs.
In practice, you don’t have to implement all of it.
Spring Batch provides its own toolkit of predefined components, which includes support for reading a file and transforming its content into a sequence of individual items.
The location of the file can be passed as a job property (more about it in the next section) so that you can execute this job for different physical files.
Your only responsibility is to provide a strategy for persisting the read data in the form of an ItemWriter implementation (PaymentWriter in this sample)
As you can see, the batch job definition can be created as Spring beans, using dependency injection for customizing various aspects such as the tasks that must be performed during the job or the read and write strategies.
In most cases, you can use one of the out-of-the box reader implementations and customize it for your application’s needs so that you can avoid the critical yet tedious tasks of dealing with file line reading, database cursors, and so on.
The only things you need to provide are what’s specific to your application: a definition of your business entities, a strategy for converting the basic records read from the file to these entities, and the functionality.
Listing 16.3 A FieldSetMapper converts raw input into a domain object.
Next you need to set up the infrastructure so that you can execute the job.
What you’ve created so far is the definition of a batch job.
To put it to work, you must start it using the job execution API.
To get access to that, you must define a distinct type of bean in your context definition: a batch job launcher, shown in the following listing.
With this definition in place, you can start a new batch job as in the following example (noting that the Job instance is the bean defined in the previous section):
The role of the JobLauncher is to create a new batch job instance according to the Job definition.
The JobParameters passed as arguments while launching the job fulfill a dual role: as their name indicates, you can use them to pass particular properties that are specific to this job execution instance (such as the name of the file that needs to be processed), and you can also use them as unique identifiers of the Job execution.
There can be no two JobExecutions for the same Job and JobParameters set of values.
This is an important point to take into account when executing batch jobs automatically: when launching a new job instance, the list of parameters must contain at least one unique attribute; otherwise, the launcher will complain that the job has already completed.
The returned JobExecution is a handle that allows the invoker to access the state of the executing Job.
As such, an application may track the progress of a batch job by repeatedly inquiring on the JobExecution instance about the current state.
This leads us to the third and final component of Spring Batch: the job execution state management infrastructure.
You got a hint in the previous section about what we discuss in the next section.
The JobLauncher bean we configured in listing 16.4 is injected with a jobRepository.
As we explained earlier, the state of the batch jobs is persistent so that jobs can be resumed after a failure or a shutdown.
Listing 16.4 A JobLauncher bean is used for starting job executions.
Integrating Spring Batch and Spring Integration processing a data stream requires avoiding the potential duplication of data that may occur as a result of processing items twice.
The job repository records the ongoing progress of the running batch jobs, including information such as the current step and how far the application got while processing the data stream that corresponds to a particular step.
Unless you have special needs, all you need to do to create a job repository is include a bean definition in your context.
The repository uses a set of database tables, which can be created automatically when the repository is instantiated for the first time:
This concludes our quick overview of Spring Batch and its capabilities.
As you can see, it allows you to create pretty complex batch jobs, launch them, and manage and interrogate their state, all while writing only a limited amount of code.
Let’s see what opportunities are available for using all these capabilities in enterprise integration scenarios and, mainly, how Spring Batch can complement Spring Integration.
Using Spring Batch greatly simplifies the process of implementing a batch job.
Apart from the job implementation, a complete solution must be able to schedule and launch jobs automatically, monitor them, and interact with the environment and other applications.
For achieving these goals, implementors must look beyond the framework.
Fortunately, this goal can be achieved with minimal effort by pairing Spring Batch with Spring Integration.
The two frameworks complement each other, and together they can provide a complete solution for creating enterprise integration applications that use batch processes for handling large quantities of data.
Spring Batch Admin is an open source project that provides a web-based administration console for Spring Batch applications.
It has access to a set of job definitions and a job repository, and it provides support for.
Figure 16.5 The relationship between Spring Batch entities: Job and Step define the logic of a job, JobExecution and StepExecution are used to save the state, and JobParameters customizes a specific job instance.
Of interest to our topic is that most of the interaction between the web application and the batch jobs is done via Spring Integration.
Spring Batch Admin provides a set of utility classes that implement most of the collaboration patterns discussed in this section and can be used out of the box in your applications.
Let’s assume that you finished implementing the batch job, as described in the previous section.
But how does this method get called in an application? You can write a web application that invokes the method from a controller, or you can invoke it from the main() method of a launcher class for command-line operation.
In the latter case, you can create a shell script or schedule the batch job using a scheduler such as cron.
But you can create even more powerful scenarios, such as running batch jobs in an event-driven fashion, for example, whenever a file is dropped in a target directory.
A file channel adapter will monitor the directory and send out a message whenever a new file is detected, as shown in figure 16.6
The configuration for such a message flow is displayed in the following listing, where the launching message is triggered by dropping a file in a directory monitored by a file channel adapter.
Figure 16.6 Event-driven batch job launch: the file channel adapter monitors a directory B, a file message is converted to a JobLaunchRequest C, which triggers the batch job launch (DE)
Other components may trigger a launch by sending a File payload F.
Listing 16.5 Triggering jobs with messages sent by a file channel adapter.
The event-driven approach for launching jobs is extremely flexible, allowing for a large number of variations in implementation.
For example, any kind of channel adapter output can be transformed into a JobLaunchRequest, so you can trigger the batch jobs for inbound emails, Java Message Service (JMS) messages, or File Transfer Protocol (FTP)-accessible files.
As a matter of fact, the trigger doesn’t even have to be a message source; for example, a generic scheduled channel adapter, as shown in chapter 15, may be used for launching batch jobs periodically.
Besides the sheer variety of sources, another benefit of integrating the batch job launch into a pipes-and-filters architecture is the ability to take on multiple input formats simultaneously.
The batch job may require a canonical format, but different sources of data may use different formats.
Spring Integration transformers could be added between the source adapters and the channel expecting the canonical format.
But message-based integration can work either way: not only can batch processes be launched using messages, but they can also send notifications while they change state.
For operators, it’s critical to get up-to-date information about its progress, and most important, whether it completed successfully or had to stop because of a nonrecoverable failure.
As many similar situations, this information can be gathered through active polling or in an event-driven manner.
Spring Batch provides a JobExecution class that reflects the current status of the eponymous job execution, so the application that launched the job can use it as a handle for inquiring whether the job is complete or still running, what its current status is, if it stopped because of an error, and so on; and in the case of running processes, it can be used to stop them.
Figure 16.7 shows an example of using listeners for message-based integration.
A router decides, based on the execution status, whether it’s.
Integrating Spring Batch and Spring Integration necessary to take any further steps (such as relaunching the job if it failed and the failures are recoverable, perhaps with a delay), in which case the message is sent to an appropriate endpoint to be processed, or it’s a matter of sending a notification message (completion was successful or the failures are nonrecoverable), in which case the message is sent to an outbound email channel adapter.
For this, you need to add the listener to the job definition and add the rest of the bus configuration, as shown in the following listing.
Message-driven job launching and message-driven notifications are two examples of Spring Batch/Spring Integration collaboration, which expands the feature set of the application and builds upon complementary features of the two frameworks to provide a scalable execution infrastructure.
The way we’ve described it so far, it would seem that the natural way to combine the two frameworks is by wrapping Spring Batch jobs in a Spring Integration outer shell that can take care of the interactions with the external world.
Spring Batch can use Spring Integration internally, too, for delegating the processing of an item or even a chunk outside the process.
Listing 16.7 Registering a job listener and handling batch notifications.
In a simple case, the ItemProcessor can be a messaging gateway, as shown in figure 16.8, thus deferring the processing of items to the message bus.
This is useful when item processing logic is fairly complex and involves invoking multiple transformations or service invocations, either local or remote (through the use of channel adapters and gateways)
When item processing on the bus takes a relatively long time, such as when the application performs a remote invocation, you can increase performance by introducing asynchronous item processing.
You use two wrapper components provided by Spring Batch Integration for your item processor and item writer: AsyncItemProcessor and the corresponding AsyncItemWriter.
In this case, the batch job only reads items and groups them, and once a chunk is sent out, it continues to read items and assemble chunks without waiting for a result.
By introducing asynchronous processing (for example, by using a queue channel instead of a direct channel), you can increase the concurrency of the system.
By using channel adapters, you can completely externalize the processing of a.
For example, you can use a JMS channel adapter to send chunks on a message queue, letting external components read the chunks, handle them individually, and return information about their successful completion.
The latter part is necessary because the batch process must update the JobExecution state continuously to acknowledge whether the step execution succeeded.
Because of that, the request and response messages contain metadata that helps the batch job executor carry out this process (Did we get responses for all chunks? Were there any errors?)
By looking at Spring Integration as underlying support for boosting the concurrency and distribution capabilities of Spring Batch, we’ve concluded our overview of the collaboration between the two frameworks.
It’s time to move on to the next chapter, but first, let’s quickly review the takeaways of this chapter.
This chapter provided an overview of batch processes and their typical features, and you gained a general understanding of how they fit in the enterprise integration landscape.
In a Java environment, you can create powerful batch processing applications by using Spring Batch, which, in a typical Inversion of Control fashion, provides the infrastructure for defining and executing jobs, including facilities for managing state.
The main strength of Spring Batch is in dealing with the complex details of defining and running batch jobs, such as chunking and transaction management as well as state management.
Spring Batch doesn’t include facilities for scheduling batch jobs or executing them in an event-driven fashion, but it finds an ideal companion in Spring Integration, whose strength is in dealing with the challenges of interacting with external systems and creating an event-driven environment that can be used for launching and managing batch jobs.
The two components of the Spring portfolio are complementary frameworks that can and should be used together for implementing a complete batch job–based enterprise integration solution.
The integration between the two goes from simple, eventdriven launching of batch jobs from Spring Integration to more sophisticated notification and automated feedback mechanisms using events published by Spring Batch and, even beyond that, to creating parallel and distributed batch jobs using Spring Integration as the underlying infrastructure.
This chapter focuses on scalability at runtime and at development time.
First we introduce the OSGi (Open Services Gateway initiative) module system and then we link it to messaging to show the complementary nature of the two.
Since 1998, OSGi has been primarily driven as a needed extension of Javaneeded because Java’s module system is simplistic.
Java loads all classes in all JARs it sees on the classpath linearly, and thereby exposes all known types on the classpath to all other known types.
In large applications, this can be catastrophic, so OSGi was introduced to resolve this problem.
OSGi didn’t start out primarily as a modularity fix for the Java classloader but as an extension that would help to run dynamic applications that wouldn’t have all their services active at all times during their lifecycle.
An important positive side effect that was recognized early was that it would also help structure large applications and address the simplistic Java classloader mechanism.
Now both modularity and dynamic type loading are essential features of OSGi.
As systems grow larger, not only modularity but also response times and deadlock prevention are concerns.
The biggest danger to these two technical requirements are synchronous invocations over the network, for which OSGi by itself doesn’t offer a solution.
At this point in the book, it should be obvious that OSGi and messaging are complementary.
Eclipse Gemini Blueprint (http://www.eclipse.org/gemini/blueprint/; the successor to Spring Dynamic Modules) starts to bring them together by providing a foundation where Spring concepts, such as Inversion of Control, meet OSGi, and from a functional point of view, Spring Integration completes the circle.
Spring Integration shouldn’t be credited too much in this because, as you’ll see in the coming sections, it just works because of Gemini Blueprint.
As a developer, you can benefit greatly by using them together.
As you can see in figure 17.1, Spring Integration can run in any OSGi container using Gemini Blueprint.
Before we show off the combination of messaging and OSGi, we must lay a foundation.
We briefly explore OSGi’s module system and then focus on the Service Registry.
Only then are we ready to combine messaging (Spring Integration) and dynamic modules (Gemini Blueprint plus OSGi)
Finally, we look in detail at how to replace service implementations in a running system.
Figure 17.1 On an OSGi platform (bottom), you can run bundles that make use of Gemini Blueprint (middle)
Spring Integration (top) can be used in combination with Gemini Blueprint to combine messaging with dynamic loading characteristics.
In this section, we describe the main functionality enabled by OSGi: its module system.
We cover the reasons behind the module system’s design, and we show you how to dynamically (re-)load bundles at runtime.
The application grows large, and you need to modularize it.
As the requirements keep coming in and the application becomes more successful, you need different teams to work on parts of the application.
After a while, you find that the teams have different release cycles and depend on each other’s work.
Modularity is becoming a hairy problem; different teams should be able to deliver on their own schedule without causing downtime.
You need a rocksolid solution—one similar to that shown in figure 17.2
Apart from a technical solution, to modularize your workflow as in the figure may require significant changes in your development and delivery methodology.
Most of the effort involves defining and changing processes and architecture.
OSGi is an option that can work to your advantage if you know how it works and what functionality it offers.
In this section, we review the features that are relevant to the application of OSGi and messaging in enterprise architecture.
But first we look a bit deeper into the Java type system.
In a normal Java runtime, your code can be modularized into different packages inside individual JARs on the classpath.
Classes can hide their internals from other classes by using modifiers, which gives them encapsulation.
Once a classloader loads a class, it can never unload it.
The only way to work around this restriction is to use a dedicated classloader for the set of classes you want to reload.
Figure 17.2 B The integration team sets up the server and deploys some basic bundles to it.
In parallel, the integration team replaces the basic notifier with a bus so they can connect multiple sources of notifications.
The obsolete basic bundles are uninstalled as their replacements kick in.
Figure 17.3 shows the internal representation of types in a traditional classloader.
Not only is the runtime representation of types too simplistic, it’s also too rigid, as explained in the next paragraph.
The second problem you run into when you want to dynamically modify your running system is changing the classpath at runtime.
If a new module must be deployed, you might not want to restart all the other modules.
The combination of these two problems becomes prohibitive for a single team with a production deadline.
In the late 1990s, many companies faced these problems, and most of them found their way out of them by using Remote Method Invocation (RMI) and web services.
The downside of this solution is that going over the network is expensive (in terms of performance and/or hardware)
It abstracts away the complex but not impossible solutions to type reloading that are provided in standard Java and introduces a powerful vocabulary that makes sense of modularity.
In OSGi, a JAR is turned into a bundle, which is a JAR with a special manifest.
In the manifest, you can tell the OSGi container what the bundle is, how to start and stop it, what it depends on, and what it has to offer.
When you use OSGi, lifecycle modules are started in the right order and services are invoked only if they can do their job.
A typical happy scenario for a bundle lifecycle starts with the installation of a bundle.
Once the bundle is installed and the OSGi framework is aware of it, the framework can start looking for the bundle’s dependencies.
The dependencies of the bundle consist of types and services.
Before a bundle can start (while it’s still in the INSTALLED state), other bundles must expose all the types that bundle needs.
If this is successful, the bundle enters the RESOLVED state.
After all the code dependencies are resolved, a bundle may be started, which takes it through the STARTING state to ACTIVE.
A diagram of these states is shown in figure 17.4
You need to learn more about bundle behavior to use it effectively.
For example, a bundle might not move to the RESOLVED state because some of its required dependencies are unavailable, and inside the ACTIVE state are substates that may interest you.
Because this book is not about OSGi, we don’t cover all the intricate details in depth.
We cover just enough to establish the relationship between OSGi and Spring Integration.
If you want to design a solution around OSGi and messaging, it’s a good idea to read OSGi in Action.
Now that you know a few basics about OSGi, you’re ready for the next section, where we look into the Service Registry and what Gemini Blueprint brings in terms of integrating OSGi with Spring.
You can use these services from Spring Integration, but you need a way to access services in other bundles, which is addressed by the OSGi Service Registry.
The OSGi Service Registry can be used to register plain old Java objects (POJOs) as OSGi services so that an object from one bundle can invoke a method on an object in another bundle.
That solves one problem, but another challenge is that this registry is a simple, low-level API that requires you to write some unwieldy code.
The following excerpt demonstrates a service lookup using the low-level API:
In contrast to plain Java, where class loading is out of the developer’s control, bundles can be installed, uninstalled, started, and stopped as needed.
The transitions between states, as depicted by the arrows, are clearly defined and restricted by the standard.
As you already know, Spring is good at decoupling dependencies through Inversion of Control (IoC)
In a normal Spring application, this means services are instantiated by the framework and no longer by their clients directly.
Gemini Blueprint extends this paradigm by registering and referencing services in the OSGi Service Registry.
This means the main benefits of Spring stay the same; the only difference is that the instance of the dependency is taken from a different place.
Because the whole point of IoC is to make the client agnostic about the origin of its dependency, you typically don’t need to change any Java code to migrate a Spring application to OSGi.
The best way to understand how Gemini Blueprint and OSGi work together is to look at the typical layout of a bundle using Gemini Blueprint (see figure 17.5)
When a bundle is loaded with Gemini Blueprint, some special steps are taken during startup to instantiate the ApplicationContext specific to this bundle.
This BundleContext is like a normal Spring application context but with some added features specific to OSGi.
The service implements an interface exposed as a type by another bundle.
Now in the SMS bundle, you can expose the service like this:
Figure 17.5 Inside the META-INF directory, we see the MANIFEST.MF file which contains the OSGi standard dependency directives.
Gemini Blueprint reads the .xml files in this folder and starts a special application context (BundleContext) containing the beans described in these files.
Subsequently, you can reference the service from the notifier bundle like this:
This creates a proxy for the object in the Service Registry that implements the given interface and can be injected into another bean in the notifier context without changing a line of code in the client.
To make this look good, we used the osgi namespace here.
This will work only if you add it to the header of your Spring .xml files, as you’ve seen earlier:1
Now you have two bundles, each with its own Spring context, but you created a bridge between them using the OSGi Service Registry.
Because Gemini Blueprint has placed a proxy in the middle that can deal with the details of OSGi, it can suspend method calls while the target service is being replaced.
You can even start one version of the SMS bundle, run the application on it sending notifications, and then replace the service in that running system without any outage!
You might have noticed that in our focus on OSGi, we haven’t put Spring Integration’s advantages to use at all.
Instead we created a reference between bundles based on the Notifiable interface, which is fine, but in that process, we also introduced a dependency from the notifier bundle on the bean name of the smsNotifier, which means generic notifications are now coupled to the SMS implementation.
OSGi offers a couple of tricks to compensate for this, but a channel seems like the perfect shared concept to bridge the gap here.
If we were to share a channel between bundles, neither sender nor receiver would have to know about anything other than the payload.
This is exactly what we explore in the next section.
It allows two services to collaborate without having any dependency on each other apart from a shared comprehension of the payload that’s passed between them.
The spring-osgi schema works as of this writing, but it has been deprecated.
Messaging between bundles chapters, but without a fix for Java’s flat classpath, the services will still be visible to each other within the same Java Virtual Machine (JVM) unless you take on the extra complexity of adding a transport layer in between virtual machines.
Let’s say you want to get cash from your account.
You could walk into a bank and wait in line, but that exposes you to all kinds of services that you aren’t interested in.
You’ll have to avoid tripping over children, you’ll have to be social to the teller, and worse, you’ll have to interact with other customers who might want to chitchat about their day or ask you directions.
If you interact with the automatic teller machine instead, your cash withdrawal would be much simpler.
The service will get only the information you want to expose.
There is no need to talk about the weather and distracting interactions.
OSGi is like that: it avoids unwanted coupling between the service and its clients.
In figure 17.6 you can see the conceptual differences between classic interactions and using OSGi in a bank analogy.
In the bank example, you might argue for the social value of talking to other human beings, but inside a JVM, such interaction is typically not an advantage.
OSGi provides clean modules that hide their internals from each other.
With OSGi and Spring Integration, two services can collaborate without any visibility between them other than their contract.
This type of decoupling becomes more valuable when a system grows larger than what a single team can manage on a single release cycle.
In a system that’s large but stays within the boundaries of one corporate entity, there’s no gain in separating modules with a transport layer, security boundaries, and the associated performance drag.
Because of this, OSGi combined with messaging is particularly useful in large enterprise architectures.
A system modularized with OSGi and decoupled with messaging can evolve much more naturally as multiple teams work on it at their own pace.
The production system runs on as few nodes as possible to reduce the network overhead, and bundles are installed in different release schedules without ever bringing the system down.
Figure 17.6 On the left is the interaction with a bank, analogous to the classic Java approach involving a lot of unwanted potential interaction.
On the right is the improved interaction with an exposed automatic teller machine, analogous to a service exposed using OSGi.
This idea might sound less reliable than redeploying everything on a single node and bringing that node back up, but there’s no fundamental difference in terms of deployment architecture except that a bundle can be brought up and down much faster and a new version can be installed before the old version is taken down without having to commission more hardware.
Compared with a typical production setup, the equivalent OSGi-based solution leaves less room for error because of the strict lifecycle definition provided by the OSGi standard.
Debugging and testing an OSGi-based application requires a different skill set than debugging a classic Java program, so as with any technical solution, OSGi should be used only when the reduction of maintenance costs more than balances the development costs.
Maintenance costs are typically undervalued during development, so even the added initial complexity might be outstripped surprisingly quickly once a system goes into production.
In any case, to a Java developer, OSGi is just one more tool that belongs in the toolbox.
In the next few sections, we show some examples of how bundles can be wired together using Spring Integration and OSGi.
We discuss publish-subscribe communication, point-to-point messaging, and load balancing, and we look into exposing gateways as a way to avoid unneeded Spring Integration dependencies.
Publish-subscribe messaging is the easiest form of messaging to implement with OSGi.
In a publish-subscribe configuration, the publisher is mostly indifferent to whether someone listens.
When someone posts a tweet, they’re probably not concerned with whether others will read the tweet.
This is an interesting contrast with point-to-point messaging in which delivery of the message is a service offered to the sender.
To connect two bundles as a publisher and subscriber, you have the publisher expose a publish-subscribe channel as a service, and then reference it inside the subscriber bundle.
In the subscriber context an OSGi reference allows you to refer to the channel as you normally would:
That’s all there is to it for publish-subscribe messaging, but in many cases, you have more stringent requirements to the relation between sender and receiver.
One such situation is when a single message may only be processed by a single consumer.
This requirement is met by point-to-point messaging in the next section.
Point-to-point messaging is used when one endpoint specifically wants to tell another endpoint something.
Because you have one receiver and one sender, the expectations between the two are stronger.
Usually, the sender changes state and, in many cases, even expects the receiver to trigger another state change.
For example, when you order a taxi, you start expecting a taxi to come pick you up.
When we talk about examples of point-to-point messaging, it’s usually the sender using a service of the receiver.
This is the exact opposite of publish-subscribe messaging in which you can subscribe to the feed of published messages.
If two OSGi bundles are connected through point-to-point messaging, the startup is a bit tricky.
You can connect bundles in two ways, depending on your purpose.
Sharing a channel makes sense, but you still have to decide which bundle gets to own the shared channel.
This decision depends on how you want to design each bundle.
Either put the channel on the sending bundle or put it on the receiving bundle.
Putting the channel on the sending bundle allows loading multiple receivers at runtime to listen to the same sender.
Depending on the dispatcher of the channel, this serves as a lightweight load balancer in the application.
If the channel sits with the receiving end, you can let multiple senders sink their messages into the same bundle.
They have the additional benefit that you can scale out at runtime.
Just add a bundle that does its work on a different (new) node in your cloud, and you’re done.
In the next section, we look at the boundaries of the messaging infrastructure and integrate with bundles that don’t depend on Spring Integration at all.
When you’re composing a large application built by several teams, you’re unlikely to want all these decoupled bundles to depend on a messaging framework, even one as awesome as Spring Integration.
The whole point of modularizing an application is to reduce the complexity, so having the same dependency from all these modules contradicts this goal.
A large application using Spring Integration typically has one or more modules using Spring Integration to wire the rest of the modules together.
This raises the question, how are these integrated in an OSGi architecture?
If a message needs to result in a service activation in a non–Spring Integration enabled bundle, you can.
In the opposite direction, you can use a similarly elegant trick.
Let’s say the Twitter support bundle receives a direct message on Twitter from a user and the resulting notification object needs to be sent on a channel in the notification bus.
You want to keep the Twitter support bundle free of Spring Integration dependencies, so you can’t expose a channel as a service through the OSGi Service Registry.
Instead of a channel, you expose a gateway, implementing an interface from the Twitter support bundle.
This is complemented by a reference in the Twitter support bundle:
With this simple trick, you can use the twitterNotifier gateway in the notifier bus without any dependencies on classes in that bundle or on Spring Integration, for that matter.
This chapter introduced OSGi and showed you how it fits with a messaging application.
We described some architectural patterns made possible through the use of OSGi.
We also showed you some of the more intricate details of loading a Spring Integration application that’s spread over multiple bundles.
Figure 17.7  The shaded bundles here have a Spring Integration dependency, but the white ones do not.
With OSGi, we focused mainly on development time scalability, and we showed you that OSGi and Spring Integration give you a lean alternative to an ESB.
Setting up a runtime that would make you comfortable working with OSGi is a bit of a job.
You can’t expect OSGi to magically make complex things happen without giving you some interesting puzzles to solve along the way.
As with any technology, be conservative when deciding whether to use it, but if you have a use case on your hands that would benefit from OSGi, be brave: it’s worth it.
In essence, TDD makes the developer responsible for proving that the software works.
If you don’t have a clue how you’re going to test the application, you don’t have any business building it.
One of the oldest ways is to test it manually.
Manual testing is still valid and in wide use because of its simplicity, but experienced developers dread the tedious work of so-called monkey testing.
Even better, if you can isolate a part of the program in such a way that it doesn’t require every test fixture to simulate human interaction, writing tests becomes a simple development task with excellent return on investment.
SUnit, invented by some of the bright minds that also started the Agile Testing.
One of the great accomplishments of our industry over the last 20 years is test-driven development (TDD)
Where many methodologies have proven only to work in theory or have never proven their need, TDD has flourished.
If you don’t know JUnit yet, firmly pull the handbrake toward you and make sure you learn JUnit before you read any further.
After reading the rest of the book and being exposed to test code in the samples as well, you’ll find little new here in terms of what you can do.
The main thrust of this chapter is about why you should pick a certain option.
Because the topic of this chapter cuts across the other topics, this chapter is organized differently.
Like the other chapters, it uses code samples from the sample application, but it doesn’t focus on a particular use case.
It also doesn’t have a dedicated “Under the hood” section, first because the test framework code is simple enough to embed with its usage and also because the test code serves as an example of how to extend JUnit to deal with the messaging domain.
This chapter builds on top of JUnit tests from the sample to show you the intricate details of testing asynchronous and concurrent programs that were built using the pipes-and-filters architecture supported by Spring Integration.
As you might’ve experienced, testing these types of applications is more convoluted than testing classical applications.
When you’re using Spring Integration, you’ll find that it’s often necessary to write tests that assert things about the payload of a message that’s received from a channel or to write assertions about particular headers on such a message.
The boilerplate code normally needed to do this is in large part taken care of by Spring Integration’s own test framework.
This test framework builds on top of Hamcrest to offer custom matchers that can be used with assertThat.
It also has some convenience classes related to the asynchronous nature of certain Spring Integration components, such as QueueChannel.
In addition to Hamcrest matchers, Spring Integration tests can make use of Mockito extensions.
Mocking services out of tests relating to the message flow through the system becomes more important as a system becomes more complex or when service implementations are developed on a different schedule than the configuration that makes up the message bus.
This chapter discusses different use cases for the test module.
The authors strongly believe that tests shouldn’t hide complexity, so, as mentioned, we include no “Under the hood” section.
Instead, you’ll find all the details right with the usage examples.
This JAR is packaged separately from the main Spring Integration distribution, because we don’t want to force transitive dependencies on Mockito and Hamcrest on all Spring Integration users:
You’ve just added another Spring Integration JAR on your classpath—now what? Let’s look at what’s inside that JAR.
Spring Integration testing framework What goes in must come out.
When a message moves into the system, it must come out in some form or another, either through being consumed by an outbound channel adapter, as another message being sent on a subsequent channel, or as an ErrorMessage being sent to the errorChannel.
In a test fixture, you’re usually interested in the properties of the outgoing messages, but these properties might be hard to reach:
As you can see here, getting to the delay requires a cast and two method invocations.
In the next section, you’ll see how to factor the unwrapping logic out of your test cases.
Testing behavior with mocks Mocking is used to allow assertions on behavior instead of state.
Frameworks like EasyMock, JMock, and Mockito help create mocks that allow verification of behavior.
Certain advanced features are not supported in Mockito, but that makes it an excellent candidate to use for illustration.
If you’re unfamiliar with mocking, you’re encouraged to read “Mocks Aren’t Stubs” by Martin Fowler (available at http://mng.bz/mq95)
With Spring Integration’s test module, you can use the matchers that deal with unwrapping internally.
First we look at an example, then we look at the underlying details.
Starting with the previous example, you probably already noticed some pain points in the test code.
The is matcher isn’t particularly well suited to deal with messages.
Ideally, you’d have a matcher that can be used like this:
It’s no coincidence that with the PayloadMatcher you can do exactly this.
All you need to do is add the following import statement:
This gives you two methods related to payloads: hasPayload(T payload) and its overloaded cousin accepting Matcher<T>
This way, you can also use other variants of the theme:
This makes your life as a Spring Integration user a lot easier, and the code you need is almost trivial.
Let’s look at the code of the PayloadMatcher in the following listing.
As you can see, this listing extends TypeSafeMatcher and implements two factories for the matcher.
A few more features are bundled in Spring Integration’s test module.
For example, you might require matching on headers too, as you’ll see in the next section.
Matching messages is particularly useful if the framework is doing work that’s important for business concerns.
In many cases, the message payload is determined by business logic in Java code, and asserting things about the payload doesn’t make much sense in an integration test (because most of that would already be covered in a unit test around the service)
In some cases, though, such as when you use the Spring expression language, things change.
In the sample application, a user can fill out a form creating a new trip, and from that a CreateTripCommand is sent into the system wrapped in a message.
The message goes through a splitter that chops the command into subcommands for rental cars, hotel rooms, and flights.
In figure 18.1 you can see how we modified the fixture to allow you to control incoming and outgoing messages.
You can now make sure the expression is correct in a controlled test case.
The test case remains relatively simple as you can see in the following code:
Figure 18.1 The test sends a message B on the tripCommands channel and receives the subcommands that were sent by the splitter.
Now a test can verify that the splitter is configured correctly by asserting that the payload C of the messages matches the contents of the original TripCommand.
The trick here is to plug into the existing system without replacing logic that you want to test.
As you saw in the previous example, it’s simple and useful to write tests that make assertions on the payload of a message.
More often than not, though, the headers of messages play at least as big a role in the integration of the system.
In the next section, we go into the details of header matching.
In many cases, when you’re testing the integrated application, it’s more important to make assertions about the infrastructural effects on messages than on the business services’ effects on messages.
Typically, the effects of business services are already covered by unit tests, so you don’t need to cover all the corner cases in your integration test again.
But headers on messages are typically set by components that are decoupled from services and can only do their work in an integrated context.
For headers set in this manner, you need to test all the corner cases in an integration test.
From the UI, a command describing the desired booking is submitted.
This command is consumed by the booking service, which puts an event on the bus that signals the result of the booking (success or failure)
To guarantee idempotence, the service activator for the booking service is preceded by a header enricher that stores a reference to the original command in the headers and a filter that drops any message containing a command that has already been executed.
It’s followed by a service activator that keeps track of all the successfully executed commands, for example, in a table that’s also used by the filter.
This construction contains enough complexity and business value to make it the target of a test, but testing all these components in isolation doesn’t assert anything about what Spring Integration will do with the header values.
You need to make assertions about headers, which you could do manually:
But similar to matching payloads, matching headers manually causes smelly code.
Again, there are matching facilities in Spring Integration’s test module that can help you.
The implementation is similar to the PayloadMatcher, so you only need to look at the usage here:
As you can see, the header matching methods are very convenient and drastically reduce the amount of noise in test code.
The example above demonstrates several of the matching options: checking for the presence of a header key, verifying that a header value is not null, validating a header value’s type, and asserting that a header contains an expected instance.
Moreover, all of the predefined header keys can be matched via explicitly named methods as shown above with the hasCorrelationId method.
On the last line, you see that there’s even a method for testing that all keyvalue pairs in a given map are present as headers on the message against which you match.
That’s far more convenient than iterating through the map directly and testing each key and value against the message headers.
Not only is the test code more readable, but the error message produced by a failed assertion will provide much more detail than if you were testing individual values directly.
If we change the correlation ID in the test message, for example, the resulting test failure message would contain the following:
This section established a solid foundation in terms of matching messages based on payloads and headers.
Regarding the matchers, it isn’t trivial to deal with the fact that a message received from a channel doesn’t have the benefit of generics in many cases.
If you choose to implement your own matchers, you should expect to invest some of your time in fine-tuning parameterization.
Matching the message state is only part of the equation.
You should also verify that service activators, transformers, and channel adapters invoke services correctly.
When you’re using a mocking framework, things get more complicated because you must deal with the particulars of the mocking framework as well.
We outline the support for Mockito in the next section.
When your test subject has dependencies that aren’t relevant for your test, you can use mocks or stubs to factor their influence out of the test fixture.
A briefing on mocking is beyond the scope of this chapter, but we keep a strict definition of a mock as something that can be used to test behavior (and is usually created by a mocking framework), as opposed to a stub, which is typically used to test state and is created as an inner class in a test case.
In this chapter, we show only mocks using Mockito, which serves our need for concise and readable code samples.
Other mocking frameworks or stubs can be used in the same manner; the particulars of Mockito are irrelevant to the point being made.
Most unit tests require a test harness that simulates the external dependencies (for example, through mocking or stubbing)
But if configuration becomes a major part of the behavior of your application, as with Spring Integration, it becomes important to test the configuration itself.
This means that it becomes sensible to mock out business code and let a message flow through the system just to see if it’s handled correctly by the infrastructure.
This would concern routing, filtering, header enrichment, and interaction with other systems.
For example, you might want to pick up a file from a certain directory, set its name as a header value, then unzip the file and unmarshal it to domain objects.
Customized infrastructure is usually important to the business without being tightly related to a particular business use case.
For example, properly setting a header is essential for your system to perform its tasks as designed, but setting this header is only a small part of the story.
The particular header enricher has a place as a unit in the system, so it should have a designated unit test.
If you’re using SpEL, you have only XML configuration to test.
Let’s say you have a header enricher that sets the original command as a header so it can be used later in the chain when the payload is already referencing the response:
Even though the expression is trivially simple, this needs to be tested thoroughly.
For example, a change that postpones the unmarshalling to a BookingCommand could cause a regression where the originatingCommand header suddenly references a Document instead.
In figure 18.2 you can see how to generically set up a test that verifies the framework behavior.
The example chosen is a service activator that invokes a method on a mock.
Arguably, this setup would make sense only as an integration test for the framework, but it serves as a simple example.
As the complexity of your configuration increases, it becomes increasingly useful to verify the flow of the messages through the system.
In a test like this, you’re not interested in the behavior and effects of the service that receives the objects as message payloads.
In fact, a failure in that service might distract you from the purpose of the test.
It therefore makes sense to replace the service with a mock, but because this service is wired as a bean in a Spring context, it isn’t as easy as injecting a component with mocked collaborators, as you would do in a normal unit test.
This code overrides the service bean with a bean that’s a mock created by Mockito.
This bean, @Autowired into your test case, can be used like any other mock with the only difference being that its lifecycle will be managed by Spring instead of JUnit directly.
This strategy is particularly useful to avoid calling services that operate on external systems.
Invoking an external system is more problematic to clean up, but it’s also more complicated to verify the invocation happened correctly.
If you use a mock, you can simply verify that it was touched, and that’s it.
This is a good option for channel adapters too, because it gives you a generic way to deal with them.
In section 18.3, we use this strategy as well to deal with the need to wait for an invocation to happen before we start asserting the result.
This section focused only on Mockito, but similar support for EasyMock, JMock, and RMock can be implemented along the same lines.
It’s unlikely that Spring Integration will natively support all of these frameworks in the near future.
After reading this chapter, you should have some idea of how to implement the test support of your choosing, and chances are good that someone out there has shared some matcher you might reuse.
Figure 18.2  First record the behavior of the mock B.
Then send a test message to the input channel C.
After waiting for the message to come out of the other end D, verify that the appropriate operations on the mock have been invoked E.
Variations on this recipe work also in more complex cases.
We already secretly used some concurrency features of Spring Integration to our advantage in tests, but now it’s time to explore the different concurrency strategies.
The combination of mocking and latching is especially powerful, so stay tuned!
One of the trickiest things to solve cleanly in tests is assertions around related actions performed by multiple threads.
A big advantage of staged event-driven architecture (SEDA; see chapter 2) is that components become passive and react to events rather than actively changing the world around them.
This opens the door to decoupling cause and effect using a framework rather than having the complexities of asynchronous handoff emerge in business code.
At runtime, though, these subtleties are essential to the proper functioning of the system.
This section focuses on the concerns around testing an asynchronous system.
An asynchronous system is a system in which multiple threads are involved in performing a bit of work (such as processing a message)
If a part of your system is designed to process messages asynchronously, you should keep an eye on certain things.
As you saw in chapter 3, processing messages asynchronously can be done in several different ways.
Also, when you use a publish-subscribe channel configured with a task executor, you’re using asynchronous handoff.
Finally, there are a few endpoints that can be configured with a TaskExecutor that will process a message in a different thread than the thread pushing the message in.
To give you a handle on this, remember that whenever an endpoint or channel is using storage or a task executor, it can cause asynchronous handoff.
Whenever asynchronous handoff is involved, there are no chronological guarantees without explicit locking.
Luckily, getting explicit locking in place is simple in Spring Integration, but if you’re not familiar with what happens under the hood, you can be sucked into hours of fruitless debugging.
You sure can! In most cases, that’s precisely what you should do.
We look at a few exceptions later, but in the vast majority of cases, plugging into the output channel of your context and just waiting for the output message to arrive is sufficient.
For a standard test case, you just follow this simple recipe: make sure your output goes to a QueueChannel and receive from this channel before doing any assertions:
As you can see, you receive m from the output channel before you assert that the service has been invoked.
Also, you use a timeout in the receive call to ensure the test doesn’t run indefinitely.
The assertions are done in the order that you expect them to succeed.
You’re piggybacking on the contract of the receive method here.
Because receive is a blocking call, you don’t have to do any additional waiting to ensure a happens-before relationship between the service invocation and the verification.
However complex your contexts get, it’s almost always possible to find some output that will arrive only after the behavior you’re trying to verify has been executed.
When designing a test case, you must understand that the main function of the test is to fail when the software doesn’t behave correctly.
This main function is best implemented when the failure clearly points out what part of the behavior was incorrect.
If the receive call had no timeout, and an exception were thrown from the service, the test could run indefinitely.
The test wouldn’t fail in this case of malfunctioning in the software under test, so the test would be flawed.
You could put a general timeout on the test to prevent it from running indefinitely.
The timeout would fix the flaw, but the failure message would have no relation to the cause of the failure.
If the service isn’t invoked, you’ll most likely get no output message.
Thinking carefully about the order of the assertions can prevent this problem.
Before we look at exceptional cases in which waiting for output isn’t an option or isn’t sufficient to prove that the system functions correctly, we examine the need for proper test cases in asynchronous scenarios a bit further.
Before you get the wrong idea, let’s make it clear that debugging is a skill that all excellent developers have and all novice developers should strive to learn.
To tweak an old saying: Debug a program and you fix it for a day; improve the tests and the logging of a program, and you fix it for its lifetime.
Before you dive into hours of debugging, you should ask yourself, what is this test not telling me that I need to know? The answer is often right in front of you, hard to reach with a debugger, easy to log.
Once you have a test suite and some decent logging around the problem, another developer can continue where you left off.
Testing an asynchronous system unthinkable scenario that the problem happens in production, you can ask the system administrator for the log file.
The reason we bring this up is because debugging is much harder in a concurrent scenario than in a single-threaded scenario.
It helps you understand the code more quickly than just reading through it would.
In many cases, you don’t want to fix all the logging in your program; you just want to see what’s going on.
But if multiple threads are entering the problem area of your code, debugging loses all its power.
Suddenly, the debugger changes the timing that led to a race condition and often completely hides the bug from your sight.
Luckily, logging and test cases are much more reliable even when dealing with concurrent access.
That’s not to say that finding and analyzing a concurrency issue is easy.
So heed this advice: especially when facing a concurrency bug, try to avoid the debugger and fix the problem with tests and logging.
Now that you’ve learned to prefer testing and logging over the debugger in concurrent scenarios, you’re ready to learn how to wait for messages to terminate inside an endpoint using latches and mocks.
As you read in chapter 4, these types of endpoints are called channel adapters.
A channel adapter takes the payload of a message and feeds it to a service.
This service may be a bean in your context, but it might also be a database, a web service, the filesystem, or standard output.
The tricky part is to wait for the invocation of this service before you start making assertions about the state of the system.
In this section, we show you how you can inject latches into endpoints that have been mocked with Mockito.
Similar strategies exist for other mocking frameworks, and the problem can also be solved with channel interceptors or AOP.
Going over all these options is beyond the scope of this book, but this section should be enough to spark your imagination.
In figure 18.2, we showed how to mock out services from tests.
That example required no latching inside the mock because you could just wait for the message to come out of the output channel, as discussed in the previous section.
When notifications are sent to the user, they’re sent over an asynchronous communication channel.
You specifically don’t want to wait for the external system to confirm something was sent synchronously.
Looking at the email outbound channel adapter, for instance, you need to confirm that a message reaches this adapter, but you don’t need to test the sending of the email in this test.
There should be another test for sending, but that’s in the scope of infrastructure testing.
You design your test to verify that a notification is passed into the smsNotifierBean’s notify method whenever a message containing the same notification is sent to the tripNotifications channel.
First you must make sure you replace the smsNotifierBean with a mock.
Once that job is done, you can focus on the test itself.
The test sends a message containing the test notification to the channel.
You have to wait for the message to arrive before you can verify.
This can be done using a latch injected into the mock:
With the answer returned by this method, you can tell Mockito to count down the latch passed in whenever a certain method is invoked.
Because the notify method returns void, you use Mockito’s doAnswer method to record the behavior.
You’re essentially telling Mockito, “When the notify method is invoked on smsNotifier, react by counting down the notifierInvoked latch.” Then it’s a matter of awaiting the latch so you can execute assertions under the safe assumption that they’ll happen after the message arrives at the endpoint.
Before we round up, we should give you some guidelines for making your applications easier to test.
This isn’t an easy thing, but it’s a skill worth honing.
We can’t overemphasize that changing the application to improve testability is a good thing.
In Spring Integration applications, you usually see good decoupled code that’s easy to test.
But what about the configuration? With all that XML containing all those little SpEL expressions and intricate dependencies, you could easily get lost.
It’s said that programming in XML is a bad thing (which it is)
That’s why Spring Integration focuses on XML as a domain-specific language for the configuration of enterprise integration patterns.
Nevertheless, it is arguably possible to cross the fuzzy line into XML programming if the configuration becomes too convoluted.
This section offers some pointers to help you spot problems in this area and combat them with your test goggles on.
You can do complex things with Spring and Spring Integration, particularly using SpEL for elaborate routing.
Don’t! It might seem powerful, even simple at first, but testing logic that’s embedded in XML is tough to the point of headache.
Instead, design your flows in linear steps as much as you can.
If you want to use SpEL, keep it simple; delegate to Java code for the complex decisions.
Also, it’s fine to invoke methods on other objects from Java directly; not every fine-grained step needs to be a service activator.
As your application gets larger, the configuration files grow too.
At some point, it becomes hard to find that part of the configuration that you need to change.
Take out the detailed flows and integrate them using import statements.
If you’re used to Spring, you might’ve put configuration related to data access in a separate file, or you might’ve created several servlet contexts using the same root context.
With a messaging application, splitting in layers isn’t a good fit.
It’s better to divide the flow into different phases and give each phase its own context.
One way to make the subcomponents more testable is to define input and output channels in each subcontext and use bridges to glue them together in the main context.
This way, a test that focuses on a particular subcontext in isolation can easily use the same concept to wire the input and output to channels that are specific to that test.
In the next section, we take a brief glimpse into the realm of threading.
You can prove the correctness of your code under concurrent access, but it’s usually unfeasible to test all possible concurrent scenarios and make sure they meet the specifications.
But there are a few things you can do to help ensure your code is thread safe.
Just repeat to yourself: Pass immutable objects between stateless services.
Where testing is concerned, you can do your best to make sure concurrency bugs have a chance to surface in your test.
For one, you should use at least the same number of threads in some of your integration tests as would be used in your production application.
This ensures that the code is at least run concurrently in your continuous integration build.
Some failures will still be unlikely to occur in a test, so this is by no means foolproof.
Tests written this way might cause intermittent failures, which in many cases means you have a concurrency bug in your code.
Concurrency bugs are best tackled by logging and testing, but they can be a huge pain to reproduce.
Some frameworks, such as ConcuTest, are helpful in provoking concurrency bugs by injecting yields and waits into your bytecode.
If you learn these tools, you’ll have a better chance of resolving concurrency bugs.
In the future, Spring Integration’s testing module may very well expand to include a full concurrency test suite.
In this chapter, we formalized our understanding of testing Spring Integration applications.
First, we discussed the test support in Spring Integration’s own test framework.
Then, we detailed the strategies and rationale for mocking out external dependencies and business services from message flow tests.
Finally, we discussed testing asynchronous applications on a broader level and showed you how to enforce chronological order in tests with asynchronous channels or mocks and latches.
Within the scope of the test framework, you saw different ways of matching messages, either by their headers or by their payloads.
Matching payloads is helpful when you want to avoid unwrapping messages and casting their payloads to the expected types.
We discussed support for unwrapping headers, including an example dealing with the whole map of headers.
We made a case for mocking business services out of tests.
Because a Spring Integration configuration defines a message flow that’s dynamically used at runtime, it.
Summary becomes important to test this configuration in relative isolation too.
Mocking away external dependencies is an excellent way to achieve this goal.
Finally, we went into the details of testing a full asynchronous message flow.
We explained how to use a blocking receive call to ensure chronological order in tests.
Also we explained that when this doesn’t work, you can use latches within mocks to enforce happens-before relationships.
This is the last chapter, but that doesn’t mean it’s least important.
A proper understanding of how to test your application is both the end and the beginning of craftsmanship in software engineering.
See mocks Eclipse Gemini Blueprint project and messaging 298 and point-to-point.
Spring Integration in Action is an introduction and guide to enterprise integration and messaging using the Spring Integration framework.
Th e book starts off  by reviewing core messaging patterns, such as those used in transformation and routing.
It then drills down into real-world enterprise integration scenarios using JMS, web services, fi lesystems, email, and more.
You’ll fi nd an emphasis on testing, along with practical coverage of topics like concurrency, scheduling, system management, and monitoring.
Th is book is accessible to developers who know Java.
Experience with Spring and EIP is helpful but not assumed.
Mark Fisher is the Spring Integration founder and project lead.
Author Online about the authors about the cover illustration Part 1 Background.
Created PDF documents can be opened with Acrobat and Adobe Reader 5.0 and later.
