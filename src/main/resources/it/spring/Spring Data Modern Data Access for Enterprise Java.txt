O’Reilly books may be purchased for educational, business, or sales promotional use.
Spring Data, the image of a giant squirrel, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
Thanks to my wife, Daniela, and sons, Gabriel and Alexandre, whose patience with me stealing.
I’d like to thank my family, friends, fellow musicians, and everyone I’ve had the pleasure to work with so far; the entire Spring Data and SpringSource team for this awesome journey; and last, but actually first of all, Sabine, for her inexhaustible love and support.
To my wife, Carol, and my son, Alex, thank you for enriching my life and for all your support and.
My special thanks go to Rod and Emil for starting the Spring Data project and to Oliver for making it great.
My family is always very supportive of my crazy work; I’m very grateful to have such.
I’d like to thank my wife, Nanette, and my kids for their support, patience, and understanding.
Thanks also to Rod and my colleagues on the Spring Data team for making all of this possible.
Familiar assumptions are under threat—among them, that the relational database should be the default choice for persistence.
While this is now widely accepted, it is far from clear how to proceed effectively into the new world.
Many newer stores require more developer effort than Java developers are used to regarding data access, pushing into the application things customarily done in a relational database.
This book helps you make sense of this new reality.
It provides an excellent overview of today’s storage world in the context of today’s hardware, and explains why NoSQL stores are important in solving modern business problems.
Because of the language’s identification with the often-conservative enterprise market (and perhaps also because of the sophistication of Java object-relational mapping [ORM] solutions), Java developers have traditionally been poorly served in the NoSQL space.
Fortunately, this is changing, making this an important and timely book.
Spring Data is an important project, with the potential to help developers overcome new challenges.
Many of the values that have made Spring the preferred platform for enterprise Java developers deliver particular benefit in a world of fragmented persistence solutions.
Part of the value of Spring is how it brings consistency (without descending to a lowest common denominator) in its approach to different technologies with which it integrates.
A distinct “Spring way” helps shorten the learning curve for developers and simplifies code maintenance.
If you are already familiar with Spring, you will find that Spring Data eases your exploration and adoption of unfamiliar stores.
If you aren’t already familiar with Spring, this is a good opportunity to see how Spring can simplify your code and make it more consistent.
The authors are uniquely qualified to explain Spring Data, being the project leaders.
They bring a mix of deep Spring knowledge and involvement and intimate experience with a range of modern data stores.
They do a good job of explaining the motivation of Spring Data and how it continues the mission Spring has long pursued regarding data access.
There is valuable coverage of how Spring Data works with other parts of.
The book also provides much value that goes beyond Spring—for example, the discussions of the repository concept, the merits of type-safe querying, and why the Java Persistence API (JPA) is not appropriate as a general data access solution.
While this is a book about data access rather than working with NoSQL, many of you will find the NoSQL material most valuable, as it introduces topics and code with which you are likely to be less familiar.
All content is up to the minute, and important topics include document databases, graph databases, key/value stores, Hadoop, and the Gemfire data fabric.
We programmers are practical creatures and learn best when we can be hands-on.
Early on, the authors show how to get the sample code working in the two leading Java integrated development environments (IDEs), including handy screenshots.
They explain requirements around database drivers and basic database setup.
I applaud their choice of hosting the sample code on GitHub, making it universally accessible and browsable.
Given the many topics the book covers, the well-designed examples help greatly to tie things together.
The emphasis on practical development is also evident in the chapter on Spring Roo, the rapid application development (RAD) solution from the Spring team.
Most Roo users are familiar with how Roo can be used with a traditional JPA architecture; the authors show how Roo’s productivity can be extended beyond relational databases.
When you’ve finished this book, you will have a deeper understanding of why modern data access is becoming more specialized and fragmented, the major categories of NoSQL data stores, how Spring Data can help Java developers operate effectively in this new environment, and where to look for deeper information on individual topics in which you are particularly interested.
Most important, you’ll have a great start to your own exploration in code!
Overview of the New Data Access Landscape The data access landscape over the past seven or so years has changed dramatically.
Relational databases, the heart of storing and processing data in the enterprise for over 30 years, are no longer the only game in town.
The past seven years have seen the birth —and in some cases the death—of many alternative data stores that are being used in mission-critical enterprise applications.
These new data stores have been designed specifically to solve data access problems that relational database can’t handle as effectively.
An example of a problem that pushes traditional relational databases to the breaking point is scale.
While data that is stored in relational databases is still crucial to the enterprise, these new types of data are not being stored in relational databases.
While general consumer demands drive the need to store large amounts of media files, enterprises are finding it important to store and analyze many of these new sources of data.
For example, companies can better understand the behavior of their products if the products themselves are sending “phone home” messages about their health.
To better understand their customers, companies can incorporate social media data into their decision-making processes.
Big data generally refers to the process in which large quantities of data are stored, kept in raw form, and continually analyzed and combined with other data sources to provide a deeper understanding of a particular domain, be it commercial or scientific in nature.
Many companies and scientific laboratories had been performing this process before the term big data came into fashion.
What makes the current process different from before is that the value derived from the intelligence of data analytics is higher than the hardware costs.
Aggregate data transfer rates for clusters of commodity hardware that use local disk are also significantly higher than SAN- or NAS-based systems—500 times faster for similarly priced systems.
On the software side, the majority of the new data access technologies are open source.
While open source does not mean zero cost, it certainly lowers the barrier for entry and overall cost of ownership versus the traditional commercial software offerings in this space.
Another problem area that new data stores have identified with relational databases is the relational data model.
If you are interested in analyzing the social graph of millions of people, doesn’t it sound quite natural to consider using a graph database so that the implementation more closely models the domain? What if requirements are continually driving you to change your relational database management system (RDBMS) schema and object-relational mapping (ORM) layer? Perhaps a “schema-less” document database will reduce the object mapping complexity and provide a more easily evolvable system as compared to the more rigid relational model.
While each of the new databases is unique in its own way, you can provide a rough taxonomy across most of them based on their data models.
Column family An extended key/value data model in which the value data type can also be a sequence of key/value pairs.
Document Collections that contain semistructured data, such as XML or JSON.
The data model has nodes and edges, each of which may have properties.
The general name under which these new databases have become grouped is “NoSQL databases.” In retrospect, this name, while catchy, isn’t very accurate because it seems to imply that you can’t query the database, which isn’t true.
It reflects the basic shift away from the relational data model as well as a general shift away from ACID (atomicity, consistency, isolation, durability) characteristics of relational databases.
One of the driving factors for the shift away from ACID characteristics is the emergence of applications that place a higher priority on scaling writes and having a partially functioning system even when parts of the system have failed.
While scaling reads in a relational database can be achieved through the use of in-memory caches that front the database, scaling writes is much harder.
To put a label on it, these new applications favor a system that has so-called “BASE” semantics, where the acronym represents basically available, scalable, eventually consistent.
Distributed data grids with a key/ value data model generally have not been grouped into this new wave of NoSQL databases.
However, they offer similar features to NoSQL databases in terms of the scale of data they can handle as well as distributed computation features that colocate computing power and data.
As you can see from this brief introduction to the new data access landscape, there is a revolution taking place, which for data geeks is quite exciting.
Relational databases are not dead; they are still central to the operation of many enterprises and will remain so for quite some time.
The trends, though, are very clear: new data access technologies are solving problems that traditional relational databases can’t, so we need to broaden our skill set as developers and have a foot in both camps.
The Spring Framework has a long history of simplifying the development of Java applications, in particular for writing RDBMS-based data access layers that use Java database connectivity (JDBC) or object-relational mappers.
In this book we aim to help developers get a handle on how to effectively develop Java applications across a wide range of these new technologies.
The Spring Data project directly addresses these new technologies so that you can extend your existing knowledge of Spring to them, or perhaps learn more about Spring as a byproduct of using Spring Data.
Spring Data also provides an extensive set of new features to Spring’s RDBMS support.
How to Read This Book This book is intended to give you a hands-on introduction to the Spring Data project, whose core mission is to enable Java developers to use state-of-the-art data processing and manipulation tools but also use traditional databases in a state-of-the-art manner.
We’ll start by introducing you to the project, outlining the primary motivation of SpringSource and the team.
We’ll also describe the domain model of the sample projects that accommodate each of the later chapters, as well as how to access and set up the code (Chapter 1)
Conventions Used in This Book The following typographical conventions are used in this book:
Italic Indicates new terms, URLs, email addresses, filenames, and file extensions.
Constant width Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Technology professionals, software developers, web designers, and business and creative professionals use Safari Books Online as their primary resource for research, problem solving, learning, and certification training.
Safari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals.
For more information about Safari Books Online, please visit us online.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
For more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.
Find us on Facebook: http://facebook.com/oreilly Follow us on Twitter: http://twitter.com/oreillymedia Watch us on YouTube: http://www.youtube.com/oreillymedia.
The Spring Data project was coined at Spring One 2010 and originated from a hacking session of Rod Johnson (SpringSource) and Emil Eifrem (Neo Technologies) early that year.
They were trying to integrate the Neo4j graph database with the Spring Framework and evaluated different approaches.
The session created the foundation for what would eventually become the very first version of the Neo4j module of Spring Data, a new SpringSource project aimed at supporting the growing interest in NoSQL data stores, a trend that continues to this day.
Spring has provided sophisticated support for traditional data access technologies from day one.
It significantly simplified the implementation of data access layers, regardless of whether JDBC, Hibernate, TopLink, JDO, or iBatis was used as persistence technology.
This support has matured over the years and the latest Spring versions contained decent upgrades to this layer of support.
The traditional data access support in Spring has targeted relational databases only, as they were the predominant tool of choice when it came to data persistence.
As NoSQL stores enter the stage to provide reasonable alternatives in the toolbox, there’s room to fill in terms of developer support.
Beyond that, there are yet more opportunities for improvement even for the traditional relational stores.
These two observations are the main drivers for the Spring Data project, which consists of dedicated modules for NoSQL stores as well as JPA and JDBC modules with additional support for relational databases.
NoSQL Data Access for Spring Developers Although the term NoSQL is used to refer to a set of quite young data stores, all of the stores have very different characteristics and use cases.
Ironically, it’s the nonfeature (the lack of support for running queries using SQL) that actually named this group of databases.
As these stores have quite different traits, their Java drivers have completely.
Trying to abstract away their differences would actually remove the benefits each NoSQL data store offers.
A graph database should be chosen to store highly interconnected data.
A document database should be used for tree and aggregate-like data structures.
A key/value store should be chosen if you need cache-like functionality and access patterns.
With the JPA, the Java EE (Enterprise Edition) space offers a persistence API that could have been a candidate to front implementations of NoSQL databases.
Unfortunately, the first two sentences of the specification already indicate that this is probably not working out:
This document is the specification of the Java API for the management of persistence and object/relational mapping with Java EE and Java SE.
The technical objective of this work is to provide an object/relational mapping facility for the Java application developer using a Java domain model to manage a relational database.
This theme is clearly reflected in the specification later on.
It defines concepts and APIs that are deeply connected to the world of relational persistence.
How should one implement the transaction API for stores like MongoDB, which essentially do not provide transactional semantics spread across multidocument manipulations? So implementing a JPA layer on top of a NoSQL store would result in a profile of the API at best.
On the other hand, all the special features NoSQL stores provide (geospatial functionality, map-reduce operations, graph traversals) would have to be implemented in a proprietary fashion anyway, as JPA simply does not provide abstractions for them.
This context rules out JPA as a potential abstraction API for these stores.
Still, we would like to see the programmer productivity and programming model consistency known from various Spring ecosystem projects to simplify working with NoSQL stores.
This led the Spring Data team to declare the following mission statement:
Spring Data provides a familiar and consistent Spring-based programming model for NoSQL and relational stores while retaining store-specific features and capabilities.
Instead of trying to abstract all stores behind a single API, the Spring Data project provides a consistent programming model across the different store implementations using patterns and abstractions already known from within the Spring Framework.
This allows for a consistent experience when you’re working with different stores.
General Themes A core theme of the Spring Data project available for all of the stores is support for configuring resources to access the stores.
This support is mainly implemented as XML namespace and support classes for Spring JavaConfig and allows us to easily set up access to a Mongo database, an embedded Neo4j instance, and the like.
Also, integration with core Spring functionality like JMX is provided, meaning that some stores will expose statistics through their native API, which will be exposed to JMX via Spring Data.
Most of the NoSQL Java APIs do not provide support to map domain objects onto the stores’ data abstractions (documents in MongoDB; nodes and relationships for Neo4j)
So, when working with the native Java drivers, you would usually have to write a significant amount of code to map data onto the domain objects of your application when reading, and vice versa on writing.
Thus, a very core part of the Spring Data modules is a mapping and conversion API that allows obtaining metadata about domain classes to be persistent and enables the actual conversion of arbitrary domain objects into storespecific data types.
On top of that, we’ll find opinionated APIs in the form of template pattern implementations already well known from Spring’s JdbcTemplate, JmsTemplate, etc.
Thus, there is a RedisTemplate, a MongoTemplate, and so on.
As you probably already know, these templates offer helper methods that allow us to execute commonly needed operations like persisting an object with a single statement while automatically taking care of appropriate resource management and exception translation.
Beyond that, they expose callback APIs that allow you to access the store-native APIs while still getting exceptions translated and resources managed properly.
These features already provide us with a toolbox to implement a data access layer like we’re used to with traditional databases.
To ease that process even more, Spring Data provides a repository abstraction on top of the template implementation that will reduce the effort to implement data access objects to a plain interface definition for the most common scenarios like performing standard CRUD operations as well as executing queries in case the store supports that.
This abstraction is actually the topmost layer and blends the APIs of the different stores as much as reasonably possible.
Thus, the store-specific implementations of it share quite a lot of commonalities.
This is why you’ll find a dedicated chapter (Chapter 2) introducing you to the basic programming model.
Now let’s take a look at our sample code and the domain model that we will use to demonstrate the features of the particular store modules.
The Domain To illustrate how to work with the various Spring Data modules, we will be using a sample domain from the ecommerce sector (see Figure 1-1)
As NoSQL data stores usually have a dedicated sweet spot of functionality and applicability, the individual chapters might tweak the actual implementation of the domain or even only partially implement it.
This is not to suggest that you have to model the domain in a certain way, but rather to emphasize which store might actually work better for a given application scenario.
At the core of our model, we have a customer who has basic data like a first name, a last name, an email address, and a set of addresses in turn containing street, city, and country.
We also have products that consist of a name, a description, a price, and arbitrary attributes.
These abstractions form the basis of a rudimentary CRM (customer relationship management) and inventory system.
On top of that, we have orders a customer can place.
An order contains the customer who placed it, shipping and billing addresses, the date the order was placed, an order status, and a set of line items.
These line items in turn reference a particular product, the number of products to be ordered, and the price of the product.
The Sample Code The sample code for this book can be found on GitHub.
It is a Maven project containing a module per chapter.
It requires either a Maven 3 installation on your machine or an IDE capable of importing Maven projects such as the Spring Tool Suite (STS)
Getting the code is as simple as cloning the repository:
You can now build the code by executing Maven from the command line as follows: $ mvn clean package.
This will cause Maven to resolve dependencies, compile and test code, execute tests, and package the modules eventually.
STS/Eclipse STS ships with the m2eclipse plug-in to easily work with Maven projects right inside your IDE.
So, if you have it already downloaded and installed (have a look at Chapter 3 for details), you can choose the Import option of the File menu.
Select the Existing Maven Projects option from the dialog box, shown in Figure 1-2
In the next window, select the folder in which you’ve just checked out the project using the Browse button.
After you’ve done so, the pane right below should fill with the individual Maven modules listed and checked (Figure 1-3)
Proceed by clicking on Finish, and STS will import the selected Maven modules into your workspace.
It will also resolve the necessary dependencies and source folder according to the pom.xml file in the module’s root directory.
Next you must add JPA support in the Spring Data JPA module to enable finder method completion and error checking of repositories.
In the resulting dialog box, check JavaEE Persistence support and select Hibernate as the persistence provider (Figure 1-9)
Implementing the data access layer of an application has been cumbersome for quite a while.
Domain classes were anemic and not designed in a real object-oriented or domain-driven manner.
The goal of the repository abstraction of Spring Data is to reduce the effort required to implement data access layers for various persistence stores significantly.
The following sections will introduce the core concepts and interfaces of Spring Data repositories.
We will use the Spring Data JPA module as an example and discuss the basic concepts of the repository abstraction.
For other stores, make sure you adapt the examples accordingly.
Quick Start Let’s take the Customer domain class from our domain that will be persisted to an arbitrary store.
A traditional approach to a data access layer would now require you to at least implement a repository class that contains necessary CRUD (Create, Read, Update, and Delete) methods as well as query methods to access subsets of the entities stored by applying restrictions on them.
As you can see, we extend the Spring Data Repository interface, which is just a generic marker interface.
Its main responsibility is to allow the Spring Data infrastructure to pick up all user-defined Spring Data repositories.
Beyond that, it captures the type of the domain class managed alongside the type of the ID of the entity, which will come in quite handy at a later stage.
We just need to configure the XML element’s base-package attribute with our root package so that Spring Data will scan it for repository interfaces.
The annotation can also get a dedicated package configured to scan for interfaces.
Without any further configuration given, it will simply inspect the package of the annotated class.
For other stores, we simply use the corresponding namespace elements or annotations.
The configuration snippet, shown in Example 2-5, will now cause the Spring Data repositories to be found, and Spring beans will be created that actually.
Thus a client could now go ahead and get access to the bean by letting Spring simply autowire it.
With our CustomerRepository interface set up, we are ready to dive in and add some easy-to-declare query methods.
A typical requirement might be to retrieve a Customer by its email address.
To do so, we add the appropriate query method (Example 2-6)
The namespace element will now pick up the interface at container startup time and trigger the Spring Data infrastructure to create a Spring bean for it.
The infrastructure will inspect the methods declared inside the interface and try to determine a query to be executed on method invocation.
If you don’t do anything more than declare the method, Spring Data will derive a query from its name.
In Example 2-6, the query can be derived because we followed the naming convention of the domain object’s properties.
The Email part of the query method name actually refers to the Customer class’s emailAddress property, and thus Spring Data will automatically derive select C from Customer c where c.emailAddress = ?1 for the method declaration if you were using the JPA module.
It will also check that you have valid property references inside your method declaration, and cause the container to fail to start on bootstrap time if it finds any errors.
Clients can now simply execute the method, causing the given method parameters to be bound to the query derived from the method name and the query to be executed (Example 2-7)
Query Lookup Strategies The interface we just saw had a simple query method declared.
The method declaration was inspected by the infrastructure and parsed, and a store-specific query was derived eventually.
However, as the queries become more complex, the method names would just become awkwardly long.
For more complex queries, the keywords supported by the method parser wouldn’t even suffice.
Here we use JPA as an example and manually define the query that would have been derived anyway.
The @Query annotation is not needed if named queries are used.
Query Derivation The query derivation mechanism built into the Spring Data repository infrastructure, shown in Example 2-9, is useful to build constraining queries over entities of the repository.
We will strip the prefixes findBy, readBy, and getBy from the method and start parsing the rest of it.
At a very basic level, you can define conditions on entity properties and concatenate them with And and Or.
The actual result of parsing that method will depend on the data store we use.
The expressions are usually property traversals combined with operators that can be concatenated.
As you can see in Example 2-9, you can combine property expressions with And and Or.
Beyond that, you also get support for various operators like Between, LessThan, GreaterThan, and Like for the property expressions.
As the operators supported can vary from data store to data store, be sure to look at each store’s corresponding chapter.
Property expressions Property expressions can just refer to a direct property of the managed entity (as you just saw in Example 2-9)
On query creation time, we already make sure that the parsed property is a property of the managed domain class.
However, you can also define constraints by traversing nested properties.
The resolution algorithm starts with interpreting the entire part (AddressZipCode) as a property and checks the domain class for a property with that name (with the first letter lowercased)
If not, it starts splitting up the source at the camel case parts from the right side into a head and a tail and tries to find the corresponding property (e.g., AddressZip and Code)
If it finds a property with that head, we take the tail and continue building the tree down from there.
Because in our case the first split does not match, we move the split point further to the left (from “AddressZip, Code” to “Address, Zip Code”)
Although this should work for most cases, there might be situations where the algorithm could select the wrong property.
Suppose our Customer class has an addressZip property as well.
To resolve this ambiguity, you can use an underscore ( _ ) inside your method name to manually define traversal points.
Pagination and Sorting If the number of results returned from a query grows significantly, it might make sense to access the data in chunks.
To achieve that, Spring Data provides a pagination API that can be used with the repositories.
The definition for what chunk of data needs to be read is hidden behind the Pageable interface alongside its implementation PageRe quest.
The data returned from accessing it page by page is held in a Page, which not only contains the data itself but also metainformation about whether it is the first or last page, how many pages there are in total, etc.
To calculate this metadata, we will have to trigger a second query as well as the initial one.
We can use the pagination functionality with the repository by simply adding a Pagea ble as a method parameter.
Unlike the others, this will not be bound to the query, but rather used to restrict the result set to be returned.
One option is to have a return type of Page, which will restrict the results, but require another query to calculate the metainformation (e.g., the total number of elements available)
Our other option is to use List, which will avoid the additional query but won’t provide the metadata.
If you don’t need pagination functionality, but plain sorting only, add a Sort parameter to the method signature (see Example 2-10)
The first method allows you to pass a Pageable instance to the query method to dynamically add paging to your statically defined query.
Sorting options can either be handed into the method by the Sort parameter explicitly, or embedded in the PageRe quest value object, as you can see in Example 2-11
Defining Repositories So far, we have seen repository interfaces with query methods derived from the method name or declared manually, depending on the means provided by the Spring Data module for the actual store.
To derive these queries, we had to extend a Spring Dataspecific marker interface: Repository.
Apart from queries, there is usually quite a bit of functionality that you need to have in your repositories: the ability to store objects, to delete them, look them up by ID, return all entities stored, or access them page by page.
The easiest way to expose this kind of functionality through the repository interfaces is by using one of the more advanced repository interfaces that Spring Data provides:
Repository A plain marker interface to let the Spring Data infrastructure pick up user-defined repositories.
CrudRepository Extends Repository and adds basic persistence methods like saving, finding, and deleting entities.
Suppose we want to expose typical CRUD operations for the CustomerRepository.
All we need to do is change its declaration as shown in Example 2-12
It contains methods to save a single entity as well as an Iterable of entities, finder methods for a single entity or all entities, and delete(…) methods of different flavors.
Each of the Spring Data modules supporting the repository approach ships with an implementation of this interface.
Thus, the infrastructure triggered by the namespace element declaration will not only bootstrap the appropriate code to execute the query methods, but also use an instance of the generic repository implementation class to back the methods declared in CrudRepository and eventually delegate calls to save(…), findAll(), etc., to that instance.
Fine-Tuning Repository Interfaces As we’ve just seen, it’s very easy to pull in chunks of predefined functionality by extending the appropriate Spring Data repository interface.
The decision to implement this level of granularity was actually driven by the trade-off between the number of interfaces (and thus complexity) we would expose in the event that we had separator interfaces for all find methods, all save methods, and so on, versus the ease of use for developers.
However, there might be scenarios in which you’d like to expose only the reading methods (the R in CRUD) or simply prevent the delete methods from being exposed in your repository interfaces.
Spring Data now allows you to tailor a custom base repository with the following steps:
Add the methods you want to expose to it and make sure they actually match the signatures of methods provided by the Spring Data base repository interfaces.
Use this interface as a base interface for the interface declarations for your entities.
To illustrate this, let’s assume we’d like to expose only the findAll(…) method taking a Pageable as well as the save methods.
Note that we additionally annotated the interface with @NoRepositoryBean to make sure the Spring Data repository infrastructure doesn’t actually try to create a bean instance for it.
Letting your CustomerRepository extend this interface will now expose exactly the API you defined.
We usually recommend starting with locally defined CRUD methods directly in the concrete repository for an entity and then moving either to the Spring Data–provided base repository interfaces or tailor-made ones if necessary.
That way, you keep the number of artifacts naturally growing with the project’s complexity.
Manually Implementing Repository Methods So far we have seen two categories of methods on a repository: CRUD methods and query methods.
Both types are implemented by the Spring Data infrastructure, either by a backing implementation or the query execution engine.
These two cases will probably cover a broad range of data access operations you’ll face when building applications.
However, there will be scenarios that require manually implemented code.
We start by implementing just the functionality that actually needs to be implemented manually, and follow some naming conventions with the implementation class (as shown in Example 2-16)
Neither the interface nor the implementation class has to know anything about Spring Data.
This works pretty much the way that you would manually implement code with Spring.
Also note that we kept both the interface as well as the implementation class as package private to prevent them being accessed from outside the package.
The final step is to change the declaration of our original repository interface to extend the just-introduced one, as shown in Example 2-17
But how does the implementation class actually get discovered and brought into the proxy to be executed eventually? The bootstrap process for a repository essentially looks as follows:
If one is found, we register this class as a Spring bean and use that.
The found custom implementation will be wired to the proxy configuration for the discovered interface and act as a potential target for method invocation.
This mechanism allows you to easily implement custom code for a dedicated repository.
The suffix used for implementation lookup can be customized on the XML namespace element or an attribute of the repository enabling annotation (see the individual store chapters for more details on that)
The reference documentation also contains some material on how to implement custom behavior to be applied to multiple repositories.
The core area of support provided for Spring Data by STS is the query derivation mechanism for finder methods.
As you can see in Figure 2-1, the IDE detects that Descrption is not valid, as there is no such property available on the Product class.
To discover these typos, it will analyze the Product domain class (something that bootstrapping the Spring Data repository infrastructure would do anyway) for properties and parse the method name into a property traversal tree.
To avoid these kinds of typos as early as possible, STS’s Spring Data support offers code completion for property names, criteria keywords, and concatenators like And and Or (see Figure 2-2)
The Order class has a few properties that you might want to refer to.
Assuming we’d like to traverse the billingAddress property, another Cmd+Space (or Ctrl+Space on Windows) would trigger a nested property traversal that proposes nested properties, as well as keywords matching the type of the property traversed so far (Figure 2-3)
To put some icing on the cake, the Spring Data STS will make the repositories firstclass citizens of your IDE navigator, marking them with the well-known Spring bean symbol.
Beyond that, the Spring Elements node in the navigator will contain a dedicated Spring Data Repositories node to contain all repositories found in your application’s configuration (see Figure 2-4)
As you can see, you can discover the repository interfaces at a quick glance and trace which configuration element they actually originate from.
IntelliJ IDEA Finally, with the JPA support enabled, IDEA offers repository finder method completion derived from property names and the available keyword, as shown in Figure 2-5
Writing queries to access data is usually done using Java Strings.
The query languages of choice have been SQL for JDBC as well as HQL/JPQL for Hibernate/JPA.
Defining the queries in plain Strings is powerful but quite error-prone, as it’s very easy to introduce typos.
Beyond that, there’s little coupling to the actual query source or sink, so column references (in the JDBC case) or property references (in the HQL/JPQL context) become a burden in maintenance because changes in the table or object model cannot be reflected in the queries easily.
The Querydsl project tries to tackle this problem by providing a fluent API to define these queries.
The API is derived from the actual table or object model but is highly store- and model-agnostic at the same time, so it allows you to create and use the query API for a variety of stores.
This versatility is the main reason why the Spring Data project integrates with Querydsl, as Spring Data also integrates with a variety of stores.
The following sections will introduce you to the Querydsl project and its basic concepts.
We will go into the details of the store-specific support in the storerelated chapters later in this book.
Introduction to Querydsl When working with Querydsl, you will usually start by deriving a metamodel from your domain classes.
Although the library can also use plain String literals, creating the metamodel will unlock the full power of Querydsl, especially its type-safe property and keyword references.
The derivation mechanism is based on the Java 6 Annotation Processing Tool (APT), which allows for hooking into the compiler and processing the sources or even compiled classes.
To kick things off, we need to define a domain class like the one shown in Example 3-1
We model our Customer with a few primitive and nonprimitive properties.
This is the default annotation, from which the Querydsl annotation processor generates the related query class.
When you’re using the integration with a particular store, the APT processor will be able to recognize the store-specific annotations (e.g., @Entity for JPA) and use them to derive the query classes.
As we’re not going to work with a store for this introduction and thus cannot use a store-specific mapping annotation, we simply stick with @QueryEntity.
The generated Querydsl query class will now look like Example 3-2
The class exposes public Path properties and references to other query classes (e.g., QEmailAddress)
This enables your IDE to list the available paths for which you might want to define predicates during code completion.
You can now use these Path expressions to define reusable predicates, as shown in Example 3-3
Changing the domain class would cause the query metamodel class to be regenerated.
Property references that have become invalidated by this change would become compiler errors and thus give us hints to places that need to be adapted.
The methods available on each of the Path types take the type of the Path into account (e.g., the like(…) method makes sense only on String properties and thus is provided only on those)
Because predicate definitions are so concise, they can easily be used inside a method declaration.
On the other hand, we can easily define predicates in a reusable manner, building up atomic predicates and combining them with more complex ones by using concatenating operators like And and Or (see Example 3-4)
We can use our newly written predicates to define a query for either a particular store or plain collections.
We’ll use the feature to query collections as an example now to keep things simple.
First, we set up a variety of Products to have something we can filter, as shown in Example 3-5
Next, we can use the Querydsl API to actually set up a query against the collection, which is some kind of filter on it (Example 3-6)
We hand it an instance of the query class for Product as well as the source collection.
In our case, we’d simply like to get back the Product instances matching the defined predicate.
As we have discovered, Querydsl allows us to define entity predicates in a concise and easy way.
These can be generated from the mapping information for a variety of stores as well as for plain Java classes.
Querydsl’s API and its support for various stores allows us to generate predicates to define queries.
Plain Java collections can be filtered with the very same API.
Generating the Query Metamodel As we’ve just seen, the core artifacts with Querydsl are the query metamodel classes.
The APT provides an API to programmatically inspect existing Java source code for certain annotations, and then call functions that in turn generate Java code.
Querydsl uses this mechanism to provide special APT processor implementation classes that inspect annotations.
If we already have domain classes mapped to a store supported by Querydsl, then generating the metamodel classes will require no extra effort.
The core integration point here is the annotation processor you hand to the Querydsl APT.
Build System Integration To integrate with Maven, Querydsl provides the maven-apt-plugin, with which you can configure the actual processor class to be used.
In Example 3-8, we bind the process goal to the generate-sources phase, which will cause the configured processor class to.
Supported Annotation Processors Querydsl ships with a variety of APT processors to inspect different sets of annotations and generate the metamodel classes accordingly.
Querying Stores Using Querydsl Now that we have the query classes in place, let’s have a look at how we can use them to actually build queries for a particular store.
As already mentioned, Querydsl provides integration modules for a variety of stores that offer a nice and consistent API to create query objects, apply predicates defined via the generated query metamodel classes, and eventually execute the queries.
The JPA module, for example, provides a JPAQuery implementation class that takes an EntityManager and provides an API to apply predicates before execution; see Example 3-9
If you remember Example 3-6, this code snippet doesn’t look very different.
In fact, the only difference here is that we use the JPAQuery as the base, whereas the former example used the collection wrapper.
So you probably won’t be too surprised to see that there’s not much difference in implementing this scenario for a MongoDB store (Example 3-10)
Integration with Spring Data Repositories As you just saw, the execution of queries with Querydsl generally consists of three major steps:
Two of these steps can be considered boilerplate, as they will usually result in very similar code being written.
On the other hand, the Spring Data repository tries to help users reduce the amount of unnecessary code; thus, it makes sense to integrate the repository extraction with Querydsl.
To expose this API through your repository interfaces, let it extend QueryDsl PredicateExecutor in addition to Repository or any of the other available base interfaces (see Example 3-12)
Extending the interface will have two important results: the first—and probably most obvious—is that it pulls in the API and thus exposes it to clients of CustomerReposi tory.
If it does and Querydsl is present on the classpath, Spring Data will select a special base class to back the repository proxy that generically implements the API methods by creating a store-specific query instance, bind the given predicates, potentially apply pagination, and eventually execute the query.
Manually Implementing Repositories The approach we have just seen solves the problem of generically executing queries for the domain class managed by the repository.
However, you cannot execute updates or deletes through this mechanism or manipulate the store-specific query instance.
This is actually a scenario that plays nicely into the feature of repository abstraction, which allows you to selectively implement methods that need hand-crafted code (see “Manually Implementing Repository Methods” on page 21 for general details on that topic)
To ease the implementation of a custom repository extension, we provide store-specific base classes.
The Java Persistence API (JPA) is the standard way of persisting Java objects into relational databases.
The JPA consists of two parts: a mapping subsystem to map classes onto relational tables as well as an EntityManager API to access the objects, define and execute queries, and more.
The Spring Framework has always offered sophisticated support for JPA to ease repository implementations.
The Spring Data JPA module implements the Spring Data Commons repository abstraction to ease the repository implementations even more, making a manual implementation of a repository obsolete in most cases.
This chapter will take you on a guided tour through the general setup and features of the module.
The base package contains a Spring JavaConfig class to configure the Spring container using a plain Java class instead of XML.
The two other packages contain our domain classes and repository interfaces.
As the name suggests, the core package contains the very basic abstractions of the domain model: technical helper classes like AbstractEntity, but also domain concepts like an EmailAddress, an Address, a Customer, and a Product.
Next, we have the orders package, which implements actual order concepts built on top of the foundational ones.
We will have a closer look at each of these classes in the following paragraphs, outlining their purpose and the way they are mapped onto the database using JPA mapping annotations.
The very core base class of all entities in our domain model is AbstractEntity (see Example 4-1)
It’s annotated with @MappedSuperclass to express that it is not a managed entity class on its own but rather will be extended by entity classes.
We declare an id of type Long here and instruct the persistence provider to automatically select the most appropriate strategy for autogeneration of primary keys.
Beyond that, we implement equals(…) and hashCode() by inspecting the id property so that entity classes of the same type with the same id are considered equal.
This class contains the main technical artifacts to persist an entity so that we can concentrate on the actual domain properties in the concrete entity classes.
Because they’re all basic ones, no additional annotations are needed, and the persistence provider will automatically map them into table columns.
If there were demand to customize the names of the columns to which the properties would be persisted, you could use the @Column annotation.
Customer contains quite a few other properties (e.g., the primitive ones firstname and lastname)
They are mapped just like the properties of Address that we have just seen.
Every Customer also has an email address represented through the EmailAddress class (see Example 4-3)
This class is a value object, as defined in Eric Evans’s book Domain Driven Design [Evans03]
Value objects are usually used to express domain concepts that you would naively implement as a primitive type (a string in this case) but that allow implementing domain constraints inside the value object.
Email addresses have to adhere to a specific format; otherwise, they are not valid email addresses.
So we actually implement the format check through some regular expression and thus prevent an EmailAddress instance from being instantiated if it’s invalid.
This means that we can be sure to have a valid email address if we deal with an instance of that type, and we don’t have to have some component validate it for us.
In terms of persistence mapping, the EmailAddress class is an @Embeddable, which will cause the persistence provider to flatten out all properties of it into the table of the surrounding class.
In our case, it’s just a single column for which we define a custom name: email.
As you can see, we need to provide an empty constructor for the JPA persistence provider to be able to instantiate EmailAddress objects via reflection (Example 5-4)
This is a significant shortcoming because you effectively cannot make the emailAddress a final one or assert make sure it is not null.
The Spring Data mapping subsystem used for the NoSQL store implementations does not impose that need onto the developer.
Have a look at “The Mapping Subsystem” on page 83 to see how a stricter implementation of the value object can be modeled in MongoDB, for example.
We use the @Column annotation on the email address to make sure a single email address cannot be used by multiple customers so that we are able to look up customers uniquely by their email address.
Finally we declare the Customer having a set of Addresses.
This property deserves deeper attention, as there are quite a few things we define here.
First, and in general, we use the @OneToMany annotation to specify that one Customer can have multiple Addresses.
Inside this annotation, we set the cascade type to Cascade Type.ALL and also activate orphan removal for the addresses.
For example, whenever we initially persist, update, or delete a customer, the Addresses will be persisted, updated, or deleted as well.
Thus, we don’t have to persist an Address instance up front or take care of removing all Addresses whenever we delete a Customer; the persistence provider will take care of that.
Note that this is not a database-level cascade but rather a cascade managed by your JPA persistence provider.
Beyond that, setting the orphan removal flag to true will take care of deleting Addresses from that database if they are removed from the collection.
All this results in the Address life cycle being controlled by the Customer, which makes the relationship a classical composition.
Plus, in domain-driven design (DDD) terminology, the Customer qualifies as aggregate root because it controls persistence operations and constraints for itself as well as other entities.
Finally, we use @JoinColumn with the addresses property, which causes the persistence provider to add another column to the table backing the Address object.
This additional column will then be used to refer to the Customer to allow joining the tables.
If we had left out the additional annotation, the persistence provider would have created a dedicated join table.
The final piece of our core package is the Product (Example 4-5)
Just as with the other classes discussed, it contains a variety of basic properties, so we don’t need to add annotations to get them mapped by the persistence provider.
We add only the @Column annotation to define the name and price as mandatory properties.
Beyond that, we add a Map to store additional attributes that might differ from Product to Product.
Now we have everything in place to build a basic customer relation management (CRM) or inventory system.
Next, we’re going to add abstractions that allow us to implement orders for Products held in the system.
First, we introduce a LineItem that captures a reference to a Product alongside the amount of products as well as the price at which the product was bought.
The final piece to complete the jigsaw puzzle is the Order entity, which is basically a pointer to a Customer, a shipping Address, a billing Address, and the LineItems actually ordered (Example 4-7)
The mapping of the line items is the very same as we already saw with Customer and Address.
The Order will automatically cascade persistence operations to the LineItem instances.
Thus, we don’t have to manage the persistence life cycle of the LineItems separately.
All other properties are many-to-one relationships to concepts already introduced.
Note that we define a custom table name to be used for Orders because Order itself is a reserved keyword in most databases; thus, the generated SQL to create the table as well as all SQL generated for queries and data manipulation would cause exceptions when executing.
ManyToOne(optional = false) private Customer customer; @ManyToOne private Address billingAddress;
A final aspect worth noting is that the constructor of the Order class does a defensive copy of the shipping and billing address.
This is to ensure that changes to the Address instance handed into the method do not propagate into already existing orders.
If we didn’t create the copy, a customer changing her Address data later on would also change the Address on all of her Orders made to that Address as well.
The Traditional Approach Before we start, let’s look at how Spring Data helps us implement the data access layer for our domain model, and discuss how we’d implement the data access layer the traditional way.
Thus, we use the Spring profiles mechanism to bootstrap the traditional implementation for only the single test case.
We don’t show these annotations in the sample code because they would not have actually been used if you implemented the entire data access layer the traditional way.
To persist the previously shown entities using plain JPA, we now create an interface and implementation for our repositories, as shown in Example 4-8
So we declare a method save(…) to be able to store accounts, and a query method to find all accounts that are assigned to a given customer by his email address.
Let’s see what an implementation of this repository would look like if we implemented it on top of plain JPA (Example 4-9)
The class is annotated with @Repository to enable exception translation from JPA exceptions to Spring’s Data AccessException hierarchy.
This helps optimize performance inside the persistence provider as well as on the database level.
This logic could, of course, be extracted into a common repository superclass, as we probably don’t want to repeat this code for every domain object–specific repository implementation.
The query method is quite straightforward as well: we create a query, bind a parameter, and execute the query to.
It’s almost so straightforward that you could regard the implementation code as boilerplate.
With a little bit of imagination, we can derive an implementation from the method signature: we expect a single customer, the query is quite close to the method name, and we simply bind the method parameter to it.
Bootstrapping the Sample Code We now have our application components in place, so let’s get them up and running inside a Spring container.
For the former we will use HSQL, a database that supports being run in-memory.
For the latter we will choose Hibernate as the persistence provider.
You can find the dependency setup in the pom.xml file of the sample project.
Second, we need to set up the Spring container to pick up our repository implementation and create a bean instance for it.
In Example 4-10, you see a Spring JavaConfig configuration class that will achieve the steps just described.
The @Configuration annotation declares the class as a Spring JavaConfig configuration class.
The methods annotated with @Bean now declare the following infrastructure components: dataSource() sets up an embedded data source using Spring’s embedded database support.
This allows you to easily set up various in-memory databases for testing purposes with almost no configuration effort.
We choose HSQL here (other options are H2 and Derby)
We use a new Spring 3.1 feature that allows us to completely abstain from creating a persistence.xml file to declare the entity classes.
The same configuration defined in XML looks something like Example 4-11
This is because the sample code is targeting the Spring Data JPA-based data access layer implementation, which renders some of the configuration just shown obsolete.
This will cause the infrastructure components declared in our ApplicationConfig configuration class and our annotated repository implementation to be discovered and instantiated.
Using Spring Data Repositories To enable the Spring data repositories, we must make the repository interfaces discoverable by the Spring Data repository infrastructure.
We do so by letting our Customer Repository extend the Spring Data Repository marker interface.
Beyond that, we keep the declared persistence methods we already have.
The only thing we now have to add to the Spring configuration is a way to activate the Spring Data repository infrastructure, which we can do in either XML or JavaConfig.
For the JavaConfig way of configuration, all you need to do is add the @EnableJpaRepo sitories annotation to your configuration class.
We remove the @ComponentScan annotation be removed for our sample because we don’t need to look up the manual implementation anymore.
The Spring Data repository infrastructure will automatically take care of the method calls to repositories taking part in transactions.
We’d probably still keep these annotations around were we building a more complete application.
We remove them for now to prevent giving the impression that they are necessary for the sole data access setup.
Finally, the header of the ApplicationConfig class looks something like Example 4-14
If you’re using XML configuration, add the repositories XML namespace element of the JPA namespace, as shown in Example 4-15
Let’s look at the individual methods declared in the repository and recap what Spring Data JPA is actually doing for each one of them.
So, solely by matching the method signatures, the calls to these two methods get routed to the implementation class.
If we wanted to expose a more complete set of CRUD methods, we might simply extend CrudRepository instead of Repository, as it contains these methods already.
Note how we actually prevent Customer instances from being deleted by not exposing the delete(…) methods that would have been exposed if we had extended CrudRepository.
As we haven’t manually declared any, the bootstrapping purpose of Spring Data JPA will inspect the method and try to derive a query from it.
Because the method returns a single Customer, the query execution expects the query to return at most one resulting entity.
First, we not only reference the description property of the product, but also qualify the predicate with the Containing keyword.
That will eventually lead to the given description parameter being surrounded by % characters, and the resulting String being.
Thus, the query is as follows: select p from Product p where p.description like ?1 with a given description of Apple bound as %Apple%
The second interesting thing is that we’re using the pagination abstraction to retrieve only a subset of the products matching the criteria.
We create a new PageRequest instance to ask for the very first page by specifying a page size of one with a descending order by the name of the Product.
We then simply hand that Pageable into the method and make sure we’ve got the iPad back, that we’re the first page, and that there are further pages available.
As you can see, the execution of the paging method retrieves the necessary metadata to find out how many items the query would have returned if we hadn’t applied pagination.
Without Spring Data, reading that metadata would require manually coding the extra query execution, which does a count projection based on the actual query.
We’d essentially like to look up all Products that have a custom attribute with a given value.
To manually define the query to be executed, we use the @Query annotation.
This also comes in handy if the queries get more complex in general.
Even if they were derivable, they’d result in awfully verbose method names.
Finally, let’s have a look at the OrderRepository (Example 4-19), which should already look remarkably familiar.
The query method findByCustomer(…) will trigger query derivation (as shown before) and result in select o from Order o where o.customer = ? SortingRepository, which in turn extends CrudRepository.
The main use case here is that we’d like to access all Orders page by page to avoid loading them all at once.
Transactionality Some of the CRUD operations that will be executed against the JPA EntityManager require a transaction to be active.
For a general introduction into Spring transactions, please consult the Spring reference documentation.
Enabling read-only transactions for reading methods results in a few optimizations: first, the flag is handed to the underlying JDBC driver which will—depending on your database vendor—result in optimizations or the driver even preventing you from accidentally executing modifying queries.
Beyond that, the Spring transaction infrastructure integrates with the life cycle of the EntityMan ager and can set the FlushMode for it to MANUAL, preventing it from checking each entity in the persistence context for changes (so-called dirty checking)
Especially with a large set of objects loaded into the persistence context, this can lead to a significant improvement in performance.
If you’d like to fine-tune the transaction configuration for some of the CRUD methods (e.g., to configure a particular timeout), you can do so by redeclaring the desired CRUD method and adding @Transactional with your setup of choice to the method declaration.
This will then take precedence over the default configuration declared in Sim pleJpaRepository.
Repository Querydsl Integration Now that we’ve seen how to add query methods to repository interfaces, let’s look at how we can use Querydsl to dynamically create predicates for entities and execute them via the repository abstraction.
Chapter 3 provides a general introduction to what Querydsl actually is and how it works.
To generate the metamodel classes, we have configured the Querydsl Maven plug-in in our pom.xml file, as shown in Example 4-21
It will cause the plug-in to consider JPA mapping annotations to discover entities, relationships to other entities, embeddables, etc.
The generation will be run during the normal build process and classes generated into a folder under target.
Thus, they will be cleaned up with each clean build, and don’t get checked into the source control system.
This will pull methods like findAll(Predicate predicate) and findOne(Predicate pred icate) into the API.
We now have everything in place, so we can actually start using the generated classes.
First, we obtain a reference to the QProduct metamodel class and keep that inside the product property.
We can now use this to navigate the generated path expressions to create predicates.
The second predicate we build specifies that we’d like to look up all products with a description containing tablet.
We then go on executing the Predicate instance against the repository and assert that we found exactly the iPad we looked up for reference before.
You see that the definition of the predicates is remarkably readable and concise.
The built predicates can be recombined to construct higher-level predicates and thus allow for querying flexibility without adding complexity.
Using JDBC is a popular choice for working with a relational database.
Most of Spring’s JDBC support is provided in the spring-jdbc module of the Spring Framework itself.
The Spring Data JDBC Extensions subproject of the Spring Data project does, however, provide some additional features that can be quite useful.
We will look at some recent developments around typesafe querying using Querydsl.
In addition to the Querydsl support, the Spring Data JDBC Extensions subproject contains some database-specific support like connection failover, message queuing, and improved stored procedure support for the Oracle database.
These features are limited to the Oracle database and are not of general interest, so we won’t be covering them in this book.
The Spring Data JDBC Extensions subproject does come with a detailed reference guide that covers these features if you are interested in exploring them further.
The Sample Project and Setup We have been using strings to define database queries in our Java programs for a long time, and as mentioned earlier this can be quite error-prone.
We might add a column or change the type of an existing one.
We are used to doing similar refactoring for our Java classes in our Java IDEs, and the IDE will guide us so we can find any references that need changing, including in comments and configuration files.
No such support is available for strings containing complex SQL query expressions.
To avoid this problem, we provide support for a type-safe query alternative in Querydsl.
Many data access technologies integrate well with Querydsl, and Chapter 3 provided some background on it.
In this section we will focus on the Querydsl SQL module and how it integrates with Spring’s JdbcTemplate usage, which should be familiar to every Spring developer.
Before we look at the new JDBC support, however, we need to discuss some general concerns like database configuration and project build system setup.
The HyperSQL Database We are using the HyperSQL database version 2.2.8 for our Querydsl examples in this chapter.
One nice feature of HyperSQL is that we can run the database in both server mode and in-memory.
The build scripts download the dependency and start the in-memory database automatically.
To use the database in standalone server mode, we need to download the distribution and unzip it to a directory on our system.
Once that is done, we can change to the hsqldb directory of the unzipped distribution and start the database using this command:
Running this command starts up the server, which generates some log output and a message that the server has started.
We are also told we can use Ctrl-C to stop the server.
For OS X or Linux, we can run the following command:
This should bring up the login dialog shown in Figure 5-1
The SQL Module of Querydsl The SQL module of Querydsl provides a type-safe option for the Java developer to work with relational databases.
Instead of writing SQL queries and embedding them in strings in your Java program, Querydsl generates query types based on metadata from your database tables.
You use these generated types to write your queries and perform CRUD operations against the database without having to resort to providing column or table names using strings.
The way you generate the query types is a bit different in the SQL module compared to other Querydsl modules.
Instead of relying on annotations, the SQL module relies on the actual database tables and available JDBC metadata for generating the query.
This means that you need to have the tables created and access to a live database before you run the query class generation.
For this reason, we recommend running this as a separate step of the build and saving the generated classes as part of the project in the source control system.
We need to rerun this step only when we have made some changes to our table structures and before we check in our code.
We expect the continuous integration system to run this code generation step as well, so any mismatch between the Java types and the database tables would be detected at build time.
We’ll take a look at what we need to generate the query types later, but first we need to understand what they contain and how we use them.
They contain information that Querydsl can use to generate queries, but they also contain information you can use to compose queries; perform updates, inserts, and deletes; and map data to domain objects.
Let’s take a quick look at an example of a table to hold address information.
The address table has three VARCHAR columns: street, city, and country.
Example 5-1 shows the SQL statement to create this table.
Example 5-2 demonstrates the generated query type based on this address table.
It has some constructors, Querydsl path expressions for the columns, methods to create primary and foreign key types, and a static field that provides an instance of the QAddress class.
In Example 5-3, we query for the street, city, and country for any address that has London as the city.
First, we create a reference to the query type and an instance of the correct SQLTem plates for the database we are using, which in our case is HSQLDBTemplates.
The SQLTem plates encapsulate the differences between databases and are similar to Hibernate’s Dialect.
We specify the table we are querying using the from method, passing in the query type.
Next, we provide the where clause or predicate via the where method, using the qAddress reference to specify the criteria that city should equal London.
Executing the SQLQuery, we use the list method, which will return a List of results.
We also provide a mapping implementation using a QBean, parameterized with the domain type and a projection consisting of the columns street, city, and country.
The result we get back is a List of Addresses, populated by the QBean.
Alternatively, you can use a MappingProjection, which is similar to Spring’s familiar RowMapper in that you have more control over how the results are mapped to the domain object.
Based on this brief example, let’s summarize the components of Querydsl that we used for our SQL query:
The SQLQueryImpl class , which will hold the target table or tables along with the predicate or where clause and possibly a join expression if we are querying multiple tables.
The Predicate, usually in the form of a BooleanExpression that lets us specify filters on the results.
The mapping or results extractor, usually in the form of a QBean or MappingProjec tion parameterized with one or more Expressions as the projection.
So far, we haven’t integrated with any Spring features, but the rest of the chapter covers this integration.
This first example is just intended to introduce the basics of the Querydsl SQL module.
Build System Integration The code for the Querydsl part of this chapter is located in the jdbc module of the sample GitHub project.
Before we can really start using Querydsl in our project, we need to configure our build system so that we can generate the query types.
Querydsl provides both Maven and Ant integration, documented in the “Querying SQL” chapter of the Querydsl reference documentation.
In our Maven pom.xml file, we add the plug-in configuration shown in Example 5-4
You can set the plug-in to execute as part of the generate-sources life cycle phase by specifying an execution goal.
We actually do this in the example project, and we also use a predefined HSQL database just to avoid forcing you to start up a live database when you build the example project.
For real work, though, you do need to have a database where you can modify the schema and rerun the Querydsl code generation.
The Database Schema Now that we have the build configured, we can generate the query classes, but let’s first review the database schema that we will be using for this section.
We already saw the address table, and we are now adding a customer table that has a one-to-many relationship with the address table.
We define the schema for our HSQLDB database as shown in Example 5-5
The two tables, customer and address, are linked by a foreign key reference from address to customer.
We also define a unique index on the email_address column of the address table.
This gives us the domain model implementation shown in Figure 5-2
The Domain Implementation of the Sample Project We have already seen the schema for the database, and now we will take a look at the corresponding Java domain classes we will be using for our examples.
We need a Customer class plus an Address class to hold the data from our database tables.
Both of these classes extend an AbstractEntity class that, in addition to equals(…) and hash Code(), has setters and getters for the id, which is a Long:
The Customer class has name and email information along with a set of addresses.
This implementation is a traditional JavaBean with getters and setters for all properties:
The email address is stored as a VARCHAR column in the database, but in the Java class we use an EmailAddress value object type that also provides validation of the email address using a regular expression.
This is the same class that we have seen in the other chapters:
The last domain class is the Address class, again a traditional JavaBean with setters and getters for the address properties.
In addition to the no-argument constructor, we have a constructor that takes all address properties:
We have a couple of finder methods and save and delete methods.
We don’t have any repository methods to save and delete the Address objects since they are always owned by the Customer instances.
We will have to persist any addresses provided when the Customer instance is saved.
It is a wrapper around a standard Spring JdbcTemplate that has methods for managing SQLQuery instances and executing queries as well as methods for executing inserts, updates, and deletes using command-specific callbacks.
We saw earlier that usually you need to provide a Connection and an SQLTemplates matching your database when you create an SQLQuery object.
However, when you use the QueryDslJdbcTem plate, there is no need to do this.
In usual Spring fashion, the JDBC layer will manage any database resources like connections and result sets.
It will also take care of providing the SQLTemplates instance based on database metadata from the managed connection.
There are two query methods: query returning a List and queryForObject returning a single result.
Both of these have three overloaded versions, each taking the following parameters:
One of the following combinations of a mapper and projection implementation:
They are responsible for extracting the data from the results returned by a query.
The ResultSetExtractor extracts data for all rows returned, while the RowMapper handles only one row at the time and will be called repeatedly, once for each row.
QBean and MappingProjection are Querydsl classes that also map one row at the time.
Which ones you use is entirely up to you; they all work equally well.
For most of our examples, we will be using the Spring types—this book is called Spring Data, after all.
We are writing a repository, so we start off with an @Repository annotation.
This is a standard Spring stereotype annotation, and it will make your component discoverable during classpath scanning.
In our case, we are using a template-based approach, and the template itself will provide this exception translation.
This is also a standard Spring annotation that indicates that any call to a method in this class should be wrapped in a database transaction.
As long as we provide a transaction manager implementation as part of our configuration, we don’t need to worry about starting and completing these transactions in our repository code.
We also define two references to the two query types that we have generated, QCusto mer and QAddress.
The constructor is annotated with @Autowired, which means that when the repository implementation is configured, the Spring container will inject the DataSource that has been defined in the application context.
The rest of the class comprises the methods from the CustomerRepository that we need to provide implementations for, so let’s get started.
Querying for a Single Object First, we will implement the findById method (Example 5-7)
The ID we are looking for is passed in as the only argument.
It never hurts to provide this optional attribute for read-only methods even if some JDBC drivers won’t make use of it.
That’s why we use the factory method newSqlQuery() to obtain an instance.
The SQLQuery class provides a fluent interface where the methods return the instance of the SQLQuery.
This makes it possible to string a number of methods together, which in turn makes it easier to read the code.
We specify the main table we are querying (the customer table) with the from method.
Then we add a left outer join against the address table using the leftJoin(…) method.
This will include any address rows that match the foreign key reference between address and customer.
If there are none, the address columns will be null in the returned results.
If there is more than one address, we will get multiple rows for each customer, one for each address row.
This is something we will have to handle in our mapping to the Java objects later on.
The last part of the SQLQuery is specifying the predicate using the where method and providing the criteria that the id column should equal the id parameter.
Remember that we mentioned earlier that since our query contained a leftJoin, we needed to handle potential multiple rows per Customer.
The method extractData is responsible for iterating over the ResultSet and extracting row data.
These methods are used when iterating over the result set to identify both the primary key and the foreign key so we can determine when there is a new root, and to help add the mapped child objects to the root object.
Next, we will take a look at the RowMapper implementations we are using.
The Implementations for the RowMappers The RowMapper implementations we are using are just what you would use with the regular JdbcTemplate.
They implement a method named mapRow with a ResultSet and the row number as parameters.
The only difference with using a QueryDslJdbcTem plate is that instead of accessing the columns with string literals, you use the query types to reference the column labels.
Since we implement the RowMappers as static inner classes, they have access to this private static method.
First, let’s look at the mapper for the Customer object.
As you can see in Example 5-11, we reference columns specified in the qCustomer reference to the QCustomer query type.
Next, we look at the mapper for the Address objects, using a qAddress reference to the QAddress query type (Example 5-12)
Insert, Update, and Delete Operations We will finish the CustomerRepository implementation by adding some insert, update, and delete capabilities in addition to the query features we just discussed.
With Querydsl, data is manipulated via operation-specific clauses like SQLInsertClause, SQLUpda teClause, and SQLDeleteClause.
We will cover how to use them with the QueryDslJdbc Template in this section.
Inserting with the SQLInsertClause When you want to insert some data into the database, Querydsl provides the SQLIn sertClause class.
Depending on whether your tables autogenerate the key or you provide the key explicitly, there are two different execute(…) methods.
For the case where the keys are autogenerated, you would use the executeWithKey(…) method.
This method will return the generated key so you can set that on your domain object.
When you provide the key, you instead use the execute method, which returns the number of affected rows.
We need to set the values using this SQLInsertClause and then call executeWithKey(…)
Updating with the SQLUpdateClause Performing an update operation is very similar to the insert except that we don’t have to worry about generated keys.
After setting the values for the update and specifying the where clause, we call execute on the SQLUpdateClause, which returns an update count.
This update count is also the value we need to return from this callback.
Deleting Rows with the SQLDeleteClause Deleting is even simpler than updating.
There’s no need to set any values herejust provide the where clause and call execute.
This chapter will introduce you to the Spring Data MongoDB project.
We will take a brief look at MongoDB as a document store and explain you how to set it up and configure it to be usable with our sample project.
A general overview of MongoDB concepts and the native Java driver API will round off the introduction.
After that, we’ll discuss the Spring Data MongoDB module’s features, the Spring namespace, how we model the domain and map it to the store, and how to read and write data using the MongoTemplate, the core store interaction API.
The chapter will conclude by discussing the implementation of a data access layer for our domain using the Spring Data repository abstraction.
MongoDB in a Nutshell MongoDB is a document data store.
Documents are structured data—basically mapsthat can have primitive values, collection values, or even nested documents as values for a given key.
MongoDB stores these documents in BSON, a binary derivative of JSON.
Thus, a sample document would look something like Example 6-1
As you can see, we have primitive String values for firstname and lastname.
The addresses field has an array value that in turn contains a nested address document.
Documents are organized in collections, which are arbitrary containers for a set of documents.
Usually, you will keep documents of the same type inside a single collection, where type essentially means “similarly structured.” From a Java point of view, this usually reads as a collection per type (one for Customers, one for Products) or type hierarchy (a single collection to hold Contacts, which can either be Persons or Companies)
Setting Up MongoDB To start working with MongoDB, you need to download it from the project’s website.
It provides binaries for Windows, OS X, Linux, and Solaris, as well as the sources.
The easiest way to get started is to just grab the binaries and unzip them to a reasonable folder on your hard disk, as shown in Example 6-2
To bootstrap MongoDB, you need to create a folder to contain the data and then start the mongod binary, pointing it to the just-created directory (see Example 6-3)
As you can see, MongoDB starts up, uses the given path to store the data, and is now waiting for connections.
Using the MongoDB Shell Let’s explore the very basic operations of MongoDB using its shell.
Switch to the directory in which you’ve just unzipped MongoDB and run the shell using the mongo binary, as shown in Example 6-4
The shell will connect to the locally running MongoDB instance.
You can now use the show dbs command to inspect all database, currently available on this instance.
In Example 6-5, we select the local database and issue a show collections command, which will not reveal anything at this point because our database is still empty.
We do so by using the save(…) command of a collection of our choice and piping the relevant data in JSON format to the function.
In Example 6-6, we add two customers, Dave and Carter.
The customers part of the command identifies the collection into which the data will go.
Collections will get created on the fly if they do not yet exist.
Carter without an email address, which shows that the documents can contain different sets of attributes.
MongoDB will not enforce a schema onto you by default.
The find(…) command actually can take a JSON document as input to create queries.
You can find out more about working with the MongoDB shell at the MongoDB home page.
Beyond that, [ChoDir10] is a great resource to dive deeper into the store’s internals and how to work with it in general.
The MongoDB Java Driver To access MongoDB from a Java program, you can use the Java driver provided and maintained by 10gen, the company behind MongoDB.
The core abstractions to interact with a store instance are Mongo, Database, and DBCollection.
The Mongo class abstracts the connection to a MongoDB instance.
Its default constructor will reach out to a locally running instance on subsequent operations.
As you can see in Example 6-8, the general API is pretty straightforward.
This appears to be classical infrastructure code that you’ll probably want to have managed by Spring to some degree, just like you use a DataSource abstraction when accessing a relational database.
Beyond that, instantiating the Mongo object or working with the DBCollection subsequently could throw exceptions, but they are MongoDB-specific and shouldn’t leak into client code.
Spring Data MongoDB will provide this basic integration into Spring through some infrastructure abstractions and a Spring namespace to ease the setup even more.
The core data abstraction of the driver is the DBObject interface alongside the Basic DBObject implementation class.
It can basically be used like a plain Java Map, as you can see in Example 6-9
First, we set up what will end up as the embedded address document.
We wrap it into a list, set up the basic customer document, and finally set the complex address property on it.
As you can see, this is very low-level interaction with the store’s data structure.
If we wanted to persist Java domain objects, we’d have to map them in and out of BasicDBObjects manually—for each and every class.
We will see how Spring Data MongoDB helps to improve that situation in a bit.
The just-created document can now be handed to the DBCollection object to be stored, as shown in Example 6-10
Setting Up the Infrastructure Using the Spring Namespace The first thing Spring Data MongoDB helps us do is set up the necessary infrastructure to interact with a MongoDB instance as Spring beans.
We have to implement two methods to set up the basic MongoDB infrastructure.
We provide a database name and a Mongo instance, which encapsulates the information about how to connect to the data store.
Right after that, we set the WriteConcern to be used to SAFE.
The WriteConcern defines how long the driver waits for the MongoDB server when doing write operations.
The default setting does not wait at all and doesn’t complain about network issues or data we’re attempting to write being illegal.
Setting the value to SAFE will cause exceptions to be thrown for network issues and makes the driver wait for the server to okay the written data.
It will also generate complaints about index constraints being violated, which will come in handy later.
The MongoDbFactory is in turn used by a MongoTemplate instance, which is also configured by the base class.
It is the central API to interact with the MongoDB instance, and persist and retrieve objects from the store.
Note that the configuration class you find in the sample project already contains extended configuration, which will be explained later.
The XML version of the previous configuration looks as follows like Example 6-12
The only difference here is that it also defaults the Mongo instance to be used to the one we had to configure manually in JavaConfig.
As we’d like to avoid that here, we set the WriteConcern to be used on the MongoTemplate directly.
This will cause all write operations invoked through the template to be executed with the configured concern.
The Mapping Subsystem To ease persisting objects, Spring Data MongoDB provides a mapping subsystem that can inspect domain classes for persistence metadata and automatically convert these objects into MongoDB DBObjects.
Let’s have a look at the way our domain model could be modeled and what metadata is necessary to tweak the object-document mapping to our needs.
The Domain Model First, we introduce a base class for all of our top-level documents, as shown in Example 6-13
It consists of an id property only and thus removes the need to repeat that property all over the classes that will end up as documents.
By default we consider properties named id or _id the ID field of the document.
Thus, the annotation comes in handy in case you’d like to use a different name for the property or simply to express a special purpose for it.
While we generally support any type to be used as id, there are a few types that allow special features to be applied to the document.
Generally, the recommended id type to end up in the persistent document is Objec tID.
ObjectIDs are value objects that allow for generating consistently growing ids even in a cluster environment.
Translating these recommendations into the Java driver world also implies it’s best to have an id of type ObjectID.
Unfortunately, this would create a dependency to the Mongo Java driver inside your domain objects, which might be something you’d like to avoid.
Because ObjectIDs are 12-byte binary values essentially, they can easily be converted into either String or BigInteger values.
So, if you’re using String, BigInteger, or Objec tID as id types, you can leverage MongoDB’s id autogeneration feature, which will automatically convert the id values into ObjectIDs before persisting and back when reading.
If you manually assign String values to your id fields that cannot be converted into an ObjectID, we will store them as is.
All other types used as id will also be stored this way.
Addresses and email addresses The Address domain class, shown in Example 6-14, couldn’t be simpler.
It’s a plain wrapper around three final primitive String values.
As you might have noticed, the Address class uses a complex constructor to prevent an object from being able to be set up in an invalid state.
In combination with the final fields, this makes up a classic example of a value object that is immutable.
An Address will never be changed, as changing a property forces a new Address instance to be created.
The class does not provide a no-argument constructor, which raises the question of how the object is being instantiated when the DBObject is read from the database and has to be turned into an Address instance.
Spring Data uses the concept of a so-called persistence constructor, the constructor being used to instantiate persisted objects.
Your class providing a no-argument constructor (either implicit or explicit) is the easy scenario.
The mapping subsystem will simply use that to instantiate the entity via reflection.
If you have a constructor taking arguments, it will try to match the parameter names against property names and eagerly pull values from the store representation—the DBObject in the case of MongoDB.
Another example of a domain concept embodied through a value object is an EmailAd dress (Example 6-16)
Value objects are an extremely powerful way to encapsulate business rules in code and make the code more expressive, readable, testable, and maintainable.
For a more in-depth discussion, refer to Dan Bergh-Johnsson’s talk on this topic, available on InfoQ.
If you carried an email address around in a plain String, you could never be sure whether it had been validated and actually represents a valid email address.
Thus, the plain wrapper class checks the given source value.
This way, clients can be sure they’re dealing with a proper email address if they get hold of an EmailAddress instance.
The value property is annotated with @Field, which allows for customizing the way a property is mapped to a field in a DBObject.
In our case, we map the rather generic value to a more specific email.
While we could have simply named the property email in the first place in our situation, this feature comes in handy in two major scenarios.
First, say you want to map classes onto existing documents that might have chosen field keys that you don’t want to let leak into your domain objects.
Field generally allows decoupling between field keys and property names.
Second, in contrast to the relational model, field keys are repeated for every document, so they can make up a large part of the document data, especially if the values you store are small.
So you could reduce the space required for keys by defining rather short ones to be used, with the trade-off of slightly reduced readability of the actual JSON representation.
Now that we’ve set the stage with our basic domain concept implementations, let’s have a look at the classes that actually will make up our documents.
The mapping subsystem would still be able to convert the class into a DBOb ject if the annotation were missing.
So why do we use it here? First, we can configure the mapping infrastructure to scan for domain classes to be persisted.
The second reason to use @Document is the ability to customize the MongoDB collection in which a domain object is stored.
If the annotation is not present at all or the collection attribute is not configured, the collection name will be the simple class name with the first letter lowercased.
So, for example, a Customer would go into the customer collection.
The code might look slightly different in the sample project, because we’re going to tweak the model slightly later to improve the mapping.
We’d like to keep it simple at this point to ease your introduction, so we will concentrate on general mapping aspects here.
The Customer class contains two primitive properties to capture first name and last name as well as a property of our EmailAddress domain class and a Set of Addresses.
The emailAddress property is annotated with @Field, which (as noted previously) allows us to customize the key to be used in the MongoDB document.
Note that we don’t actually need any annotations to configure the relationship between the Customer and EmailAddress and the Addresses.
This is mostly driven from the fact that MongoDB documents can contain complex values (i.e., nested documents)
This has quite severe implications for the class design and the persistence of the objects.
From a design point of view, the Customer becomes an aggregate root, in the domaindriven design terminology.
Addresses and EmailAddresses are never accessed individually but rather through a Customer instance.
We essentially model a tree structure here that maps nicely onto MongoDB’s document model.
This results in the object-todocument mapping being much less complicated than in an object-relational scenario.
From a persistence point of view, storing the entire Customer alongside its Addresses.
In a relational world, persisting this object would require an insert for each Address plus one for the Customer itself (assuming we’d inline the EmailAddress into a column of the table the Customer ends up in)
As the rows in the table are only loosely coupled to each other, we have to ensure the consistency of the insert by using a transaction mechanism.
Beyond that, the insert operations have to be ordered correctly to satisfy the foreign key relationships.
However, the document model not only has implications on the writing side of persistence operations, but also on the reading side, which usually makes up even more of the access operations for data.
Because the document is a self-contained entity in a collection, accessing it does not require reaching out into other collections, documents or the like.
Speaking in relational terms, a document is actually a set of prejoined data.
Especially if applications access data of a particular granularity (which is what is usually driving the class design to some degree), it hardly makes sense to tear apart the data on writes and rejoin it on each and every read.
A complete customer document would look something like Example 6-18
Note that modeling an email address as a value object requires it to be serialized as a nested object, which essentially duplicates the key and makes the document more complex than necessary.
Products The Product domain class (Example 6-19) again doesn’t contain any huge surprises.
The most interesting part probably is that Maps can be stored natively—once again due to the nature of the documents.
The attributes will be just added as a nested document with the Map entries being translated into document fields.
Note that currently, only Strings can be used as Map keys.
Orders and line items Moving to the order subsystem of our application, we should look first at the LineItem class, shown in Example 6-20
DBRef private Product product; private BigDecimal price; private int amount;
First we see two basic properties, price and amount, declared without further mapping annotations because they translate into document fields natively.
This will cause the Product object inside the LineItem to not be embedded.
Instead, there will be a pointer to a document in the collection that stores Products.
This is very close to a foreign key in the world of relational databases.
Note that when we’re storing a LineItem, the Product instance referenced has to be saved already—so currently, there’s no cascading of save operations available.
When we’re reading LineItems from the store, the reference to the Product will be resolved eagerly, causing the referenced document to be read and converted into a Product instance.
To round things off, the final bit we should have a look at is the Order domain class (Example 6-21)
The Customer is referenced using an @DBRef, as we’d rather point to one than embedding it into the document.
The Address properties and the LineItems are embedded as is.
Setting Up the Mapping Infrastructure As we’ve seen how the domain class is persisted, now let’s have a look at how we actually set up the mapping infrastructure to work for us.
In most cases this is pretty simple, and some of the components that use the infrastructure (and which we’ll introduce later) will fall back to reasonable default setups that generally enable the mapping subsystem to work as just described.
However, if you’d like to customize the setup, you’ll need to tweak the configuration slightly.
The former is actually responsible for building up the domain class metamodel to avoid reflection lookups (e.g., to detect the id property or determine the field key on each and every persistence operation)
The latter is actually performing the conversion using the mapping information provided by the MappingContext.
The latter is necessary to potentially load @DBRef annotated documents eagerly.
We then set up a Customer instance as well as a BasicDBObject and invoke the converter to do its work.
After that, the DBObject is populated with the data as expected.
We can tweak this by using the db-factory-ref attribute of the namespace element.
We point the base-package attribute to our project’s base package to pick up domain classes and build the persistence metadata at application context startup.
In Spring JavaConfig To ease the configuration when we’re working with Spring JavaConfig classes, Spring Data MongoDB ships with a configuration class that declares the necessary infrastructure components in a default setup and provides callback methods to allow us to tweak them as necessary.
To mimic the setup just shown, our configuration class would have to look like Example 6-24
This is not strictly necessary, as the mapping infrastructure will scan the package of the configuration class by default.
We just list it here to demonstrate how it could be reconfigured.
Indexing MongoDB supports indexes just as a relational store does.
You can configure the index programatically or by using a mapping annotation.
Because we’re usually going to retrieve Customers by their email addresses, we’d like to index on those.
We’d like to prevent duplicate email addresses in the system, so we set the unique flag to true.
This will cause MongoDB to prevent Customers from being created or updated with the same email address as another Customer.
We can define indexes including multiple properties by using the @CompoundIndex annotation on the domain class.
Index metadata will be discovered when the class is discovered by the MappingContext.
As the information is stored alongside the collection, to which the class gets persisted, it will get lost if you drop the collection.
Note that we expect a Dupli cateKeyException to be thrown, as we persist a second customer with the email address obtained from an already existing one.
Customizing Conversion The mapping subsystem provides a generic way to convert your Java objects into MongoDB DBObjects and vice versa.
However, you might want to manually implement a conversion of a given type.
For example, you’ve seen previously that the introduction of a value object to capture email addresses resulted in a nested document that you.
To recap the scenario, Example 6-26 shows where we’d like to start.
What we would actually like to end up with is a simpler document looking something like Example 6-27
Registering custom converters The just-implemented converters now have to be registered with the mapping subsystem.
Both the Spring XML namespace as well as the provided Spring JavaConfig configuration base class make this very easy.
In the JavaConfig world, the configuration base class provides a callback method for you to return an instance of CustomConversions.
This class is a wrapper around the Converter instances you hand it, which we can inspect later to configure the Mapping Context and MongoConverter appropriately, as well as the ConversionService to eventually perform the conversions.
In Example 6-30, we access the Converter instances by enabling component scanning and autowiring them into the configuration class to eventually wrap them into the CustomConversions instance.
MongoTemplate Now that we have both the general infrastructure in place and understand the way that object mapping works and can be configured, let’s continue with the API we provide to interact with the store.
As with all the other Spring Data modules, the core of the API is the MongoOperations interface, backed by a MongoTemplate implementation.
Template implementations in Spring serve two primary purposes: resource management and exception translation.
This means that MongoTemplate will take care of acquiring a connection through the configured MongoDbFactory and clean it up properly after the interaction with the store has ended or an exception has occurred.
Exceptions being thrown by MongoDB will be transparently converted into Spring’s DataAccessExcep tion hierarchy to prevent the clients from having to know about the persistence technology being used.
To illustrate the usage of the API, we will look at a repository implementation of the CustomerRepository interface (Example 6-32)
As you can see, we have a standard Spring component annotated with @Repository to make the class discoverable by classpath scanning.
We add the @Profile annotation to make sure it will be activated only if the configured Spring profile is activated.
This will prevent the class from leaking into the default bean setup, which we will use later when introducing the Spring Data repositories for MongoDB.
Low-level, callback-driven methods that allow you to interact with the MongoDB driver API in the even that the high-level operations don’t suffice for the functionality you need.
We simply use the save(…) method of the MongoOpera tions interface to hand it the provided Customer.
It will in turn convert the domain object into a MongoDB DBObject and save that using the MongoDB driver API.
The query(…) method is actually a static factory method of the Query class statically imported at the very top of the class declaration.
The same applies to the where(…) method, except it’s originating from Criteria.
As you can see, defining a query is remarkably simple.
Because it has been mapped to the email key by the @Field annotation, the property reference will be automatically translated into the correct field reference.
Also, we hand the plain EmailAddress object to the criteria to build the equality predicate.
It will also be transformed by the mapping subsystem before the query is actually applied.
Thus, the DBObject representing the query will look something like Example 6-33
Mongo Repositories As just described, the MongoOperations interface provides a decent API to implement a repository manually.
We’ll walk through the repository interface declarations of the sample project and see how invocations to the repository methods get handled.
Infrastructure Setup We activate the repository mechanism by using either a JavaConfig annotation (Example 6-34) or an XML namespace element.
It will set up the repository infrastructure to scan for repository interfaces in the package of the annotated configuration class by default.
We can alter this by configuring either the basePackage or basePackageClasses attributes of the annotation.
The XML equivalent looks very similar, except we have to configure the base package manually (Example 6-35)
Repositories in Detail For each of the repositories in the sample application, there is a corresponding integration test that we can run against a local MongoDB instance.
These tests interact with the repository and invoke the methods exposed.
With the log level set to DEBUG, you should be able to follow the actual discovery steps, query execution, etc.
Let’s start with CustomerRepository since it’s the most basic one.
The first thing to notice is that ProductRepository extends CrudRepository instead of the plain Repository marker interface.
This causes the CRUD methods to be pulled into our repository definition.
The difference from the former method is that this one additionally qualifies the.
This will cause the description parameter handed into the method call to be massaged into a regular expression to match descriptions that contain the given String as a substring.
The second thing worth noting here is the use of the pagination API (introduced in “Pagination and Sorting” on page 18)
Clients can hand in a Pageable to the method to restrict the results returned to a certain page with a given number and page size, as shown in Example 6-38
The returned Page then contains the results plus some metainformation, such as about how many pages there are in total.
As expected, the iPod dock is returned from the method call.
This way, the business logic could easily implement Product recommendations based on matching attribute pairs (connector plug and socket)
Last but not least, let’s have a look at the OrderRepository (Example 6-40)
Given that we already discussed two repository interfaces, the last one shouldn’t come with too many surprises.
The query method declared here is just a straightforward one using the query derivation mechanism.
What has changed compared to the previous repository interfaces is the base interface we extend from.
Mongo Querydsl Integration Now that we’ve seen how to add query methods to repository interfaces, let’s have a look at how we can use Querydsl to dynamically create predicates for entities and execute them via the repository abstraction.
Chapter 3 provides a general introduction to what Querydsl actually is and how it works.
If you’ve read “Repository Querydsl Integration” on page 51, you’ll see how remarkably similar the setup and usage of the API is, although we query a totally different store.
To generate the metamodel classes, we have configured the Querydsl Maven plug-in in our pom.xml file, as shown in Example 6-41
The only difference from the JPA approach is that we configure a MongoAnnotationPro cessor.
It will configure the APT processor to inspect the annotations provided by the Spring Data MongoDB mapping subsystem to generate the metamodel correctly.
Beyond that, we provide integration to let Querydsl consider our mappings—and thus potentially registered custom converters—when creating the MongoDB queries.
Again, the code is pretty much a 1:1 copy of the JPA code.
Graph Databases This chapter introduces an interesting kind of NoSQL store: graph databases.
Graph databases are clearly post-relational data stores, because they evolve several database concepts much further while keeping other attributes.
They provide the means of storing semistructured but highly connected data efficiently and allow us to query and traverse the linked data at a very high speed.
Graph data consists of nodes connected with directed and labeled relationships.
In property graphs, both nodes and relationships can hold arbitrary key/value pairs.
Graphs form an intricate network of those elements and encourage us to model domain and real-world data close to the original structure.
Unlike relational databases, which rely on fixed schemas to model data, graph databases are schema-free and put no constraints onto the data structure.
Relationships can be added and changed easily, because they are not part of a schema but rather part of the actual data.
We can attribute the high performance of graph databases to the fact that moving the cost of relating entities (joins) to the insertion time—by materializing the relationships as first-level citizens of the data structure—allows for constant time traversal from one entity (node) to another.
So, regardless of the dataset size, the time for a given traversal across the graph is always determined by the number of hops in that traversal, not the number of nodes and relationships in the graph as a whole.
In other database models, the cost of finding connections between two (or more) entities occurs on each query instead.
Thanks to this, a single graph can store many different domains, creating interesting connections between entities from all of them.
Secondary access or index structures can be integrated into the graph to allow special grouping or access paths to a number of nodes or subgraphs.
Due to the nature of graph databases, they don’t rely on aggregate bounds to manage atomic operations but instead build on the well-established transactional guarantees of an ACID (atomicity, consistency, isolation, durability) data store.
It is written predominantly in Java and leverages a custom storage format and the facilities of the Java Transaction Architecture (JTA) to provide XA transactions.
The Java API offers an object-oriented way of working with the nodes and relationships of the graph (show in the example)
Neo4j integrates a transactional, pluggable indexing subsystem that uses Lucene as the default.
The index is used primarily to locate starting points for traversals.
Example 7-1 lists the code for creating nodes and relationships with properties within transactional bounds.
With the declarative Cypher query language, Neo4j makes it easier to get started for everyone who knows SQL from working with relational databases.
Developers as well as operations and business users can run ad-hoc queries on the graph for a variety of use cases.
Cypher draws its inspiration from a variety of sources: SQL, SparQL, ASCIIArt, and functional programming.
The core concept is that the user describes the patterns to be matched in the graph and supplies starting points.
The database engine then efficiently matches the given patterns across the graph, enabling users to define sophisticated queries like “find me all the customers who have friends who have recently bought similar products.” Like other query languages, it supports filtering, grouping, and paging.
The Cypher statement in Example 7-2 shows a typical use case.
It starts by looking up a customer from an index and then following relationships via his orders to the products he ordered.
Filtering out older orders, the query then calculates the top 20 largest volumes he purchased by product.
Being written in Java, Neo4j is easily embeddable in any Java application which refers to single-instance deployments.
The Neo4j server is a simple download, and can be uncompressed and started directly.
In the web interface, you can see statistics about your database.
In the data browser, you can find nodes by ID, with index lookups, and with cypher queries (click the little blue question mark for syntax help), and switch to the graph visualizer with the righthand button to explore your graph visually (as shown in Figure 7-2)
The console allows you to enter Cypher statements directly or even issue HTTP requests.
Server Info lists JMX beans, which, especially in the Enterprise edition, come with much more information.
As an open source product, Neo4j has a very rich and active ecosystem of contributors, community members, and users.
Neo Technology, the company sponsoring the development of Neo4j, makes sure that the open source licensing (GPL) for the community edition, as well as the professional support for the enterprise editions, promote the continuous development of the product.
To access Neo4j, you have a variety of drivers available, most of them being maintained by the community.
There are libraries for many programming languages for both the embedded and the server deployment mode.
It was developed in close collaboration with VMware and Neo Technology and offers Spring developers an easy and familiar way to interact with Neo4j.
It intends to leverage the well-known annotation-based programming models with a tight integration in the Spring Framework ecosystem.
As in JPA, a few annotations on POJO (plain old Java object) entities and their fields provide the necessary metainformation for Spring Data Neo4j to map Java objects into graph elements.
Spring Data Neo4j allows us to store the type information (hierarchy) of the entities for performing advanced operations and type conversions.
Like the other Spring Data projects, Spring Data Neo4j is configured via two XML namespace elements—for general setup and repository configuration.
Two different mapping modes support the custom needs of developers.
In the simple mapping mode, the graph data is copied into domain objects, being detached from the graph.
The more advanced mapping mode leverages AspectJ to provide a live, connected representation of the graph elements bound to the domain objects.
To allow some more advanced graph operations, we’re going to normalize it further and add some additional relationships to enrich the model.
The code samples listed here are not complete but contain the necessary information for understanding the mapping concepts.
See the Neo4j project in the sample sourcerepository for a more complete picture.
Annotating the id is required in the simple mapping mode, as it is the only way to keep the node or relationship id stored in the entity.
It can contain any number of primitive fields, which will be treated as node properties.
Converters for Enum and Date fields come with the library.
In Country, both fields are just simple strings, as shown in Example 7-5
Customers are stored as nodes; their unique key is the emailAddress.
Here we meet the first references to other objects (in this case, Address), which are represented as relationships in the graph.
So fields of single references or collections of references always cause relationships to be created when updated, or navigated when accessed.
If we do not provide the type, it defaults to the field name.
Example 7-7 shows how the country reference field doesn’t have to be annotated—it just uses the field name as the relationship type for the outgoing relationship.
The customers connected to this address are not represented in the mapping because they are not necessary for our use case.
The Product has a unique name and shows the use of a nonprimitive field; the price will be converted to a primitive representation by Springs’ converter facilities.
You can register your own converters for custom types (e.g., value objects) in your application context.
The description field will be indexed by an index that allows full-text search.
We have to name the index explicitly, as it uses a different configuration than the default, exact index.
To enable interesting graph operations, we added a Tag entity and relate to it from the Product.
These tags can be used to find similar products, provide recommendations, or analyze buying behavior.
To handle dynamic attributes of an entity (a map of arbitrary key/values), there is a special support class in Spring Data Neo4j.
We decided against handling maps directly because they come with a lot of additional semantics that don’t fit in the context.
Currently, DynamicProperties are converted into properties of the node with prefixed names for separation.
The only unusual thing about the Tag is the Object value property.
This property is converted according to the runtime value into a primitive value that can be stored by Neo4j.
To allow for some more interesting graph operations we add a Rating relationship between a Customer and a Product.
Besides two simple fields holding the rating stars and a comment, we can see that it contains fields for the actual start and end of the relationship, which are annotated appropriately (Example 7-10)
In Example 7-11, we show with the Order how these entities can be retrieved as part of the mapping.
The Order is the most connected entity so far; it sits in the middle of our domain.
The easiest way to model the different types of addresses (shipping and billing) is to use different relationship types—in this case, we just rely on the different field names.
Please note that a single address object/node can be used in multiple places for example, as both the shipping and billing address of a single customer, or even across customers (e.g., for a family)
In practice, a graph is often much more normalized than a relational database, and the removal of duplication actually offers multiple benefits both in terms of storage and the ability to run more interesting queries.
The LineItems are not modeled as nodes but rather as relationships between Order and Product.
A LineItem has no identity of its own and just exists as long as both its endpoints exist, which it refers to via its order and product fields.
In this model, LineItem only contains the quantity attribute, but in other use cases, it can also contain different attributes.
The annotation on the lineItems field is similar to @RelatedTo in that it applies only to references to relationship entities.
It is possible to specify a custom relationship type or direction.
This takes us to one important aspect of object-graph mapping: fetch declarations.
For now we’ve kept things simple in Spring Data Neo4j by not fetching related entities by default.
Because the simple mapping mode needs to copy data out of the graph into objects, it must be careful about the fetch depth; otherwise you can easily end up with the whole graph pulled into memory, as graph structures are often cyclic.
That’s why the default strategy is to load related entities only in a shallow way.
This applies both to single relationships (one-to-one) and multi-relationship fields (one-to-many)
In the Order, the LineItems are fetched by default, becuse they are important in most cases when an order is loaded.
For the LineItem itself, the Product is eagerly fetched so it is directly available.
Depending on your use case, you would model it differently.
Now that we have created the domain classes, it’s time to store their data in the graph.
Persisting Domain Objects with Spring Data Neo4j Before we can start storing domain objects in the graph, we should set up the project.
The minimal Spring configuration is a single namespace config that also sets up the graph database (Example 7-13)
This even allows you to use an in-memory Impermanent GraphDatabase for testing.
Both the domain classes, as well as the dataset generation and integration tests documenting the use cases, can be found in the GitHub repository for the book (see “The Sample Code” on page 6 for details)
That depends on mapped IDs and possibly unique field declarations, which would be used to identify existing entities in the graph with which we're merging.
The entities shown here use some convenience methods for construction to provide a more readable setup (Figure 7-4)
It adds the usual benefits, like transaction handling and exception translation, but more importantly, automatic mapping from and to domain entities.
Most of the other mechanisms deal with more advanced ways to look up interesting things in the graph—by issuing index queries with lookup, executing Cypher statements with query(), or running a traversal with tra verse()
Combining Graph and Repository Power With all that set up, we can now look into how repositories integrate with Spring Data Neo4j and how they are used in a “graphy” way.
Spring Data Commons repositories (see Chapter 2) make it easy to keep persistence access related code (or rather noncode) in one place and allow us to write as little of it as possible to satisfy the specific use cases.
The basic setup for repositories is just another line of the namespace configuration, as shown in Example 7-16
Each domain class will be bound to an individual, concrete repository interface (see Example 7-17)
To understand how this mapping works, you need to be aware of the expressive syntax of Cypher, which is explained in the next sidebar, “Cypher Query Language”
But often data operations are better expressed declaratively by asking “what” than by specifying “how.” That’s why the Cypher query language was developed.
It builds upon matching patterns in a graph that are bound to specified nodes and relationships and allows further filtering and paging of the results.
Cypher has data manipulation features that allow us to modify the graph.
Cypher query parts can be chained (piped) to enable more advanced and powerful graph operations.
Each of the subgraphs found during the query execution spawns an individual result.
If aggregation functions are used, all nonaggregated values will be used as grouping values.
With CREATE [UNIQUE], SET, DELETE, the graph can be modified on the fly.
The results returned by Cypher are inherently tabular, much like JDBC ResultSets.
There is a Java DSL for Cypher that, instead of using semantic-free strings for queries, offers a type-safe API to build up Cypher queries.
It allows us to optionally leverage Querydsl (see Chapter 3) to build expressions for filters and index queries out of generated domain object literals.
With an existing JDBC driver, cypher queries can be easily integrated into existing Java (Spring) applications and other JDBC tools.
Basic Graph Repository Operations The basic operations provided by the repositories mimic those offered by the Neo4jTem plate, only bound to the declared repository domain class.
Spring Data Neo4j stores the type (hierarchy) information of the mapped entities in the graph.
It uses one of several strategies for this purpose, defaulting to an index-based storage.
This type information is used for all repository and template methods that operate on all instances of a type and for verification of requested types versus stored types.
The updating repository methods are transactional by default, so there is no need to declare a transaction around them.
For domain use cases, however, it is sensible to do so anyway, as usually more than one database operation is encapsulated by a business transaction.
Other provided repository interfaces offer methods for spatial queries or the Cypher-DSL integration.
Derived and Annotated Finder Methods Besides the previously discussed basic operations, Spring Data Neo4j repositories support custom finder methods by leveraging the Cypher query language.
For both annotated and derived finder methods, additional Pageable and Sort method parameters are taken into account during query execution.
There is support for an interface-based simple mapping of query results.
For mapping the results, we have to create an interface annotated with @MapResult.
In the interface we declare methods for retrieving each column-value.
To avoid the proliferation of query methods for different granularities, result types, and container classes, Spring Data Neo4j provides a small fluent API for result handling.
The core of the result handling API centers on converting an iterable result into different types using a configured or given ResultConverter, deciding on the granularity of the result size and optionally on the type of the target container.
They leverage the existing mapping information about the targeted domain entity and an intelligent parsing of the finder method name to generate a query that fetches the information needed.
Each of the property expressions either points to a property name of the current type or to another, related domain entity type and one of its properties.
If that is not the case, the repository creation fails early during ApplicationContext startup.
For all valid finder methods, the repository constructs an appropriate query by using the mapping information about domain entities.
Many aspects—like in-graph type representation, indexing information, field types, relationship types, and directionsare taken into account during the query construction.
This is also the point at which appropriate escaping takes place.
This example demonstrates the use of index lookups for indexed attributes and the simple property comparison.
If the method name refers to properties on other, related entities, then the query builder examines those entities for inclusion in the generated query.
The builder also adds the direction and type of the relationship to that entity.
If there are more properties further along the path, the same action is repeated.
Contains, StartsWith, EndsWith and Like are used for string comparison.
The Not prefix can be used to negate an expression.
For many of the typical query use cases, it is easy enough to just code a derived finder declaration in the repository interface and use it.
Only for more involved queries is an annotated query, traversal description, or manual traversing by following relationships necessary.
Advanced Graph Use Cases in the Example Domain Besides the ease of mapping real-world, connected data into the graph, using the graph data model allows you to work with your data in interesting ways.
By focusing on the value of relationships in your domain, you can find new insights and answers that are waiting to be revealed in the connections.
Multiple Roles for a Single Node Due to the schema-free nature of Neo4j, a single node or relationship is not limited to be mapped to a single domain class.
Sometimes it is sensible to structure your domain classes into smaller concepts/roles that are valid for a limited scope/context.
For example, an Order is used differently in different stages of its life cycle.
Depending on the current state, it is either a shopping cart, a customer order, a dispatch note, or a return order.
Each of those states is associated with different attributes, constraints, and operations.
Usually, this would have been modeled either in different entities stored in separate tables or in a single Order class stored in a very large and sparse table row.
With the schemaless nature of the graph database, the order will be stored in a node but only contains the state (and relationships) that are needed in the current state (and those still needed from past states)
Usually, it gains attributes and relationships during its life, and gets simplified and locked down only when being retired.
Spring Data Neo4j allows us to model such entities with different classes, each of which covers one period of the life cycle.
Those entities share a few attributes; each has some unique ones.
When the projected entity is stored, only its current attributes (and relationships) are updated; the other existing ones are left alone.
Product Categories and Tags as Examples for In-Graph Indexes For handling larger product catalogs and ease of exploration, it is important to be able to put products into categories.
A naive approach that uses a single category attribute with just one value per product falls short in terms of long-term usability.
In a graph, multiple connections to category nodes per entity are quite natural.
Adding a tree of categories, where each has relationships to its children and each product has relationships to the categories it belongs to, is really simple.
The same goes for tags, which are less restrictive than categories and often form a natural graph, with all the entities related to tags instead of a hierarchical tree like categories.
In a graph database, both multiple categories as well as tags form implicit secondary indexing structures that allow navigational access to the stored entities in many different ways.
There can be other secondary indexes (e.g., geoinformation, timerelated indices, or other interesting dimensions)
The Category forms a nested composite structure with parent-child relationships.
Each category has a unique name and a set of children.
The category objects are used for creating the structure and relating products to categories.
For leveraging the connectedness of the products, a custom (annotated) query navigates from a start (or root) category, via the next zero through five relationships, to the products connected to this subtree.
Leverage Similar Interests (Collaborative Filtering) Collaborative filtering, demonstrated in Example 7-23, relies on the assumption that we can find other “people” who are very similar/comparable to the current user in their interests or behavior.
The more information the algorithm gets, the better the results.
In the next step, the products that those similar people also bought or liked are taken into consideration (measured by the number of their mentions and/or their rating scores) optionally excluding the items that the user has already bought, owns, or is not interested in.
Recommendations Generally in all domains, but particularly in the ecommerce domain, making recommendations of interesting products for customers is key to leveraging the collected information on product reviews and buying behavior.
Obviously, we can derive recommendations from explicit customer reviews, especially if there is too little actual buying history or no connected user account.
For more advanced recommendations, we use algorithms that take multiple input data vectors into account (e.g., ratings, buying history, demographics, ad exposure, and geoinformation)
The query in Example 7-24 looks up a product and all the ratings by any customer and returns a single page of top-rated products (depending on the average rating)
As transaction management is configured by default, @Transactional annotations are all that’s needed to define transactional scopes.
Transactions are needed for all write operations to the graph database, but reads don’t need transactions.
It is possible to nest transactions, but nested transactions will just participate in the running parent transaction (like REQUIRED)
For the simple mapping mode, the life cycle is straightforward: a new entity is just a POJO instance until it has been stored to the graph, in which case it will keep the internal id of the element (node or relationship) in the @GraphId annotated field for later reattachment or merging.
Without the id set, it will be handled as a new entity and trigger the creation of a new graph element when saved.
Whenever entities are fetched in simple mapping mode from the graph, they are automatically detached.
The data is copied out of the graph and stored in the domain object instances.
An important aspect of using the simple mapping mode is the fetch depth.
As a precaution, the transaction fetches only the direct properties of an entity and doesn’t follow relationships by default when loading data.
To achieve a deeper fetch graph, we need to supply a @Fetch annotation on the fields that should be eagerly fetched.
For entities and fields not already fetched, the tem plate.fetch(…) method will load the data from the graph and update them in place.
Its main difference from the simple mapping mode is that it offers a live view of the graph projected into the domain objects.
So each field access will be intercepted and routed to the appropriate properties or relationships (for @RelatedTo[Via] fields)
This interception uses AspectJ under the hood to work its magic.
Fields are automatically read from the graph at any time, but for immediate writethrough the operation must happen inside of a transaction.
Because objects can be modified outside of a transaction, a life cycle of attached/detached objects has been established.
Objects loaded from the graph or just saved inside a transaction are attached; if an object is modified outside of a transaction or newly created, it is detached.
Changes to detached objects are stored in the object itself, and will only be reflected in the graph with the next save operation, causing the entity to become attached again.
This live view of the graph database allows for faster operation as well as “direct” manipulation of the graph.
Changes will be immediately visible to other graph operations like traversals, Cypher queries, or Neo4j Core API methods.
Because reads always happen against the live graph, all changes by other committed transactions are immediately visible.
Due to the immediate live reads from the graph database, the advanced mapping mode has no need of fetch handling and the @Fetch annotation.
You can easily use the highperformance, embeddable Java database with any JVM language, preferably with that language's individual idiomatic APIs/drivers.
Integrating the embedded database is as simple as adding the Neo4j libraries to your dependencies.
The Neo4j server module is a simple download or operating system package that is intended to be run as an independent service.
A comprehensive REST API offers programmatic access to the database functionality.
Please note that with the current implementation, not all calls are optimally transferred over the network API, so the server interaction for individual operations will be affected by network latency and bandwidth.
It is recommended to use remotely executed queries as much as possible to reduce that impact.
For instance, creating entities with immediate property population, both for conventional or unique entities, is more efficient with the RestAPI.
The next thing you should do is consider the data you are working with (or want to work with) and see how connected the entities are.
Look closely—you’ll see they’re often much more connected than you’d think at first glance.
Taking one of these domains, and putting it first on a whiteboard and then into a graph database, is your first step toward realizing the power behind these concepts.
For writing an application that uses the connected data, Spring Data Neo4j is an easy way to get started.
To learn how that process works for a complete web application, see [Hunger12] in the Bibliography, which is part of the reference documentation and the GitHub repository.
The tutorial is a comprehensive walkthrough of creating the social movie database cineasts.net, and explains data modeling, integration with external services, and the web layer.
Feel free to reach out at any time to the Springsource Forums, Stackoverflow, or the Neo4j Google Group for answers to your questions.
In this chapter, we’ll look at the support Spring Data offers for the key/value store Redis.
We’ll briefly look at how Redis manages data, show how to install and configure the server, and touch on how to interact with it from the command line.
Then we’ll look at how to connect to the server from Java and how the RedisTemplate organizes the multitude of operations we can perform on data stored in Redis.
Redis in a Nutshell Redis is an extremely high-performance, lightweight data store.
It provides key/value data access to persistent byte arrays, lists, sets, and hash data structures.
It supports atomic counters and also has an efficient topic-based pub/sub messaging functionality.
Redis is simple to install and run and is, above all, very, very fast at data access.
What it lacks in complex querying functionality (like that found in Riak or MongoDB), it makes up for in speed and efficiency.
Redis servers can also be clustered together to provide for very flexible deployment.
It’s easy to interact with Redis from the command line using the redis-cli binary that comes with the installation.
Setting Up Redis To start working with Redis, you’ll want to have a local installation.
Depending on your platform, the installation process ranges from easy to literally one command.
The easiest installation process, shown in Example 8-1, is on Mac OS X using Homebrew.
Other Unix systems are natively supported if you build the server from source.
Build instructions are on the Redis website, though they are identical to most other *NIX packages we’ve built—namely, unzip it, cd into that directory, and type make.
The download page for Redis also lists a couple of unofficial efforts to port Redis to the Win32/64 platform, though those are not considered production quality.
For the purposes of this chapter, we’ll stick to the *NIX version, where Redis is most at home.
Just so we can get a server running quickly and see some results, let’s run the server in a terminal, in the foreground.
This is good for debugging because it logs directly to the console to let you know what the server is doing internally.
Instructions on installing a boot script to get the server running when you restart your machine will, of course, vary by platform.
Setting that up is an exercise left to the reader.
We’re just going to use the default settings for the server, so starting it is simply a matter of executing redis-server, as in Example 8-2
Using the Redis Shell Redis comes with a very useful command-line shell that you can use interactively or from batch jobs.
We’ll just be using the interactive part of the shell so we can poke around inside the server, look at our data, and interact with it.
The command shell has an extensive help system (Example 8-3) so once you’re in there, hit the Tab key a couple of times to have the shell prompt you for help.
The Redis documentation is quite helpful here, as it gives a nice overview of all the commands available and shows you some example usage.
Keep this page handy because you’ll be referring back to it often.
It will pay dividends to spend some time familiarizing yourself with the basic SET and GET commands.
Let’s take a moment and play with inserting and retrieving data (Example 8-4)
Notice that we didn’t put quotes around the value 1 when we SET it.
Redis doesn’t have datatypes like other datastores, so it sees every value as a list of bytes.
In the command shell, you’ll see these printed as strings.
When we GET the value back out, we see "1" in the command shell.
We know by the quotes, then, that this is a string.
Which you choose doesn’t make any difference to your use of the Spring Data Redis library.
The differences between the drivers have been abstracted out into a common set of APIs and template-style helpers.
For the sake of simplicity, the example project uses the Jedis driver.
Since the feature set of Redis is really too large to effectively encapsulate into a single class, the various operations on data are split up into separate Operations classes as follows (names are self-explanatory):
Object Conversion Because Redis deals directly with byte arrays and doesn’t natively perform Object to byte[] translation, the Spring Data Redis project provides some helper classes to make it easier to read and write data from Java code.
By default, all keys and values are stored as serialized Java objects.
To influence how keys and values are serialized and deserialized, Spring Data Redis provides a RedisSerializer abstraction that is responsible for actually reading and writing the bytes stored in Redis.
There is already a built-in RedisSerializer for Strings, so to use Strings for keys and Longs for values, you would create a simple serializer for Longs, as shown in Example 8-7
To use these serializers to make it easy to do type conversion when working with Redis, set the keySerializer and valueSerializer properties of the template like in the snippet of JavaConfig code shown in Example 8-8
You’re now ready to start storing counts in Redis without worrying about byte[]-toLong conversion.
Since Redis supports such a large number of operations—which makes for a lot of methods on the helper classes—the methods for getting and setting values are defined in the various RedisOperations interfaces.
You can access each of these interfaces by calling the appropriate opsForX method on the RedisTemplate.
Since we’re only storing discrete values in this example, we’ll be using the ValueOperations template (Example 8-9)
Object Mapping It’s great to be able to store simple values like counters and strings in Redis, but it’s often necessary to store richer sets of related information.
In some cases, these might be properties of an object.
In other cases, they might be the keys and values of a hash.
Using the RedisSerializer, you can store an object into Redis as a single value.
But doing so won’t make the properties of that object very easy to inspect or retrieve individually.
What you probably want in that case is to use a Redis hash.
Since everything in Redis is a byte[], for this hash example we’re going to simpify by using Strings for keys and values.
The operations for hashes, like those for values, sets, and so on, are accessible from the RedisTemplate opsForHash() method.
Atomic Counters Many people choose to use Redis because of the atomic counters that it supports.
If multiple applications are all pointing at the same Redis instance, then those distributed applications can consistently and atomically increment a counter to ensure uniqueness.
Java already contains AtomicInteger and AtomicLong classes for atomically incrementing counters across threads, but that won’t help us if those counters are in other JVM processes or ClassLoaders.
Spring Data Redis implements a couple of helper classes similar to AtomicInteger and AtomicLong and backs them by a Redis instance.
Accessing distributed counters within your application is as easy as creating an instance of these helper classes and pointing them all to the same Redis server (Example 8-12)
Pub/Sub Functionality Another important benefit of using Redis is the simple and fast publish/subscribe functionality.
Although it doesn’t have the advanced features of a full-blown message broker, Redis’ pub/sub capability can be used to create a lightweight and flexible event bus.
Spring Data Redis exposes a couple of helper classes that make working with this functionality extremely easy.
You can define an optional second parameter, which will be the channel or pattern that triggered this invocation.
There is also a MessageListener interface to give your beans a solid contract to implement if you want to avoid the reflection-based invocation that’s done when passing in a POJO.
Our RedisSerializer will store the contents of this String as bytes.
As demonstrated in Example 8-16, it needs to be configured with a RedisCon nectionFactory and a set of listeners.
The container has life cycle methods on it that will be called by the Spring container if you create it inside an ApplicationContext.
If you create this container programmatically, you’ll need to call the afterProperties Set() and start() methods manually.
Remember to assign your listeners before you call the start() method, though, or your handlers will not be invoked since the wiring is done in the start() method.
Using Spring’s Cache Abstraction with Redis Spring 3.1 introduced a common and reusable caching abstraction.
This makes it easy to cache the results of method calls in your POJOs without having to explicitly manage the process of checking for the existence of a cache entry, loading new ones, and expiring old cache entries.
Spring 3.1 gives you some helpers that work with a variety of cache backends to perform these functions for you.
To designate Redis as the backend for using the caching annotations in Spring, you just need to define a RedisCacheManager bean in your ApplicationContext.
Then annotate your POJOs like you normally would, with @Cacheable on methods you want cached.
In this example, we’re letting the caching abstraction generate a unique integer for us to serve as the cache key.
There are lots of options for how the cache manager stores your results.
You can configure this behavior by placing the appropriate annotation on your @Cache able methods.
Using JDK serialization allows us to cache any Serializable Java object.
Spring Roo is a rapid application development tool for Java developers.
With Roo, you can easily build full Java applications in minutes.
We won’t be covering all aspects of Roo development in this chapter.
We will focus on the new repository support for JPA and MongoDB that uses Spring Data to provide this support.
If you want to read more about Roo, go to the Spring Roo project home page, where you can find links to the reference manual.
While on the project home page, look for a link to download a free O’Reilly ebook by Josh Long and Steve Mayzak called Getting Started with Roo [LongMay11]
This ebook covers an older 1.1 version of Roo that does not support the repository layer, but it is a good introduction to Roo in general.
A Brief Introduction to Roo Roo works its magic using code generation combined with AspectJ for injecting behavior into your domain and web classes.
When you work on a Roo project, the project files are monitored by Roo and additional artifacts are generated.
You still have your regular Java classes that you can edit, but there are additional features provided for free.
When you create a class with Roo and annotate that class with one or more annotations that provide additional capabilities, Roo will generate a corresponding AspectJ file that contains one or more AspectJ inter type declarations (ITD)
There is, for instance, an @RooJavaBean annotation that triggers the generation of an AspectJ aspect declaration that provides ITDs that introduce getters and setters in your Java class.
Let’s see a quick example of how that would look.
As you can see, we don’t code the getters and setters.
They will be introduced by the backing AspectJ aspect file since we used the @RooJavaBean annotation.
You may push code into the target .java compilation unit if you wish to edit any member(s)
You can see that this is defined as a privileged aspect, which means that it will have access to any private variables declared in the target class.
The way you would define ITDs is by preceding any method names with the target class name, separated by a dot.
As you can see, Roo follows a specific naming pattern that makes it easier to identify what files it has generated.
To work with Roo, you can either use a command-line shell or edit your source files directly.
Roo will synchronize all changes and maintain the source and generated files as necessary.
When you ask Roo to create a project for you, it generates a pom.xml file that is ready for you to use when you build the project with Maven.
In this pom.xml file, there is a Maven compile as well as an AspectJ plug-in defined.
This means that all the AspectJ aspects are woven at compile time.
In fact, nothing from Roo remains in the Java class files that your build generates.
Also, the Roo annotations are source-level retention only, so they will not be part of your class files.
You can, in fact, easily get rid of Roo if you so choose.
You have the option of pushing all of the code defined in the AspectJ files into the appropriate source files and removing any of these AspectJ files.
This is called push-in refactoring, and it will leave you with a pure Java solution, just as if you had written everything from scratch yourself.
It also was opinionated in terms of the data access layer.
Roo prescribed an active record data access style where each entity provides its finder, save, and delete methods.
Roo now allows you to choose between the default active record style and a repository-based persistence layer.
If you choose the repository approach, you have a choice between JPA and MongoDB as the persistence providers.
In addition to an optional repository layer, Roo now also lets you define a service layer on top of either the active record style or repository style persistence layer.
Quick Start You can use Roo either as a command-line tool or within an IDE, like the free Spring Tool Suite, that has built-in Roo support.
Another IDE that has support for Roo is IntelliJ IDEA, but we won’t be covering the support here.
Using Roo from the Command Line First, you need to download the latest Spring Roo distribution from the download page.
Once you have the file downloaded, unzip it somewhere on your system.
In the bin directory, there is a roo.sh shell script for Unix-style systems as well as a roo.bat.
When you want to create a Roo project, simply create a project directory and start Roo using the shell script or the batch file.
If you add the bin directory to your path, you can just use the command name to start Roo; otherwise, you will have to provide the fully qualified path.
Once Roo starts up, you are greeted with the following screen (we entered hint at the prompt to get the additional information):
For assistance press TAB or type "hint" then hit ENTER.
Before you can use many features of Roo, you need to start a new project.
To do this, type 'project' (without the quotes) and then hit TAB.
Your new project will then be created in the current working directory.
Note that Roo frequently allows the use of TAB, so press TAB regularly.
Once your project is created, type 'hint' and ENTER for the next suggestion.
We are now ready to create a project and start developing our application.
At any time, you can enter hint, and Roo will respond with some instruction on what to do next based on the current state of your application development.
To cut down on typing, Roo will attempt to complete the commands you enter whenever you hit the Tab key.
This opens a “Create a new Roo Project” dialog screen, as shown in Figure 9-3
Just fill in the “Project name” and “Top level package name,” and then select WAR as the packaging.
Click Next, and then click Finish on the next screen.
The project should now be created, and you should also see the Roo shell window, as shown in Figure 9-4
We will start with a customer service application based on the same domain model that we have seen in earlier chapters.
We will create a Customer class and an associated Address class, link them together, and create repositories and really basic data entry screens for them.
Since Roo’s repository support supports both JPA and MongoDB, using the Spring Data repository support, we will create one of each kind of application.
As you will see, they are almost identical, but there are a couple of differences that we will highlight.
Creating the Project If you are using Spring Tool Suite, then just follow the aforementioned instructions to create a new Spring Roo project.
On the “Create a new Roo Project” dialog screen, provide the following settings:
You now have created a new project, and we are ready to start developing the application.
From here on, the actions will be the same whether you are using the Roo shell from the command line or inside the Spring Tool Suite.
Setting Up JPA Persistence Setting up the JPA persistence configuration consists of selecting a JPA provider and a database.
We will use Hibernate together with HSQLDB for this example.
Remember that when entering these commands, you can always press the Tab key to get completion and suggestions for available options.
Creating the Entities Let’s create our entities, starting with the Address class:
Note that we specified --activeRecord false, which means that we will have to provide the CRUD functionality using a repository.
These annotations have corresponding AspectJ aspect declarations that you can find in the same directory as the Java class.
The EmailAddress is an embeddable class with a single value field.
We need to ask Roo to ignore the fact that value is a reserved word for some SQL databases.
We also provide a column name of email since that will be more descriptive for anyone inspecting the database table.
Using this embeddable in a field declarations, we specify it as an embedded field.
The last command creates a many-to-many relationship to the address table, allowing us to provide a number of addresses for each customer.
The most interesting part of this class is that it is @Embeddable and that we have defined the value property to be stored in a database column named email.
Defining the Repositories With the entities in place, we can now create the JPA repositories.
At this point we could also create a service layer, but since this is such a simple application, we’ll skip this step.
Creating the Web Layer Now we need some really simple web pages so we can enter and modify our customer and address data.
Roo doesn’t know how to map the EmailAddress class between the String representation used for web pages and the EmailAddress type used for persistence.
Running the Example Now we are ready to build and deploy this example.
For Spring Tool Suite, just drag the application to the tc server instance and start the server.
If you use the command line, simply exit the Roo shell and from the command line run the following Maven commands:
Our application is now complete, and we can add some addresses and then a customer or two.
We just won’t have the option of using the active record style for the persistence layer; we can only use the repositories.
Other than this difference, the process is very much the same as for a JPA solution.
Creating the Project If you are using Spring Tool Suite, then just follow the aforementioned instructions to create a new Spring Roo project.
On the “Create a new Roo Project” dialog screen, provide the following settings:
Change to this new directory and then start the Roo Shell as previously explained.
If you wish, you can provide a host, port, username, and password, but for a default local MongoDB installation the defaults work well.
Creating the Entities When creating the entities, we don’t have the option of using the active record style, so there is no need to provide an --activeRecord parameter to opt out of it.
Repositories are the default, and the only option for the persistence layer with MongoDB.
When we move on to the Customer class, the first thing you’ll notice that is different is that with MongoDB you don’t use an embeddable class.
With MongoDB, you just create a plain class and specify --rooAnnotations true to enable the @RooJavaBean support.
To use this class, you specify the field as other.
Other than these minor differences, the entity declaration is very similar to the JPA example:
Defining the Repositories We declare the MongoDB repositories the same way as the JPA repositories except for the mongo keyword:
Creating the Web Layer The web layer is exactly the same as for the JPA example:
Running the Example Now we are ready to build and deploy this example.
This is again exactly the same as the JPA example, except that we need to have MongoDB running on our system.
See Chapter 6 for instructions on how to install and run MongoDB.
For Spring Tool Suite, just drag the application to the tc server instance and start the server.
If you use the command line, simply exit the Roo shell and from the command line run the following Maven commands:
Our second example application is now complete, and we can add some addresses and then a customer or two.
When you are working with the Spring Data repository abstraction (see Chapter 2 for details), the repository interface managing an entity becomes the central point of access for it.
Using the Spring Data REST repository exporter project, you can now export (as the name suggests) these managed entities via a REST web service to easily interact with the data.
The exporter mechanism will transparently expose a resource per repository, map CRUD operations onto HTTP methods for that resource, and provide a means to execute query methods exposed on the repository interface.
What Is REST? Representational State Transfer (REST) is an architectural style initially described by Roy Fielding in his dissertation analyzing styles of network-based software architectures [Fielding00]
It is a generalization of the principles behind the HTTP protocol, from which it derives the following core concepts:
Resources Systems expose resources to other systems: an order, a customer, and so on.
Verbs Each resource can be accessed and manipulated though a well-defined set of verbs.
These verbs have dedicated semantics and have to be used according to those.
In HTTP the commonly used verbs are GET, PUT, POST, DELETE, HEAD, and OPTIONS, as well as the rather seldomly used (or even unused) ones, TRACE and CONNECT.
Not every resource has to support all of the verbs just listed, but it is required that you don’t set up special verbs per resource.
Representations A client never interacts with the resource directly but rather through representations of it.
Representations are defined through media types, which clearly identify the structure of the representation.
Hypermedia The representations of a resource usually contain links to point to other resources, which allows the client to navigate the system based on the resource state and the links provided.
This concept is described as Hypermedia as the Engine of Application State (HATEOAS)
Web services built based on these concepts have proven to be scalable, reliable, and evolvable.
That’s why REST web services are a ubiquitous means to integrate software systems.
The Sample Project The easiest way to run the sample app is from the command line using the Maven Jetty plug-in.
Jetty is a tiny servlet container capable of running Servlet 3.0 web applications.
The Maven plug-in will allow you to bootstrap the app from the module folder using the command shown in Example 10-1
The first thing to notice here is that we pipe a JVM argument, spring.pro files.active, into the execution.
This populates the in-memory database with some sample products, customers, and orders so that we actually have some data to interact with.
Maven now dumps some general activity information to the console.
It allows us to get rid of the XML-based way to configure web application infrastructure components and instead use an API.
The context will invoke the listener later, which will cause the Application Context to be bootstrapped in turn.
The ApplicationContext configured will now bootstrap an embedded database, the JPA infrastructure including a transaction manager, and enable the repositories eventually.
It registers quite a bit of Spring MVC infrastructure components and inspects the root application context for Spring Data repository instances.
It will expose HTTP resources for each of these repositories as long as they implement CrudRepository.
This is currently a limitation, which will be removed in later versions of the module.
We map the servlet to the servlet root so that the application will be available via http://localhost:8080 for now.
Interacting with the REST Exporter Now that we have bootstrapped the application, let’s see what we can actually do with it.
We will use the command-line tool curl to interact with the system, as it provides a convenient way to trigger HTTP requests and displays responses in a way that we can show here in the book nicely.
However, you can, of course, use any other client capable of triggering HTTP requests: command-line tools (like wget on Windows) or simply your web browser of choice.
Note that the latter will only allow you to trigger GET requests through the URL bar.
If you’d like to follow along with the more advanced requests (POST, PUT, DELETE), we recommend a browser plug-in like the Dev HTTP Client for Google Chrome.
Let’s trigger some requests to the application, as shown in Example 10-3
All we know right now is that we’ve deployed it to listen to http://localhost:8080, so let’s see what this resource actually provides.
The first thing to notice here is that we triggered the curl command with the -v flag.
This flag activates verbose output, listing all the request and response headers alongside the actual response data.
We see that the server returns data of the content type appli cation/json by default.
The actual response body contains a set of links we can follow to explore the application.
Each of the links provided is actually derived from a Spring Data repository available in the ApplicationContext.
The resource URIs are derived using that default as well.
To customize this behavior, you can annotate the repository interface with @RestResource, which allows you to explicitly define path (the part of the URI) as well as the rel (the relation type)
Links The representation of a link is usually derived from the link element defined in the Atom RFC.
It basically consists of two attributes: a relation type (rel) and a hypertext reference (href)
The former defines the actual semantics of the link (and thus has to be documented or standardized), whereas the latter is actually opaque to the client.
A client will usually inspect a link’s response body for relation types and follow the links with relation types it is interested in.
So basically the client knows it will find all orders behind links with a rel of order.
This structure results in decoupling of the client and the server, as the latter tells the client where to go.
This is especially useful in case a URI changes or the server actually wants to point the client to a different machine to load-balance requests.
Let’s move on to inspecting the products available in the system.
We know that the products are exposed via the relation type product; thus, we follow the link with that rel.
Accessing Products Example 10-4 demonstrates how to access all products available in the system.
Triggering the request to access all products returns a JSON representation containing two major fields.
The content field consists of a collection of all available products rendered directly into the response.
The individual elements contain the serialized properties of the Product class as well as an artificial links container.
This container carries a single link with a relation type of self.
The self type usually acts as a kind of identifier, as it points to the resource itself.
So we can access the iPad product directly by following the link with the relation type self inside its representation (see Example 10-5)
To update a product, you simply issue a PUT request to the resource providing the new content, as shown in Example 10-6
We set the HTTP method to PUT using the -X parameter and provide a Content-Type header to indicate we’re sending JSON.
We submit an updated price and name attribute provided through the -d parameter.
The server returns a 204 No Content to indicate that the request was successful.
Triggering another GET request to the product’s URI returns the updated content, as shown in Example 10-7
The JSON representation of the collection resource also contained a links attribute, which points us to a generic resource that will allow us to explore the query methods exposed by the repository.
The convention is using the relation type of the collection resource (in our case, product) extended by .search.
Let’s follow this link and see what searches we can actually execute—see Example 10-8
As you can see, the repository exporter exposes a resource for each of the query methods declared in the ProductRepository interface.
The relation type pattern is again based on the relation type of the resource extended by the query method name, but we can customize it using @RestResource on the query method.
Let’s search for products that have a connector plug, as shown in Example 10-10
Accessing Customers Now that we’ve seen how to navigate through the products available and how to execute the finder methods exposed by the repository interface, let’s switch gears and have a look at the customers registered in the system.
Let’s follow that link in Example 10-11 and see what customers we find.
We’re getting a 500 Server Error response, indicating that something went wrong with processing the request.
Your terminal output is probably even more verbose, but the important lines are listed in Example 10-11 right underneath the HTTP status code.
Jackson (the JSON marshalling technology used by Spring Data REST) seems to choke on serializing the EmailAddress value object.
This is due to the fact that we don’t expose any getters or setters, which Jackson uses to discover properties to be rendered into the response.
Actually, we don’t even want to render the EmailAddress as an embedded object but rather as a plain String value.
We can achieve this by customizing the rendering using the @JsonSerialize annotation provided by Jackson.
Well, it’s not that much better, but at least we seem to get one step further.
This time the Jackson renderer complains about the Address class exposing a copy property, which in turn causes a recursion.
The reason for this issue is that the getCopy() method of Address class follows the Java bean property semantics but is not a getter method in the classic sense.
Rather, it returns a copy of the Address object to allow us to easily create a clone of an Address instance and assign it to an Order to shield against changes to the Customer’s Address leaking into an already existing Order (see Example 4-7)
So we have two options here: we could either rename the method to not match the Java bean property convention or add an annotation to tell Jackson to simply ignore the.
We’ll choose the latter for now, as we don’t want to get into refactoring the client code.
Having made this change, let’s restart the server and invoke the request again (Example 10-14)
As you can see, the entities can now be rendered correctly.
We also find the expected links section to point us to the available query methods for customers.
What’s new, though, is that we have an additional page attribute set in the returned JSON.
It contains the current page number (number), the requested page size (size, defaulted to 20 here), the total number of pages available (totalPages), and the overall total number of elements available (totalElements)
These attributes appear due to the fact that our CustomerRepository extends PagingAnd SortingRepository and thus allows accessing all customers on a page-by-page basis.
This allows us to restrict the number of customers to be returned for the collection resource by using page and limit parameters when triggering the request.
As we have three customers present, let’s request an artificial page size of one customer, as shown in Example 10-15
Note how the metainformation provided alongside the single result changed.
The totalPages field now reflects three pages being available due to our selecting a page size of one.
Even better, the server indicates that we can navigate the customers to the next page by following the customer.next link.
It already includes the request parameters needed to request the second page, so the client doesn’t need to construct the URI manually.
Let’s follow that link and see how the metadata changes while navigating through the collection (Example 10-16)
If you’re working with a dedicated HTTP client, escaping is not necessary.
Besides the actual content returned, note how the number attribute reflects our move to page two.
Beyond that, the server detects that there is now a previous page available and offers to navigate to it through the customer.prev link.
Following the cus tomer.next link a second time would result in the next representation not listing the customer.next link anymore, as we have reached the end of the available pages.
Accessing Orders The final root link relation to explore is order.
As the name suggests, it allows us to access the Orders available in the system.
Let’s access the resource and see what gets returned by the server (Example 10-17)
The response contains a lot of well-known patterns we already have discussed: a link to point to the exposed query methods of the OrderRepository, and the nested content field, which contains serialized Order objects, inlined Address objects, and LineItems.
Also, we see the pagination metadata due to OrderRepository implementing PagingAnd SortingRepository.
The new thing to notice here is that the Customer instance held in the Order object is not inline but instead pointed to by a link.
This is because Customers are managed by a Spring Data repository.
Thus, they are exposed as subordinate resources of the Order to allow for manipulating the assignment.
Let’s follow that link and access the Customer who triggered the Order, as shown in Example 10-18
This call returns the detailed information of the linked Customer and provides two links.
What’s the difference here? The former represents the assignment of the Customer to the order.
We can alter this assignment by issuing a PUT request to the URI, and we could unassign it by triggering a DELETE request to it.
If the relationship is optional, a DELETE request will just work fine.
Assume we discovered that it’s actually not Dave who placed the Order initially, but Carter.
How do we update the assignment? First, the HTTP method of choice is PUT, as we already know the URI of the resource to manipulate.
It wouldn’t really make sense to put actual data to the server, since all we’d like to do is tell the server “this existing Customer is the issuer of the Order.” Because the Customer is identified through its URI, we’re going to PUT it to the server, setting the Content-Type request header to text/uri-list so that the server knows what we send.
Note that we get a 204 No Content from the server, indicating that it has accepted the request and completed the reassignment.
Apache Hadoop is an open source project that originated in Yahoo! as a central component in the development of a new web search engine.
Hadoop’s architecture is based on the architecture Google developed to implement its own closed source web search engine, as described in two research publications that you can find here and here.
The Hadoop architecture consists of two major components: a distributed filesystem and a distributed data processing engine that run on a large cluster of commodity servers.
The Hadoop Distributed File System (HDFS) is responsible for storing and replicating data reliably across the cluster.
Hadoop MapReduce is responsible for providing the programming model and runtime that is optimized to execute the code close to where the data is stored.
The colocation of code and data on the same physical node is one of the key techniques used to minimize the time required to process large amounts (up to petabytes) of data.
While Apache Hadoop originated out of a need to implement a web search engine, it is a general-purpose platform that can be used for a wide variety of large-scale data processing tasks.
The combination of open source software, low cost of commodity servers, and the real-world benefits that result from analyzing large amounts of new unstructured data sources (e.g., tweets, logfiles, telemetry) has positioned Hadoop to be a de facto standard for enterprises looking to implement big data solutions.
In this chapter, we start by introducing the “Hello world” application of Hadoop, wordcount.
The wordcount application is written using the Hadoop MapReduce API.
It reads text files as input and creates an output file with the total count of each word it has read.
We will first show how a Hadoop application is traditionally developed and executed using command-line tools and then show how this application can be developed as a standard Java application and configured using dependency injection.
To copy the input text files into HDFS and the resulting output file out of HDFS, we use Spring for Apache Hadoop’s HDFS scripting features.
Challenges Developing with Hadoop There are several challenges you will face when developing Hadoop applications.
The first challenge is the installation of a Hadoop cluster.
While outside the scope of this book, creating and managing a Hadoop cluster takes a significant investment of time, as well as expertise.
The good news is that many companies are actively working on this front, and offerings such as Amazon’s Elastic Map Reduce let you get your feet wet using Hadoop without significant upfront costs.
The second challenge is that, very often, developing a Hadoop application does not consist solely of writing a single MapReduce, Pig, or Hive job, but rather it requires you to develop a complete data processing pipeline.
Collecting the raw data from many remote machines or devices.
Data cleansing and transformation of the raw data in order to prepare it for analysis.
Selecting a framework and programming model to write data analysis jobs.
Coordinating the execution of many data analysis jobs (e.g., workflow)
Each individual job represents a step to create the final analysis results.
Spring for Apache Hadoop along with two other Spring projects, Spring Integration and Spring Batch, provide the basis for creating a complete data pipeline solution that has a consistent configuration and programming model.
While this topic is covered in Chapter 13, in this chapter we must start from the basics: how to interact with HDFS and MapReduce, which in itself provides its own set of challenges.
Command-line tools are currently promoted in Hadoop documentation and training classes as the primary way to interact with HDFS and execute data analysis jobs.
This feels like the logical equivalent of using SQL*Plus to interact with Oracle.
Using command-line tools can lead you down a path where your application becomes a loosely organized collection of bash, Perl, Python, or Ruby scripts.
Command-line tools also require you to create ad-hoc conventions to parameterize the application for different environments and to pass information from one processing step to another.
There should be an easy way to interact with Hadoop programmatically, as you would with any other filesystem or data access technology.
It builds upon the Spring Framework to provide structure when you are writing Hadoop applications.
This enables you to write Hadoop applications in the same style as you would write other Spring-based applications.
Hello World The classic introduction to programming with Hadoop MapReduce is the wordcount example.
This application counts the frequency of words in text files.
While this sounds simple to do using Unix command-line utilities such as sed, awk, or wc, what compels us to use Hadoop for this is how well this problem can scale up to match Hadoop’s distributed nature.
Unix command-line utilities can scale to many megabytes or perhaps a few gigabytes of data.
However, they are a single process and limited by the disk transfer rates of a single machine, which are on the order of 100 MB/s.
Reading a 1 TB file would take about two and a half hours.
Using Hadoop, you can scale up to hundreds of gigabytes, terabytes, or even petabytes of data by distributing the data across the HDFS cluster.
The MapReduce code that represents the logic to perform on the data is sent to the nodes where the data resides, executing close to the data in order to increase the I/O bandwidth and reduce latency of the overall job.
To join the partial results from each node together, a single node in the cluster is used to “Reduce” the partial results into a final set of data.
In the case of the wordcount example, the word counts accumulated on individual machines are combined into the final list of word frequencies.
The fun part of running wordcount is selecting some sample text to use as input.
While it was surely not the intention of the original authors, Project Gutenberg provides an easily accessible means of downloading large amounts of text in the form of public domain books.
Project Gutenberg is an effort to digitize the full text of public domain books and has over 39,000 books currently available.
You can browse the project website and download a few classic texts using wget.
Now we need to get this data into HDFS using a HDFS shell command.
Before running the shell command, we need an installation of Hadoop.
A good guide to setting up your own Hadoop cluster on a single machine is described in Michael Noll’s excellent online tutorial.
We invoke an HDFS shell command by calling the hadoop command located in the bin directory of the Hadoop distribution.
The Hadoop command-line argument dfs lets you work with HDFS and in turn is followed by traditional file commands and arguments, such as cat or chmod.
The command to copy files from the local filesystem into HDFS is copyFromLocal, as shown in Example 11-2
To check if the files were stored in HDFS, use the ls command, as shown in Example 11-3
To run the wordcount application, we use the example jar file that ships as part of the Hadoop distribution.
The arguments for the application is the name of the application to run—in this case, wordcount—followed by the HDFS input directory and output directory, as shown in Example 11-4
You can view the output in HDFS using the command in Example 11-5
Depending on how many input files you have, there may be more than one output file.
By default, output filenames follow the scheme shown in Example 11-5 with the last set of numbers incrementing for each additional file that is output.
To copy the results from HDFS onto the local filesystem, use the command in Example 11-6
If there are multiple output files in HDFS, the getmerge option merges them all into a single file when copying the data out of HDFS to the local filesystem.
Listing the file contents shows the words in alphabetical order followed by the number of times they appeared in the file.
Sample output of the wordcount application output is shown in Example 11-7
In the next section, we will peek under the covers to see what the sample application that is shipped as part of the Hadoop distribution is doing to submit a job to Hadoop.
This will help you understand what’s needed to develop and run your own application.
Hello World Revealed There are a few things going on behind the scenes that are important to know if you want to develop and run your own MapReduce applications and not just the ones that come out of the box.
An abbreviated version of ExampleDriver is shown in Example 11-8
The WordCount class also has a main method that gets invoked not directly by the JVM when starting, but when the driver method of ProgramDriver is invoked.
This gets at the heart of what you have to know in order to configure and execute your own MapReduce applications.
The Mapper and Reducer classes form the core of the logic to write when creating your own application.
While they have rather generic names, TokenizerMapper and IntSumReducer are static inner classes of the WordCount class and are responsible for counting the words and summing the total counts.
Since there are many out-of-the-box examples included in the Hadoop distribution, the ProgramDriver utility helps to specify which Hadoop job to run based off the first command-line argument.
You can also run the WordCount application as a standard Java main application without using the ProgramDriver utility.
A few minor modifications related to command-line argument handling are all that you need.
The modified Word Count class is shown in Example 11-11
A Maven build file for WordCount application is also provided; this lets you run the WordCount application as part of a regular Java application, independent of using the Hadoop command-line utility.
Using Maven to build your application and run a standard Java main application is the first step toward treating the development and deployment of Hadoop applications as regular Java applications, versus one that requires a separate Hadoop command-line utility to execute.
The Maven build uses the Appassembler plug-in to generate Unix and Windows start scripts and to collect all the required dependencies into a local lib directory that is used as the classpath by the generated scripts.
To rerun the previous WordCount example using the same output direction, we must first delete the existing files and directory, as Hadoop does not allow you to write to a preexisting directory.
The command rmr in the HDFS shell achieves this goal, as you can see in Example 11-12
To build the application, run Appassembler’s assemble target and then run the generated wordcount shell script (Example 11-13)
One difference to using the Hadoop command line is that the directories in HDFS need to be prefixed with the URL scheme hdfs:// along with the hostname and port of the namenode.
This is required because the Hadoop command line sets environment variables recognized by the Hadoop Configuration class that prepend this information to the paths passed in as command-line arguments.
The webhdfs scheme is very useful because it provides an HTTPbased protocol to communicate with HDFS that does not require the client (our application) and the HDFS server to use the exact same version (down to the minor point release) of the HDFS libraries.
Hello World Using Spring for Apache Hadoop If you have been reading straight through this chapter, you may well be wondering, what does all this have to do with Spring? In this section, we start to show the features that Spring for Apache Hadoop provides to help you structure, configure, and run Hadoop applications.
The first feature we will examine is how to use Spring to configure and run Hadoop jobs so that you can externalize application parameters in separate configuration files.
This lets your application easily adapt to running in different environments—such as development, QA, and production—without requiring any code changes.
Using Spring to configure and run Hadoop jobs lets you take advantage of Spring’s rich application configuration features, such as property placeholders, in the same way you use Spring to configure and run other applications.
The additional effort to set up a Spring application context might not seem worth it for such a simple application as wordcount, but it is rare that you’ll build such simple applications.
Applications will typically involve chaining together several HDFS operations and MapReduce jobs (or equivalent Pig and Hive scripts)
Also, as mentioned in “Challenges Developing with Hadoop” on page 176, there are many other development activities that you need to consider in creating a complete data pipeline solution.
Using Spring for Apache Hadoop as a basis for developing Hadoop applications gives us a foundation to build upon and to reuse components as our application complexity grows.
Let’s start by running the version of WordCount developed in the previous section inside of the Spring container.
We use the XML namespace for Hadoop to declare the location.
Some of the properties that were set on the job class programmatically in the previous example can be inferred from the class signature of the Mapper and Reducer.
There are many other job properties you can set that are similar to the Hadoop command-line options (e.g., combiner, input format, output format, and general job key/value properties)
Use XML schema autocompletion in Eclipse or another editor to see the various options available, and also refer to the Spring for Apache Hadoop reference documentation for more information.
Right now, the namenode location, input, and output paths are hardcoded.
We will extract them to an external property file shortly.
Similar to using the Hadoop command line to run a job, we don’t need to specify the URL scheme and namenode host and port when specifying the input and output paths.
The <configuration/> element defines the default URL scheme and namenode information.
To launch a MapReduce job when a Spring ApplicationContext is created, use the utility class JobRunner to reference one or more managed Job objects and set the run-atstartup attribute to true.
By default, the application looks in a well-known directory for the XML configuration file, but we can override this by providing a command-line argument that references another configuration location.
You can build and run the application just like in the previous sections, as shown in Example 11-16
Be sure to remove the output files in HDFS that were created from running wordcount in previous sections.
Now that the Hadoop job is a Spring-managed object, it can be injected into any other object managed by Spring.
For example, if we want to have the wordcount job launched in a web application, we can inject it into a Spring MVC controller, as shown in Example 11-17
This file is located in the src/main/resources directory so that it is made available on the classpath by the build script.
In a real QA environment, the location of the HDFS cluster, as well as the HDFS input and output paths, would be different.
To run the application from the command line for the QA environment, run the commands in Example 11-22
Notice how the shell environment variable (ENV) is set to qa.
As we have seen over the course of these examples, when rerunning a Hadoop application we always need to write out the results to an empty directory; otherwise, the Hadoop job will fail.
In the development cycle, it’s tedious to remember to do this before launching the application each time, in particular when inside the IDE.
Let’s see how we can use Spring’s HDFS scripting features to help us with this common task.
Scripting HDFS on the JVM When developing Hadoop applications, you will quite often need to interact with HDFS.
A common way to get started with it is to use the HDFS command-line shell.
For example, here’s how to get a list of the files in a directory:
While that is sufficient for getting started, when writing Hadoop applications you often need to perform more complex chains of filesystem operations.
For example, you might need to test if a directory exists; if it does, delete it, and copy in some new files.
As a Java developer, you might feel that adding this functionality into bash scripts is a step backward.
Surely there is a programmatic way to do this, right? Good news: there is.
The bad news is that the Hadoop filesystem API is not very easy to use.
It throws checked exceptions and requires us to construct Path instances for many of its method arguments, making the calling structure more verbose and awkward than needed.
In addition, the Hadoop filesystem API does not provide many of the higher-level methods available from the command line, such as test and chmod.
It provides a simple wrapper for Hadoop’s FileSystem class that accepts Strings instead of Path arguments.
More importantly, it provides an FsShell class that mimics the functionality in the.
Instead of printing out information to the console, FsShell methods return objects or collections that you can inspect and use programmatically.
The FsShell class was also designed to integrate with JVM-based scripting languages, allowing you to fall back to a scripting style interaction model but with the added power of using JRuby, Jython, or Groovy instead of bash.
Example 11-23 uses the FsShell from inside a Groovy script.
The <script/> element is used to create an instance of FsShell, which is implicitly passed into the Groovy script under the variable name fsh.
Additional options control when the script gets executed and how it is evaluated.
To reevaluate the script in case it was changed on the filesystem, set evalu ate="IF_MODIFIED" in the script tag’s element.
You can also parameterize the script that is executed and pass in variables that are resolved using Spring’s property placeholder functionality, as shown in Example 11-25
The distcp utility is used for copying large amounts of files within a single Hadoop cluster or between Hadoop clusters, and leverages Hadoop itself to execute the copy in a distributed manner.
To sequence these tasks, you can use the pre-action and post-action attributes of the JobRunner so that HDFS scription operations occur before and after the job submission.
Build and run the application using the commands shown in Example 11-30
Example 11-31 shows the JobRunner configured to execute several HDFS scripts before and after the execution of multiple Hadoop jobs.
While this approach is convenient for executing simple workflows, it only goes so far as your application becomes more complex.
Treating the chain of HDFS and job operations as a first class concern is where the Hadoop-specific extensions to Spring Batch come in handy.
Job Scheduling Applications often require tasks to be scheduled and executed.
The task could be sending an email or running a time-consuming end-of-day batch process.
Job schedulers are a category of software that provides this functionality.
As you might expect, there is a wide range of product offerings in this category, from the general-purpose Unix cron utility to sophisticated open source and commercial products such as Quartz, ControlM, and Autosys.
In this section, we will show how you can use Quartz and Spring’s TaskScheduler to schedule and execute Hadoop applications.
Scheduling MapReduce Jobs with a TaskScheduler In this example, we will add task scheduling functionality to the application developed in the previous section, which executed an HDFS script and the wordcount MapReduce job.
Spring’s TaskScheduler and Trigger interfaces provides the basis for scheduling tasks that will run at some point in the future.
We can use an XML namespace, in addition to an annotation-based programming model, to configure tasks to be scheduled with a trigger.
In the configuration shown in Example 11-32, we show the use of an XML namespace that will invoke a method on any Spring-managed object—in this case, the call method on the JobRunner instance.
The trigger is a cron expression that will fire every 30 seconds starting on the third second of the minute.
The default value of JobRunner's run-at-startup element is false so in this configuration the HDFS scripts and Hadoop jobs will not execute when the application starts.
Using this configuration allows the scheduler to be the only component in the system that is responsible for executing the scripts and jobs.
This sample application can be found in the directory hadoop/scheduling.
When running the application, we can see the (truncated) output shown in Example 11-33, where the timestamps correspond to those defined by the cron expression.
Output from the scheduled wordcount application removing existing input and output directories in HDFS...
That’s it! As you can see, the task namespace is simple to use, but it also has many features (which we will not cover) related to the thread pool policies or delegation to the CommonJ WorkManager.
The Spring Framework reference documentation describes these features in more detail.
Scheduling MapReduce Jobs with Quartz Quartz is a popular open source job scheduler that includes many advanced features such as clustering.
In this example, we replace the Spring TaskScheduler used in the previous example with Quartz.
The Quartz scheduler requires you to define a Job (aka a JobDetail), a Trigger, and a Scheduler, as shown in Example 11-34
Quartz’s JobDetail class encapsulates what code will execute when the trigger condition is satisfied.
Running the application gives similar output to the previous Spring Task Scheduler-based example.
While the MapReduce programming model is at the heart of Hadoop, it is low-level and as such becomes a unproductive way for developers to write complex analysis jobs.
To increase developer productivity, several higher-level languages and APIs have been created that abstract away the low-level details of the MapReduce programming model.
There are several choices available for writing data analysis jobs.
The Hive and Pig projects are popular choices that provide SQL-like and procedural data flow-like languages, respectively.
HBase is also a popular way to store and analyze data in HDFS.
It is a column-oriented database, and unlike MapReduce, provides random read and write access to data with low latency.
MapReduce jobs can read and write data in HBase’s table format, but data processing is often done via HBase’s own client API.
In this chapter, we will show how to use Spring for Apache Hadoop to write Java applications that use these Hadoop technologies.
Using Hive The previous chapter used the MapReduce API to analyze data stored in HDFS.
While counting the frequency of words is relatively straightforward with the MapReduce API, more complex analysis tasks don’t fit the MapReduce model as well and thus reduce developer productivity.
In response to this difficulty, Facebook developed Hive as a means to interact with Hadoop in a more declarative, SQL-like manner.
Hive provides a language called HiveQL to analyze data stored in HDFS, and it is easy to learn since it is similar to SQL.
Under the covers, HiveQL queries are translated into multiple jobs based on the MapReduce API.
Hive is now a top-level Apache project and is still heavily developed by Facebook.
While providing a deep understanding of Hive is beyond the scope of this book, the basic programming model is to create a Hive table schema that provides structure on top of the data stored in HDFS.
HiveQL queries are then pared by the Hive engine, translating them into MapReduce jobs in order to execute the queries.
HiveQL statements can be submitted to the Hive engine through the command line or through a component called the Hive Server, which provides access via JDBC, ODBC, or Thrift.
For more details on how to install, run, and develop with Hive and HiveQL, refer to the project website as well as the book Programming Hive (O’Reilly)
As with MapReduce jobs, Spring for Apache Hadoop aims to simplify Hive programming by removing the need to use command-line tools to develop and execute Hive applications.
Instead, Spring for Apache Hadoop makes it easy to write Java applications that connect to a Hive server (optionally embedded), create Hive Thrift clients, and use Spring’s rich JDBC support (JdbcTemplate) via the Hive JDBC driver.
Hello World As an introduction to using Hive, in this section we will perform a small analysis on the Unix password file using the Hive command line.
The goal of the analysis is to create a report on the number of users of a particular shell (e.g., bash or sh)
To install Hive, download it from the main Hive website.
After installing the Hive distribution, add its bin directory to your path.
Now, as shown in Example 12-1, we start the Hive command-line console to execute some HiveQL commands.
You can also put the HiveQL commands in a file and execute that from the command line (Example 12-2)
If you want to run a standalone server for use in the sample application, start Hive using the command line:
Another alternative, useful for development or to avoid having to run another server, is to bootstrap the Hive server in the same Java process that will run Hive client applications.
The Hadoop namespace makes embedding the Hive server a one-line configuration task, as shown in Example 12-3
You can change those values using the host and port attributes.
When the Applica tionContext is created, the Hive server is started automatically.
If you wish to override this behavior, set the auto-startup element to false.
Lastly, you can reference a specific Hadoop configuration object, allowing you to create multiple Hive servers that connect to different Hadoop clusters.
Calling the method getHiveClient on HiveClientFactory will return a new instance of the HiveClient.
This is a convenient pattern that Spring provides since the HiveClient is not a thread-safe class, so a new instance needs to be created inside methods that are shared across multiple threads.
Some of the other parameters that we can set on the HiveClient through the XML namespace are the connection timeout and a collection of scripts to execute once the client connects.
The sample code for this application is located in ./hadoop/hive.
Refer to the readme file in the sample's directory for more information on running the sample application.
The error handling is shown in this example to highlight the data access layer development best practice of avoiding throwing checked exceptions to the calling code.
The helper class HiveTemplate, which provides a number of benefits that can simplify the development of using Hive programmatically.
It translates the HiveClient’s checked exceptions and error codes into Spring’s portable DAO exception hierarchy.
This means that calling code does not have to be aware of Hive.
You can instead focus on executing HSQL and getting results.
To create a HiveTemplate, use the XML namespace and optionally pass in a reference to the name of the HiveClientFactory.
Example 12-8 is a minimal configuration for the use of a new implementation of PasswordRepository that uses the HiveTemplate.
This is a common implementation style of Spring template classes since it facilitates unit testing, as interfaces can be easily mocked or stubbed.
HiveTemplate's query methods also let you pass a reference to a script location using Spring’s Resource abstraction, which provides great flexibility for loading an InputStream from the classpath, a file, or over HTTP.
The query method's second argument is used to replace substitution variables in the script with values.
HiveTemplate also provides an execute callback method that will hand you a managed HiveClient instance.
As with other template classes in Spring, this will let you get at the lower-level API if any of the convenience methods do not meet your needs but you will still benefit from the template's exception, translation, and resource management features.
Spring for Apache Hadoop also provides a HiveRunner helper class that like the JobRun ner, lets you execute HDFS script operations before and after running a HiveQL script.
You can configure the runner using the XML namespace element <hive-runner/>
It returns a new connection for each call to the DataSource’s getConnection method.
That should be sufficient for most Hive JDBC applications, since the overhead of creating the connection is low compared to the length of time for executing the Hive operation.
If a connection pool is needed, it is easy to change the configuration to use Apache Commons DBCP or c3p0 connection pools.
JdbcTemplate brings a wide range of ResultSet to POJO mapping functionality as well as translating error codes into Spring’s portable DAO (data access object) exception hierarchy.
As of Hive 0.10, the JDBC driver supports generating meaningful error codes.
Transient exceptions indicate that the operation can be retried and will probably succeed, whereas a nontransient exception indicates that retrying the operation will not succeed.
An implementation of the PasswordRepository using JdbcTemplate is shown in Example 12-11
The structure of the configuration to run this analysis is similar to the one used previously to analyze the password file with the HiveTemplate.
The HiveQL script shown in Example 12-12 generates a file that contains the cumulative number of hits for each URL.
It also extracts the minimum and maximum hit numbers and a simple table that can be used to show the distribution of hits in a simple chart.
The hive-contrib.jar can be downloaded from Maven central or built directly from the source.
While we have parameterized the location of the hive-contrib.jar another option is to put a copy of the jar into the Hadoop library directory on all task tracker machines.
The sample code for this application is located in ./ hadoop/hive.
Refer to the readme file in the sample's directory for more information on running the application.
This gives us an indication of which URLs would benefit from being cached.
The sample application shows two ways to execute the Hive script, using the HiveTem plate and the HiveRunner.
The configuration for the HiveRunner is shown in Example 12-14
While the size of this dataset was very small and we could have analyzed it using Unix command-line utilities, using Hadoop lets us scale the analysis over very large sets of data.
Hadoop also lets us cheaply store the raw data so that we can redo the analysis without the information loss that would normally result from keeping summaries of historical data.
Using Pig Pig provides an alternative to writing MapReduce applications to analyze data stored in HDFS.
Pig applications are written in the Pig Latin language, a high-level data processing language that is more in the spirit of using sed or awk than the SQL-like language that Hive provides.
A Pig Latin script describes a sequence of steps, where each step performs a transformation on items of data in a collection.
A simple sequence of steps would be to load, filter, and save data, but more complex operation—such as joining two data items based on common values—are also available.
Pig can be extended by user-defined functions (UDFs) that encapsulate commonly used functionality such as algorithms or support for reading and writing well-known data formats such as Apache HTTPD logfiles.
A PigServer is responsible for translating Pig Latin scripts into multiple jobs based on the MapReduce API and executing them.
A common way to start developing a Pig Latin script is to use the interactive console that ships with Pig, called Grunt.
The first is the LOCAL mode, which works with data stored on the local filesystem and runs MapReduce jobs locally using an embedded version of Hadoop.
The second mode, MAPREDUCE, uses HDFS and runs MapReduce jobs on the Hadoop cluster.
By using the local filesystem, you can work on a small set of the data and develop your scripts iteratively.
When you are satisfied with your script’s functionality, you can easily switch to running the same script on the cluster over the full dataset.
As an alternative to using the interactive console or running Pig from the command line, you can embed the Pig in your application.
The PigServer class encapsulates how you can programmatically connect to Pig, execute scripts, and register functions.
Spring for Apache Hadoop makes it very easy to embed the PigServer in your application and to run Pig Latin scripts programmatically.
Since Pig Latin does not have control flow statements such as conditional branches (if-else) or loops, Java can be useful to fill in those gaps.
Using Pig programmatically also allows you to execute Pig scripts in response to event-driven activities using Spring Integration, or to take part in a larger workflow using Spring Batch.
To get familiar with Pig, we will first write a basic application to analyze the Unix password files using Pig’s command-line tools.
Then we show how you can use Spring for Apache Hadoop to develop Java applications that make use of Pig.
For more details on how to install, run, and develop with Pig and Pig Latin, refer to the project website as well as the book Programming Pig (O’Reilly)
Hello World As a Hello World exercise, we will perform a small analysis on the Unix password file.
The goal of the analysis is to create a report on the number of users of a particular shell (e.g., bash or sh)
Using familiar Unix utilities, you can easily see how many people are using the bash shell (Example 12-15)
To perform a similar analysis using Pig, we first load the /etc/password file into HDFS (Example 12-16)
To install Pig, download it from the main Pig website.
Now we start the Pig interactive console, Grunt, in LOCAL mode and execute some Pig Latin commands (Example 12-17)
Since the example dataset is small, all the results fit in a single tab-delimited file, as shown in Example 12-18
The general flow of the data transformations taking place in the Pig Latin script is as follows.
The first line loads the data from the HDFS file, /test/passwd, into the variable named passwd.
The LOAD command takes the location of the file in HDFS, as well as the format by which the lines in the file should be broken up, in order to create a dataset (aka, a Pig relation)
In this example, we are using the PigStorage function to load the text file and separate the fields based on a colon character.
Pig can apply a schema to the columns that are in the file by defining a name and a data type to each column.
With the input dataset assigned to the variable passwd, we can now perform operations on the dataset to transform it into other derived datasets.
The grouped_by_shell dataset will have the shell name as the key and a collection of all records in the passwd dataset that have that shell value.
The key is the shell name and the value is a collection, or bag, of password records that have the same key.
The new dataset that is created will group together all the records with the same key and count them.
We can also parameterize the script to avoid hardcoding values—for example, the input and output locations.
To run this script in the interactive console, use the run command, as shown in Example 12-21
Or, to run the script directly in the command line, see Example 12-22
Running a PigServer Now we’ll shift to using a more structured and programmatic way of running Pig scripts.
Spring for Apache Hadoop makes it easy to declaratively configure and create a PigServer in a Java application, much like the Grunt shell does under the covers.
Spring’s XML namespace for Hadoop makes it very easy to create and configure a PigServer.
As Example 12-23 demonstrates, this configuration is done in the same way as the rest of your application configuration.
The location of the optional Pig initialization script can be any Spring resource URL, located on the filesystem, classpath, HDFS, or HTTP.
Some of the other attributes on the <pig-factory/> namespace are properties-loca tion, which references a properties file to configure properties of the PigServer; jobtracker, which sets the location of the job tracker used to a value different than that used in the Hadoop configuration; and job-name, which sets the root name of the MapReduce jobs created by Pig so they can be easily identified as belonging to this script.
Since the PigServer class is not a thread-safe object and there is state created after each execution that needs to be cleaned up, the <pig-factory/> namespace creates an instance of a PigServerFactory so that you can easily create new PigServer instances as needed.
Similar in purpose to JobRunner and HiveRunner, the PigRunner helper class to provide a convenient way to repeatedly execute Pig jobs and also execute HDFS scripts before and after their execution.
The configuration of the PigRunner is shown in Example 12-25
We set the run-at-startup element to true, enabling the Pig script to be executed when the Spring ApplicationContext is started (the default is false)
To run the sample password analysis application, run the commands shown in Example 12-26
Since the PigServerFactory and PigRunner classes are Spring-managed objects, they can also be injected into any other object managed by Spring.
It is often convenient to inject the PigRunner helper class to ensure that a new instance of the PigServer is created for each execution of the script and that its resources used are cleaned up after execution.
Controlling Runtime Script Execution To have more runtime control over what Pig scripts are executed and the arguments passed into them, we can use the PigTemplate class.
As with other template classes in Spring, PigTemplate manages the underlying resources on your behalf, is thread-safe once configured, and will translate Pig errors and exceptions into Spring’s portable DAO exception hierarchy.
Spring’s DAO exception hierarchy also helps to separate out nontransient and transient exceptions.
In the case of a transient exception being thrown, the failed operation might be able to succeed if it is retried again.
Using retry advice on a data access layer via Spring’s AOP (aspect-oriented programming) support is one way to implement this functionality.
Since Spring’s JDBC helper classes also perform the same exception translation, exceptions thrown in any Hive-based data access that uses Spring’s JDBC support will also map into the DAO hierarchy.
While switching between Hive and Pig is not a trivial task, since analysis scripts need to be rewritten, you can at least insulate calling code from the differences in implementation between a Hive-based and a Pig-based data access layer.
It will also allow you to more easily mix calls to Hive- and Pig-based data access classes and handle errors in a consistent way.
For example, you may want to select a group of input files based on a complex set of criteria that cannot be specified inside Pig Latin or a Pig user-defined function.
This is a common implementation style of Spring template classes since it facilitates unit testing, as interfaces can be easily mocked or stubbed.
To run an application that uses the PigPass wordRepository, use the commands in Example 12-30
The essential pieces of code that are executed by this application are shown in Example 12-31
Whether the service activator is executed asynchronously or synchronously depends on the type of input channel used.
If it is a DirectChannel (the default), then it will be executed synchronously; if it is an ExecutorChannel, it will be executed asynchronously, delegating the execution to a TaskExecutor.
The service activator class, PasswordSer vice, is shown in Example 12-33
The process method’s argument will be taken from the header of the Spring Integration message.
The value of the method argument is the value associated with the key hdfs_path in the message header.
The examples in this section show that there is a steady progression from creating a simple Pig-based application that runs a script, to executing scripts with runtime parameter substitution, to executing scripts within a Spring Integration data pipeline.
The section “Hadoop Workflows” on page 238 will show you how to orchestrate the execution of a Pig script inside a larger collection of steps using Spring Batch.
Apache Logfile Analysis Using Pig Next, we will perform a simple analysis on an Apache HTTPD logfile and show the use of a custom loader for Apache logfiles.
As you can see in Example 12-34, the structure of the configuration to run this analysis is similar to the one used previously to analyze the password file.
The Pig script generates a file that contains the cumulative number of hits for each URL and is intended to be a starting point for a more comprehensive analysis.
The script also extracts minimum and maximum hit numbers and a simple table that can be used to show the distribution of hits in a simple chart.
Pig makes it easy to filter and preprocess the data on a variety of criteria—for example, retain only GET requests that were successful and remove GET requests for images.
The arguments to the Pig script specify the location of a jar file that contains the custom loader to read and parse Apache logfiles and the location of the input and output paths.
To parse the Apache HTTPD logfiles, we will use a custom loader provided as part of the Pig distribution.
It is distributed as source code as part of the Piggybank project.
The compiled Piggybank jar file is provided in the sample application’s lib directory.
It models data as tables, which are then stored in HDFS and can scale to support tables with billions of rows and millions of columns.
Unlike MapReduce, HBase provides near real-time key-based access to data, and therefore can be used in interactive, non-batchbased applications.
The HBase data model consists of a table identified by a unique key with associated columns.
These columns are grouped together into column families so that data that is often accessed together can be stored together on disk to increase I/O performance.
The data stored in a column is a collection of key/value pairs, not a single value as is commonly the case in a relational database.
A schema is used to describe the column families and needs to be defined in advance, but the collection of key/value pairs stored as values does not.
This gives the system a great amount of flexibility to evolve.
There are many more details to the data model, which you must thoroughly understand in order to use HBase effectively.
The book HBase: The Definitive Guide (O’Reilly) is an excellent reference to HBase and goes into great detail about the data model, architecture, API, and administration of HBase.
Spring for Apache Hadoop provides some basic, but quite handy, support for developing HBase applications, allowing you to easily configure your connection to HBase and provide thread-safe data access to HBase tables, as well as a lightweight object-tocolumn data mapping functionality.
After installing the distribution, you start the HBase server by executing the start-hbase.sh script in the bin directory.
As with Pig and Hive, HBase comes with an interactive console, which you can start by executing the command hbase shell in the bin directory.
Once inside the interactive console, you can start to create tables, define column families, and add rows of data to a specific column family.
Example 12-37 demonstrates creating a user table with two column families, inserting some sample data, retrieving data by key, and deleting a row.
The two column families created are named cfInfo and cfStatus.
The key names, called qualifiers in HBase, that are stored in the cfInfo column family are the username, email, and password.
The cfStatus column family stores other information that we do not frequently need to access, along with the data stored in the cfInfo column family.
As an example, we place the status of the email address validation process in the cfSta tus column family, but other data—such as whether the user has participated in any online surveys—is also a candidate for inclusion.
The deleteall command deletes all data for the specified table and row.
The Java client is what we will use in this section but REST, Thrift, and Avro clients are also available.
The HTable class is the main way in Java to interact with HBase.
It allows you to put data into a table using a Put class, get data by key using a Get class, and delete data using a Delete class.
You query that data using a Scan class, which lets you specify key ranges as well as filter criteria.
The HBase API requires that you work with the data as byte arrays and not other primitive types.
The HTable class is also not thread safe, and requires you to carefully manage the underlying resources it uses and catch HBase-specific exceptions.
Spring’s HBaseTemplate class provides a higher-level abstraction for interacting with HBase.
As with other Spring template classes, it is thread-safe once created and provides exception translation into Spring’s portable data access exception hierarchy.
Similar to JdbcTem plate, it provides several callback interfaces, such as TableCallback, RowMapper, and ResultsExtractor, that let you encapsulate commonly used functionality, such as mapping HBase result objects to POJOs.
The TableCallback callback interface provides the foundation for the functionality of HBaseTemplate.
It performs the table lookup, applies configuration settings (such as when to flush data), closes the table, and translates any thrown exceptions into Spring’s DAO exception hierarchy.
The RowMapper callback interface is used to map one row from the HBase query ResultScanner into a POJO.
HBaseTemplate has several overloaded find methods that take additional criteria and that automatically loop over HBase’s ResultScanner “result set” object, converting each row to a POJO, and return a list of mapped objects.
For more control over the mapping process—for example, when one row does not directly map onto one POJO—the ResultsExtractor interface hands you the ResultScanner object so you can perform the iterative result set processing yourself.
To create and configure the HBaseTemplate, create a HBaseConfiguration object and pass it to HBaseTemplate.
Configuring a HBaseTemplate using Spring’s Hadoop XML namespace is demonstrated in Example 12-39, but it is also easy to achieve programmatically in pure Java code.
In this example, we used an anonymous inner class to implement the TableCallback and RowMapper interfaces, but creating standalone classes is a common implementation strategy that lets you reuse mapping logic across various parts of your application.
While you can develop far more functionality with HBase to make it as feature-rich as Spring’s MongoDB support, we’ve seen that the basic plumbing for interacting with HBase available with Spring Hadoop at the time of this writing simplifies HBase application development.
In addition, HBase allows for a consistent configuration and programming model that you can further use and extend across Spring Data and other Spring-related projects.
The goal of Spring for Apache Hadoop is to simplify the development of Hadoop applications.
Hadoop applications involve much more than just executing a single MapReduce job and moving a few files into and out of HDFS as in the wordcount example.
There is a wide range of functionality needed to create a real-world Hadoop application.
This includes collecting event-driven data, writing data analysis jobs using programming languages such as Pig, scheduling, chaining together multiple analysis jobs, and moving large amounts of data between HDFS and other systems such as databases and traditional filesystems.
Spring Integration provides the foundation to coordinate event-driven activities—for example, the shipping of logfiles, processing of event streams, real-time analysis, or triggering the execution of batch data analysis jobs.
Spring Batch provides the framework to coordinate coarse-grained steps in a workflow, both Hadoop-based steps and those outside of Hadoop.
Spring Batch also provides efficient data processing capabilities to move data into and out of HDFS from diverse sources such as flat files, relational databases, or NoSQL databases.
Spring for Apache Hadoop in conjunction with Spring Integration and Spring Batch provides a comprehensive and consistent programming model that can be used to implement Hadoop applications that span this wide range of functionality.
Another product, Splunk, also requires a wide range of functionality to create real-world big data pipeline solutions.
Spring’s support for Splunk helps you to create complex Splunk applications and opens the door for solutions that mix these two technologies.
Collecting and Loading Data into HDFS The examples demonstrated until now have relied on a fixed set of data files existing in a local directory that get copied into HDFS.
In practice, files that are to be loaded into HDFS are continuously generated by another process, such as a web server.
It is also common that logfiles are being continuously created on remote machines, such as a web farm, and need to be transferred to a separate machine and loaded into HDFS.
We can implement these use cases by using Spring Integration in combination with Spring for Apache Hadoop.
In this section, we will provide a brief introduction to Spring Integration and then implement an application for each of the use cases just described.
In addition, we will show how Spring Integration can be used to process and load into HDFS data that comes from an event stream.
Lastly, we will show the features available in Spring Integration that enable rich runtime management of these applications through JMX (Java management extensions) and over HTTP.
These patterns provide the key building blocks to develop integration applications that tie new and existing system together.
The patterns are based upon a messaging model in which messages are exchanged within an application as well as between external systems.
Adopting a messaging model brings many benefits, such as the logical decoupling between components as well as physical decoupling; the consumer of messages does not need to be directly aware of the producer.
This decoupling makes it easier to build integration applications, as they can be developed by assembling individual building blocks together.
The messaging model also makes it easier to test the application, since individual blocks can be tested first in isolation from other components.
This allows bugs to be found earlier in the development process rather than later during distributed system testing, where tracking down the root cause of a failure can be very difficult.
The key building blocks of a Spring Integration application and how they relate to each other is shown in Figure 13-1
Endpoints are producers or consumers of messages that are connected through channels.
Messages are a simple data structure that contains key/value pairs in a header and an arbitrary object type for the payload.
Endpoints can be adapters that communicate with external systems such as email, FTP, TCP, JMS, RabbitMQ, or syslog, but can also be operations that act on a message as it moves from one channel to another.
Common messaging operations that are supported in Spring Integration are routing to one or more channels based on the headers of message, transforming the payload from a string to a rich data type, and filtering messages so that only those that pass the filter criteria are passed along to a downstream channel.
This diagram shows financial trade messages being received on the left via three RabbitMQ adapters that correspond to three external sources of trade data.
The messages are then parsed, validated, and transformed into a canonical data format.
Note that this format is not required to be XML and is often a POJO.
The message header is then enriched, and the trade is stored into a relational database and also passed into a filter.
The filter selects only high-value trades that are subsequently placed into a GemFirebased data grid where real-time processing can occur.
We can define this processing pipeline declaratively using XML or Scala, but while most of the application can be declaratively configured, any components that you may need to write are POJOs that can be easily unit-tested.
In addition to endpoints, channels, and messages, another key component of Spring Integration is its management functionality.
You can easily expose all components in a data pipeline via JMX, where you can perform operations such as stopping and starting adapters.
The control bus component allows you to send in small fragments of code—for example, using Groovy—that can take complex actions to modify the state.
The control bus is then connected to a middleware adapter so it can receive code to execute; HTTP and message-oriented middleware adapters are common choices.
We will not be able to dive into the inner workings of Spring Integration in great depth, nor cover every feature of the adapters that are used, but you should end up with a good feel for how you can use Spring Integration in conjunction with Spring for Apache Hadoop to create very rich data pipeline solutions.
The example applications developed here contain some custom code for working with HDFS that is planned to be incorporated into the Spring Integration project.
For additional information on Spring Integration, consult the project website, which contains links to extensive reference documentation, sample applications, and links to several books on Spring Integration.
Copying Logfiles Copying logfiles into Hadoop as they are continuously generated is a common task.
We will create two applications that continuously load generated logfiles into HDFS.
One application will use an inbound file adapter to poll a directory for files, and the other will poll an FTP site.
The diagram for this data pipeline is shown in Figure 13-3
A Spring Integration data pipeline that polls a directory for files and copies them into HDFS.
The file inbound adapter is configured with the directory to poll for files as well as the filename pattern that determines what files will be detected by the adapter.
These values are externalized into a properties file so they can easily be changed across different runtime environments.
The adapter uses a poller to check the directory since the filesystem is not an event-driven source.
There are several ways you can configure the poller, but the most common are to use a fixed delay, a fixed rate, or a cron expression.
In this example, we do not make use of any additional operations in the pipeline that would sit between the two adapters, but we could easily add that functionality if required.
The configuration file to configure this data pipeline is shown in Example 13-1
The relevant configuration parameters for the pipeline are externalized in the poll ing.properties file, as shown in Example 13-2
By default, duplicate files are prevented when we specify a filename-pattern; the state is kept in memory.
A future enhancement of the file adapter is to persistently store this application state.
You can also lock files to prevent them from being picked up concurrently if more than one process is reading from the same directory.
To build and run this application, use the commands shown in Example 13-3
The relevant parts of the output are shown in Example 13-4
In this log, we can see that the first time around the poller detects the one file that was in the directory and then afterward considers it processed, so the file inbound adapter does not process it a second time.
There are additional options in FsShellWritingMes sageHandler to enable the generation of an additional directory path that contains an embedded date or a UUID (universally unique identifier)
Another way to move files into HDFS is to collect them via FTP from remote machines, as illustrated in Figure 13-4
A Spring Integration data pipeline that polls an FTP site for files and copies them into HDFS.
The configuration in Example 13-6 is similar to the one for file polling, only the configuration of the inbound adapter is changed.
You can build and run this application using the commands shown in Example 13-7
The configuration assumes there is a testuser account on the FTP host machine.
Once you place a file in the outgoing FTP directory, you will see the data pipeline in action, copying the file to a local directory and then copying it into HDFS.
Event Streams Streams are another common source of data that you might want to store into HDFS and optionally perform real-time analysis as it flows into the system.
To meet this need, Spring Integration provides several inbound adapters that we can use to process streams of data.
Once inside a Spring Integration, the data can be passed through a processing chain and stored into HDFS.
The pipeline can also take parts of the stream and write data to other databases, both relational and NoSQL, in addition to forwarding the stream to other systems using one of the many outbound adapters.
Figure 13-2 showed one example of this type of data pipeline.
Next, we will use the TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) inbound adapters to consume data produced by syslog and then write the data into HDFS.
The configuration that sets up a TCP-syslog-to-HDFS processing chain is shown in Example 13-8
The relevant configuration parameters for the pipeline are externalized in the stream ing.properties file, as shown in Example 13-9
The diagram for this data pipeline is shown in Figure 13-5
The serializer segments the incoming byte stream based on the newline character in order to break up the incoming syslog stream into events.
Note that this lower-level serializer configuration will be encapsulated in a syslog XML namespace in the future so as to simplify the configuration.
The inbound channel adapter takes the syslog message off the TCP data stream and parses it into a byte array, which is set as the payload of the incoming message.
A Spring Integration data pipeline that streams data from syslog into HDFS.
Spring Integration’s chain component groups together a sequence of endpoints without our having to explicitly declare the channels that connect them.
The first element in the chain parses the byte[] array and converts it to a java.util.Map containing the key/ value pairs of the syslog message.
At this stage, you could perform additional operations on the data, such as filtering, enrichment, real-time analysis, or routing to other databases.
In this example, we have simply transformed the payload (now a Map) to a String using the built-in object-to-string transformer.
HdfsWritingMessage Handler lets you configure the HDFS directory to write the files, the file naming policy, and the file size rollover policy.
To build and run this application, use the commands shown in Example 13-10
To send a test message, use the logger utility demonstrated in Example 13-11
To use UDP instead of TCP, remove the TCP-related definitions and add the commands shown in Example 13-13
Event Forwarding When you need to process a large amount of data from several different machines, it can be useful to forward the data from where it is produced to another server (as opposed to processing the data locally)
The TCP inbound and outbound adapters can be paired together in an application so that they forward data from one server to another.
The channel that connects the two adapters can be backed by several persistent message stores.
Message stores are represented in Spring Integration by the interface MessageStore, and implementations are available for JDBC, Redis, MongoDB, and GemFire.
Pairing inbound and outbound adapters together in an application affects the message processing flow such that the message is persisted in the message store of the producer application before the message is sent to the consumer application.
The message is removed from the producer’s message store once the acknowledgment from the consumer is received.
This configuration enables an additional level of "store and forward" guarantee via TCP normally found in messaging middleware such as JMS or RabbitMQ.
Example 13-14 is a simple demonstration of forwarding TCP traffic and using Spring’s support to easily bootstrap an embedded HSQL database to serve as the message store.
Management Spring Integration provides two key features that let you manage data pipelines at runtime: the exporting of channels and endpoints to JMX and a control bus.
Much like JMX, the control bus lets you invoke operations and view metric information related to each component, but it is more general-purpose because it allows you to run small programs inside the running application to change its state and behavior.
Exporting channels and endpoints to JMX is as simple as adding the lines of XML configuration shown in Example 13-15
Running the TCP streaming example in the previous section and then starting JConsole shows the JMX metrics and operations that are available (Figure 13-6)
Some examples are to start and stop the TCP adapter, and to get the min, max, and mean duration of processing in a MessageHandler.
A control bus can execute Groovy scripts or Spring Expression Language (SpEL) expressions, allowing you to manipulate the state of components inside the application.
By default, Spring Integration exposes all of its components to be accessed through a control bus.
A Groovy script to perform the same action would not have the @ prefix.
To declare a control bus, add the configuration shown in Example 13-16
By attaching an inbound channel adapter or gateway to the control bus’s input channel, you can execute scripts remotely.
It is also possible to create a Spring MVC application and have the controller send the message to the control bus’s input channel, as shown in Example 13-17
This approach might be more natural if you want to provide additional web application functionality, such as security or additional views.
Example 13-17 shows a Spring MVC controller that forwards the body of the incoming web request to the control bus and returns a String-based response.
Running the sample application again, we can interact with the control bus over HTTP using curl to query and modify the state of the inbound TCP adapter (Example 13-18)
An Introduction to Spring Batch The Spring Batch project was started in 2007 as a collaboration between SpringSource and Accenture to provide a comprehensive batch framework to support the development of robust batch applications.
These batch applications require performing bulk processing of large amounts of data that are critical to the operation of a business.
Spring Batch has been widely adopted and used in thousands of enterprise applications worldwide.
Batch jobs have their own set of best practices and domain concepts, gathered over many years of building up Accenture’s consulting business and encapsulated into the Spring Batch project.
Thus, Spring Batch supports the processing of large volumes of data with features such as automatic retries after failure, skipping of records, job restarting from the point of last failure, periodic batch commits to a transactional database, reusable components (such as parsers, mappers, readers, processors, writers, and validators), and workflow definitions.
As part of the Spring ecosystem, the Spring Batch project builds upon the core features of the Spring Framework, such as the use of the Spring Expression Language.
Spring Batch also carries over the design philosophy of the Spring Framework, which emphasizes a POJO-based development approach and promotes the creation of maintainable, testable code.
The concept of a workflow in Spring Batch translates to a Spring Batch Job (not to be confused with a MapReduce job)
A batch Job is a directed graph, each node of the graph being a processing Step.
Steps can be executed sequentially or in parallel, depending on the configuration.
Restarting jobs is possible since the progress of executed steps in a Job is persisted in a database via a JobRepository.
Jobs are composable as well, so you can have a job of jobs.
Figure 13-7 shows the basic components in a Spring Batch application.
The JobLauncher is responsible for starting a job and is often triggered via a scheduler.
The Spring Framework provides basic scheduling functionality as well as integration with Quartz; however, often enterprises use their own scheduling software such as Tivoli or ControlM.
Other options to launch a job are through a RESTful administration API, a web application, or programmatically in response to an external event.
In the latter case, using the Spring Integration project and its many channel adapters for communicating with external systems is a common choice.
You can read more about combining Spring Integration and Spring Batch in the book Spring Integration in Action [Fisher12].For additional information on Spring Batch, consult the project website, which contains links to extensive reference documentation, sample applications, and links to several books.
The processing performed in a step is broken down into three stages—an ItemReader, ItemProcessor, and ItemWriter—as shown in Figure 13-8
One of the primary use cases for Spring Batch is to process the contents of large files and load the data into a relational database.
In this case, a FlatFileItemReader and a JdbcItemWriter are used along with custom logic either configured declaratively or coded directly in an ItemProcessor.
To increase the performance, the Step is “chunked,” meaning that chunks of data, say 100 rows, are aggregated together and then passed to the ItemWriter.
This allows us to efficiently process a group of records by using the batch APIs available in many databases to insert data.
A snippet of configuration using the Spring Batch XML namespace that reads from a file and writes to a database is shown in Example 13-19
In subsequent sections, we will dive into the configuration of readers, writers, and processors.
Additional features available in Spring Batch allow you to scale up and out the job execution in order to handle the requirements of high-volume and high-performance batch jobs.
It is important to note that the execution model of a Spring Batch application takes place outside of the Hadoop cluster.
Spring Batch applications can scale up by using different threads to concurrently process different files or scale out using Spring Batch’s own master-slave remote partitioning model.
In practice, scaling up with threads has been sufficient to meet the performance requirements of most users.
You should try this option as a first strategy to scale before using remote partitioning.
Another execution model that will be developed in the future is to run a Spring Batch job inside the Hadoop cluster itself, taking advantage of the cluster’s resource management functionality to scale out processing across the nodes of the cluster, taking into account the locality of data stored in HDFS.
Both models have their advantages, and performance isn’t the only criteria to decide which execution model to use.
Executing a batch job outside of the Hadoop cluster often enables easier data movement between different systems and multiple Hadoop clusters.
In the following sections, we will use the Spring Batch framework to process and load data into HDFS from a relational database.
In the section “Exporting Data from HDFS” on page 243, we will export data from HDFS into a relational database and the MongoDB document database.
Processing and Loading Data from a Database To process and load data from a relational database to HDFS, we need to configure a Spring Batch tasklet with a JdbcItemReader and a HdfsTextItemWriter.
The domain for the sample application is an online store that needs to maintain a catalog of the products it sells.
We have modified the example only slightly to write to HDFS instead of a flat file system.
The configuration of the Spring Batch tasklet is shown in Example 13-20
To start and initialize the database with sample data, run the commands shown in Example 13-21
A browser will pop up that lets you browse the contents of the database containing the product table in addition to the tables for Spring Batch used to implement the job repository.
The commit interval is set to 100, which is more than the amount of data available in this simple application, but represents a typical number to use.
For each 100 records read from the database, the transaction that updates the job execution metadata will be committed to the database.
This allows for a restart of the job upon a failure to pick up where it left off.
The RowMapper interface provides a simple way to convert a JDBC ResultSet to a POJO when a single row in a ResultSet maps onto a single POJO instance.
Iteration over the ResultSet as well as exception handling (which is normally quite verbose and error-prone) is encapsulated by Spring, letting you focus on writing only the required mapping code.
The Product RowMapper used in this application converts each row of the ResultSet object to a Prod uct Java object and is shown in Example 13-22
The Product class is a simple POJO with getters and setters that correspond to the columns selected from the product table.
You can set the property fetchSize to give a hint to the driver to load only a certain amount of data into the driver that runs in the client process.
The value to set the fetchSize to depends on the JDBC driver.
The last part of the configuration for the application to run is the hdfsWriter shown in Example 13-23
The Hadoop configuration is as we have seen in the previous sections, but the <hdp:file-system/> is new.
The possible options are implementations that communicate with HDFS using the standard HDFS protocol (hdfs://), HFTP (hftp://), or WebHDFS (webhdfs://)
The FileSystem is used by HdfsTextItemWriter to write plain-text files to HDFS.
ItemWriters often require a collaborating object, an implementation of the LineAggre gator interface that is responsible for converting the item being processed into a string.
The toString() method of Product is a simple comma-delimited concatenation of the ID, name, description, and price values.
To run the application and import from a database to HDFS, execute the commands shown in Example 13-24
There are many other LineAggregator implementations available in Spring Batch that give you a great deal of declarative control over what fields are written to the file and what characters are used to delimit each field.
Running the application again with this configuration of a LineAggregator will create files in HDFS that have the content shown in Example 13-27
Hadoop Workflows Hadoop applications rarely consist of one MapReduce job or Hive script.
Analysis logic is usually broken up into several steps that are composed together into a chain of execution to perform the complete analysis task.
In the previous MapReduce and Apache weblog examples, we used JobRunners, HiveRunners, and PigRunners to execute HDFS operations and MapReduce, Hive, or Pig jobs, but that is not a completely satisfactory solution.
As the number of steps in an analysis chain increases, the flow of execution is hard to visualize and not naturally structured as a graph structure in the XML namespace.
There is also no tracking of the execution steps in the analysis chain when we’re using the various Runner classes.
This means that if one step in the chain fails, we must restart it (manually) from the beginning, making the overall “wall clock” time for the analysis task significantly larger as well as inefficient.
In this section, we will introduce extensions to the Spring Batch project that will provide structure for chaining together multiple Hadoop operations into what are loosely called workflows.
Spring Batch Support for Hadoop Because Hadoop is a batch-oriented system, Spring Batch’s domain concepts and workflow provide a strong foundation to structure Hadoop-based analysis tasks.
To make Spring Batch “Hadoop aware,” we take advantage of the fact that the processing actions that compose a Spring Batch Step are pluggable.
The plug-in point for a Step is known as a Tasklet.
This allows for creating workflows as shown in Figure 13-9
There is support in the Eclipse-based Spring Tool Suite (STS) to support the visual authoring of Spring Batch jobs.
The underlying XML for a Spring Batch job with a linear flow of steps has the general pattern shown in Example 13-28
You can also make the flow conditional based on the exit status of the step.
There are several well-known ExitStatus codes that a step returns, the most common of which are COMPLETED and FAILED.
To create a conditional flow, you use the nested next element of the step, as shown in Example 13-29
In this example, if the exit code matches FAILED, the next step executed is stepC; otherwise, stepB is executed followed by stepC.
There is a wide range of ways to configure the flow of a Spring that we will not cover in this section.
To learn more about how to configure more advanced job flows, see the reference documentation or one of the aforementioned Spring Batch books.
To configure a Hadoop-related step, you can use the XML namespace provided by Spring for Apache Hadoop.
Next, we’ll show how we can configure the wordcount example as a Spring Batch application, reusing the existing configuration of a MapReduce and HDFS script that were part of the standalone Hadoop application examples used previously.
Then we will show how to configure other Hadoop-related steps, such as for Hive and Pig.
Wordcount as a Spring Batch Application The wordcount example has two steps: importing data into HDFS and then running a MapReduce job.
Example 13-30 shows the Spring Batch job representing the workflow using the Spring Batch XML namespace.
We use the namespace prefix batch to distinguish the batch configuration from the Hadoop configuration.
The main application that runs the batch application passes in values for these parameters, but we could also set them using other ways to launch a Spring Batch application.
Example 13-32 is the main driver class for the sample application.
To run the batch application, execute the commands shown in Example 13-33
Hive and Pig Steps To execute a Hive script as part of a Spring Batch workflow, use the Hive Tasklet element, as shown in Example 13-34
To execute a Pig script as part of a Spring Batch workflow, use the Pig Tasklet element (Example 13-35)
Exporting Data from HDFS The results from data analysis in Hadoop are often copied into structured data stores, such as a relational or NoSQL database, for presentation purposes or further analysis.
One of the main use cases for Spring Batch is moving data back between files and databases and processing it along the way.
In this section, we will use Spring Batch to export data from HDFS, perform some basic processing on the data, and then store the data outside of HDFS.
The target data stores are a relational database and MongoDB.
From HDFS to JDBC Moving the result data created from MapReduce jobs located in HDFS into a relational database is very common.
Spring Batch provides many out-of-the-box components that you can configure to perform this activity.
The domain for the sample application is an online store that needs to maintain a catalog of the products it sells.
The application as it was originally written reads product data from flat files on a local filesystem and then writes the product data into a product table in a relational database.
We have modified the example to read from HDFS and also added error handling to show an additional Spring Batch feature.
To read from HDFS instead of a local filesystem, we need to register a HdfsResource Loader with Spring to read data from HDFS using Spring’s Resource abstraction.
This lets us use the Spring Batch’s existing FlatFileItemReader class, as it is based on the Resource abstraction.
Spring’s Resource abstraction provides a uniform way to read an InputStream from a variety of sources such as a URL (http, ftp), the Java ClassPath, or the standard filesystem.
In this section, we will configure these components and discuss some of their configuration properties.
However, we can’t go into detail on all the ways to configure and run Spring Batch applications; there is a great deal of richness in Spring Batch relating to error handling, notifications, data validation, data processing, and scaling that we simply can’t cover here.
For additional information, you should consult the Spring Reference manual or one of the books on Spring Batch mentioned earlier.
Example 13-37 is the top-level configuration to create a Spring Batch job with a single step that processes the output files of a MapReduce job in HDFS and writes them to a database.
The job defines only one step, which contains a reader, processor, and a writer.
The commit interval refers to the number of items to process and aggregate before committing them to the database.
In practice, the commit interval value is varied to determine which value will result in the highest performance.
Values between 10 and a few hundred are what you can expect to use for this property.
These properties determine how many times a specific error in processing will be allowed to occur before failing the step.
The skiplimit determines how many times an exception can be thrown before failing the job.
To keep track of all the lines that were not processed correctly, we configure a listener that will write the offending data into a separate database table.
Setting the scope of the bean to step enables resolution of jobParameter variables.
Example 13-39 uses a main Java class to load the Spring Batch configuration and launch the job.
A sample of the content in each file is shown in Example 13-40
There are four columns in this data, representing the product ID, name, description, and price.
The field names and each value of the token are placed into Spring Batch’s FieldSet object.
The FieldSet object is similar to a JDBC result set but for data read from files.
It allows you to access fields by name or position and to convert the values of those fields to Java types such as String, Integer, or BigDecimal.
The two parts that remain to be configured are the ItemProcessor and ItemWriter; they are shown in Example 13-42
ItemProcessors are commonly used to transform, filter, or validate the data.
Returning a null value from an ItemProcessor is the contract to filter out the item.
Note that if you do not want to perform any processing and directly copy the input file to the database, you can simply remove the processor attribute from the tasklet’s chunk XML configuration.
The batch size is equal to the commit interval defined previously.
We connect to the database using a standard JDBC DataSource, and the SQL statement is specified inline.
What is nice about the SQL statement is that we can use named parameters instead of positional ? placeholders.
To start the database, copy the sample data into HDFS, create the Spring Batch schema, and execute the commands shown in Example 13-45
Running these commands will also launch the H2 interactive web console.
Next run the export process, as shown in Example 13-46
We can view the imported data using the H2 web console.
Spring Batch also has an administrative console, where you can browse what jobs are available to be run, as well as look at the status of each job execution.
By clicking on the link for a specific job execution, you can view details about the state of the job.
Example 13-47 shows a simple implementation of MongoItemWriter that writes the list of items using MongoDB’s batch functionality, which inserts the items into the collection by making a single call to the database.
Spring’s MongoTemplate (which implements the interface MongoOperations) provides support for converting Java classes to MongoDB’s internal data structure format, DbOb ject.
We can specify the connectivity to MongoDB using the Mongo XML namespace.
Example 13-48 shows the configuration of MongoItemWriter and its underlying dependencies to connect to MongoDB.
Running the application again will produce the contents of the products collection in the test database, shown in Example 13-49
This example shows how various Spring Data projects build upon each other to create important new features with a minimal amount of code.
Following the same pattern to implement a MongoItemWriter, you can also easily create ItemWriters for Redis or GemFire that would be as simple as the code shown in this section.
Collecting and Loading Data into Splunk Splunk collects, indexes, searches, and monitors large amounts of machine-generated data.
Splunk can process streams of real-time data as well as historical data.
The first version of Splunk was released in 2005, with a focus on analyzing the data generated inside of datacenters to help solve operational infrastructure problems.
As such, one of its core capabilities is moving data generated on individual machines into a central repository, as well as having out-of-the box knowledge of popular log file formats and infrastructure software like syslog.
Splunk’s base architecture consists of a splunkd daemon that processes and indexes streaming data, and a web application that allows users to search and create reports.
Splunk can scale out by adding separate indexer, search, and forwarder instances as your data requirements grow.
For more details on how to install, run, and develop with Splunk, refer to the product website, as well as the book Exploring Splunk.
While the process of collecting log files and syslog data are supported out-of-the-box by Splunk, there is still a need to collect, transform, and load data into Splunk that comes from a variety of other sources, in order to reduce the need to use regular expressions when analyzing data.
There is also a need to transform and extract data out of Splunk into other databases and filesystems.
To address these needs, Spring Integration inbound and outbound channel adapters were created, and Spring Batch support is on the road map.
At the time of this writing, the Spring Integration channel adapters for Splunk are located in the GitHub repository for Spring Integration extensions.
The adapters support all the ways you can get data in and out of Splunk.
The outbound adapters support putting data into Splunk through its RESTful API, streams, or TCP.
All of the functionality of Splunk is exposed via a comprehensive REST API, and several language SDKs are available to make developing with the REST API as simple as possible.
As an introduction to using Splunk with Spring Integration, we will create an application that stores the results from a Twitter search into Splunk.
The configuration for this application is shown in Example 13-50
Spring Social’s TwitterTemplate class is used by the inbound channel adapter to interact with Twitter.
This requires you to create a new application on the Twitter Developer website, in order to access the full range of functionality offered by Twitter.
Note that there will soon be support for consuming data from the Twitter garden hose, which provides a randomly selected stream of data capped at a small percent of the full stream.
The processing chain applies a filter and then converts the Tweet payload object into a data structure with a format optimized to help Splunk index and search the tweets.
In the sample application, the filter is set to be a pass-through.
The outbound channel adapter writes to the Splunk source with the REST API, specified by the attribute inject="SUB MIT"
The data is written to the Splunk source named twitter and uses the default index.
You can also set the index attribute to specify that data should be written to the nondefault index.
To run the example and see who is the most popular candidate (or pseudo-candidate), follow the directions in the directory splunk/tweets.
GemFire in a Nutshell GemFire provides an in-memory data grid that offers extremely high throughput, low latency data access, and scalability.
Tools to aid system administrators in managing and configuring the GemFire distributed system.
GemFire may be configured to support a number of distributed system topologies and is completely integrated with the Spring Framework.
Figure 14-1 shows a typical client server configuration for a production LAN.
The locator acts as a broker for the distributed system to support discovery of new member nodes.
Client applications use the locator to acquire connections to cache servers.
Once a server comes online, it communicates directly with its peers.
Likewise, once a client is initialized, it communicates directly with cache servers.
Since a locator is a single point of failure, two instances are required for redundancy.
Note that the book’s code samples are configured very simply as a single process with an embedded cache, suitable for development and integration testing.
In a client server scenario, the application process uses a connection pool (Figure 14-2) to manage connections between the client cache and the servers.
The connection pool manages network connections, allocates threads, and provides a number of tuning options to balance resource usage and performance.
The pool is typically configured with the address of the locator(s) [not shown in Figure 14-2]
Once the locator provides a server connection, the client communicates directly with the server.
If the primary server becomes unavailable, the pool will acquire a connection to an alternate server if one is available.
Caches and Regions Conceptually, a cache is a singleton object that provides access to a GemFire member and offers a number of configuration options for memory tuning, network communications, and other features.
The cache also acts as a container for regions, which provide data management and access.
A region is required to store and retrieve data from the cache.
Region is an interface that extends java.uti.Map to perform basic data access using familiar key/value semantics.
The Region interface is wired into classes that require it, so the actual region type is decoupled from the programming model (with some caveats, the discovery of which will be left as an exercise for the reader)
Typically, each region is associated with one domain object, similar to a table in a relational database.
Looking at the sample code, you will see three regions defined: Customer, Product, and Order.
Note that GemFire does not manage associations or enforce relational integrity among regions.
Replicated Data is replicated across all cache members that define the region.
This provides very high read performance, but writes take longer due to the need to perform the replication.
Partitioned Data is partitioned into buckets among cache members that define the region.
This provides high read and write performance and is suitable for very large datasets that are too big for a single node.
Client Technically, a client region is a local region that acts as a proxy to a replicated or partitioned region hosted on cache servers.
It may hold data created or fetched locally; alternatively, it can be empty.
Also, a client region may subscribe to events in order to stay synchronized with changes originating from remote processes that access the same region.
Hopefully, this brief overview gives you a sense of GemFire’s flexibility and maturity.
A complete discussion of GemFire options and features is beyond the scope of this book.
Interested readers will find more details on the product website.
How to Get GemFire The vFabric GemFire website provides detailed product information, reference guides, and a link to a free developer download, limited to three node connections.
For a more comprehensive evaluation, a 60-day trial version is also available.
The product download is not required to run the code samples included with this book.
The GemFire jar file that includes the free developer license is available in public repositories and will be automatically downloaded by build tools such as Maven and Gradle when you declare a dependency on Spring Data GemFire.
A full product install is necessary to use locators, the management tools, and so on.
Configuring GemFire with the Spring XML Namespace Spring Data GemFire includes a dedicated XML namespace to allow full configuration of the data grid.
In fact, the Spring namespace is considered the preferred way to configure GemFire, replacing GemFire’s native cache.xml file.
GemFire will continue to support cache.xml for legacy reasons, but you can now do everything in Spring XML and take advantage of the many wonderful things Spring has to offer, such as modular XML configuration, property placeholders, SpEL, and environment profiles.
Behind the namespace, Spring Data GemFire makes extensive use of Spring’s FactoryBean pattern to simplify the creation and initialization of GemFire components.
GemFire provides several callback interfaces, such as CacheListener, CacheWriter, and CacheLoader to allow developers to add custom event handlers.
Using the Spring IoC container, these may configured as normal Spring beans and injected into GemFire components.
This is a significant improvement over cache.xml, which provides relatively limited configuration options and requires callbacks to implement GemFire’s Declarable interface.
In addition, IDEs such as the Spring Tool Suite (STS) provide excellent support for XML namespaces, such as code completion, pop-up annotations, and real-time validation, making them easy to use.
The following sections are intended to get you started using the Spring XML namespace for GemFire.
For a more comprehensive discussion, please refer to the Spring Data GemFire reference guide at the project website.
Notice that, in addition to the gfe namespace, there is a gfe-data namespace for Spring Data POJO mapping and repository support.
Click Finish to open the bean definition file in an XML editor with the correct namespace declarations.
Now use the gfe namespace to add a cache element.
That’s it! This simple cache declaration will create an embedded cache, register it in the Spring ApplicationContext as gemfireCache, and initialize it when the context is created.
Prior releases of Spring Data GemFire created default bean names using hyphens (e.g., gemfire-cache)
The old-style names are registered as aliases to provide backward compatibility.
You can easily change the bean name by setting the id attribute on the cache element.
However, all other namespace elements assume the default name unless explicitly overridden using the cache-ref attribute.
So you can save yourself some work by following the convention.
The cache element provides some additional attributes, which STS will happily suggest if you press Ctrl-Space.
In addition to the API, GemFire exposes a number of global configuration options via external properties.
While this is convenient, it may result in unintended consequences if you happen to have these files laying around.
Spring alleviates this problem by offering several better alternatives via its standard property loading mechanisms.
It is generally preferable to maintain properties of interest to system administrators in an agreed-upon location in the filesystem rather than defining them in Spring XML or packaging them in .jar files.
Note the use of Spring’s util namespace to create a Properties object.
This is related to, but not the same as, Spring’s property placeholder mechanism, which uses tokenbased substitution to allow properties on any bean to be defined externally from a variety of sources.
Additionally, the cache element includes a cache-xml-location attribute to enable the cache to be configured with GemFire’s native configuration schema.
As previously noted, this is mostly there for legacy reasons.
The cache element also provides some pdx-* attributes required to enable and configure GemFire’s proprietary serialization feature (PDX)
The factory bean responsible for creating the cache uses an internal Spring type called a BeanFactoryLocator to enable user classes declared in GemFire’s native cache.xml file to be registered as Spring beans.
The BeanFactoryLocator implementation also permits only one bean definition for a cache with a given id.
In certain situations, such as running JUnit integration tests from within Eclipse, you’ll need to disable the BeanFactoryLocator by setting this value to false to prevent an exception.
This exception may also arise during JUnit tests running from a build script.
Generally, there is no harm in setting this value to false.
Region Configuration As mentioned in the chapter opener, GemFire provides a few types of regions.
Again, this does not cover all available features but highlights some of the more common ones.
A simple region declaration, as shown in Example 14-3, is all you need to get started.
The default cache declaration creates a Spring bean named gemfireCache.
If you prefer, you can supply any valid bean name, but be sure to set cache-ref to the corresponding bean name as required.
Typically, GemFire is deployed as a distributed data grid, hosting replicated or partitioned regions on cache servers.
For development and integration testing, it is a best practice to eliminate any dependencies on an external runtime environment.
You can do this by simply declaring a replicated or local region with an embedded cache, as is done in the sample code.
Spring environment profiles are extremely useful in configuring GemFire for different environments.
In Example 14-4, the dev profile is intended for integration testing, and the prod profile is used for the deployed cache configuration.
The cache and region configuration is transparent to the application code.
Spring provides a few ways to activate the appropriate environment profile(s)
As shown in Figure 14-7, there are a number of common region configuration options as well as specific options for each type of region.
For example, you can configure all regions to back up data to a local disk store synchronously or asynchronously.
Additionally, you may configure regions to synchronize selected entries over a WAN gateway to distribute data over a wide geographic area.
You may also register Cache Listeners, CacheLoaders, and CacheWriters to handle region events.
Each of these interfaces is used to implement a callback that gets invoked accordingly.
For example, you can write a simple CacheListener to log cache events, which is particularly useful in a distributed environment (see Example 14-5)
A Cache Loader is invoked whenever there is a cache miss (i.e., the requested entry does not exist), allowing you to “read through” to a database or other system resource.
A Cache Writer is invoked whenever an entry is updated or created to provide “write through” or “write behind” capabilities.
Other options include expiration, the maximum time a region or an entry is held in the cache, and eviction, policies that determine which items are removed from the cache when the defined memory limit or the maximum number of entries is reached.
Evicted entries may optionally be stored in a disk overflow.
You can configure partitioned regions to limit the amount of local memory allocated to each partition node, define the number of buckets used, and more.
You may even implement your own PartitionResolver to control how data is colocated in partition nodes.
Cache Client Configuration In a client server configuration, application processes are cache clients—that is, they produce and consume data but do not distribute it directly to other processes.
Neither does a cache client implicitly see updates performed by remote processes.
As you might expect by now, this is entirely configurable.
Example 14-6 shows a basic clientside setup using a client-cache, client-region, and a pool.
The pool represents a connection pool acting as a bridge to the distributed system and is configured with any number of locators.
Typically, two locators are sufficient: the first locator is primary, and the remaining ones are strictly for failover.
Every distributed system member should use the same locator configuration.
A locator is a separate process, running in a dedicated JVM, but is not strictly required.
For development and testing, the pool also provides a server child element to access cache servers directly.
This is useful for setting up a simple client/server environment (e.g., on your local machine) but not recommended for production systems.
As mentioned in the chapter opener, using a locator requires a full GemFire installation, whereas you can connect to a server directly just using the APIs provided in the publicly available gemfire.jar for development, which supports up to three cache members.
You can configure the pool to control thread allocation for connections and network communications.
With subscriptions enabled, the client-region may register interest in all keys or specific keys.
The subscription may be durable, meaning that the client-region is updated with any events that may have occurred while the client was offline.
Also, it is possible to improve performance in some cases by suppressing transmission of values unless.
In this case, new keys are visible, but the value must be retrieved explicitly with a region.get(key) call, for example.
Cache Server Configuration Spring also allows you to create and initialize a cache server process simply by declaring the cache and region(s) along with an additional cache-server element to address server-side configuration.
To start a cache server, simply configure it using the namespace and start the application context, as shown in Example 14-8
Figure 14-8 shows a Spring-configured cache server hosting two partitioned regions and one replicated region.
The cache-server exposes many parameters to tune network communications, system resources, and the like.
For example, a global organization may need to share data across the London, Tokyo, and New York offices.
Each location manages its transactions locally, but remote locations need to be synchronized.
Since WAN communications can be very costly in terms of performance and reliability, GemFire queues events, processed by a WAN gateway to achieve.
It is possible to control which events get synchronized to each remote location.
It is also possible to tune the internal queue sizes, synchronization scheduling, persistent backup, and more.
While a detailed discussion of GemFire’s WAN gateway architecture is beyond the scope of this book, it is important to note that WAN synchronization must be enabled at the region level.
This example shows a region enabled for WAN communications using the APIs available in GemFire 6 versions.
The enable-gateway attribute must be set to true (or is implied by the presence of the hub-id attribute), and the hub-id must reference a gate way-hub element.
The first has an optional GatewayListener to handle gateway events and configures the gateway queue.
The WAN architecture will be revamped in the upcoming GemFire 7.0 release.
This will include new features and APIs, and will generally change the way gateways are configured.
Spring Data GemFire is planning a concurrent release that will support all new features introduced in GemFire 7.0
Disk Store Configuration GemFire allows you to configure disk stores for persistent backup of regions, disk overflow for evicted cache entries, WAN gateways, and more.
Because a disk store may serve multiple purposes, it is defined as a top-level element in the namespace and may be referenced by components that use it.
For asynchronous writes, entries are held in a queue, which is also configurable.
In Example 14-10, an overflow disk store is configured to store evicted entries.
The region is configured for eviction to occur if the total memory size exceeds 2 GB.
A custom ObjectSizer is used to estimate memory allocated per entry.
Data Access with GemfireTemplate Spring Data GemFire provides a template class for data access, similar to the JdbcTem plate or JmsTemplate.
The GemfireTemplate wraps a single region and provides simple data access and query methods as well as a callback interface to access region operations.
Example 14-11 is a simple demonstration of a data access object wired with the Gem fireTemplate.
This method requires only a boolean predicate defining the query criteria.
The body of the query, SELECT * from [region name] WHERE..., is assumed.
We can configure the GemfireTemplate as a normal Spring bean, as shown in Example 14-12
All the core repository features described in Chapter 2 are supported, with the exception of paging and sorting.
By convention, the region name is the same as the simple class name; however, we can override this by setting the annotation value to the desired region name.
This must correspond to the region name—that is, the value of id attribute or the name attribute, if provided, of the region element.
Creating a Repository GemFire repositories support basic CRUD and query operations, which we define using Spring Data’s common method name query mapping mechanism.
You can enable repository discovery using a dedicated gfe-data namespace, which is separate from the core gfe namespace.
It is highly efficient, configurable, interoperable with GemFire client applications written in C# or C++, and supports object versioning.
In general, objects must be serialized for operations requiring network transport and disk persistence.
Cache entries, if already serialized, are stored in serialized form.
A standalone cache with no persistent backup generally does not perform serialization.
If PDX is not enabled, Java serialization will be used.
Additionally, PDX is highly configurable and may be customized to optimize or enhance serialization to satisfy your application requirements.
Example 14-17 shows how to set up a GemFire repository to use PDX.
One limitation to note is that each cache instance can have only one PDX serializer, so if you’re using PDX for repositories, it is advisable to set up a dedicated cache node (i.e., don’t use the same process to host nonrepository regions)
Continuous Query Support A very powerful feature of GemFire is its support for continuous queries (CQ), which provides a query-driven event notification capability.
In traditional distributed applications, data consumers that depend on updates made by other processes in near-real time have to implement some type of polling scheme.
Alternatively, using a publish-subscribe messaging system, the application, upon receiving an event, typically has to access related data stored in a disk-based data store.
Using CQ, the client application registers a query that is executed periodically on cache servers.
The client also provides a callback that gets invoked whenever a region event affects the state of the query’s result set.
To configure CQ, create a CQLC using the namespace, and register a listener for each continuous query (Example 14-18)
The handleEvent() method will be invoked whenever any process makes a change in the range of the query.
Notice that CQListener does not need to implement any interface, nor is there anything special about the method name.
The continuous query container is smart enough to automatically invoke a method that has a single CqEvent parameter.
If there is more than one, declare the method name in the listener configuration.
We’d like to hear your suggestions for improving our indexes.
Spring Data REST repository exporter project (see REST repository exporter)
Spring for Apache Hadoop about, 176 combining HDFS scripting and job.
Mark Pollack worked on big data solutions in high-energy physics at Brookhaven National Laboratory and then moved to the financial services industry as a technical lead or architect for front-office trading systems.
Mark now leads the Spring Data project that aims to simplify application development with new data technologies around big data and NoSQL databases.
Oliver Gierke is an engineer at SpringSource, a division of VMware, and project lead of the Spring Data JPA, MongoDB, and core module.
He has been involved in developing enterprise applications and open source projects for over six years.
His working focus is centered on software architecture, Spring, and persistence technologies.
He speaks regularly at German and international conferences and is the author of several technology articles.
Thomas Risberg is currently a member of the Spring Data team, focusing on the MongoDB and JDBC Extensions projects.
He is also a committer on the Spring Framework project, primarily contributing to enhancements of the JDBC framework portion.
Thomas works on the VMware’s Cloud Foundry team, developing integration for the various frameworks and languages supported by the Cloud Foundry project.
Jon Brisbin is a member of the SpringSource Spring Data team and focuses on providing developers with useful libraries to facilitate next-generation data manipulation.
He’s helped bring elements of the Grails GORM object mapper to Java-based MongoDB applications, and has provided key integration components between the Riak datastore and the RabbitMQ message broker.
In addition, he blogs and speaks on evented application models, and is working diligently to bridge the gap between the bleeding-edge nonblocking and traditional JVM-based applications.
Michael Hunger has been passionate about software development for a long time.
He is particularly interested in the people who develop software, software craftsmanship, programming languages, and improving code.
For the last two years, he has been working with Neo Technology on the Neo4j graph database.
As the project lead of Spring Data Neo4j, he helped develop the idea for a convenient and complete solution for object graph mapping.
Michael is also an active editor and interviewer at InfoQ.
Colophon The animal on the cover of Spring Data is the giant squirrel (genus Ratufa), which is the largest squirrel in the world.
These squirrels are found throughout tropical Asiatic forests and have a conspicuous two-toned color scheme with a distinctive white spot between the ears.
Their ears are round and they have pronounced paws used for gripping.
A healthy adult weighs in at around four and a half pounds.
With their tan, rust, brown, or beige coloring, they are possibly the most colorful of the 280 squirrel species.
They are herbivorous, surviving on flowers, fruits, eggs, insects, and even bark.
The giant squirrel is an upper-canopy dwelling species, which rarely leaves the trees, and requires high branches for the construction of nests.
It travels from tree to tree with jumps of up to 20 feet.
When in danger, the giant squirrel often freezes or flattens itself against the tree trunk, instead of fleeing.
The giant squirrel is mostly active in the early hours of the morning and in the evening, resting in the midday.
It is a shy, wary animal and not easy to discover.
