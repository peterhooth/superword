At the time, Google served an average of 18 million queries/day.
C(A) = number of links going out of page A.
Probability that random surfer visits a page is its PR.
If there are some pages that point to it and have a high PageRank.
Associate the text of a link with the page that the link is on and the page the link points to.
Full raw HTML of pages is available in a repository.
In the past, focused largely on scientific stories, articles, etc.
Current doc status, a pointer into the repository, a doc checksum, and various statistics.
Implemented with a hash table of pointers (word ids) to barrels (that are sorted lists)
Types of hits: Plain, Fancy and anchor (2 bytes per hit)
Can be sorted by doc id or by ranking of word occurrence.
Crawlers have different states: DNS lookup, connecting to host, send request, receiving response.
Words converted into word id and occurrences into hit lists.
Consider each hit to be one of several different type and each of which has its own type-weight.
Scan through the doclists until there is a document that matches all the search terms.
Additional storage used by Google for indexes, temporary storage, lexicon, etc.
To improve efficiency, the Indexer and Crawlers need to be capable of simultaneous execution.
Sorting needs to be done in parallel on several machines.
Complete architecture for gathering web pages, indexing them, and performing search queries over them.
