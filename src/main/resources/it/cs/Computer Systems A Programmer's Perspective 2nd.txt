Many of the designations by manufacturers and seller to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps.
This book (CS:APP) is for computer scientists, computer engineers, and others who want to be able to write better programs by learning what is going on “under the hood” of a computer system.
Our aim is to explain the enduring concepts underlying all computer systems, and to show you the concrete ways that these ideas affect the correctness, performance, and utility of your application programs.
Other systems books are written from a builder’s perspective, describing how to implement the hardware or the systems software, including the operating system, compiler, and network interface.
This book is written from a programmer’s perspective, describing how application programmers can use their knowledge of a system to write better programs.
Of course, learningwhat a system is supposed to do provides a good ﬁrst step in learning how to build one, and so this book also serves as a valuable introduction to those who go on to implement systems hardware and software.
If you study and learn the concepts in this book, you will be on your way to becoming the rare “power programmer” who knows how things work and how to ﬁx them when they break.
Our aim is to present the fundamental concepts in ways that youwill ﬁnd useful right away.Youwill also be prepared to delve deeper, studying such topics as compilers, computer architecture, operating systems, embedded systems, and networking.
To simplify our presentation, we will use the term “Unix” as an umbrella term for systems having Unix as their heritage, including Solaris, Mac OS, and Linux.
The text contains numerous programming examples that have been compiled and run on Linux systems.
We assume that you have access to such amachine and are able to log in and do simple things such as changing directories.
If your computer runs Microsoft Windows, you have two choices.
Windows and have an environment very close to that provided by Linux.
Not all features of Linux are available under Cygwin, however.
We also assume that you have some familiarity with C or C++
If your only prior experience is with Java, the transition will require more effort on your part, but we will help you.
However, there are aspects of C, particularly pointers, explicit dynamic memory allocation, and formatted I/O, that do not exist in Java.
Regardless of your programming background, consider K&R an essential part of your personal systems library.
Several of the early chapters in the book explore the interactions between C programs and their machine-language counterparts.
We do not assume any prior experience with hardware, machine language, or assembly-language programming.
To help readers whose background in C programming is weak (or nonexistent), we have also included these special notes to highlight features that are especially important in C.
Learning how computer systems work from a programmer’s perspective is great fun, mainly because you can do it actively.
Whenever you learn something new, you can try it out right away and see the result ﬁrst hand.
In fact, we believe that the only way to learn systems is to do systems, either working concrete problems or writing and running programs on real systems.
When a new concept is introduced, it is followed in the text by one or more practice problems that you should work immediately to test your understanding.
Solutions to the practice problems are at the end of each chapter.
As you read, try to solve each problem on your own, and then check the solution to make sure you are on the right track.
Each chapter is followed by a set of homework problems of varying difﬁculty.
Your instructor has the solutions to the homework problems in an Instructor’s Manual.
For each homeworkproblem,we showa rating of the amount of effortwe feel itwill require:
Many of these are derived from problems we have given on exams.
Each code example in the text was formatted directly, without any manual intervention, from a C program compiled with gcc and tested on a Linux system.
Of course, your systemmay have a different version of gcc, or a different compiler altogether, and so your compiler might generate different machine code, but the overall behavior should be the same.
In the text, the ﬁle names of the source programs are documented in horizontal bars that surround the formatted code.
For example, the program in Figure 1 can be found in the ﬁle hello.c in directory code/intro/
We encourage you to try running the example programs on your system as you encounter them.
To avoid having a book that is overwhelming, both in bulk and in content, we have created a number of Web asides containing material that supplements the main presentation of the book.
These asides are referenced within the book with a notation of the form CHAP:TOP, where CHAP is a short encoding of the chapter subject, and TOP is short code for the topic that is covered.
All of these Web asides are available from the CS:APP Web page.
You will encounter asides of this form throughout the text.
Asides are parenthetical remarks that give you some additional insight into the current topic.
For example, where did C, Linux, and the Internet come from? Other asides are meant to clarify ideas that students often ﬁnd confusing.
For example, what is the difference between a cache line, set, and block? Other asides give real-world examples.
For example, how a ﬂoating-point error crashed a French rocket, or what the geometry of an actual Seagate disk drive looks like.
The CS:APP book consists of 12 chapters designed to capture the core ideas in computer systems:
This chapter introduces the major ideas and themes in computer systems by tracing the life cycle of a simple “hello, world” program.
Chapter 2: Representing and Manipulating Information.We cover computer arithmetic, emphasizing the properties of unsigned and two’s-complement number representations that affect programmers.
We consider how numbers are represented and thereforewhat range of values can be encoded for a given word size.We consider the effect of casting between signed andunsignednumbers.
Novice programmers are often surprised to learn that the (two’s-complement) sum or product of two positive numbers can be negative.
On the other hand, two’scomplement arithmetic satisﬁes the algebraic properties of a ring, and hence a compiler can safely transform multiplication by a constant into a sequence of shifts and adds.
We use the bit-level operations of C to demonstrate the principles and applications of Boolean algebra.We cover the IEEE ﬂoating-point format in terms of how it represents values and the mathematical properties of ﬂoating-point operations.
Having a solid understanding of computer arithmetic is critical to writing reliable programs.
They cannot even replace it with the expression (-y < -x), due to the asymmetric range of negative and positive numbers in the two’s-complement representation.
Arithmetic overﬂow is a common source of programming errors and security vulnerabilities, yet few other books cover the properties of computer arithmetic from a programmer’s perspective.
We cover the basic instruction patterns generated for different control constructs, such as conditionals, loops, and switch statements.
We cover the implementation of procedures, including stack allocation, register usage conventions, and parameter passing.
We cover the way different data structures such as structures, unions, and arrays are allocated and accessed.
We also use the machine-level view of programs as a way to understand common code security vulnerabilities, such as buffer overﬂow, and steps that the programmer, the compiler, and the operating system can take to mitigate these threats.
Learning the concepts in this chapter helps you become a better programmer, because you will understand how programs are represented on a machine.
One certain beneﬁt is that you will develop a thorough and concrete understanding of pointers.
This chapter covers basic combinational and sequential logic elements, and then shows how these elements can be.
This design is conceptually very simple, but it would not be very fast.We then introduce pipelining, where the different steps required to process an instruction are implemented as separate stages.
At any given time, each stage can work on a different instruction.
Chapter 5: Optimizing Program Performance.This chapter introduces a number of techniques for improving code performance, with the idea being that programmers learn to write their C code in such a way that a compiler can then generate efﬁcient machine code.
We start with transformations that reduce the work to be done by a program and hence should be standard practice when writing any program for any machine.
To motivate these transformations, we introduce a simple operational model of how modern out-of-order processors work, and show how to measure the potential performance of a program in terms of the critical paths through a graphical representation of a program.
You will be surprised how much you can speed up a program by simple transformations of the C code.
To this point, you have relied on a conceptualmodel of thememory system as a linear arraywith uniform access times.
In practice, a memory system is a hierarchy of storage devices with different capacities, costs, and access times.
We cover the different types of RAM andROMmemories and the geometry and organization of magnetic-disk and solid-state drives.
We describe how these storage devices are arranged in a hierarchy.
We show how this hierarchy is made possible by locality of reference.
We make these ideas concrete by introducing a unique view of a memory system as a “memory mountain” with ridges of temporal locality and slopes of spatial locality.
Finally, we show you how to improve the performance of application programs by improving their temporal and spatial locality.
Linking is not covered in most systems texts, but we cover it for several reasons.
First, some of the most confusing errors that programmers can encounter are related to glitches during linking, especially for large software packages.
Second, the object ﬁles produced by linkers are tied to concepts such as loading, virtual memory, and memory mapping.
In this part of the presentation, we step beyond the single-program model by introducing the general concept of exceptional control ﬂow (i.e., changes in control ﬂow that are outside the normal branches and procedure calls)
We cover examples of exceptional control ﬂow that exist at all levels of the system, from low-level hardware exceptions and interrupts, to context switches between concurrent processes, to abrupt changes in control ﬂow caused by the delivery of Unix signals, to the nonlocal jumps in C that break the stack discipline.
This is the part of the book where we introduce the fundamental idea of a process, an abstraction of an executing program.
You will learn how processes work and how they can be created and manipulated from application programs.
We show how application programmers can make use of multiple processes via Unix system calls.
When you ﬁnish this chapter, you will be able to write a Unix shell with job control.
It is also your ﬁrst introduction to the nondeterministic behavior that arises with concurrent program execution.
Our presentation of the virtual memory system seeks to give some understanding of how it works and its characteristics.
We want you to know how it is that the different simultaneous processes can each use an identical range of addresses, sharing some pages but having individual copies of others.
We also cover issues involved in managing and manipulating virtual memory.
In particular, we cover the operation of storage allocators such as the Unix malloc and free operations.
It reinforces the concept that the virtual memory space is just an array of bytes that the program can subdivide into different storage units.
It helps you understand the effects of programs containing memory referencing errors such as storage leaks and invalid pointer references.
Finally, many application programmers write their own storage allocators optimized toward the needs and characteristics of the application.
This chapter, more than any other, demonstrates the beneﬁt of covering both the hardware and the software aspects of computer systems in a uniﬁed way.
Traditional computer architecture and operating systems texts present only part of the virtual memory story.
Chapter 10: System-Level I/O.We cover the basic concepts of Unix I/O such as ﬁles and descriptors.
We describe how ﬁles are shared, how I/O redirection works, and how to access ﬁlemetadata.We also develop a robust buffered I/O package that deals correctly with a curious behavior known as short counts, where the library function reads only part of the input data.
We cover the C standard I/O library and its relationship to Unix I/O, focusing on limitations of standard I/O that make it unsuitable for network programming.
In general, the topics covered in this chapter are building blocks for the next two chapters on network and concurrent programming.
Network programs also provide a compelling context for concurrency, which is the topic of the next chapter.
This chapter is a thin slice through network programming that gets you to the point where you can write a Web server.
We cover the client-server model that underlies all network applications.
We present a programmer’s view of the Internet, and show how to write Internet clients and servers using the sockets interface.
Finally, we introduce HTTP and develop a simple iterative Web server.
This chapter introduces concurrent programming using Internet server design as the running motivational example.
We cover basic principles of synchronization usingP andV semaphore operations, thread safety and reentrancy, race conditions, and deadlocks.
We also describe the use of thread-level programming to express parallelism in an application program, enabling faster execution on multi-core processors.
Getting all of the cores working on a single computational problem requires a careful coordination of the concurrent threads, both for correctness and to achieve high performance.
Considering the rapid evolution of computer technology, the book content has held up surprisingly well.
Intel x86 machines running Unix-like operating systems and programmed in C proved to be a combination that continues to encompass many systems today.
Changes in hardware technology and compilers and the experience of many instructors teaching the material have prompted a substantial revision.
We moved some of the more theoretical aspects to Web asides.
We also describe some of the security vulnerabilities that arise due to the overﬂow properties of computer arithmetic.
We have created Web asides on two different classes of instructions for ﬂoating point, and also a view of the more exotic transformations made when compilers attempt higher degrees of optimization.
Another Web aside describes how to embed x86 assembly code within a C program.
We include a more careful exposition of exception detection and handling in our processor design.
We have also created a Web aside showing a mapping of our processor designs into Verilog, enabling synthesis into working hardware.
We have greatly changed our description of how an out-of-order processor operates, and we have created a simple technique for analyzing program performance based on the paths in a data-ﬂow graph representation of a program.
We have added material on solid-state disks, and we have updated our presentation to be based on the memory hierarchy of an Intel Core i7 processor.
We have enhanced our discussion of how theprocessmodel introduces some fundamental concepts of concurrency, such as nondeterminism.
Chapter 12: Concurrent Programming.We have increased our coverage of the general principles of concurrency, and we also describe how programmers can use thread-level parallelism to make programs run faster on multi-core machines.
In addition, we have added and revised a number of practice and homework problems.
The ICS course has been taught every semester since then, each time to about 150–250 students, ranging from sophomores tomasters degree students and with a wide variety of majors.
It is a required course for all undergraduates in the CS and ECE departments at Carnegie Mellon, and it has become a prerequisite for most upper-level systems courses.
The idea with ICS was to introduce students to computers in a different way.
Few of our students would have the opportunity to build a computer system.
On the other hand, most students, including all computer scientists and computer engineers, will be required to use and program computers on a daily basis.
For example, topics such as hardware adder and bus designs were out.
Topics such as machine language were in, but instead of focusing on how to write assembly language by hand, we would look at how a C compiler translates C constructs into machine code, including pointers, loops, procedure calls, and switch statements.
Further, we would take a broader and more holistic view of the system as both hardware and systems software, covering such topics as linking, loading, processes, signals, performance optimization, virtual memory, I/O, and network and concurrent programming.
This approach allowed us to teach the ICS course in a way that is practical, concrete, hands-on, and exciting for the students.
The response from our students and faculty colleagues was immediate and overwhelmingly positive, and we realized that others outside of CMU might beneﬁt from using our approach.
Hence this book, which we developed from the ICS lecture notes, and which we have now revised to reﬂect changes in technology and how computer systems are implemented.
Instructors can use the CS:APP book to teach ﬁve different kinds of systems courses (Figure 2)
The particular course depends on curriculum requirements, personal taste, and the backgrounds and abilities of the students.
From left to right in the ﬁgure, the courses are characterized by an increasing emphasis on the programmer’s perspective of a system.
ORG: A computer organization course with traditional topics covered in an untraditional style.
Traditional topics such as logic design, processor architecture, assembly language, and memory systems are covered.
However, there is more emphasis on the impact for the programmer.
For example, data representations are related back to the data types and operations of C programs, and the presentation on assembly code is based on machine code generated by a C compiler rather than hand-written assembly code.
ORG+: TheORG course with additional emphasis on the impact of hardware on the performance of application programs.
Compared to ORG, students learn more about code optimization and about improving the memory performance of their C programs.
ICS: The baseline ICS course, designed to produce enlightened programmers who understand the impact of the hardware, operating system, and compilation systemon the performance and correctness of their application programs.
A signiﬁcant difference fromORG+ is that low-level processor architecture is not covered.
Instead, programmers work with a higher-level model of a modern out-of-order processor.
Figure 2 Five systems courses based on the CS:APP book.
Notes: (a) Hardware only, (b) No dynamic storage allocation, (c) No dynamic linking, (d) No ﬂoating point.
ICS+: The baseline ICS course with additional coverage of systems programming topics such as system-level I/O, network programming, and concurrent programming.
Similar to the ICS+ course, but drops ﬂoating point and performance optimization, and places more emphasis on systems programming, including process control, dynamic linking, systemlevel I/O, network programming, and concurrent programming.
Instructors might want to supplement from other sources for advanced topics such as daemons, terminal control, and Unix IPC.
The main message of Figure 2 is that the CS:APP book gives a lot of options to students and instructors.
If you want your students to be exposed to lowerlevel processor architecture, then that option is available via theORG andORG+ courses.
On the other hand, if you want to switch from your current computer organization course to an ICS or ICS+ course, but are wary are making such a drastic change all at once, then you can move toward ICS incrementally.
You can start with ORG, which teaches the traditional topics in a nontraditional way.
Once you are comfortable with that material, then you can move to ORG+, and eventually to ICS.
If students have no experience in C (for example they have only programmed in Java), you could spend several weeks on C and then cover the material of ORG or ICS.
Finally, we note that the ORG+ and SP courses would make a nice two-term (either quarters or semesters) sequence.
Or you might consider offering ICS+ as one term of ICS and one term of SP.
The ICS+ course at CarnegieMellon receives very high evaluations from students.
Students cite the fun, exciting, and relevant laboratory exercises as the primary reason.
Here are examples of the labs that are provided with the book:
This lab requires students to implement simple logical and arithmetic functions, but using a highly restricted subset of C.
For example, they must compute the absolute value of a number using only bit-level operations.
This lab helps students understand the bit-level representations of C data types and the bit-level behavior of the operations on data.
A binary bomb is a program provided to students as an object-code ﬁle.
When run, it prompts the user to type in six different strings.
If any of these is incorrect, the bomb “explodes,” printing an error message and logging the event on a grading server.
Students must “defuse” their own unique bombs by disassembling and reverse engineering the programs to determine what the six strings should be.
The lab teaches students to understand assembly language, and also forces them to learn how to use a debugger.
Buffer Overﬂow Lab.Students are required to modify the run-time behavior of a binary executable by exploiting a buffer overﬂow vulnerability.
This lab teaches the students about the stack discipline, and teaches them about the danger of writing code that is vulnerable to buffer overﬂow attacks.
Several of the homework problems of Chapter 4 can be combined into a lab assignment, where students modify the HCL description of a processor to add new instructions, change the branch prediction policy, or add or remove bypassing paths and register ports.
The resulting processors can be simulated and run through automated tests that will detect most of the possible bugs.
This lab lets students experience the exciting parts of processor design without requiring a complete background in logic design and hardware description languages.
Performance Lab.Students must optimize the performance of an application kernel function such as convolution or matrix transposition.
This lab provides a very clear demonstration of the properties of cache memories, and gives students experience with low-level program optimization.
Shell Lab.Students implement their ownUnix shell programwith job control, including the ctrl-c and ctrl-zkeystrokes, fg, bg, and jobs commands.
This is the student’s ﬁrst introduction to concurrency, and gives them a clear idea of Unix process control, signals, and signal handling.
Students implement their own versions of malloc, free, and (optionally) realloc.
This lab gives students a clear understanding of data layout and organization, and requires them to evaluate different trade-offs between space and time efﬁciency.
Students implement a concurrent Web proxy that sits between their browsers and the rest of the World Wide Web.
This lab exposes the students to such topics as Web clients and servers, and ties together many of the concepts from the course, such as byte ordering, ﬁle I/O, process control, signals, signal handling, memorymapping, sockets, and concurrency.
Students like being able to see their programs in actionwith realWebbrowsers andWeb servers.
The CS:APP Instructor’s Manual has a detailed discussion of the labs, as well as directions for downloading the support software.
Our Intel Labs colleagues Andrew Chien and Limor Fix were exceptionally supportive throughout the writing of the text.
Michael Kozuch, Babu Pillai, and Jason Campbell provided valuable insight on memory system performance, multi-core.
Phil Gibbons and Shimin Chen shared their considerable expertise on solid-state disk designs.
We have been able to call on the talents of many, including Wen-Mei Hwu, Markus Pueschel, and Jiri Simsa, to provide both detailed comments and highlevel advice.
James Hoe helped us create a Verilog version of the Y86 processor and did all of the work needed to synthesize working hardware.
Paul Anagnostopoulos of Windfall Software did an outstanding job of typesetting the book and leading the production team.
Our editor Matt Goldstein provided stellar leadership from beginning to end.We are profoundly grateful for their help, encouragement, and insights.
We are deeply indebted to many friends and colleagues for their thoughtful criticisms and encouragement.
A special thanks to our 15-213 students, whose infectious energy and enthusiasm spurred us on.
Nick Carter and Vinny Furia generously provided their malloc package.
GuyBlelloch, GregKesden, BruceMaggs, andToddMowry taught the course over multiple semesters, gave us encouragement, and helped improve the course material.
A suggestion from Garth early on got the whole ball rolling, and this was picked up and reﬁned with the help of a group led by Allan Fisher.
Mark Stehlik and Peter Lee have been very supportive about building this material into the undergraduate curriculum.
Greg Kesden provided helpful feedback on the impact of ICS on the OS course.
Greg Ganger and Jiri Schindler graciously provided some disk drive characterizations and answered our questions on modern disks.
James Hoe provided useful ideas and feedback on how to present processor architecture.
In particular, Chris Colohan established a fun (and funny) tone that persists to this day, and invented the legendary “binary bomb” that has proven to be a great tool for teachingmachine code and debugging concepts.
We would also like to thank our colleagues at Prentice Hall.
Marcia Horton, Eric Frank, and Harold Stone have been unﬂagging in their support and vision.
Harold also helped us present an accurate historical perspective on RISC and CISC processor architectures.
Jerry Ralya provided sharp insights and taught us a lot about good writing.
Finally, we would like to acknowledge the great technical writers Brian Kernighan and the late W.
Richard Stevens, for showing us that technical books can be beautiful.
RandalE.Bryant received hisBachelor’s degree from the University of Michigan in 1973 and then attended graduate school at the Massachusetts Institute of Technology, receiving a Ph.D.
He spent three years as an Assistant Professor at the California Institute of Technology, and has been on the faculty at Carnegie Mellon since Science.
He has taught courses in computer systems at both the undergraduate and.
Over many years of teaching computer architecture courses, he began shifting the focus from how computers are designed to one of how programmers can write more efﬁcient and reliable programs if they understand the system better.
Together with Professor O’Hallaron, he developed the course 15-213 “Introduction to Computer Systems” at Carnegie Mellon that is the basis for this book.
He has also taught courses in algorithms, programming, computer networking, and VLSI design.
Most of Professor Bryant’s research concerns the design of software tools to help software and hardware designers verify the correctness of their systems.
These include several types of simulators, as well as formal veriﬁcation tools that prove the correctness of a design using mathematical methods.
His research results are used by major computer manufacturers, including Intel, FreeScale, IBM, and Fujitsu.
These include two inventor recognition awards and a technical achievement award from the Semiconductor Research Corporation, the Kanellakis Theory and Practice Award from the Association for Computer Machinery (ACM), and theW.
He is a Fellow of both the ACM and the IEEE and a member of the U.S.
He has taught computer systems courses at the undergraduate and graduate levels on such topics as computer architecture, introductory computer systems, parallel processor design, and Internet services.
Together with Professor Bryant, he developed the course at Carnegie Mellon that led to this book.
CMUSchool of Computer Science, an award for which the winner is chosen based on a poll of the students.
Professor O’Hallaron works in the area of computer systems, with speciﬁc interests in software systems for scientiﬁc computing, data-intensive computing, and virtualization.
The best known example of his work is the Quake project, a group of computer scientists, civil engineers, and seismologists who have developed the ability to predict the motion of the ground during strong earthquakes.
In Gordon Bell Prize, the top international prize in high-performance computing.
A computer system consists of hardware and systems software that work together to run application programs.
Speciﬁc implementations of systems change over time, but the underlying concepts do not.
All computer systems have similar hardware and software components that perform similar functions.
This book is written for programmers who want to get better at their craft by understanding how these components work and how they affect the correctness and performance of their programs.
If you dedicate yourself to learning the concepts in this book, then youwill be on yourway to becoming a rare “power programmer,” enlightened by an understanding of the underlying computer system and its impact on your application programs.
You are going to learn practical skills such as how to avoid strange numerical errors caused by the way that computers represent numbers.
You will learn how to optimize your C code by using clever tricks that exploit the designs of modern processors and memory systems.
You will learn how to write your own Unix shell, your own dynamic storage allocation package, and even your ownWeb server.
You will learn the promises and pitfalls of concurrency, a topic of increasing importance as multiple processor cores are integrated onto single chips.
Although hello is a very simple program, every major part of the system must work in concert in order for it to run to completion.
In a sense, the goal of this book is to help you understand what happens and why, when you run hello on your system.
We begin our study of systems by tracing the lifetime of the hello program, from the time it is created by a programmer, until it runs on a system, prints its simple message, and terminates.
As we follow the lifetime of the program, we will brieﬂy introduce the key concepts, terminology, and components that come into play.
Our hello program begins life as a source program (or source ﬁle) that the programmer creates with an editor and saves in a text ﬁle called hello.c.
Mostmodern systems represent text characters using theASCII standard that represents each character with a unique byte-sized integer value.
For example, Figure 1.2 shows the ASCII representation of the hello.c program.
The hello.c program is stored in a ﬁle as a sequence of bytes.
Each byte has an integer value that corresponds to some character.
Files such as hello.c that consist exclusively of ASCII characters are known as text ﬁles.
The representation of hello.c illustrates a fundamental idea:All information in a system—including disk ﬁles, programs stored in memory, user data stored in memory, and data transferred across a network—is represented as a bunch of bits.
The only thing that distinguishes different data objects is the context in which we view them.
For example, in different contexts, the same sequence of bytes might represent an integer, ﬂoating-point number, character string, or machine instruction.
As programmers, we need to understandmachine representations of numbers because they are not the same as integers and real numbers.
They are ﬁnite approximations that can behave in unexpected ways.
The American National Standards Institute (ANSI) ratiﬁed theANSIC standard in 1989, and this standardization later became the responsibility of the International Standards Organization (ISO)
The standards deﬁne the C language and a set of library functions known as theC standard library.
In Ritchie’s words [88], C is “quirky, ﬂawed, and an enormous success.” So why the success?
Most of the Unix kernel, and all of its supporting tools and libraries, were written in C.
Since Unix was written almost entirely in C, it could be easily ported to new machines, which created an even wider audience for both C and Unix.
The simplicity of C made it relatively easy to learn and to port to different computers.
Later, other people found that they could write the programs they wanted, without the language getting in the way.
However, it is not perfect for all programmers and all situations.
Newer languages such as C++ and Java address these issues for application-level programs.
The hello program begins life as a high-level C program because it can be read and understood by human beings in that form.
These instructions are then packaged in a form called an executable object program and stored as a binary disk ﬁle.
Object programs are also referred to as executable object ﬁles.
On a Unix system, the translation from source ﬁle to object ﬁle is performed by a compiler driver:
Here, the gcc compiler driver reads the source ﬁle hello.c and translates it into an executable object ﬁle hello.
The translation is performed in the sequence of four phases shown in Figure 1.3
The programs that perform the four phases (preprocessor, compiler, assembler, and linker) are known collectively as the compilation system.
Preprocessing phase.The preprocessor (cpp) modiﬁes the original C program according to directives that begin with the # character.
The result is another C program, typically with the .i sufﬁx.
The compiler (cc1) translates the text ﬁle hello.i into the text ﬁle hello.s, which contains an assembly-language program.
Each statement in an assembly-language program exactly describes one low-level machine-language instruction in a standard text form.
Assembly language is useful because it provides a common output language for different compilers for different high-level languages.
For example, C compilers and Fortran compilers both generate output ﬁles in the same assembly language.
Assembly phase.Next, the assembler (as) translates hello.s into machinelanguage instructions, packages them in a form known as a relocatable object program, and stores the result in the object ﬁle hello.o.
The hello.o ﬁle is a binary ﬁle whose bytes encode machine language instructions rather than characters.
If we were to view hello.o with a text editor, it would appear to be gibberish.
The printf function resides in a separate precompiled object ﬁle called printf.o, which must somehow bemergedwith our hello.o program.
The result is the hello ﬁle, which is an executable object ﬁle (or simply executable) that is ready to be loaded into memory and executed by the system.
The GNU project is a tax-exempt charity started by Richard Stallman in 1984, with the ambitious goal of developing a complete Unix-like system whose source code is unencumbered by restrictions on how it can be modiﬁed or distributed.
The GNU project has developed an environment with all the major components of a Unix operating system, except for the kernel, which was developed separately by the Linux project.
The GNU environment includes the emacs editor, gcc compiler, gdb debugger, assembler, linker, utilities for manipulating binaries, and other components.
The gcc compiler has grown to support many different languages, with the ability to generate code for many different machines.
The GNU project is a remarkable achievement, and yet it is often overlooked.
The modern opensource movement (commonly associated with Linux) owes its intellectual origins to the GNU project’s notion of free software (“free” as in “free speech,” not “free beer”)
Further, Linux owes much of its popularity to the GNU tools, which provide the environment for the Linux kernel.
For simple programs such as hello.c, we can rely on the compilation system to produce correct and efﬁcient machine code.
However, there are some important reasons why programmers need to understand how compilation systems work:
As programmers, we do not need to know the inner workings of the compiler in order to write efﬁcient code.
However, in order to make good coding decisions in our C programs, we do need a basic understanding of machine-level code and how the compiler translates different C statements into machine code.
For example, is a switch statement always more efﬁcient than a sequence of if-else statements? How much overhead is incurred by a function call? Is a while loop more efﬁcient than a for loop? Are pointer references more efﬁcient than array indexes? Why does our loop run so much faster if we sum into a local variable instead of an argument that is passed by reference? How can a function run faster when we simply rearrange the parentheses in an arithmetic expression?
We describe how compilers translate different C constructs into these languages.
In Chapter 5, you will learn how to tune the performance of your C programs by making simple transformations to the C code that help the compiler do its job better.
In Chapter 6, you will learn about the hierarchical nature of the memory system, how C compilers store data arrays in memory, and how your C programs can exploit this knowledge to run more efﬁciently.
In our experience, some of the most perplexing programming errors are related to the operation of the linker, especially when you are trying to build large software systems.
For many years, buffer overﬂow vulnerabilities have accounted for the majority of security holes in network and Internet servers.
These vulnerabilities exist because too few programmers understand the need to carefully restrict the quantity and forms of data they accept from untrusted sources.
A ﬁrst step in learning secure programming is to understand the consequences of the way data and control information are stored on the program stack.
We cover the stack discipline and buffer overﬂow vulnerabilities in Chapter 3 as part of our study of assembly language.
We will also learn about methods that can be used by the programmer, compiler, and operating system to reduce the threat of attack.
At this point, our hello.c source program has been translated by the compilation system into an executable object ﬁle called hello that is stored on disk.
To run the executable ﬁle on a Unix system, we type its name to an application program known as a shell:
The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then performs the command.
If the ﬁrst word of the command line does not correspond to a built-in shell command, then the shell assumes that it is the name of an executable ﬁle that it should load and run.
So in this case, the shell loads and runs the hello program and then waits for it to terminate.
The hello programprints itsmessage to the screen and then terminates.
The shell then prints a prompt and waits for the next input command line.
To understand what happens to our hello program when we run it, we need to understand the hardware organization of a typical system, which is shown in Figure 1.4
This particular picture is modeled after the family of Intel Pentium.
I/O bus Expansion slots for other devices such as network adaptersDisk.
Don’t worry about the complexity of this ﬁgure just now.
We will get to its various details in stages throughout the course of the book.
Running throughout the system is a collection of electrical conduits called buses that carry bytes of information back and forth between the components.
Buses are typically designed to transfer ﬁxed-sized chunks of bytes known aswords.
The number of bytes in a word (theword size) is a fundamental system parameter that varies across systems.
For the sake of our discussion here, wewill assume aword size of 4 bytes, and we will assume that buses transfer only one word at a time.
Input/output (I/O) devices are the system’s connection to the external world.
Our example system has four I/O devices: a keyboard and mouse for user input, a display for user output, and a disk drive (or simply disk) for long-term storage of data and programs.
Each I/Odevice is connected to the I/Obusby either a controller or an adapter.
The distinction between the two is mainly one of packaging.
Controllers are chip sets in the device itself or on the system’s main printed circuit board (often called themotherboard)
An adapter is a card that plugs into a slot on the motherboard.
Regardless, the purpose of each is to transfer information back and forth between the I/O bus and an I/O device.
Chapter 6 has more to say about how I/O devices such as disks work.
In Chapter 10, youwill learn how to use theUnix I/O interface to access devices from your application programs.
We focus on the especially interesting class of devices known as networks, but the techniques generalize to other kinds of devices as well.
The main memory is a temporary storage device that holds both a program and the data it manipulates while the processor is executing the program.
Physically, mainmemory consists of a collection ofdynamic randomaccessmemory (DRAM) chips.
Logically, memory is organized as a linear array of bytes, each with its own unique address (array index) starting at zero.
In general, each of the machine instructions that constitute a program can consist of a variable number of bytes.
The sizes of data items that correspond to C program variables vary according to type.
For example, on an IA32machine runningLinux, data of type short requires two bytes, types int, float, and long four bytes, and type double eight bytes.
Chapter 6 has more to say about how memory technologies such as DRAM chips work, and how they are combined to form main memory.
The central processing unit (CPU), or simply processor, is the engine that interprets (or executes) instructions stored in main memory.
At its core is a word-sized storage device (or register) called the program counter (PC)
At any point in time, the PC points at (contains the address of) some machine-language instruction in main memory.1
From the time that power is applied to the system, until the time that the power is shut off, a processor repeatedly executes the instruction pointed at by the program counter and updates the program counter to point to the next instruction.
A processor appears to operate according to a very simple instruction execution model, deﬁnedby its instruction set architecture.
In thismodel, instructions execute in strict sequence, and executing a single instruction involves performing a series of steps.
The processor reads the instruction from memory pointed at by the program counter (PC), interprets the bits in the instruction, performs some simple operation dictated by the instruction, and then updates the PC to point to the next instruction, whichmay ormay not be contiguous inmemory to the instruction that was just executed.
There are only a few of these simple operations, and they revolve around main memory, the register ﬁle, and the arithmetic/logic unit (ALU)
The register ﬁle is a small storage device that consists of a collection of word-sized registers, each with its own unique name.
Here are some examples of the simple operations that the CPU might carry out at the request of an instruction:
Load: Copy a byte or a word from main memory into a register, overwriting the previous contents of the register.
Store: Copy a byte or a word from a register to a location in main memory, overwriting the previous contents of that location.
Jump: Extract a word from the instruction itself and copy that word into the program counter (PC), overwriting the previous value of the PC.
We say that a processor appears to be a simple implementation of its instruction set architecture, but in fact modern processors use far more complex mechanisms to speed up program execution.
Thus, we can distinguish the processor’s instruction set architecture, describing the effect of each machine-code instruction, from its microarchitecture, describing how the processor is actually implemented.
When we study machine code in Chapter 3, we will consider the abstraction provided by the machine’s instruction set architecture.
Chapter 4 has more to say about how processors are actually implemented.
Given this simple view of a system’s hardware organization and operation, we can begin to understand what happens when we run our example program.
We must omit a lot of details here that will be ﬁlled in later, but for now we will be content with the big picture.
Initially, the shell program is executing its instructions, waiting for us to type a command.
As we type the characters “./hello” at the keyboard, the shell program reads each one into a register, and then stores it in memory, as shown in Figure 1.5
When we hit the enter key on the keyboard, the shell knows that we have ﬁnished typing the command.
The shell then loads the executable hello ﬁle by executing a sequence of instructions that copies the code and data in the hello object ﬁle from disk to main memory.
The data include the string of characters “hello, world\n” that will eventually be printed out.
Using a technique known as direct memory access (DMA, discussed in Chapter 6), the data travels directly fromdisk tomainmemory, without passing through the processor.
Once the code and data in the hello object ﬁle are loaded into memory, the processor begins executing the machine-language instructions in the hello program’s main routine.
These instructions copy the bytes in the “hello, world\n” string frommemory to the register ﬁle, and from there to the display device, where they are displayed on the screen.
I/O bus Expansion slots for other devices such as network adaptersDisk.
I/O bus Expansion slots for other devices such as network adaptersDisk.
Figure 1.6 Loading the executable from disk into main memory.
Figure 1.7 Writing the output string from memory to the display.
I/O bus Expansion slots for other devices such as network adaptersDisk.
An important lesson from this simple example is that a system spends a lot of time moving information from one place to another.
The machine instructions in the hello program are originally stored on disk.
When the program is loaded, they are copied to main memory.
As the processor runs the program, instructions are copied from main memory into the processor.
Similarly, the data string “hello,world\n”, originally on disk, is copied to main memory, and then copied frommainmemory to the display device.
From a programmer’s perspective, much of this copying is overhead that slows down the “real work” of the program.
Thus, a major goal for system designers is to make these copy operations run as fast as possible.
Because of physical laws, larger storage devices are slower than smaller storage devices.
And faster devices are more expensive to build than their slower counterparts.
Similarly, a typical register ﬁle stores only a few hundred bytes of information, as opposed to billions of bytes in the main memory.
However, the processor can read data from the register ﬁle almost 100 times faster than from memory.
Even more troublesome, as semiconductor technology progresses over the years, this processor-memory gap continues to increase.
It is easier and cheaper to make processors run faster than it is to make main memory run faster.
To deal with the processor-memory gap, system designers include smaller faster storage devices called cache memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to need in the near future.
Figure 1.8 shows the cache memories in a typical system.
A larger L2 cache with hundreds of thousands to millions of bytes is connected to the processor by a special bus.
The idea behind caching is that a system can get the effect of both a very large memory and a very fast one by exploiting locality, the tendency for programs to access data and code in localized regions.
By setting up caches to hold data that is likely to be accessed often, we can perform most memory operations using the fast caches.
One of the most important lessons in this book is that application programmers who are aware of cache memories can exploit them to improve the performance of their programs by an order of magnitude.
This notion of inserting a smaller, faster storage device (e.g., cache memory) between the processor and a larger slower device (e.g., main memory) turns out to be a general idea.
In fact, the storage devices in every computer system are organized as a memory hierarchy similar to Figure 1.9
As we move from the top of the hierarchy to the bottom, the devices become slower, larger, and less costly per byte.
The main idea of a memory hierarchy is that storage at one level serves as a cache for storage at the next lower level.
Thus, the register ﬁle is a cache for the L1 cache.
The L3 cache is a cache for the main memory, which is a cache for the disk.
On some networked systemswith distributed ﬁle systems, the local disk serves as a cache for data stored on the disks of other systems.
Local disks hold files retrieved from disks on remote network server.
Just as programmers can exploit knowledge of the different caches to improve performance, programmers can exploit their understanding of the entire memory hierarchy.
Chapter 6 will have much more to say about this.
When the shell loaded and ran the hello program, and when the hello program printed its message, neither program accessed the keyboard, display, disk, or main memory directly.
Rather, they relied on the services provided by the operating system.
We can think of the operating system as a layer of software interposed between the application program and the hardware, as shown in Figure 1.10
All attempts by an application program tomanipulate the hardware must go through the operating system.
As this ﬁgure suggests, ﬁles are abstractions for I/O devices, virtual memory is an abstraction for both the main memory and disk I/O devices, and processes are abstractions for the processor, main memory, and I/O devices.
While OS/360 was one of the most successful software projects in history, Multics dragged on for years and never achievedwide-scale use.
Bell Laboratories was an original partner in the Multics project, but dropped out in 1969 because of concern over the complexity of the project and the lack of progress.
Many of the ideas in the new system, such as the hierarchical ﬁle system and the notion of a shell as a user-level process, were borrowed fromMultics but implemented in a smaller, simpler package.
Because Bell Labsmade the source code available to schools with generous terms, Unix developed a large following at universities.
Concurrently, Bell Labs was releasing their own versions, which became known as System V Unix.
Versions from other vendors, such as the Sun Microsystems Solaris system, were derived from these original BSD and System V versions.
Trouble arose in the mid 1980s as Unix vendors tried to differentiate themselves by adding new and often incompatible features.
To combat this trend, IEEE (Institute for Electrical and Electronics Engineers) sponsored an effort to standardize Unix, later dubbed “Posix” by Richard Stallman.
The result was a family of standards, known as the Posix standards, that cover such issues as the C language interface for Unix system calls, shell programs and utilities, threads, and network programming.
As more systems comply more fully with the Posix standards, the differences between Unix versions are gradually disappearing.
When a program such as hello runs on a modern system, the operating system provides the illusion that the program is the only one running on the system.
The program appears to have exclusive use of both the processor, main memory, and I/O devices.
The processor appears to execute the instructions in the program, one after theother, without interruption.And the code anddata of theprogramappear to be the only objects in the system’s memory.
These illusions are provided by the notion of a process, one of the most important and successful ideas in computer science.
A process is the operating system’s abstraction for a running program.
Multiple processes can run concurrently on the same system, and each process appears to have exclusive use of the hardware.
By concurrently, we mean that the instructions of one process are interleaved with the instructions of another process.
In most systems, there are more processes to run than there are CPUs to run them.
Traditional systems could only execute one program at a time, while newermulticore processors can execute several programs simultaneously.
In either case, a single CPU can appear to execute multiple processes concurrently by having the processor switch among them.
The operating system performs this interleaving with a mechanism known as context switching.
To simplify the rest of this discussion, we consider only a uniprocessor system containing a single CPU.
We will return to the discussion of multiprocessor systems in Section 1.9.1
The operating system keeps track of all the state information that the process needs in order to run.
This state, which is known as the context, includes information such as the current values of the PC, the register ﬁle, and the contents of main memory.
At any point in time, a uniprocessor system can only execute the code for a single process.
When the operating system decides to transfer control from the current process to some new process, it performs a context switch by saving the context of the current process, restoring the context of the new process, and then passing control to the new process.
The new process picks up exactly where it left off.
Figure 1.12 shows the basic idea for our example hello scenario.
There are two concurrent processes in our example scenario: the shell process and the hello process.
Initially, the shell process is running alone, waiting for input on the command line.
When we ask it to run the hello program, the shell carries.
The operating system saves the shell’s context, creates a new hello process and its context, and then passes control to the new hello process.
After hello terminates, the operating system restores the context of the shell process and passes control back to it, where it waits for the next command line input.
Implementing the process abstraction requires close cooperation between both the low-level hardware and the operating system software.
Althoughwe normally think of a process as having a single control ﬂow, inmodern systems a process can actually consist of multiple execution units, called threads, each running in the context of the process and sharing the same code and global data.
Virtualmemory is an abstraction that provides each process with the illusion that it has exclusive use of the main memory.
Each process has the same uniform view of memory, which is known as its virtual address space.
The virtual address space for Linux processes is shown in Figure 1.13
In Linux, the topmost region of the address space is reserved for code and data in the operating system that is common to all processes.
The lower region of the address space holds the code and data deﬁned by the user’s process.
Note that addresses in the ﬁgure increase from the bottom to the top.
The virtual address space seen by each process consists of a number of welldeﬁned areas, each with a speciﬁc purpose.
You will learn more about these areas later in the book, but it will be helpful to look brieﬂy at each, starting with the lowest addresses and working our way up:
Heap.The code and data areas are followed immediately by the run-timeheap.
Unlike the code and data areas, which are ﬁxed in size once the process begins running, the heap expands and contracts dynamically at run time as a result of calls to C standard library routines such as malloc and free.
Shared libraries.Near the middle of the address space is an area that holds the code and data for shared libraries such as the C standard library and the math library.
The notion of a shared library is a powerful but somewhat difﬁcult concept.
At the top of the user’s virtual address space is the user stack that the compiler uses to implement function calls.
Like the heap, the user stack expands and contracts dynamically during the execution of the program.
In particular, each time we call a function, the stack grows.
Kernel virtual memory.The kernel is the part of the operating system that is always resident inmemory.
For virtual memory to work, a sophisticated interaction is required between the hardware and the operating system software, including a hardware translation of every address generated by the processor.
Chapter 9 explains how this works and why it is so important to the operation of modern systems.
A ﬁle is a sequence of bytes, nothing more and nothing less.
Every I/O device, including disks, keyboards, displays, and even networks, is modeled as a ﬁle.
All input and output in the system is performed by reading and writing ﬁles, using a small set of system calls known as Unix I/O.
This simple and elegant notion of a ﬁle is nonetheless very powerful because it provides applications with a uniform view of all of the varied I/O devices that might be contained in the system.
For example, application programmers who manipulate the contents of a disk ﬁle are blissfully unaware of the speciﬁc disk technology.
Further, the same program will run on different systems that use different disk technologies.
InAugust 1991, a Finnish graduate student namedLinus Torvaldsmodestly announced a newUnix-like operating system kernel:
Subject: What would you like to see most in minix?
This implies that I’ll get something practical within a few months, and.
I’d like to know what features most people would want.
By combining forces with the GNU project, the Linux project has developed a complete, Posix-compliant version of the Unix operating system, including the kernel and all of the supporting infrastructure.
Linux is available on a wide array of computers, from hand-held devices to mainframe computers.
A group at IBM has even ported Linux to a wristwatch!
Up to this point in our tour of systems, we have treated a system as an isolated collection of hardware and software.
In practice, modern systems are often linked to other systems by networks.
From the point of view of an individual system, the network can be viewed as just another I/O device, as shown in Figure 1.14
When the system copies a sequence of bytes frommain memory to the network adapter, the data ﬂows across the network to another machine, instead of, say, to a local disk drive.
Similarly, the system can read data sent from other machines and copy this data to its main memory.
With the advent of global networks such as the Internet, copying information from one machine to another has become one of the most important uses of computer systems.
For example, applications such as email, instant messaging, the World Wide Web, FTP, and telnet are all based on the ability to copy information over a network.
Returning to our hello example, we could use the familiar telnet application to run hello on a remote machine.
Server sends “hello” string to the shell, which runs the hello program and passes the output.
Figure 1.15 Using telnet to run hello remotely over a network.
After we log in to the remote machine and run a shell, the remote shell is waiting to receive an input command.
From this point, running the hello program remotely involves the ﬁve basic steps shown in Figure 1.15
After we type the “hello” string to the telnet client and hit the enter key, the client sends the string to the telnet server.
After the telnet server receives the string from the network, it passes it along to the remote shell program.
Next, the remote shell runs the hello program, and passes the output line back to the telnet server.
Finally, the telnet server forwards the output string across the network to the telnet client, which prints the output string on our local terminal.
This type of exchange between clients and servers is typical of all network applications.
In Chapter 11, you will learn how to build network applications, and apply this knowledge to build a simple Web server.
An important idea to take away from this discussion is that a system is more than just hardware.
It is a collection of intertwined hardware and systems software that must cooperate in order to achieve the ultimate goal of running application programs.
The rest of this book will ﬁll in some details about the hardware and the software, and it will show how, by knowing these details, you can write programs that are faster, more reliable, and more secure.
To close out this chapter, we highlight several important concepts that cut across all aspects of computer systems.
We will discuss the importance of these concepts at multiple places within the book.
Throughout the history of digital computers, two demands have been constant forces driving improvements: we want them to do more, and we want them to run faster.
Both of these factors improve when the processor does more things at once.We use the term concurrency to refer to the general concept of a systemwith multiple, simultaneous activities, and the term parallelism to refer to the use of concurrency to make a system run faster.
Building on the process abstraction, we are able to devise systems where multiple programs execute at the same time, leading to concurrency.
With threads, we can even have multiple control ﬂows executing within a single process.
Support for concurrent execution has been found in computer systems since the advent of time-sharing in the early 1960s.
Traditionally, this concurrent execution was only simulated, by having a single computer rapidly switch among its executing processes, much as a juggler keeps multiple balls ﬂying through the air.
This form of concurrency allows multiple users to interact with a system at the same time, such as when many people want to get pages from a single Web server.
It also allows a single user to engage in multiple tasks concurrently, such as having a Web browser in one window, a word processor in another, and streaming music playing at the same time.
Until recently, most actual computing was done by a single processor, even if that processor had to switch among multiple tasks.
When we construct a system consisting of multiple processors all under the control of a single operating system kernel, we have a multiprocessor system.
Such systems have been available for large-scale computing since the 1980s, but they have more recently become commonplace with the advent of multi-core processors and hyperthreading.
Figure 1.16 shows a taxonomy of these different processor types.
Multi-core processors have several CPUs (referred to as “cores”) integrated onto a single integrated-circuit chip.
Industry experts predict that they will be able to have dozens, and ultimately hundreds, of cores on a single chip.
Hyperthreading, sometimes called simultaneous multi-threading, is a technique that allows a single CPU to execute multiple ﬂows of control.
It involves having multiple copies of some of the CPU hardware, such as program counters and register ﬁles, while having only single copies of other parts of the hardware, such as the units that perform ﬂoating-point arithmetic.
Multiprocessors are becoming prevalent with the advent of multicore processors and hyperthreading.
It enables the CPU to make better advantage of its processing resources.
For example, if one thread must wait for some data to be loaded into a cache, the CPU can proceed with the execution of a different thread.
As an example, the Intel Core i7 processor can have each core executing two threads, and so a four-core system can actually execute eight threads in parallel.
The use of multiprocessing can improve system performance in two ways.
First, it reduces the need to simulate concurrency when performingmultiple tasks.
Asmentioned, even a personal computer being used by a single person is expected to perform many activities concurrently.
Second, it can run a single application program faster, but only if that program is expressed in terms of multiple threads that can effectively execute in parallel.
Thus, although the principles of concurrency have been formulated and studied for over 50 years, the advent ofmulti-core and hyperthreaded systems has greatly increased the desire to ﬁnd ways to write application programs that can exploit the thread-level parallelism available with the hardware.
Chapter 12 will look much more deeply into concurrency and its use to provide a sharing of processing resources and to enable more parallelism in program execution.
At a much lower level of abstraction, modern processors can execute multiple instructions at one time, a property known as instruction-level parallelism.
More recent processors can sustain execution rates of 2–4 instructions per clock cycle.
InChapter 4, wewill explore the use of pipelining, where the actions required to execute an instruction are partitioned into different steps and the processor hardware is organized as a series of stages, each performing one of these steps.
The stages can operate in parallel, working on different parts of different instructions.
We will see that a fairly simple hardware design can sustain an execution rate close to one instruction per clock cycle.
In Chapter 5, we will describe a high-level model of such processors.
We will see that application programmers can use this model to understand the performance of their programs.
They can then write programs such that the generated code achieves higher degrees of instruction-level parallelism and therefore runs faster.
For example, recent generations of Intel and AMD processors have instructions that can add four pairs of single-precision ﬂoating-point numbers (C data type float) in parallel.
These SIMD instructions are provided mostly to speed up applications that process image, sound, and video data.
Although some compilers attempt to automatically extract SIMDparallelism fromC programs, amore reliablemethod is to write programs using special vector data types supported in compilers such as gcc.
The use of abstractions is one of themost important concepts in computer science.
Different programming languages provide different forms and levels of support for abstraction, such as Java class declarations and C function prototypes.
We have already been introduced to several of the abstractions seen in computer systems, as indicated in Figure 1.18
On the processor side, the instruction set architecture provides an abstraction of the actual processor hardware.
A major theme in computer systems is to provide abstract representations at different levels to hide the complexity of the actual implementations.
The underlying hardware is far more elaborate, executing multiple instructions in parallel, but always in a way that is consistent with the simple, sequential model.
By keeping the same execution model, different processor implementations can execute the same machine code, while offering a range of cost and performance.
On the operating system side, we have introduced three abstractions: ﬁles as an abstraction of I/O, virtual memory as an abstraction of program memory, and processes as an abstraction of a running program.
To these abstractions we add a new one: the virtual machine, providing an abstraction of the entire computer, including the operating system, the processor, and the programs.
The idea of a virtual machine was introduced by IBM in the 1960s, but it has become more prominent recently as a way to manage computers that must be able to run programs designed for multiple operating systems (such as Microsoft Windows, MacOS, and Linux) or different versions of the same operating system.
We will return to these abstractions in subsequent sections of the book.
A computer system consists of hardware and systems software that cooperate to run application programs.
Information inside the computer is represented as groups of bits that are interpreted in different ways, depending on the context.
Programs are translated by other programs into different forms, beginning as ASCII text and then translated by compilers and linkers into binary executable ﬁles.
Processors read and interpret binary instructions that are stored in main memory.
Since computers spendmost of their time copying data betweenmemory, I/O devices, and the CPU registers, the storage devices in a system are arranged in a hierarchy, with the CPU registers at the top, followed by multiple levels of hardware cache memories, DRAM main memory, and disk storage.
Storage devices that are higher in the hierarchy are faster and more costly per bit than those lower in the hierarchy.
Storage devices that are higher in the hierarchy serve as caches for devices that are lower in the hierarchy.
Programmers can optimize the performance of their C programs by understanding and exploiting thememory hierarchy.
The operating system kernel serves as an intermediary between the application and the hardware.
It provides three fundamental abstractions: (1) Files are abstractions for I/O devices.
Virtual memory is an abstraction for both main memory and disks.
Processes are abstractions for the processor, mainmemory, and I/O devices.
Finally, networks provide ways for computer systems to communicate with one another.
From the viewpoint of a particular system, the network is just another I/O device.
Ritchie and Thompson presented the ﬁrst published account of Unix [89]
Our exploration of computer systems starts by studying the com-puter itself, comprising a processor and a memory subsystem.
Atthe core, we require ways to represent basic data types, such as approximations to integer and real arithmetic.
From there we can consider how machine-level instructions manipulate data and how a compiler translates C programs into these instructions.
Next, we study several methods of implementing a processor to gain a better understanding of how hardware resources are used to execute instructions.
Once we understand compilers and machine-level code, we can examine how to maximize program performance by writing C programs that, when compiled, achieve the maximum possible performance.
We conclude with the design of the memory subsystem, one of the most complex components of a modern computer system.
This part of the book will give you a deep understanding of how application programs are represented and executed.
You will gain skills that help you write programs that are secure, reliable, and make the best use of the computing resources.
Modern computers store and process information represented as 2-valued signals.
These lowly binary digits, or bits, form the basis of the digital revolution.
Using decimal notation is natural for ten-ﬁngered humans, but binary values work better when building machines that store and process information.
Two-valued signals can readily be represented, stored, and transmitted, for example, as the presence or absence of a hole in a punched card, as a high or low voltage on a wire, or as a magnetic domain oriented clockwise or counterclockwise.
The electronic circuitry for storing and performing computations on 2-valued signals is very simple and reliable, enabling manufacturers to integrate millions, or even billions, of such circuits on a single silicon chip.
When we group bits together and apply some interpretation that gives meaning to the different possible bit patterns, however, we can represent the elements of any ﬁnite set.
For example, using a binary number system, we can use groups of bits to encode nonnegative numbers.
By using a standard character code, we can encode the letters and symbols in a document.
We cover both of these encodings in this chapter, as well as encodings to represent negative numbers and to approximate real numbers.
Two’s-complement encodings are the most common way to represent signed integers, that is, numbers that may be either positive or negative.
Floating-point encodings are a base-two version of scientiﬁc notation for representing real numbers.
Computers implement arithmetic operations, such as addition and multiplication, with these different representations, similar to the corresponding operations on integers and real numbers.
Computer representations use a limited number of bits to encode a number, and hence some operations can overﬂow when the results are too large to be represented.
For example, onmost of today’s computers (those using a 32-bit representation of data type int), computing the expression.
The computer might not generate the expected result, but at least it is consistent! Floating-point arithmetic has altogether different mathematical properties.
By studying the actual number representations, we can understand the ranges of values that can be represented and the properties of the different arithmetic operations.
Whereas in an earlier era program bugs would only inconvenience people when they happened to be triggered, there are now legions of hackers who try to exploit any bug they can ﬁnd to obtain unauthorized access to other people’s systems.
This puts a higher level of obligation on programmers to understand how their programs work and how they can be made to behave in undesirable ways.
Computers use several different binary representations to encode numeric values.
We describe these encodings in this chapter and show you how to reason about number representations.
We derive several ways to perform arithmetic operations by directly manipulating the bit-level representations of numbers.
Understanding these techniques will be important for understanding the machine-level code generated by compilers in their attempt to optimize the performance of arithmetic expression evaluation.
Our treatment of this material is based on a core set of mathematical principles.
We start with the basic deﬁnitions of the encodings and then derive such properties as the range of representable numbers, their bit-level representations, and the properties of the arithmetic operations.
We believe it is important for you to examine the material from this abstract viewpoint, because programmers need to have a clear understanding of how computer arithmetic relates to the more familiar integer and real arithmetic.
If you ﬁnd equations and formulas daunting, do not let that stop you from getting the most out of this chapter! We provide full derivations of mathematical ideas for completeness, but the best way to read this material is often to skip over the derivation on your initial reading.
The examples will give you an intuition behind the ideas, and the practice problems engage you in active learning, helping you put thoughts into action.With these as background, youwill ﬁnd it much easier to go back and follow the derivations.
Be assured, as well, that the mathematical skills required to understand this material are within reach of someone with good grasp of high school algebra.
TheC++programming language is built uponC, using the exact same numeric representations and operations.
Everything said in this chapter about C also holds for C++
The Java language deﬁnition, on the other hand, created a new set of standards for numeric representations and operations.
Whereas the C standards are designed to allow a wide range of implementations, the Java standard is quite speciﬁc on the formats and encodings of data.
We highlight the representations and operations supported by Java at several places in the chapter.
Aswas described in an aside in Section 1.2, the C programming language was ﬁrst developed byDennis Ritchie of Bell Laboratories for use with the Unix operating system (also developed at Bell Labs)
At the time, most system programs, such as operating systems, had to be written largely in assembly code, in order to have access to the low-level representations of different data types.
For example, it was not feasible to write a memory allocator, such as is provided by the malloc library function, in other high-level languages of that era.
The original Bell Labs version of C was documented in the ﬁrst edition of the book by Brian Kernighan and Dennis Ritchie [57]
Over time, C has evolved through the efforts of several standardization groups.
The ﬁrst major revision of the original Bell Labs C led to the ANSI C standard in 1989, by a group working under the auspices of the American National Standards Institute.
For example, to compile program prog.c according to ISO C99, we could give the command line.
This version can be speciﬁed explicitly using the option -std=gnu89
Rather than accessing individual bits in memory, most computers use blocks of eight bits, or bytes, as the smallest addressable unit of memory.
A machinelevel program views memory as a very large array of bytes, referred to as virtual memory.
Every byte of memory is identiﬁed by a unique number, known as its address, and the set of all possible addresses is known as the virtual address space.
As indicated by its name, this virtual address space is just a conceptual image presented to the machine-level program.
The actual implementation (presented in Chapter 9) uses a combination of random-access memory (RAM), disk storage, special hardware, andoperating system software to provide the programwithwhat appears to be a monolithic byte array.
In subsequent chapters, we will cover how the compiler and run-time system partitions this memory space into more manageable units to store the different program objects, that is, program data, instructions, and control information.
Various mechanisms are used to allocate and manage the storage for different parts of the program.
This management is all performed within the virtual address space.
For example, the value of a pointer in C—whether it points to an integer, a structure, or some other program object—is the virtual address of the ﬁrst byte of some block of storage.
The C compiler also associates type information with each pointer, so that it can generate different machine-level code to access the value stored at the location designated by the pointer depending on the type of that value.
Although the C compiler maintains this type information, the actual machine-level program it generates has no information about data types.
It simply treats each programobject as a block of bytes, and the program itself as a sequence of bytes.
They provide the mechanism for referencing elements of data structures, including arrays.
Just like a variable, a pointer has two aspects: its value and its type.
The value indicates the location of some object, while its type indicates what kind of object (e.g., integer or ﬂoating-point number) is stored at that location.
Binary notation is too verbose, while with decimal notation, it is tedious to convert to and from bit patterns.
Instead, we write bit patterns as base-16, or hexadecimal numbers.
We will use the C notation for representing hexadecimal values in this book.
A common task in working with machine-level programs is to manually convert between decimal, binary, and hexadecimal representations of bit patterns.
Converting between binary and hexadecimal is straightforward, since it can be performed one hexadecimal digit at a time.
Digits can be converted by referring to a chart such as that shown in Figure 2.2
One simple trick for doing the conversion in your head is to memorize the decimal equivalents of hex digits A, C, and F.
The hex values B, D, and E can be translated to decimal by computing their values relative to the ﬁrst three.
You can convert this to binary format by expanding each hexadecimal digit, as follows:
Then you translate each group of 4 bits into the corresponding hexadecimal digit:
Converting between decimal and hexadecimal representations requires using multiplication or division to handle the general case.
To convert a decimal number x to hexadecimal, we can repeatedly divide x by 16, giving a quotient q and a remainder r , such that x = q.
From this we can read off the hexadecimal representation as 0x4CB2C.
Conversely, to convert a hexadecimal number to decimal, we can multiply.
Practice Problem 2.3 A single byte can be represented by two hexadecimal digits.
Fill in the missing entries in the following table, giving the decimal, binary, and hexadecimal values of different byte patterns:
For converting larger values between decimal and hexadecimal, it is best to let a computer or calculator do the work.
For example, the following script in the Perl language converts a list of numbers (given on the command line) from decimal to hexadecimal:
Once this ﬁle has been set to be executable, the command.
Practice Problem 2.4 Without converting the numbers to decimal or binary, try to solve the following arithmetic problems, giving the answers in hexadecimal.
Computers and compilers support multiple data formats using different ways to encode data, such as integers and ﬂoating point, as well as different lengths.
The C language supports multiple data formats for both integer and ﬂoatingpoint data.
Although the name “char” derives from the fact that it is used to store a single character in a text string, it can also be used to store integer values.
The C data type int can also be preﬁxed by the qualiﬁers short, long, and recently long long, providing integer representations of various sizes.
Figure 2.3 Sizes (in bytes) of C numeric data types.
The number of bytes allocated varies with machine and compiler.
The exact number depends on both the machine and the compiler.
A “long” integer uses the full word size of the machine.
Most machines also support two different ﬂoating-point formats: single precision, declared in C as float, and double precision, declared in C as double.
Programmers should strive to make their programs portable across different machines and compilers.
One aspect of portability is to make the program insensitive to the exact sizes of the different data types.
The C standards set lower bounds on the numeric ranges of the different data types, as will be covered later, but there are no upper bounds.
Given the increasing availability of 64-bit machines, many hidden word size dependencies will show up as bugs in migrating these programs to new machines.
For example, many programmers assume that a program object declared as type int can be used to store a pointer.
For program objects that span multiple bytes, we must establish two conventions: what the address of the object will be, and how we will order the bytes in memory.
In virtually all machines, a multi-byte object is stored as a contiguous sequence of bytes, with the address of the object given by the smallest address of the bytes used.
People get surprisingly emotional aboutwhichbyte ordering is the proper one.
In fact, the terms “little endian” and “big endian” come from the book Gulliver’s Travels by Jonathan Swift, where two warring factions could not agree as to how a soft-boiled egg should be opened—by the little end or by the big.
Just like the egg issue, there is no technological reason to chooseonebyte ordering conventionover theother, andhence the arguments degenerate intobickering about socio-political issues.
As long as one of the conventions is selected and adhered to consistently, the choice is arbitrary.
Here is how Jonathan Swift, writing in 1726, described the history of the controversy between big and little endians:
It is allowed on all hands, that the primitive way of breaking eggs, before we eat them, was upon the larger end; but his present majesty’s grandfather, while he was a boy, going to eat an egg, and breaking it according to the ancient practice, happened to cut one of his ﬁngers.
Whereupon the emperor his father published an edict, commanding all his subjects, upon great penalties, to break the smaller end of their eggs.
The people so highly resented this law, that our histories tell us, there have been six rebellions raised on that account; wherein one emperor lost his life, and another his crown.
These civil commotions were constantly fomented by the monarchs of Blefuscu; and when they were quelled, the exiles always ﬂed for refuge to that empire.
It is computed that eleven thousand persons have at several times suffered death, rather than submit to break their eggs at the smaller end.
Many hundred large volumes have been published upon this controversy: but the books of the Big-endians have been long forbidden, and the whole party rendered incapable by law of holding employments.
Danny Cohen, an early pioneer in networking protocols, ﬁrst applied these terms to refer to byte ordering [25], and the terminology has been widely adopted.
For most application programmers, the byte orderings used by their machines are totally invisible; programs compiled for either class of machine give identical results.
The ﬁrst is when binary data are communicated over a network between different machines.
A common problem is for data produced by a little-endian machine to be sent to a big-endian machine, or vice versa, leading to the bytes within the words being in reverse order for the receiving program.
To avoid such problems, code written for networking applications must follow established conventions for byte ordering to make sure the sendingmachine converts its internal representation to the network standard, while the receivingmachine converts thenetwork standard to its internal representation.
A second case where byte ordering becomes important is when looking at the byte sequences representing integer data.
As an example, the following line occurs in a ﬁle that gives a text representation of the machine-level code for an Intel IA32 processor:
This line was generated by a disassembler, a tool that determines the instruction sequence represented by an executable program ﬁle.
Having bytes appear in reverse order is a common occurrence when reading machine-level program representations generated for little-endian.
Figure 2.4 Code to print the byte representation of program objects.
The natural way to write a byte sequence is to have the lowest-numbered byte on the left and the highest on the right, but this is contrary to the normal way of writing numbers with the most signiﬁcant digit on the left and the least on the right.
A third case where byte ordering becomes visible is when programs are written that circumvent the normal type system.
In the C language, this can be done using a cast to allow an object to be referenced according to a different data type from which it was created.
Such coding tricks are strongly discouraged for most application programming, but they can be quite useful and even necessary for system-level programming.
Figure 2.4 shows C code that uses casting to access and print the byte representations of different program objects.
We use typedef to deﬁne data type byte_pointer as a pointer to an object of type “unsigned char.” Such a byte pointer references a sequence of bytes where each byte is considered to be a nonnegative integer.
The ﬁrst routine show_bytes is given the address of a sequence of bytes, indicated by a byte pointer, and a byte count.
The C formatting directive “%.2x” indicates that an integer should be printed in hexadecimal with at least two digits.
The typedef declaration in C provides a way of giving a name to a data type.
This can be a great help in improving code readability, since deeply nested type declarations can be difﬁcult to decipher.
The syntax for typedef is exactly like that of declaring a variable, except that it uses a type name rather than a variable name.
The printf function (along with its cousins fprintf and sprintf) provides a way to print information with considerable control over the formatting details.
The ﬁrst argument is a format string, while any remaining arguments are values to be printed.
Within the format string, each character sequence starting with ‘%’ indicates how to format the next argument.
In C, we can dereference a pointer with array notation, and we can reference array elements with pointer notation.
In this example, the reference start[i] indicates that we want to read the byte that is i positions beyond the location pointed to by start.
This pointer will then be to the lowest byte address occupied by the object.
On all three lines, the expression &x creates a pointer to the location holding the object indicated by variable x.
Data type void * is a special kind of pointer with no associated type information.
The cast operator converts from one data type to another.
The casts shown here do not change the actual pointer; they simply direct the compiler to refer to the data being pointed to according to the new data type.
In general, the expression sizeof(T ) returns the number of bytes required to store an object of type T.
Using sizeof rather than a ﬁxed value is one step toward writing code that is portable across different machine types.
For the int data, we get identical results for all machines, except for the byte ordering.
Similarly, the bytes of the float data are identical, except for the byte ordering.
On the other hand, the pointer values are completely different.
This code prints the byte representations of sample data objects.
Results for int and float are identical, except for byte ordering.
If we expand these hexadecimal patterns into binary form and shift them appropriately, we ﬁnd a sequence of 13 matching bits, indicated by a sequence of asterisks, as follows:
We will return to this example when we study ﬂoatingpoint formats.
Indicate which of the following values will be printed by each call on a littleendian machine and on a big-endian machine:
Shift these two strings relative to one another to maximize the number of matching bits.
A string in C is encoded by an array of characters terminated by the null (having value 0) character.
Each character is represented by some standard encoding, with the most common being the ASCII character code.
This same result would be obtained on any system using ASCII as its character code, independent of the byte ordering and word size conventions.
You can display a table showing the ASCII character code by executing the command man ascii.
The base encoding, known as the “Universal Character Set” of Unicode, uses a 32-bit representation of characters.
This would seem to require every string of text to consist of 4 bytes per character.
The Java programming language uses Unicode in its representations of strings.
Program libraries are also available for C to support Unicode.
When compiled on our sample machines, we generate machine code having the following byte representations:
Different machine types use different and incompatible instructions and encodings.
Even identical processors running different operating systems have differences in their coding conventions and hence are not binary compatible.
Binary code is seldom portable across different combinations of machine and operating system.
A fundamental concept of computer systems is that a program, from the perspective of the machine, is simply a sequence of bytes.
Claude Shannon (1916–2001), who later founded the ﬁeld of information theory, ﬁrst made the connection between Boolean algebra and digital logic.
In his 1937 master’s thesis, he showed that Boolean algebra could be applied to the design and analysis of networks of electromechanical relays.
Although computer technology has advanced considerably since, Boolean algebra still plays a central role in the design and analysis of digital systems.
Practice Problem 2.8 Fill in the following table showing the results of evaluating Boolean operations on bit vectors.
Boolean algebra has many of the same properties as arithmetic over integers.
For example, just as multiplication distributes over addition, written a.
We will see the encoding of sets by bit vectors in a number of practical applications.
For example, in Chapter 8, we will see that there are a number of different signals that can interrupt the execution of a program.
Practice Problem 2.9 Computers generate color pictures on a video screen or liquid crystal display by mixing three different colors of light: red, green, and blue.
Imagine a simple scheme, with three different lights, each of which can be turned on or off, projecting onto a glass screen:
Each of these colors can be represented as a bit vector of length 3, and we can apply Boolean operations to them.
The complement of a color is formed by turning off the lights that are on and turning on the lights that are off.
What would be the complement of each of the eight colors listed above?
Describe the effect of applying Boolean operations on the following colors:
One useful feature of C is that it supports bit-wise Boolean operations.
These can be applied to any “integral” data type, that is, one declared as type char or int, with or without qualiﬁers such as short, long, long long, or unsigned.
Here are some examples of expression evaluation for data type char:
As our examples show, the best way to determine the effect of a bit-level expression is to expand the hexadecimal arguments to their binary representations, perform the operations in binary, and then convert back to hexadecimal.
As the name implies, we claim that the effect of this procedure is to swap the values stored at the locations denoted by pointer variables x and y.
Note that unlike the usual technique for swapping two values, we do not need a third location to temporarily store one value while we are moving the other.
There is no performance advantage to this way of swapping; it is merely an intellectual amusement.
Startingwith values a and b in the locations pointed to by x and y, respectively, ﬁll in the table that follows, giving the values stored at the two locations after each step of the procedure.
Use the properties of ^ to show that the desired effect is achieved.
In fact, you discover that the code always works correctly on arrays of even length, but it sets the middle element to 0 whenever the array has odd length.
What simple modiﬁcation to the code for reverse_array would eliminate this problem?
One common use of bit-level operations is to implementmasking operations, where a mask is a bit pattern that indicates a selected set of bits within a word.
The expression ~0 will yield a mask of all ones, regardless of the word size of.
All but the least signiﬁcant byte of x complemented, with the least signiﬁcant byte left unchanged.
The least signiﬁcant byte set to all 1s, and all other bytes of x left unchanged.
Rather than instructions for Boolean operations And and Or, it had instructions bis (bit set) and bic (bit clear)
Both instructions take a data word x and a mask word m.
They generate a result z consisting of the bits of x modiﬁed according to the bits of m.
Hint: Write C expressions for the operations bis and bic.
These can easily be confused with the bit-level operations, but their function is quite different.
The logical operations treat any nonzero argument as representing True and argument 0 as representing False.
Fill in the following table indicating the byte values of the different C expressions:
Practice Problem 2.15 Using only bit-level and logical operations, write a C expression that is equivalent to x == y.
As examples, the following table shows the effect of applying the different shift operations to some sample 8-bit data:
The italicized digits indicate the values that ﬁll the right (left shift) or left (right shift) ends.Observe that all but one entry involves ﬁllingwith zeros.
The exception is the case of shifting [10010101] right arithmetically.
Since its most signiﬁcant bit is 1, this will be used as the ﬁll value.
The C standards do not precisely deﬁne which type of right shift should be used.
For unsigned data (i.e., integral objects declared with the qualiﬁer unsigned), right shifts must be logical.
For signed data (the default), either arithmetic or logical shifts may be used.
This unfortunately means that any code assuming one form or the other will potentially encounter portability problems.
In practice, however, almost all compiler/machine combinations use arithmetic right shifts for signed data, and many programmers assume this to be the case.
Java, on the other hand, has a precise deﬁnition of how right shifts should be performed.
TheC standards carefully avoid stating what should be done in such a case.
Onmanymachines, the shift instructions consider only the lower log2 w bits of the shift amount when shifting a w-bit value, and so the shift amount is effectively computed as k mod w.
This behavior is not guaranteed for C programs, however, and so shift amounts should be kept less than the word size.
Java, on the other hand, speciﬁcally requires that shift amounts should be computed in themodular fashion we have shown.
Getting the precedence wrong in C expressions is a common source of program errors, and often these are difﬁcult to spot by inspection.
Practice Problem 2.16 Fill in the table below showing the effects of thedifferent shift operations on singlebyte quantities.
The best way to think about shift operations is to work with binary representations.
Convert the initial values to binary, perform the shifts, and then convert back to hexadecimal.
Each type can specify a size with keyword char, short, long, or long long, as well as an indication of whether the represented numbers are all nonnegative (declared as unsigned), or possibly.
The C standards require that the data types have at least these ranges of values.
As we saw in Figure 2.3, the number of bytes allocated for the different sizes vary according to machine’s word size and the compiler.
Based on the byte allocations, the different sizes allow different ranges of values to be represented.
The only machine-dependent range indicated is for size designator long.
We will see why this happens when we consider how negative numbers are represented.
The C standards deﬁne minimum ranges of values that each data type must be able to represent.
In this equation, the notation “ .=” means that the left-hand side is deﬁned to be equal to the right-hand side.
The function B2Uw maps strings of zeros and ones of lengthw to nonnegative integers.
In the ﬁgure, we represent each bit position i by a rightward-pointing blue bar of length 2i.
Let us consider the range of values that can be represented using w bits.
Themost common computer representation of signed numbers is known as two’s-complement form.
This is deﬁned by interpreting the most signiﬁcant bit of the word to have negative weight.
We express this interpretation as a function B2Tw (for “binary to two’s-complement” length w):
In the ﬁgure, we indicate that the sign bit has negative weight by showing it as a leftward-pointing gray bar.
The numeric value associated with a bit vector is then given by the combination of the possible leftward-pointing gray bar and the rightward-pointing blue bars.
Let us consider the range of values that can be represented as a w-bit two’scomplement number.
Figure 2.13 shows the bit patterns and numeric values for several important numbers for different word sizes.
The ﬁrst three give the ranges of representable integers in terms of the values of UMaxw, TMinw, and TMaxw.
We will refer to these three special values often in the ensuing discussion.
We will drop the subscriptw and refer to the valuesUMax,TMin, andTMaxwhenw canbe inferred from context or is not central to the discussion.
As we shall see, this leads to some peculiar properties of two’s-complement arithmetic and can be.
The C standards do not require signed integers to be represented in two’scomplement form, but nearly all machines do so.
The ﬁle <limits.h> in the C library deﬁnes a set of constants delimiting the ranges of the different integer data types for the particular machine on which the compiler is running.
For a two’s-complement machine in which data type int has w bits, these constants correspond to the values of TMaxw, TMinw, and UMaxw.
For some programs, it is essential that data types be encoded using representations with speciﬁc sizes.
For example, when writing programs to enable a machine to communicate over the Internet according to a standard protocol, it is important to have data types compatiblewith those speciﬁed by the protocol.
We have seen that some C data types, especially long, have different ranges on different machines, and in fact the C standards only specify the minimum ranges for any data type, and not the exact ranges.
Although we can choose data types that will be compatible with standard representations on most machines, there is not guarantee of portability.
The ISO C99 standard introduces another class of integer types in the ﬁle stdint.h.
Along with these data types are a set of macros deﬁning the minimum and maximum values for each value of N.
The Java standard is quite speciﬁc about integer data type ranges and representations.
In Java, the single-byte data type is called byte instead of char, and there is no long long data type.
These detailed requirements are intended to enable Java programs to behave identically regardless of the machines running them.
Sign-Magnitude: The most signiﬁcant bit is a sign bit that determines whether the remaining bits should be given negative or positive weight:
These ﬁles contain many hexadecimal numbers, typically representing values in two’scomplement form.
For the lines labeled A–J (on the right) in the following listing, convert the hexadecimal values (in 32-bit two’s-complement form) shown to the right of the instruction names (sub, mov, and add) into their decimal equivalents:
For example, suppose variable x is declared as int and u as unsigned.
The expression (unsigned) x converts the value of x to an unsigned value, and (int) u converts the value of u to a signed integer.
What should be the effect of casting signed value to unsigned, or vice versa? From amathematical perspective, one can imagine several different conventions.
Clearly, we want to preserve any value that can be represented in both forms.On theother hand, converting a negative value to unsignedmight yield zero.
Converting an unsigned value that is too large to be represented in two’scomplement form might yield TMax.
For most implementations of C, however, the answer to this question is based on a bit-level perspective, rather than on a numeric one.
When run on a two’s-complement machine, it generates the following output:
What we see here is that the effect of casting is to keep the bit values identical but change how these bits are interpreted.
When run on a two’s-complement machine, it generates the following output:
This is a general rule for how most C implementations handle conversions between signed and unsigned numbers with the same word size—the numeric values might change, but the bit patterns do not.
Let us capture this principle in a more mathematical form.
These describe the effect of casting between these data types in most C implementations.
As it shows, when mapping a signed number to its unsigned counterpart, negative numbers are converted to large positive numbers, while nonnegative numbers remain unchanged.
Although the C standard does not specify a particular representation of signed numbers, almost all machines use two’s complement.
The rule is that the underlying bit representation is not changed.
Conversions can happen due to explicit casting, such as in the following code:
Alternatively, they can happen implicitly when an expression of one type is assigned to a variable of another, as in the following code:
When printing numeric values with printf, the directives %d, %u, and %x are used to print a number as a signed decimal, an unsigned decimal, and in hexadecimal format, respectively.
Note that printf does notmake use of any type information, and so it is possible to print a value of type int with directive %u and a value of type unsigned with directive %d.
When run on a 32-bit machine, it prints the following:
Unfortunately, a curious interaction between the asymmetry of the two’s-complement representation and the conversion rules of C force us to write TMin32 in this unusual way.
Although understanding this issue requires us to delve into one of the murkier corners of the C language standards, it will help us appreciate some of the subtleties of integer data types and representations.
One common operation is to convert between integers having different word sizes while retaining the same numeric value.
Of course, this may not be possible when the destination data type is too small to represent the desired value.
Converting froma smaller to a larger data type, however, should always bepossible.
When either operand of a comparison is unsigned, the other operand is implicitly cast to unsigned.
When run on a 32-bit big-endian machine using a two’s-complement representation, this code prints the output.
Can we justify that sign extension works? What we want to prove is that.
Expanding the left-hand expression with Equation 2.3 gives the following:
Observe that the second and third bit vectors can be derived from the ﬁrst by sign extension.
One point worth making is that the relative order of conversion from one data size to another and between unsigned and signed can affect the behavior of a program.
When run on a big-endian machine, this code causes the following output to be printed:
This shows that when converting from short to unsigned, we ﬁrst change the size and then from signed to unsigned.
Assume these are executed on a machine with a 32-bit word size that uses two’scomplement arithmetic.
Fill in the following table showing the effect of these functions for several example arguments.
You will ﬁnd it more convenient to work with a hexadecimal representation.
Describe in words the useful computation each of these functions performs.
Suppose that, rather than extending a value with extra bits, we reduce the number of bits representing a number.
Fill in the table below showing the effect of this truncation for some cases, in terms of the unsigned and two’scomplement interpretations of those bit patterns.
As we have seen, the implicit casting of signed to unsigned leads to some nonintuitive behavior.
Nonintuitive features often lead to program bugs, and ones involving the nuances of implicit casting can be especially difﬁcult to see.
Since the casting takes place without any clear indication in the code, programmers often overlook its effects.
The following two practice problems illustrate some of the subtle errors that can arise due to implicit casting and the unsigned data type.
Practice Problem 2.25 Consider the following code that attempts to sum the elements of an array a, where the number of elements is given by parameter length:
Practice Problem 2.26 You are given the assignment of writing a function that determines whether one string is longer than another.
You decide tomake use of the string library function strlen having the following declaration:
Determine whether string s is longer than string t */
When you test this on some sample data, things do not seem to work quite right.
You investigate further and determine that data type size_t is deﬁned (via typedef) in header ﬁle stdio.h to be unsigned int.
For what cases will this function produce an incorrect result?
Show how to ﬁx the code so that it will work reliably.
In 2002, programmers involved in the FreeBSD open source operating systems project realized that their implementation of the getpeername library function had a security vulnerability.
A simpliﬁed version of their code went something like this:
Copy at most maxlen bytes from kernel region to user buffer */
Byte count len is minimum of buffer size and maxlen */
In this code, we show the prototype for library function memcpy on line 7, which is designed to copy a speciﬁed number of bytes n from one region of memory to another.
Most of the data structures maintained by the kernel should not be readable by a user, since they may contain sensitive information about other users and about other jobs running on the system, but the region shown as kbufwas intended to be one that the user could read.
The parameter maxlen is intended to be the length of the buffer allocated by the user and indicated by argument user_dest.
The computation at line 16 then makes sure that no more bytes are copied than are available in either the source or the destination buffer.
Then the minimum computation on line 16 will compute this value for len, which will then be passed as the parameter n to memcpy.
Note, however, that parameter n is declared as having data type size_t.
This data type is declared (via typedef) in the library ﬁle stdio.h.
Typically it is deﬁned to be unsigned int on 32-bit machines.
Since argument n is unsigned, memcpy will treat it as a very large, positive number and attempt to copy that many bytes from the kernel region to the user’s buffer.
Copying that many bytes (at least 231) will not actually work, because the program will encounter invalid addresses in the process, but the program could read regions of the kernel memory for which it is not authorized.
We can see that this problem arises due to the mismatch between data types: in one place the length parameter is signed; in another place it is unsigned.
Such mismatches can be a source of bugs and, as this example shows, can even lead to security vulnerabilities.
Fortunately, therewere no reported cases where a programmer had exploited the vulnerability in FreeBSD.
We should also declare local variable len and the return value to be of type size_t.
We have seen multiple ways in which the subtle features of unsigned arithmetic, and especially the implicit conversion of signed to unsigned, can lead to errors or vulnerabilities.
One way to avoid such bugs is to never use unsigned numbers.
In fact, few languages other than C support unsigned integers.
Apparently these other language designers viewed them as more trouble than they are worth.
For example, Java supports only signed integers, and it requires that they be implemented with two’s-complement arithmetic.
The normal right shift operator >> is guaranteed to perform an arithmetic shift.
The special operator >>> is deﬁned to perform a logical right shift.
Unsigned values are very useful when we want to think of words as just collections of bits with no numeric interpretation.
This occurs, for example, when packing a word with ﬂags describing various Boolean conditions.
Addresses are naturally unsigned, so systems programmers ﬁnd unsigned types to be helpful.
Unsigned values are also useful when implementing mathematical packages for modular arithmetic and for multiprecision arithmetic, in which numbers are represented by arrays of words.
These properties are artifacts of the ﬁnite nature of computer arithmetic.
Understanding the nuances of computer arithmetic can help programmers write more reliable code.
This continued “word size inﬂation” means we cannot place any bound on the word size required to fully represent the results of arithmetic operations.
Some programming languages, such as Lisp, actually support inﬁnite precision arithmetic to allow arbitrary (within the memory limits of the machine, of course) integer arithmetic.
More commonly, programming languages support ﬁxed-precision arithmetic, and hence operations such as “addition” and “multiplication” differ from their counterpart operations over integers.
This value can be computed by simply discarding the high-order bit in the w+1-bit representation of x + y.
This is precisely the result we get in C when performing addition on two w-bit unsigned values.
Practice Problem 2.27 Write a function with the following prototype:
This function should return 1 if arguments x and y can be added without causing overﬂow.
Modular addition forms a mathematical structure known as an abelian group, named after the Danish mathematician Niels Henrik Abel (1802–1829)
That is, it is commutative (that’s where the “abelian” part comes in) and associative; it has an identity element 0, and every element has an additive inverse.
Let us consider the set of w-bit unsigned numbers with addition operation +u.
For an unsigned interpretation of these digits, use Equation 2.12 to ﬁll in the following.
Thew-bit two’s-complement sum of two numbers has the exact same bit-level representation as the unsigned sum.
In fact, most computers use the samemachine instruction to perform either unsigned or signed addition.
Thus, we can deﬁne two’s-complement addition for word size w, denoted as +t.
Each example is labeled by the case to which it corresponds in the derivation of Equation 2.14
We include bit-level representations of the operands and the result.
Observe that the result can be obtained by performing binary addition of the operands and truncating the result to four bits.
Equation 2.14 also lets us identify the cases where overﬂow has occurred.
When both x and y are negative but x +t.
Practice Problem 2.30 Write a function with the following prototype:
This function should return 1 if arguments x and y can be added without causing overﬂow.
Having just written the code for Problem 2.30, you write the following:
Forwhat values ofx andywill this function give incorrect results?Writing a correct version of this function is left as an exercise (Problem 2.74)
For a two’scomplement interpretation of these digits, ﬁll in the following table to determine the additive inverses of the digits shown:
What do you observe about the bit patterns generated by two’s-complement and unsigned (Problem 2.28) negation?
There are several cleverways to determine the two’s-complement negation of a value represented at the bit level.
These techniques are both useful, such as when one encounters the value 0xfffffffa when debugging a program, and they lend insight into the nature of the two’s-complement representation.
One technique for performing two’s-complement negation at the bit level is to complement the bits and then increment the result.
The bitlevel representations of both truncated products are identical for both unsigned and two’s-complement multiplication, even though the full 6-bit representations differ.
Although the bit-level representations of the full products may differ, those of the truncated products are identical.
We can see that unsigned arithmetic and two’s-complement arithmetic over w-bit numbers are isomorphic—the operations +u.
Either x is zero, or dividing p by x gives y */
You test this code for a number of values of x and y, and it seems to work properly.
Your coworker challenges you, saying, “If I can’t use subtraction to test whether addition has overﬂowed (see Problem 2.31), then how can you use division to test whether multiplication has overﬂowed?”
Devise a mathematical justiﬁcation of your approach, along the following lines.
First, argue that the case x = 0 is handled correctly.
Show that p can be written in the form p = x.
In 2002, it was discovered that code supplied by Sun Microsystems to implement the XDR library, a widely used facility for sharing data structures between programs, had a security vulnerability arising from the fact that multiplication can overﬂow without any notice being given to the program.
Code similar to that containing the vulnerability is shown below:
The loop starting at line 16 will attempt to copy all of those bytes, overrunning the end of the allocated buffer, and therefore corrupting other data structures.
This could cause the program to crash or otherwise misbehave.
The Sun code was used by almost every operating system, and in such widely used programs as Internet Explorer and the Kerberos authentication system.
The Computer Emergency Response Team (CERT), an organization run by the Carnegie Mellon Software Engineering Institute to track security vulnerabilities and breaches, issued advisory “CA-2002-25,” andmany companies rushed to patch their code.
Fortunately, there were no reported security breaches caused by this vulnerability.
A similar vulnerability existed inmany implementations of the library function calloc.
Practice Problem 2.37 You are given the task of patching the vulnerability in theXDRcode shown above.
You decide to eliminate the possibility of the multiplication overﬂowing (on a 32bit machine, at least) by computing the number of bytes to allocate using data type long long unsigned.
You replace the original call to malloc (line 10) as follows:
As a consequence, one important optimization used by compilers is to attempt to replace multiplications by constant factors with combinations of shift and addition operations.
We will ﬁrst consider the case of multiplying by a power of 2, and then generalize this to arbitrary constants.
Note that multiplying by a power of 2 can cause overﬂowwith either unsigned.
Our result shows that even then we will get the same effect by shifting.
Considering cases where b is either 0 or equal to a, and all possible values of k, what multiples of a can be computed with a single lea instruction?
Generalizing from our example, consider the task of generating code for the expression x * K , for some constant K.
The compiler can express the binary representation of K as an alternating sequence of zeros and ones:
By adding together the results for each run, we are able to compute x * K without any multiplications.
Most compilers only perform this optimization when a small number of shifts, adds, and subtractions sufﬁce.
Practice Problem 2.39 How could we modify the expression for form B for the case where bit position n is the most signiﬁcant bit?
Youmay need to use some tricks beyond the simple form A and B rules we have considered so far.
The examples illustrate how performing a logical right shift by k has the same effect as dividing by 2k and then rounding toward zero.
Dividing by a power of 2 can also be performed using shift operations, but we use a right shift rather than a left shift.
The two different shifts—logical and arithmetic—serve this purpose for unsigned and two’scomplement numbers, respectively.
Consider the effect of applying a logical right shift by k to an unsigned number.
We claim this gives the same result as dividing by 2k.
The zeros shifted in from the left are shown in italics.
We also show the result we would obtain if we did these divisions with real arithmetic.
These examples show that the result of shifting consistently rounds toward zero, as is the convention for integer division.
This analysis shows that for a two’s-complement machine using arithmetic right shifts, the C expression.
Figure 2.29 demonstrates how adding the appropriate bias before performing.
You may assume that data type int is 32 bits long and uses a two’s-complement representation, and that right shifts are performed arithmetically.
We now see that division by a power of 2 can be implemented using logical or arithmetic right shifts.
This is precisely the reason the two types of right shifts are available on most machines.
Unfortunately, this approach does not generalize to division by arbitrary constants.
Practice Problem 2.43 In the following code, we have omitted the deﬁnitions of constants M and N:
We compiled this code for particular values of M and N.
The compiler optimized the multiplication and division using the methods we have discussed.
The following is a translation of the generated machine code back into C:
As we have seen, the “integer” arithmetic performed by computers is really a form of modular arithmetic.
The ﬁnite word size used to represent numbers limits the range of possible values, and the resulting operations can overﬂow.
We have seen that some of the conventions in the C language can yield some surprising results, and these can be sources of bugs that are hard to recognize or understand.
We have especially seen that the unsigned data type, while conceptually straightforward, can lead to behaviors that even experienced programmers do not expect.
We have also seen that this data type can arise in unexpected ways, for example, when writing integer constants and when invoking library routines.
Right shifts are performed arithmetically for signed values and logically for unsigned values.
In addition, they often did not worry too much about the accuracy of the operations, viewing speed and ease of implementation as being more critical than numerical precision.
They hired William Kahan, a professor at the University of California, Berkeley, as a consultant to help design a ﬂoating-point standard for its future processors.
They allowed Kahan to join forces with a committee generating an industry-wide standard under the auspices of the Institute of Electrical and Electronics Engineers (IEEE)
Nowadays, virtually all computers support what has become known as IEEE ﬂoating point.
This has greatly improved the portability of scientiﬁc application programs across different machines.
The Institute of Electrical and Electronic Engineers (IEEE—pronounced “Eye-Triple-Eee”) is a professional society that encompasses all of electronic and computer technology.
It publishes journals, sponsors conferences, and sets up committees to deﬁne standards on topics ranging from power transmission to software engineering.
In this section, we will see how numbers are represented in the IEEE ﬂoatingpoint format.
We will also explore issues of rounding, when a number cannot be represented exactly in the format and hence must be adjusted upward or downward.
We will then explore the mathematical properties of addition, multiplication, and relational operators.
Many programmers consider ﬂoating point to be at best uninteresting and at worst arcane and incomprehensible.
We will see that since the IEEE format is based on a small and consistent set of principles, it is really quite elegant and understandable.
Practice Problem 2.45 Fill in the missing information in the following table:
The Scud struck an American Army barracks and killed 28 soldiers.
General AccountingOfﬁce (GAO) conducted a detailed analysis of the failure [72] and determined that the underlying cause was an imprecision in a numeric calculation.
In this exercise, you will reproduce part of the GAO’s analysis.
The Patriot system contains an internal clock, implemented as a counter that is incremented every 0.1 seconds.
What was the difference between the actual time and the time computed by the software?
The system predicts where an incoming missile will appear based on its velocity and the time of the last radar detection.
Given that a Scud travels at around 2000 meters per second, how far off was its prediction?
Normally, a slight error in the absolute time reported by a clock readingwould not affect a tracking computation.
Instead, it should depend on the relative time between two successive readings.
As a result, the tracking software used the accurate time for one reading and the inaccurate time for the other [100]
The bit representation of a ﬂoating-point number is divided into three ﬁelds to encode these values:
The single sign bit s directly encodes the sign s.
Figure 2.31 shows the packing of these three ﬁelds into words for the two most common formats.
The value encoded by a given bit representation can be divided into three different cases (the latter having two variants), depending on the value of exp.
These are illustrated in Figure 2.32 for the single-precision format.
Aside Why set the bias this way for denormalized values?
A second function of denormalized numbers is to represent numbers that are very close to 0.0
They provide a property known as gradual underﬂow in which possible numeric values are spaced evenly near 0.0
One interesting property of this representation is that if we interpret the bit representations of the values in Figure 2.34 as unsigned integers, they occur in ascending order, as do the values they represent as ﬂoating-point numbers.
This is no accident—the IEEE format was designed so that ﬂoating-point numbers could be sorted using an integer sorting routine.
The table that follows enumerates the entire nonnegative range for this 5-bit ﬂoating-point representation.
Fill in the blank table entries using the following directions:
Figure 2.35 shows the representations and numeric values of some important single- and double-precision ﬂoating-point numbers.
The value +0.0 always has a bit representation of all zeros.
Derive this ﬂoating-point representation and explain the correlation between the bits of the integer and ﬂoating-point representations.
For a ﬂoating-point format with an n-bit fraction, give a formula for the smallest positive integer that cannot be represented exactly (because it would require an n+1-bit fraction to be exact)
Assume the exponent ﬁeld size k is large enough that the range of representable exponents does not provide a limitation for this problem.
What is the numeric value of this integer for single-precision format (n = 23)?
Figure 2.36 illustrates the four rounding modes applied to the problem of rounding a monetary amount to the nearest whole dollar.
The only design decision is to determine the effect of rounding values that are halfway between two possible results.
Round-to-even at ﬁrst seems like it has a rather arbitrary goal—why is there any reason to prefer even numbers? Why not consistently round values halfway between two representable values upward? The problem with such a convention is that one can easily imagine scenarios in which rounding a set of data values would then introduce a statistical bias into the computation of an average of the values.
The average of a set of numbers that we rounded by this means would be slightly higher than the average of the numbers themselves.
Conversely, if we always rounded numbers halfway between downward, the average of a set of rounded numbers would be slightly lower than the average of the numbers themselves.
Rounding toward even numbers avoids this statistical bias in most real-life situations.
Round-to-even rounding can be applied even when we are not rounding to a whole number.
We simply consider whether the least signiﬁcant digit is even or odd.
For example, suppose we want to round decimal numbers to the nearest hundredth.
In general, the rounding mode is only signiﬁcant when we have a bit pattern of the form XX.
Only bit patterns of this form denote values that are halfway between two possible results.
In each case, show the numeric values, both before and after rounding.
How far off would the program’s prediction of the position of the Scud missile have been?
Neither has a sign bit—they can only represent nonnegative numbers.
Below, you are given somebit patterns inFormatA, and your task is to convert them to the closest value in Format B.
In addition, give the values of numbers given by the FormatA.
One strength of the IEEE standard’s method of specifying the behavior of ﬂoating-point operations is that it is independent of any particular hardware or software realization.
Thus, we can examine its abstract mathematical properties without considering how it is actually implemented.
The lack of associativity in ﬂoating-point addition is themost important group property that is lacking.
It has important implications for scientiﬁc programmers and compiler writers.
For example, suppose a compiler is given the following code fragment:
The compiler might be tempted to save one ﬂoating-point addition by generating the following code:
However, this computation might yield a different value for x than would the original, since it uses a different association of the addition operations.
In most applications, the difference would be so small as to be inconsequential.
Unfortunately, compilers have no way of knowing what trade-offs the user is willing to make between efﬁciency and faithfulness to the exact behavior of the original program.
As a result, they tend to be very conservative, avoiding any optimizations that could have even the slightest effect on functionality.
On the other hand, ﬂoating-point multiplication satisﬁes the following monotonicity properties for any values of a, b, and c other than NaN :
All versions of C provide two different ﬂoating-point data types: float and double.
On machines that support IEEE ﬂoating point, these data types correspond to single- and double-precision ﬂoating point.
More recent versions of C, including ISO C99, include a third ﬂoating-point data type, long double.
For many machines and compilers, this data type is equivalent to the double data type.
The properties of this format are investigated in Problem 2.85
When casting values between int, float, and double formats, the program changes the numeric values and the bit representations as follows (assuming a 32-bit int):
From int to float, the number cannot overﬂow, but it may be rounded.
From int or float to double, the exact numeric value can be preserved because double has both greater range (i.e., the range of representable values), as well as greater precision (i.e., the number of signiﬁcant bits)
Any conversion from ﬂoating point to integer that cannot assign a reasonable integer approximation yields this value.
In the next chapter, we will begin an in-depth study of Intel IA32 processors, the processor found in many of today’s personal computers.
Here we highlight an idiosyncrasy of these machines that can seriously affect the behavior of programs operating on ﬂoating-point numbers when compiledwith gcc.
IA32 processors, like most other processors, have special memory elements called registers for holding ﬂoating-point values as they are being computed and used.
All single- and double-precision numbers are converted to this format as they are loaded frommemory into ﬂoating-point registers.
Numbers are converted from extended precision to single- or double-precision format as they are stored in memory.
This extension to 80 bits for all register data and then contraction to a smaller format for memory data has some undesirable consequences for programmers.
It means that storing a number from a register to memory and then retrieving it back into the register can cause it to change, due to rounding, underﬂow, or overﬂow.
This storing and retrieving is not always visible to the C programmer, leading to some very peculiar results.
The peculiarities of the historic IA32 approach will diminish in importance with new hardware and with compilers that generate code based on the newer ﬂoating-point instructions.
Converting large ﬂoating-point numbers to integers is a common source of programming errors.
Just 37 seconds after liftoff, the rocket veered off its ﬂight path, broke up, and exploded.
Communication satellites valued at $500 million were on board the rocket.
In the design of the Ariane 4 software, they had carefully analyzed the numeric values and determined that the horizontal velocity.
Unfortunately, they simply reused this part of the software in the Ariane 5 without checking the assumptions on which it had been based.
Computers encode information as bits, generally organized as sequences of bytes.
Different encodings are used for representing integers, real numbers, and character strings.
Different models of computers use different conventions for encoding numbers and for ordering the bytes within multi-byte data.
The C language is designed to accommodate a wide range of different implementations in terms of word sizes and numeric encodings.
Most machines use two’s-complement encoding of integers and IEEE encoding of ﬂoating point.
Understanding these encodings at the bit level, as well as understanding themathematical characteristics of the arithmetic operations, is important for writing programs that operate correctly over the full range of numeric values.
When casting between signed and unsigned integers of the same size, most C implementations follow the convention that the underlying bit pattern does not change.
The implicit casting of C gives results that many programmers do not anticipate, often leading to program bugs.
Due to the ﬁnite lengths of the encodings, computer arithmetic has properties quite different from conventional integer and real arithmetic.
The ﬁnite length can cause numbers to overﬂow, when they exceed the range of the representation.
Floating-point values can also underﬂow, when they are so close to 0.0 that they are changed to zero.
The ﬁnite integer arithmetic implemented by C, as well as most other programming languages, has some peculiar properties compared to true integer arithmetic.
For example, the expression x*x can evaluate to a negative number due to overﬂow.
Nonetheless, both unsigned and two’s-complement arithmetic satisfy many of the other properties of integer arithmetic, including associativity, commutativity, and distributivity.
Floating-point arithmetic must be used very carefully, because it has only limited range and precision, and because it does not obey common mathematical properties such as associativity.
The C standards do not specify details such as precise word sizes or numeric encodings.
Such details are intentionally omitted to make it possible to implement C on a wide range of different machines.
These books also provide helpful advice on variable naming, coding styles, and code testing.
Seacord’s book on security issues in C and C++ programs [94], combines information about C programs, how they are compiled and executed, and how vulnerabilities may arise.
Books on Java (we recommend the one coauthored by James Gosling, the creator of the language [4]) describe the data formats and arithmetic operations supported by Java.
Overton’s book on IEEE ﬂoating point [78] provides a detailed description of the format as well as the properties from the perspective of a numerical applications programmer.
Here are some examples showing how the function should work:
In several of the following problems, wewill artiﬁcially restrict what programming constructs you can use to help you gain a better understanding of the bit-level, logic, and arithmetic operations of C.
In answering these problems, your code must follow these rules:
Forbidden Conditionals (if or ?:), loops, switch statements, function calls, andmacro invocations.
Even with these rules, you should try tomake your code readable by choosing descriptive variable names and using comments to describe the logic behind your solutions.
As an example, the following code extracts the most signiﬁcant byte from integer argument x:
Your code should follow the bit-level integer coding rules (page 120), with the additional restriction that you may not use equality (==) or inequality (!=) tests.
Your code should contain a total of at most 12 arithmetic, bit-wise, and logical operations.
Your code should contain a total of at most 15 arithmetic, bit-wise, and logical operations.
The following code does not run properly on some machines */
The following compiler message gives us an indication of the problem:
In what way does our code fail to comply with the C standard?
Modify the code to run properly on any machine for which data type int is at least 32 bits.
Modify the code to run properly on any machine for which data type int is at least 16 bits.
Your function should follow the bit-level integer coding rules (page 120)
Your function should follow the bit-level integer coding rules (page 120)
That is, the function will extract the designated byte and sign extend it to be a 32-bit int.
Your predecessor (whowas ﬁred for incompetence) wrote the following code:
Give a correct implementation of the function that uses only left and right shifts, along with one subtraction.
Although its use is a bit artiﬁcial here, where we simply want to copy an int, it illustrates an approach commonly used to copy larger data structures.
You carefully test the code and discover that it always copies the value to the buffer, even when maxbytes is too small.
Explain why the conditional test in the code always succeeds.
Hint: The sizeof operator returns a value of type size_t.
Show how you can rewrite the conditional test to make it work properly.
Instead of overﬂowing the way normal two’s-complement addition does, saturating addition returns TMax when there would be positive overﬂow, and TMin when there would be negative overﬂow.
Saturating arithmetic is commonly used in programs that perform digital signal processing.
Your function should follow the bit-level integer coding rules (page 120)
Write code calling this procedure to implement the function for unsigned arguments.
We generate arbitrary values x and y, and convert them to unsigned values as follows:
If it always yields 1, describe the underlying mathematical principles.
Hint:Consider the effect of shifting the binary point k positions to the right.
What is the numeric value of the string for the following values of y?
Give an expression using only ux, uy, sx, and sy */
Fill in the table that follows for each of the numbers given, with the following instructions for each column:
We generate arbitrary integer values x, y, and z, and convert them to values of type double as follows:
If it always yields 1, describe the underlying mathematical principles.
What is the fractional binary number denoted by this ﬂoating-point value?
In the following problems, you will write code to implement ﬂoating-point functions, operating directly on bit-level representations of ﬂoating-point numbers.
Your code should exactly replicate the conventions for IEEE ﬂoating-point operations, including using round-to-even mode when rounding is required.
Toward this end, we deﬁne data type float_bits to be equivalent to unsigned:
Rather than using data type float in your code, you will use float_bits.
You may use both int and unsigned data types, including unsigned and integer constants and operations.
Instead, your code should perform the bit manipulations that implement the speciﬁed ﬂoating-point operations.
Test your function by evaluating it for all 232 values of argument f and comparing the result to what would be obtained using your machine’s ﬂoating-point operations.
Test your function by evaluating it for all 232 values of argument f and comparing the result to what would be obtained using your machine’s ﬂoating-point operations.
If f is NaN , your function should simply return f.
Test your function by evaluating it for all 232 values of argument f and comparing the result to what would be obtained using your machine’s ﬂoating-point operations.
If f is NaN , your function should simply return f.
Test your function by evaluating it for all 232 values of argument f and comparing the result to what would be obtained using your machine’s ﬂoating-point operations.
If conversion causes overflow or f is NaN, return 0x80000000
For ﬂoating-point number f , this function computes (int) f.
If f cannot be represented as an integer (e.g., it is out of range, or it is NaN), then the function should return 0x80000000
Test your function by evaluating it for all 232 values of argument f and comparing the result to what would be obtained using your machine’s ﬂoating-point operations.
For argument i, this function computes the bit-level representation of (float) i.
Test your function by evaluating it for all 232 values of argument f and comparing the result to what would be obtained using your machine’s ﬂoating-point operations.
The method for doing these conversions is in the text, but it takes a little practice to become familiar.
For larger ones, it becomes much more convenient and reliable to use a calculator or conversion program.
You can always convert numbers to decimal, perform the arithmetic, and convert them back, but being able to work directly in hexadecimal is more efﬁcient and informative.
Since this digit is 0, we must also borrow from the fourth position.
Recall that show_bytes enumerates a series of bytes starting from the one with lowest address and working toward the one with highest address.
On a big-endian machine, it will list bytes from the most signiﬁcant byte to the least.
It also gets you thinking about integer and ﬂoating-point representations.
We will explore these representations in more detail later in this chapter.
Using the notation of the example in the text, we write the two strings as follows:
With the second word shifted two positions to the right relative to the ﬁrst, we ﬁnd a sequence with 21 matching bits.
Such is the case for the example in the text as well.
In addition, the ﬂoating-point number has some nonzero high-order bits that do not match those of the integer.
Colors are complemented by complementing the values of R, G, and B.
From this, we can see that White is the complement of Black, Yellow is the complement of Blue, Magenta is the complement of Green, and Cyan is the complement of Red.
We perform Boolean operations based on a bit-vector representation of the colors.
See Problem 2.11 for a case where this function will fail.
Both first and last have value k, so we are attempting to swap the middle element with itself.
In this case, arguments x and y to inplace_swap both point to the same location.
We can see that our reasoning in Problem 2.10 implicitly assumed that x and y denote different locations.
These expressions are typical of the kind commonly found in performing low-level bit operations.
Observe that such a mask will be generated regardless of the word size.
The bis operation is equivalent to Boolean Or—a bit is set in z if either this bit is set in x or it is set in m.
Given that, we can implement | with a single call to bis.
That is, x^ywill be zero if and only if every bit of xmatches the corresponding bit of y.
We then exploit the ability of ! to determine whether a word contains any nonzero bit.
There is no real reason to use this expression rather than simply writing x == y, but it demonstrates some of the nuances of bit-level and logical operations.
It is quite common to see numbers beginning with a string of f’s, since the leading bits of a negative number are all ones.
Filling this out with a leading zero gives 0x08048337, a positive number.
We solve this problem by reordering the rows in the solution of Problem 2.17 according to the two’s-complement value and then listing the unsigned value as the result of the function application.
We show the hexadecimal values to make this process more concrete.
For the remaining two entries, the values of x are nonnegative and T2U4(x) = x.
This exercise lets you explore its properties using very small word sizes.
It seems quite natural to pass parameter length as an unsigned, since one would never want to use a negative length.
The code can be ﬁxed either by declaring length to be an int, or by changing the test of the for loop to be i < length.
For what cases will this function produce an incorrect result? The function will incorrectly return 1 when s is shorter than t.
Since strlen is deﬁned to yield an unsigned result, the difference and the comparison are both computed using unsigned arithmetic.
Show how to ﬁx the code so that it will work reliably.Replace the test with the following:
The easiest way to solve it is to convert the hex pattern into its unsigned decimal value.
In this case, we will have -y also equal to TMin, and so function tadd_ok will consider there to be.
One lesson to be learned from this exercise is that TMin should be included as one of the cases in any test procedure for a function.
The bit patterns are the same as for unsigned negation.
Here’s a more principled approach, following the proposed set of arguments:
Let u denote the unsigned number represented by the lower w bits, and v denote the two’s-complement number represented by the upper w bits.
Then, based on Equation 2.3, we can see that x.
When x equals 0, multiplication does not overﬂow, and sowe see that our code.
We then test whether casting the product to 32 bits changes the value:
Note that the casting on the right-hand side of line 4 is critical.
Even though the computation of asizewill be accurate, the call to mallocwill cause this value to be converted to a 32-bit unsigned number, and so the same overﬂow conditions will occur.
The instruction is provided to support pointer arithmetic, but the C compiler often uses it as a way to perform multiplication by small constants.
Observe that the fourth case uses a modiﬁed version of form B.
By masking off the appropriate bits, we get the desired bias value.
It becomes more clear when put in the form shown in optarith.
When shifted left by 29, this will become the sign bit.
Two’s-complement and unsigned addition have the same bit-level behavior, and they are commutative.
One simple way to think about fractional binary representations is to represent a number as a fraction of the form x.
We then put the binary point four positions from the right to get 1.10012
In this example, however, the system was sensitive to the absolute error.
This code seems to work on a variety of machines, however.
Computers execute machine code, sequences of bytes encoding the low-level operations that manipulate data, manage memory, read and write data on storage devices, and communicate over networks.
A compiler generates machine code through a series of stages, based on the rules of the programming language, the instruction set of the target machine, and the conventions followed by the operating system.
The gcc C compiler generates its output in the form of assembly code, a textual representation of the machine code giving the individual instructions in the program.
In this chapter, we will take a close look at machine code and its human-readable representation as assembly code.
When programming in a high-level language such as C, and even more so in Java, we are shielded from the detailed, machine-level implementation of our program.
In contrast, when writing programs in assembly code (as was done in the early days of computing) a programmermust specify the low-level instructions the programuses to carry out a computation.Most of the time, it ismuchmore productive and reliable to work at the higher level of abstraction provided by a high-level language.
The type checking provided by a compiler helps detect many program errors and makes sure we reference and manipulate data in consistent ways.
With modern, optimizing compilers, the generated code is usually at least as efﬁcient as what a skilled, assembly-language programmer would write by hand.
Best of all, a program written in a high-level language can be compiled and executed on a number of different machines, whereas assembly code is highly machine speciﬁc.
So why should we spend our time learning machine code? Even though compilers do most of the work in generating assembly code, being able to read and understand it is an important skill for serious programmers.
By invoking the compiler with appropriate command-line parameters, the compiler will generate a ﬁle showing its output in assembly-code form.
By reading this code, we can understand the optimization capabilities of the compiler and analyze the underlying inefﬁciencies in the code.
As we will experience in Chapter 5, programmers seeking to maximize the performance of a critical section of code often try different variations of the source code, each time compiling and examining the generated assembly code to get a sense of how efﬁciently the programwill run.
Furthermore, there are times when the layer of abstraction provided by a high-level language hides information about the run-time behavior of a program that we need to understand.
For example, whenwriting concurrent programs using a thread package, as covered in Chapter 12, it is important to knowwhat region ofmemory is used to hold the different program variables.
As another example, many of the ways programs can be attacked, allowing worms and viruses to infest a system, involve nuances of the way programs store their run-time control information.
Many attacks involve exploiting weaknesses in systemprograms to overwrite information and thereby take control of the system.
Understanding how these vulnerabilities arise and how to guard against them requires a knowledge of the machine-level representation of programs.
The need for programmers to learn assembly code has shifted over the years from one of being able to write programs directly in assembly to one of being able to read and understand the code generated by compilers.
In this chapter, we will learn the details of two particular assembly languages and see how C programs get compiled into these forms of machine code.
Reading the assembly code generated by a compiler involves a different set of skills than writing assembly code by hand.
We must understand the transformations typical compilers make in converting the constructs of C into machine code.
Relative to the computations expressed in the C code, optimizing compilers can rearrange execution order, eliminate unneeded computations, replace slow operations with faster ones, and even change recursive computations into iterative ones.
Understanding the relation between source code and the generated assembly can often be a challenge—it’s much like putting together a puzzle having a slightly different design than the picture on the box.
It is a form of reverse engineering—trying to understand the process by which a system was created by studying the system and working backward.
In this case, the system is a machine-generated assemblylanguage program, rather than something designed by a human.
This simpliﬁes the task of reverse engineering, because the generated code follows fairly regular patterns, and we can run experiments, having the compiler generate code for many different programs.
In our presentation, we give many examples and provide a number of exercises illustrating different aspects of assembly language and compilers.
This is a subject where mastering the details is a prerequisite to understanding the deeper and more fundamental concepts.
Those who say “I understand the general principles, I don’t want to bother learning the details” are deluding themselves.
It is critical for you to spend time studying the examples, working through the exercises, and checking your solutions with those provided.
The result is a rather peculiar design with features that make sense only when viewed from a historical perspective.
It is also laden with features providing backward compatibility that are not used by modern compilers and operating systems.
We will focus on the subset of the features used by gcc and Linux.
This allows us to avoid much of the complexity and arcane features of IA32
Our technical presentation starts with a quick tour to show the relation between C, assembly code, and machine code.
We then proceed to the details of IA32, starting with the representation and manipulation of data and the implementation of control.
We see how control constructs in C, such as if, while, and switch statements, are implemented.
We then cover the implementation of procedures, including how the program maintains a run-time stack to support the passing of data and control between procedures, as well as storage for local variables.
Next, we consider how data structures such as arrays, structures, and unions are implemented at themachine level.With this background inmachine-level programming, we can examine the problems of out of boundsmemory references and the vulnerability of systems to buffer overﬂow attacks.
However, most of the operating systems running on these machines support only 32-bit applications, and so the capabilities of the hardware are not fully utilized.
As memory prices drop, and the desire to perform computations involving very large data sets increases, 64-bit machines and applications will become commonplace.
It is therefore appropriate to take a close look at x86-64
We provide Web Asides to cover material intended for dedicated machinelanguage enthusiasts.
In one, we examine the code generated when code is compiled using higher degrees of optimization.
Another Web Aside gives a brief presentation of ways to incorporate assembly code into C programs.
For some applications, the programmer must drop down to assembly code to access low-level features of the machine.
One approach is to write entire functions in assembly code and combine themwith C functions during the linking stage.
A second is to use gcc’s support for embedding assembly code directly within C programs.
We provide separate Web Asides for two different machine languages for ﬂoating-point code.
The “x87” ﬂoating-point instructions have been available since the early days of Intel processors.
This implementation of ﬂoating point is particularly arcane, and so we advise that only people determined to work with ﬂoating-point code on older machines attempt to study this section.
The Intel processor line, colloquially referred to as x86, has followed a long, evolutionary development.
It started with one of the ﬁrst single-chip, 16-bit microprocessors, where many compromises had to be made due to the limited capabilities of integrated circuit technology at the time.
Since then, it has grown to take advantage of technology improvements as well as to satisfy the demands for higher performance and for supporting more advanced operating systems.
The list that follows shows some models of Intel processors and some of their key features, especially those affecting machine-level programming.
The original models came with 32,768 bytes of memory and two ﬂoppy drives (no hard drive)
Formed the basis of the IBM PC-AT personal computer, the original platform for MS Windows.
Added the ﬂat addressing model used by Linux and recent versions of the Windows family of operating system.
This was the ﬁrst machine in the series that could support a Unix operating system.
Improved performance and integrated the ﬂoating-point unit onto the processor chip but did not signiﬁcantly change the instruction set.
Improved performance, but only added minor extensions to the instruction set.
Introduced a radically new processor design, internally known as the P6 microarchitecture.
Added a class of “conditional move” instructions to the instruction set.
Introduced SSE, a class of instructions formanipulating vectors of integer or ﬂoating-point data.
With these extensions, compilers can use SSE instructions, rather than x87 instructions, to compile ﬂoating-point code.
Introduced theNetBurst microarchitecture, which could operate at very high clock speeds, but at the cost of high power consumption.
Incorporated both hyperthreading and multi-core, with the initial version supporting two executing programs on each core and up to four cores on each chip.
Each successive processor has been designed to be backward compatibleable to run code compiled for any earlier version.
As we will see, there are many strange artifacts in the instruction set due to this evolutionary heritage.
If we plot the number of transistors in the different Intel processors versus the year of introduction, and use a logarithmic scale for the y-axis, we can see that the growth has been phenomenal.
This growth has been sustained over the multiple-decade history of x86 microprocessors.
As it turns out, his prediction was just a little bit optimistic, but also too short-sighted.
Similar exponential growth rates have occurred for other aspects of computer technology—disk capacities, memory-chip capacities, and processor performance.
These remarkable growth rates have been the major driving forces of the computer revolution.
Over the years, several companies have produced processors that are compatible with Intel processors, capable of running the exact same machine-level programs.
For years, AMD lagged just behind Intel in technology, forcing a marketing strategy where they produced processors that were less expensive although somewhat lower in performance.
Although we will talk about Intel processors, our presentation holds just as well for the compatible processors produced by Intel’s rivals.
Muchof the complexity of x86 is not of concern to those interested inprograms for the Linux operating system as generated by the gcc compiler.
Instead, Linux uses what is referred to as ﬂat addressing, where the entirememory space is viewed by the programmer as a large array of bytes.
Aswe can see in the list of developments, a number of formats and instructions have been added to x86 for manipulating vectors of small integers and ﬂoatingpoint numbers.
These features were added to allow improved performance on multimedia applications, such as image processing, audio and video encoding and decoding, and three-dimensional computer graphics.
Only by giving speciﬁc command-line options, or by compiling for 64-bit operation, will the compiler make use of the more recent extensions.
For the next part of our presentation, we will focus only on the IA32 instruction set.
We can then compile this code on an IA32 machine using a Unix command line:
Since this is the default compiler on Linux, we could also invoke it as simply cc.
The command-line option -O1 instructs the compiler to apply level-one optimizations.
In general, increasing the level of optimization makes the ﬁnal program run faster, but at a risk of increased compilation time and difﬁculties running debugging tools on the code.
As we will also see, invoking higher levels of optimization can generate code that is so heavily transformed that the relationship between the generated machine code and the original source code is difﬁcult to understand.
We will therefore use level-one optimization as a learning tool and then see what happens as we increase the level of optimization.
In practice, level-two optimization (speciﬁedwith the option -O2) is considered a better choice in terms of the resulting program performance.
The gcc command actually invokes a sequence of programs to turn the source code into executable code.
Object code is one formofmachine code—it contains binary representations of all of the instructions, but the addresses of global values are not yet ﬁlled in.
Finally, the linkermerges these two object-code ﬁles alongwith code implementing library functions (e.g., printf) and generates the ﬁnal executable code ﬁle p.
Executable code is the second form of machine code we will consider—it is the exact form of code that is executed by the processor.
As described in Section 1.9.2, computer systems employ several different forms of abstraction, hiding details of an implementation through the use of a simpler, abstract model.
First, the format and behavior of a machine-level program is deﬁned by the instruction set architecture, or “ISA,” deﬁning the processor state, the format of the instructions, and the effect each of these instructions will have on the state.
The processor hardware is far more elaborate, executing many instructions concurrently, but they employ safeguards to ensure that the overall behavior matches the sequential operation dictated by the ISA.
Second, the memory addresses used by a machine-level program are virtual addresses, providing a memory model that appears to be a very large byte array.
Its main feature is that it is in a more readable textual format, as compared to the binary format of machine code.
Being able to understand assembly code and how it relates to the original C code is a key step in understanding how computers execute programs.
IA32 machine code differs greatly from the original C code.
Parts of the processor state are visible that normally are hidden from the C programmer:
The program counter (commonly referred to as the “PC,” and called %eip in IA32) indicates the address in memory of the next instruction to be executed.
The integer register ﬁle contains eight named locations storing 32-bit values.
These registers can hold addresses (corresponding to C pointers) or integer data.
Some registers are used to keep track of critical parts of the program state, while others are used to hold temporary data, such as the local variables of a procedure, and the value to be returned by a function.
The condition code registers hold status information about the most recently executed arithmetic or logical instruction.
These are used to implement conditional changes in the control or data ﬂow, such as is required to implement if and while statements.
Whereas C provides a model in which objects of different data types can be declared and allocated in memory, machine code views the memory as simply a large, byte-addressable array.
Aggregate data types in C such as arrays and structures are represented in machine code as contiguous collections of bytes.
Even for scalar data types, assembly codemakes no distinctions between signed or unsigned integers, between different types of pointers, or even between pointers and integers.
At any given time, only limited subranges of virtual addresses are considered valid.
The operating system manages this virtual address space, translating virtual addresses into the physical addresses of values in the actual processor memory.
A single machine instruction performs only a very elementary operation.
For example, it might add two numbers stored in registers, transfer data between memory and a register, or conditionally branch to a new instruction address.
The compiler must generate sequences of such instructions to implement program constructs such as arithmetic expression evaluation, loops, or procedure calls and returns.
In our presentation, we will show the code generated by a particular version of gcc with particular settings of the command-line options.
If you compile code on your ownmachine, chances are youwill be using a different compiler or a different version ofgcc and hencewill generate different code.
The opensource community supporting gcc keeps changing the code generator, attempting to generate more efﬁcient code according to changing code guidelines provided by the microprocessor manufacturers.
Our goal in studying the examples shown in our presentation is to demonstrate how to examine assembly code and map it back to the constructs found in high-level programming languages.
You will need to adapt these techniques to the style of code generated by your particular compiler.
Suppose we write a C code ﬁle code.c containing the following procedure deﬁnition:
To see the assembly code generated by the C compiler, we can use the “-S” option on the command line:
This will cause gcc to run the compiler, generating an assembly ﬁle code.s, and go no further.
Normally it would then invoke the assembler to generate an objectcode ﬁle.
The assembly-code ﬁle contains various declarations including the set of lines:
Each indented line in the above code corresponds to a single machine instruction.
For example, the pushl instruction indicates that the contents of register %ebp should be pushed onto the program stack.
All information about local variable names or data types has been stripped away.
If we use the ‘-c’ command-line option, gcc will both compile and assemble the code:
This will generate an object-code ﬁle code.o that is in binary format and hence cannot be viewed directly.
Embedded within the 800 bytes of the ﬁle code.o is a.
This is the object-code corresponding to the assembly instructions listed above.
A key lesson to learn from this is that the program actually executed by the machine is simply a sequence of bytes encoding a series of instructions.
The machine has very little information about the source code from which these instructions were generated.
Aside How do I ﬁnd the byte representation of a program?
To generate these bytes, we used a disassembler (to be described shortly) to determine that the code for sum is 17 bytes long.
Then we ran the GNU debugging tool gdb on ﬁle code.o and gave it the command.
You will ﬁnd that gdb has many useful features for analyzing machine-level programs, as will be discussed in Section 3.11
To inspect the contents of machine-code ﬁles, a class of programs known as disassemblers can be invaluable.
These programs generate a format similar to assembly code from the machine code.
With Linux systems, the program objdump (for “object dump”) can serve this role given the ‘-d’ command-line ﬂag:
The result is (where we have added line numbers on the left and annotations in italicized text) as follows:
Each of these groups is a single instruction, with the assembly-language equivalent shown on the right.
Several features about machine code and its disassembled representation are worth noting:
The instruction encoding is designed so that commonly used instructions and those with fewer operands require a smaller number of bytes than do less common ones or ones with more operands.
The instruction format is designed in such a way that from a given starting position, there is a unique decoding of the bytes into machine instructions.
The disassembler determines the assembly code based purely on the byte sequences in the machine-code ﬁle.
It does not require access to the source or assembly-code versions of the program.
The disassembler uses a slightly different naming convention for the instructions than does the assembly code generated by gcc.
In our example, it has omitted the sufﬁx ‘l’ from many of the instructions.
These sufﬁxes are size designators and can be omitted in most cases.
Generating the actual executable code requires running a linker on the set of object-code ﬁles, one of which must contain a function main.
Then, we could generate an executable program prog as follows:
The ﬁle prog has grown to 9,123 bytes, since it contains not just the code for our two procedures but also information used to start and terminate the program as well as to interact with the operating system.We can also disassemble the ﬁle prog:
The disassembler will extract various code sequences, including the following:
This code is almost identical to that generated by the disassembly of code.c.
One important difference is that the addresses listed along the left are different—the linker has shifted the location of this code to a different range of addresses.
A second difference is that the linker has determined the location for storing global variable accum.
In the disassembly of prog, the address has been set to 0x804a018
This is shown in the assembly-code rendition of the instruction.
The assembly code generated by gcc is difﬁcult for a human to read.
On one hand, it contains information with which we need not be concerned, while on the other hand, it does not provide any description of the program or how it works.
For example, suppose the ﬁle simple.c contains the following code:
All of the lines beginning with ‘.’ are directives to guide the assembler and linker.We can generally ignore these.On the other hand, there are no explanatory remarks about what the instructions do or how they relate to the source code.
To provide a clearer presentation of assembly code, we will show it in a form that omits most of the directives, while including line numbers and explanatory annotations.
For our example, an annotated version would appear as follows:
We typically show only the lines of code relevant to the point being discussed.
Each line is numbered on the left for reference and annotated on the right by a brief description of the effect of the instruction and how it relates to the computations of the original C code.
This is a stylized version of theway assembly-language programmers format their code.
In our presentation, we show assembly code in ATT (named after “AT&T,” the company that operated Bell Laboratories for many years) format, the default format for gcc, objdump, and the other tools we will consider.
Other programming tools, including those from Microsoft as well as the documentation from Intel, show assembly code in Intel format.
As an example, gcc can generate code in Intel format for the sum function using the following command line:
We see that the Intel and ATT formats differ in the following ways:
The Intel code omits the ‘%’ character in front of register names, using esp instead of %esp.
This can be very confusing when switching between the two formats.
Although we will not be using Intel format in our presentation, you will encounter it in IA32 documentation from Intel and Windows documentation from Microsoft.
Most of the common data types are stored as double words.
This includes both regular and long int’s, whether or not they are signed.
Instead, the compiler must generate sequences of instructions that operate on these data 32 bits.
Compiling code with long long data requires generating sequences of operations to perform the arithmetic in 32-bit chunks.
It also stores them as 12-byte quantities to improve memory system performance, as will be discussed later.
For most other machines, this data type will be represented using the same 8-byte format of the ordinary double data type.
As the table indicates, most assembly-code instructions generated by gcchave a single-character sufﬁx denoting the size of the operand.
For example, the data movement instruction has three variants: movb (move byte), movw (move word), and movl (move double word)
This causes no ambiguity, since ﬂoating point involves an entirely different set of instructions and registers.
These registers are used to store integer data as well as pointers.
Their names all begin with %e, but otherwise, they have peculiar names.
The names were chosen to reﬂect these different purposes.With ﬂat addressing, the need for specialized registers is greatly reduced.
The 2 loworder bytes of the ﬁrst four registers can be accessed independently.
We said “for the most part,” because some instructions use ﬁxed registers as sources and/or destinations.
In addition, within procedures there are different conventions for saving and restoring the ﬁrst three registers (%eax, %ecx, and %edx) than for the next three (%ebx, %edi, and %esi)
The ﬁnal two registers (%ebp and %esp) contain pointers to important places in the program stack.
They should only be altered according to the set of standard conventions for stack management.
When a byte instruction updates one of these single-byte “register elements,” the remaining 3 bytes of the register do not change.
Similarly, the low-order 16 bits of each register can be read or written by word operation instructions.
Most instructions have one or more operands, specifying the source values to reference in performing an operation and the destination location into which to place the result.
Source values can be given as constants or read from registers or memory.
Thus, the different operand possibilities can be classiﬁed into three types.
In Figure 3.3, we use the notation Ea to denote an arbitrary register a, and indicate its value with the reference R[Ea], viewing the set of registers as an array R indexed by register identiﬁers.
The third type of operand is a memory reference, in which we access some memory location according to a computed address, often called the effective address.
Since we view the memory as a large array of bytes, we use the notation Mb[Addr] to denote a reference to the b-byte value stored in memory starting at address Addr.
To simplify things, we will generally drop the subscript b.
As Figure 3.3 shows, there are many different addressing modes allowing different forms of memory references.
Themost general form is shown at the bottom of the table with syntax Imm(Eb,Ei,s)
The effective address is then computed as Imm + R[Eb]+ R[Ei]
This general form is often seen when referencing elements of arrays.
The other forms are simply special cases of this general form where some of the components are omitted.
As we will see, the more complex addressing modes are useful when referencing array and structure elements.
Practice Problem 3.1 Assume the following values are stored at the indicated memory addresses and registers:
Fill in the following table showing the values for the indicated operands:
Among the most heavily used instructions are those that copy data from one location to another.
The generality of the operand notation allows a simple data movement instruction to performwhat inmanymachines would require a number of instructions.
As can be seen, we group the many different instructions into instruction classes, where the instructions in a class perform the same operation, but with different operand sizes.
For example, the mov class consists of three instructions: movb, movw, and movl.
The instructions in themov class copy their source values to their destinations.
The source operand designates a value that is immediate, stored in a register, or stored in memory.
The destination operand designates a location that is either a register or amemory address.
IA32 imposes the restriction that amove instruction cannot have both operands refer to memory locations.
Copying a value from one memory location to another requires two instructions—the ﬁrst to load the source value into a register, and the second to write this register value to the destination.
Recall that the source operand comes ﬁrst and the destination second:
Both themovs and themovz instruction classes serve to copy a smaller amount of source data to a larger data location, ﬁlling in the upper bits by either sign expansion (movs) or by zero expansion (movz)
With sign expansion, the upper bits of the destination are ﬁlled in with copies of the most signiﬁcant bit of the source value.
With zero expansion, the upper bits are ﬁlled with zeros.
Observe that the three byte-movement instructions movb, movsbl, and movzbl differ from each other in subtle ways.
In these examples, all set the low-order byte of register %eax to the second byte of %edx.
The movb instruction does not change the other 3 bytes.
The movsbl instruction sets the other 3 bytes to either all ones or all zeros, depending on the high-order bit of the source byte.
The movzbl instruction sets the other 3 bytes to all zeros in any case.
The ﬁnal two data movement operations are used to push data onto and pop data from the program stack.
As we will see, the stack plays a vital role in the handling of procedure calls.
By way of background, a stack is a data structure where values can be added or deleted, but only according to a “last-in, ﬁrst-out” discipline.
We add data to a stack via a push operation and remove it via a pop operation, with the property that the value popped will always be the value that was most recently pushed and is still on the stack.
A stack can be implemented as an array, wherewe always insert and remove elements fromone end of the array.
The stack pointer %esp holds the address of the top stack element.
By convention, we draw stacks upside down, so that the “top” of the stack is shown at the bottom.
IA32 stacks grow toward lower addresses, so pushing involves decrementing the stack pointer (register %esp) and storing to memory, while popping involves reading from memory and incrementing the stack pointer.
The pushl instruction provides the ability to push data onto the stack, while the popl instruction pops it.
Each of these instructions takes a single operand—the data source for pushing and the data destination for popping.
Pushing a double-word value onto the stack involves ﬁrst decrementing the stack pointer by 4 and then writing the value at the new top of stack address.
Therefore, the behavior of the instruction pushl %ebp is equivalent to that of the pair of instructions.
Therefore, the instruction popl %eax is equivalent to the following pair of instructions:
The third columnofFigure 3.5 illustrates the effect of executing the instruction popl %edx immediately after executing the pushl.
However, the stack top is always considered to be the address indicated by %esp.
Any value stored beyond the stack top is considered invalid.
Since the stack is contained in the same memory as the program code and other forms of program data, programs can access arbitrary positions within the stack using the standard memory addressing methods.
For example, assuming the topmost element of the stack is a doubleword, the instruction movl 4(%esp),%edx will copy the second double word from the stack to register %edx.
Practice Problem 3.2 For each of the following lines of assembly language, determine the appropriate instruction sufﬁx based on the operands.
For example, mov can be rewritten as movb, movw, or movl.
Practice Problem 3.3 Each of the following lines of code generates an error message when we invoke the assembler.
The code we are left with is called the “body.”
Function exchange (Figure 3.6) provides a good illustration of the use of pointers in C.
Argument xp is a pointer to an integer, while y is an integer itself.
This is also a form of pointer dereferencing (and hence the operator *), but it indicates a write operation since it is on the left-hand side of the assignment.
TheC operator & (called the “address of” operator) creates a pointer, in this case to the location holding local variable a.
By copying to %eax below, x becomes the return value.
Figure 3.6 C and assembly code for exchange routine body.
Later, register %eax will be used to return a value from this function, and so the return value will be x.
First, we see thatwhat we call “pointers” in C are simply addresses.
Dereferencing a pointer involves copying that pointer into a register, and then using this register in a memory reference.
Second, local variables such as x are often kept in registers rather than stored in memory locations.
Practice Problem 3.4 Assume variables v and p declared with types.
We wish to use the appropriate data movement instruction to implement the operation.
Recall that when performing a cast that involves both a size change and a change of “signedness” in C, the operation should change the signedness ﬁrst (Section 2.2.6)
Write C code for decode1 that will have an effect equivalent to the assembly code above.
Figure 3.7 lists some of the integer and logic operations.
Most of the operations are given as instruction classes, as they can have different variants with different operand sizes.
For example, the instruction class add consists of three addition instructions: addb, addw, and addl, adding bytes, words, and double words, respectively.
Indeed, each of the instruction classes shown has instructions for operating on byte, word, and double-word data.
The operations are divided into four groups: load effective address, unary, binary, and shifts.Binary operations have two operands, while unary operations have one operand.
These operands are speciﬁed using the same notation as described in Section 3.4
The load effective address instruction leal is actually a variant of the movl instruction.
It has the form of an instruction that reads from memory to a register, but it does not reference memory at all.
This instruction can be used to generate pointers for later memory references.
In addition, it can be used to compactly describe common arithmetic operations.
Compilers often ﬁnd clever uses of leal that have nothing to do with effective address computations.
Practice Problem 3.6 Suppose register %eax holds value x and %ecx holds value y.
Fill in the table below with formulas indicating the value that will be stored in register %edx for each of the given assembly code instructions:
Operations in the second group are unary operations, with the single operand serving as both source and destination.
For example, the instruction incl (%esp) causes the 4-byte element on the top of the stack to be incremented.
This syntax is reminiscent of the C increment (++) and decrement (--) operators.
The third group consists of binary operations, where the second operand is used as both a source and a destination.
This syntax is reminiscent of the C assignment operators, such as x += y.
Observe, however, that the source operand is given ﬁrst and the destination second.
For example, the instruction subl %eax,%edx decrements register %edx by the value in %eax.
The second can be either a register or a memory location.
As with the movl instruction, however, the two operands cannot both be memory locations.
Practice Problem 3.7 Assume the following values are stored at the indicated memory addresses and registers:
Fill in the following table showing the effects of the following instructions, both in terms of the register or memory location that will be updated and the resulting value:
The ﬁnal group consists of shift operations, where the shift amount is given ﬁrst, and the value to shift is given second.
The shift amount is given either as an immediate or in the singlebyte register element %cl.
These instructions are unusual in only allowing this speciﬁc register as operand.
As Figure 3.7 indicates, there are two names for the.
Both have the same effect, ﬁlling from the right with zeros.
The right shift instructions differ in that sar performs an arithmetic shift (ﬁll with copies of the sign bit), whereas shr performs a logical shift (ﬁll with zeros)
The destination operand of a shift operation can be either a register or a memory location.
Practice Problem 3.8 Suppose we want to generate assembly code for the following C function:
The code that follows is a portion of the assembly code that performs the actual shifts and leaves the ﬁnal value in register %eax.
Fill in the missing instructions, following the annotations on the right.
We see that most of the instructions shown in Figure 3.7 can be used for either unsignedor two’s-complement arithmetic.Only right shifting requires instructions that differentiate between signed versus unsigned data.
This is one of the features that makes two’s-complement arithmetic the preferred way to implement signed integer arithmetic.
Figure 3.8 shows an example of a function that performs arithmetic operations and its translation into assembly code.
As before, we have omitted the stack setup and completion portions.
The assembly code instructions occur in a different order than in the C source code.
Since the destination of the multiply is register %eax, this will be the value returned by the function.
Figure 3.8 C and assembly code for arithmetic routine body.
In general, compilers generate code that uses individual registers for multiple program values and moves program values among the registers.
The portion of the generated assembly code implementing these expressions is as follows:
Based on this assembly code, ﬁll in the missing portions of the C code.
Practice Problem 3.10 It is common to ﬁnd assembly code lines of the form.
Explain the effect of this particular Exclusive-Or instruction and what useful operation it implements.
What would be the more straightforward way to express this operation in assembly code?
Compare the number of bytes to encode these two different implementations of the same operation.
The imull instruction, a member of the imul instruction class listed in Figure 3.7, is known as a “two-operand” multiply instruction.
Recall that when truncating the product to 32 bits, both unsigned multiply and two’s-complement multiply have the same bitlevel behavior.
For both of these, one argument must be in register %eax, and the other is given as the instruction.
Although the name imull is used for two distinct multiplication operations, the assembler can tell which one is intended by counting the number of operands.
Observe that the locations in which we store the two registers are correct for a little-endian machine—the high-order bits in register %edx are stored at offset 4 relative to the low-order bits in %eax.
With the stack growing toward lower addresses, that means that the low-order bits are at the top of the stack.
Our earlier table of arithmetic operations (Figure 3.7) does not list any division or modulus operations.
These operations are provided by the single-operand divide instructions similar to the single-operand multiply instructions.
The instruction stores the quotient in register %eax and the remainder in register %edx.
Thus, we have the combined registers %edx and %eax storing a 64-bit, sign-extended version of x.
Amore conventional method of setting up the divisor makes use of the cltd1
We can see that the ﬁrst two instructions have the same overall effect as the ﬁrst three instructions in our earlier code sequence.
Different versions of gcc generate these two different ways of setting up the dividend for integer division.
Practice Problem 3.11 Modify the assembly code shown for signed division so that it computes the unsigned quotient and remainder of numbers x and y and stores the results on the stack.
This instruction is called cdq in the Intel documentation, one of the few cases where theATT-format name for an instruction bears no relation to the Intel name.
Describe the algorithm used to compute the product and argue that it is correct.
So far, we have only considered the behavior of straight-line code, where instructions follow one another in sequence.
Some constructs in C, such as conditionals, loops, and switches, require conditional execution, where the sequence of operations that gets performed depends on the outcomes of tests applied to the data.
Machine code provides two basic low-level mechanisms for implementing conditional behavior: it tests data values and then either alters the control ﬂow or the data ﬂow based on the result of these tests.
Data-dependent control ﬂow is themore general andmore common approach for implementing conditional behavior, and sowewill examine this ﬁrst.
Normally, both statements in C and instructions in machine code are executed sequentially, in the order they appear in the program.
The execution order of a set of machinecode instructions can be altered with a jump instruction, indicating that control should pass to some other part of the program, possibly contingent on the result of some test.
The compiler must generate instruction sequences that build upon this low-level mechanism to implement the control constructs of C.
In our presentation, we ﬁrst cover the machine-level mechanisms and then show how the different control constructs of C are implemented with them.
We then return to the use of conditional data transfer to implement data-dependent behavior.
In addition to the integer registers, the CPUmaintains a set of single-bit condition code registers describing attributes of the most recent arithmetic or logical operation.
These registers can then be tested to perform conditional branches.
The most recent operation generated a carry out of the most signiﬁcant bit.
The most recent operation caused a two’s-complement overﬂow—either negative or positive.
These instructions set the condition codes without updating any other registers.
For example, suppose we used one of the add instructions to perform the equivalent of the C assignment t=a+b, where variables a, b, and t are integers.
Then the condition codes would be set according to the following C expressions:
The leal instruction does not alter any condition codes, since it is intended to be used in address computations.
Otherwise, all of the instructions listed in Figure 3.7 cause the condition codes to be set.
For reasons that we will not delve into, the inc and dec instructions set the overﬂow and zero ﬂags, but they leave the carry ﬂag unchanged.
The cmp instructions set the condition codes according to the differences of their two operands.
They behave in the same way as the sub instructions, except that they set the condition codes without updating their destinations.
With ATT format, the operands are listed in reverse order, making the code difﬁcult to read.
These instructions set the zero ﬂag if the two operands are equal.
The other ﬂags can be used to determine ordering relations between the two operands.
The test instructions behave in the same manner as the and instructions, except that they set the condition codes without altering their destinations.
Typically, the same operand is repeated (e.g., testl %eax,%eax to see whether %eax is negative, zero, or positive), or one of the operands is a mask indicating which bits should be tested.
We refer to this entire class of instructions as the set instructions; they differ from one another based on which combinations of condition codes they consider, as indicated by the different sufﬁxes for the instruction names.
It is important to recognize that the sufﬁxes for these instructions denote different conditions and not different operand sizes.
For example, instructions setl and setb denote “set less” and “set below,” not “set long word” or “set byte.”
A typical instruction sequence to compute the C expression a < b, where a and b are both of type int, proceeds as follows:
The movzbl instruction clears the high-order 3 bytes of %eax.
For some of the underlying machine instructions, there are multiple possible names, which we list as “synonyms.” For example, both setg (for “set greater”) and setnle (for “set not less or equal”) refer to the same machine instruction.
Compilers and disassemblers make arbitrary choices of which names to use.
Although all arithmetic and logical operations set the condition codes, the descriptions of the different set instructions apply to the case where a comparison instruction has been executed, setting the condition codes according to the computation t = a-b.
More speciﬁcally, let a, b, and t be the integers represented in two’s-complement form by variables a, b, and t, respectively, and so t = a -t.
Similarly, consider testing for signed comparison with the setl, or “set when less,” instruction.
On the other hand, when overﬂow occurs, we will have a < b when a -t.
Combining these cases, theExclusive-Or of the overﬂow and sign bits provides a test for whether a < b.
The other signed comparison tests are based on other combinations of SF ^ OF and ZF.
It is important to note how machine code distinguishes between signed and unsigned values.
Unlike in C, it does not associate a data type with each program value.
Instead, it mostly uses the same instructions for the two cases, because many arithmetic operations have the same bit-level behavior for unsigned and two’s-complement arithmetic.
Some circumstances require different instructions to handle signed and unsigned operations, such as using different versions of right shifts, division and multiplication instructions, and different combinations of condition codes.
Suppose a is in %edx and b is in %eax.
For each of the following instruction sequences, determinewhich data types data_t andwhich comparisons COMP could.
There can be multiple correct answers; you should list them all.
For each of the following instruction sequences, determine which data types data_t and which comparisons TEST could cause the compiler to generate this code.
There can be multiple correct answers; list all correct ones.
Under normal execution, instructions follow each other in the order they are listed.
A jump instruction can cause the execution to switch to a completely new position in the program.
These instructions jump to a labeled destination when the jump condition holds.
Some instructions have “synonyms,” alternate names for the same machine instruction.
The instruction jmp .L1 will cause the program to skip over the movl instruction and instead resume execution with the popl instruction.
In generating the object-code ﬁle, the assembler determines the addresses of all labeled instructions and encodes the jump targets (the addresses of the destination instructions) as part of the jump instructions.
It can be either a direct jump, where the jump target is encoded as part of the instruction, or an indirect jump, where the jump target is read from a register or a memory location.
Direct jumps are written in assembly by giving a label as the jump target, e.g., the label “.L1” in the code shown.
The remaining jump instructions in the table are conditional—they either jumpor continue executing at thenext instruction in the code sequence, depending on some combination of the condition codes.
The names of these instructions and the conditions under which they jump match those of the set instructions (see Figure 3.11)
As with the set instructions, some of the underlying machine instructions have multiple names.
In addition, it helps when interpreting the output of a disassembler.
In assembly code, jump targets are written using symbolic labels.
The assembler, and later the linker, generate the proper encodings of the jump targets.
There are several different encodings for jumps, but some of the most commonly used ones are PC relative.
That is, they encode the difference between the address of the target instruction and the address of the instruction immediately following the jump.
A second encoding method is to give an “absolute” address, using 4 bytes to directly specify the target.
The assembler and linker select the appropriate encodings of the jump destinations.
As an example of PC-relative addressing, the following fragment of assembly code was generated by compiling a ﬁle silly.c.
The disassembled version of the “.o” format generated by the assembler is as follows:
As these examples illustrate, the value of the program counter when performing PC-relative addressing is the address of the instruction following the jump, not that of the jump itself.
This convention dates back to early implementations, when the processor would update the program counter as its ﬁrst step in executing an instruction.
The following shows the disassembled version of the program after linking:
By using a PC-relative encodingof the jump targets, the instructions canbe compactly encoded (requiring just 2 bytes), and the object code can be shifted to different positions in memory without alteration.
Practice Problem 3.15 In the following excerpts from a disassembled binary, some of the information has been replaced by Xs.
In the code that follows, the jump target is encoded inPC-relative formas a 4byte, two’s-complement number.
Thebytes are listed from least signiﬁcant to most, reﬂecting the little-endian byte ordering of IA32
Explain the relation between the annotation on the right and the byte coding on the left.
To implement the control constructs of C via conditional control transfer, the compiler must use the different types of jump instructions we have just seen.
We will go through the most common constructs, starting from simple conditional branches, and then consider loops and switch statements.
The assembly-code implementation ﬁrst compares the two operands (line 3), setting the condition codes.
Actually, it can return a negative value if one of the subtractions overﬂows.
Our interest here is to demonstrate machine code, not to implement robust code.
The generated assembly code is shown (part (c)), along with a C procedure gotodiff (part (b)) that mimics the control ﬂow of the assembly code.
The stack set-up and completion portions of the assembly code have been omitted.
Otherwise, it continues with the execution of code that computes y-x (line 5)
In both cases, the computed result is stored in register %eax, and the program reaches line 10, at which point it executes the stack completion code (not shown)
The general form of an if-else statement in C is given by the template.
Only one of the two branch statements (then-statement or else-statement) is executed.
For this general form, the assembly implementation typically adheres to the following form, where we use C syntax to describe the control ﬂow:
That is, the compiler generates separate blocks of code for then-statement and else-statement.
It inserts conditional and unconditional branches to make sure the correct block is executed.
Write a goto version in C that performs the same computation and mimics the control ﬂow of the assembly code, in the style shown in Figure 3.13(b)
Youmight ﬁnd it helpful to ﬁrst annotate the assembly code as we have done in our examples.
Explain why the assembly code contains two conditional branches, even though the C code has only one if statement.
Practice Problem 3.17 An alternate rule for translating if statements into goto code is as follows:
Rewrite the goto version of absdiff based on this alternate rule.
Can you think of any reasons for choosing one rule over the other?
Practice Problem 3.18 Starting with C code of the form.
To make the code ﬁt into the C code template, you will need to undo some of the reordering of computations done by gcc.
Instead, combinations of conditional tests and jumps are used to implement the effect of loops.
Most compilers generate loop code based on the do-while form of a loop, even though this form is relatively uncommon in actual programs.
Other loops are transformed into dowhile form and then compiled into machine code.
We will study the translation of loops as a progression, starting with do-while and then working toward ones with more complex implementations.
The general form of a do-while statement is as follows:
The effect of the loop is to repeatedly execute body-statement, evaluate test-expr, and continue the loop if the evaluation result is nonzero.
This general form can be translated into conditionals and goto statements as follows:
That is, on each iteration the program evaluates the body statement and then the test expression.
If the test succeeds, we go back for another iteration.
As an example, Figure 3.14(a) shows an implementation of a routine to compute the factorial of its argument, written n!, with a do-while loop.
The C code, the generated assembly code, and a table of register usage is shown.
What is the maximum value of n for which we can represent n!with a 32-bit.
The assembly code shown in Figure 3.14(c) shows a standard implementation of a do-while loop.
Following the initialization of register %edx to hold n and %eax to hold result, the program begins looping.
It ﬁrst executes the body of the loop, consisting here of the updates to variables result and n (lines 4–5)
We see here that the conditional jump (line 7) is the key instruction in implementing a loop.
It determines whether to continue iterating or to exit the loop.
Determining which registers are used for which program values can be challenging, especially with loop code.
Furthermore, since %eax is used to return the function value, it is often chosen to hold program values that are returned.
We therefore conclude that %eax corresponds to program value result.
Akey to understanding how the generated assembly code relates to the original source code is to ﬁnd a mapping between program values and registers.
This taskwas simple enough for the loop of Figure 3.14, but it can be much more challenging for more complex programs.
The C compiler will often rearrange the computations, so that some variables in the C code have no counterpart in the machine code, and new values are introduced into the machine code that do not exist in the source code.
Moreover, it will often try to minimize register usage by mapping multiple program values onto a single register.
The process we described for fact_do works as a general strategy for reverse engineering loops.
Look at how registers are initialized before the loop, updated and tested within the loop, and used after the loop.
Each of these provides a clue that can be combined to solve a puzzle.
Be prepared for surprising transformations, some of which are clearly cases where the compiler was able to optimize the code, and others where it is hard to explain why the compiler chose that particular strategy.
In our experience, gcc often makes transformations that provide no performance beneﬁt and can even decrease code performance.
Make a table of register usage, similar to the one shown in Figure 3.14(b)
Identify test-expr and body-statement in the C code, and the corresponding lines in the assembly code.
Add annotations to the assembly code describing the operation of the program, similar to those shown in Figure 3.14(b)
The general form of a while statement is as follows:
It differs from do-while in that test-expr is evaluated and the loop is potentially terminated before the ﬁrst execution of body-statement.
There are a number of ways to translate a while loop into machine code.
One common approach, also used by gcc, is to transform the code into a do-while loop by using a conditional branch to skip the ﬁrst execution of the body if needed:
This, in turn, can be transformed into goto code as.
Using this implementation strategy, the compiler can often optimize the initial test, for example determining that the test condition will always hold.
The compiler closely followed our template for converting a while loop to a do-while loop, and for translating this loop to goto code.
Figure 3.15 C and assembly code for while version of factorial.
The fact_while_ goto function illustrates the operation of the assembly code version.
In generating this code, gcc makes an interesting transformation that, in effect, introduces a new program variable.
Describe how it relates to the variables in the C code.
Write a goto version of the function (in C) that mimics how the assembly code program operates.
Reverse engineer the operation of this code and then do the following:
Use the assembly-code version to ﬁll in the missing parts of the C code.
The general form of a for loop is as follows:
The C language standard states (with one exception, highlighted in Problem 3.24) that the behavior of such a loop is identical to the following code, which uses a while loop:
It enters a loop where it ﬁrst evaluates the test condition test-expr, exiting if the test fails, then executes the body of the loop body-statement, and ﬁnally evaluates the update expression update-expr.
The compiled form of this code is based on the transformation from while to do-while described previously, ﬁrst giving a do-while form:
This, in turn, can be transformed into goto code as.
As an example, consider a factorial function written with a for loop:
As shown, the natural way of writing a factorial function with a for loop is to multiply factors from 2 up to n, and so this function is quite different from the code we showed using either a while or a do-while loop.
We can identify the different components of the for loop in this code as follows:
Substituting these components into the template we have shown yields the following version in goto code:
Indeed, a close examination of the assembly code produced by gcc closely follows this template:
We see from this presentation that all three forms of loops in C—do-while, while, and for—can be translated by a single strategy, generating code that contains one or more conditional branches.
Conditional transfer of control provides the basic mechanism for translating loops into machine code.
Reverse engineer the operation of this code and then do the following:
Use the assembly-code version to ﬁll in the missing parts of the C code.
Practice Problem 3.24 Executing a continue statement in C causes the program to jump to the end of the current loop iteration.
What would we get if we naively applied our rule for translating the for loop into a while loop? What would be wrong with this code?
How could you replace the continue statement with a goto statement to ensure that the while loop correctly duplicates the behavior of the for loop?
The conventional way to implement conditional operations is through a conditional transfer of control, where the program follows one execution path when a condition holds and another when it does not.
This mechanism is simple and general, but it can be very inefﬁcient on modern processors.
An alternate strategy is through a conditional transfer of data.
This approach computes both outcomes of a conditional operation, and then selects one based on whether or not the condition holds.
This strategy makes sense only in restricted cases, but it can then be implemented by a simple conditional move instruction that is better matched to the performance characteristics of modern processors.
We will examine this strategy and its implementation with more recent versions of IA32 processors.
By giving special command-line parameters on other machines, we can indicate to gcc that the target machine supports conditional move instructions.
This version uses a conditional expression rather than a conditional statement to illustrate the concepts behind conditional data transfers more clearly, but in fact gcc.
The generated assembly code is shown (c), along with a C function cmovdiff (b) that mimics the operation of the assembly code.
The stack set-up and completion portions of the assembly code have been omitted.
Studying the C version, we can see that it computes both y-x and x-y, naming these tval and rval, respectively.
It then tests whether x is less than y, and if so, copies tval to rval before returning rval.
The assembly code in Figure 3.16(c) follows the same logic.
This instruction has the same syntax as a mov instruction, except that it only performs the data movement if the speciﬁed condition holds.
To understand why code based on conditional data transfers can outperform codebasedon conditional control transfers (as inFigure 3.13), wemust understand something about how modern processors operate.
This approach achieves high performance by overlapping the steps of the successive instructions, such as fetching one instruction while performing the arithmetic operations for a previous instruction.
To do this requires being able to determine the sequence of instructions to be executed well ahead of time in order to keep the pipeline full of instructions to be executed.
When the machine encounters a conditional jump (referred to as a “branch”), it often cannot determine yet whether or not the jump will be followed.
Processors employ sophisticated branch prediction logic to try to guesswhether or not each jump instructionwill be followed.As long as it can guess reliably (modern microprocessor designs try to achieve success rates on the order of 90%), the instruction pipeline will be kept full of instructions.
Mispredicting a jump, on the other hand, requires that the processor discard much of the work it has already done on future instructions and then begin ﬁlling the pipeline with instructions starting at the correct location.
As we will see, such a misprediction can incur a serious penalty, say, 20–40 clock cycles of wasted effort, causing a serious degradation of program performance.
As an example, we ran timings of the absdiff function on an Intel Core i7 processor using both methods of implementing the conditional operation.
In addition, the computations performed in each of the two code sequences require only a single clock cycle.As a consequence, the branch misprediction penalty dominates the performance of this function.
From this we can infer that the branch misprediction penalty is around 44 clock cycles.
On the other hand, the code compiled using conditional moves requires around 14 clock cycles regardless of the data being tested.
The ﬂow of control does not depend on data, and this makes it easier for the processor to keep its pipeline full.
How many cycles would the function require when the branch is mispredicted?
The source value is read from either memory or the source register, but it is copied to the destination only if the speciﬁed condition holds.
Unlike the unconditional instructions, where the operand length is explicitly encoded in the instruction name (e.g., movw and movl), the assembler can infer the operand length of a conditional move instruction from the name of the destination register, and so the same instruction name can be used for all operand lengths.
Unlike conditional jumps, the processor can execute conditional move instructions without having to predict the outcome of the test.
These instructions copy the source value S to its destination R when the move condition holds.
Some instructions have “synonyms,” alternate names for the same machine instruction.
To understand how conditional operations can be implemented via conditional data transfers, consider the following general formof conditional expression and assignment:
With traditional IA32, the compiler generates code having a form shown by the following abstract code:
This code contains two code sequences—one evaluating then-expr and one evaluating else-expr.
A combination of conditional and unconditional jumps is used to ensure that just one of the sequences is evaluated.
For the code based on conditional move, both the then-expr and the else-expr are evaluated, with the ﬁnal value chosen based on the evaluation test-expr.
The ﬁnal statement in this sequence is implemented with a conditional movevalue vt is copied to v only if test condition t holds.
Not all conditional expressions can be compiled using conditional moves.
Most signiﬁcantly, the abstract code we have shown evaluates both then-expr and else-expr regardless of the test outcome.
If one of those two expressions could possibly generate an error condition or a side effect, this could lead to invalid behavior.
At ﬁrst, this seems like a good candidate to compile using a conditional move to read the value designated by pointer xp, as shown in the following assembly code:
This implementation is invalid, however, since the dereferencing of xp by the cmovne instruction (line 3) occurs even when the test fails, causing a null pointer dereferencing error.
A similar case holds when either of the two branches causes a side effect, as illustrated by the following function:
This function increments global variable lcount as part of then-expr.
Thus, branching code must be used to ensure this side effect only occurs when the test condition holds.
Using conditional moves also does not always improve code efﬁciency.
For example, if either the then-expr or the else-expr evaluation requires a signiﬁcant.
Compilers must take into account the relative performance of wasted computation versus the potential for performance penalty due to branch misprediction.
In truth, they do not really have enough information tomake this decision reliably; for example, they do not know how well the branches will follow predictable patterns.
Our experiments with gcc indicate that it only uses conditional moves when the two expressions can be computed very easily, for example, with single add instructions.
In our experience, gcc uses conditional control transfers even in many cases where the cost of branch misprediction would exceed even more complex computations.
Overall, then, we see that conditional data transfers offer an alternative strategy to conditional control transfers for implementing conditional operations.
They can only be used in restricted cases, but these cases are fairly common and provide a much better match to the operation of modern processors.
Practice Problem 3.26 In the followingC function, we have left the deﬁnition of operation OP incomplete:
Practice Problem 3.27 Starting with C code of the form.
A switch statement provides a multi-way branching capability based on the value of an integer index.
They are particularly useful when dealing with tests where there can be a large number of possible outcomes.
Not only do they make the C code more readable, they also allow an efﬁcient implementation using a data structure called a jump table.
A jump table is an array where entry i is the address of a code segment implementing the action the program should takewhen the switch index equals i.
The code performs an array reference into the jump table using the switch index to determine the target for a jump instruction.
The advantage of using a jump table over a long sequence of if-else statements is that the time taken to perform the switch is independent of the number of switch cases.
Jump tables are used when there are a number of cases (e.g., four or more) and they span a small range of values.
Figure 3.18(a) shows an example of a C switch statement.
This example has a number of interesting features, including case labels that do not span a contiguous.
Figure 3.18 Switch statement example with translation into extended C.
The translation shows the structure of jump table jt and how it is accessed.
Such tables are supported by gcc as an extension to the C language.
The array jt contains seven entries, each of which is the address of a block of code.
In making this extension, the authors of gcc created a new operator && to create a pointer for a code location.
We recommend that you study the C procedure switch_eg_impl and how it relates assembly code version.
It further simpliﬁes the branching possibilities by treating index as an unsigned value, making use of the fact that negative numbers in a two’s-complement representation map to large positive numbers in an unsigned representation.
In the C and assembly code, there are ﬁve distinct locations to jump to, based on the value of index.
Each of these labels identiﬁes a block of code implementing one of the case branches.
In both the C and the assembly code, the program compares index to 6 and jumps to the code for the default case if it is greater.
The key step in executing a switch statement is to access a code location through the jump table.
This occurs in line 16 in the C code, with a goto statement that references the jump table jt.
This computed goto is supported by gcc as an extension to the C language.
We will see in Section 3.8 how array references are translated into machine code.
Our C code declares the jump table as an array of seven elements, each of which is a pointer to a code location.
In the assembly code, the jump table is indicated by the following declarations, to which we have added comments:
The address associated with this label serves as the base for the indirect jump (line 6)
Most of them simply compute a value for result and then go to the end of the function.
Similarly, the assembly-code blocks compute a value for register %eax and jump to the position indicated by label .L8 at the end of the function.
Examining all of this code requires careful study, but the key point is to see that the use of a jump table allows a very efﬁcient way to implement a multiway branch.
In our case, the program could branch to ﬁve distinct locations with a single jump table reference.
Even if we had a switch statement with hundreds of cases, they could be handled by a single jump table access.
Practice Problem 3.28 In the C function that follows, we have omitted the body of the switch statement.
In the C code, the case labels did not span a contiguous range, and some cases had multiple labels.
In compiling the function, gcc generates the assembly code that follows for the initial part of the procedure and for the jump table.
Variable x is initially at offset 8 relative to register %ebp.
What were the values of the case labels in the switch statement body?
Practice Problem 3.29 For a C function switcher with the general structure.
A procedure call involves passing both data (in the form of procedure parameters and return values) and control from one part of a program to another.
In addition, it must allocate space for the local variables of the procedure on entry and deallocate them on exit.
Most machines, including IA32, provide only simple instructions for transferring control to and from procedures.
The passing of data and the allocation and deallocation of local variables is handled by manipulating the program stack.
IA32 programs make use of the program stack to support procedure calls.
The machine uses the stack to pass procedure arguments, to store return information, to save registers for later restoration, and for local storage.
The portion of the stack allocated for a single procedure call is called a stack frame.
Figure 3.21 diagrams the general structure of a stack frame.
The topmost stack frame is delimited by two pointers, with register %ebp serving as the frame pointer, and register %esp.
The stack is used for passing arguments, for storing return information, for saving registers, and for local storage.
The stack pointer can move while the procedure is executing, and hence most information is accessed relative to the frame pointer.
Suppose procedure P (the caller) calls procedure Q (the callee)
The arguments to Q are contained within the stack frame for P.
In addition, when P calls Q, the return address within P where the program should resume execution when it returns from Q is pushed onto the stack, forming the end of P’s stack frame.
The stack frame for Q starts with the saved value of the frame pointer (a copy of register %ebp), followed by copies of any other saved register values.
Procedure Q also uses the stack for any local variables that cannot be stored in registers.
There are not enough registers to hold all of the local data.
Someof the local variables are arrays or structures andhencemust be accessed by array or structure references.
The address operator ‘&’ is applied to a local variable, and hence we must be able to generate an address for it.
In addition, Q uses the stack frame for storing arguments to any procedures it calls.
Larger arguments (such as structures and larger numeric formats) require larger regions on the stack.
As described earlier, the stack grows toward lower addresses and the stack pointer %esp points to the top element of the stack.
Data can be stored on and retrieved from the stack using the pushl and popl instructions.
Space for datawith no speciﬁed initial value can be allocated on the stack by simply decrementing the stack pointer by an appropriate amount.
Similarly, space can be deallocated by incrementing the stack pointer.
The instructions supporting procedure calls and returns are shown in the following table:
The call instruction has a target indicating the address of the instruction where the called procedure starts.
Like jumps, a call can either be direct or indirect.
The effect of a call instruction is to push a return address on the stack and jump to the start of the called procedure.
The return address is the address of the instruction immediately following the call in the program, so that execution will resume at this location when the called procedure returns.
The ret instruction pops an address off the stack and jumps to this location.
The proper use of this instruction is to have prepared the stack so that the stack pointer points to the place where the preceding call instruction stored its return address.
The call instruction transfers control to the start of a function, while the ret instruction returns back to the instruction following the call.
The following are excerpts of the disassembled code for the two functions:
In this code, we can see that the call instruction with address 0x080483dc in main calls function sum.
This status is shown in Figure 3.22(a), with the indicated values for the stack pointer %esp and the program counter %eip.
The execution of function sum continues until it hits the ret instruction at address 0x080483a4
The leave instruction can be used to prepare the stack for returning.
Alternatively, this preparation can be performed by an explicit sequence of move and pop operations.
Register %eax is used for returning the value from any function that returns an integer or pointer.
Practice Problem 3.30 The following code fragment occurs often in the compiled version of library routines:
Explain why there is no matching ret instruction to this call.
The set of program registers acts as a single resource shared by all of the procedures.
Although only one procedure can be active at a given time, we must make sure that when one procedure (the caller) calls another (the callee), the callee does not overwrite some register value that the caller planned to use later.
For this reason, IA32 adopts a uniform set of conventions for register usage that must be respected by all procedures, including those in program libraries.
By convention, registers %eax, %edx, and %ecx are classiﬁed as caller-save registers.
When procedure Q is called by P, it can overwrite these registers without destroying any data required by P.
On the other hand, registers %ebx, %esi, and %edi are classiﬁed as callee-save registers.
This means that Qmust save the values of any of these registers on the stack before overwriting them, and restore them before returning, because P (or some higher-level procedure) may need these values for its future computations.
In addition, registers %ebp and %esp must be maintained according to the conventions described here.
Procedure P computes y before calling Q, but it must also ensure that the value of y is available after Q returns.
It can store the value of y in its own stack frame before calling Q; when Q returns, procedure P can then retrieve the value of y from the stack.
It can store the value of y in a callee-save register.
If Q, or any procedure called by Q, wants to use this register, it must save the register value in its stack frame and restore the value before it returns (in other words, the callee saves the value)
When Q returns to P, the value of y will be in the callee-save register, either because the register was never altered or because it was saved and restored.
Either convention can be made to work, as long as there is agreement as to which function is responsible for saving which value.
IA32 follows both approaches, partitioning the registers into one set that is caller-save, and another set that is callee-save.
Practice Problem 3.31 The following code sequence occurs right near the beginning of the assembly code generated by gcc for a C procedure:
We see that just three registers (%ebx, %esi, and %edi) are saved on the stack (lines 2–4)
The programmodiﬁes these and three other registers (%eax, %ecx, and %edx)
At the end of the procedure, the values of registers %edi, %esi, and %ebx are restored (not shown), while the other three are left in their modiﬁed states.
Explain this apparent inconsistency in the saving and restoring of register states.
Procedure swap_add retrieves its arguments from the stack frame for caller.
Some of the instructions access stack locations relative to the stack pointer %espwhile others access locations relative to the base pointer %ebp.
These offsets are identiﬁed by the lines shown relative to the two pointers.
Some languages, such as Pascal, provide two different ways to pass parameters to procedures—by value, where the caller provides the actual parameter value, and by reference, where the caller provides a pointer to the value.
InC, all parameters are passed by value, butwe canmimic the effect of a reference parameter by explicitly generating a pointer to a value and passing this pointer to a procedure.
One of the ways in which C++ extends C is the inclusion of reference parameters.
This code saves a copy of %ebp and sets %ebp to the beginning of the stack frame (lines 2–3)
It then allocates 24 bytes on the stack by decrementing the stack pointer (recall that the stack grows toward lower addresses)
Aside Why does gcc allocate space that never gets used?
Themotivation for this convention is to ensure a proper alignment for accessing data.
We will explain the reason for having alignment conventions and how they are implemented in Section 3.9.3
The compiled code for swap_add has three parts: the “setup,” where the stack frame is initialized; the “body,” where the actual computation of the procedure is performed; and the “ﬁnish,” where the stack state is restored and the procedure returns.
Recall that before reaching this part of the code, the call instruction will have pushed the return address onto the stack.
Since this is a callee-save register, it pushes the old value onto the stack as part of the stack frame setup.
At this point, the state of the stack is as shown on the right-hand side of Figure 3.24
Register %ebp has been shifted to serve as the frame pointer for swap_add.
This code retrieves its arguments from the stack frame for caller.
The sum of variables x and y is stored in register %eax to be passed as the returned value.
This code restores the values of registers %ebx and %ebp, while also resetting the stack pointer so that it points to the stored return address, so that the ret instruction transfers control back to caller.
The following code in caller comes immediately after the instruction calling swap_add:
Observe the use of the leave instruction to reset both the stack and the frame pointer prior to return.
Wehave seen in our code examples that the code generated by gcc sometimes uses a leave instruction to deallocate a stack frame, and sometimes it uses one or two popl instructions.
Either approach is acceptable, and the guidelines from Intel and AMD as to which is preferable change over time.
We can see from this example that the compiler generates code tomanage the stack structure according to a simple set of conventions.
Space can be allocated on the stack either by using push instructions or by subtracting offsets from the stack pointer.
Before returning, a function must restore the stack to its original condition by restoring any callee-saved registers and %ebp, and by resetting %esp so that it points to the return address.
It is important for all procedures to follow a consistent set of conventions for setting up and restoring the stack in order for the program to execute properly.
Practice Problem 3.32 A C function fun has the following code body:
Write a prototype for function fun, showing the types and ordering of the arguments p, d, x, and c.
Assume that procedure proc starts executing with the following register values:
Assume that the string “%x %x” is stored at memory location 0x300070
What value does %ebp get set to on line 3?
What value does %esp get set to on line 4?
At what addresses are local variables x and y stored?
Drawadiagramof the stack frame for proc right after scanf returns.
Include as much information as you can about the addresses and the contents of the stack frame elements.
Indicate the regions of the stack frame that are not used by proc.
The stack and linkage conventions described in the previous section allow procedures to call themselves recursively.
Since each call has its own private space on the stack, the local variables of the multiple outstanding calls do not interfere with one another.
Furthermore, the stack discipline naturally provides the proper policy for allocating local storage when the procedure is called and deallocating it when it returns.
The state of the frame is shown just before the recursive call.
For both cases—the terminal condition and the recursive call—the code proceeds to the completion section (lines 15–17) to restore the stack and callee-saved register, and then it returns.
We can see that calling a function recursively proceeds just like any other function call.
Our stack discipline provides a mechanism where each invocation of a function has its own private storage for state information (saved values of the return location, frame pointer, and callee-save registers)
If need be, it can also provide storage for local variables.
The stack discipline of allocation and deallocation naturally matches the call-return ordering of functions.
This method of implementing function calls and returns evenworks formore complex patterns, including mutual recursion (for example, when procedure P calls Q, which in turn calls P)
Practice Problem 3.34 For a C function having the general structure.
What value does rfun store in the callee-save register %ebx?
Fill in the missing expressions in the C code shown above.
Arrays in C are one means of aggregating scalar data into larger data types.
One unusual feature of C is that we can generate pointers to elements within arrays and perform arithmetic with these pointers.
Optimizing compilers are particularly good at simplifying the address computations used by array indexing.
This can make the correspondence between the C code and its translation into machine code somewhat difﬁcult to decipher.
For data type T and integer constant N , the declaration.
The memory referencing instructions of IA32 are designed to simplify array access.
For example, suppose E is an array of int’s, and we wish to evaluate E[i], where the address of E is stored in register %edx and i is stored in register %ecx.
Fill in the following table describing the element size, the total size, and the address of element i for each of these arrays.
That is, for an expressionExpr denoting some object, &Expr is a pointer giving the address of the object.
For an expression AExpr denoting an address, *AExpr gives the value at that address.
The array subscripting operation can be applied to both arrays and pointers.
The array reference A[i] is identical to the expression *(A+i)
It computes the address of the ith array element and then accesses this memory location.
Expanding on our earlier example, suppose the starting address of integer array E and integer index i are stored in registers %edx and %ecx, respectively.
We also show an assembly-code implementation of each expression, with the result being stored in register %eax.
In these examples, the leal instruction is used to generate an address, while movl is used to reference memory (except in the ﬁrst and last cases, where the former copies an address and the latter copies the index)
The ﬁnal example shows that one can compute the difference of two pointers within the same data structure, with the result divided by the size of the data type.
Practice Problem 3.36 Suppose the address of short integer array S and integer index i are stored in registers %edx and %ecx, respectively.
For each of the following expressions, give its type, a formula for its value, and an assembly code implementation.
The result should be stored in register %eax if it is a pointer and register element %ax if it is a short integer.
The general principles of array allocation and referencing hold even when we create arrays of arrays.
Data type row3_t is deﬁned to be an array of three integers.
Array A contains ﬁve such elements, each requiring 12 bytes to store the three integers.
To access elements ofmultidimensional arrays, the compiler generates code to compute the offset of the desired element and thenuses oneof themov instructions with the start of the array as the base address and the (possibly scaled) offset as an index.
In compiling this program, gcc generates the following assembly code:
Use your reverse engineering skills to determine the values of M and N based on this assembly code.
The following is the actual assembly code for the loop.
We see that four variables are maintained in registers within the loop: Arow, Bptr, j, and result.
Machine code considers every pointer to be a byte address, and so in compiling pointer arithmetic, it must scale every increment by the size of the underlying data type.
Practice Problem 3.38 The following C code sets the diagonal elements of one of our ﬁxed-size arrays to val:
Use expressions involving the parameter N rather than integer constants, so that your code will work correctly if N is redeﬁned.
Historically, C only supported multidimensional arrays where the sizes (with the possible exception of the ﬁrst dimension) could be determined at compile time.
Programmers requiring variable-sized arrays had to allocate storage for these arrays using functions such as malloc or calloc, and had to explicitly encode themapping ofmultidimensional arrays into single-dimension ones via row-major indexing, as expressed in Equation 3.1
Figure 3.28 Original and optimized code to compute element i, k of matrix product for ﬁxed-length arrays.
The parameter n must precede the parameter A[n][n], so that the function can compute the array dimensions as the parameter is encountered.
Figure 3.29 Code to compute element i, k of matrix product for variable-sized arrays.
The compiler performs optimizations similar to those for ﬁxed-size arrays.
The dynamic version must use a multiplication instruction to scale i by n, rather than a series of shifts and adds.
In some processors, this multiplication can incur a signiﬁcant performance penalty, but it is unavoidable in this case.
The following is the assembly code for the loop of var_prod_ele:
The need for two values does not show up in the C code, due to the scaling of pointer arithmetic.
The code retrieves the value ofn frommemoryon each iteration to check for loop termination (line 7)
This is an example of register spilling: there are not enough registers to hold all of the needed temporary data, and hence the compiler must keep some local variables in memory.
In this case the compiler chose to spill n, because it is a “read-only” value—it does not change.
IA32 must often spill loop values to memory, since the processor has so few registers.
In general, reading frommemory can be donemore readily than writing to memory, and so spilling read-only variables is preferable.
See Problem 3.61 regarding how to improve this code to avoid register spilling.
The C struct declaration creates a data type that groups objects of possibly different types into a single object.
The different components of a structure are referenced by names.
The implementation of structures is similar to that of arrays in that all of the components of a structure are stored in a contiguous region of memory, and a pointer to a structure is the address of its ﬁrst byte.
The compiler maintains information about each structure type indicating the byte offset of each ﬁeld.
It generates references to structure elements using these offsets as displacements in memory referencing instructions.
The struct data type constructor is the closest thing C provides to the objects of C++ and Java.
It allows the programmer to keep information about some entity in a single data structure, and reference that information with names.
For example, a graphics program might represent a rectangle as a structure:
We could declare a variable r of type struct rect and set its ﬁeld values as follows:
Alternatively, we can both declare the variable and initialize its ﬁelds with a single statement:
It is common to pass pointers to structures from one place to another rather than copying them.
For example, the following function computes the area of a rectangle, where a pointer to the rectangle struct is passed to the function:
The expression (*rp).width dereferences the pointer and selects the width ﬁeld of the resulting structure.
This combination of dereferencing and ﬁeld selection is so common that C provides an alternative notation using ->
For example, we could write a function that rotates a rectangle counterclockwise by.
The objects of C++ and Java are more elaborate than structures in C, in that they also associate a set of methods with an object that can be invoked to perform computation.
In C, we would simply write these as ordinary functions, such as the functions area and rotate_left shown above.
The numbers along the top of the diagram give the byte offsets of the ﬁelds from the beginning of the structure.
To access the ﬁelds of a structure, the compiler generates code that adds the appropriate offset to the address of the structure.
For example, suppose variable r of type struct rec * is in register %edx.
Since the offset of ﬁeld i is 0, the address of this ﬁeld is simply the value of r.
To store into ﬁeld j, the code adds offset 4 to the address of r.
To generate a pointer to an object within a structure, we can simply add the ﬁeld’s offset to the structure address.
For pointer r in register %eax and integer variable i in register %edx, we can generate the pointer value &(r->a[i]) with the single instruction.
As a ﬁnal example, the following code implements the statement.
As these examples show, the selection of the different ﬁelds of a structure is handled completely at compile time.
The machine code contains no information about the ﬁeld declarations or the names of the ﬁelds.
This declaration illustrates that one structure can be embedded within another, just as arrays can be embedded within structures, and arrays can be embedded within arrays.
The following procedure (with some expressions omitted) operates on this structure:
What are the offsets (in bytes) of the following ﬁelds?
The compiler generates the following assembly code for the body of sp_ init:
On the basis of this information, ﬁll in the missing expressions in the code for sp_init.
Unions provide a way to circumvent the type system of C, allowing a single object to be referenced according to multiple types.
The syntax of a union declaration is identical to that for structures, but its semantics are very different.
Rather than having the different ﬁelds reference different blocks of memory, they all reference the same block.
Observe also that the overall size of a union equals the maximum size of any of its ﬁelds.
However, they can also lead to nasty bugs, since they bypass the safety provided by the C type system.
One application is when we know in advance that the use of two different ﬁelds in a data structure will bemutually exclusive.
Then, declaring these twoﬁelds as part of a union rather than a structure will reduce the total space allocated.
For example, suppose we want to implement a binary tree data structure where each leaf node has a double data value, while each internal node has pointers to two children, but no data.
On the other hand, if we declare a node as.
With this encoding, however, there is no way to determine whether a given node is a leaf or an internal node.
A common method is to introduce an enumerated type deﬁning the different possible choices for the union, and then create a structure containing a tag ﬁeld and the union:
In this case, the savings gain of using a union is small relative to the awkwardness of the resulting code.
For data structures with more ﬁelds, the savings can be more compelling.
Unions can also be used to access the bit patterns of different data types.
For example, the following code returns the bit representation of a float as an unsigned:
In this code, we store the argument in the union using one data type, and access it using another.
Interestingly, the code generated for this procedure is identical to that for the following procedure:
The body of both procedures is just a single instruction:
This demonstrates the lack of type information in machine code.
The argument will be at offset 8 relative to %ebp regardless of whether it is a float or an unsigned.
The procedure simply copies its argument as the return value without modifying any bits.
When using unions to combine data types of different sizes, byte-ordering issues can become important.
On a bigendian machine, the role of the two arguments will be reversed.
Practice Problem 3.40 Suppose you are given the job of checking that a C compiler generates the proper code for structure and union access.
You then examine the code generated when compiling the functions to see if they match your expectations.
Suppose in these functions that up and dest are loaded into registers %eax and %edx, respectively.
Fill in the following table with data type TYPE and sequences of 1–3 instructions to compute the expression and store the result at dest.
Try to use just registers %eax and %edx, using register %ecx when these do not sufﬁce.
Such alignment restrictions simplify the design of the hardware forming the interface between the processor and the memory system.
If we can guarantee that any double will be aligned to have its address be a multiple of 8, then the value can be read or written with a single memory operation.
The IA32 hardware will work correctly regardless of the alignment of data.
However, Intel recommends that data be aligned to improve memory system performance.
Note that this requirement means that the least signiﬁcant bit of the address of an object of type shortmust equal zero.
Similarly, any object of type int, or any pointer, must be at an address having the low-order 2 bits equal to zero.
For most IA32 instructions, keeping data aligned improves efﬁciency, but it does not affect program behavior.
On the other hand, some of the SSE instructions for implementing multimedia operations will not work correctly with unaligned data.
Any attempt to access memory with an address that does not satisfy this alignment will lead to an exception, with the default behavior for the program to terminate.
The compiler can allocate storage within a stack frame in such a way that a block can be stored with a 16-byte alignment.
This requirement enhances the memory performance at the expense of some wasted space.
Alignment is enforced by making sure that every data type is organized and allocated in such a way that every object within the type satisﬁes its alignment restrictions.
The compiler places directives in the assembly code indicating the desired alignment for global data.
For code involving structures, the compiler may need to insert gaps in the ﬁeld allocation to ensure that each structure element satisﬁes its alignment requirement.
The structure then has some required alignment for its starting address.
Suppose the compiler used the minimal 9-byte allocation, diagrammed as follows:
Instead, the compiler inserts a 3-byte gap (shown here as shaded in blue) between ﬁelds c and j:
Using our earlier notation, let pointer p have value xp.
In addition, the compiler may need to add padding to the end of the structure so that each element in an array of structureswill satisfy its alignment requirement.
As long as xd is a multiple of 4, all of the alignment restrictions will be satisﬁed.
What are the byte offsets of all the ﬁelds in the structure?
Rearrange the ﬁelds of the structure to minimize wasted space, and then show the byte offsets and total size for the rearranged structure.
Pointers are a central feature of the C programming language.
They serve as a uniform way to generate references to elements within different data structures.
Pointers are a source of confusion for novice programmers, but the underlying concepts are fairly simple.
Here we highlight some key principles of pointers and their mapping into machine code.
This type indicates what kind of object the pointer points to.
In general, if the object has type T , then the pointer has type *T.
For example, the malloc function returns a generic pointer, which is converted to a typed pointer via either an explicit cast or by the implicit casting of the assignment operation.
Pointer types are not part of machine code; they are an abstraction provided by C to help programmers avoid addressing errors.
This value is an address of some object of the designated type.
The special NULL (0) value indicates that the pointer does not point anywhere.
This operator can be applied to any C expression that is categorized as an lvalue, meaning an expression that can appear on the left side of an assignment.
Examples include variables and the elements of structures, unions, and arrays.
We have seen that the machinecode realization of the & operator often uses the leal instruction to compute the expression value, since this instruction is designed to compute the address of a memory reference.
The result is a value having the type associated with the pointer.
Dereferencing is implemented by a memory reference, either storing to or retrieving from the speciﬁed address.
The nameof an array can be referenced (but not updated) as if it were a pointer variable.
Both array referencing and pointer arithmetic require scaling the offsets by the object size.
When we write an expression p+i for pointer p with value p, the resulting address is computed as p + L.
Casting from one type of pointer to another changes its type but not its value.
One effect of casting is to change any scaling of pointer arithmetic.
So for example, if p is a pointer of type char * having value p, then the expression.
This provides a powerful capability for storing and passing references to code, which can be invoked in some other part of the program.
For example, if we have a function deﬁned by the prototype.
The value of a function pointer is the address of the ﬁrst instruction in the machine-code representation of the function.
The syntax for declaring function pointers is especially difﬁcult for novice programmers to understand.
Finally, we see that it is a pointer to a function that takes an int * as an argument and returns int.
The parentheses around *f are required, because otherwise the declaration.
The GNU debugger gdb provides a number of useful features to support the run-time evaluation and analysis of machine-level programs.
With the examples and exercises in this book, we attempt to infer the behavior of a program by just looking at the code.
Using gdb, it becomes possible to study the behavior by watching the program in action, while having considerable control over its execution.
It is very helpful to ﬁrst run objdump to get a disassembled version of the program.
The general scheme is to set breakpoints near points of interest in the program.
These can be set to just after the entry of a function, or at a program address.
When one of the breakpoints is hit during program execution, the program will halt and return control to the user.
From a breakpoint, we can examine different registers and memory locations in various formats.
We can also single-step the program, running just a few instructions at a time, or we can proceed to the next breakpoint.
As our examples suggest, gdb has an obscure command syntax, but the online help information (invoked within gdb with the help command) overcomes this shortcoming.
Rather than using the command-line interface to gdb, many programmers prefer using ddd, an extension to gdb that provides a graphic user interface.
Web Aside ASM:OPT Machine code generated with higher levels of optimization.
In our presentation, we have looked at machine code generated with level-one optimization (speciﬁed with the command-line option ‘-O1’)
In practice, most heavily used programs are compiled with higher levels of optimization.
For example, all of the GNU libraries and packages are compiled with level-two optimization, speciﬁed with the command-line option ‘-O2’
Recent versions of gcc employ an extensive set of optimizations at level two, making the mapping between the source code and the generated code more difﬁcult to discern.
Here are some examples of the optimizations that can be found at level two:
Most procedures have multiple return points, and the stack management code to set up and complete a function is intermixed with the code implementing the operations of the procedure.
Procedure calls are often inlined, replacing them by the instructions implementing the procedures.
This eliminates much of the overhead involved in calling and returning from a function, and it enables optimizations that are speciﬁc to individual function calls.
On the other hand, if we try to set a breakpoint for a function in a debugger, we might never encounter a call to this function.
Starting and stopping quit Exit gdb run Run your program (give command line arguments here) kill Stop your program.
Execution stepi Execute one instruction stepi 4 Execute four instructions nexti Like stepi, but proceed through function calls continue Resume execution finish Run until current function returns.
Useful information info frame Information about current stack frame info registers Values of all the registers help Get information about gdb.
These examples illustrate some of the ways gdb supports debugging of machine-level programs.
Again, this can lead to some surprises when we try to monitor program execution with a debugger.
These optimizations can signiﬁcantly improve program performance, but they make the mapping between source and machine code much more difﬁcult to discern.
Nonetheless, these higher level optimizations have now become standard, and so those who study programs at the machine level must become familiar with the possible optimizations they may encounter.
We have seen that C does not perform any bounds checking for array references, and that local variables are stored on the stack along with state information such as saved register values and return addresses.
This combination can lead to serious program errors, where the state stored on the stack gets corrupted by a write to an out-of-bounds array element.
When the program then tries to reload the register or execute a ret instruction with this corrupted state, things can go seriously wrong.
A particularly common source of state corruption is known as buffer overﬂow.
Typically some character array is allocated on the stack to hold a string, but the size of the string exceeds the space allocated for the array.
The preceding code shows an implementation of the library function gets to demonstrate a serious problem with this function.
It reads a line from the standard input, stopping when either a terminating newline character or some error condition is encountered.
It copies this string to the location designated by argument s, and terminates the string with a null character.
We show the use of gets in the function echo, which simply reads a line from standard input and echoes it back to standard output.
The problem with gets is that it has no way to determine whether sufﬁcient space has been allocated to hold the entire string.
In our echo example, we have purposely made the buffer very small—just eight characters long.
Any string longer than seven characters will cause an out-of-bounds write.
Examining the assembly code generated by gcc for echo shows how the stack is organized.
The location of character array buf is computed as Figure 3.31
As long as the user types at most seven characters, the string returned by gets (including the terminating null) will ﬁt within the space allocated for buf.
A longer string, however, will cause gets to overwrite some of the information.
Character array buf is just below part of the saved state.
An outof-bounds write to buf can corrupt the program state.
As the string gets longer, the following information will get corrupted:
As this table indicates, the corruption is cumulative—as the number of characters increases, more state gets corrupted.
Depending on which portions of the state are affected, the program can misbehave in several different ways:
If the stored value of %ebx is corrupted, then this register will not be restored properly in line 12, and so the caller will not be able to rely on the integrity of this register, even though it should be callee-saved.
If the stored value of %ebp is corrupted, then this register will not be restored properly on line 13, and so the caller will not be able to reference its local variables or parameters properly.
If the stored value of the return address is corrupted, then the ret instruction (line 14) will cause the program to jump to a totally unexpected location.
None of these behaviors would seem possible based on the C code.
The impact of out-of-bounds writing to memory by functions such as gets can only be understood by studying the program at the machine-code level.
A better version involves using the function fgets, which includes as an argument a count on the maximum number of bytes to read.
Problem 3.68 asks you to write an echo function that can handle an input string of arbitrary length.
In general, using gets or any function that can overﬂow storage is considered a bad programming practice.
You run gdb and determine that the error occurs during the execution of the ret instruction of getline.
Fill in the diagram that follows, indicating as much as you can about the stack just after executing the instruction at line 7 in the disassembly.
Label the quantities stored on the stack (e.g., “Return address”) on the right, and their hexadecimal values (if known) within the box.
Modify your diagram to show the effect of the call to gets (line 10)
Besides the potential for buffer overﬂow, what two other things are wrong with the code for getline?
A more pernicious use of buffer overﬂow is to get a program to perform a function that it would otherwise be unwilling to do.
This is one of the most common methods to attack the security of a system over a computer network.
Typically, the program is fed with a string that contains the byte encoding of some executable code, called the exploit code, plus some extra bytes that overwrite the return address with a pointer to the exploit code.
The effect of executing the ret instruction is then to jump to the exploit code.
In one form of attack, the exploit code then uses a system call to start up a shell program, providing the attacker with a range of operating system functions.
In another form, the exploit code performs some otherwise unauthorized task, repairs the damage to the stack, and then executes ret a second time, causing an (apparently) normal return to the caller.
As an example, the famous Internet worm of November 1988 used four different ways to gain access to many of the computers across the Internet.
One was a buffer overﬂow attack on the ﬁnger daemon fingerd, which serves requests by the ﬁnger command.
By invoking ﬁnger with an appropriate string, the worm could make the daemon at a remote site have a buffer overﬂow and execute code that gave the worm access to the remote system.Once the worm gained access to a system, it would replicate itself and consume virtually all of themachine’s computing resources.
As a consequence, hundreds ofmachines were effectively paralyzed until security experts could determine how to eliminate the worm.
Even to this day, however, people continue to ﬁnd security leaks in systems that leave them vulnerable to buffer overﬂow attacks.
Any interface to the external environment should be made “bullet proof” so that no behavior by an external agent can cause the system to misbehave.
Both worms and viruses are pieces of code that attempt to spread themselves among computers.
As described by Spafford [102], aworm is a program that can runby itself and can propagate a fullyworking version of itself to othermachines.
A virus is a piece of code that adds itself to other programs, including operating systems.
In the popular press, the term “virus” is used to refer to a variety of different strategies for spreading attacking code among systems, and so you will hear people saying “virus” for what more properly should be called a “worm.”
Buffer overﬂow attacks have become so pervasive and have caused so many problems with computer systems that modern compilers and operating systems have implemented mechanisms to make it more difﬁcult to mount these attacks and to limit the ways bywhich an intruder can seize control of a system via a buffer overﬂow attack.
In this section, we will present ones that are provided by recent versions of gcc for Linux.
In order to insert exploit code into a system, the attacker needs to inject both the code as well as a pointer to this code as part of the attack string.
Generating this pointer requires knowing the stack address where the string will be located.
Historically, the stack addresses for a program were highly predictable.
For all systems running the same combination of program and operating system version, the stack locations were fairly stable across many machines.
So, for example, if an attacker could determine the stack addresses used by a common Web server, it could devise an attack that would work on many machines.
Using infectious disease as an analogy, many systems were vulnerable to the exact same strain of a virus, a phenomenon often referred to as a security monoculture [93]
The idea of stack randomization is to make the position of the stack vary from one runof aprogram toanother.Thus, even ifmanymachines are running identical code, they would all be using different stack addresses.
This is implemented by allocating a random amount of space between 0 and n bytes on the stack at the start of a program, for example, by using the allocation function alloca, which allocates space for a speciﬁed number of bytes on the stack.
This allocated space is not used by the program, but it causes all subsequent stack locations to vary from one execution of a program to another.
The allocation range n needs to be large enough to get sufﬁcient variations in the stack addresses, yet small enough that it does not waste too much space in the program.
The following code shows a simpleway to determine a “typical” stack address:
This code simply prints the address of a local variable in the main function.
By comparison, running on an older Linux system, the same address occurred every time.
It is one of a larger class of techniques known as address-space layout randomization, or ASLR [95]
With ASLR, different parts of the program, including program code, library code, stack, global variables, and heap data, are loaded into different regions ofmemory each time a program is run.
Thatmeans that a program running on onemachine will have very different address mappings than the same program running on other machines.
Overall, however, a persistent attacker can overcome randomization by brute force, repeatedly attempting attacks with different addresses.
A common trick is to include a long sequence of nop (pronounced “no op,” short for “no operation”) instructions before the actual exploit code.
Executing this instruction has no effect, other than incrementing the program counter to the next instruction.
As long as the attacker can guess an address somewhere within this sequence, the program will run through the sequence and then hit the exploit code.
The common term for this sequence is a “nop sled” [94], expressing the idea that the program “slides” through the sequence.
We can see that stack randomization and other aspects ofASLR can increase the effort required to successfully attack a system, and therefore greatly reduce the rate at which a virus or worm can spread, but it cannot provide a complete safeguard.
If we attempted a buffer overrun with a 128-byte nop sled, how many attempts would it take to exhaustively test all starting addresses?
Figure 3.33 Stack organization for echo function with stack protector enabled.
A special “canary” value is positioned between array buf and the saved state.
The code checks the canary value to determine whether or not the stack state has been corrupted.
A second line of defense is to be able to detect when a stack has been corrupted.
We saw in the example of the echo function (Figure 3.31) that the corruption typically occurs when we overrun the bounds of a local buffer.
In C, there is no reliable way to prevent writing beyond the bounds of an array.
Instead, we can try to detect when such a write has occurred before any harmful effects can occur.
Recent versions ofgcc incorporate amechanismknownas stack protector into the generated code to detect buffer overruns.
This canary value, also referred to as a guard value, is generated randomly each time the program is run, and so there is no easy way for an attacker to determine what it is.
Before restoring the register state and returning from the function, the program checks if the canary has been altered by some operation of this function or one that it has called.
Recent versions of gcc try to determine whether a function is vulnerable to a stack overﬂow, and insert this type of overﬂow detection automatically.
When we compile the function echo without this option, and hence with stack protector enabled, we get the following assembly code:
The term “canary” refers to the historic use of these birds to detect the presence of dangerous gasses in coal mines.
Stack protection does a good job of preventing a buffer overﬂow attack from corrupting state stored on the program stack.
It incurs only a small performance penalty, especially because gcc only inserts it when there is a local buffer of type char in the function.
Of course, there are other ways to corrupt the state of an executing program, but reducing the vulnerability of the stack thwarts many common attack strategies.
Practice Problem 3.45 The function intlen, along with the functions len and iptoa, provides a very convoluted way of computing the number of decimal digits required to represent an integer.Wewill use this as away to study someaspects of thegcc stackprotector facility.
The following show portions of the code for intlen, compiled both with and without stack protector:
For both versions: What are the positions in the stack frame for buf, v, and (when present) the canary value?
How would the rearranged ordering of the local variables in the protected code provide greater security against a buffer overrun attack?
A ﬁnal step is to eliminate the ability of an attacker to insert executable code into a system.
One method is to limit which memory regions hold executable code.
In typical programs, only the portion of memory holding the code generated by the compiler need be executable.
The other portions can be restricted to allow just reading and writing.
The hardware supports different forms of memory protection, indicating the forms of access allowed by both user programs and by the operating system kernel.
Many systems allow control over three forms of access: read (reading data from memory), write (storing data into memory), and execute (treating the memory contents as machine-level code)
The stack had to be kept both readable andwritable, and therefore the bytes on the stack were also executable.
Various schemes were implemented to be able to limit some pages to being readable but not executable, but these generally introduced signiﬁcant inefﬁciencies.
More recently, AMD introduced an “NX” (for “no-execute”) bit into the memoryprotection for its 64-bit processors, separating the read andexecute access modes, and Intel followed suit.With this feature, the stack can bemarked as being readable and writable, but not executable, and the checking of whether a page is executable is performed in hardware, with no penalty in efﬁciency.
Some types of programs require the ability to dynamically generate and execute code.
They all have the properties that they require no special effort on the part of the programmer and incur very little or no performance penalty.
Each separately reduces the level of vulnerability, and in combination they become even more effective.
Although a C compiler does a good job of converting the computations we express in a program into machine code, there are some features of a machine that cannot be accessed by a C program.
It is ironic that the hardware performs this computation as part of every arithmetic or logical operation, but there is no way for a C program to determine the value of the PF condition code.
There are two ways to incorporate assembly code into C programs.
First, we can write an entire function as a separate assembly-code ﬁle and let the assembler and linker combine this with code we havewritten inC.
Second, we canuse the inline assembly featureofgcc, wherebrief sections of assembly code can be incorporated into a C program using the asm directive.
This approach has the advantage that it minimizes the amount of machine-speciﬁc code.
Of course, including assembly code in a C program makes the code speciﬁc to a particular class of machines (such as IA32), and so it should only be used when the desired feature can only be accessed in this way.
Intel’s IA32 instruction set architecture (ISA) has been the dominant instruction format for the world’s computers for many years.
Even though subsequent processor generations have introduced new instruction types and formats, many compilers, including gcc, have avoided using these features in the interest ofmaintaining backward compatibility.
A shift is underway to a 64-bit version of the Intel instruction set.
Newer versions of Linux and Windows support this extension, although systems still run only 32bit versions of these operating systems.
The developers of gcc were able to exploit these features, as well as those of more recent generations of the IA32 architecture, to obtain substantial performance improvements.
For example, procedure parameters are now passed via registers rather than on the stack, greatly reducing the number of memory read and write operations.
It is now feasible to buy more than this amount of RAM for a machine, but the system cannot make effective use of it.
For applications that involve manipulating large data sets, such as scientiﬁc computing, databases, and data mining, the 32-bit word size makes life difﬁcult for programmers.
They must write code using out-of-core algorithms,5 where the data reside on disk and are explicitly read into memory for processing.
Further progress in computing technology requires shifting to a larger word size.
Following the tradition of growing word sizes by doubling, the next logical step is 64 bits.
In fact, 64-bit machines have been available for some time.
Digital Equipment Corporation introduced its Alpha processor in 1992, and it became a popular choice for high-end computing.
At the time, however, Intel was not a serious contender for high-end computers, and so the company was under less pressure to switch to 64 bits.
The physical memory of a machine is often referred to as core memory,dating to an era when each bit of a random-access memory was implemented with a magnetized ferrite core.
Its Very Large Instruction Word (VLIW) format packs multiple instructions into bundles, allowing higher degrees of parallel execution.
Although the performance of Itanium-based systems has improved, they have not captured a signiﬁcant share of the computer market.
For years, AMD had lagged just behind Intel in technology, and so they were relegated to competing with Intel on the basis of price.
Typically, Intel would introduce a newmicroprocessor at a price premium.
It maintains full backward compatibility with IA32, but it adds new data formats, as well as other features that enable higher capacity and higher performance.
With x86-64, AMD captured some of the high-end market that had historically belonged to Intel.
AMD’s recent generations of processors have indeed proved very successful as high-performance machines.
These features would only be used when code was compiled with special settings of command-line options.
Switching to x86-64 as a target provided an opportunity for gcc to give up backward compatibility and instead exploit these newer features even with standard command-line options.
Much of the program state is held in registers rather than on the stack.
Integer and pointer procedure arguments (up to 6) are passed via registers.
Some procedures do not need to access the stack at all.
Conditional operations are implemented using conditional move instructions when possible, yielding better performance than traditional branching code.
We also see that the preﬁx “long” changes integers to 64 bits, allowing a considerably larger range of values.
In fact, data type long becomes identical to long long.
Moreover, the hardware provides registers that can hold 64-bit integers and instructions that can operate on these quad words.
Moreover, the long double data type is only supported by an older class of ﬂoating-point instructions that have some idiosyncratic properties (see Web Aside data:ia32-fp), while both the float and double data types are supported by the more recent SSE instructions.
The long double data type should only be used by programs requiring the additional precision and range the extended-precision format provides over the double-precision format.
How much earlier would these transition points occur if we raised our DRAM budget to $10,000?
Below is the C code for simple_l, similar to simple, except that it uses long integers:
When gcc is run on an x86-64 Linux machine with the command line.
Instead of movl and addl instructions, we see movq and addq.
We see the 64-bit versions of registers (e.g., %rsi and %rdi, rather than %esi and %edi)
The procedure returns a value by storing it in register %rax.
Arguments xp and y are passed in registers (%rdi and %rsi, respectively) rather than on the stack.
This eliminates the need to fetch the arguments from memory.
The relative performance of the two versions depends greatly on the hardware on which they are executed.
This 50% performance improvement on the same machine with the same C code is quite striking.
On other machines we have tried, the performance difference lies somewhere between these two extremes.
The low-order 32 bits of each register can be accessed directly.
The low-order 8 bits of each register can be accessed directly.
This is true in IA32 only for the ﬁrst four registers (%al, %cl, %dl, %bl)
The byte-size versions of the other IA32 registers are named %sil, %dil, %spl, and %bpl.
The byte-size versions of the new registers are named %r8b–%r15b.
For backward compatibility, the second byte of registers %rax, %rcx, %rdx, and %rbx can be directly accessed by instructions having single-byte operands.
As with IA32, most of the registers can be used interchangeably, but there are some special cases.
Register %rsp has special status, in that it holds a pointer to the top stack element.
Unlike in IA32, however, there is no frame pointer register; register %rbp is available for use as a general-purpose register.
Particular conventions are used for passing procedure arguments via registers and for how registers are to be saved and restored during procedure calls, as is discussed in Section 3.13.4
In addition, some arithmetic instructions make special use of registers %rax and %rdx.
The existing eight registers are extended to 64-bit versions, and eight new registers are added.
As an example of PC-relative data addressing, consider the following procedure, which calls the function simple_l examined earlier:
When this function is compiled, assembled, and linked, we get the following executable code (as generated by the disassembler objdump):
It does this by copying the constant value 0x601020 into register %edi.
The upper 32 bits of %rdi are automatically set to zero.
Some instructions require the destination to be a register, indicated by R.
Others can have either a register or a memory location as destination, indicated by D.
Most of these instructions fall within a class of instructions seenwith IA32
The movabsq instruction, on the other hand, has no counterpart in IA32
The movabsq instruction only allows immediate data (shown as I ) as the source value.
Others allow immediate data, a register, or memory (shown as S)
Some instructions require the destination to be a register (shown as R), while others allow both register and memory destinations (shown as D)
Moving from a smaller data size to a larger one can involve either sign extension (movs) or zero extension (movz)
Similarly, the instruction movzbq has the exact same behavior as movzbl when the destination is a register—both set the upper 56 bits of the destination register to zero.
The new stack instructions pushq and popq allow pushing and popping of 64-bit values.
Assume argument x is in the appropriately named portion of register %rdi (i.e., %rdi, %edi, %di, or %dil), and that some form of data movement instruction is to be used to perform the type conversion and to copy the value to the appropriately named portion of register %rax.
Fill in the following table indicating the instruction, the source register, and the destination register for the following combinations of source and destination type:
In Figure 3.7, we listed a number of arithmetic and logic instructions, using a class name, such as “add”, to represent instructions for different operand sizes, such as addb (byte), addw (word), and addl (long word)
To each of these classes we now add instructions that operate on quad words with the sufﬁx ‘q’
Examples of these quad-word instructions include leaq (load effective address), incq (increment), addq (add), and salq (shift left)
These quad-word instructions have the same argument types as their shorter counterparts.
When mixing operands of different sizes, gcc must choose the right combinations of arithmetic instructions, sign extensions, and zero extensions.
Thesedepend on subtle aspects of type conversion and the behavior of the instructions for different operand sizes.
The assembly code generated for this function is as follows:
It is then sign-extended to 64 bits using the cltq instruction, which we will see is a special instruction equivalent to executing the instruction movslq %eax,%rax.
Practice Problem 3.48 A C function arithprob with arguments a, b, c, and d has the following body:
The arguments and return value are all signed integers of various lengths.
Arguments a, b, c, and d are passed in the appropriate regions of registers %rdi, %rsi, %rdx, and %rcx, respectively.
Based on this assembly code, write a function prototype describing the return and argument types for arithprob.
For example, the imulq and mulq instructions store the result of multiplying two 64-bit values—the ﬁrst as given by the source operand and the second from register %rax.
They then store the quotient in register %rax and the remainder in register %rdx.
Preparing the dividend depends on whether unsigned (divq) or signed (idivq) division is to be performed.
In the former case, register %rdx is simply set to zero.
ATT-format instruction cqto is called cqo in Intel and AMD documentation.
Instruction cltq is called cdqe in Intel and AMD documentation.
These instructions set the condition codes without updating any other registers.
Both were compiled from the C code shown in Figure 3.15
Aside Why is there a rep instruction in this code?
The answer to this puzzle can be seen in AMD’s guidelines to compiler writers [1]
They recommend using the combination of rep followed by ret to avoid making the ret instruction be the destination of a conditional jump instruction.
Without the rep instruction, the jg instruction would proceed to the ret instruction when the branch is not taken.
According to AMD, their processors cannot properly predict the destination of a ret instruction when it is reached from a jump instruction.
The rep instruction serves as a form of no-operation here, and so inserting it as the jump destination does not change behavior of the code, except to make it faster on AMD processors.
You will ﬁnd it useful to convert the decimal constant on line 4 to hexadecimal.
Use the assembly-code version to ﬁll in the missing parts of the C code.
By doubling the register set, programs need not be so dependent on the stack for storing and retrieving procedure information.
This can greatly reduce the overhead for procedure calls and returns.
Here are some of the highlights of how procedures are implemented with x86-64:
Arguments (up to the ﬁrst six) are passed to procedures via registers, rather thanon the stack.
This eliminates the overheadof storing and retrieving values on the stack.
The callq instruction stores a 64-bit return address on the stack.
Only functions that cannot keep all local variables in registers need to allocate space on the stack.
Functions can access storage on the stack up to 128 bytes beyond (i.e., at a lower address than) the current value of the stack pointer.
This allows some functions to store information on the stack without altering the stack pointer.
Instead, references to stack locations are made relative to the stack pointer.
Most functions allocate their total stack storage needs at the beginning of the call and keep the stack pointer at a ﬁxed position.
As with IA32, some registers are designated as callee-save registers.
These must be saved and restored by any procedure that modiﬁes them.
The registers are used in a speciﬁed order and named according to the argument sizes.
Up to six integral (i.e., integer and pointer) arguments can be passed via registers.
The registers are used in a speciﬁed order, with the name used for a register depending on the size of the data type being passed.
Arguments are allocated to these registers according to their ordering in the argument list.
For example, if the ﬁrst argument is 32 bits, it can be accessed as %edi.
As an example of argument passing, consider the following C function having eight arguments:
Practice Problem 3.50 A C function incrprob has arguments q, t, and x of different sizes, and each may be signed or unsigned.
Determine all four valid function prototypes for incrprob by determining the ordering and possible types of the three parameters.
We have already seen that many compiled functions do not require a stack frame.
If all of the local variables can be held in registers, and the function does not call any other functions (sometimes referred to as a leaf procedure, in reference to the tree structure of procedure calls), then the only need for the stack is to save the return address.
On the other hand, there are several reasons a function may require a stack frame:
There are too many local variables to hold in registers.
The function uses the address-of operator (&) to compute the address of a local variable.
The function must pass some arguments on the stack to another function.
The function needs to save the state of a callee-save register before modifying it.
When any of these conditions hold, we ﬁnd the compiled code for the function creating a stack frame.Unlike the code for IA32, where the stackpointer ﬂuctuates back and forth as values are pushed and popped, the stack frames for x86-64 procedures usually have a ﬁxed size, set at the beginning of the procedure by decrementing the stack pointer (register %rsp)
The stack pointer remains at a ﬁxedpositionduring the call, making it possible to access data using offsets relative to the stack pointer.
As a consequence, the frame pointer (register %ebp) seen in IA32 code is no longer needed.
Whenever one function (the caller) calls another (the callee), the return address gets pushed onto the stack.
By convention, we consider this part of the caller’s stack frame, in that it encodes part of the caller’s state.
But this information gets popped from the stack as control returns to the caller, and so it does not affect the offsets used by the caller for accessing values within the stack frame.
The following function illustrates many aspects of the x86-64 stack discipline.
Despite the length of this example, it is worth studying carefully.
The parameters are allocated eight bytes each, even though parameter x4 requires only a single byte.
After proc returns, the local variables are combined to compute the ﬁnal expression, which is returned in register %rax.
The stack space is deallocated by simply incrementing the stack pointer before the ret instruction.
Figure 3.41(b) illustrates the stack during the execution of proc.
Observe how call_proc changed the stack pointer only once during its execution.
Minimizing the amount of movement by the stack pointer simpliﬁes the compiler’s task of generating reference to stack elements using offsets from the stack pointer.
Of course, an argument register can be used when there are fewer than six arguments or when the function is done using that argument, and %rax can be used multiple times before the ﬁnal result is generated.
We illustrate the use of callee-saved registerswith a somewhat unusual version of a recursive factorial function:
To compute the factorial of a value x, this function would be called at the top level as follows:
This function decrements the stack pointer after saving some of the state.
Being able to accessmemory beyond the stack pointer is an unusual feature of x86-64
It requires that the virtual memory management system allocate memory for that region.
The ABI refers to this area as the red zone.
It must be kept available for reading and writing as the stack pointer moves.
Draw a diagram indicating the stack locations used by this function and their offsets relative to the stack pointer.
Annotate the assembly code to describe the effect of each instruction.
What interesting feature does this example illustrate about the x86-64 stack discipline?
Annotate the assembly code to describe the effect of each instruction.
How does this function manage the stack frame differently from others we have seen?
Data structures follow the same principles in x86-64 as they do in IA32: arrays are allocated as sequences of identically sized blocks holding the array elements, structures are allocated as sequences of variably sized blocks holding the structure elements, and unions are allocated as a single block big enough to hold the largest union element.
One difference is that x86-64 follows a more stringent set of alignment requirements.
For any scalar data type requiring K bytes, its starting address must be a multiple of K.
Thus, data types long and double as well as pointers, must be aligned on 8-byte boundaries.
Both AMD and the authors of gcc deserve credit for moving x86 processors into a new era.
The formulation of both the x86-64 hardware and the programming conventions changed the processor from one that relied heavily on the stack to hold program state to one where the most heavily used part of the state is held in the much faster and expanded register set.
Thus far, we have only considered programs that represent and operate on integer data types.
In order to implement programs that make use of ﬂoating-point data, we must have some method of storing ﬂoating-point data and additional instructions to operate on ﬂoating-point values, to convert between ﬂoating-point and integer values, and to perform comparisons between ﬂoating-point values.
We also require conventions on how to pass ﬂoating-point values as function arguments and to return themas function results.We call this combination of storage model, instructions, and conventions the ﬂoating-point architecture for a machine.
The ﬁrst, referred to as “x87,” dates back to the earliest days of Intel microprocessors and until recently was the standard implementation.
The second, referred to as “SSE,” is based on recent additions to x86 processors to support multimedia applications.
In the original Intel machines, ﬂoating point was performed by a separate coprocessor, a unit with its own registers and processing capabilities that executes a subset of the instructions.
In a stack model, some instructions read values from memory and push them onto the stack; others pop operands from the stack, perform an operation, and then push the result; while others pop values from the stack and store them to memory.
This approach has the advantage that there is a simple algorithm by which a compiler can map the evaluation of arithmetic expressions into stack code.
Modern compilers can make many optimizations that do not ﬁt well within a stack model, for example, making use of a single computed result multiple times.
Consequently, the x87 architecture implements an odd hybrid between a stack and a register model, where the different elements of the stack can be read and written explicitly, as well as shifted up and down by pushing and popping.
In addition, the x87 stack is limited to a depth of eight values; when additional values are pushed, the ones at the bottom are simply discarded.
Hence, the compiler must keep track of the stack depth.
Furthermore, a compiler must treat all ﬂoating-point registers as being caller-save, since their values might disappear off the bottom if other procedures push more values onto the stack.
Unlike the stack-based architecture of x87, SSE-based ﬂoating point uses a straightforward register-based approach, a much better target.
With SSE2, ﬂoating-point code is similar to integer code, except that it uses a different set of registers and instructions.
In this chapter, we have peered beneath the layer of abstraction provided by the C language to get a view of machine-level programming.
By having the compiler generate an assembly-code representation of the machine-level program, we gain insights into both the compiler and its optimization capabilities, alongwith themachine, its data types, and its instruction set.
In Chapter 12, we will see many examples where application programmers need to know whether a program variable is on the run-time stack, in some dynamically allocated data structure, or part of the global program data.
Understanding how programs map onto machines makes it easier to understand the differences between these kinds of storage.
Machine-level programs, and their representation by assembly code, differ in many ways from C programs.
The program is expressed as a sequence of instructions, each of which performs a single operation.
Parts of the program state, such as registers and the run-time stack, are directly visible to the programmer.
Only low-level operations are provided to support data manipulation and program control.
The compiler must usemultiple instructions to generate and operate on different data structures and to implement control constructs such as conditionals, loops, and procedures.
We have covered many different aspects of C and how it gets compiled.
We have seen that the lack of bounds checking in C makes many programs prone to buffer overﬂows.
This has made many systems vulnerable to attacks by malicious intruders, although recent safeguards provided by the run-time system and the compiler help make programs more secure.
For example, compiling C++ is very similar to compiling C.
In fact, early implementations of C++ ﬁrst performed a source-to-source conversion from C++ to C and generated object-code by running a C compiler on the result.
C++ objects are represented by structures, similar to a C struct.
Methods are represented by pointers to the code implementing the methods.
By contrast, Java is implemented in an entirely different fashion.
The object code of Java is a special binary representation known as Java byte code.
This code can be viewed as a machine-level program for a virtual machine.
As its name suggests, this machine is not implemented directly in hardware.
Alternatively, an approach known as just-in-time compilation dynamically translates byte code sequences into machine instructions.
This approach provides faster execution when code is executedmultiple times, such as in loops.
Both Intel and AMD provide extensive documentation on their processors.
Still, these remain the authoritative references about the behavior of each instruction.
This interface describes details for procedure linkages, binary code ﬁles, and a number of other features that are required for machine-code programs to execute properly.
As we have discussed, the ATT format used by gcc is very different from the Intel format used in Intel documentation and by other compilers (including the Microsoft compilers)
Blum’s book [9] is one of the few references based on ATT format, and it provides an extensive description of how to embed assembly code into C programs using the asm directive.
Muchnick’s book on compiler design [76] is considered the most comprehensive reference on code-optimization techniques.
It covers many of the techniques we discuss here, such as register usage conventions and the advantages of generating code for loops based on their do-while form.
Much has beenwritten about the use of buffer overﬂow to attack systems over the Internet.
Since then a number of papers and projects have generated ways both to create and to prevent buffer overﬂow attacks.
Seacord’s book [94] provides a wealth of information about buffer overﬂow and other attacks on code generated by C compilers.
Write C code for decode2 that will have an effect equivalent to our assembly code.
Describe the algorithm used to compute the product, and annotate the assembly code to show how it realizes your algorithm.
The preceding code was generated by compiling C code that had the following overall form:
Your task is to ﬁll in themissing parts of theC code to get a programequivalent to the generated assembly code.
Recall that the result of the function is returned in register %eax.
You will ﬁnd it helpful to examine the assembly code before, during, and after the loop to form a consistent mapping between the registers and the program variables.
Which registers hold program values x, n, result, and mask?
Fill in all the missing parts of the C code.
Weshowed a trial implementation using a conditionalmove instruction but argued that it was not valid, since it could attempt to read from a null address.
Write a C function cread_alt that has the same behavior as cread, except that it can be compiled to use conditional data transfer.
When compiled with the command-line option ‘-march=i686’, the generated code should use a conditional move instruction rather than one of the jump instructions.
This code implements the different branches of a switch statement.
Figure 3.44 shows the disassembled machine code for the procedure.
The jump table resides in a different area of memory.
Fill in the body of the switch statement with C code that will have the same behavior as the machine code.
In compiling this program, gcc generates the following assembly code:
Extend Equation 3.1 from two dimensions to three to provide a formula for the location of array element A[i][j][k]
Use your reverse engineering skills to determine the values of R, S, and T based on the assembly code.
Recall that the processor only has six registers available to hold temporary data, since registers %ebp and %esp cannot be used for this purpose.
One of these registers must be used to hold the result of the multiply instruction.
Hence, you must reduce the number of values in the loop from six (result, Arow, Bcol, j, n, and 4*n) to ﬁve.
You will need to ﬁnd a strategy that works for your particular compiler.
Keep trying different strategies until you ﬁnd one that works.
When compiled with optimization level -O2, gcc generates the following code for the inner loop of the function:
Write a C code version of transpose that makes use of the optimizations that occur in this loop.Use the parameter M in your code rather than numeric constants.
In compiling this program, gcc generates the following assembly code:
These get used as ﬁve ﬁelds of 4 bytes each.
How would you describe the general strategy for passing structures as arguments to a function?
How would you describe the general strategy for handling a structure as a return value from a function?
The declarations of the compile-time constant CNT and the structure a_struct are in a ﬁle for which you do not have the necessary access privilege.
Assume that the only ﬁelds in this structure are idx and x.
This declaration illustrates that structures can be embedded within unions.
The following procedure (with some expressions omitted) operates on a.
The compiler generates the following assembly code for the body of proc:
On the basis of this information, ﬁll in the missing expressions in the code for proc.
These ambiguities get resolved as you see where the references lead.
There is only one answer that does not perform any casting and does not violate any type constraints.
Generate a C version of the function, using a while loop.
Being able to switch between these two forms is an important skill to learn.
One important feature is that memory references in IA32 are always given with double-word registers, such as %eax, even if the operand is a byte or single word.
Nonetheless, this exercise will help you become more familiar with the different instruction and operand types.
In this case, we want to reverse the effect of the C compiler to determine what C code gave rise to this assembly code.
The best way is to run a “simulation,” starting with values x, y, and z at the locations designated by pointers xp, yp, and zp, respectively.
Although the operand forms are classiﬁed as type “Memory” in Figure 3.3, no memory access occurs.
The instruction sequence is designed so that the result of each instruction does not affect the behavior of subsequent ones.
By loading parameter n in register %ecx, it can then use byte register %cl to specify the shift amount for the sarl instruction:
Amore direct way of setting register %edx to zero is with the instruction movl $0,%edx.
We can see that the program is performing multiprecision operations on 64-bit data.
Let x denote the value of variable x, and let y denote the value of y, which we can write as y = yh.
We can therefore let s be the low-order 32 bits of x.
The ﬁnal result has tl as the low-order part, and s + th as the high-order part.
Instead, the different instructions determine the operand sizes and whether they are signed or unsigned.
When mapping from instruction sequences back to C code, we must do a bit of detective work to infer the data types of the program values.
We can infer that data_t could be either int, unsigned, or some form of pointer.
For the ﬁrst two cases, they could also have the long size designator.
For the ﬁrst two cases, they could also have the long size designator.
According to the annotation produced by the disassembler, the jump target is at absolute address 0x8048391
According to the byte encoding, this must be at an address 0x12 bytes beyond that of the mov instruction.
Subtracting these gives address 0x804837f, as conﬁrmed by the disassembled code:
The address from which the jump target is to be read is encoded explicitly by the following 4 bytes.
This problem gives you practice for an example with simple control ﬂow.
It also gives you a chance to examine the implementation of logical operations.
The ﬁrst conditional branch is part of the implementation of the && expression.
Converting to this alternate form involves only switching around a few lines of the code:
But the original rule works better for the common case where there is no else statement.
For this case, we can simply modify the translation rule to be as follows:
A translation based on the alternate rule is more cumbersome.
If we build up a table of factorials computed with data type int, we get the following:
We start practicing this skill with a fairly simple loop.
The register usage can be determined by simply looking at how the arguments get fetched.
The test-expr portion is on line 6 in the C code.
We can see that the register is initialized to a + b and then incremented on each iteration.
Similarly, the value of a (held in register %ecx) is incremented on each iteration.
We can therefore see that the value in register %edx will always equal a + b.
This code reverses the bits in x, creating a mirror image.
It does this by shifting the bits of x from left to right, and then ﬁlling these bits in as it shifts val from right to left.
This code has an inﬁnite loop, since the continue statement would prevent index variable i from being updated.
The general solution is to replace the continue statement with a goto statement that skips the rest of the loop body and goes directly to the update portion:
Although it might seemdaunting toﬁt this code into the frameworkof theoriginalC code, youwill ﬁnd that it follows the translation rules fairly closely.
Answering the questions requires you to combine information from several places in the assembly code.
We can see from the ja instruction (line 3) that the code for the default case.
We can see that the only other repeated label in the jump table is .L4, and so this must be the code for the cases C and D.
At ﬁrst it seems quite peculiar—a call instruction with no matching ret.
Then we realize that it is not really a procedure call after all.
This is not a true procedure call, since the control follows the same ordering as the instructions and the return address is popped from the stack.
This is the only way in IA32 to get the value of the program counter into an integer register.
The procedure must save them on the stack before altering their values and restore them before returning.
They can be altered without affecting the behavior of the caller.
The key to solving this problem is to note that the storage of d at p is implemented by the instruction at line 3 of the assembly code, from which you work backward to determine the types and positions of arguments d and p.
Similarly, the subtraction is performed at line 6, and from this you canwork backward to determine the types and positions of arguments x and c.
As this example shows, reverse engineering is like solving a puzzle.
It’s important to identify the points where there is a unique choice, and then work around these points to ﬁll in the rest of the details.
As this example illustrates, the compiler may allocate a signiﬁcant amount of space that never gets used.
An important lesson to learn is that recursive code has the exact same structure as the other functions we have seen.
The stack and register-saving disciplines sufﬁce to make recursive functions operate correctly.
Register %ebx holds the value of parameter x, so that it can be used to compute the result expression.
The assembly code was generated from the following C code:
Like the code of Problem 3.49, this function computes the sum of the bits in argument x.
It recursively computes the sum of all but the least signiﬁcant bit, and then it adds the least signiﬁcant bit to get the result.
Observe that a pointer of any kind is 4 bytes long.
It is important to understand the difference between a pointer and the object being pointed to.
Rather than using movl, as before, we now use movw.
The ﬁrst step is to annotate the assembly code to determine how the address references are computed:
In this case, the compiler was clever in its optimizations.
Let us ﬁrst study the following C code, and then see how it is derived from the assembly code generated for the original function.
This function introduces a variable Abase, of type int *, pointing to the start of array A.
This pointer designates a sequence of 4-byte integers consisting of elements of A in row-major order.
The structure declaration is a variant of the example shown in the text.
It shows that nested structures are allocated by embedding the inner structures within the outer ones.
This problem lets you work out the details of some example structures.
One strategy that works, when all data elements have a length equal to a power of two, is to order the structure elements in descending order of size.
It demonstrates the dangers of out-ofbounds memory references and the basic ideas behind buffer overﬂow.
The low-order byte was overwritten by the terminating null character.
These values will be loaded into the registers before getline returns.
The call to malloc should have had strlen(buf)+1 as its argument, and the code should also check that the returned value is not equal to NULL.
This example clearly shows that the degree of randomization in this version of Linux would provide only minimal deterrence against an overﬂow attack.
In the protected code, local variable v is positioned closer to the top of the stack than buf, and so an overrun of buf will not corrupt the value of v.
In fact, buf is positioned so that any buffer overrun will corrupt the canary value.
These numbers, of course, should not be taken too literally.
It would require scaling memory technology well beyond what are believed to be fundamental physical limits of the current technology.
Nonetheless, it indicates that, within the lifetimes of many readers of this book, there will be systems with exabyte-scale memory systems.
In some cases, we make use of the property that the movl instruction will set the upper 32 bits of the destination register to zeros.
We show that the long to int conversion can use either movslq or movl, even though one will sign extend the upper 32 bits, while the other will zero extend it.
This is because the values of the upper 32 bits are ignored for any subsequent instruction having %eax as an operand.
The ﬁrst movslq instruction sign extends d to a long integer prior to its multiplication by c.
This implies that d has type int and c has type long.
The movsbl instruction (line 4) sign extends b to an integer prior to its multiplication by a.
This means that b has type char and a has type int.
The sum is computed using a leaq instruction, indicating that the return value has type long.
From this, we can determine that the unique prototype for arithprob is.
It uses several tricks that look fairly obscure at the assembly-code level.
Itmasks off the high-order bits to get the ﬁnal result.
The movslq instruction sign extends the sum (a copy of *t) to a long integer.
From this, we can infer that t must be a pointer to a signed integer.
The addq instruction adds the sign-extended value of the previous sum to the location indicated by the second argument register.
From this, we can infer that q is the second argument and that it is a pointer to a long integer.
There are four valid prototypes for incrprob, depending on whether or not x is long, and whether it is signed or unsigned.
It can use space beyond the stack pointer for its local storage, never altering the stack pointer.
It stores all of its local values in the region beyond the stack pointer.
Since %rbx is callee-saved, itmust be stored on the stack.
Since this is the only use of the stack for this function, the code uses push and pop instructions to save and restore the register.
Instead of explicitly decrementing and incrementing the stack pointer, the code can use pushq and popq to both modify the stack pointer and to save and restore register state.
Modern microprocessors are among the most complex systems ever created by humans.
A single silicon chip, roughly the size of a ﬁngernail, can contain a complete high-performance processor, large cache memories, and the logic required to interface it to external devices.
Even the embedded processors found in everyday appliances such as cell phones, personal digital assistants, and handheld game systems are far more powerful than the early developers of computers ever envisioned.
Thus far, wehaveonly viewed computer systemsdown to the level ofmachinelanguage programs.
We have seen that a processor must execute a sequence of instructions, where each instruction performs some primitive operation, such as adding two numbers.
An instruction is encoded in binary form as a sequence of 1 or more bytes.
The instructions supported by a particular processor and their byte-level encodings are known as its instruction-set architecture (ISA)
Different “families” of processors, such as Intel IA32, IBM/Freescale PowerPC, and the ARM processor family have different ISAs.
A program compiled for one type of machine will not run on another.
On the other hand, there are many different models of processors within a single family.
Each manufacturer produces processors of ever-growing performance and complexity, but the differentmodels remain compatible at the ISA level.
Popular families, such as IA32, have processors supplied by multiple manufacturers.
Thus, the ISA provides a conceptual layer of abstraction between compiler writers, who need only know what instructions are permitted and how they are encoded, and processor designers, who must build machines that execute those instructions.
In this chapter, we take a brief look at the design of processor hardware.
We study the way a hardware system can execute the instructions of a particular ISA.
This view will give you a better understanding of how computers work and the technological challenges faced by computer manufacturers.
One important concept is that the actual way a modern processor operates can be quite different from the model of computation implied by the ISA.
The ISA model would seem to imply sequential instruction execution, where each instruction is fetched and executed to completion before the next one begins.
By executing different parts of multiple instructions simultaneously, the processor can achieve higher performance than if it executed just one instruction at a time.
Special mechanisms are used to make sure the processor computes the same results as it would with sequential execution.
This idea of using clever tricks to improve performance while maintaining the functionality of a simpler and more abstract model is well known in computer science.
Examples include the use of caching in Web browsers and information retrieval data structures such as balanced binary trees and hash tables.
This is a task for experts working at fewer than 100 companies worldwide.
It is intellectually interesting and important.There is an intrinsic value in learning how things work.
It is especially interesting to learn the inner workings of.
Processor design embodies many of the principles of good engineering practice.
It requires creating a simple and regular structure to perform a complex task.
Understanding how the processor works aids in understanding how the overall computer system works.
In Chapter 6, we will look at the memory system and the techniques used to create an image of a very large memory with a very fast access time.
Seeing the processor side of the processor-memory interface will make this presentation more complete.
Although few people design processors, many design hardware systems that contain processors.This has become commonplace as processors are embedded into real-world systems such as automobiles and appliances.
Embeddedsystem designers must understand how processors work, because these systems are generally designed and programmed at a lower level of abstraction than is the case for desktop systems.
You justmight work on a processor design.Although the number of companies producing microprocessors is small, the design teams working on those processors are already large and growing.
There can be over 1000 people involved in the different aspects of a major processor design.
In this chapter, we start by deﬁning a simple instruction set that we use as a running example for our processor implementations.
Still, it is sufﬁciently complete to allow us to write simple programs manipulating integer data.
Designing a processor to implement Y86 requires us to face many of the challenges faced by processor designers.
We describe the basic building blocks used in a processor and how they are connected together and operated.
We also introduce a simple language, HCL (for “HardwareControl Language”), to describe the control portions of hardware systems.
We will later use this language to describe our processor designs.
Even if you already have some background in logic design, read this section to understand our particular notation.
As a ﬁrst step in designing a processor, we present a functionally correct, but somewhat impractical, Y86 processor based on sequential operation.
This processor executes a complete Y86 instruction on every clock cycle.
The clock must run slowly enough to allow an entire series of actions to complete within one cycle.
Such a processor could be implemented, but its performance would be well below what could be achieved for this much hardware.
With the sequential design as a basis, we then apply a series of transformations to create a pipelined processor.
This processor breaks the execution of each instruction into ﬁve steps, each of which is handled by a separate section or stage.
Instructions progress through the stages of the pipeline, with one instruction entering the pipeline on each clock cycle.
As a result, the processor can be executing the different steps of up to ﬁve instructions simultaneously.
Making this processor preserve the sequential behavior of the Y86 ISA requires handling a variety of hazard conditions, where the location or operands of one instruction depend on those of other instructions that are still in the pipeline.
We have devised a variety of tools for studying and experimenting with our processor designs.
The control logic for these designs is described by ﬁles in HCL notation.
By editing these ﬁles and recompiling the simulator, you can alter and extend the simulator’s behavior.
A number of exercises are provided that involve implementing new instructions andmodifying how themachine processes instructions.
Testing code is provided to help you evaluate the correctness of your modiﬁcations.
These exercises will greatly aid your understanding of the material and will give you an appreciation for the many different design alternatives faced by processor designers.
WebAside arch:vlog presents a representation of our pipelined Y86 processor in the Verilog hardware description language.
This involves creating modules for the basic hardware building blocks and for the overall processor structure.
We automatically translate the HCL description of the control logic into Verilog.
Given a Verilog description, there are commercial and open-source tools to support simulation and logic synthesis, generating actual circuit designs for the microprocessors.
So, although much of the effort we expend here is to create pictorial and textual descriptions of a system, much as one would when writing software, the fact that these designs can be automatically synthesized demonstrates that we are indeed creating a system that can be realized as hardware.
Deﬁning an instruction set architecture, such as Y86, includes deﬁning the different state elements, the set of instructions and their encodings, a set of programming conventions, and the handling of exceptional events.
This is referred to as the programmer-visible state, where the “programmer” in this case is either someone writing programs in assembly code or a compiler generating machine-level code.
We will see in our processor implementations that we do not need to represent and organize this state in exactly the manner implied by the ISA, as long as we can make sure that machine-level programs appear to have access to the programmer-visible state.
The status code indicates whether the program is running normally, or some special event has occurred.
Register %esp is used as a stack pointer by the push, pop, call, and return instructions.
There are three single-bit condition codes, ZF, SF, and OF, storing information about the effect of the most recent arithmetic or logical instruction.
The program counter (PC) holds the address of the instruction currently being executed.
The memory is conceptually a large array of bytes, holding both program and data.
A combination of hardware and operating system software translates these into the actual, or physical, addresses indicating where the values are actually stored in memory.
For now, we can think of the virtual memory system as providing Y86 programs with an image of a monolithic byte array.
A ﬁnal part of the program state is a status code Stat, indicating the overall state of program execution.
It will indicate either normal operation, or that some sort of exception has occurred, such as when an instruction attempts to read from an invalid memory address.
The possible status codes and the handling of exceptions is described in Section 4.1.4
We use this instruction set as a target for our processor implementations.
It includes only 4-byte integer operations, has fewer addressing modes, and includes a smaller set of operations.
Since we only use 4-byte data, we can refer to these as “words” without any ambiguity.
In this ﬁgure, we show the assembly-code representation of the instructions on the left and the byte encodings on the right.
The assemblycode format is similar to the ATT format for IA32
Here are some further details about the different Y86 instructions.
The IA32 movl instruction is split into four different instructions: irmovl, rrmovl, mrmovl, and rmmovl, explicitly indicating the form of the source and destination.
The source is either immediate (i), register (r), or memory (m)
Field fn speciﬁes a particular integer operation (OPl), data movement condition (cmovXX), or branch condition (jXX)
It is designated by the ﬁrst character in the instruction name.
It is designated by the second character in the instruction name.
Explicitly identifying the four types of data transfer will prove helpful when we decide how to implement them.
Thememory references for the twomemorymovement instructions have a simple base and displacement format.
We do not support the second index register or any scaling of a register’s value in the address computation.
As with IA32, we do not allow direct transfers from onememory location to another.
In addition, we do not allow a transfer of immediate data to memory.
There are four integer operation instructions, shown in Figure 4.2 as OPl.
They operate only on register data, whereas IA32 also allows operations on memory data.
These instructions set the three condition codes ZF, SF, and OF (zero, sign, and overﬂow)
Branches are taken according to the type of branch and the settings of the condition codes.
These have the same format as the register-register move instruction rrmovl, but the destination register is updated only if the condition codes satisfy the required constraints.
The call instruction pushes the return address on the stack and jumps to the destination address.
The pushl and popl instructions implement push and pop, just as they do in IA32
IA32 application programs are not permitted to use this instruction, since it causes the entire system to suspend operation.
For Y86, executing the halt instruction causes the processor to stop, with the status code set to HLT.
Figure 4.2 also shows the byte-level encoding of the instructions.
Every instruction has an initial byte identifying the instruction type.
This byte is split into two 4-bit parts: the high-order, or code, part, and the low-order, or function, part.
The function values are signiﬁcant only for the cases where a group of related instructions share a common code.
These are given in Figure 4.3, showing the speciﬁc encodings of the integer operation, conditional move, and branch instructions.
Observe that rrmovl has the same instruction code as the conditional moves.
The program registers are stored within the CPU in a register ﬁle, a small random-access memory where the register IDs serve.
The code speciﬁes a particular integer operation, branch condition, or data transfer condition.
These instructions are shown as OPl, jXX, and cmovXX in Figure 4.2
Some instructions are just 1 byte long, but those that require operands have longer encodings.
First, there can be an additional register speciﬁer byte, specifying either one or two registers.
These register ﬁelds are called rA and rB in Figure 4.2
As the assembly-code versions of the instructions show, they can specify the registers used for data sources and destinations, as well as the base register used in an address computation, depending on the instruction type.
Instructions that have no register operands, such as branches and call, do not have a register speciﬁer byte.
Those that require just one register operand (irmovl, pushl, and popl) have the other register speciﬁer set to value 0xF.
This word can serve as the immediate data for irmovl, the displacement for rmmovl and mrmovl address speciﬁers, and the destination of branches and calls.
Note that branch and call destinations are given as absolute addresses, rather than using the PC-relative addressing seen in IA32
Processors use PC-relative addressing to give more compact encodings of branch instructions and to allow code to be copied from one part of memory to another without the need to update all of the branch target addresses.
Since we are more concerned with simplicity in our presentation, we use absolute addressing.
When the instruction iswritten in disassembled form, these bytes appear in reverse order.
As an example, let us generate the byte encoding of the instruction rmmovl %esp,0x12345(%edx) in hexadecimal.
We can also see that source register %esp should be encoded in the rA ﬁeld, and base register %edx should be encoded in the rB ﬁeld.
One important property of any instruction set is that the byte encodings must have a unique interpretation.
An arbitrary sequence of bytes either encodes a unique instruction sequence or is not a legal byte sequence.
This property holds for Y86, because every instruction has a unique combination of code and function in its initial byte, and given this byte, we can determine the length and meaning of any additional bytes.
This property ensures that a processor can execute an objectcode program without any ambiguity about the meaning of the code.
Even if the code is embedded within other bytes in the program, we can readily determine the instruction sequence as long as we start from the ﬁrst byte in the sequence.
On the other hand, if we do not know the starting position of a code sequence, we cannot reliably determine how to split the sequence into individual instructions.
This causes problems for disassemblers and other tools that attempt to extract machine-level programs directly from object-code byte sequences.
If there is some invalid byte in the sequence, show the instruction sequence up to that point and indicate where the invalid value occurs.
For each sequence, we show the starting address, then a colon, and then the byte sequence.
We use a 4-bit encoding of registers, even though there are only eight possible registers.
IA32 is sometimes labeled as a “complex instruction set computer” (CISC—pronounced “sisk”), and is deemed to be the opposite of ISAs that are classiﬁed as “reduced instruction set computers” (RISC—pronounced “risk”)
Historically, CISC machines came ﬁrst, having evolved from the earliest computers.
By the early 1980s, instruction sets formainframe andminicomputers had grownquite large, as machine designers incorporated new instructions to support high-level tasks, such as manipulating circular buffers, performing decimal arithmetic, and evaluating polynomials.
Even the x86 line continues to evolve as new classes of instructions are added based on the needs of emerging applications.
TheRISCdesign philosophy developed in the early 1980s as an alternative to these trends.A group of hardware and compiler experts at IBM, strongly inﬂuenced by the ideas of IBM researcher John Cocke, recognized that they could generate efﬁcient code for a much simpler form of instruction set.
In fact, many of the high-level instructions that were being added to instruction sets were very difﬁcult to generate with a compiler and were seldom used.
A simpler instruction set could be implemented with much less hardware and could be organized in an efﬁcient pipeline structure, similar to those described later in this chapter.
Patterson gave the name RISC to this new class of machines, and CISC to the existing class, since there had previously been no need to have a special designation for a nearly universal form of instruction set.
Comparing CISC with the original RISC instruction sets, we ﬁnd the following general characteristics:
These include instructions that copy an entire block from one part of memory to another and others that copy multiple registers to and from memory.
Some early RISC machines did not even have an integer multiply instruction, requiring compilers to implement multiplication as a sequence of additions.
In IA32, a memory operand speciﬁer can have many different combinations of displacement, base and index registers, and scale factors.
Arithmetic and logical operations can be applied to both memory and register operands.
Memory referencing is only allowed by load instructions, reading from memory into a register, and store instructions, writing from a register to memory.
The ISA provides a clean abstraction between programs and how they get executed.
Some RISC machines prohibit particular instruction sequences and have jumps that do not take effect until the following instruction is executed.
The compiler is given the task of optimizing performance within these constraints.
Special ﬂags are set as a side effect of instructions and then used for conditional branch testing.
Instead, explicit test instructions store the test results in normal registers for use in conditional evaluation.
The stack is used for procedure arguments and return addresses.
Typically, the processor has many more (up to 32) registers.
The Y86 instruction set includes attributes of both CISC and RISC instruction sets.
On the CISC side, it has condition codes, variable-length instructions, and stack-intensive procedure linkages.
On the RISC side, it uses a load-store architecture and a regular encoding.
It can be viewed as taking a CISC instruction set (IA32) and simplifying it by applying some of the principles of RISC.
Through the 1980s, battles raged in the computer architecture community regarding themerits of RISC versus CISC instruction sets.
Proponents of RISC claimed they could get more computing power for a given amount of hardware through a combination of streamlined instruction set design, advanced compiler technology, and pipelined processor implementation.
Computers Ltd., developed its own architecture, ARM (originally an acronym for “Acorn RISC Machine”), which is widely used in embedded applications, such as cellphones.
In the early 1990s, the debate diminished as it became clear that neither RISC nor CISC in their purest forms were better than designs that incorporated the best ideas of both.
RISCmachines evolved and introduced more instructions, many of which take multiple cycles to execute.
As new processor models were developed using more advanced hardware structures, many of these artifacts became irrelevant, but they still remained part of the instruction set.
Still, the core of RISC design is an instruction set that is well-suited to execution on a pipelined machine.
More recent CISC machines also take advantage of high-performance pipeline structures.
As we will discuss in Section 5.7, they fetch the CISC instructions and dynamically translate them into a sequence of simpler, RISC-like operations.
For example, an instruction that adds a register to memory is translated into three operations: one to read the original memory value, one to perform the addition, and a third to write the sum to memory.
Since the dynamic translation can generally be performed well in advance of the actual instruction execution, the processor can sustain a very high execution rate.
Marketing issues, apart from technological ones, have also played a major role in determining the success of different instruction sets.
Bymaintaining compatibility with its existing processors, Intel with x86 made it easy to keep moving from one generation of processor to the next.
In the areas of desktop and laptop computing, x86 has achieved total domination, and it is increasingly popular for high-end server machines.
In these applications, saving on cost and power is more important than maintaining backward compatibility.
In terms of the number of processors sold, this is a very large and growing market.
The possible values for this code are shown in Figure 4.5
Code value 1, named AOK, indicates that the program is executing normally, while the other codes indicate that some type of exception has occurred.
Code 2, named HLT, indicates that the processor has executed a halt instruction.
Code 3, named ADR, indicates that the processor attempted to read from or write to an invalid memory address, either while fetching an instruction or while reading or writing data.
We limit the maximum address (the exact limit varies by implementation), and any access to an address beyond this limit will trigger an ADR exception.
Code 4, named INS, indicates that an invalid instruction code has been encountered.
In our design, the processor halts for any code other than AOK.
ForY86, wewill simply have the processor stop executing instructions when it encounters any of the exceptions listed.
In a more complete design, the processor would typically invoke an exception handler, a procedure designated to handle the speciﬁc type of exception encountered.
As described in Chapter 8, exception handlers can be conﬁgured to have different effects, such as aborting the program or invoking a user-deﬁned signal handler.
This code follows many of the programming conventions we have seen for IA32, including the use of the stack and framepointers.
For simplicity, it does not follow the IA32 conventionof having some registers designated as callee-save registers.
This is just a programming convention that we can either adopt or ignore as we please.
Directives indicate where to place code or data and how to align it.
The Sum function computes the sum of an integer array.
In this program, words beginning with “.” are assembler directives telling the assembler to adjust the address at which it is generating code or to insert some words of data.
Our stack will therefore start at this address and grow toward lower addresses.
We must ensure that the stack does not grow so large that it overwrites the code or other program data.
The label array denotes the start of this array, and is aligned on a 4-byte boundary (using the .align directive)
The Sum function is called to compute the sum of a four-element array.
As this example shows, since our only tool for creating Y86 code is an assembler, the programmer must perform tasks we ordinarily delegate to the compiler, linker, and run-time system.
Fortunately, we only do this for small programs, for which simple mechanisms sufﬁce.
The assembler output is in ASCII format to make it more readable.
We have implemented an instruction set simulator we call yis, the purpose of which is to model the execution of a Y86 machine-code program, without attempting to model the behavior of any speciﬁc processor implementation.
This form of simulation is useful for debugging programs before actual hardware is available, and for checking the result of either simulating the hardware or running the program on the hardware itself.
Running on our sample object code, yis generates the following output:
The ﬁrst line of the simulation output summarizes the execution and the resulting values of the PC and program status.
In printing register and memory values, it only prints out words that change during simulation, either in registers or in memory.
The original values (here they are all zero) are shown on the left, and the ﬁnal values are shown on the right.
We can see in this output that register %eax contains 0xabcd, the sumof the four-element array passed to subroutine Sum.
This is well away from 0x7c, the maximum address of the executable code.
Most Y86 instructions transform the program state in a straightforward manner, and so deﬁning the intended effect of each instruction is not difﬁcult.
The pushl instruction both decrements the stack pointer by 4 and writes a register value tomemory.
It is therefore not totally clearwhat the processor should do when executing the instruction pushl %esp, since the register being pushed is being changed by the same instruction.
TheC compilerwould not normally generate this instruction, so we must use hand-generated assembly.
Here is a test function we have written (Web Aside asm:easm describes how to write programs that combine C code with hand-written assembly code):
In our experiments, we ﬁnd that function pushtest always returns zero.What does this imply about the behavior of the instruction pushl %esp under IA32?
It could either set %esp to the value read from memory or to the incremented stack pointer.
What does this imply about the behavior of popl %esp? What other Y86 instruction would have the exact same behavior?
There seems to be little reason why one would want to perform either of these operations, and so a natural question to ask is “Why worry about such picky details?”
Several useful lessons can be learned about the importance of consistency from the following excerpt from the Intel documentation of the pop instruction [29]:
What this note states is that different models of x86 processors do different things when instructed to push the stack pointer register.
Some push the original value, while others push the decremented value.
Interestingly, there is no corresponding ambiguity about popping to the stack pointer register.
Programs may have different behavior depending on the processor model.
Although the particular instruction is not at all common, even the potential for incompatibility can have serious consequences.
As we see here, a special note is required to try to clarify the differences.
The documentation for x86 is already complex enough without special cases such as this one.
We conclude, therefore, that working out details in advance and striving for complete consistency can save a lot of trouble in the long run.
In hardware design, electronic circuits are used to compute functions on bits and to store bits in different kinds of memory elements.
Most contemporary circuit technology represents different bit values as highor lowvoltages on signalwires.
Threemajor components are required to implement a digital system: combinational logic to compute functions on the bits, memory elements to store bits, and clock signals to regulate the updating of the memory elements.
In this section, we provide a brief description of these different components.
We also introduce HCL (for “hardware control language”), the language that we use to describe the control logic of the different processor designs.
A complete reference for HCL can be found in Web Aside arch:hcl.
At one time, hardware designers created circuit designs by drawing schematic diagrams of logic circuits (ﬁrst with paper and pencil, and later with computer graphics terminals)
Nowadays, most designs are expressed in a hardware description language (HDL), a textual notation that looks similar to a programming language but that is used to describe hardware structures rather than program behaviors.
The most commonly used languages are Verilog, having a syntax similar to C, and VHDL, having a syntax similar to the Ada programming language.
These languages were originally designed for expressing simulation models of digital circuits.
In the mid-1980s, researchers developed logic synthesis programs that could generate efﬁcient circuit designs fromHDL descriptions.
There are now a number of commercial synthesis programs, and this has become the dominant technique for generating digital circuits.
This shift from hand-designed circuits to synthesized ones can be likened to the shift from writing programs in assembly code to writing them in a high-level language and having a compiler generate the machine code.
OurHCL language expresses only the control portions of a hardware design, with only a limited set of operations andwith nomodularity.
As wewill see, however, the control logic is themost difﬁcult part of designing a microprocessor.
We have developed tools that can directly translate HCL into Verilog, and by combining this code with Verilog code for the basic hardware units, we can generate HDL descriptions from which actual working microprocessors can be synthesized.
By carefully separating out, designing, and testing the control logic, we can create a working microprocessor with reasonable effort.
Web Aside arch:vlog describes how we can generate Verilog versions of a Y86 processor.
Logic gates are the basic computing elements for digital circuits.
They generate an output equal to some Boolean function of the bit values at their inputs.
Figure 4.9 shows the standard symbols used for Boolean functions And, Or, and Not.
If some input to a gate changes, then within some small amount of time, the output will change accordingly.
Each gate generates output equal to some Boolean function of its inputs.
By assembling a number of logic gates into a network, we can construct computational blocks known as combinational circuits.
Two restrictions are placed on how the networks are constructed:
The outputs of two or more logic gates cannot be connected together.
Otherwise, the two could try to drive thewire in opposite directions, possibly causing an invalid voltage or a circuit malfunction.
That is, there cannot be a path through a series of gates that forms a loop in the network.
Such loops can cause ambiguity in the function computed by the network.
Figure 4.10 shows an example of a simple combinational circuit that we will ﬁnd useful.
We write the function of this network in HCL as.
This code simply deﬁnes the bit-level (denoted by data type bool) signal eq as a function of inputs a and b.
As this example shows, HCL uses C-style syntax, with ‘=’ associating a signal name with an expression.
Unlike C, however, we do not view this as performing a computation and assigning the result to some memory location.
Instead, it is simply a way to give a name to an expression.
Practice Problem 4.8 Write an HCL expression for a signal xor, equal to the Exclusive-Or of inputs a and b.
What is the relation between the signals xor and eq deﬁned above?
In this single-bit multiplexor, the two data signals are the input bits a and b, while the control signal is the input bit s.
In this circuit, we can see that the two And gates determine whether to pass their respective data inputs to the Or gate.
Again, we can write an HCL expression for the output signal, using the same operations as are present in the combinational circuit:
Our HCL expressions demonstrate a clear parallel between combinational logic circuits and logical expressions in C.
They both use Boolean operations to compute functions over their inputs.
Several differences between these two ways of expressing computation are worth noting:
Since a combinational circuit consists of a series of logic gates, it has the property that the outputs continually respond to changes in the inputs.
If some input to the circuit changes, then after some delay, the outputs will change accordingly.
In contrast, a C expression is only evaluated when it is encountered during the execution of a program.
Logical expressions inC allow arguments to be arbitrary integers, interpreting 0 as false and anything else as true.
Logical expressions in C have the property that they might only be partially evaluated.
If the outcome of an And or Or operation can be determined by just evaluating the ﬁrst argument, then the second argument will not be evaluated.
In contrast, combinational logic does not have any partial evaluation rules.
By assembling large networks of logic gates, we can construct combinational circuits that compute much more complex functions.
These are groups of bit-level signals that represent an integer or some control pattern.
The output will equal 1 when each bit from word A equals its counterpart from word B.
Combinational circuits to perform word-level computations are constructed using logic gates to compute the individual bits of the output word, based on the individual bits of the input words.
That is, the outputwill equal 1 if and only if each bit of A equals the corresponding bit of B.
The outputs of these single-bit circuits are combined with an And gate to form the circuit output.
In HCL, we will declare any word-level signal as an int, without specifying the word size.
In a full-featured hardware description language, everyword canbedeclared to have a speciﬁc number of bits.HCLallows words to be compared for equality, and so the functionality of the circuit shown in Figure 4.12 can be expressed at the word level as.
As is shown on the right side of Figure 4.12, we will draw word-level circuits using medium-thickness lines to represent the set of wires carrying the individual bits of the word, and we will show the resulting Boolean signal as a dashed line.
The output will equal input word A when the control signal s is 1, and it will equal B otherwise.
This circuit generates a 32-bit word Out equal to one of the two input words, A or B, depending on the control input bit s.
Rather than simply replicating the bit-level multiplexor 32 times, the word-level version reduces the number of inverters by generating !s once and reusing it at each bit position.
We will use many forms of multiplexors in our processor designs.
They allow us to select a word from a number of sources depending on some control condition.
The expression contains a series of cases, where each case i consists of a Boolean expression selecti, indicating when this case should be selected, and an integer expression expri, indicating the resulting value.
Unlike the switch statement of C, we do not require the different selection expressions to bemutually exclusive.
Logically, the selection expressions are evaluated in sequence, and the case for the ﬁrst one yielding 1 is selected.
For example, the word-level multiplexor of Figure 4.13 can be described in HCL as.
In this code, the second selection expression is simply 1, indicating that this case should be selected if no prior one has been.
This is the way to specify a default case in HCL.
An actual hardware multiplexor must have mutually exclusive signals controlling which input word should be passed to the output, such as the signals s and !s in Figure 4.13
To translate an HCL case expression into hardware, a logic synthesis program would need to analyze the set of selection expressions and resolve any possible conﬂicts by making sure that only the ﬁrst matching case would be selected.
The selection expressions can be arbitrary Boolean expressions, and there can be an arbitrary number of cases.
This allows case expressions to describe blocks where there are many choices of input signals with complex selection criteria.
For example, consider the diagram of a four-way multiplexor shown in Figure 4.14
We can express this in HCL using Boolean expressions to describe the different combinations of control bit patterns:
Observe that the selection expressions can sometimes be simpliﬁed, since only the ﬁrst matching case is selected.
As a ﬁnal example, suppose we want to design a logic circuit that ﬁnds the minimum value among a set of words A, B, and C, diagrammed as follows:
We can express this using an HCL case expression as.
Practice Problem 4.10 Write HCL code describing a circuit that for word inputs A, B, and C selects the median of the three values.
That is, the output equals the word lying between the minimum and maximum of the three inputs.
Combinational logic circuits can be designed to performmany different types of operations on word-level data.
The detailed design of these is beyond the scope of our presentation.
One important combinational circuit, known as an arithmetic/logic unit (ALU), is diagrammed at an abstract level in Figure 4.15
This circuit has three inputs: two data inputs labeled A and B, and a control input.
Depending on the setting of the control input, the circuit will perform different arithmetic or logical operations on the data inputs.
Depending on the setting of the function input, the circuit will perform one of four different arithmetic and logical operations.
Note also the ordering of operands for subtraction, where the A input is subtracted from the B input.
This ordering is chosen in anticipation of the ordering of arguments in the subl instruction.
In our processor designs, we will ﬁnd many examples where we want to compare one signal against a number of possible matching signals, such as to test whether the code for some instruction being processed matches some category of instruction codes.
In this circuit, the 2-bit signal code would then control the selection among the four data words A, B, C, andD.
Combinational circuits, by their very nature, donot store any information.
Instead, they simply react to the signals at their inputs, generating outputs equal to some function of the inputs.
To create sequential circuits, that is, systems that have state and perform computations on that state, we must introduce devices that store information represented as bits.
Our storage devices are all controlled by a single clock, a periodic signal that determines when new values are to be loaded into the devices.
Clocked registers (or simply registers) store individual bits or words.
The clock signal controls the loading of the register with the value at its input.
Random-access memories (or simply memories) store multiple words, using an address to select which word should be read or written.
As we can see, the word “register” means two slightly different things when speaking of hardware versus machine-language programming.
In hardware, a register is directly connected to the rest of the circuit by its input and output wires.
In machine-level programming, the registers represent a small collection of addressable words in the CPU, where the addresses consist of register IDs.
These words are generally stored in the register ﬁle, although we will see that the hardware can sometimes pass a word directly from one instruction to another to avoid the delay of ﬁrst writing and then reading the register ﬁle.
When necessary to avoid ambiguity, we will call the two classes of registers “hardware registers” and “program registers,” respectively.
Figure 4.16 gives a more detailed view of a hardware register and how it operates.
For most of the time, the register remains in a ﬁxed state (shown as x), generating an output equal to its current state.
Signals propagate through the combinational logic preceding the register, creating a new value for the register input (shown as y), but the register output remains ﬁxed as long as the clock is low.
As the clock rises, the input signals are loaded into the register as its next state (y), and this becomes the new register output until the next rising clock edge.
A key point is that the registers serve as barriers between the combinational logic in different parts of the circuit.
Values only propagate from a register input to its output once every clock cycle at the rising clock edge.
Input = y Output = x Output =  yRising clock.
The register outputs remain held at the current register state until the clock signal rises.
When the clock rises, the values at the register inputs are captured to become the new register state.
This register ﬁle has two read ports, named A and B, and one write port, named W.
Such a multiported random-access memory allows multiple read and write operations to takeplace simultaneously.
In the register ﬁle diagrammed, the circuit can read the values of two program registers and update the state of a third.
Each port has an address input, indicating which program register should be selected, and a data output or input giving a value for that program register.
The addresses are register identiﬁers, using the encoding shown in Figure 4.4
The two read ports have address inputs srcA and srcB (short for “source A” and “source B”) and data outputs valA and valB (short for “value A” and “value B”)
The write port has address input dstW (short for “destination W”) and data input valW (short for “value W”)
The register ﬁle is not a combinational circuit, since it has internal storage.
In our implementation, however, data can be read from the register ﬁle as if it were a block of combinational logic having addresses as inputs and the data as outputs.
When either srcA or srcB is set to some register ID, then, after some delay, the value stored in the corresponding program register will appear on either valA or valB.
For example, setting srcA to 3 will cause the value of program register %ebx to be read, and this value will appear on output valA.
The writing of words to the register ﬁle is controlled by the clock signal in a manner similar to the loading of values into a clocked register.
Every time the clock rises, the value on input valW is written to the program register indicated by.
When dstW is set to the special ID value 0xF, no program register is written.
Since the register ﬁle can be both read and written, a natural question to ask is “What happens if we attempt to read and write the same register simultaneously?” The answer is straightforward: if we update a register while using the same register ID on the read port, we would observe a transition from the old value to the new.
When we incorporate the register ﬁle into our processor design, we will make sure that we take this property into consideration.
Our processor has a random-access memory for storing program data, as illustrated below:
This memory has a single address input, a data input for writing, and a data output for reading.
Like the register ﬁle, reading from our memory operates in a manner similar to combinational logic: If we provide an address on the address input and set the write control signal to 0, then after some delay, the value stored at that address will appear on data out.
When we then operate the clock, the speciﬁed location in the memory will be updated, as long as the address is valid.Aswith the read operation, the error signal will be set to 1 if the address is invalid.
This signal is generated by combinational logic, since the required bounds checking is purely a function of the address input and does not involve saving any state.
The memory system in a full-scale microprocessor is far more complex than the simple one we assume in our design.
It consists of several forms of hardware memories, including several random-access memories plus magnetic disk, as well as a variety of hardware and software mechanisms for managing these devices.
Nonetheless, our simple memory design can be used for smaller systems, and it provides us with an abstraction of the interface between the processor and memory for more complex systems.
Our processor includes an additional read-only memory for reading instructions.
In most actual systems, these memories are merged into a single memory with two ports: one for reading instructions and the other for reading or writing data.
Now we have the components required to implement a Y86 processor.
As a ﬁrst step, we describe a processor called SEQ (for “sequential” processor)
On each clock cycle, SEQperforms all the steps required to process a complete instruction.
This would require a very long cycle time, however, and so the clock rate would be unacceptably low.
Our purpose in developing SEQ is to provide a ﬁrst step toward our ultimate goal of implementing an efﬁcient, pipelined processor.
The detailed processing at each step depends on the particular instruction being executed.
Creating this framework will allow us to design a processor that makes best use of the hardware.
The following is an informal description of the stages and the operations performed within them:
Fetch: The fetch stage reads the bytes of an instruction from memory, using the program counter (PC) as the memory address.
From the instruction it extracts the two 4-bit portions of the instruction speciﬁer byte, referred to as icode (the instruction code) and ifun (the instruction function)
It possibly fetches a register speciﬁer byte, giving one or both of the register operand speciﬁers rA and rB.
It computes valP to be the address of the instruction following the current one in sequential order.
That is, valP equals the value of the PC plus the length of the fetched instruction.
Decode: The decode stage reads up to two operands from the register ﬁle, giving values valA and/or valB.
Typically, it reads the registers designated by instruction ﬁelds rA and rB, but for some instructions it reads register %esp.
Execute: In the execute stage, the arithmetic/logic unit (ALU) either performs the operation speciﬁed by the instruction (according to the value of ifun), computes the effective address of a memory reference, or increments or decrements the stack pointer.
For a jump instruction, the stage tests the condition codes and branch condition (given by ifun) to see whether or not the branch should be taken.
Memory: The memory stage may write data to memory, or it may read data from memory.
Write back: The write-back stage writes up to two results to the register ﬁle.
In our simpliﬁed implementation, the processor will stop when any exception occurs: it executes a.
We will trace the processing of these instructions through the different stages.
In a more complete design, the processor would enter an exception-handling mode and begin executing special code determined by the type of exception.
As can be seen by the preceding description, there is a surprising amount of processing required to execute a single instruction.
Not only must we perform the stated operation of the instruction, we must also compute addresses, update stack pointers, and determine the next instruction address.
Fortunately, the overall ﬂow can be similar for every instruction.
Using a very simple and uniform structure is important when designing hardware, since we want to minimize the total amount of hardware, andwemust ultimatelymap it onto the two-dimensional surface of an integrated-circuit chip.Oneway tominimize the complexity is tohave thedifferent instructions share as much of the hardware as possible.
For example, each of our processor designs contains a single arithmetic/logic unit that is used in different ways depending on the type of instruction being executed.
The cost of duplicating blocks of logic in hardware is much higher than the cost of having multiple copies of code in software.
It is also more difﬁcult to deal with many special cases and idiosyncrasies in a hardware system than with software.
Figure 4.18 shows the processing required for instruction types OPl (integer and logical operations), rrmovl (register-register move), and irmovl (immediateregister move)
Examining Figure 4.2, we can see that we have carefully chosen an encoding of instructions so that the four integer operations (addl, subl, andl, and xorl) all have the same value of icode.
We can handle them all by an identical sequence of steps, except that the ALU computation must be set according to the particular instruction operation, encoded in ifun.
The stages would proceed as shown in the following table, which lists the generic rule for processing an OPl instruction (Figure 4.18) on the left, and the computations for this speciﬁc instruction on the right.
Executing an rrmovl instruction proceeds much like an arithmetic operation.
We do not need to fetch the second register operand, however.
Instead, we set the second ALU input to zero and add this to the ﬁrst, giving valE = valA, which is then written to the register ﬁle.
Similar processing occurs for irmovl, except that we use constant value valC for the ﬁrstALU input.
In addition, wemust increment the program counter by 6 for irmovl due to the long instruction format.
Figure 4.19 shows the processing required for the memory write and read instructions rmmovl and mrmovl.
In the memory stage we either write the register value valA to memory, or we read valM from memory.
Figure 4.20 shows the steps required to process pushl and popl instructions.
These are among themost difﬁcultY86 instructions to implement, because they involve both accessingmemory and incrementing or decrementing the stack pointer.
Although the two instructions have similar ﬂows, they have important differences.
The pushl instruction starts much like our previous instructions, but in the decode stage we use %esp as the identiﬁer for the second register operand, giving the stack pointer as value valB.
This decremented value is used for the memory write address and is also stored back to %esp in the write-back stage.
The popl instruction proceeds much like pushl, except that we read two copies of the stack pointer in the decode stage.
This is clearly redundant, but we will see that having the stack pointer as both valA and valB makes the subsequent ﬂow more similar to that of other instructions, enhancing the overall uniformity of the design.
We use the ALU to increment the stack pointer by 4 in the execute stage, but use the unincremented value as the address for the memory operation.
In the write-back stage, we update both the stack pointer register with the incremented stack pointer, and register rA with the value read from memory.
What effect does this instruction execution have on the registers and the PC?
Figure 4.21 indicates the processing of our three control transfer instructions: the different jumps, call, and ret.
We see that we can implement these instructions with the same overall ﬂow as the preceding ones.
As with integer operations, we can process all of the jumps in a uniform manner, since they differ only when determining whether or not to take the branch.
A jump instruction proceeds through fetch and decode much like the previous instructions, except that it does not require a register speciﬁer byte.
In the execute stage, we check the condition codes and the jump condition to determine whether or not to take the branch, yielding a 1-bit signal Cnd.
Our notation x ? a : b is similar to the conditional expression in C—it yields a when x is nonzero and b when x is zero.
The condition codes were all set to zero by the subl instruction (line 3), and so the branch will not be taken.
Show how you would modify the steps for the rrmovl instruction below to also handle the six conditional move instructions.
You may ﬁnd it useful to see how the implementation of the jXX instructions (Figure 4.21) handles conditional behavior.
Instructions call and retbear some similarity to instructions pushl andpopl, except that we push and pop program counter values.
With instruction call, we push valP, the address of the instruction that follows the call instruction.
During the PC update stage, we set the PC to valC, the call destination.
With instruction ret, we assign valM, the value popped from the stack, to the PC in the PC update stage.
What effect would this instruction execution have on the registers, the PC, and the memory?
Wehave created a uniform framework that handles all of the different types of Y86 instructions.
Even though the instructions have widely varying behavior, we can organize the processing into six stages.
Our task now is to create a hardware design that implements the stages and connects them together.
The computations required to implement all of the Y86 instructions can be organized as a series of six basic stages: fetch, decode, execute, memory, write back, andPCupdate.
Figure 4.22 shows an abstract viewof a hardware structure that can perform these computations.
The program counter is stored in a register, shown in the lower left-hand corner (labeled “PC”)
Information then ﬂows along wires (shown grouped together as a heavy black line), ﬁrst upward and then around to the right.
Processing is performed by hardware units associated with the different stages.
The feedback paths coming back down on the right-hand side contain the updated values to write to the register ﬁle and the updated program counter.
In SEQ, all of the processing by the hardware units occurs within a single clock cycle, as is discussed in Section 4.3.3
This diagram omits some small blocks of combinational logic as well as all of the control logic needed to operate the different hardware units and to route the appropriate values to the units.
Our method of drawing processors with the ﬂow going from bottom to top is unconventional.
We will explain the reason for our convention when we start designing pipelined processors.
The hardware units are associated with the different processing stages:
Fetch: Using the program counter register as an address, the instruction memory reads the bytes of an instruction.
The information processed during execution of an instruction follows a clockwise ﬂow starting with an instruction fetch using the program counter (PC), shown in the lower left-hand corner of the ﬁgure.
Decode: The register ﬁle has two read ports, A and B, via which register values valA and valB are read simultaneously.
Execute: The execute stage uses the arithmetic/logic (ALU) unit for different purposes according to the instruction type.
For other instructions, it serves as an adder to compute an incremented or decremented stack pointer, to compute an effective address, or simply to pass one of its inputs to its outputs by adding zero.
The condition code register (CC) holds the three condition-code bits.
New values for the condition codes are computed by the ALU.
When executing a jump instruction, the branch signal Cnd is computed based on the condition codes and the jump type.
Memory: The data memory reads or writes a word of memory when executing a memory instruction.
The instruction and data memories access the same memory locations, but for different purposes.
Port E is used to write values computed by the ALU, while port M is used to write values read from the data memory.
Figure 4.23 gives a more detailed view of the hardware required to implement SEQ(althoughwewill not see the complete details untilwe examine the individual stages)
We see the same set of hardware units as earlier, but now the wires are shown explicitly.
In this ﬁgure, as well as in our other hardware diagrams, we use the following drawing conventions:
Control logic blocks are drawn as gray rounded rectangles.These blocks serve to select fromamong a set of signal sources, or to compute someBoolean function.
We will examine these blocks in complete detail, including developing HCL descriptions.
Wire names are indicated in white round boxes.These are simply labels on the wires, not any kind of hardware element.
Each of these lines actually represents a bundle of 32 wires, connected in parallel, for transferring a word from one part of the hardware to another.
Byte and narrower data connections are shown as thin lines.Each of these lines actually represents a bundle of four or eight wires, depending on what type of values must be carried on the wires.
Single-bit connections are shown as dotted lines.These represent control values passed between the units and blocks on the chip.
Some of the control signals, as well as the register and control word connections, are not shown.
These computations and actions are listed in the second column of Figure 4.24
In addition to the signals we have already described, this list includes four register ID signals: srcA, the source of valA; srcB, the source of valB; dstE, the register to which valE gets written; and dstM, the register to which valM gets written.
The two right-hand columns of this ﬁgure show the computations for the OPl and mrmovl instructions to illustrate the values being computed.
To map the computations into hardware, we want to implement control logic that will transfer thedatabetween thedifferent hardwareunits andoperate theseunits in such away that the speciﬁed operations are performed for each of the different instruction types.
That is the purpose of the control logic blocks, shown as gray rounded boxes in Figure 4.23
Our task is to proceed through the individual stages and create detailed designs for these blocks.
On the other hand, the hardware structure of Figure 4.23 operates in a fundamentally different way, with a single clock transition triggering a ﬂow through combinational logic to execute an entire.
Let us see how the hardware can implement the behavior listed in these tables.
Our implementation of SEQ consists of combinational logic and two forms of memory devices: clocked registers (the program counter and condition code register) and random-access memories (the register ﬁle, the instruction memory, and the data memory)
Combinational logic does not require any sequencing or control—values propagate through a network of logic gates whenever the inputs change.
As we have described, we also assume that reading from a randomaccess memory operates much like combinational logic, with the output word generated based on the address input.
This is a reasonable assumption for smaller memories (such as the register ﬁle), and we canmimic this effect for larger circuits using special clock circuits.
Since our instruction memory is only used to read instructions, we can therefore treat this unit as if it were combinational logic.
We are left with just four hardware units that require an explicit control over their sequencing—the program counter, the condition code register, the data memory, and the register ﬁle.
These are controlled via a single clock signal that triggers the loading of new values into the registers and the writing of values to the random-access memories.
The program counter is loaded with a new instruction address every clock cycle.
The condition code register is loaded only when an integer operation instruction is executed.
The data memory is written only when an rmmovl, pushl, or call instruction is executed.
The two write ports of the register ﬁle allow two program registers to be updated on every cycle, but we can use the special register ID 0xF as a port address to indicate that no write should be performed for this port.
This clocking of the registers and memories is all that is required to control the sequencing of activities in our processor.
This equivalence holds because of the nature of the Y86 instruction set, and because we have organized the computations in such a way that our design obeys the following principle:
The processor never needs to read back the state updated by an instruction in order to complete the processing of this instruction.
This principle is crucial to the success of our implementation.
As an illustration, suppose we implemented the pushl instruction by ﬁrst decrementing %esp by 4 and then using the updated value of %esp as the address of a write operation.
It would require reading the updated stack pointer from the register ﬁle in order to perform the memory operation.
Instead, our implementation (Figure 4.20) generates the decremented value of the stack pointer as the signal valE and then uses this signal both as the data for the register write and the address for the memory write.
As a result, it can perform the register and memory writes simultaneously as the clock rises to begin the next clock cycle.
As another illustration of this principle, we can see that some instructions (the integer operations) set the condition codes, and some instructions (the jump instructions) read these condition codes, but no instructionmust both set and then read the condition codes.
Even though the condition codes are not set until the clock rises to begin the next clock cycle, theywill be updated before any instruction attempts to read them.
We show the combinational logic as being wrapped around the condition code register, because some of the combinational logic (such as the ALU) generates the input to the condition code register, while other parts (such as the branch computation and the PC selection logic) have the condition code register as input.
We show the register ﬁle and the data memory as having separate connections for reading and writing, since the read operations propagate through these units as if they were combinational logic, while the write operations are controlled by the clock.
The color coding in Figure 4.25 indicates how the circuit signals relate to the different instructions being executed.
The combinational logic is shown in white, indicating that it has not yet had time to react to the changed state.
The clock cycle begins with address 0x00c loaded into the program counter.
This causes the addl instruction (line 3 of the listing), shown in blue, to be fetched and processed.
Values ﬂow through the combinational logic, including the reading of the random-access memories.
At this point, the combinational logic has been updated according to the addl instruction (shown in blue), but the state still holds the values set by the second irmovl instruction (shown in light gray)
In this cycle, theje instruction (line 4 in the listing), shown in dark gray, is fetched and executed.
Since condition code ZF is 0, the branch is not.
Each cycle begins with the state elements (program counter, condition code register, register ﬁle, and data memory) set according to the previous instruction.
Signals propagate through the combinational logic creating new values for the state elements.
These values are loaded into the state elements to start the next cycle.
The combinational logic has been updated according to the je instruction (shown in dark gray), but the state still holds the values set by the addl instruction (shown in blue) until the next cycle begins.
As this example illustrates, the use of a clock to control the updating of the state elements, combined with the propagation of values through combinational logic, sufﬁces to control the computations performed for each instruction in our implementation of SEQ.
Every time the clock transitions from low to high, the processor begins executing a new instruction.
In this section, we devise HCL descriptions for the control logic blocks required to implement SEQ.
We show some example blocks here, while others are given as practice problems.We recommend that youwork these practice problems as away to check your understanding of how the blocks relate to the computational requirements of the different instructions.
Part of theHCL description of SEQ that we do not include here is a deﬁnition of the different integer and Boolean signals that can be used as arguments to the HCL operations.
These include the names of the different hardware signals, as well as constant values for the different instruction codes, function codes, register names, ALU operations, and status codes.
Only those that must be explicitly referenced in the control logic are shown.
The halt instruction causes the processor status to be set to HLT, causing it to halt operation.
As shown in Figure 4.27, the fetch stage includes the instructionmemory hardware unit.
This byte is interpreted as the instruction byte and is split (by the unit labeled “Split”) into two 4-bit quantities.
The control logic blocks labeled “icode” and “ifun” then compute the instruction and function codes as equaling either the values read from memory or, in the event that the instruction address is not valid (as indicated by the signal imem_error), the values corresponding to a nop instruction.
Based on the value of icode, we can compute three 1-bit signals (shown as dashed lines):
These values represent the encodings of the instructions, function codes, register IDs, ALU operations, and status codes.
As an example, the HCL description for need_regids simply determines whether the value of icode is one of the instructions that has a register speciﬁer byte:
Six bytes are read from the instruction memory using the PC as the starting address.
These bytes are processed by the hardware unit labeled “Align” into the register ﬁelds and the constant word.
Otherwise, these two ﬁelds are set to 0xF (RNONE), indicating there are no registers speciﬁed by this instruction.
Thus, we can assume that the signals rA and rB either encode registers we want to access or indicate that register access is not required.
The unit labeled “Align” also generates the constant word valC.
Figure 4.28 provides a detailed view of logic that implements both the decode and write-back stages in SEQ.
These two stages are combined because they both access the register ﬁle.
It supports up to two simultaneous reads (on ports A and B) and two simultaneous writes (on ports E and M)
Each port has both an address connection and a data connection, where the address connection is a register ID, and the data connection is a set of 32 wires serving as either an output word (for a read port) or an input word (for a write port) of the register ﬁle.
The two read ports have address inputs srcA and srcB, while the two write.
The instruction ﬁelds are decoded to generate register identiﬁers for four addresses (two read and two write) used by the register ﬁle.
The values read from the register ﬁle become the signals valA and valB.
The two write-back values valE and valM serve as the data for the writes.
The special identiﬁer 0xF (RNONE) on an address port indicates that no register should be accessed.
The four blocks at the bottom of Figure 4.28 generate the four different register IDs for the register ﬁle, based on the instruction code icode, the register speciﬁers rA and rB, and possibly the condition signalCnd computed in the execute stage.
Register ID srcA indicates which register should be read to generate valA.
Combining all of these entries into a single computation gives the following HCL description of srcA (recall that RESP is the register ID of %esp):
Practice Problem 4.18 The register signal srcB indicates which register should be read to generate the signal valB.
Register ID dstE indicates the destination register for write port E, where the computed value valE is stored.
If we ignore for the moment the conditional move instructions, then we can combine the destination registers for all of the different instructions to give the following HCL description of dstE:
We will revisit this signal and how to implement conditional moves when we examine the execute stage.
Practice Problem 4.19 Register ID dstM indicates the destination register for write port M, where valM, the value read from memory, is stored.
Practice Problem 4.20 Only the popl instruction uses both register ﬁle write ports simultaneously.
For the instruction popl %esp, the same address will be used for both the E and M write ports, but with different data.
To handle this conﬂict, we must establish a priority among the two write ports so that when both attempt to write the same register on the same cycle, only the write from the higher-priority port takes place.
Which of the two ports should be given priority in order to implement the desired behavior, as determined in Problem 4.7?
This unit performs the operation add, subtract, and, or Exclusive-Or on inputs aluA and aluB based on the setting of the alufun signal.
These data and control signals are generated by three control blocks, as diagrammed in Figure 4.29
The ALU either performs the operation for an integer operation instruction or it acts as an adder.
The condition code registers are set according to the ALU value.
The condition code values are tested to determine whether or not a branch should be taken.
Looking at the operations performed by the ALU in the execute stage, we can see that it is mostly used as an adder.
For the OPl instructions, however, we want it to use the operation encoded in the ifun ﬁeld of the instruction.
We can therefore write the HCL description for the ALU control as follows:
Our ALU generates the three signals on which the condition codes are based—zero, sign, and overﬂow—every time it operates.
However, we only want to set the condition codes when an OPl instruction is executed.
We therefore generate a signal set_cc that controls whether or not the condition code register should be updated:
The hardware unit labeled “cond” uses a combination of the condition codes and the function code to determine whether a conditional branch or data transfer should take place (Figure 4.3)
It generates theCnd signal used both for the setting of dstE with conditional moves, and in the next PC logic for conditional branches.
The data memory can either write or read memory values.
Practice Problem 4.22 The conditional move instructions, abbreviated cmovXX, have instruction code IRRMOVL.
As Figure 4.28 shows, we can implement these instructions by making use of the Cnd signal, generated in the execute stage.
Modify the HCL code for dstE to implement these instructions.
The memory stage has the task of either reading or writing program data.
As shown in Figure 4.30, two control blocks generate the values for the memory address and the memory input data (for write operations)
Two other blocks generate the control signals indicating whether to perform a read or a write operation.
When a read operation is performed, the data memory generates the value valM.
Observe that the address for memory reads and writes is always valE or valA.
We want to set the control signal mem_read only for instructions that read data from memory, as expressed by the following HCL code:
The ﬁnal stage in SEQ generates the new value of the program counter.
The next value of the PC is selected from among the signals valC, valM, and valP, depending on the instruction code and the branch ﬂag.
We have now stepped through a complete design for a Y86 processor.
We have seen that by organizing the steps required to execute each of the different instructions into a uniform ﬂow, we can implement the entire processor with a small number of different hardware units and with a single clock to control the sequencing of computations.
The control logic must then route the signals between these units and generate the proper control signals based on the instruction types and the branch conditions.
The only problem with SEQ is that it is too slow.
The clock must run slowly enough so that signals can propagate through all of the stages within a single cycle.
As an example, consider the processing of a ret instruction.
Starting with an updated program counter at the beginning of the clock cycle, the instruction must be read from the instruction memory, the stack pointer must be read from the register ﬁle, the ALU must decrement the stack pointer, and the return address must be read from the memory in order to determine the next value for the program counter.
All of this must be completed by the end of the clock cycle.
This style of implementation does not make very good use of our hardware units, since each unit is only active for a fraction of the total clock cycle.
We will see that we can achieve much better performance by introducing pipelining.
Before attempting to design a pipelined Y86 processor, let us consider some general properties and principles of pipelined systems.
Such systems are familiar to anyonewho has been through the serving line at a cafeteria or run a car through an automated car wash.
In a pipelined system, the task to be performed is divided into a series of discrete stages.
In a cafeteria, this involves supplying salad, a main dish, dessert, and beverage.
In a car wash, this involves spraying water and soap, scrubbing, applying wax, and drying.
Rather than having one customer run through the entire sequence from beginning to end before the next can begin, we allow multiple customers to proceed through the system at once.
In a typical cafeteria line, the customers maintain the same order in the pipeline and pass through all stages, even if they do not want some of the courses.
In the case of the car wash, a new car is allowed to enter the spraying stage as the preceding car moves from the spraying stage to the scrubbing stage.
In general, the cars must move through the system at the same rate to avoid having one car crash into the next.
A key feature of pipelining is that it increases the throughput of the system, that is, the number of customers served per unit time, but it may also slightly increase the latency, that is, the time required to service an individual customer.
For example, a customer in a cafeteria who only wants a salad could pass through a nonpipelined system very quickly, stopping only at the salad stage.
A customer in a pipelined system who attempts to go directly to the salad stage risks incurring the wrath of other customers.
Shiftingour focus to computational pipelines, the “customers” are instructions and the stages perform some portion of the instruction execution.
Figure 4.32 shows an example of a simple nonpipelined hardware system.
It consists of some logic that performs a computation, followed by a register to hold the results of this computation.
A clock signal controls the loading of the register at some regular time interval.
An example of such a system is the decoder in a compact disk (CD) player.
The incoming signals are the bits read from the surface of the CD, and the logic decodes these to generate audio signals.
The computational block in the ﬁgure is implemented as combinational logic, meaning that the signals will pass through a series of logic gates, with the outputs becoming some function of the inputs after some time delay.
We express throughput in units of giga-instructions per second (abbreviated GIPS), or billions of instructions per second.
The total time required to perform a single instruction from beginning to end is known as the latency.
In this system, the latency is 320 ps, the reciprocal of the throughput.
The computation is split into stages A, B, and C.
On each 120-ps cycle, each instruction progresses through one stage.
To better understand how pipelining works, let us look in some detail at the timing and operation of pipeline computations.
The transfer of the instructions between pipeline stages is controlled by a clock signal, as shown above the pipeline diagram.
The rising edge of the clock signal controls the movement of instructions from one pipeline stage to the next.
The values computed in stageB for instruction I1 have reached the input of the second pipeline register.
As the clock rises, these inputs are loaded into the pipeline registers, becoming the register outputs (point 2)
In addition, the input to stage A is set to initiate the computation of instruction I3
The signals then propagate through the combinational logic for the different stages (point 3)
As the curved wavefronts in the diagram at point 3 suggest, signals can propagate through different sections at different rates.
When the clock rises at time 360, each of the instructions will have progressed through one pipeline stage.
We can see from this detailed view of pipeline operation that slowing down the clock would not change the pipeline behavior.
The signals propagate to the pipeline register inputs, but no change in the register states will occur until the clock rises.
On the other hand, we could have disastrous effects if the clock were run too fast.
The values would not have time to propagate through the combinational logic, and so the register inputs would not yet be valid when the clock rises.
As with our discussion of the timing for the SEQ processor (Section 4.3.3), we see that the simple mechanism of having clocked registers between blocks of combinational logic sufﬁces to control the ﬂow of instructions in the pipeline.
As the clock rises and falls repeatedly, the different instructions ﬂow through the stages of the pipeline without interfering with one another.
The example of Figure 4.33 shows an ideal pipelined system in which we are able to divide the computation into three independent stages, each requiring one-third of the time required by the original logic.
Unfortunately, other factors often arise that diminish the effectiveness of pipelining.
Just before the clock rises again, the results for the instructions have propagated to the inputs of the pipeline registers (point 4)
The sum of the delays through all of the stages remains 300 ps.
Figure 4.36 Limitations of pipelining due to nonuniform stage delays.
The system throughput is limited by the speed of the slowest stage.
In addition, the latency would increase to 510 ps due to the slower clock rate.
Devising a partitioning of the system computation into a series of stages having uniform delays can be a major challenge for hardware designers.
Often, some of the hardware units in a processor, such as the ALU and the memories, cannot be subdivided into multiple units with shorter delay.
This makes it difﬁcult to create a set of balanced stages.
We will not concern ourselves with this level of detail in designing our pipelined Y86 processor, but it is important to appreciate the importance of timing optimization in actual system design.
We can create pipelined versions of this design by inserting pipeline registers between pairs of these blocks.
Different combinations of pipeline depth (how many stages) and maximum throughput arise, depending on where we insert the pipeline registers.
Assume that a pipeline register has a delay of 20 ps.
Where should two registers be inserted to maximize the throughput of a three-stage pipeline? What would be the throughput and latency?
Where should three registers be inserted to maximize the throughput of a four-stage pipeline? What would be the throughput and latency?
What is the minimum number of stages that would yield a design with the maximum achievable throughput? Describe this design, its throughput, and its latency.
In this example, we have divided the computation into six stages, each requiring 50 ps.
Inserting a pipeline register between each pair of stages yields a six-stage pipeline.
Even though we have cut the time required for each computation block by a factor of 2, we do not get a doubling of the throughput, due to thedelay through thepipeline registers.
This delay becomes a limiting factor in the throughput of the pipeline.
In our new design, this delay consumes 28.6% of the total clock period.
Modern processors employ very deep (15 or more stages) pipelines in an attempt to maximize the processor clock rate.
The processor architects divide the instruction execution into a large number of very simple steps so that each stage can have a very small delay.
The circuit designers carefully design the pipeline registers to minimize their delay.
As the combinational logic is split into shorter blocks, the delay due to register updating becomes a limiting factor.
All of these factors contribute to the challenge of designing high-speed microprocessors.
What would be the latency and the throughput of the system, as functions of k?
Up to this point, we have considered only systems in which the objects passing through the pipeline—whether cars, people, or instructions—are completely independent of one another.
Another source of sequential dependencies occurs due to the instruction control ﬂow.
In going from an unpipelined system with feedback (a) to a pipelined one (c), we change its computational behavior, as can be seen by the two pipeline diagrams (b and d)
In our design for SEQ, these dependencies were handled by the feedback paths shown on the righthand side of Figure 4.22
This feedback brings the updated register values down to the register ﬁle and the new PC value down to the PC register.
Figure 4.38 illustrates the perils of introducing pipelining into a system containing feedback paths.
In the original system (Figure 4.38(a)), the result of each instruction is fed back around to the next instruction.
If we attempt to convert this to a three-stage pipeline in the most straightforward manner (Figure 4.38(c)), we change the behavior of the system.
In attempting to speed up the system via pipelining, we have changed the system behavior.
When we introduce pipelining into a Y86 processor, we must deal with feedback effects properly.
Clearly, it would be unacceptable to alter the system behavior as occurred in the example of Figure 4.38
Somehow we must deal with the data and control dependencies between instructions so that the resulting behavior matches the model deﬁned by the ISA.
We are ﬁnally ready for the major task of this chapter—designing a pipelined Y86 processor.We start bymaking a small adaptation of the sequential processor SEQ to shift the computation of the PC into the fetch stage.
Our ﬁrst attempt at this does not handle the different data and control dependencies properly.
By making some modiﬁcations, however, we achieve our goal of an efﬁcient pipelined processor that implements the Y86 ISA.
As a transitional step toward a pipelined design, we must slightly rearrange the order of the ﬁve stages in SEQ so that the PC update stage comes at the beginning of the clock cycle, rather than at the end.
This transformation requires only minimal change to the overall hardware structure, and it will work better with the sequencing of activities within the pipeline stages.
We can move the PC update stage so that its logic is active at the beginning of the clock cycle by making it compute the PC value for the current instruction.
Figure 4.39 shows how SEQ and SEQ+ differ in their PC computation.
With SEQ (Figure 4.39(a)), the PC computation takes place at the end of the clock cycle, computing the new value for the PC register based on the values of signals.
With SEQ+, we compute the value of the program counter for the current state as the ﬁrst step in instruction execution.
With SEQ+ (Figure 4.39(b)), we create state registers to hold the signals computed during an instruction.
Then, as a new clock cycle begins, the values propagate through the exact same logic to compute the PC for the now-current instruction.
We label the registers “pIcode,” “pCnd,” and so on, to indicate that on any given cycle, they hold the control signals generated during the previous cycle.
Figure 4.40 shows a more detailed view of the SEQ+ hardware.
We can see that it contains the exact same hardware units and control blocks that we had in SEQ (Figure 4.23), but with the PC logic shifted from the top, where it was active at the end of the clock cycle, to the bottom, where it is active at the beginning.
One curious feature of SEQ+ is that there is no hardware register storing the program counter.
Instead, the PC is computed dynamically based on some state information stored from the previous instruction.
This is a small illustration of the fact that we can implement a processor in a way that differs from the conceptual model implied by the ISA, as long as the processor correctly executes arbitrary machinelanguage programs.Weneed not encode the state in the form indicated by the programmer-visible state, as long as the processor can generate correct values for any part of the programmer-visible state (such as the program counter)
We will exploit this principle even more in creating a pipelined design.
Outof-order processing techniques, as described in Section 5.7, take this idea to an extreme by executing instructions in a completely different order than they occur in the machine-level program.
The shift of state elements from SEQ to SEQ+ is an example of a general transformation known as circuit retiming [65]
Retiming changes the state representation for a system without changing its logical behavior.
It is often used to balance the delays between different stages of a system.
In our ﬁrst attempt at creating a pipelined Y86 processor, we insert pipeline registers between the stages of SEQ+ and rearrange signals somewhat, yielding the PIPE– processor, where the “–” in the name signiﬁes that this processor has somewhat less performance than our ultimate processor design.
The pipeline registers are shown in this ﬁgure as black boxes, each containing different ﬁelds that are shown as white boxes.
As indicated by the multiple ﬁelds, each pipeline register holds multiple bytes and words.
Observe that PIPE– uses nearly the same set of hardware units as our sequential design SEQ (Figure 4.40), but with the pipeline registers separating the stages.
The differences between the signals in the two systems is discussed in Section 4.5.3
Shifting the PC computation from the end of the clock cycle to the beginning makes it more suitable for pipelining.
Figure 4.41 Hardware structure of PIPE–, an initial pipelined implementation.
By inserting pipeline registers into SEQ+ (Figure 4.40), we create a ﬁve-stage pipeline.
There are several shortcomings of this version that we will deal with shortly.
It holds information about themost recently fetched instruction for processing by the decode stage.
It holds information about the most recently decoded instruction and the values read from the register ﬁle for processing by the execute stage.
It holds the results of the most recently executed instruction for processing by the memory stage.
It also holds information about branch conditions and branch targets for processing conditional jumps.
The right side of the ﬁgure shows a pipeline diagram for this instruction sequence.
As with the pipeline diagrams for the simple pipelined computation units of Section 4.4, this diagram shows theprogressionof each instruction through the pipeline stages, with time increasing from left to right.
The numbers along the top identify the clock cycles at which the different stages occur.
At this point, there is an instruction in each of the pipeline stages.
From Figure 4.42, we can also justify our convention of drawing processors so that the instructions ﬂow from bottom to top.
If we look at the ordering of instructions in the pipeline stages, we see that they appear in the same order as they do in the program listing.
Since normal program ﬂow goes from top to bottom of a listing, we preserve this ordering by having the pipeline ﬂow go from bottom to top.
This convention is particularly useful when working with the simulators that accompany this text.
Our sequential implementations SEQ and SEQ+ only process one instruction at a time, and so there are unique values for signals such as valC, srcA, and valE.
In our pipelined design, there will be multiple versions of these values associated with the different instructions ﬂowing through the system.
For example, in the detailed structure of PIPE–, there are four white boxes labeled “stat” that hold the status codes for four different instructions.
We need to take great care to make sure we use the proper version of a signal, or else we could have serious errors, such as storing the result computed for one instruction at the destination register speciﬁed by another instruction.
We adopt a naming scheme where a signal stored in a pipeline register can be uniquely identiﬁed by preﬁxing its name with that of the pipe register written in uppercase.
We also need to refer to some signals that have just been computed within a stage.
These are labeled by preﬁxing the signal name with the ﬁrst character of the stage name, written in lowercase.
Using the status codes as examples, we can see control logic blocks labeled “stat” in the fetch and memory stages.
We can also see that the actual status of the overall processor Stat is computed by a block in the write-back stage, based on the status value in pipeline register W.
With our naming system, the uppercase preﬁxes “D,” “E,” “M,” and “W” refer to pipeline registers, and so M_stat refers to the status code ﬁeld of pipeline register M.
The lowercase preﬁxes “f,” “d,” “e,” “m,” and “w” refer to the pipeline stages, and som_stat refers to the status signal generated in the memory stage by a control logic block.
Understanding this naming convention is critical to understanding the operation of our pipelined processors.
The decode stages of SEQ+ and PIPE– both generate signals dstE and dstM indicating the destination register for values valE and valM.
In SEQ+, we could connect these signals directly to the address inputs of the register ﬁle write ports.
WithPIPE–, these signals are carried along in the pipeline through the execute and memory stages, and are directed to the register ﬁle only once they reach the writeback stage (shown in the more detailed views of the stages)
We do this to make sure the write port address and data inputs hold values from the same instruction.
Otherwise, the write back would be writing the values for the instruction in the write-back stage, but with register IDs from the instruction in the decode stage.
As a general principle, we want to keep all of the information about a particular instruction contained within a single pipeline stage.
One block of PIPE– that is not present in SEQ+ in the exact same form is the block labeled “Select A” in the decode stage.
We can see that this block generates the value valA for the pipeline register E by choosing either valP from pipeline register D or the value read from the A port of the register ﬁle.
This block is included to reduce the amount of state that must be carried forward to pipeline registers E and M.
Of all the different instructions, only the call requires valP in the memory stage.
Only the jump instructions require the value of valP in the execute stage (in the event the jump is not taken)
None of these instructions requires a value read from the register ﬁle.
Therefore, we can reduce the amount of pipeline register state by merging these two signals and carrying them through the pipeline as a single signal valA.
In hardware design, it is common to carefully identify how signals get used and then reduce the amount of register state and wiring by merging signals such as these.
Sufﬁce it to say at this point that themost systematic approach is to associate a status code with each instruction as it passes through the pipeline, as we have indicated in the ﬁgure.
We have taken some measures in the design of PIPE– to properly handle control dependencies.
Our goal in the pipelined design is to issue a new instruction on.
Achieving this goal would yield a throughput of one instruction per cycle.
To do this, we must determine the location of the next instruction right after fetching the current instruction.
Unfortunately, if the fetched instruction is a conditional branch, we will not know whether or not the branch should be taken until several cycles later, after the instruction has passed through the execute stage.
Similarly, if the fetched instruction is a ret, we cannot determine the return location until the instruction has passed through the memory stage.
With the exception of conditional jump instructions and ret, we can determine the address of the next instruction based on information computed during the fetch stage.
For call and jmp (unconditional jump), it will be valC, the constant word in the instruction, while for all others it will be valP, the address of the next instruction.
We can therefore achieve our goal of issuing a new instruction every clock cycle in most cases by predicting the next value of the PC.
For most instruction types, our prediction will be completely reliable.
For conditional jumps, we can predict either that a jump will be taken, so that the new PC value would be valC, or we can predict that it will not be taken, so that the new PC value would be valP.
In either case, we must somehow deal with the case where our prediction was incorrect and therefore we have fetched and partially executed the wrong instructions.
This techniqueof guessing thebranchdirection and then initiating the fetching of instructions according to our guess is known as branch prediction.
Some systems devote large amounts of hardware to this task.
In our design, we will use the simple strategy of predicting that conditional branches are always taken, and so we predict the new value of the PC to be valC.
Conversely, a never taken (NT) strategy has around a 40% success rate.
A slightly more sophisticated strategy, known as backward taken, forward not-taken (BTFNT), predicts that branches to lower addresses than the next instruction will be taken, while those to higher addresses will not be taken.
This improvement stems from the fact that loops are closed by backward branches, and loops are generally executed multiple times.
Forward branches are used for conditional operations, and these are less likely to be taken.
As we saw in Section 3.6.6, mispredicted branches can degrade the performance of a program considerably, thus motivating the use of conditional data transfer rather than conditional control transfer when possible.
We are still left with predicting the new PC value resulting from a ret instruction.
Unlike conditional jumps, we have a nearly unbounded set of possible.
In our design, we will not attempt to predict any value for the return address.
Instead, we will simply hold off processing any more instructions until the ret instruction passes through the write-back stage.
We will return to this part of the implementation in Section 4.5.11
With most programs, it is very easy to predict return addresses, since procedure calls and returns occur in matched pairs.
Most of the time that a procedure is called, it returns to the instruction following the call.
This property is exploited in high-performance processors by including a hardware stack within the instruction fetch unit that holds the return address generated by procedure call instructions.
Every time a procedure call instruction is executed, its return address is pushed onto the stack.When a return instruction is fetched, the top value is popped from this stack and used as the predicted return address.
Like branch prediction, a mechanism must be provided to recover when the prediction was incorrect, since there are times when calls and returns do not match.
This hardware stack is not part of the programmer-visible state.
The PIPE– fetch stage, diagrammed at the bottom of Figure 4.41, is responsible for both predicting the next value of the PC and for selecting the actual PC for the instruction fetch.We can see the block labeled “Predict PC” can choose either valP, as computed by the PC incrementer or valC, from the fetched instruction.
This value is stored in pipeline register F as the predicted value of the program counter.
The block labeled “Select PC” is similar to the block labeled “PC” in the SEQ+ PC selection stage (Figure 4.40)
We will return to the handling of jump and return instructions when we complete the pipeline control logic in Section 4.5.11
Our structure PIPE– is a good start at creating a pipelined Y86 processor.
Recall from our discussion in Section 4.4.4, however, that introducing pipelining into a systemwith feedback can lead to problems when there are dependencies between successive instructions.
We must resolve this issue before we can complete our design.
When such dependencies have the potential to cause an erroneous computation by the pipeline, they are called hazards.
Like dependencies, hazards can be classiﬁed as either data hazards or control hazards.
In cycle 6, the second irmovl writes its result to program register %eax.
The addl instruction reads its source operands in cycle 7, so it gets correct values for both %edx and %eax.
Control hazards will be discussed as part of the overall pipeline control (Section 4.5.11)
We focus our attention on the potential data hazards resulting from thedata dependencies between the twoirmovl instructions and the addl instruction.
On the right-hand side of the ﬁgure, we show a pipeline diagram for the instruction sequence.
After the start of cycle 7, both of the irmovl instructions have passed through the writeback stage, and so the register ﬁle holds the updated values of %edx and %eax.
As the addl instruction passes through the decode stage during cycle 7, it will therefore read the correct values for its source operands.
The data dependencies between the two irmovl instructions and the addl instruction have not created data hazards in this example.
The write to program register %eax does not occur until the start of cycle 7, and so the addl instruction gets the incorrect value for this register in the decode stage.
We saw that prog1 will ﬂow through our pipeline and get the correct results, because the three nop instructions create a delay between instructions with data dependencies.
Let us see what happens as these nop instructions are removed.
In this case, the crucial step occurs in cycle 6, when the addl instruction reads its operands from the register ﬁle.
An expanded view of the pipeline activities during this cycle is shown at the bottom of the ﬁgure.
The ﬁrst irmovl instruction has passed through the write-back stage, and so program register %edx has been updated in the register ﬁle.
The second irmovl instruction is in the writeback stage during this cycle, and so the write to program register %eax only occurs at the start of cycle 7 as the clock rises.
As a result, the incorrect value zero would be read for register %eax (recall that we assume all registers are initially 0), since the pending write for this register has not yet occurred.
Clearly we will have to adapt our pipeline to handle this hazard properly.
Figure 4.45 shows what happens when we have only one nop instruction between the irmovl instructions and the addl instruction, yielding a program.
In cycle 5, the addl instruction reads its source operands from the register ﬁle.
The pending write to register %edx is still in the write-back stage, and the pending write to register %eax is still in the memory stage.
Now we must examine the behavior of the pipeline during cycle 5 as the addl instruction passes through the decode stage.
Unfortunately, the pending write to register %edx is still in the write-back stage, and the pending write to %eax is still in the memory stage.
Therefore, the addl instruction would get the incorrect values for both operands.
Now we must examine the behavior of the pipeline during cycle 4 as the addl instruction passes through the decode stage.
Unfortunately, the pending write to register %edx is still in the memory stage, and the new value for %eax is just being computed in the execute stage.
Therefore, the addl instruction would get the incorrect values for both operands.
These examples illustrate that a data hazard can arise for an instruction when one of its operands is updated by any of the three preceding instructions.
These hazards occur because our pipelined processor reads the operands for an.
In cycle 4, the addl instruction reads its source operands from the register ﬁle.
The pending write to register %edx is still in the memory stage, and the new value for register %eax is just being computed in the execute stage.
Hazards can potentially occur when one instruction updates part of the program state that will be read by a later instruction.
For Y86, the program state includes the program registers, the program counter, the memory, the condition code register, and the status register.
Let us look at the hazard possibilities in our proposed design for each of these forms of state.
Program registers: These are the hazards we have already identiﬁed.
They arise because the register ﬁle is read in one stage and written in another, leading to possible unintended interactions between different instructions.
Program counter: Conﬂicts between updating and reading the program counter give rise to control hazards.
No hazard arises when our fetch-stage logic correctly predicts the new value of the program counter before fetching the next instruction.
Mispredicted branches and ret instructions require special handling, as will be discussed in Section 4.5.11
Memory: Writes and reads of the data memory both occur in the memory stage.
By the time an instruction reading memory reaches this stage, any preceding instructions writing memory will have already done so.
On the other hand, there can be interference between instructions writing data in the memory stage and the reading of instructions in the fetch stage, since the instruction and data memories reference a single address space.
This can only happen with programs containing self-modifying code, where instructions write to a portion of memory from which instructions are later fetched.
Some systems have complex mechanisms to detect and avoid such hazards, while others simply mandate that programs should not use selfmodifying code.
We will assume for simplicity that programs do not modify themselves, and therefore we do not need to take special measures to update the instruction memory based on updates to the data memory during program execution.
Condition code register: These are written by integer operations in the execute stage.
They are read by conditional moves in the execute stage and by conditional jumps in the memory stage.
By the timea conditionalmoveor jump reaches the execute stage, anypreceding integer operation will have already completed this stage.
Status register: The program status can be affected by instructions as they ﬂow through the pipeline.
Our mechanism of associating a status code with each instruction in the pipeline enables the processor to come to an orderly halt when an exception occurs, as will be discussed in Section 4.5.9
This analysis shows that we only need to deal with register data hazards, control hazards, and making sure exceptions are handled properly.
A systematic analysis of this form is important when designing a complex system.
It can identify the potential difﬁculties in implementing the system, and it can guide the generation of test programs to be used in checking the correctness of the system.
One very general technique for avoiding hazards involves stalling, where the processor holds back one or more instructions in the pipeline until the hazard condition no longer holds.
Our processor can avoid data hazards by holding back an instruction in the decode stage until the instructions generating its source operands have passed through the write-back stage.
The details of this mechanism will be discussed in Section 4.5.11
We omit prog3 from this discussion, since it operates similarly to the other two examples.
When the addl instruction is in the decode stage, the pipeline control logic detects that at least one of the instructions in the execute, memory, or write-back stage will update either register %edx or register %eax.
For all three programs, the addl instruction ﬁnally gets correct values for its two source operands in cycle 7 and then proceeds down the pipeline.
After decoding the addl instruction in cycle 6, the stall control logic detects a data hazard due to the pending write to register %eax in the write-back stage.
After decoding the addl instruction in cycle 4, the stall control logic detects data hazards for both source registers.
In holding back the addl instruction in the decode stage, we must also hold back the halt instruction following it in the fetch stage.We can do this by keeping the program counter at a ﬁxed value, so that the halt instruction will be fetched repeatedly until the stall has completed.
Stalling involves holding back one group of instructions in their stages while allowing other instructions to continue ﬂowing through the pipeline.
What then shouldwe do in the stages thatwould normally be processing the addl instruction? We handle these by injecting a bubble into the execute stage each time we hold an instruction back in the decode stage.
A bubble is like a dynamically generated nop instruction—it does not cause any changes to the registers, the memory, the condition codes, or the program status.
In these ﬁgures, the arrow between the box labeled “D” for the addl instruction and the box labeled “E” for one of.
This mechanism can be implemented fairly easily (see Problem 4.51), but the resulting performance is not very good.
There are numerous cases in which one instruction updates a register and a closely following instruction uses the same register.
This will cause the pipeline to stall for up to three cycles, reducing the overall throughput signiﬁcantly.
Our design for PIPE– reads source operands from the register ﬁle in the decode stage, but there can also be a pending write to one of these source registers in the write-back stage.
Rather than stalling until the write has completed, it can simply pass the value that is about to be written to pipeline register E as the source operand.
Figure 4.49 shows this strategy with an expanded view of the.
In cycle 6, the decodestage logic detects the presence of a pending write to register %eax in the write-back stage.
It uses this value for source operand valB rather than the value read from the register ﬁle.
In cycle 5, the decodestage logic detects a pending write to register %edx in the write-back stage and to register %eax in the memory stage.
It uses these as the values for valA and valB rather than the values read from the register ﬁle.
The decode-stage logic detects that register %eax is the source register for operand valB, and that there is also a pending write to %eax on write port E.
It can therefore avoid stalling by simply using the data word supplied to port E (signal W_valE) as the value for operand valB.
This technique of passing a result value directly from one pipeline stage to an earlier one is commonly known as data forwarding (or simply forwarding, and sometimes bypassing)
It allows the instructions of prog2 to proceed through the pipeline without any stalling.
Data forwarding requires adding additional data connections and control logic to the basic hardware structure.
In cycle 4, the decodestage logic detects a pending write to register %edx in the memory stage.
It also detects that a new value is being computed for register %eax in the execute stage.
It uses these as the values for valA and valB rather than the values read from the register ﬁle.
In cycle 4, the decode-stage logic detects a pending write to register %edx in the memory stage, and also that the value being computed by the ALU in the execute stage will later be written to register %eax.
It can use the value in thememory stage (signalM_valE) for operand valA.
It can also use the ALU output (signal e_valE) for operand valB.
Note that using the ALU output does not introduce any timing problems.
The decode stage only needs to generate signals valA and valB by the end of the clock cycle so that pipeline register E can be loaded with the results from the decode stage as the clock rises to start the next cycle.
Forwarding can also be used with values read from the memory and destined for write port M.
From thememory stage, we can forward the value that has just been read from the data memory (signal m_valM)
From the write-back stage, we can forward the pending write to port M (signal W_valM)
Associated with every value that will be written back to the register ﬁle is the destination register ID.
The logic can compare these IDs with the source register IDs srcA and srcB to detect a case for forwarding.
It is possible to have multiple destination register IDs match one of the source IDs.
We must establish a priority among the different forwarding sources to handle such cases.
This will be discussed when we look at the detailed design of the forwarding logic.
Figure 4.52 shows the structure of PIPE, an extension of PIPE– that can handle data hazards by forwarding.
Comparing this to the structure of PIPE(Figure 4.41), we can see that the values from the ﬁve forwarding sources are fed back to the two blocks labeled “Sel+Fwd A” and “Fwd B” in the decode stage.
The block labeled “Sel+FwdA” combines the role of the block labeled “Select A” in PIPE– with the forwarding logic.
It allows valA for pipeline register E to be either the incremented program counter valP, the value read from the A port of the register ﬁle, or one of the forwarded values.
The block labeled “Fwd B” implements the forwarding logic for source operand valB.
One class of data hazards cannot be handled purely by forwarding, because memory reads occur late in the pipeline.
In order to “forward” from the mrmovl to the addl, the forwarding logic would have to make the value go backward in time! Since this is clearly impossible, we must ﬁnd some other mechanism for handling this form of data hazard.
As Figure 4.54 demonstrates, we can avoid a load/use data hazard with a combination of stalling and forwarding.
This requires modiﬁcations of the control logic, but it can use existing bypass paths.
As the mrmovl instruction passes through the execute stage, the pipeline control logic detects that the instruction in the decode stage (the addl) requires the result read from memory.
It stalls the instruction in the decode stage for one cycle, causing a bubble to be injected into the execute stage.
As the expanded view of cycle 8 shows, the value read from memory can then be forwarded from the memory stage to the addl instruction in the decode stage.
The value for register %ebx is also forwarded from the writeback to the memory stage.
Figure 4.52 Hardware structure of PIPE, our ﬁnal pipelined implementation.
The additional bypassing paths enable forwarding the results from the three preceding instructions.
This allows us to handle most forms of data hazards without stalling the pipeline.
The preceding mrmovl reads a new value for this register during the memory stage in cycle 8, which is too late for the addl instruction.
This use of a stall to handle a load/use hazard is called a load interlock.
Load interlocks combined with forwarding sufﬁce to handle all possible forms of data hazards.
Since only load interlocks reduce the pipeline throughput, we can nearly achieve our throughput goal of issuing one new instruction on every clock cycle.
As we will discuss in Chapter 8, a variety of activities in a processor can lead to exceptional control ﬂow, where the normal chain of program execution gets broken.
Exceptions can be generated either internally, by the executing program, or externally, by some outside signal.
A more complete processor design would also handle external exceptions, such as when the processor receives a signal that the network interface has received a new packet, or the user has clicked a mouse button.
Handling exceptions correctly is a challenging aspect of anymicroprocessor design.They can.
By stalling the addl instruction for one cycle in the decode stage, the value for valB can be forwarded from the mrmovl instruction in the memory stage to the addl instruction in the decode stage.
Our handling of the three internal exceptions gives just a glimpse of the true complexity of correctly detecting and handling exceptions.
Let us refer to the instruction causing the exception as the excepting instruction.
In the case of an invalid instruction address, there is no actual excepting instruction, but it is useful to think of there being a sort of “virtual instruction” at the invalid address.
In our simpliﬁed ISA model, we want the processor to halt when it reaches an exception and to set the appropriate status code, as listed in Figure 4.5
It should appear that all instructions up to the excepting instruction have completed, but none of the following instructions should have any effect on the programmer-visible state.
In a more complete design, the processor would continue by invoking an exception handler, a procedure that is part of the operating.
First, it is possible to have exceptions triggered by multiple instructions simultaneously.
For example, during one cycle of pipeline operation, we could have a halt instruction in the fetch stage, and the data memory could report an out-of-bounds data address for the instruction in thememory stage.Wemust determinewhich of these exceptions the processor should report to the operating system.
The basic rule is to put priority on the exception triggered by the instruction that is furthest along the pipeline.
In the example above, thiswouldbe theout-of-bounds address attempted by the instruction in thememory stage.
A second subtlety occurs when an instruction is ﬁrst fetched and begins execution, causes an exception, and later is canceled due to amispredicted branch.
The following is an example of such a program in its object code form:
In this program, the pipeline will predict that the branch should be taken, and so it will fetch and attempt to use a byte with value 0xFF as an instruction (generated in the assembly code using the .byte directive)
The decode stage will therefore detect an invalid instruction exception.
Later, the pipeline will discover that the branch should not be taken, and so the instruction at address 0x00e should never even have been fetched.
The pipeline control logic will cancel this instruction, but we want to avoid raising an exception.
A third subtlety arises because a pipelined processor updates different parts of the system state in different stages.
It is possible for an instruction following one causing an exception to alter some part of the state before the excepting instruction completes.
The pushl instruction causes an address exception, because decrementing the stack pointer causes it towrap around to 0xfffffffc.
This would violate our requirement that none of the instructions following the excepting instruction should have had any effect on the system state.
In general, we can both correctly choose among the different exceptions and avoid raising exceptions for instructions that are fetched due to mispredicted branches bymerging the exception-handling logic into the pipeline structure.
If an instruction generates an exception at some stage in its processing, the status ﬁeld is set to indicate the nature of the exception.
The exception status propagates through the pipeline with the rest of the information for that instruction, until it reaches the write-back stage.
At this point, the pipeline control logic detects the occurrence of the exception and stops execution.
To avoid having any updating of the programmer-visible state by instructions beyond the excepting instruction, the pipeline control logic must disable any updating of the condition code register or the datamemory when an instruction in thememory orwrite-back stages has caused an exception.
In the example program above, the control logic would detect that the pushl in the memory stage has caused an exception, and therefore the updating of the condition code register by the addl instruction would be disabled.
Let us consider how this method of handling exceptions deals with the subtleties we have mentioned.
When an exception occurs in one or more stages of a pipeline, the information is simply stored in the status ﬁelds of the pipeline registers.
The event has no effect on the ﬂow of instructions in the pipeline until an excepting instruction reaches the ﬁnal pipeline stage, except to disable any updating of the programmer-visible state (the condition code register and the memory) by later instructions in the pipeline.
Since instructions reach the write-back stage in the same order as they would be executed in a nonpipelined processor, we are guaranteed that the ﬁrst instruction encountering an exception will arrive ﬁrst in the write-back stage, at which point program execution can stop and the status code in pipeline register W can be recorded as the program status.
If some instruction is fetched but later canceled, any exception status information about the instruction gets canceled as well.
No instruction following one that causes an exception can alter the programmer-visible state.
The simple rule of carrying the exception status together with all other information about an instruction through the pipeline provides a simple and reliable mechanism for handling exceptions.
We have now created an overall structure for PIPE, our pipelined Y86 processor with forwarding.
It uses the same set of hardware units as the earlier sequential designs, with the addition of pipeline registers, some reconﬁgured logic blocks, and additional pipeline control logic.
In this section, we go through the design of the different logic blocks, deferring the design of the pipeline control logic to the next section.
As an example, compare the HCL code for the logic that generates the srcA signal in SEQ to the corresponding code in PIPE:
To avoid repetition, we will not show the HCL code here for blocks that only differ from those in SEQ because of the preﬁxes on names.
As a reference, the complete HCL code for PIPE is given in Web Aside arch:hcl.
Figure 4.55 provides a detailed view of the PIPE fetch stage logic.
As discussed earlier, this stage must also select a current value for the program counter and predict the next PC value.
The hardware units for reading the instruction from memory and for extracting the different instruction ﬁelds are the same as those we considered for SEQ (see the fetch stage in Section 4.3.4)
The PC selection logic chooses between three program counter sources.
As a mispredicted branch enters thememory stage, the value of valP for this instruction (indicating the address of the following instruction) is read from pipeline register M (signalM_valA)
When a ret instruction enters the write-back stage, the return address is read from pipeline register W (signal W_valM)
All other cases use the predicted value of the PC, stored in pipeline register F (signal F_predPC):
Within the one cycle time limit, the processor can only predict the address of the next instruction.
The PC prediction logic chooses valC for the fetched instruction when it is either a call or a jump, and valP otherwise:
The logic blocks labeled “Instr valid,” “Need regids,” and “Need valC” are the same as for SEQ, with appropriately named source signals.
Unlike in SEQ, we must split the computation of the instruction status into two parts.
In the fetch stage, we can test for amemory error due to an out-of-range instruction address, and we can detect an illegal instruction or a halt instruction.
Detecting an invalid data address must be deferred to the memory stage.
No instruction requires both valP and the value read from register port A, and so these two can be merged to form the signal valA for later stages.
The block labeled “Sel+Fwd A” performs this task and also implements the forwarding logic for source operand valA.
The block labeled “Fwd B” implements the forwarding logic for source operand valB.
The register write locations are speciﬁed by the dstE and dstM signals from the write-back stage rather than from the decode stage, since it is writing the results of the instruction currently in the write-back stage.
Figure 4.56 gives a detailed view of the decode and write-back logic for PIPE.
The blocks labeled “dstE”, “dstM”, “srcA”, and “srcB” are very similar to their counterparts in the implementation of SEQ.
This is because we want the writes to occur to the destination registers speciﬁed by the instruction in the write-back stage.
Practice Problem 4.29 The block labeled “dstE” in the decode stage generates the register ID for the E port of the register ﬁle, based on ﬁelds from the fetched instruction in pipeline.
The resulting signal is named d_dstE in the HCL description of PIPE.
Write HCL code for this signal, based on the HCL description of the SEQ signal dstE.
Do not concern yourself with the logic to implement conditional moves yet.
Most of the complexity of this stage is associated with the forwarding logic.
As mentioned earlier, the block labeled “Sel+Fwd A” serves two roles.
It merges the valP signal into the valA signal for later stages in order to reduce the amount of state in the pipeline register.
It also implements the forwarding logic for source operand valA.
The merging of signals valA and valP exploits the fact that only the call and jump instructions need the value of valP in later stages, and these instructions do not need the value read from the A port of the register ﬁle.
This selection is controlled by the icode signal for this stage.
As mentioned in Section 4.5.7, there are ﬁve different forwarding sources, each with a data word and a destination register ID:
If none of the forwarding conditions hold, the block should select d_rvalA, the value read from register port A as its output.
Putting all of this together, we get the following HCL description for the new value of valA for pipeline register E:
The priority given to the ﬁve forwarding sources in the above HCL code is very important.
This priority is determined in the HCL code by the order in which.
In cycle 4, values for %edx are available from both the execute and memory stages.
The forwarding logic should choose the one in the execute stage, since it represents the most recently generated value for this register.
If any order other than the one shown were chosen, the pipeline would behave incorrectly for some programs.
Figure 4.57 shows an example of a program that requires a correct setting of priority among the forwarding sources in the execute andmemory stages.
In this program, the ﬁrst two instructions write to register %edx, while the third uses this register as its source operand.
When the rrmovl instruction reaches the decode stage in cycle 4, the forwarding logic must choose between two values destined for its source register.
To imitate this behavior, our pipelined implementation should always give priority to the forwarding source in the earliest pipeline stage, since it holds the latest instruction in the program sequence setting the register.
Thus, the logic in the HCL code above ﬁrst tests the forwarding source in the execute stage, then those in the memory stage, and ﬁnally the sources in the write-back stage.
The forwarding priority between the two sources in either the memory or the write-back stages are only a concern for the instruction popl %esp, since only this instruction can write two registers simultaneously.
Describe the resulting behavior of the rrmovl instruction (line 5) for the following program:
Describe how the error would occur and its effect on the program behavior.
As shown in Figure 4.52, the overall processor status Stat is computed by a block based on the status value in pipeline register W.
Recall from Section 4.1.1 that the code should indicate either normal operation (AOK) or one of the three exception conditions.
Since pipeline register W holds the state of the most recently completed instruction, it is natural to use this value as an indication of the overall processor status.
The only special case to consider is when there is a bubble in the write-back stage.
This is part of normal operation, and so we want the status code to be AOK for this case as well:
The hardware units and the logic blocks are identical to those in SEQ, with an appropriate renaming of signals.
One difference is that the logic labeled “Set CC,” which determines whether or not update the condition codes, has signals m_stat and.
This part of the design is very similar to the logic in the SEQ implementation.
These signals are used to detect cases where an instruction causing an exception is passing through later pipeline stages, and therefore any updating of the condition codes should be suppressed.
This aspect of the design is discussed in Section 4.5.11
Suppose instead that we use signal E_dstE, the destination register ID in pipeline register E for this selection.
Write a Y86 program that would give an incorrect result with this modiﬁed forwarding logic.
This block served to select between data sources valP (for call instructions) and valA, but this selection is now performed by the block labeled “Sel+Fwd A” in the decode stage.
Most other blocks in this stage are identical to their counterparts in SEQ, with an appropriate renaming of the signals.
In this ﬁgure, you can also see that many of the values in pipeline registers and M and W are supplied to other parts of the circuit as part of the forwarding and pipeline control logic.
Figure 4.59 PIPE memory stage logic.Many of the signals from pipeline registers M and W are passed down to earlier stages to provide write-back results, instruction addresses, and forwarded results.
Practice Problem 4.34 In this stage, we can complete the computation of the status code Stat by detecting the case of an invalid address for the data memory.
Weare now ready to complete our design for PIPEby creating the pipeline control logic.
This logic must handle the following four control cases for which other mechanisms, such as data forwarding and branch prediction, do not sufﬁce:
Processing ret: The pipeline must stall until the ret instruction reaches the write-back stage.
Load/use hazards: The pipeline must stall for one cycle between an instruction that reads a value from memory and an instruction that uses this value.
Mispredicted branches: By the time the branch logic detects that a jump should not have been taken, several instructions at the branch target will have started down the pipeline.
Exceptions: When an instruction causes an exception, we want to disable the updating of the programmer-visible state by later instructions and halt execution once the excepting instruction reaches the write-back stage.
Wewill go through the desired actions for eachof these cases and thendevelop control logic to handle all of them.
This program is shown in assembly code, but with the addresses of the different instructions on the left for reference:
Figure 4.60 shows how we want the pipeline to process the ret instruction.
As with our earlier pipeline diagrams, this ﬁgure shows the pipeline activity with time growing to the right.
Unlike before, the instructions are not listed in the same order they occur in the program, since this program involves a control ﬂow where instructions are not executed in a linear sequence.
Look at the instruction addresses to see from where the different instructions come in the program.
While it passes through the decode, execute, and memory stages, the pipeline cannot do any useful activity.
Once the ret instruction reaches the write-back stage, the PC selection logic will set the program counter to the return address, and therefore the fetch stage will fetch the irmovl instruction at the return point (address 0x00b)
The pipeline should stall while the ret passes through the decode, execute, and memory stages, injecting three bubbles in the process.
The PC selection logic will choose the return address as the instruction fetch address once the ret reaches the write-back stage (cycle 7)
The fetch stage repeatedly fetches the rrmovl instruction following the ret instruction, but then the pipeline control logic injects a bubble into the decode stage rather than allowing the rrmovl instruction to proceed.
The resulting behavior is equivalent to that shown in Figure 4.60
On every cycle, the fetch stage reads some instruction from the instruction memory.
Looking at the HCL code for implementing the PC prediction logic in Section 4.5.10, we can see that for the ret instruction the new value of the PC is predicted to be valP, the address of the following instruction.
In our example program, this would be 0x021, the address of the rrmovl instruction following the ret.
This prediction is not correct for this example, nor would it be for most cases, but we are not attempting to predict return addresses correctly in our design.
For three clock cycles, the fetch stage stalls, causing the rrmovl instruction to be fetched but then replaced by a bubble in the decode stage.
This process is illustrated in Figure 4.61 by the three fetches, with an arrow leading down to the bubbles passing through the remaining pipeline stages.
Only the mrmovl and popl instructions read data from memory.
When either of these is in the execute stage, and an instruction requiring the destination register is in the decode stage, we want to hold back the second instruction in the decode stage and inject a bubble into the execute stage on the next cycle.
After this, the forwarding logic will resolve the data hazard.
The pipeline can hold back an instruction in the decode stage by keeping pipeline registerD in a ﬁxed state.
In doing so, it should also keep pipeline register F in a ﬁxed state, so that the next instruction will be fetched a second time.
The pipeline predicts branches will be taken and so starts fetching instructions at the jump target.
Two instructions are fetched before the misprediction is detected in cycle 4 when the jump instruction ﬂows through the execute stage.
In cycle 5, the pipeline cancels the two target instructions by injecting bubbles into the decode and execute stages, and it also fetches the instruction following the jump.
To handle a mispredicted branch, consider the following program, shown in assembly code, but with the instruction addresses shown on the left for reference:
Figure 4.62 shows how these instructions are processed.As before, the instructions are listed in the order they enter the pipeline, rather than the order they occur in the program.
By the time the branch logic detects that the jump should not be taken during cycle 4, two instructions have been fetched that should not continue being executed.
Fortunately, neither of these instructions has caused a change in the programmer-visible state.
That can only occur when an instruction reaches the execute stage, where it can cause the condition codes to change.
We can simply cancel (sometimes called instruction squashing) the twomisfetched instructions by injecting bubbles into the decode and execute instructions on the following cycle while also fetching the instruction following the jump instruction.
The two misfetched instructions will then simply disappear from the pipeline.
As we will discuss in Section 4.5.11, a simple extension to the basic clocked register.
Our stage designs include a status code stat in each pipeline register to track the status of each instruction as it passes through the pipeline stages.
The pipeline diagram in Figure 4.63 illustrates how our pipeline control handles the situationwhere an instruction causing an exception is followed by one that would change the condition codes.
On cycle 6, the pushl instruction reaches the memory stage and generates amemory error.
On the same cycle, the addl instruction in the execute stage generates new values for the condition codes.
On cycle 6, the invalid memory reference by the pushl instruction causes the updating of the condition codes to be disabled.
The pipeline starts injecting bubbles into the memory stage and stalling the excepting instruction in the write-back stage.
We can also see the combination of injecting bubbles into the memory stage and stalling the excepting instruction in the write-back stage in the example of Figure 4.63—the pushl instruction remains stalled in the writeback stage, and none of the subsequent instructions get past the execute stage.
By this combination of pipelining the status signals, controlling the setting of condition codes, and controlling the pipeline stages, we achieve the desired behavior for exceptions: all instructions prior to the excepting instruction are completed, while none of the following instructions has any effect on the programmer-visible state.
It gives expressions describing the conditions under which the three special cases arise.
These expressions are implemented by simple blocks of combinational logic that must generate their results before the end of the clock cycle in order to control the action of the pipeline registers as the clock rises to start the next cycle.
During a clock cycle, pipeline registers D, E, and M hold the states of the instructions that are in the decode, execute, and memory pipeline stages, respectively.
Detecting a ret instruction as it passes through the pipeline simply involves checking the instruction codes of the instructions in the decode, execute, and memory stages.
Detecting a load/use hazard involves checking the instruction type (mrmovl or popl) of the instruction in the execute stage and comparing its destination register with the source registers of the instruction in the decode stage.
The pipeline control logic should detect a mispredicted branch while the jump instruction is in the execute stage, so that it can set up the conditions required to recover from themisprediction as the instruction enters thememory stage.When a jump instruction is in the execute stage, the signal e_Cnd indicates whether or not the jump should be taken.
We detect an excepting instruction by examining the instruction status values in the memory and write-back stages.
This internal signal incorporates the possibility of a data memory address error.
Figure 4.65 shows low-level mechanisms that allow the pipeline control logic to hold back an instruction in a pipeline register or to inject a bubble into the pipeline.
Thesemechanisms involve small extensions to the basic clocked register described in Section 4.2.5
Suppose that each pipeline register has two control inputs stall and bubble.
The settings of these signals determine how the pipeline register is updated as the clock rises.
Processing ret stall bubble normal normal normal Load/use hazard stall stall bubble normal normal Mispredicted branch normal bubble bubble normal normal.
The different conditions require altering the pipeline ﬂow by either stalling the pipeline or by canceling partially executed instructions.
The particular pattern of ones and zeros for a pipeline register’s reset conﬁguration depends on the set of ﬁelds in the pipeline register.
For example, to inject a bubble into pipeline register D, we want the icode ﬁeld to be set to the constant value INOP (Figure 4.26)
To inject a bubble into pipeline register E, we want the icode ﬁeld to be set to INOP and the dstE, dstM, srcA, and srcB ﬁelds to be set to the constant RNONE.
Determining the reset conﬁguration is one of the tasks for the hardware designer in designing a pipeline register.
The table in Figure 4.66 shows the actions the different pipeline stages should take for each of the three special conditions.
Each involves some combination of normal, stall, and bubble operations for the pipeline registers.
In terms of timing, the stall andbubble control signals for the pipeline registers are generated by blocks of combinational logic.
These values must be valid as the clock rises, causing each of the pipeline registers to either load, stall, or bubble as the next clock cycle begins.
With this small extension to the pipeline register designs, we can implement a complete pipeline, including all of its control, using the basic building blocks of combinational logic, clocked registers, and randomaccess memories.
In our discussion of the special pipeline control conditions so far, we assumed that atmost one special case could arise during any single clock cycle.A commonbug in designing a system is to fail to handle instances where multiple special conditions arise simultaneously.
We need not worry about combinations involving program exceptions, since we have carefully designed our exception-handling mechanism to consider other instructions in the pipeline.
Figure 4.67 diagrams the pipeline states that cause the other three special control conditions.
These diagrams show blocks for the decode, execute, and memory stages.
The shaded boxes represent particular constraints that must be satisﬁed for the condition to arise.
A mispredicted branch requires the instruction in the execute stage to have a jump instruction.
There are three possible cases for ret—the instruction can be in either the decode, execute, or memory stage.
As the ret instruction moves through the pipeline, the earlier pipeline stages will have bubbles.
We can see by these diagrams that most of the control conditions aremutually exclusive.
For example, it is not possible to have a load/use hazard and a mispredicted branch simultaneously, since one requires a load instruction (mrmovl or popl) in the execute stage, while the other requires a jump.
Similarly, the second and third ret combinations cannot occur at the same time as a load/use hazard or a mispredicted branch.
Only the two combinations indicated by arrows can arise simultaneously.
CombinationA involves a not-taken jump instruction in the execute stage and a ret instruction in the decode stage.
Setting up this combination requires the ret to be at the target of a not-taken branch.
The pipeline control logic should detect that the branch was mispredicted and therefore cancel the ret instruction.
Processing ret stall bubble normal normal normal Mispredicted branch normal bubble bubble normal normal.
That is, it would be handled like a mispredicted branch, but with a stall in the fetch stage.
Fortunately, on the next cycle, the PC selection logic will choose the address of the instruction following the jump, rather than the predicted program.
We conclude that the pipeline will correctly handle this combination.
Combination B involves a load/use hazard, where the loading instruction sets register %esp, and the ret instruction then uses this register as a source operand, since it must pop the return address from the stack.
The pipeline control logic should hold back the ret instruction in the decode stage.
Combining the control actions for the combinationB conditions (Figure 4.66), we get the following pipeline control actions:
Processing ret stall bubble normal normal normal Load/use hazard stall stall bubble normal normal.
If both sets of actionswere triggered, the control logicwould try to stall theret instruction to avoid the load/use hazard but also inject a bubble into the decode stage due to the ret instruction.
Clearly, we do not want the pipeline to perform both sets of actions.
Instead, we want it to just take the actions for the load/use hazard.
The actions for processing the ret instruction should be delayed for one cycle.
In fact, our original implementation of the PIPE control logic did not handle this combination correctly.
Even though the design had passedmany simulation tests, it had a subtle bug that was uncovered only by the analysis we have just shown.When a program having combination B was executed, the control logic would set both the bubble and the stall signals for pipeline registerD to 1.This example shows the importance of systematic analysis.
It would be unlikely to uncover this bug by just running normal programs.
If left undetected, the pipeline would not faithfully implement the ISA behavior.
Figure 4.68 shows the overall structure of the pipeline control logic.
Based on signals from the pipeline registers and pipeline stages, the control logic generates stall and bubble control signals for the pipeline registers, and also determines whether the condition code registers should be updated.
This logic overrides the normal ﬂow of instructions through the pipeline to handle special conditions such as procedure returns, mispredicted branches, load/use hazards, and program exceptions.
Pipeline register F must be stalled for either a load/use hazard or a ret instruction:
Pipeline register D must be set to bubble for a mispredicted branch or a ret instruction.
As the analysis in the preceding section shows, however, it should.
This should only occur for OPl instructions, and should consider the effects of program exceptions.
The latter signal requires modifying the exception condition listed in Figure 4.64
This covers all of the special pipeline control signal values.
In the complete HCL code for PIPE, all other pipeline control signals are set to zero.
With pipelining, there aremany subtle interactions between the instructions at different pipeline stages.
We have seen that many of the design challenges involve unusual instructions (such as popping to the stack pointer) or unusual instruction combinations (such as a not-taken jump followed by a ret)
We also see that exception handling adds an entirely new dimension to the possible pipeline behaviors.
How then can we be sure that our design is correct? For hardware manufacturers, this is a dominant concern, since they cannot simply report an error and have users download code patches over the Internet.
Even a simple logic design error can have serious consequences, especially asmicroprocessors are increasingly used to operate systems that are critical to our lives and health, such as automotive antilock braking systems, heart pacemakers, and aircraft control systems.
Simply simulating a design while running a number of “typical” programs is not a sufﬁcient means of testing a system.
Instead, thorough testing requires devising ways of systematically generating many tests that will exercise as many different instructions and instruction combinations as possible.
In creating our Y86 processor designs, we also devised a number of testing scripts, each of which generates many different tests, runs simulations of the processor, and compares the resulting register andmemory values to those produced by our yis instruction set simulator.
The key idea of this testing method is that we want to be as systematic as possible, generating tests that create the different conditions that are likely to cause pipeline errors.
Evenwhen a design passes an extensive set of tests, we cannot be certain that it will operate correctly for all possible programs.
The number of possible programs we could test is unimaginably large, even if we only consider tests consisting of short code segments.
Newer methods of formal veriﬁcation, however, hold the promise that we can have tools that rigorously consider all possible behaviors of a system and determine whether or not there are any design errors.
We set up a framework to compare the behavior of the pipelined design PIPE to the unpipelined version SEQ.
That is, it was able to prove that for an arbitrary Y86 program, the two processors would have identical effects on the programmer-visible state.
Of course, our veriﬁer cannot actually run all possible programs, since there are an inﬁnite number of them.
Instead, it uses a form of proof by induction, showing a consistency between the two processors on a cycle-by-cycle basis.
Carrying out this analysis requires reasoning about the hardware using symbolicmethods in whichwe consider all program values to be arbitrary integers, and we abstract the ALU as a sort of “black box,” computing some unspeciﬁed function over its arguments.
We assume only that the ALUs for SEQ and PIPE compute identical functions.
We used the HCL descriptions of the control logic to generate the control logic for our symbolic processor models, and so we could catch any bugs in the HCL code.
Being able to show that SEQ and PIPE are identical does not guarantee that either of them faithfully implements the Y86 instruction set.
However, it would uncover any bug due to an incorrect pipeline design, and this is the major source of design errors.
In our experiments, we veriﬁed not only the version of PIPEwe have considered in this chapter but also several variants that we give as homework problems, in which we add more instructions, modify the hardware capabilities, or use different branch prediction strategies.
This exposed a weakness in our testing regime that caused us to add additional cases to the ctest testing script.
Formal veriﬁcation is still in an early stage of development.
The tools are often difﬁcult to use, and they do not have the capacity to verify large-scale designs.
We were able to verify our Y86 processors in part because of their relative simplicity.
Even then, it required several weeks of effort and multiple runs of the tools, each requiring up to eight hours of computer time.
This is an active area of research, with some tools becoming commercially available, and some in use at companies such as Intel, AMD, and IBM.
As we have mentioned, modern logic design involves writing textual representations of hardware designs in a hardware description language.
The design can then be tested by both simulation and by a variety of formal veriﬁcation tools.
Once we have conﬁdence in the design, we can use logic synthesis tools to translate the design into actual logic circuits.
We have developed models of our Y86 processor designs in the Verilog hardware description language.
These designs combine modules implementing the basic building blocks of the processor, along with control logic generated directly from theHCL descriptions.We have been able to synthesize some of these designs, download the logic circuit descriptions onto ﬁeld-programmable gate array (FPGA) hardware, and run the processors on actual Y86 programs.
We can see that the conditions requiring special action by the pipeline control logic all cause our pipeline to fall short of the goal of issuing a new instruction on every clock cycle.
We can measure this inefﬁciency by determining how often a bubble gets injected into the pipeline, since these cause unused pipeline cycles.
A return instruction generates three bubbles, a load/use hazard generates one, and a mispredicted branch generates two.
We can quantify the effect these penalties have on the overall performance by computing an estimate of the average number of clock cycles PIPE would require per instruction it executes, a measure known as the CPI (for “cycles per instruction”)
This measure is the reciprocal of the average throughput of the pipeline, but with time measured in clock cycles rather than picoseconds.
It is a useful measure of the architectural efﬁciency of a design.
If we ignore the performance implications of exceptions (which, by deﬁnition, will only occur rarely), another way to think about CPI is to imagine we run the processor on some benchmark program and observe the operation of the execute stage.
If the stage processes a total of Ci instructions and Cb bubbles, then the processor has required around Ci + Cb total clock cycles to execute Ci instructions.
We say “around” because we ignore the cycles required to start the instructions ﬂowing through the pipeline.We can then compute the CPI for this benchmark as follows:
That is, theCPI equals 1.0 plus apenalty termCb/Ci indicating the averagenumber of bubbles injected per instruction executed.
Since only three different instruction types can cause a bubble to be injected, we can break this penalty term into three components:
Each of these penalties indicates the total number of bubbles injected for the stated reason (some portion of Cb) divided by the total number of instructions that were executed (Ci)
To estimate each of these penalties, we need to know how frequently the relevant instructions (load, conditional branch, and return) occur, and for each of these how frequently the particular condition arises.
Load instructions (mrmovl and popl) account for 25% of all instructions executed.
We can therefore estimate each of our penalties as the product of the frequency of the instruction type, the frequency the condition arises, and the number of bubbles that get injected when the condition occurs:
Our goal was to design a pipeline that can issue one instruction per cycle, giving a CPI of 1.0
We did not quite meet this goal, but the overall performance is still quite good.We can also see that any effort to reduce the CPI further should focus onmispredicted branches.
What wouldbe the impact onCPI, assuming all of theother frequencies arenot affected?
Assume we are using these programs to compute the sum of the absolute values of a very long array, and so the overall performance is determined largely by the number of cycles required by the inner loop.
Assume our jump instructions are predicted as being taken, and that around 50% of the array values are positive.
On average, how many instructions are executed in the inner loops of the two programs?
On average, how many bubbles would be injected into the inner loop of the two programs?
What is the average number of clock cycles required per array element for the two programs?
We have created a structure for the PIPE pipelined microprocessor, designed the control logic blocks, and implemented pipeline control logic to handle special cases where normal pipeline ﬂow does not sufﬁce.
Still, PIPE lacks several key features that would be required in an actual microprocessor design.
We highlight a few of these and discuss what would be required to add them.
All of the instructions in the Y86 instruction set involve simple operations such as adding numbers.
These can be processed in a single clock cycle within the execute stage.
In a more complete instruction set, we would also need to implement instructions requiring more complex operations such as integer multiplication and division, and ﬂoating-point operations.
To implement these instructions, we require both additional hardware to perform the computations and a mechanism to coordinate the processing of these instructions with the rest of the pipeline.
One simple approach to implementing multicycle instructions is to simply expand the capabilities of the execute stage logic with integer and ﬂoating-point arithmetic units.
An instruction remains in the execute stage for as many clock cycles as it requires, causing the fetch and decode stages to stall.
This approach is simple to implement, but the resulting performance is not very good.
Better performance can be achieved by handling the more complex operations with special hardware functional units that operate independently of the main pipeline.
Typically, there is one functional unit for performing integer multiplication and division, and another for performing ﬂoating-point operations.
As an instruction enters the decode stage, it can be issued to the special unit.While the unit performs the operation, the pipeline continues processing other instructions.
Typically, the ﬂoating-point unit is itself pipelined, and thus multiple operations can execute concurrently in the main pipeline and in the different units.
The operations of the different units must be synchronized to avoid incorrect behavior.
For example, if there are data dependencies between the different operations being handled by different units, the control logic may need to stall one part of the system until the results from an operation handled by some other part of the system have been completed.
Often, different forms of forwarding are used to convey results from one part of the system to other parts, just as we saw between the different stages of PIPE.
The overall design becomes more complex than we have seen with PIPE, but the same techniques of stalling, forwarding, and pipeline control can be used to make the overall behavior match the sequential ISA model.
In our presentation of PIPE, we assumed that both the instruction fetch unit and the data memory could read or write any memory location in one clock cycle.
We also ignored the possible hazards caused by self-modifying code where one instruction writes to the region of memory from which later instructions are fetched.
Furthermore, we reference memory locations according to their virtual addresses, and these require a translation into physical addresses before the actual read or write operation can be performed.
Clearly, it is unrealistic to do all of this processing in a single clock cycle.
Even worse, the memory values being accessed may reside on disk, requiring millions of clock cycles to read into the processor memory.
The memory system is organized as a hierarchy, with faster but smaller memories holding a subset of the memory being backed up by slower and larger memories.
At the level closest to the processor, the cache memories provide fast access to the most heavily referenced memory.
A typical processor has two ﬁrst-level caches—one for reading instructions and one for reading andwriting data.
Another type of cachememory, known as a translation look-aside buffer, or TLB, provides a fast translation from virtual to physical addresses.
Using a combination of TLBs and caches, it is indeed possible to read instructions and read or write data in a single clock cycle most of the time.
Thus, our simpliﬁed view of memory referencing by our processors is actually quite reasonable.
Although the caches hold the most heavily referenced memory locations, there will be times when a cache miss occurs, where some reference is made to a location that is not held in the cache.
Meanwhile, the pipeline simply stalls, holding the instruction in the fetch or memory stage until the cache can perform the read or write operation.
In terms of our pipeline design, this can be implemented by adding more stall conditions to the pipeline control logic.
A cache miss and the consequent synchronization with the pipeline is handled completely by hardware, keeping the time required down to a small number of clock cycles.
In some cases, the memory location being referenced is actually stored in the disk memory.
When this occurs, the hardware signals a page fault exception.
Like other exceptions, this will cause the processor to invoke the operating system’s exception handler code.
This code will then set up a transfer from the disk to the main memory.
Once this completes, the operating system will return back to the original program, where the instruction causing the page fault will be reexecuted.
This time, the memory reference will succeed, although it might cause a cache miss.
Having the hardware invoke an operating system routine, which then returns control back to the hardware, allows the hardware and system software to cooperate in the handling of page faults.
Since accessing a disk can require millions of clock cycles, the several thousand cycles of processing performed by the OS page fault handler has little impact on performance.
From the perspective of the processor, the combination of stalling to handle short-duration cache misses and exception handling to handle long-duration page faults takes care of any unpredictability in memory access times due to the structure of the memory hierarchy.
Aﬁve-stage pipeline, such as we have shownwith the PIPE processor, represented the state of the art in processor design in the mid-1980s.
These pipelined designs are limited to a throughput of at most one instruction per clock cycle.
The different stages can only process one instruction at a time.
As superscalar processors have become widespread, the accepted performance measure has shifted from CPI to its reciprocal—the average number of instructions executed per cycle, or IPC.
The most advanced designs use a technique known as out-of-order execution to execute multiple instructions in parallel, possibly in a totally different order than they occur in the program, while preserving the overall behavior implied by the sequential ISAmodel.
This form of execution is described in Chapter 5 as part of our discussion of program optimization.
The majority of processors sold are used in embedded systems, controlling automotive functions, consumer products, and other devices where the processor is not directly visible to the system user.
In these applications, the simplicity of a pipelined processor, such as the one we have explored in this chapter, reduces its cost and power requirements compared to higher-performance models.
More recently, as multicore processors have gained a following, some have argued that we could get more overall computing power by integrating many simple processors on a single chip rather than a smaller number of more complex ones.
This strategy is sometimes referred to as “many-core” processors [10]
We have seen that the instruction set architecture, or ISA, provides a layer of abstractionbetween thebehavior of a processor—in termsof the set of instructions and their encodings—and how the processor is implemented.
The ISA provides a very sequential view of program execution, with one instruction executed to completion before the next one begins.
The resulting ISA has attributes of both RISC and CISC instruction sets.
We then organized the processing required for the different instructions into a series of ﬁve stages, where the operations at each stage vary according to the instruction being executed.
From this, we constructed the SEQ processor, in which an entire instruction is executed every clock cycle by having it ﬂow through all ﬁve stages.
Pipelining improves the throughput performance of a system by letting the different stages operate concurrently.
At any given time, multiple operations are being processed by the different stages.
In introducing this concurrency, we must be careful to provide the same program-level behavior as would a sequential execution of the program.
We introduced pipelining by reordering parts of SEQ to get SEQ+, and then adding pipeline registers to create the PIPE– pipeline.
We enhanced the pipeline performance by adding forwarding logic to speed the sending of a result from one instruction to another.
Several special cases require additional pipeline control logic to stall or cancel some of the pipeline stages.
Our design included rudimentary mechanisms to handle exceptions, where we make sure that only instructions up to the excepting instruction affect the programmer-visible state.
Properly handling exceptions gets even more complex in systems that employ greater degrees of pipelining and parallelism.
In this chapter, we have learned several important lessons about processor design:
We want to make optimum use of the hardware resources to get maximum performance at minimum cost.
We did this by creating a very simple and uniform framework for processing all of the different instruction types.With this framework, we could share the hardware units among the logic for processing the different instruction types.
A direct implementation of the ISA would imply a very sequential design.
To achieve higher performance, we want to exploit the ability in hardware to perform many operations simultaneously.
By careful design and analysis, we can handle the various pipeline hazards, so that the overall effect of running a program exactly matches what would be obtained with the ISA model.
Once a chip has been fabricated, it is nearly impossible to correct any errors.
It is very important to get the design right on theﬁrst try.Thismeans carefully analyzingdifferent instruction types and combinations, even ones that do not seem to make sense, such as popping to the stack pointer.
Designs must be thoroughly tested with systematic simulation test programs.
In developing the control logic for PIPE, our design had a subtle bug that was uncovered only after a careful and systematic analysis of control combinations.
In this chapter, we have looked at portions of the HCL code for several simple logic designs, and for the control logic for Y86 processors SEQ and PIPE.
For reference, we provide documentation of the HCL language and complete HCL descriptions for the control logic of the two processors.
Each of these descriptions requires only 5–7 pages of HCL code, and it is worthwhile to study them in their entirety.
The lab materials for this chapter include simulators for the SEQ and PIPE processors.
TheGUI (graphic user interface) version displays thememory, program code, and processor state in graphicwindows.
This provides away to readily see how the instructions ﬂow through the processors.
The control panel also allows you to reset, single-step, or run the simulator interactively.
The text version runs the same simulator, but it only displays information by printing to the terminal.
This version is not as useful for debugging, but it allows automated testing of the processor.
The control logic for the simulators is generated by translating the HCL declarations of the logic blocks into C code.
This code is then compiled and linked with the rest of the simulation code.
This combination makes it possible for you to test out variants of the original designs using the simulators.
Testing scripts are also available that thoroughly exercise the different instructions and the different hazard possibilities.
For those interested in learning more about logic design, Katz’s logic design textbook [56] is a standard introductory text, emphasizing the use of hardware description languages.
Hennessy and Patterson’s computer architecture textbook [49] provides extensive coverage of processor design, including both simple pipelines, such as the one we have presented here, and more advanced processors that execute more instructions in parallel.
In light of analysis done in Problem 4.6, does this code sequence correctly describe the behavior of the instruction pushl %esp? Explain.
How could you rewrite the code sequence so that it correctly describes both the cases where REG is %esp as well as any other register?
In light of analysis done in Problem 4.7, does this code sequence correctly describe the behavior of the instruction popl %esp? Explain.
How could you rewrite the code sequence so that it correctly describes both the cases where REG is %esp as well as any other register?
Write and test a C version that references the array elements with pointers, rather than using array indexing.
Write and test a Y86 program consisting of the function and test code.
You may ﬁnd it useful to pattern your implementation after IA32 code generated by compiling your C code.
Although pointer comparisons are normally done using unsigned arithmetic, you can use signed arithmetic for this exercise.
This instruction adds the constant value V to register rB.
Use the computations for irmovl and OPl (Figure 4.18) as a guide.
Suppose we add this instruction to the Y86 instruction set, using the following encoding:
Use the computations for popl (Figure 4.20) as a guide.
The ﬁle pipe-stall.hcl contains a modiﬁed version of the HCL code for PIPE in which the bypassing logic has been disabled.
Modify the pipeline control logic at the end of this ﬁle so that it correctly handles all possible control and data hazards.
As part of your design effort, you should analyze the different combinations of control cases, as we did in the design of the pipeline control logic for PIPE.
You will ﬁnd that many different combinations can occur, since many more conditions require the pipeline to stall.
See the lab material for directions on how to generate a simulator for your solution and how to test it.
For cases where the second instruction stores the source operand to memory, such as with an rmmovl or pushl instruction, this stalling is not necessary.
Our design for PIPE would stall the pushl instruction to avoid a load/use hazard.
Observe, however, that the value of %edx is not required by the pushl instruction until it reaches the memory stage.
Figure 4.69 Execute and memory stages capable of load forwarding.
By adding a bypass path from the memory output to the source of valA in pipeline register M, we can use forwarding rather than stalling for one form of load/use hazard.
On the next clock cycle, this forwarded value can then be written to memory.
The value loaded by the popl instruction is used as part of the address computation by the next instruction, and this value is required in the execute stage rather than the memory stage.
Write a logic formula describing the detection condition for a load/use hazard, similar to the one given in Figure 4.64, except that it will not cause a stall in cases where load forwarding can be used.
The ﬁle pipe-lf.hcl contains a modiﬁed version of the control logic for PIPE.
It also has the conditions for a load/use hazard in the pipeline control logic set to zero, and so the pipeline control logic will not detect any forms of load/use hazards.
See the lab material for directions on how to generate a simulator for your solution and how to test it.
The logic for performing the merges is written in HCL as follows:
The control for these multiplexors is determined by dstE—when it indicates there is some register, then it selects the value for port E, and otherwise it selects the value for port M.
In the simulation model, we can then disable register port M, as shown by the following HCL code:
The challenge then becomes to devise a way to handle popl.
One method is to use the control logic to dynamically process the instruction popl rA so that it has the same effect as the two-instruction sequence.
Note the ordering of the two instructions to make sure popl %esp works properly.
You can do this by having the logic in the decode stage treat popl the same as it would the iaddl listed above, except that it predicts the next PC to be equal to the current PC.
On the next cycle, the popl instruction is refetched, but the instruction code is converted to a special value IPOP2
This is treated as a special instruction that has the same behavior as the mrmovl instruction listed above.
The ﬁle pipe-1w.hcl contains the modiﬁed write-port logic described above.
It contains a declaration of the constant IPOP2 having hexadecimal value E.
It also contains the deﬁnition of a signal f_icode that generates the icode ﬁeld for pipeline register D.
This deﬁnition can be modiﬁed to insert the instruction code IPOP2 the second time the popl instruction is fetched.
Modify the control logic in this ﬁle to process popl instructions in the manner we have described.
See the lab material for directions on how to generate a simulator for your solution and how to test it.
In the following output from our Y86 assembler, each line shows an address and a byte sequence that starts at that address:
It must read byte sequences and determine what instructions are to be executed.
In the following, we show the assembly code used to generate each of the byte sequences.
To the left of the assembly code, you can see the address and byte sequence for each instruction.
Code containing an invalid second byte in a pushl instruction:
We want to determine a reasonable convention for the instruction’s behavior and make sure each of our implementations adheres to this convention.
The subl instruction in this test compares the starting value of %esp to the value pushed onto the stack.
The fact that the result of this subtraction is zero implies that the old value of %esp gets pushed.
Still, we should decide on a convention and stick with it.
This code sequence pushes 0xabcd onto the stack, pops to %esp, and returns the popped value.
Since the result equals 0xabcd, we can deduce that popl %esp sets the stack pointer to the value read frommemory.
In general, the signals eq and xor will be complements of each other.
Using DeMorgan’s laws (Web Aside data:bool), we can implement And using Or and Not, yielding the following circuit:
We can see from the object code that this instruction is located at address 0x00e.
Since the one writing valM would occur last, the net effect of the instruction will be to write the value read from memory to %esp, just as we saw for IA32
We simply condition the write-back step on the outcome of the conditional test:
It provides a number of opportunities to compute throughputs and latencies in pipelines.
For a two-stage pipeline, the best partition would be to have blocks A, B, and C in the ﬁrst stage and D, E, and F in the second.
For a three-stage pipeline, we should have blocks A and B in the ﬁrst stage, blocks C and D in the second, and blocks E and F in the third.
For a four-stage pipeline, we should have block A in the ﬁrst stage, blocks B and C in the second, block D in the third, and blocks E and F in the fourth.
The optimal design would be a ﬁve-stage pipeline, with each block in its own stage, except that the ﬁfth stage has blocks E and F.
Adding more stages would not help, since we cannot run the pipeline any faster than one cycle every 100 ps.
Of course, this would give us an inﬁnite latency, as well.
As we try to subdivide the logic into many stages, the latency of the pipeline registers becomes a limiting factor.
If the two cases were reversed, then the write back from M_valE would take priority, causing the incremented stack pointer to be passed as the argument to the rrmovl instruction.
This would not be consistent with the convention for handling popl %esp determined in Practice Problem 4.7
In general, we should have test programs that will exercise all of the different hazard possibilities and will generate incorrect results if some dependency is not handled properly.
For this example, we can use a slightly modiﬁed version of the program shown in Practice Problem 4.30:
The two nop instructionswill cause the popl instruction to be in thewrite-back stage when the rrmovl instruction is in the decode stage.
If the two forwarding sources in the write-back stage are given the wrong priority, then register %eax will be set to the incremented program counter rather than the value read from memory.
The resulting value could get forwarded to the next instruction, even though the conditional transfer does not occur.
This program is designed so that if something goes wrong (for example, if the ret instruction is actually executed), then the program will execute one of the extra irmovl instructions and then halt.
Thus, an error in the pipeline would cause some register to be updated incorrectly.
This code illustrates the care required to implement a test program.
It must set up a potential error condition and then detect whether or not an error occurs.
The simulatorwill detect a casewhere thebubble and stall control signals for a pipeline register are both set to zero, and so our test program need only set up the combination for it to be detected.
The biggest challenge is to make the program do something sensible when handled correctly.
The ﬁrst word (mem) holds the address of the second (stack—the desired stack pointer)
The second word holds the address of the desired return point for the ret instruction.
The program loads the stack pointer into %esp and executes the ret instruction.
Start injecting bubbles as soon as exception passes through memory stage.
For stalling the write-back stage, we check only the status of the instruction in this stage.
If we also stalled when an excepting instruction was in the memory stage, then this instruction would not be able to enter the write-back stage.
As long as the array is sufﬁciently large, the time spent in other parts of the code will be negligible.
The inner loop of the code using the conditional move has 10 instructions, all of which are executed every time.
The loop-closing jump will be predicted correctly, except when the loop terminates.
For a very long array, this one misprediction will have negligible effect on the performance.
The only other source of bubbles for the jumpbased code is the conditional jump depending on whether or not the array element is positive.
Our pipeline has a branch misprediction penalty of only two cycles—far better than those for the deep pipelines of higher-performance processors.
As a result, using conditional moves does not affect program performance very much.
The biggest speedup you’ll ever get with a program will be.
The primary objective in writing a program must be to make it work correctly under all possible conditions.
A program that runs fast but gives incorrect results serves no useful purpose.
On the other hand, there are many occasions when making a program run fast is also an important consideration.
If a programmust process video frames or network packets in real time, then a slow-running program will not provide the needed functionality.
When a computation task is so demanding that it requires days or weeks to execute, then making it run just 20% faster can have signiﬁcant impact.
In this chapter, we will explore how to make programs run faster via several different types of program optimization.
First, we must select an appropriate set of algorithms and data structures.
Second, we must write source code that the compiler can effectively optimize to turn into efﬁcient executable code.
For this second part, it is important to understand the capabilities and limitations of optimizing compilers.
Seemingly minor changes in how a program is written can make large differences in how well a compiler can optimize it.
Some features of C, such as the ability to perform pointer arithmetic and casting, make it challenging for a compiler to optimize.
Programmers can often write their programs in ways that make it easier for compilers to generate efﬁcient code.
A third technique for dealing with especially demanding computations is to divide a task into portions that can be computed in parallel, on some combination of multiple cores and multiple processors.
Evenwhen exploiting parallelism, it is important that each parallel thread execute with maximum performance, and so the material of this chapter remains relevant in any case.
In approaching program development and optimization, we must consider how the code will be used and what critical factors affect it.
In general, programmers must make a trade-off between how easy a program is to implement and maintain, and how fast it runs.
At an algorithmic level, a simple insertion sort can be programmed in a matter of minutes, whereas a highly efﬁcient sort routine may take a day or more to implement and optimize.
At the coding level, many low-level optimizations tend to reduce code readability and modularity, making the programs more susceptible to bugs and more difﬁcult to modify or extend.
One challenge is to maintain some degree of elegance and readability in the code despite extensive transformations.
We describe a number of techniques for improving code performance.
Ideally, a compiler would be able to take whatever code we write and generate the most.
Modern compilers employ sophisticated forms of analysis and optimization, and they keep getting better.
Even the best compilers, however, can be thwarted by optimization blockers—aspects of the program’s behavior that depend strongly on the execution environment.
Programmers must assist the compiler by writing code that can be optimized readily.
The ﬁrst step in optimizing a program is to eliminate unnecessary work, making the code perform its intended task as efﬁciently as possible.
This includes eliminating unnecessary function calls, conditional tests, and memory references.
These optimizations do not depend on any speciﬁc properties of the target machine.
To maximize the performance of a program, both the programmer and the compiler require a model of the target machine, specifying how instructions are processed and the timing characteristics of the different operations.
For example, the compiler must know timing information to be able to decide whether it should use a multiply instruction or some combination of shifts and adds.
Modern computers use sophisticated techniques to process amachine-level program, executing many instructions in parallel and possibly in a different order than they appear in the program.
Programmers must understand how these processors work to be able to tune their programs for maximum speed.
We present a high-level model of such a machine based on recent designs of Intel and AMD processors.
We also devise a graphical data-ﬂow notation to visualize the execution of instructions by the processor, with which we can predict program performance.
With this understanding of processor operation, we can take a second step in program optimization, exploiting the capability of processors to provide instruction-level parallelism, executing multiple instructions simultaneously.
We cover several program transformations that reduce the data dependencies between different parts of a computation, increasing the degree of parallelism with which they can be executed.
We conclude the chapter by discussing issues related to optimizing large programs.We describe the use of code proﬁlers—tools that measure the performance of different parts of a program.
This analysis can help ﬁnd inefﬁciencies in the code and identify the parts of the program on which we should focus our optimization efforts.
Finally, we present an important observation, known as Amdahl’s law, which quantiﬁes the overall effect of optimizing some portion of a system.
In this presentation, we make code optimization look like a simple linear process of applying a series of transformations to the code in a particular order.
This is especially true as we approach the later optimization stages, where seemingly small changes can cause major changes in performance, while some very promising techniques prove ineffective.
As we will see in the examples that follow, it can be difﬁcult to explain exactly why a particular code sequence has a particular execution time.
Performance can depend on many detailed features of the processor design for which we have relatively little documentation or understanding.
This is another reason to try a number of different variations and combinations of techniques.
Studying the assembly-code representation of a program is one of the most effective means for gaining an understanding of the compiler and how the generated code will run.
Starting with the assembly code, we can also predict what operations will be performed in parallel and how well they will use the processor resources.
As we will see, we can often determine the time (or at least a lower bound on the time) required to execute a loop by identifying critical paths, chains of data dependencies that form during repeated executions of a loop.
We can then go back and modify the source code to try to steer the compiler toward more efﬁcient implementations.
Most major compilers, including gcc, are continually being updated and improved, especially in terms of their optimization abilities.
One useful strategy is to do only as much rewriting of a program as is required to get it to the point where the compiler can then generate efﬁcient code.
By this means, we avoid compromising the readability, modularity, and portability of the code asmuch as if we had to work with a compiler of only minimal capabilities.
Again, it helps to iteratively modify the code and analyze its performance both through measurements and by examining the generated assembly code.
To novice programmers, it might seem strange to keep modifying the source code in an attempt to coax the compiler into generating efﬁcient code, but this is indeed how many high-performance programs are written.
Compared to the alternative of writing code in assembly language, this indirect approach has the advantage that the resulting code will still run on other machines, although perhaps not with peak performance.
Modern compilers employ sophisticated algorithms to determine what values are computed in a program and how they are used.
They can then exploit opportunities to simplify expressions, to use a single computation in several different places, and to reduce the number of times a given computation must be performed.
Most compilers, including gcc, provide users with some control over which optimizations they apply.
As discussed in Chapter 3, the simplest control is to specify the optimization level.
For example, invoking gcc with the command-line ﬂag ‘-O1’ will cause it to apply a basic set of optimizations.
These can further improve program performance, but theymay expand the program size and they may make the program more difﬁcult to debug using standard debugging tools.
We purposely limit the level of optimization to demonstrate how different ways of writing a function in C can affect the efﬁciency of the code generated by a compiler.
We will ﬁnd that we can write C code that, when compiled just with optimization level 1, vastly outperforms a more naive version compiled with the highest possible optimization levels.
Compilers must be careful to apply only safe optimizations to a program, meaning that the resulting program will have the exact same behavior as would an unoptimized version for all possible cases the program may encounter, up to the limits of the guarantees provided by the C language standards.
Constraining the compiler to perform only safe optimizations eliminates possible sources of undesired run-time behavior, but it also means that the programmer must make more of an effort to write programs in a way that the compiler can then transform into efﬁcient machine-level code.
To appreciate the challenges of deciding which program transformations are safe or not, consider the following two procedures:
At ﬁrst glance, both procedures seem to have identical behavior.
They both add twice the value stored at the location designated by pointer yp to that designated by pointer xp.
Consider, however, the case in which xp and yp are equal.
On the other hand, function twiddle2 will perform the following computation:
The case where two pointers may designate the same memory location is known as memory aliasing.
As another example, for a program with pointer variables p and q, consider the following code sequence:
This leads to one of the major optimization blockers, aspects of programs that can severely limit the opportunities for a compiler to generate optimized code.
If a compiler cannot determine whether or not two pointers may be aliased, it must assume that either case is possible, limiting the set of possible optimizations.
Practice Problem 5.1 The following problem illustrates the way memory aliasing can cause unexpected program behavior.
Swap value x at xp with value y at yp */
If this procedure is called with xp equal to yp, what effect will it have?
This function has a side effect—it modiﬁes some part of the global program state.
Changing the number of times it gets called changes the program behavior.
Most compilers do not try to determine whether a function is free of side effects and hence is a candidate for optimizations such as those attempted in func2
Instead, the compiler assumes the worst case and leaves function calls intact.
As described inWebAside asm:opt, code involving function calls can be optimized by a process known as inline substitution (or simply “inlining”), where the function call is replaced by the code for the body of the function.
For example, we can expand the code for func1 by substituting four instantiations of function f:
This transformation both reduces the overhead of the function calls and allows further optimization of the expanded code.
For example, the compiler can consolidate the updates of global variable counter in func1in to generate an optimized version of the function:
This code faithfully reproduces the behavior of func1 for this particular deﬁnition of function f.
Recent versions of gcc attempt this form of optimization, either when directed to with the.
Since we are considering optimization level 1 in our presentation, we will assume that the compiler does not perform inline substitution.
Among compilers, gcc is considered adequate, but not exceptional, in terms of its optimization capabilities.
It performs basic optimizations, but it does not perform the radical transformations on programs that more “aggressive” compilers do.
As a consequence, programmers using gcc must put more effort into writing programs in a way that simpliﬁes the compiler’s task of generating efﬁcient code.
We introduce the metric cycles per element, abbreviated “CPE,” as a way to express program performance in a way that can guide us in improving the code.
It is appropriate for programs that performa repetitive computation, such as processing the pixels in an image or computing the elements in a matrix product.
Function psum1 computes one element of the result vector per iteration.
The second uses a technique known as loop unrolling to compute two elements per iteration.
We will explore the beneﬁts of loop unrolling later in this chapter.
The time required by such a procedure can be characterized as a constant plus a factor proportional to thenumberof elements processed.
For example, Figure 5.2 shows a plot of the number of clock cycles required by the two functions for a range of values of n.
The slope of the lines indicates the number of clock cycles per element (CPE)
For large values of n (say, greater than 200), the run times will be dominated by the linear factors.We refer to the coefﬁcients in these terms as the effective number of cycles per element, abbreviated “CPE.”We prefer measuring the number of cycles per element rather than thenumber of cycles per iteration, because techniques such as loop unrolling allow us to use fewer iterations to complete the computation, but our ultimate concern is how fast the procedure will run for a given vector length.
We focus our efforts on minimizing the CPE for our computations.
With a least squares ﬁt, we look for a line of the form y = mx + b that minimizes the following error measure:
Practice Problem 5.2 Later in this chapter, we will start with a single function and generate many different variants that preserve the function’s behavior, but with different performance characteristics.
For three of these variants, we found that the run times (in clock cycles) can be approximated by the following functions:
For what values of n would each version be the fastest of the three? Remember that n will always be an integer.
To demonstrate how an abstract program can be systematically transformed into more efﬁcient code, we will use a running example based on the vector data structure shown in Figure 5.3
A vector is represented with two blocks of memory: the header and the data array.
A vector is represented by header information plus array of designated length.
The declaration uses data type data_t to designate the data type of the underlying elements.
In our evaluation, wemeasure the performance of our code for integer (C int), single-precision ﬂoating-point (C float), and double-precision ﬂoating-point (C double) data.
We do this by compiling and running the program separately for different type declarations, such as the following for data type int:
We allocate the data array block to store the vector elements as an array of len objects of type data_t.
Figure 5.4 shows some basic procedures for generating vectors, accessing vector elements, and determining the length of a vector.
An important feature to note is that get_vec_element, the vector access routine, performs bounds checking for every vector reference.
This code is similar to the array representations used in many other languages, including Java.
Bounds checking reduces the chances of program error, but it can also slow down program execution.
As an optimization example, consider the code shown in Figure 5.5, which combines all of the elements in a vector into a single value according to some operation.
By using different deﬁnitions of compile-time constants IDENT and OP, the code can be recompiled to perform different operations on the data.
In our presentation, we will proceed through a series of transformations of.
In the actual program, data type data_t is declared to be int, float, or double.
Using different declarations of identity element IDENT and combining operation OP, we can measure the routine for different operations.
Some characteristics of this processor were given in Section 3.1
These measurements characterize performance in terms of how the programs run on just one particular machine, and so there is no guarantee of comparable performance on other combinations of machine and compiler.
However, we have compared the results with those for a number of different compiler/processor combinations and found them quite comparable.
As we proceed through a set of transformations, we will ﬁnd that many lead to only minimal performance gains, while others have more dramatic effects.
Determining which combinations of transformations to apply is indeed part of the “black art” of writing fast code.
Some combinations that do not provide measurable beneﬁts are indeed ineffective, while others are important as ways to enable further optimizations by the compiler.
In our experience, the best approach involves a combination of experimentation and analysis: repeatedly attempting different approaches, performing measurements, and examining the assemblycode representations to identify underlying performance bottlenecks.
As a starting point, the following are CPE measurements for combine1 running on our referencemachine, trying all combinations of data type and combining operation.
For single-precision and double-precision ﬂoating-point data, our experiments on this machine gave identical performance for addition, but differing performance for multiplication.
Rather than “fudging” our numbers to make them look good, we will present the measurements we actually obtained.
There are many factors that complicate the task of reliably measuring the precise number of clock cycles required by some code sequence.
It helps when examining these numbers to mentally round the results up or down by a few hundredths of a clock cycle.
The unoptimized code provides a direct translation of theC code intomachine code, often with obvious inefﬁciencies.
By simply giving the command-line option ‘-O1’, we enable a basic set of optimizations.
As can be seen, this signiﬁcantly improves the program performance—more than a factor of two—with no effort on behalf of the programmer.
In general, it is good to get into the habit of enabling at least this level of optimization.
For the remainder of our measurements, we use optimization levels 1 and higher in generating and measuring our programs.
Recall from our discussion of how to translate code containing loops into machine-level programs (Section 3.6.5) that the test condition must be evaluated on every iteration of the loop.
On the other hand, the length of the vector does not change as the loop proceeds.
We could therefore compute the vector length only once and use this value in our test condition.
This transformation has noticeable effect on the overall performance for some data types and.
By moving the call to vec_ length out of the loop test, we eliminate the need to execute it on every iteration.
In any case, this transformation is required to eliminate inefﬁciencies that would become bottlenecks as we attempt further optimizations.
This optimization is an instance of a general class of optimizations known as code motion.
They involve identifying a computation that is performed multiple times (e.g., within a loop), but such that the result of the computation will not change.
We can therefore move the computation to an earlier section of the code that does not get evaluated as often.
In this case, wemoved the call to vec_length from within the loop to just before the loop.
Unfortunately, as discussed previously, they are typically very cautious about making transformations that change where or how many times a procedure is called.
They cannot reliably detect whether or not a function will have side effects, and so they assume that it might.
To improve the code, the programmer must often help the compiler by explicitly performing code motion.
This procedure is styled after routines submitted by several students as part of a network programming project.
Its purpose is to convert all of the uppercase letters in a string to lowercase.
The procedure steps through the string, converting each uppercase character to lowercase.
The library function strlen is called as part of the loop test of lower1
Since strings in C are null-terminated character sequences, strlen can only determine the length of a string by stepping through the sequence until it hits a null character.
For a string of length n, strlen takes time proportional to n.
This analysis is conﬁrmed by actual measurements of the functions for different length strings, as shown in Figure 5.8 (and using the library version of strlen)
Observe that for lower1 each doubling of the string length causes a quadrupling of the run time.
This is a clear indicator of a quadratic run time.
Each doubling of the string length causes a doubling of the run time—a clear indicator of linear run time.
For longer strings, the run-time improvement will be even greater.
In an ideal world, a compiler would recognize that each call to strlen in the loop test will return the same result, and thus the call could be moved out of the loop.
This would require a very sophisticated analysis, since strlen checks.
The original code lower1 has a quadratic run time due to an inefﬁcient loop structure.
The compilerwouldneed todetect that even though the characterswithin the string are changing, none are being set from nonzero to zero, or vice versa.
Such an analysis is well beyond the ability of even the most sophisticated compilers, even if they employ inlining, and so programmers must do such transformations themselves.
This example illustrates a common problem in writing programs, in which a seemingly trivial piece of code has a hidden asymptotic inefﬁciency.
One would not expect a lowercase conversion routine to be a limiting factor in a program’s performance.
Typically, programs are tested and analyzed on small data sets, for which the performance of lower1 is adequate.
When the program is ultimately deployed, however, it is entirely possible that the procedure could be applied to strings of over one million characters.
All of a sudden this benign piece of code has become a major performance bottleneck.
By contrast, the performance of lower2 will be adequate for strings of arbitrary length.
Stories abound of major programming projects in which problems of this sort occur.
Part of the job of a competent programmer is to avoid ever introducing such asymptotic inefﬁciency.
Fill in the following table indicating the number of times each of the four functions is called in code fragments A–C:
This function checks the vector index i against the loop bounds with every vector reference, a clear source of inefﬁciency.
Bounds checkingmight be a useful feature when dealing with arbitrary array accesses, but a simple analysis of the code for combine2 shows that all references will be valid.
Suppose instead that we add a function get_vec_start to our abstract data type.
This function returns the starting address of the data array, as shown in Figure 5.9
We could then write the procedure shown as combine3 in this ﬁgure, having no function calls in the inner loop.
Rather than making a function call to retrieve each vector element, it accesses the array directly.
A purist might say that this transformation seriously impairs the program modularity.
In principle, the user of the vector abstract data type should not even need to know that the vector.
The resulting code runs much faster, at some cost in program modularity.
A more pragmatic programmer would argue that this transformation is a necessary step toward achieving high-performance results.
The resulting improvement is surprisingly modest, only improving the performance for integer sum.
Again, however, this inefﬁciency would become a bottleneck as we attempt further optimizations.
For applications in which performance is a signiﬁcant issue, one must often compromise modularity and abstraction for speed.
It is wise to include documentation on the transformations applied, as well as the assumptions that led to them, in case the code needs to be modiﬁed later.
The code for combine3 accumulates the value being computed by the combining operation at the location designated by the pointer dest.
This attribute can be seen by examining the assembly code generated for the compiled loop.
Here, we brieﬂy review the relevant aspects of x86-64 and its ﬂoating-point instructions.
Eight more registers are available, named %r8–%r15, greatly improving the ability to hold temporary values in registers.
Floating-point data are held in a set of XMM registers, named %xmm0–%xmm15
Each of these registers is 128 bits long, able to hold four single-precision (float) or two double-precision (double) ﬂoating-point numbers.
For our initial presentation, we will only make use of instructions that operate on single values held in SSE registers.
Like the various mov instructions of IA32, both the source and the destination can be memory locations or registers, but it uses XMM registers, rather than general-purpose registers.
The mulss instruction multiplies single-precision numbers, updating its second operand with the product.
Again, the source and destination operands can be memory locations or XMM registers.
On iteration i, the program reads the value at this location, multiplies it by data[i], and stores the result back at dest.
This reading and writing is wasteful, since the value read from dest at the beginning of each iteration should simply be the value written at the end of the previous iteration.
We introduce a temporary variable acc that is used in the loop to accumulate the computed value.
The result is stored at dest only after the loop has been completed.
As the assembly code that follows shows, the compiler can now use register %xmm0 to hold the accumulated value.
Holding the accumulated value in local variable acc (short for “accumulator”) eliminates the need to retrieve it from memory and write back the updated value on every loop iteration.
Compared to the loop in combine3, we have reduced the memory operations per iteration from two reads and one write to just a single read.
We see a signiﬁcant improvement in program performance, as shown in the following table:
In fact, however, the two functions can have different behaviors due to memory aliasing.
Consider, for example, the case of integer data with multiplication as the operation and 1 as the identity element.
That is, we create an alias between the last element of the vector and the destination for storing the result.
As shown previously, combine3 accumulates its result at the destination, which in this case is the ﬁnal vector element.
One could argue that the behavior of combine4 more closely matches the intention of the function description.
Unfortunately, a compiler cannot make a judgment about the conditions under which a function might be used and what the programmer’s intentions might be.
Instead, when given combine3 to compile, the conservative approach is to keep reading and writing memory, even though this is less efﬁcient.
We achieve performance comparable to that for combine4, except for the case of integer sum, but even it improves signiﬁcantly.On examining the assembly code generated by the compiler, we ﬁnd an interesting variant for the inner loop:
We can compare this to the version created with optimization level 1:
We see that, besides some reordering of instructions, the only difference is that the more optimized version does not contain the movss implementing the read from the location designated by dest (line 2)
How does the role of register %xmm0 differ in these two loops?
Will the more optimized version faithfully implement the C code of combine3, includingwhen there ismemory aliasing between dest and the vector data?
Explain either why this optimization preserves the desired behavior, or give an example where it would produce different results than the less optimized code.
With this ﬁnal transformation, we reached a point where we require just 2–5 clock cycles for each element to be computed.
This is a considerable improvement over the original 11–13 cycles when we ﬁrst enabled optimization.
We would now like to see just what factors are constraining the performance of our code and how we can improve things even further.
Up to this point, we have applied optimizations that did not rely on any features of the target machine.
They simply reduced the overhead of procedure calls and eliminated some of the critical “optimization blockers” that cause difﬁculties for optimizing compilers.
As we seek to push the performance further, we must consider optimizations that exploit the microarchitecture of the processor, that is, the underlying system design by which a processor executes instructions.
Getting every last bit of performance requires a detailed analysis of the program as well as code generation tuned for the target processor.
Nonetheless, we can apply some basic optimizations that will yield an overall performance improvement on a large class of processors.
The detailed performance results we report here may not hold for othermachines, but the general principles of operation and optimization apply to a wide variety of machines.
To understand ways to improve performance, we require a basic understanding of the microarchitectures of modern processors.
Due to the large number of transistors that can be integrated onto a single chip, modern microprocessors employ complex hardware that attempts to maximize program performance.
One result is that their actual operation is far different from the view that is perceived by looking at machine-level programs.
At the code level, it appears as if instructions are executed one at a time, where each instruction involves fetching values from registers or memory, performing an operation, and storing results back to a register or memory location.
In the actual processor, a number of instructions are evaluated simultaneously, a phenomenon referred to as instruction-level parallelism.
In some designs, there can be 100 ormore instructions “in ﬂight.” Elaborate mechanisms are employed to make sure the behavior of this parallel execution exactly captures the sequential semantic model required by the machine-level program.
Although the detailed design of a modern microprocessor is well beyond the scope of this book, having a general idea of the principles by which they operate sufﬁces to understand how they achieve instruction-level parallelism.
We will ﬁnd that two different lower bounds characterize the maximum performance of a program.
The latency bound is encountered when a series of operations must be performed in strict sequence, because the result of one operation is requiredbefore thenext one canbegin.This bound can limit programperformance when the data dependencies in the code limit the ability of the processor to.
The instruction control unit is responsible for reading instructions from memory and generating a sequence of primitive operations.
The execution unit then performs the operations and indicates whether the branches were correctly predicted.
The throughput bound characterizes the raw computing capacity of the processor’s functional units.
Figure 5.11 shows a very simpliﬁed view of a modern microprocessor.
It is described in the industry as being superscalar, which means it can performmultiple operations on every clock cycle, and out-of-order, meaning that the order in which instructions execute need not correspond to their ordering in themachine-level program.
The overall design has two main parts: the instruction control unit (ICU), which is responsible for reading a sequence of instructions from memory and generating from these a set of primitive operations to perform on program data, and the execution unit (EU), which then executes these operations.
Compared to the simple in-order pipeline we studied in Chapter 4, out-of-order processors require far greater and more.
The ICU reads the instructions from an instruction cache—a special highspeed memory containing the most recently accessed instructions.
In general, the ICU fetches well ahead of the currently executing instructions, so that it has enough time to decode these and send operations down to the EU.
One problem, however, is that when a program hits a branch,1 there are two possible directions the programmight go.
The branch can be taken, with control passing to the branch target.
Alternatively, the branch can be not taken, with control passing to the next instruction in the instruction sequence.
Modern processors employ a technique known as branch prediction, in which they guess whether or not a branch will be taken and also predict the target address for the branch.
Using a technique known as speculative execution, the processor begins fetching and decoding instructions at where it predicts the branch will go, and even begins executing these operations before it has been determined whether or not the branch prediction was correct.
If it later determines that the branch was predicted incorrectly, it resets the state to that at the branch point and begins fetching and executing instructions in the other direction.
The block labeled “Fetch control” incorporates branch prediction to perform the task of determining which instructions to fetch.
The instruction decoding logic takes the actual program instructions and converts them into a set of primitive operations (sometimes referred to as microoperations)
Each of these operations performs some simple computational task such as adding two numbers, reading data from memory, or writing data to memory.
Formachines with complex instructions, such as x86 processors, an instruction can be decoded into a variable number of operations.
The details of how instructions are decoded into sequences of more primitive operations varies between machines, and this information is considered highly proprietary.
Fortunately, we can optimize our programs without knowing the low-level details of a particular machine implementation.
In a typical x86 implementation, an instruction that only operates on registers, such as.
On the other hand, an instruction involving one or more memory references, such as.
This particular instruction would be decoded as three operations: one to load a value frommemory into the processor, one to add the loaded value to the.
We use the term “branch” speciﬁcally to refer to conditional jump instructions.
Other instructions that can transfer control to multiple destinations, such as procedure return and indirect jumps, provide similar challenges for the processor.
This decoding splits instructions to allow a division of labor among a set of dedicated hardware units.
These units can then execute the different parts of multiple instructions in parallel.
Typically, it can receive a number of them on each clock cycle.
These operations are dispatched to a set of functional units that perform the actual operations.
We can see that three functional units are dedicated to computation, while the remaining two are for reading (load) and writing (store) memory.
Each computational unit can perform multiple different operations: all can perform at least basic integer operations, such as addition and bit-wise logical operations.
Floating-point operations and integer multiplication require more complex hardware, and so these can only be handled by speciﬁc functional units.
Reading and writing memory is implemented by the load and store units.
The load unit handles operations that read data from the memory into the processor.
Similarly, the store unit handles operations that write data from the processor to the memory.
As shown in the ﬁgure, the load and store units access memory via a data cache, a high-speed memory containing the most recently accessed data values.
With speculative execution, the operations are evaluated, but the ﬁnal results are not stored in the program registers or data memory until the processor can be certain that these instructions should actually have been executed.
Branch operations are sent to the EU, not to determine where the branch should go, but rather to determinewhether or not they were predicted correctly.
If the prediction was incorrect, the EUwill discard the results that have been computed beyond the branch point.
It will also signal the branch unit that the prediction was incorrect and indicate the correct branch destination.
In this case, the branch unit begins fetching at the new location.Aswe saw in Section 3.6.6, such amisprediction incurs a signiﬁcant cost in performance.
It takes a while before the new instructions can be fetched, decoded, and sent to the execution units.
Within the ICU, the retirement unit keeps track of the ongoing processing and makes sure that it obeys the sequential semantics of the machine-level program.
Our ﬁgure shows a register ﬁle containing the integer, ﬂoating-point, and more recently SSE registers as part of the retirement unit, because this unit controls the updating of these registers.
As an instruction is decoded, information about it is placed into a ﬁrst-in, ﬁrst-out queue.
This information remains in the queue until one of two outcomes occurs.
First, once the operations for the instruction have completed and any branch points leading to this instruction are conﬁrmed as having been correctly predicted, the instruction can be retired, with any updates to the program registers being made.
If some branch point leading to this instruction was mispredicted, on the other hand, the instruction will be ﬂushed, discarding any results that may have been computed.
By this means, mispredictions will not alter the program state.
As we have described, any updates to the program registers occur only as instructions are being retired, and this takes place only after the processor can be certain that any branches leading to this instruction have been correctly predicted.
To expedite the communication of results from one instruction to another, much of this information is exchanged among the execution units, shown in the ﬁgure as “Operation results.”As the arrows in the ﬁgure show, the execution units can send results directly to each other.
This is a more elaborate form of the data forwarding techniques we incorporated into our simple processor design in Section 4.5.7
Themost commonmechanism for controlling the communication of operands among the execution units is called register renaming.When an instruction that updates register r is decoded, a tag t is generated giving a unique identiﬁer to the result of the operation.
An entry (r, t) is added to a tablemaintaining the association between program register r and tag t for an operation that will update this register.
When a subsequent instruction using register r as an operand is decoded, the operation sent to the execution unit will contain t as the source for the operand value.
When some execution unit completes the ﬁrst operation, it generates a result (v, t) indicating that the operation with tag t produced value v.
Any operation waiting for t as a source will then use v as the source value, a form of data forwarding.
By this mechanism, values can be forwarded directly from one operation to another, rather than being written to and read from the register ﬁle, enabling the second operation to begin as soon as the ﬁrst has completed.
The renaming table only contains entries for registers having pending write operations.
When a decoded instruction requires a register r , and there is no tag associated with this register, the operand is retrieved directly from the register ﬁle.
With register renaming, an entire sequence of operations can be performed speculatively, even though the registers are updated only after the processor is certain of the branch outcomes.
Instructions were processed by ten different functional units, each of which could be operated independently.
In its day, this machine, with a clock rate of 10 Mhz, was considered the premium machine for scientiﬁc computing.
Latency indicates the total number of clock cycles required to perform the actual operations, while issue time indicates the minimum number of cycles between two operations.
Each operation is characterized by its latency, meaning the total time required to perform the operation, and the issue time, meaning theminimumnumber of clock cycles between two successive operations of the same type.
We see that the latencies increase as the word sizes increase (e.g., from single to double precision), for more complex data types (e.g., from integer to ﬂoating point), and for more complex operations (e.g., from addition to multiplication)
We see also that most forms of addition and multiplication operations have issue times of 1, meaning that on each clock cycle, the processor can start a new one of these operations.
This short issue time is achieved through the use of pipelining.
A pipelined function unit is implemented as a series of stages, each of which performs part of the operation.
For example, a typical ﬂoating-point adder contains three stages (and hence the three-cycle latency): one to process the exponent values, one to add the fractions, and one to round the result.
The arithmetic operations can proceed through the stages in close succession rather than waiting for one operation to complete before the next begins.
This capability can be exploited only if there are successive, logically independent operations to be performed.
Functional units with issue times of 1 cycle are said to be fully pipelined: they can start a new operation every clock cycle.
The issue time of 0.33 given for integer addition is due to the fact that the hardware has three fully pipelined functional units capable of performing integer addition.
The processor has the potential to perform three additions every clock cycle.
We see also that the divider (used for integer and ﬂoating-point division, as well as ﬂoating-point square root) is not fully pipelined—its issue time is just a few cycles less than its latency.
What this means is that the divider must complete all but the last few steps of a division before it can begin a newone.We also see the latencies and issue times for division are given as ranges, because some combinations of dividend and divisor requiremore steps than others.
The long latency and issue times of division make it a comparatively costly operation.
A more common way of expressing issue time is to specify the maximum throughput of the unit, deﬁned as the reciprocal of the issue time.A fully pipelined functional unit has a maximum throughput of one operation per clock cycle, while units with higher issue times have lower maximum throughput.
Circuit designers can create functional units with wide ranges of performance characteristics.
Creating a unit with short latency or with pipelining requires more hardware, especially for more complex functions such as multiplication and ﬂoating-point operations.
Since there is only a limited amount of space for these units on the microprocessor chip, CPU designers must carefully balance the number of functional units and their individual performance to achieve optimal overall performance.
They evaluate many different benchmark programs and dedicate the most resources to the most critical operations.
On the other hand, division is relatively infrequent and difﬁcult to implement with either short latency or full pipelining.
Both the latencies and the issue times (or equivalently, themaximum throughput) of these arithmetic operations can affect the performance of our combining functions.
We can express these effects in terms of two fundamental bounds on the CPE values:
The latency bound gives a minimum value for the CPE for any function that must perform the combining operation in a strict sequence.
The throughput bound gives a minimum bound for the CPE based on the maximum rate at which the functional units can produce results.
Unfortunately, the need to read elements from memory creates an additional throughput bound for the CPE of 1.00 for the combining functions.
We will demonstrate the effect of both of the latency and throughput bounds with different versions of the combining functions.
Asa tool for analyzing the performance of amachine-level programexecuting on a modern processor, we will use a data-ﬂow representation of programs, a graphical notation showing how the data dependencies between the different operations constrain the order in which they are executed.
These constraints then lead to critical paths in the graph, putting a lower bound on the number of clock cycles required to execute a set of machine instructions.
Before proceeding with the technical details, it is instructive to examine the CPE measurements obtained for function combine4, our fastest code up to this point:
We can see that these measurements match the latency bound for the processor, except for the case of integer addition.
This is not a coincidence—it indicates that the performance of these functions is dictated by the latency of the sum or product computation being performed.
Computing the product or sum of n elements requires around L.
The CPE is therefore equal to the latency bound L.
We consider the case of ﬂoating-point data with multiplication as the combining operation, although other combinations of data type and operation have nearly identical structure.
The compiled code for this loop consists of four instructions, with registers %rdx holding loop index i, %rax holding array address data, %rcx holding loop bound limit, and %xmm0 holding accumulator value acc.
As Figure 5.13 indicates, with our hypothetical processor design, the four instructions are expanded by the instruction decoder into a series of ﬁve operations, with the initial multiplication instruction being expanded into a load operation to read the source operand from memory, and a mul operation to perform the multiplication.
Instructions are dynamically translated into one or two operations, each of which receives values from other operations or from registers and produces values for other operations and for registers.
We show the target of the ﬁnal instruction as the label loop.
As a step toward generating a data-ﬂow graph representation of the program, the boxes and lines along the left-hand side of Figure 5.13 show how the registers are used and updated by the different operations, with the boxes along the top representing the register values at the beginning of the loop, and those along the bottom representing the values at the end.
For example, register %rax is only used as a source valueby the loadoperation inperforming its address calculation, and so the register has the same value at the end of the loop as at the beginning.
Similarly, register %rcx is only used by the cmp operation.
Register %rdx, on the other hand, is both used and updated within the loop.
Its initial value is used by the load and add operations; its new value is generated by the add operation, which is then used by the cmp operation.
Register %xmm0 is also updated within the loop by the mul operation, which ﬁrst uses the initial value as a source value.
Some of the operations in Figure 5.13 produce values that do not correspond to registers.
We show these as arcs between operations on the right-hand side.
The load operation reads a value from memory and passes it directly to the mul operation.
Since these two operations arise from decoding a single mulss instruction, there is no register associated with the intermediate value passing between them.
The cmp operation updates the condition codes, and these are then tested by the jg operation.
For a code segment forming a loop, we can classify the registers that are accessed into four categories:
Read-only: These are used as source values, either as data or to compute memory addresses, but they are not modiﬁed within the loop.
The readonly registers for the loop combine4 are %rax and %rcx.
Write-only: These are used as the destinations of data-movement operations.
Local: These are updated and used within the loop, but there is no dependency from one iteration to another.
Loop: These are both used as source values and as destinations for the loop, with the value generated in one iteration being used in another.
We see in Figure 5.14(a) that we rearranged the operators to show more clearly the ﬂow of data from the source registers at the top (both read-only and loop registers), and to the destination registers at the bottom (both write-only and loop registers)
In Figure 5.14(a), we also color operators white if they are not part of some chain of dependencies between loop registers.
The purpose of the compare and branch operations is to test the branch condition and notify the ICU if it is not.
We assume this checking can be done quickly enough that it does not slow down the processor.
In Figure 5.14(b), we have eliminated the operators that were colored white on the left, and we have retained only the loop registers.
What we have left is an abstract template showing the data dependencies that form among loop registers due to one iteration of the loop.
We can see in this diagram that there are two data dependencies from one iteration to the next.
Along one side, we see the dependencies between successive values of program value acc, stored in register %xmm0
The loop computes a new value for acc by multiplying the old value by.
The sequence of multiplication operations forms a critical path that limits program performance.
Along the other side, we see the dependencies between successive values of loop index i.
On each iteration, the old value is used to compute the address for the load operation, and it is also incremented by the add operation to compute the new value.
We can see that this graph was obtained by simply replicating the template shown on the right-hand side of Figure 5.14 n times.
We can see that the program has two chains of data dependencies, corresponding to the updating of program values acc and i with operations mul and add, respectively.
The chain on the left would require only n cycles to execute, and so it does not limit the program performance.
When executing the function, the ﬂoating-point multiplier becomes the limiting resource.
As each successive value of acc is computed, it is fed back around to compute the next value, but this will not be completed until four cycles later.
The ﬂow for other combinations of data type and operation are identical to those shown inFigure 5.15, butwith a different data operation forming the chain of data dependencies shown on the left.
This illustrates the principle that the critical paths in a dataﬂow representation provide only a lower bound on how many cycles a program will require.
Other factors can also limit performance, including the total number of functional units available and the number of data values that can be passed among the functional units on any given step.
For the case of integer addition as the combining operation, the data operation is sufﬁciently fast that the rest of the operations cannot supply data fast enough.
Determining exactly why the program requires 2.00 cycles per element would require a much more detailed knowledge of the hardware design than is publicly available.
It may seem that the latency bound forms a fundamental limit on how fast our combining operations can be performed.
Our next task will be to restructure the operations to enhance instruction-level parallelism.We want to transform the program in such a way that our only limitation becomes the throughput bound, yielding CPEs close to 1.00
For a value x, we evaluate the polynomial by computing.
In this function, we compute both the successive terms of the equation and the successive powers of x within a single loop:
For degree n, how many additions and how many multiplications does this code perform?
Explain how this CPE arises based on the data dependencies formed between iterations due to the operations implementing lines 7–8 of the function.
We can reduce the number of multiplications in evaluating a polynomial by applying Horner’s method, named after British mathematician William G.
The idea is to repeatedly factor out the powers of x to get the following evaluation:
For degree n, how many additions and how many multiplications does this code perform?
Explain how this CPE arises based on the data dependencies formed between iterations due to the operations implementing line 7 of the function.
Explain how the function shown in Problem 5.5 can run faster, even though it requires more operations.
Loop unrolling is a program transformation that reduces the number of iterations for a loop by increasing the number of elements computed on each iteration.
First, it reduces the number of operations that do not contribute directly to the program result, such as loop indexing and conditional branching.
Second, it exposes ways in which we can further transform the code to reduce the number of operations in the critical paths of the overall computation.
In this section, we will examine simple loop unrolling, without any further transformations.
Figure 5.16 shows a version of our combining code using two-way loop unrolling.
The ﬁrst loop steps through the array two elements at a time.
We see that CPEs for both integer addition andmultiplication improve, while those for the ﬂoating-point operations do not.
Figure 5.17 CPE performance for different degrees of loop unrolling.
This result can be attributed to the beneﬁts of reducing loop overhead operations.
It turns out that the compiler is making anoptimization basedon a reassociation transformation, altering the order in which values are combined.
The fact that gcc applies this transformation to integer multiplication but not to ﬂoating-point addition or multiplication is due to the associativity properties of the different operations and data types, as will also be discussed later.
To understand why the three ﬂoating-point cases do not improve by loop unrolling, consider the graphical representation for the inner loop, shown in Figure 5.18 for the case of single-precision multiplication.
We see here that the mulss instructions each get translated into two operations: one to load an array element from memory, and one to multiply this value by the accumulated value.
We see here that there is still a critical path of nmuloperations in this graph—there are half asmany iterations, but each iteration has two multiplication operations in sequence.
Since the critical path was the limiting factor for the performance of the code without loop unrolling, it remains so with simple loop unrolling.
Each iteration has two mulss instructions, each of which is translated into a load and a mul operation.
We rearrange, simplify, and abstract the representation of Figure 5.18 to show the data dependencies between successive iterations (a)
We see that each iteration must perform two multiplications in sequence (b)
Many compilers do it routinely whenever the optimization level is set sufﬁciently high.
Even though the loop has been unrolled by a factor of 2, there are still n mul operations along the critical path.
At this point, our functions have hit the bounds imposed by the latencies of the arithmetic units.
As we have noted, however, the functional units performing addition andmultiplication are all fully pipelined, meaning that they can start new operations every clock cycle.
Our code cannot take advantage of this capability, even with loop unrolling, since we are accumulating the value as a single variable acc.
We cannot compute a new value for acc until the preceding computation has.
Even though the functional unit can start a new operation every clock cycle, it will only start one every L cycles, where L is the latency of the combining operation.
We will now investigate ways to break this sequential dependency and get performance better than the latency bound.
Comparing loop unrolling alone to loop unrolling with two-way parallelism, we obtain the following performance:
This approach makes use of the pipelining capability of the functional units.
Of course, we also reached this bound for integer addition with standard unrolling.
Figure 5.22 CPE performance for kway loop unrolling with k-way parallelism.
All of the CPEs improve with this transformation, up to the limiting value of 1.00
Each iteration has two mulss instructions, each of which is translated into a load and a mul operation.
We rearrange, simplify, and abstract the representation of Figure 5.23 to show the data dependencies between successive iterations (a)
We see that there is no dependency between the two mul operations (b)
We can derive a template showing the data dependencies between iterations through the process shown in Figure 5.24
We now have two critical paths, each containing n/2 operations.
A similar analysis explains our observed CPE of L/2 for operations with latency L for the different combinations of data type and combining operation.
When we apply this transformation for larger values of k, we ﬁnd that we cannot reduce the CPE below 1.00
Once we reach this point, several of the functional units are operating at maximum capacity.
We have seen in Chapter 2 that two’s-complement arithmetic is commutative and associative, even when overﬂow occurs.
Many compilers do loop unrolling automatically, but relatively few then introduce this form of parallelism.
We now explore another way to break the sequential dependencies and thereby improve performance beyond the latency bound.
We saw that the simple loop unrolling of combine5 did not change the set of operations performed in combining the vector elements to form their sum or product.
By a very small change in the code, however, we can fundamentally change theway the combining is performed, and also greatly increase the program performance.
To an untrained eye, the two statements may seem essentially the same, but when we measure the CPE, we get surprising results:
In our experiments, we found the measured CPEs for combine7 to be more variable than for the other functions.
Figure 5.27 demonstrates the effect of applying the reassociation transformation to achieve k-way loop unrolling with reassociation.
We can see that the CPEs for all of our combining cases improve with increasing values of k.
This approach also increases the number of operations that can be performed in parallel.
Figure 5.27 CPE performance for kway loop unrolling with reassociation.
All of the CPEs improve with this transformation, up to the limiting value of 1.00
We see that the load operations resulting from the movss and the ﬁrst mulss instructions load vector elements i and i + 1 from memory, and the ﬁrst mul operation multiplies them together.
The second mul operation then multiples this result by the accumulated value acc.
We rearrange, simplify, and abstract the representation of Figure 5.28 to show the data dependencies between successive iterations (a)
The ﬁrst mul operation multiplies the two vector elements, while the second one multiplies the result by loop variable acc (b)
The ﬁrst multiplication within each iteration can be performed without waiting for the accumulated value from the previous iteration.
As we increase k, we continue to have only one operation per iteration along the critical path.
In performing the reassociation transformation, we once again change the order inwhich the vector elements will be combined together.
For integer addition and multiplication, the fact that these operations are associative implies that this reordering will have no effect on the result.
For the ﬂoating-point cases, we must once again assess whether this reassociation is likely to signiﬁcantly affect the outcome.
We would argue that the difference would be immaterial for most applications.
We can now explain the surprising improvement we saw with simple loop unrolling (combine5) for the case of integer multiplication.
In compiling this code, gcc performed the reassociation that we have shown in combine7, and hence it achieved the same performance.
It also performed the transformation for code with higher degrees of unrolling.
It would be gratifying to ﬁnd that gcc performed this transformation recognizing that the resulting code would run faster, but unfortunately this seems not to be the case.
In our experiments, we found that very minor changes to the C code caused gcc.
We have a single critical path, but it contains only n/2 operations.
Optimizing compilers must choose which factors they try to optimize, and it appears that gcc does not use maximizing instructionlevel parallelism as one of its optimization criteria when selecting how to associate integer operations.
In summary, a reassociation transformation can reduce the number of operations along the critical path in a computation, resulting in better performance by better utilizing the pipelining capabilities of the functional units.
Most compilers will not attempt any reassociations of ﬂoating-point operations, since these operations are not guaranteed to be associative.
Current versions of gcc do perform reassociations of integer operations, but not always with good effects.
In general, we have found that unrolling a loop and accumulating multiple values in parallel is a more reliable way to achieve improved program performance.
Practice Problem 5.8 Consider the following function for computing the product of an array of n integers.
For the line labeled Product computation, we can use parentheses to create ﬁve different associations of the computation, as follows:
Assume we run these functions on a machine where double-precision multiplication has a latency of 5 clock cycles.Determine the lower boundon theCPE set by the data dependencies of the multiplication.
Hint: It helps to draw a pictorial representation of how r is computed on every iteration.
In our examples, we consider the cases where they can hold either four integer or single-precision values, or two double-precision values.
This coding style is preferable to writing code directly in assembly language, since gcc can also generate code for the SIMD instructions found on other processors.
Using a combination of gcc instructions, loop unrolling, and multiple accumulators, we are able to achieve the following performance for our combining functions:
As this chart shows, using SSE instructions lowers the throughput bound, and we have nearly achieved these bounds for all ﬁve cases.
The double-precision instructions can only perform two in parallel, giving a throughput bound of 0.50
Our efforts at maximizing the performance of a routine that adds or multiplies the elements of a vector have clearly paid off.
The following summarizes the results we obtain with scalar code, not making use of the SIMD parallelism provided by SSE vector instructions:
As covered inWebAside opt:simd, we can improve performance even further by making use of gcc’s support for SIMD vector instructions:
The processor can sustain up to four combining operations per cycle for integer and single-precision data, and twoper cycle for double-precision data.
This represents a performance of over 6 gigaﬂops (billions of ﬂoating-point operations per second) on a processor now commonly found in laptop and desktopmachines.
First, the processor can only read 16 bytes from the data cache on each cycle, and then only by reading into anXMM register.
Second, the multiplier and adder units can only start a new operation every clock cycle (in the case of SIMD instructions, each of these “operations” actually computes two or four sums or products)
Thus, we have succeeded in producing the fastest possible versions of our combining function for thismachine.
We have seen that the critical path in a data-ﬂow graph representation of a program indicates a fundamental lower bound on the time required to execute a program.
That is, if there is some chain of data dependencies in a program where the sum of all of the latencies along that chain equals T , then the program will require at least T cycles to execute.
We have also seen that the throughput bounds of the functional units also impose a lower bound on the execution time for a program.
That is, assume that a program requires a total of N computations of some operation, that the microprocessor has only m functional units capable of performing that operation, and that these units have an issue time of i.
In this section, we will consider some other factors that limit the performance of programs on actual machines.
The beneﬁts of loop parallelism are limited by the ability to express the computation in assembly code.
In particular, the IA32 instruction set only has a small.
If we have a degree of parallelism p that exceeds the number of available registers, then the compiler will resort to spilling, storing some of the temporary values on the stack.
As an illustration, compare the performance of our parallel accumulator code for integer sum on x86-64 vs.
We demonstrated via experiments in Section 3.6.6 that a conditional branch can incur a signiﬁcant misprediction penalty when the branch prediction logic does.
Now that we have learned something about how processors operate, we can understand where this penalty arises.
Modern processors work well ahead of the currently executing instructions, reading new instructions from memory and decoding them to determine what operations to perform on what operands.
For the case of a conditional jump, this means predicting whether or not the branch will be taken.
For an instruction such as an indirect jump (as we saw in the code to jump to an address speciﬁed by a jump table entry) or a procedure return, this means predicting the target address.
In a processor that employs speculative execution, the processor begins executing the instructions at the predicted branch target.
It does this in a way that avoids modifying any actual register or memory locations until the actual outcome has been determined.
If the prediction is correct, the processor can then “commit” the results of the speculatively executed instructions by storing them in registers or memory.
If the prediction is incorrect, the processor must discard all of the speculatively executed results and restart the instruction fetch process at the correct location.
The misprediction penalty is incurred in doing this, because the instruction pipeline must be reﬁlled before useful results are generated.
Thebasic idea for translating into conditional moves is to compute the values along both branches of a conditional expression or statement, and then use conditional moves to select the desired value.
We saw in Section 4.5.10 that conditional move instructions can be implemented as part of the pipelined processing of ordinary instructions.
There is no need to guess whether or not the condition will hold, and hence no penalty for guessing incorrectly.
There is no simple answer to this question, but the following general principles apply.
We have seen that the effect of a mispredicted branch can be very high, but that does not mean that all program branches will slow a program down.
In fact, the branch prediction logic found in modern processors is very good at discerning regular patterns and long-term trends for the different branch instructions.
For example, the loop-closing branches in our combining routines would typically be predicted as being taken, and hence would only incur a misprediction penalty on the last time around.
The CPE hardly changed, even though this function uses two conditionals to check whether the vector index is within bounds.
These checks always determine that the index is within bounds, and hence they are highly predictable.
This code performs bounds checking and also references the vector elements through the vector data structure.
We can then directly compare the CPE for the functions with and without bounds checking:
Although the performance of the version with bounds checking is not quite as good, it increases theCPEby atmost 2 clock cycles.
This is a fairly small difference, considering that the bounds checking code performs two conditional branches.
The processor is able to predict the outcomes of these branches, and so none of this evaluation has much effect on the fetching and processing of the instructions that form the critical path in the program execution.
Many tests in a program are completely unpredictable, dependent on arbitrary features of the data, such as whether a number is negative or positive.
For these, the branchprediction logicwill do very poorly, possibly giving a prediction rate of 50%—no better than random guessing.
In principle, branch predictors can have prediction rates less than 50%, but such cases are very rare.
For inherently unpredictable cases, program performance can be greatly enhanced if the compiler is able to generate code using conditional data transfers rather than conditional control transfers.
This cannot be controlled directly by the C programmer, but some ways of expressing conditional behavior can be more directly translated into conditional moves than others.
We have found that gcc is able to generate conditional moves for codewritten in a more “functional” style, where we use conditional operations to compute values and then update the program state with these values, as opposed to a more “imperative” style, where we use conditionals to selectively update program state.
There are no strict rules for these two styles, and so we illustrate with an example.
Suppose we are given two arrays of integers a and b, and at each position i, we want to set a[i] to theminimum of a[i] and b[i], and b[i] to themaximum.
An imperative style of implementing this function is to check at each position i and swap the two elements if they are out of order:
A functional style of implementing this function is to compute the minimum and maximum values at each position i and then assign these values to a[i] and b[i], respectively:
Our measurements for this function show a CPE of around 5.0 regardless of whether the data are arbitrary or predictable.
We also examined the generated assembly code to make sure that it indeed used conditional moves.
As discussed in Section 3.6.6, not all conditional behavior can be implemented with conditional data transfers, and so there are inevitably cases where programmers cannot avoid writing code that will lead to conditional branches for which the processor will do poorly with its branch prediction.
But, as we have shown, a little cleverness on the part of the programmer can sometimes make code more amenable to translation into conditional data transfers.
This requires someamount of experimentation, writing different versions of the function and then examining the generated assembly code and measuring performance.
Practice Problem 5.9 The traditional implementation of the merge step of mergesort requires three loops:
Rewrite the code so that the effect of the conditional statement in the ﬁrst loop (lines 6–9) can be implemented with a conditional move.
All of the code we have written thus far, and all the tests we have run, access relatively small amounts of memory.
All modern processors contain one or more cache memories to provide fast access to such small amounts of memory.
In this section, we will further investigate the performance of programs that involve load (reading from memory into registers) and store (writing from registers to memory) operations, considering only the cases where all data are held in cache.
In Chapter 6, we go into much more detail about how caches work, their performance characteristics, and how to write code that makes best use of caches.
As Figure 5.11 shows, modern processors have dedicated functional units to perform load and store operations, and these units have internal buffers to hold sets of outstanding requests formemory operations.
Each of these units can typically initiate one operation every clock cycle.
The performance of a program containing load operations depends on both the pipelining capability and the latency of the load unit.
One factor limiting the CPE for our examples is that they all require reading one value from memory for each element computed.
Since the load unit can only initiate one load operation every clock cycle, the CPE cannot be less than 1.00
For applications where we must load k values for every element computed, we can never achieve a CPE lower than k (see, for example, Problem 5.17)
In our examples so far, we have not seen any performance effects due to the latency of load operations.
To determine the latency of the load operation on a machine, we can set up a computation with a sequence of load operations, where the outcome of one.
In the loop of this function, each successive value of variable ls depends on the value read by the pointer reference ls->next.
To see this, consider the assembly code for the loop.
The movq instruction on line 3 forms the critical bottleneck in this loop.
Each successive value of register %rdi depends on the result of a load operation having the value in %rdi as its address.
Thus, the load operation for one iteration cannot begin until the one for the previous iteration has completed.
The CPE of 4.00 for this function is determined by the latency of the load operation.
In all of our examples thus far, we analyzed only functions that reference memory mostly with load operations, reading from a memory location into a register.
Its counterpart, the store operation, writes a register value to memory.
The performance of this operation, particularly in relation to its interactions with load operations, involves several subtle issues.
As with the load operation, in most cases, the store operation can operate in a fully pipelined mode, beginning a new store on every cycle.
For example, consider the functions shown in Figure 5.32 that set the elements of an array dest of length.
Ourmeasurements for the ﬁrst version show a CPE of 2.00
Thus, we have achieved the optimum of one new store operation per cycle.
As a consequence, a series of ascending values will be stored in this location.
Figure 5.33 Code to write and read memory locations, along with illustrative executions.
This function highlights the interactions between stores and loads when arguments src and dest are equal.
To see how the processor can distinguish between these two cases and why one runs slower than the other, we must take a more detailed look at the load and store execution units, as shown in Figure 5.34
The store unit contains a store buffer containing the addresses and data of the store operations that have been issued to the store unit, but have not yet been completed, where completion involves updating the data cache.
This buffer is provided so that a series of store operations can be executed without having to wait for each one to update the cache.
The load unit must check its address with those in the store unit to detect a write/read dependency.
If it ﬁnds a match (meaning that any of the bytes being written have the same address as any of the bytes being read), it retrieves the corresponding data entry as the result of the load operation.
The instruction movl %eax,(%ecx) is translated into two operations: The s_addr instruction computes the address for the store operation, creates an entry in the store buffer, and sets the address ﬁeld for that entry.
The s_data operation sets the data ﬁeld for the entry.
As we will see, the fact that these two computations are performed independently can be important to program performance.
In addition to the data dependencies between the operations caused by the writing and reading of registers, the arcs on the right of the operators denote a set of implicit dependencies for these operations.
The ﬁrst movl instruction is decoded into separate operations to compute the store address and to store the data to memory.
We ﬁrst rearrange the operators of Figure 5.35 (a) and then show only those operations that use values from one iteration to produce new values for the next (b)
The ﬁgure shows a dashed arc between the s_data and load operations.
This dependency is conditional: if the two addresses match, the load operation must wait until the s_data has deposited its result into the store buffer, but if the two addresses differ, the two operations can proceed independently.
In Figure 5.36(a), we have rearranged the operations to allow the dependencies to be seen more clearly.
We have labeled the three dependencies involving the load and store operations for special attention.
The arc labeled (1) represents the requirement that the store address must be computed before the data can be stored.
The arc labeled (2) represents the need for the load operation to compare its address with that for any pending store operations.
Finally, the dashed arc labeled (3) represents the conditional data dependency that arises when the load and store addresses match.
Figure 5.36(b) illustrates what happens when we take away those operations that do not directly affect the ﬂow of data from one iteration to the next.
The data-ﬂow graph shows just two chains of dependencies: the one on the left, with data values being stored, loaded, and incremented (only for the case of matching addresses), and the one on the right, decrementing variable cnt.
We can now understand the performance characteristics of function write_ read.
For the case of Example A of Figure 5.33, with differing source and destination addresses, the load and store operations can proceed independently, and hence the only critical path is formed by the decrementing of variable cnt.
We have found similar behavior for any function where data are both being stored and loaded within a loop.
Apparently the effort to compare load addresses with those of the pending store operations forms an additional bottleneck.
When the two addresses do not match, the only critical path is formed by the decrementing of cnt (Example A)
When they do match, the chain of data being stored, loaded, and incremented forms the critical path (Example B)
We found that these three operations in sequence require a total of 6 clock cycles.
As these two examples show, the implementation of memory operations involves many subtleties.
With operations on registers, the processor can determine which instructions will affect which others as they are being decoded into operations.
With memory operations, on the other hand, the processor cannot predict which will affect which others until the load and store addresses have been computed.
Efﬁcient handling of memory operations is critical to the performance of many programs.
The memory subsystem makes use of many optimizations, such as the potential parallelism when operations can proceed independently.
Practice Problem 5.10 As another example of code with potential load-store interactions, consider the following function to copy the contents of one array to another:
Suppose a is an array of length 1000 initialized so that each element a[i] equals i.
Let us try to understand why our function performs so poorly.
The following is the assembly code for the inner loop of the function:
You may not be able to justify the exact CPE, but you should be able to describe why it runs more slowly than one might expect.
We measured the resulting code to have a CPE of 3.00, limited by the latency of ﬂoating-point addition.
Although we have only considered a limited set of applications, we can draw important lessons on how to write efﬁcient code.
We have described a number of basic strategies for optimizing program performance:
High-level design.Choose appropriate algorithms and data structures for the problem at hand.
Be especially vigilant to avoid algorithms or coding techniques that yield asymptotically poor performance.
Avoid optimization blockers so that a compiler can generate efﬁcient code.
Consider selective compromises of program modularity to gain greater efﬁciency.
Store a result in an array or global variable only when the ﬁnal value has been computed.
Unroll loops to reduce overhead and to enable further optimizations.
Find ways to increase instruction-level parallelism by techniques such as multiple accumulators and reassociation.
Rewrite conditional operations in a functional style to enable compilation via conditional data transfers.
A ﬁnal word of advice to the reader is to be vigilant to avoid introducing errors as you rewrite programs in the interest of efﬁciency.
It is very easy to make mistakes when introducing new variables, changing loop bounds, and making the code more complex overall.
One useful technique is to use checking code to test each version of a function as it is being optimized, to ensure no bugs are introduced during this process.
Checking code applies a series of tests to the new versions of a function and makes sure they yield the same results as the original.
The set of test cases must become more extensive with highly optimized code, since there are more cases to consider.
For example, checking code that uses loop unrolling requires testing for many different loop bounds to make sure it handles all of the different possible numbers of single-step iterations required at the end.
Up to this point, we have only considered optimizing small programs, where there is some clear place in the program that limits its performance and therefore should be the focus of our optimization efforts.
When working with large programs, even knowing where to focus our optimization efforts can be difﬁcult.
In this section we describe how to use code proﬁlers, analysis tools that collect performance data about a program as it executes.
We also present a general principle of system optimization known as Amdahl’s law.
Program proﬁling involves running a version of a program in which instrumentation code has been incorporated to determine how much time the different parts of the program require.
It can be very useful for identifying the parts of a program we should focus on in our optimization efforts.
One strength of proﬁling is that it can be performed while running the actual program on realistic benchmark data.
First, it determines how much CPU time was spent for each of the functions in the program.
Second, it computes a count of how many times each function gets called, categorized by which function performs the call.
The timings give a sense of the relative importance of the different functions in determining the overall run time.
The calling information allows us to understand the dynamic behavior of the program.
Proﬁling with gprof requires three steps, as shown for a C program prog.c, which runs with command line argument file.txt:
It runs slightly (around a factor of 2) slower than normal, but otherwise the only difference is that it generates a ﬁle gmon.out.
The ﬁrst part of the proﬁle report lists the times spent executing the different functions, sorted in descending order.
As an example, the following listing shows this part of the report for the three most time-consuming functions in a program:
Each row represents the time spent for all calls to some function.
The ﬁrst column indicates the percentage of the overall time spent on the function.
The second shows the cumulative time spent by the functions up to and including the one on this row.
The third shows the time spent on this particular function, and the fourth shows how many times it was called (not counting recursive calls)
Function Strlen computes the length of a string by calling the library function strlen.
Library function calls are normally not shown in the results by gprof.
Their times are usually reported as part of the function calling them.
The second part of the proﬁle report shows the calling history of the functions.
The following is the history for a recursive function find_ele_rec:
This history shows both the functions that called find_ele_rec, as well as the functions that it called.
From this calling information, we can often infer useful information about the program behavior.
For example, the function find_ele_rec is a recursive procedure that scans the linked list for a hash bucket looking for a particular string.
For this function, comparing the number of recursive calls with the number of top-level calls provides statistical information about the lengths of the traversals through these lists.
Statistically, every function should be charged according to the relative time spent executing it.
For programs that run for less than around 1 second, however, the numbers should be viewed as only rough estimates.
The compiled program maintains a counter for each combination of caller and callee.
The appropriate counter is incremented every time a procedure is called.
By default, the timings for library functions are not shown.
Instead, these times are incorporated into the times for the calling functions.
This application analyzes the n-gram statistics of a text document, where an n-gram is a sequence of n words occurring in a document.
For a given value of n, our program reads a text ﬁle, creates a table of unique n-grams specifying howmany times each one occurs, then sorts the n-grams in descending order of occurrence.
For the case of n = 2, n-grams are referred to as bigrams (pronounced “bye-grams”)
We created multiple versions, starting with simple algorithms for the different parts and then replacing them with more sophisticated ones:
Eachword is read from the ﬁle and converted to lowercase.
The program scans down this list looking for a matching entry.
Figure 5.38 Proﬁle resultss for different versions of n-gram frequency counting program.
Time is divided according to the different major operations in the program.
Our initial version performed this operation recursively, inserting new elements at the end of the list.
Once the table has been generated, we sort all of the elements according to the frequencies.
Figure 5.38 shows the proﬁle results for six different versions of our n-gramfrequency analysis program.
For each version, we divide the time into the following categories:
List: Scanning the linked list for a matching n-gram, inserting a new element if necessary.
As part (a) of the ﬁgure shows, our initial version required nearly 3 minutes, with most of the time spent sorting.
This is not surprising, since insertion sort has quadratic run time, and the program sorted 363,039 values.
In our next version, we performed sorting using the library function qsort, which is based on the quicksort algorithm, having run timeO(n log n)
The more efﬁcient sorting algorithm reduces the time spent sorting to become negligible, and the overall run time to around 4.7 seconds.
Part (b) of the ﬁgure shows the times for the remaining version on a scale where we can see them more clearly.
With improved sorting, we nowﬁnd that list scanning becomes the bottleneck.
Thinking that the inefﬁciency is due to the recursive structure of the function, we replaced it by an iterative one, shown as “Iter ﬁrst.” Surprisingly, the run time increases to around 5.9 seconds.
On closer study, we ﬁnd a subtle difference between the two list functions.
The recursive version inserted new elements at the end of the list, while the iterative one inserted them at the front.
To maximize performance, we want the most frequent n-grams to occur near the beginnings of the lists.
That way, the function will quickly locate the common cases.
Assuming that n-grams are spread uniformly throughout the document, we would expect the ﬁrst occurrence of a frequent one to come before that of a less frequent one.
By inserting new n-grams at the end, the ﬁrst function tended to order ngrams in descending order of frequency, while the second function tended to do just the opposite.
We therefore created a third list-scanning function that uses iteration, but inserts new elements at the end of this list.
With this version, shown as “Iter last,” the time dropped to around 4.2 seconds, slightly better than with the recursive version.
These measurements demonstrate the importance of running experiments on a program as part of an optimization effort.
We initially assumed that converting recursive code to iterative code would improve its performance and did not consider the distinction between adding to the end or to the beginning of a list.
The initial version had only 1021 buckets (typically, the number of buckets is chosen to be a prime number to enhance the ability of the hash function to distribute keys uniformly among the buckets)
That explains why so much of the time is spent performing list operations—the searches involve testing a signiﬁcant number of candidate ngrams.
It also explains why the performance is so sensitive to the list ordering.
On further inspection, we can see that the minimal performance gain with a larger tablewas due to a poor choice of hash function.
Simply summing the character codes for a string does not produce a very wide range of values.
In addition, a commutative hash function, such as addition, does not differentiate among the different possible orderings of characters with a string.
For example, the words “rat” and “tar” will generate the same sums.
We switched to a hash function that uses shift and Exclusive-Or operations.
With this version, shown as “Better hash,” the time drops to 0.4 seconds.
A more systematic approach would be to study the distribution of keys among the buckets more carefully, making sure that it comes close to what one would expect if the hash function had a uniform output distribution.
Finally, we have reduced the run time to the point where most of the time is spent in strlen, andmost of the calls to strlen occur as part of the lowercase conversion.
We have already seen that function lower1 has quadratic performance, especially for long strings.
The words in this document are short enough to avoid the disastrous consequences of quadratic performance; the longest bigram is just 32 characters.
The proﬁler helps us focus our attention on the most time-consuming parts of the program and also provides useful information about the procedure call structure.
Some of the bottlenecks in our code, such as using a quadratic sort routine, are easy to anticipate, while others, such as whether to append to the beginning or end of a list, emerge only through a careful analysis.
We can see that proﬁling is a useful tool to have in the toolbox, but it should not be the only one.
The timingmeasurements are imperfect, especially for shorter (less than 1 second) run times.
More signiﬁcantly, the results apply only to the particular data tested.
For example, if we had run the original function on data consisting of a smaller number of longer strings, we would have found that the lowercase conversion routinewas themajor performance bottleneck.
Evenworse, if it only proﬁled documents with short words, we might never detect hidden bottlenecks such as the quadratic performance of lower1
In general, proﬁling can help us optimize for typical cases, assuming we run the program on representative data, butwe should alsomake sure the programwill have respectable performance for all possible cases.
This mainly involves avoiding algorithms (such as insertion sort) and bad programming practices (such as lower1) that yield poor asymptotic performance.
Gene Amdahl, one of the early pioneers in computing, made a simple but insightful observation about the effectiveness of improving the performance of one part of a system.
This observation has come to be known as Amdahl’s law.
The main idea is that when we speed up one part of a system, the effect on the overall system performance depends on both how signiﬁcant this part was and how much it sped up.
Consider a system in which executing some application requires time.
From this, we can compute the speedup S = Told/Tnew as.
You hear on the news that Montana has just abolished its speed limit, which constitutes 1500 km of the trip.
Your truck can travel at 150 km/hr.What will be your speedup for the trip?
They stock a variety of models, but the faster you want to go, the more it will cost.
How fast must you travel through Montana to get an overall speedup for your trip of 5/3?
Amdahl’s law describes a general principle for improving any process.
In addition to applying to speeding up computer systems, it can guide a company trying to reduce the cost of manufacturing razor blades, or a student trying to improve his or her gradepoint average.
Perhaps it is most meaningful in the world of computers, where we routinely improve performance by factors of 2 or more.
Such high factors can only be achieved by optimizing large parts of a system.
Although most presentations on code optimization describe how compilers can generate efﬁcient code, much can be done by an application programmer to assist the compiler in this task.
No compiler can replace an inefﬁcient algorithm or data structure by a good one, and so these aspects of program design should remain a primary concern for programmers.
We also have seen that optimization blockers, such as memory aliasing and procedure calls, seriously restrict the ability of compilers to perform extensive optimizations.
Again, the programmer must take primary responsibility for eliminating these.
These should simply be considered parts of good programming practice, since they serve to eliminate unneededwork.
Tuning performance beyond a basic level requires some understanding of the processor’s microarchitecture, describing the underlying mechanisms by which the processor implements its instruction set architecture.
For the case of out-oforder processors, just knowing something about the operations, latencies, and issue times of the functional units establishes a baseline for predicting program performance.
We have studied a series of techniques, including loop unrolling, creating multiple accumulators, and reassociation, that can exploit the instruction-level parallelism provided by modern processors.
As we get deeper into the optimization, it becomes important to study the generated assembly code, and to try to understand how the computation is being performed by the machine.
Much can be gained by identifying the critical paths determined by the data dependencies.
We can also compute a throughput bound for a computation, based on the number of operations that must be computed and the number and issue times of the units that perform those operations.
Programs that involve conditional branches or complex interactions with the memory system are more difﬁcult to analyze and optimize than the simple loop programs we ﬁrst considered.
The basic strategy is to try to make branches more predictable or make them amenable to implementation using conditional data transfers.
We must also watch out for the interactions between store and load operations.
Keeping values in local variables, allowing them to be stored in registers, can often be helpful.
When working with large programs, it becomes important to focus our optimization efforts on the parts that consume the most time.
Code proﬁlers and related tools can help us systematically evaluate and improve program performance.
More sophisticated proﬁlers are available, such as the vtune program development system from Intel, and valgrind, commonly available on Linux systems.
These tools can break down the execution time below the procedure level, to estimate the performance of eachbasic block of the program.
Abasic block is a sequence of instructions that has no transfers of control out of its middle, and so the block is always executed in its entirety.
Amdahl’s law provides a simple but powerful insight into the performance gains obtained by improving just one part of the system.
The gain depends both on how much we improve this part and how large a fraction of the overall time this part originally required.
Many publications describe code optimization from a compiler’s perspective, formulating ways that compilers can generate more efﬁcient code.
Wadleigh and Crawford’s book on software optimization [114] covers some of the material we have presented, but it also describes the process of getting high performance on parallel machines.
This paper covers the code transformations we presented, including loop unrolling, multiple accumulators (which they refer to as accumulator variable expansion), and reassociation (which they refer to as tree height reduction)
Our presentation of the operation of an out-of-order processor is fairly brief and abstract.More complete descriptions of the general principles can be found in.
Shen and Lipasti’s book [96] provides an in-depth treatment of modern processor design.
Amdahl’s law is presented in most books on computer architecture.
Our measurements show that this function has a CPE of 3.00 for integer and ﬂoating-point data.
For data type float, the x86-64 assembly code for the inner loop is as follows:
Assume that the functional units have the characteristics listed in Figure 5.12
For data type float, what lower bound on the CPE is determined by the critical path?
Assuming similar instruction sequences for the integer code as well, what lower bound on the CPE is determined by the critical path for integer data?
Explain why any version of any inner product procedure cannot achieve a CPE less than 2.00
Explain why the performance for ﬂoating-point data did not improve with loop unrolling.
What factor limits the performance to a CPE of 2.00?
This function ﬁlls n bytes of the memory area starting at s with copies of the loworder byte of c.
For example, it can be used to zero out a region of memory by giving argument 0 for c, but other values are possible.
You might ﬁnd it helpful to do additional loop unrolling as well.
In this discussion, letK denote the value of sizeof(unsigned long) for the machine on which you run your program.
Your code should work for arbitrary values of n, including when it is not a multiple of K.
You can do this in a manner similar to the way we ﬁnish the last few iterations with loop unrolling.
You should write your code so that it will compile and run correctly regardless of the value of K.
On some machines, unaligned writes can be much slower than aligned ones.
On some non-x86 machines, they can even cause segmentation faults.
Write your code so that it starts with byte-level writes until the destination address is a multiple of K , then do word-level writes, and then (if necessary) ﬁnish with byte-level writes.
Beware of the case where cnt is small enough that the upper bounds on some of the loops become negative.
With expressions involving the sizeof operator, the testing may be performed with unsigned arithmetic.
At the very least, you should be able to achieve a CPE less than the latency of ﬂoating-point addition for your machine.
For example, our version with two-way unrolling requires three additions per iteration, while our version with three-way unrolling requires ﬁve.
As the following commented code shows, the effect will be to set the value at xp to zero:
This example illustrates that our intuition about program behavior can often be wrong.
We naturally think of the case where xp and yp are distinct but overlook the possibility that they might be equal.
Bugs often arise due to conditions the programmer does not anticipate.
It is worth studying this code carefully to better understand the subtleties of code optimization.
In the less optimized code, register%xmm0 is simply used as a temporary value, both set and used on each loop iteration.
In the more optimized code, it is used more in the manner of variable x in combine4, accumulating the product of the vector elements.
The difference with combine4, however, is that location dest is updated on each iteration by the second movss instruction.
We can see that this optimized version operates much like the following C code:
The two versions of combine3 will have identical functionality, even with memory aliasing.
This transformation can be made without changing the program behavior, because, with the exception of the ﬁrst iteration, the value read from dest at the beginning of each iteration will be the same value written to this register at the end of the previous iteration.
Therefore, the combining instruction can simply use the value already in %xmm0 at the beginning of the loop.
For example, polynomial functions are commonly used to approximate trigonometric functions in math libraries.
We can see that the performance limiting computation here is the repeated computation of the expression xpwr = x * xpwr.
The updating of result only requires a ﬂoating-point addition (3 clock cycles) between successive iterations.
The function performs n multiplications and n additions, half the number of multiplications as the original function poly.
We can see that the performance limiting computation here is the repeated computation of the expression result = a[i] + x*result.
Thus, each iteration imposes aminimum latency of 8 cycles, exactly our measured CPE.
Although each iteration in function poly requires twomultiplications rather than one, only a single multiplication occurs along the critical path per iteration.
The operations shown as blue boxes form the critical path for the iteration.
Figure 5.39 diagrams the three multiplication operations for a single iteration of the function.
In this ﬁgure, theoperations shownasblueboxes are along the critical path—they need to be computed in sequence to compute a new value for loop variable r.
The operations shown as light boxes can be computed in parallel with the critical path operations.
This is due to the fact that write_read increments the value before storing it, requiring one clock cycle.
It will give a CPE of 2.00, the same as for Example A, since there are no dependencies between stores and subsequent loads.
At the start of iteration i, it holds the value of p[i-1]
We then compute val to be the value of p[i] and to be the new value for last_val.
This one requires you to look at Equation 5.4 from an unusual perspective.
To this point in our study of systems, we have relied on a simple model of a computer system as a CPU that executes instructions and a memory system that holds instructions and data for the CPU.
In our simple model, the memory system is a linear array of bytes, and the CPU can access each memory location in a constant amount of time.
While this is an effective model as far as it goes, it does not reﬂect the way that modern systems really work.
In practice, a memory system is a hierarchy of storage devices with different capacities, costs, and access times.
Small, fast cache memories nearby the CPU act as staging areas for a subset of the data and instructions stored in the relatively slow main memory.
The main memory stages data stored on large, slow disks, which in turn often serve as staging areas for data stored on the disks or tapes of other machines connected by networks.
Memory hierarchies work because well-written programs tend to access the storage at any particular level more frequently than they access the storage at the next lower level.
So the storage at the next level can be slower, and thus larger and cheaper per bit.
The overall effect is a large pool of memory that costs as much as the cheap storage near the bottom of the hierarchy, but that serves data to programs at the rate of the fast storage near the top of the hierarchy.
As a programmer, you need to understand the memory hierarchy because it has a big impact on the performance of your applications.
If the data your program needs are stored in a CPU register, then they can be accessed in zero cycles during the execution of the instruction.
And if stored in disk tens of millions of cycles!
Here, then, is a fundamental and enduring idea in computer systems: if you understand how the system moves data up and down the memory hierarchy, then you can write your application programs so that their data items are stored higher in the hierarchy, where the CPU can access them more quickly.
This idea centers around a fundamental property of computer programs known as locality.
Programs with good locality tend to access the same set of data items over and over again, or they tend to access sets of nearby data items.
Programs with good locality tend to access more data items from the upper levels of thememory hierarchy than programswith poor locality, and thus run faster.
In this chapter, we will look at the basic storage technologies—SRAM memory, DRAM memory, ROM memory, and rotating and solid state disks—and describe how they are organized into hierarchies.
In particular, we focus on the cache memories that act as staging areas between the CPU andmain memory, because they have the most impact on application program performance.
We show you how to analyze your C programs for locality and we introduce techniques for improving the locality in your programs.
You will also learn an interesting way to characterize the performance of the memory hierarchy on a particular machine as a “memory mountain” that shows read access times as a function of locality.
Much of the success of computer technology stems from the tremendous progress in storage technology.
The earliest IBM PCs didn’t even have a hard disk.
Typically, a desktop system will have no more than a few megabytes of SRAM, but hundreds or thousands of megabytes of DRAM.
This circuit has the property that it can stay indeﬁnitely in either of two different voltage conﬁgurations, or states.
Any other state will be unstable—starting from there, the circuit will quickly move toward one of the stable states.
Such amemory cell is analogous to the inverted pendulum illustrated in Figure 6.1
The pendulum is stable when it is tilted either all the way to the left or all the way to the right.
From any other position, the pendulumwill fall to one side or the other.
In principle, the pendulum could also remain balanced in a vertical position indeﬁnitely, but this state is metastable—the smallest disturbance would make it start to fall, and once it fell it would never return to the vertical position.
Due to its bistable nature, an SRAM memory cell will retain its value indefinitely, as long as it is kept powered.
Even when a disturbance, such as electrical noise, perturbs the voltages, the circuit will return to the stable value when the disturbance is removed.
Like an SRAM cell, the pendulum has only two stable conﬁgurations, or states.
Fortunately, for computers operating with clock cycle times measured in nanoseconds, this retention time is quite long.
The memory system must periodically refresh every bit of memory by reading it out and then rewriting it.
Figure 6.2 summarizes the characteristics of SRAM and DRAM memory.
The trade-off is that SRAM cells use more transistors than DRAM cells, and thus have lower densities, are more expensive, and consume more power.
The storage community has never settled on a standard name for a DRAM array element.
Computer architects tend to refer to it as a “cell,” overloading the term with the DRAM storage cell.
Circuit designers tend to refer to it as a “word,” overloading the term with a word of main memory.
To avoid confusion, we have adopted the unambiguous term “supercell.”
Each DRAM chip is connected to some circuitry, known as the memory controller, that can transferw bits at a time to and from eachDRAMchip.
To read the contents of supercell (i, j), the memory controller sends the row address i to the DRAM, followed by the column address j.
The DRAM responds by sending the contents of supercell (i, j) back to the controller.
The row address i is called a RAS (RowAccess Strobe) request.
The column address j is called aCAS (Column Access Strobe) request.
Notice that the RAS and CAS requests share the same DRAM address pins.
One reason circuit designers organize DRAMs as two-dimensional arrays instead of linear arrays is to reduce the number of address pins on the chip.
The disadvantage of the two-dimensional array organization is that addresses must be sent in two distinct steps, which increases the access time.
To retrieve a 64-bit doubleword at memory addressA, the memory controller converts A to a supercell address (i, j) and sends it to the memory module, which then broadcasts i and j to each DRAM.
In response, each DRAM outputs the 8bit contents of its (i, j) supercell.
Circuitry in themodule collects these outputs and forms them into a 64-bit doubleword, which it returns to the memory controller.
Main memory can be aggregated by connecting multiple memory modules to thememory controller.
In this case, when the controller receives an addressA, the controller selects the module k that contains A, converts A to its (i, j) form, and sends (i, j) to module k.
Practice Problem 6.1 In the following, let r be the number of rows in a DRAM array, c the number of columns, br the number of bits needed to address the rows, and bc the number of bits needed to address the columns.
There are many kinds of DRAM memories, and new kinds appear on the market with regularity as manufacturers attempt to keep up with rapidly increasing.
Each is based on the conventional DRAM cell, with optimizations that improve the speed with which the basic DRAM cells can be accessed.
A conventional DRAM copies an entire rowof supercells into its internal rowbuffer, uses one, and then discards the rest.
For example, to read four supercells from row i of a conventional DRAM, the memory controller must send four RAS/CAS requests, even though the row address i is identical in each case.
To read supercells from the same row of an FPM DRAM, the memory controller sends an initial RAS/CAS request, followed by three CAS requests.
The initial RAS/CAS request copies row i into the row buffer and returns the supercell addressed by the CAS.
The next three supercells are served directly from the row buffer, and thus more quickly than the initial supercell.
Conventional, FPM, and EDODRAMs are asynchronous in the sense that they communicate with thememory controller using a set of explicit control signals.
Thus, the system can be painting the screen with the pixels in the frame buffer (reads) while concurrently writing new values for the next update (writes)
DRAMsandSRAMsare volatile in the sense that they lose their information if the supply voltage is turned off.Nonvolatile memories, on the other hand, retain their information even when they are powered off.
For historical reasons, they are referred to collectively as read-only memories (ROMs), even though some types of ROMs can be written to as well as read.
ROMs are distinguished by the number of times they can be reprogrammed (written to) and by the mechanism for reprogramming them.
PROMs include a sort of fuse with each memory cell that can be blown once by zapping it with a high current.
An erasable programmableROM (EPROM)has a transparent quartzwindow that permits light to reach the storage cells.
The EPROM cells are cleared to zeros by shining ultraviolet light through the window.
Programming an EPROM is done by using a special device to write ones into the EPROM.
An EPROM can be erased and reprogrammed on the order of 1000 times.
Flash memory is a type of nonvolatile memory, based on EEPROMs, that has become an important storage technology.
Flash memories are everywhere, providing fast and durable nonvolatile storage for a slew of electronic devices, including digital cameras, cell phones, music players, PDAs, and laptop, desktop, and server computer systems.
In Section 6.1.3, we will look in detail at a new form of ﬂash-based disk drive, known as a solid state disk (SSD), that provides a faster, sturdier, and less power-hungry alternative to conventional rotating disks.
Programs stored in ROM devices are often referred to as ﬁrmware.
When a computer system is powered up, it runs ﬁrmware stored in a ROM.
Some systems provide a small set of primitive input and output functions in ﬁrmware, for example, a PC’s BIOS (basic input/output system) routines.
Complicated devices such as graphics cards and disk drive controllers also rely on ﬁrmware to translate I/O (input/output) requests from the CPU.
Data ﬂows back and forth between the processor and the DRAM main memory over shared electrical conduits called buses.
Each transfer of data between the CPU and memory is accomplished with a series of steps called a bus transaction.
A read transaction transfers data from the main memory to the CPU.
A write transaction transfers data from the CPU to the main memory.
A bus is a collection of parallel wires that carry address, data, and control signals.
Depending on the particular bus design, data and address signals can share the same set ofwires, or they canusedifferent sets.Also,more than twodevices can share the samebus.
The controlwires carry signals that synchronize the transaction and identify what kind of transaction is currently being performed.
Figure 6.6 Example bus structure that connects the CPU and main memory.
Figure 6.6 shows the conﬁguration of an example computer system.
The main components are the CPU chip, a chipset that we will call an I/O bridge (which includes the memory controller), and the DRAM memory modules that make up main memory.
These components are connected by a pair of buses: a system bus that connects the CPU to the I/O bridge, and a memory bus that connects the I/O bridge to the main memory.
The I/O bridge translates the electrical signals of the system bus into the electrical signals of the memory bus.
As we will see, the I/O bridge also connects the system bus and memory bus to an I/O bus that is shared by I/O devices such as disks and graphics cards.
For now, though, we will focus on the memory bus.
Bus design is a complex and rapidly changing aspect of computer systems.
Different vendors develop different bus architectures as a way to differentiate their products.
For example, Intel systems use chipsets known as the northbridge and the southbridge to connect the CPU tomemory and I/O devices, respectively.
In older Pentium and Core 2 systems, a front side bus (FSB) connects the CPU to the northbridge.
Systems from AMD replace the FSB with the HyperTransport interconnect, while newer Intel Core i7 systems use the QuickPath interconnect.
The details of these different bus architectures are beyond the scope of this text.
Instead, wewill use the high-level bus architecture fromFigure 6.6 as a running example throughout the text.
It is a simple but useful abstraction that allows us to be concrete, and captures the main ideas without being tied too closely to the detail of any proprietary designs.
Consider what happens when the CPU performs a load operation such as.
Circuitry on the CPU chip called the bus interface initiates a read transaction on the bus.
First, the CPU places the address A on the system bus.
The I/O bridge passes the signal along to the memory bus (Figure 6.7(a))
Next, the main memory senses the address signal on the memory.
Figure 6.7 Memory read transaction for a load operation: movl A,%eax.
The I/O bridge translates the memory bus signal into a system bus signal, and passes it along to the system bus (Figure 6.7(b))
Finally, the CPU senses the data on the system bus, reads it from the bus, and copies it to register %eax (Figure 6.7(c))
Conversely, when the CPU performs a store instruction such as.
First, the CPU places the address on the system bus.
Finally, the main memory reads the data word from the memory bus and stores the bits in the DRAM (Figure 6.8(c))
Main memory reads it and waits for the data word.
Figure 6.8 Memory write transaction for a store operation: movl %eax,A.
Disks are workhorse storage devices that hold enormous amounts of data, on the order of hundreds to thousands of gigabytes, as opposed to the hundreds or thousands of megabytes in a RAM-basedmemory.
However, it takes on the order of milliseconds to read information from a disk, a hundred thousand times longer than from DRAM and a million times longer than from SRAM.
Each platter consists of two sides, or surfaces, that are coated with magnetic recording material.
A rotating spindle in the center of the platter spins the platter at a ﬁxed rotational rate, typically between 5400 and.
A disk will typically contain one or more of these platters encased in a sealed container.
Figure 6.9(a) shows the geometry of a typical disk surface.
Each surface consists of a collection of concentric rings called tracks.
Each sector contains an equal number of data bits (typically 512 bytes) encoded in the magnetic material on the sector.
Sectors are separated by gaps where no data bits are stored.
A disk consists of one or more platters stacked on top of each other and encased in a sealed package, as shown in Figure 6.9(b)
The entire assembly is often referred to as a disk drive, although we will usually refer to it as simply a disk.
We will sometime refer to disks as rotating disks to distinguish them from ﬂash-based solid state disks (SSDs), which have no moving parts.
Disk manufacturers describe the geometry of multiple-platter drives in terms of cylinders, where a cylinder is the collection of tracks on all the surfaces that are equidistant from the center of the spindle.
For example, if a drive has three platters and six surfaces, and the tracks on each surface are numbered consistently, then cylinder k is the collection of the six instances of track k.
The maximum number of bits that can be recorded by a disk is known as its maximum capacity, or simply capacity.
Recording density (bits/in): The number of bits that can be squeezed into a 1-inch segment of a track.
Track density (tracks/in): The number of tracks that can be squeezed into a 1-inch segment of the radius extending from the center of the platter.
Areal density (bits/in2): The product of the recording density and the track density.
Disk manufacturers work tirelessly to increase areal density (and thus capacity), and this is doubling every few years.
The original disks, designed in an age of low areal density, partitioned every track into the same number of sectors, which was determined by the number of sectors that could be recorded on the innermost track.
Tomaintain a ﬁxed number of sectors per track, the sectors were spaced farther apart on the outer tracks.
However, as areal densities increased, the gaps between sectors (where no data bits were stored) became unacceptably large.
Thus, modern high-capacity disks use a technique known as multiple zone recording, where the set of cylinders is partitioned into disjoint subsets known as recording zones.
Each track in each cylinder in a zone has the same number of sectors, which is determined by the number of sectors that can be packed into the innermost track of the zone.
Note that diskettes (ﬂoppy disks) still use the old-fashioned approach, with a constant number of sectors per track.
The capacity of a disk is given by the following formula:
Unfortunately, the meanings of preﬁxes such as kilo (K), mega (M), giga (G), and tera (T ) depend on the context.
Rates and throughputs usually use these preﬁx values as well.
The disk surface spins at a fixed rotational rate The read/write head.
By moving radially, the arm can position the read/write head over any track.
Disks read and write bits stored on the magnetic surface using a read/write head connected to the end of an actuator arm, as shown in Figure 6.10(a)
By moving the arm back and forth along its radial axis, the drive can position the head over any track on the surface.
Once the head is positioned over the desired track, then as each bit on the track passes underneath, the head can either sense the value of the bit (read the bit) or alter the value of the bit (write the bit)
Disks with multiple platters have a separate read/write head for each surface, as shown in Figure 6.10(b)
The heads are lined up vertically and move in unison.
At any point in time, all heads are positioned on the same cylinder.
If the head were to strike one of these boulders, the head would cease ﬂying and crash into the surface (a so-called head crash)
For this reason, disks are always sealed in airtight packages.
The access time for a sector has three main components: seek time, rotational latency, and transfer time:
Seek time: To read the contents of some target sector, the arm ﬁrst positions the head over the track that contains the target sector.
The time required to move the arm is called the seek time.
The seek time, Tseek, depends on the previous position of the head and the speed that the arm moves across the surface.
The maximum time for a single seek, Tmax seek, can be as high as 20 ms.
Rotational latency:Once the head is in position over the track, the drive waits for the ﬁrst bit of the target sector to pass under the head.
The performance of this step depends on both the position of the surface when the head arrives at the target sector and the rotational speed of the disk.
In the worst case, the head just misses the target sector andwaits for the disk tomake a full rotation.
Thus, the maximum rotational latency, in seconds, is given by.
The average rotational latency, Tavg rotation, is simply half of Tmax rotation.
Transfer time: When the ﬁrst bit of the target sector is under the head, the drive can begin to read or write the contents of the sector.
The transfer time for one sector depends on the rotational speed and the number of sectors per track.
Thus, we can roughly estimate the average transfer time for one sector in seconds as.
We can estimate the average time to access the contents of a disk sector as the sum of the average seek time, the average rotational latency, and the average transfer time.
For this disk, the average rotational latency (in ms) is.
Putting it all together, the total estimated access time is.
The time to access the 512 bytes in a disk sector is dominated by the seek time and the rotational latency.
Accessing the ﬁrst byte in the sector takes a long time, but the remaining bytes are essentially free.
Since the seek time and rotational latency are roughly the same, twice the seek time is a simple and reasonable rule for estimating disk access time.
The difference in access times is even more dramatic if we compare the times to access a single word.
Practice Problem 6.3 Estimate the average time (in ms) to access a sector on the following disk:
When the operating systemwants to perform an I/Ooperation such as reading a disk sector into main memory, it sends a command to the disk controller asking it to read a particular logical block number.
Firmware on the controller performs a fast table lookup that translates the logical block number into a (surface, track, sector) triple that uniquely identiﬁes the corresponding physical sector.
Hardware on the controller interprets this triple to move the heads to the appropriate cylinder, waits for the sector to pass under the head, gathers up the bits sensed.
Before a disk can be used to store data, it must be formatted by the disk controller.
This involves ﬁlling in the gaps between sectors with information that identiﬁes the sectors, identifying any cylinders with surface defects and taking them out of action, and setting aside a set of cylinders in each zone as spares that can be called into action if one or more cylinders in the zone goes bad during the lifetime of the disk.
The formatted capacity quoted by disk manufacturers is less than the maximum capacity because of the existence of these spare cylinders.
For each case below, suppose that a program reads the logical blocks of the ﬁle sequentially, one after the other, and that the time to position the head over the ﬁrst block is Tavg seek + Tavg rotation.
Best case: Estimate the optimal time (in ms) required to read the ﬁle given.
Random case: Estimate the time (in ms) required to read the ﬁle if blocks are mapped randomly to disk sectors.
Input/output (I/O) devices such as graphics cards, monitors, mice, keyboards, and disks are connected to the CPU and main memory using an I/O bus such as Intel’s Peripheral Component Interconnect (PCI) bus.
Unlike the system bus and memory buses, which are CPU-speciﬁc, I/O buses such as PCI are designed to be independent of the underlying CPU.
Figure 6.11 shows a typical I/O bus structure (modeled on PCI) that connects the CPU, main memory, and I/O devices.
Although the I/O bus is slower than the system and memory buses, it can accommodate a wide variety of third-party I/O devices.
For example, the bus in Figure 6.11 has three different types of devices attached to it.
Figure 6.11 Example bus structure that connects the CPU, main memory, and I/O devices.
A Universal Serial Bus (USB) controller is a conduit for devices attached to a USB bus, which is a wildly popular standard for connecting a variety of peripheral I/O devices, including keyboards, mice, modems, digital cameras, game controllers, printers, external disk drives, and solid state disks.
A graphics card (or adapter) contains hardware and software logic that is responsible for painting the pixels on the display monitor on behalf of the CPU.
A host bus adapter that connects one or more disks to the I/O bus using a communication protocol deﬁned by a particular host bus interface.
The two most popular such interfaces for disks are SCSI (pronounced “scuzzy”) and SATA (pronounced “sat-uh”)
A SCSI host bus adapter (often called a SCSI controller) can support multiple disk drives, as opposed to SATA adapters, which can only support one drive.
Additional devices such as network adapters can be attached to the I/O bus by plugging the adapter into empty expansion slots on the motherboard that provide a direct electrical connection to the bus.
While a detailed description of how I/O devices work and how they are programmed is outside our scope here, we can give you a general idea.
For example, Figure 6.12 summarizes the steps that take place when a CPU reads data from a disk.
The CPU issues commands to I/O devices using a technique called memorymapped I/O (Figure 6.12(a))
Each of these addresses is known as an I/O port.
Each device is associated with (or mapped to) one or more ports when it is attached to the bus.
As a simple example, suppose that the disk controller is mapped to port 0xa0
Then the CPU might initiate a disk read by executing three store instructions to address 0xa0: The ﬁrst of these instructions sends a command word that tells the disk to initiate a read, along with other parameters such as whether to interrupt the CPU when the read is ﬁnished.
The second instruction indicates the logical block number that should be read.
The third instruction indicates the main memory address where the contents of the disk sector should be stored.
After it issues the request, the CPU will typically do other work while the disk is performing the read.
Simply waiting and doing nothing while the transfer is taking place would be enormously wasteful.
After the disk controller receives the read command from the CPU, it translates the logical block number to a sector address, reads the contents of the sector, and transfers the contents directly tomainmemory, without any intervention from the CPU (Figure 6.12(b))
This process, whereby a device performs a read or write bus transaction on its own, without any involvement of theCPU, is known as direct memory access (DMA)
The transfer of data is known as a DMA transfer.
After the DMA transfer is complete and the contents of the disk sector are safely stored in main memory, the disk controller notiﬁes the CPU by sending an interrupt signal to the CPU (Figure 6.12(c))
The basic idea is that an interrupt signals an external pin on the CPU chip.
Disk manufacturers publish a lot of useful high-level technical information on theirWeb pages.
If we consult the online product manual on the Seagate Web page, we can glean the geometry and performance information shown in Figure 6.13
Disk manufacturers rarely publish detailed technical information about the geometry of the individual recording zones.
However, storage researchers at Carnegie Mellon University have developed a useful tool, called DIXtrac, that automatically discovers a wealth of low-level information about the geometry and performance of SCSI disks [92]
For example, DIXtrac is able to discover the detailed zone geometry of our example Seagate disk, which we’ve shown in Figure 6.14
Each row in the table characterizes one of the 15 zones.
The second column gives the number of sectors contained in each track in that zone.
The third column shows the number of cylinders assigned to that zone, where each cylinder consists of eight tracks, one from each surface.
Similarly, the fourth column gives the total number of logical blocks assigned to each zone, across all eight surfaces.
The tool was not able to extract valid data for the innermost zone, so these are omitted.
The zone map reveals some interesting facts about the Seagate disk.
First, more sectors are packed into the outer zones (which have a larger circumference) than the inner zones.
Zone Sectors Cylinders Logical blocks number per track per zone per zone.
If the recording material on a sector goes bad, the disk controller will automatically remap the logical blocks on that cylinder to an available spare.
So we see that the notion of a logical block not only provides a simpler interface to the operating system, it also provides a level of indirection that enables the disk to be more robust.
A solid state disk (SSD) is a storage technology, based on ﬂash memory (Section 6.1.1), that in some situations is an attractive alternative to the conventional rotating disk.
An SSD package plugs into a standard disk slot on the I/O bus (typically USB or SATA) and behaves like any other.
Figure 6.16 Performance characteristics of a typical solid state disk.
An SSD package consists of one or more ﬂash memory chips, which replace the mechanical drive in a conventional rotating disk, and a ﬂash translation layer, which is a hardware/ﬁrmware device that plays the same role as a disk controller, translating requests for logical blocks into accesses of the underlying physical device.
SSDshavedifferent performance characteristics than rotatingdisks.As shown in Figure 6.16, sequential reads and writes (where the CPU accesses logical disk blocks in sequential order) have comparable performance, with sequential reading somewhat faster than sequential writing.
However, when logical blocks are accessed in random order, writing is an order of magnitude slower than reading.
The difference between random reading andwriting performance is caused by a fundamental property of the underlying ﬂash memory.
As shown in Figure 6.15, a ﬂash memory consists of a sequence of B blocks, where each block consists of P pages.
A page can be written only after the entire block to which it belongs has been erased (typically this means that all bits in the block are set to 1)
However, once a block is erased, each page in the block can be written once with no further erasing.
Once a block wears out it can no longer be used.
First, erasing a block takes a relatively long time, on the order of 1 ms, which is more than an order of magnitude longer than it takes to access a page.
Second, if a write operation attempts to modify a page p that contains existing data (i.e., not all ones), then any pages in the same block with useful data must be copied to a new (erased) block before the write to page p can occur.
Manufacturers have developed sophisticated logic in the ﬂash translation layer that attempts to amortize the high cost of erasing blocks and to minimize the number of internal copies on writes, but it is unlikely that random writing will ever perform as well as reading.
They are built of semiconductor memory, with no moving parts, and thus have much faster random access times than rotating disks, use less power, and are more rugged.
First, because ﬂash blocks wear out after repeated writes, SSDs have the potential to wear out as well.Wear leveling logic in the ﬂash translation layer attempts to maximize the lifetime of each block by spreading erasures evenly across all blocks, but the fundamental limit remains.
However, SSD prices are decreasing rapidly as they become more popular, and the gap between the two appears to be decreasing.
SSDs have completely replaced rotating disks in portable music devices, are popular as disk replacements in laptops, and have even begun to appear in desktops and servers.
While rotating disks are here to stay, it is clear that SSDs are an important new storage technology.
Practice Problem 6.6 Aswehave seen, a potential drawbackof SSDs is that the underlying ﬂashmemory can wear out.
Given this assumption, estimate the lifetime (in years) of the SSD in Figure 6.16 for the following workloads:
Worst case for sequential writes: The SSD is written to continuously at a rate of 170 MB/s (the average sequential write throughput of the device)
Worst case for random writes: The SSD is written to continuously at a rate of 14 MB/s (the average random write throughput of the device)
Average case: The SSD is written to at a rate of 20 GB/day (the average daily write rate assumed by some computer manufacturers in their mobile computer workload simulations)
There are several important concepts to take away from our discussion of storage technologies.
On the other hand, fast storage is always more expensive than slower storage.
The price and performance properties of different storage technologies are changing at dramatically different rates.
The numbers were culled from back issues of trade magazines and the Web.
Although they were collected in an informal survey, the numbers reveal some interesting trends.
Since 1980, both the cost and performance of SRAM technology have improved at roughly the same rate.
Disk technology has followed the same trend as DRAM and in even more dramatic fashion.
These startling long-term trends highlight a basic truth of memory and disk technology: it is easier to increase density (and thereby reduce cost) than to decrease access time.
The split in theCPUperformance curve around 2003 reﬂects the introduction ofmulticore processors (see aside on next page)
After this split, cycle times of individual cores actually increased a bit before starting to decrease again, albeit at a slower rate than before.
Note that while SRAM performance lags, it is roughly keeping up.
However, the gap between DRAM and disk performance and CPU performance is actually widening.Until the advent ofmulti-core processors around2003, this performance gap was a function of latency, with DRAM and disk access times increasing more slowly than the cycle time of an individual processor.
However, with the introduction of multiple cores, this performance gap is increasingly a function of throughput, withmultiple processor cores issuing requests to theDRAMand disk in parallel.
Disk seek time SSD write time SSD read time DRAM access time SRAM access time CPU cycle time Effective CPU cycle time.
Figure 6.18 The increasing gap between disk, DRAM, and CPU speeds.
As we will see in Section 6.4, modern computers make heavy use of SRAMbased caches to try to bridge the processor-memory gap.
This approach works because of a fundamental property of application programs known as locality, which we discuss next.
Aside When cycle time stood still: the advent of multi-core processors.
The history of computers is marked by some singular events that caused profound changes in the industry and the world.
The most recent such event occurred early in the 21st century, when computer manufacturers ran headlong into the so-called “power wall,” discovering that they could no longer increase CPU clock frequencies as quickly because the chips would then consume too much power.
The solution was to improve performance by replacing a single large processor with multiple smaller processor cores, each a complete processor capable of executing programs independently and in parallel with the other cores.
This multi-core approach works in part because the power consumed by a processor is proportional to P = fCv2, where f is the clock frequency, C is the capacitance, and v is the voltage.
The capacitance C is roughly proportional to the area, so the power drawn by multiple cores can be held constant as long as the total area of the cores is constant.
As long as feature sizes continue to shrink at the exponentialMoore’s law rate, the number of cores in each processor, and thus its effective performance, will continue to increase.
From this point forward, computers will get faster not because the clock frequency increases, but because the number of cores in eachprocessor increases, andbecause architectural innovations increase the efﬁciency of programs running on those cores.
That is, they tend to reference data items that are near other recently referenced data items, or that were recently referenced themselves.
This tendency, known as the principle of locality, is an enduring concept that has enormous impact on the design and performance of hardware and software systems.
Locality is typically described as having two distinct forms: temporal locality and spatial locality.
In a program with good temporal locality, a memory location that is referenced once is likely to be referenced again multiple times in the near future.
In a program with good spatial locality, if a memory location is referenced once, then the program is likely to reference a nearbymemory location in the near future.
Programmers should understand the principle of locality because, in general, programs with good locality run faster than programs with poor locality.
All levels of modern computer systems, from the hardware, to the operating system, to application programs, are designed to exploit locality.
At the hardware level, the principle of locality allows computer designers to speed upmainmemory accesses by introducing small fast memories known as cache memories that hold blocks of the most recently referenced instructions and data items.
At the operating system level, the principle of locality allows the system to use themainmemory as a cache of the most recently referenced chunks of the virtual address space.
Similarly, the operating system usesmainmemory to cache themost recently used disk blocks in the disk ﬁle system.
The principle of locality also plays a crucial role in the design of application programs.
For example, Web browsers exploit temporal locality by caching recently referenced documents on a local disk.
High-volumeWeb servers hold recently requested documents in front-end disk caches that satisfy requests for these documents without requiring any intervention from the server.
Consider the simple function in Figure 6.19(a) that sums the elements of a vector.
Does this function have good locality? To answer this question, we look at the reference pattern for each variable.
In this example, the sum variable is referenced once in each loop iteration, and thus there is good temporal locality with respect to sum.
On the other hand, since sum is a scalar, there is no spatial locality with respect to sum.
Thus, with respect to variable v, the function has good spatial locality but poor temporal locality since each vector element.
Notice how the vector elements are accessed in the same order that they are stored in memory.
There is good spatial locality because the array is accessed in the same row-major order in which it is stored in memory.
Since the function has either good spatial or temporal locality with respect to each variable in the loop body, we can conclude that the sumvec function enjoys good locality.
A function such as sumvec that visits each element of a vector sequentially is said to have a stride-1 reference pattern (with respect to the element size)
We will sometimes refer to stride-1 reference patterns as sequential reference patterns.
Visiting every kth element of a contiguous vector is called a stride-k reference pattern.
Stride-1 reference patterns are a common and important source of spatial locality in programs.
In general, as the stride increases, the spatial locality decreases.
For example, consider the sumarrayrows function in Figure 6.20(a) that sums the elements of a two-dimensional array.
The doubly nested loop reads the elements of the array in row-major order.
That is, the inner loop reads the elements of the ﬁrst row, then the second row, and so on.
The sumarrayrows function enjoys good spatial locality because it references the array in the same row-major order that the array is stored (Figure 6.20(b))
The result is a nice stride-1 reference pattern with excellent spatial locality.
Seemingly trivial changes to a program can have a big impact on its locality.
The only difference is that we have interchanged the i and j loops.What impact does interchanging the loops have on its locality? The sumarraycols function suffers from poor spatial locality because it scans the array column-wise instead of row-wise.
Since C arrays are laid out in memory row-wise, the result is a stride-N reference pattern, as shown in Figure 6.21(b)
Since program instructions are stored in memory and must be fetched (read) by the CPU, we can also evaluate the locality of a program with respect to its instruction fetches.
For example, in Figure 6.19 the instructions in the body of the.
The function has poor spatial locality because it scans memory with a stride-N reference pattern.
Since the loop body is executed multiple times, it also enjoys good temporal locality.
An important property of code that distinguishes it from program data is that it is rarely modiﬁed at run time.
While a program is executing, the CPU reads its instructions from memory.
In this section, we have introduced the fundamental idea of locality and have identiﬁed some simple rules for qualitatively evaluating the locality in a program:
Programs that repeatedly reference the same variables enjoy good temporal locality.
For programswith stride-k reference patterns, the smaller the stride the better the spatial locality.
Programs that hop around memory with large strides have poor spatial locality.
Loops have good temporal and spatial locality with respect to instruction fetches.
The smaller the loop body and the greater the number of loop iterations, the better the locality.
Later in this chapter, after we have learned about cache memories and how they work, we will show you how to quantify the idea of locality in terms of cache hits and misses.
It will also become clear to you why programs with good locality typically run faster than programswith poor locality.
Rank-order the functions with respect to the spatial locality enjoyed by each.
Storage technology: Different storage technologies have widely different access times.
Faster technologies cost more per byte than slower ones and have less capacity.
The gap between CPU and main memory speed is widening.
In one of the happier coincidences of computing, these fundamental properties of hardware and software complement each other beautifully.
Their complementary nature suggests an approach for organizing memory systems, known as the memory hierarchy, that is used in all modern computer systems.
In general, the storage devices get slower, cheaper, and larger as we move from higher to lower levels.
At the highest level (L0) are a small number of fast CPU registers that the CPU can access in a single clock cycle.
Next are one or more small to moderate-sized SRAM-based cache memories that can be accessed in a few CPU clock cycles.
These are followed by a large DRAMbased main memory that can be accessed in tens to hundreds of clock cycles.
Finally, some systems even include an additional level of disks on remote servers that can be accessed over a network.
For example, distributed ﬁle systems such as the Andrew File System (AFS) or the Network File System (NFS) allow a program to access ﬁles that are stored on remote network-connected servers.
Similarly, theWorldWideWeb allows programs to access remote ﬁles stored on Web servers anywhere in the world.
Local disks hold files retrieved from disks on remote network servers.
We have shown you one example of a memory hierarchy, but other combinations are possible, and indeed common.
For example, many sites back up local disks onto archival magnetic tapes.
At some of these sites, human operators manually mount the tapes onto tape drives as needed.
In either case, the collection of tapes represents a level in the memory hierarchy, below the local disk level, and the same general principles apply.
Tapes are cheaper per byte than disks, which allows sites to archive multiple snapshots of their local disks.
The tradeoff is that tapes take longer to access than disks.
As another example, solid state disks are playing an increasingly important role in the memory hierarchy, bridging the gulf between DRAM and rotating disk.
In general, a cache (pronounced “cash”) is a small, fast storage device that acts as a staging area for the data objects stored in a larger, slower device.
The process of using a cache is known as caching (pronounced “cashing”)
In other words, each level in the hierarchy caches data objects from the next lower level.
For example, the local disk serves as a cache for ﬁles (such as Web pages) retrieved from remote disks over the network, the main memory serves as a cache for data on the local disks, and so on, until we get to the smallest cache of all, the set of CPU registers.
Figure 6.24 The basic principle of caching in a memory hierarchy.
Figure 6.24 shows the general concept of caching in a memory hierarchy.
The storage at level k + 1 is partitioned into contiguous chunks of data objects called blocks.
Each block has a unique address or name that distinguishes it from other blocks.
Blocks can be either ﬁxed-sized (the usual case) or variable-sized (e.g., the remote HTML ﬁles stored on Web servers)
Data is always copied back and forth between level k and level k + 1 in blocksized transfer units.
It is important to realize that while the block size is ﬁxed between any particular pair of adjacent levels in the hierarchy, other pairs of levels can have different block sizes.
In general, devices lower in the hierarchy (further from the CPU) have longer access times, and thus tend to use larger block sizes in order to amortize these longer access times.
When a program needs a particular data object d from level k + 1, it ﬁrst looks for d in one of the blocks currently stored at level k.
If d happens to be cached at level k, then we have what is called a cache hit.
For example, a program with good temporal locality might read a data object from block 14, resulting in a cache hit from level k.
If, on the other hand, the data object d is not cached at level k, then we have what is called a cache miss.
When there is a miss, the cache at level k fetches the block containing d from the cache at level k + 1, possibly overwriting an existing block if the level k cache is already full.
This process of overwriting an existing block is known as replacing or evicting the block.
The block that is evicted is sometimes referred to as a victim block.
The decision about which block to replace is governed by the cache’s replacement policy.
For example, a cache with a random replacement policy would choose a random victim block.
A cache with a least-recently used (LRU) replacement policy would choose the block that was last accessed the furthest in the past.
After the cache at level k has fetched the block from level k + 1, the program can read d from level k as before.
It is sometimes helpful to distinguish between different kinds of cache misses.
If the cache at level k is empty, then any access of any data object will miss.
An empty cache is sometimes referred to as a cold cache, and misses of this kind are called compulsory misses or cold misses.
Cold misses are important because they are often transient events that might not occur in steady state, after the cache has been warmed up by repeated memory accesses.
The most ﬂexible placement policy is to allow any block from level k + 1 to be stored in any block at level k.
For caches high in the memory hierarchy (close to the CPU) that are implemented in hardware and where speed is at a premium, this policy is usually too expensive to implement because randomly placed blocks are expensive to locate.
Restrictive placement policies of this kind lead to a type of miss known as a conﬂict miss, in which the cache is large enough to hold the referenced data objects, but because they map to the same cache block, the cache keeps missing.
Programs often run as a sequence of phases (e.g., loops) where each phase accesses some reasonably constant set of cache blocks.
For example, a nested loop might access the elements of the same array over and over again.
This set of blocks is called the working set of the phase.
When the size of the working set exceeds the size of the cache, the cache will experience what are known as capacity misses.
In other words, the cache is just too small to handle this particular working set.
As we have noted, the essence of the memory hierarchy is that the storage device at each level is a cache for the next lower level.
At each level, some form of logic mustmanage the cache.
By this wemean that something has to partition the cache storage into blocks, transfer blocks between different levels, decidewhen there are hits and misses, and then deal with them.
The logic that manages the cache can be hardware, software, or a combination of the two.
For example, the compiler manages the register ﬁle, the highest level of the cache hierarchy.
It decides when to issue loads when there are misses, and determines which register to store the data in.
In a system with virtual memory, the DRAM main memory serves as a cache for data blocks stored on disk, and is managed by a combination of operating system software and address translation hardware on the CPU.
For a machine with a distributed ﬁle system such as AFS, the local disk serves as a cache that is managed by the AFS client process running on the local machine.
In most cases, caches operate automatically and do not require any speciﬁc or explicit actions from the program.
To summarize, memory hierarchies based on cachingwork because slower storage is cheaper than faster storage and because programs tend to exhibit locality:
Exploiting temporal locality.Because of temporal locality, the same data objects are likely to be reusedmultiple times.Once a data object has been copied into the cache on the ﬁrst miss, we can expect a number of subsequent hits on that object.
Since the cache is faster than the storage at the next lower level, these subsequent hits can be served much faster than the original miss.
Because of spatial locality, we can expect that the cost of copying a block after a miss will be amortized by subsequent references to other objects within that block.
As you can see from Figure 6.25, caches are used in CPU chips, operating systems, distributed ﬁle systems, and on the World Wide Web.
They are built from and managed by various combinations of hardware and software.
Note that there are a number of terms and acronyms in Figure 6.25 that we haven’t covered yet.
We include them here to demonstrate how common caches are.
Network cache Parts of ﬁles Local disk 10,000,000 AFS/NFS client.
Web cache Web pages Remote server disks 1,000,000,000 Web proxy server.
Figure 6.25 The ubiquity of caching in modern computer systems.
The memory hierarchies of early computer systems consisted of only three levels: CPU registers, main DRAM memory, and disk storage.
While there is considerable variety in the arrangements, the general principles are the same.
For our discussion in the next section, we will assume a simple memory hierarchy with a single L1 cache between the CPU and main memory.
When the CPU is instructed by a load instruction to read a word from addressA ofmainmemory, it sends the addressA to the cache.
If the cache is holding a copy of the word at address A, it sends the word immediately back to the CPU.
Figure 6.27 General organization of cache (S, E, B, m)
Each line contains a valid bit, some tag bits, and a block of data.
So how does the cache know whether it contains a copy of the word at address A? The cache is organized so that it can ﬁnd the requested word by simply inspecting the bits of the address, similar to a hash table with an extremely simple hash function.
The parameters S and B induce a partitioning of the m address bits into the three ﬁelds shown in Figure 6.27(b)
The s set index bits in A form an index into the array of S sets.
When interpreted as an unsigned integer, the set index bits tell us which set the word must be stored in.
Once we know which set the word must be contained in, the t tag bits in A tell us which line (if any) in the set contains the word.
A line in the set contains the word if and only if the valid bit is set and the tag bits in the line match the tag bits in the address A.
Once we have located the line identiﬁed by the tag in the set identiﬁed by the set index, then the b block offset bits give us the offset of the word in the B-byte data block.
As you may have noticed, descriptions of caches use a lot of symbols.
Practice Problem 6.10 The following table gives the parameters for a number of different caches.
For each cache, determine the number of cache sets (S), tag bits (t), set index bits (s), and block offset bits (b)
Caches are grouped into different classes based on E, the number of cache lines per set.
Direct-mapped caches are the simplest both to implement and to understand, so we will use them to illustrate some general concepts about how caches work.
Suppose we have a system with a CPU, a register ﬁle, an L1 cache, and a main memory.
When the CPU executes an instruction that reads a memory word w, it requests the word from the L1 cache.
Otherwise, we have a cache miss, and the CPU must wait while the L1 cache requests a copy of the block containing w from the main memory.
When the requested block ﬁnally arrives from memory, the L1 cache stores the block in one of its cache lines, extracts word w from the stored block, and returns it to the CPU.
In this step, the cache extracts the s set index bits from the middle of the address for w.
These bits are interpreted as an unsigned integer that corresponds to a set number.
In other words, if we think of the cache as a one-dimensional array of sets, then the set index bits form an index into this array.
Figure 6.30 shows how set selection works for a direct-mapped cache.
Now that we have selected some set i in the previous step, the next step is to determine if a copy of the word w is stored in one of the cache lines contained in.
In a direct-mapped cache, this is easy and fast because there is exactly one line per set.
A copy of w is contained in the line if and only if the valid bit is set and the tag in the cache line matches the tag in the address of w.
Figure 6.31 shows how line matching works in a direct-mapped cache.
In this example, there is exactly one cache line in the selected set.
The valid bit for this line is set, so we know that the bits in the tag and block are meaningful.
Since the tag bits in the cache line match the tag bits in the address, we know that a copy of the word we want is indeed stored in the line.
On the other hand, if either the valid bit were not set or the tags did not match, then we would have had a cache miss.
Once we have a hit, we know that w is somewhere in the block.
This last step determines where the desired word starts in the block.
As shown in Figure 6.31, the block offset bits provide us with the offset of the ﬁrst byte in the desired word.
Similar to our view of a cache as an array of lines, we can think of a block as an array of bytes, and the byte offset as an index into that array.
If the cache misses, then it needs to retrieve the requested block from the next level in the memory hierarchy and store the new block in one of the cache lines of.
Figure 6.31 Line matching and word selection in a directmapped cache.
In general, if the set is full of valid cache lines, then one of the existing lines must be evicted.
For a direct-mapped cache, where each set contains exactly one line, the replacement policy is trivial: the current line is replaced by the newly fetched line.
The mechanisms that a cache uses to select sets and identify lines are extremely simple.
They have to be, because the hardware must perform them in a few nanoseconds.
However, manipulating bits in this way can be confusing to us humans.
We will also assume that each word is a single byte.
Of course, these assumptions are totally unrealistic, but they will help us keep the example simple.
There are some interesting things to notice about this enumerated space:
The concatenation of the tag and index bits uniquely identiﬁes each block in memory.
Since there are eight memory blocks but only four cache sets, multiple blocks map to the same cache set (i.e., they have the same set index)
Blocks that map to the same cache set are uniquely identiﬁed by the tag.
Let us simulate the cache in action as the CPU performs a sequence of reads.
Remember that for this example, we are assuming that the CPU reads 1-byte words.
While this kind of manual simulation is tedious and you may be tempted to skip it, in our experience students do not really understand how caches work until they work their way through a few of them.
Initially, the cache is empty (i.e., each valid bit is zero):
The ﬁrst column indicates the set that the line belongs to, but keep inmind that this is provided for convenience and is not really part of the cache.
The next three columns represent the actual bits in each cache line.
Now, let us see what happens when the CPU performs a sequence of reads:
Since the valid bit for set 0 is zero, this is a cachemiss.
Since the cache line in set 2 is not valid, this is a cache miss.
The cache line in set 0 is indeed valid, but the tags do not match.
This kind of miss, where we have plenty of room in the cache but keep alternating references to blocks that map to the same set, is an example of a conﬂict miss.
Conﬂict misses are common in real programs and can cause bafﬂing performance problems.
For example, consider a function that computes the dot product of two vectors:
This function has good spatial locality with respect to x and y, and so we might expect it to enjoy a good number of cache hits.
We will assume that the variable sum is actually stored in a CPU register and thus does not require a memory reference.
Given these assumptions, each x[i] and y[i] will map to the identical cache set:
So now we have a conﬂict miss, and in fact each subsequent reference to x and ywill result in a conﬂict miss as we thrash back and forth between blocks of x and y.
The term thrashing describes any situation where a cache is repeatedly loading and evicting the same sets of cache blocks.
The bottom line is that even though the program has good spatial locality and we have room in the cache to hold the blocks for both x[i] and y[i], each reference results in a conﬂict miss because the blocksmap to the same cache set.
Also, be aware that even though our example is extremely simple, the problem is real for larger and more realistic direct-mapped caches.
Luckily, thrashing is easy for programmers to ﬁx once they recognize what is going on.
One easy solution is to put B bytes of padding at the end of each array.
Assuming y starts immediately after x in memory, we have the following mapping of array elements to sets:
With the padding at the end of x, x[i] and y[i] now map to different sets, which eliminates the thrashing conﬂict misses.
Practice Problem 6.11 In the previous dotprod example, what fraction of the total references to x and y will be hits once we have padded array x?
Practice Problem 6.12 In general, if the high-order s bits of an address are used as the set index, contiguous chunks of memory blocks are mapped to the same cache set.
How many blocks are in each of these contiguous array chunks?
What is the maximum number of array blocks that are stored in the cache at any point in time?
You may be wondering why caches use the middle bits for the set index instead of the high-order bits.
There is a good reason why the middle bits are better.
If the high-order bits are used as an index, then some contiguous memory blocks will map to the same cache set.
For example, in the ﬁgure, the ﬁrst four blocks map to the ﬁrst cache set, the second four blocks map to the second set, and so on.
If a program has good spatial locality and scans the elements of an array sequentially, then the cache can only hold a block-sized chunk of the array at any point in time.
Contrast this with middle-bit indexing, where adjacent blocks always map to different cache lines.
In this case, the cache can hold an entire C-sized chunk of the array, where C is the cache size.
The problem with conﬂict misses in direct-mapped caches stems from the constraint that each set has exactly one line (or in our terminology, E = 1)
A set associative cache relaxes this constraint so each set holds more than one cache line.
We will discuss the special case, whereE = C/B, in the next section.
Figure 6.34 shows the organization of a two-way set associative cache.
In a set associative cache, each set contains more than one line.
Set selection is identical to a direct-mapped cache, with the set index bits identifying the set.
Line matching is more involved in a set associative cache than in a direct-mapped cache because it must check the tags and valid bits of multiple lines in order to determine if the requestedword is in the set.A conventionalmemory is an array of values that takes an address as input and returns the value stored at that address.
An associative memory, on the other hand, is an array of (key, value) pairs that takes as input the key and returns a value from one of the (key, value) pairs that matches the input key.
Thus, we can think of each set in a set associative cache as a small associative memory where the keys are the concatenation of the tag and valid bits, and the values are the contents of a block.
Figure 6.36 shows the basic idea of line matching in an associative cache.
An important idea here is that any line in the set can contain any of thememory blocks.
Figure 6.36 Line matching and word selection in a set associative cache.
So the cache must search each line in the set, searching for a valid line whose tag matches the tag in the address.
If the cache ﬁnds such a line, then we have a hit and the block offset selects a word from the block, as before.
If the word requested by the CPU is not stored in any of the lines in the set, then we have a cache miss, and the cache must fetch the block that contains the word frommemory.
However, once the cache has retrieved the block, which line should it replace? Of course, if there is an empty line, then it would be a good candidate.
But if there are no empty lines in the set, thenwemust choose one of the nonempty lines and hope that the CPU does not reference the replaced line anytime soon.
It is very difﬁcult for programmers to exploit knowledge of the cache replacement policy in their codes, so we will not go into much detail about it here.
The simplest replacement policy is to choose the line to replace at random.Othermore sophisticated policies draw on the principle of locality to try tominimize the probability that the replaced line will be referenced in the near future.
But as we move further down the memory hierarchy, away from the CPU, the cost of a miss becomes more expensive and it becomes more worthwhile to minimize misses with good replacement policies.
A fully associative cache consists of a single set (i.e., E = C/B) that contains all of the cache lines.
Set selection in a fully associative cache is trivial because there is only one set, summarized in Figure 6.38
Notice that there are no set index bits in the address, which is partitioned into only a tag and a block offset.
Line matching and word selection in a fully associative cache work the same as with a set associative cache, as we show in Figure 6.39
In a fully associative cache, a single set contains all of the lines.
The entire cache is one set, so by default set 0 is always selected.
Figure 6.39 Line matching and word selection in a fully associative cache.
As a result, fully associative caches are only appropriate for small caches, such as the translation lookaside buffers (TLBs) in virtual memory systems that cache page table entries (Section 9.6.2)
Practice Problem 6.13 The problems that follow will help reinforce your understanding of how caches work.
The contents of the cache are as follows, with all numbers given in hexadecimal notation.
The following ﬁgure shows the format of an address (one bit per box)
Indicate (by labeling the diagram) the ﬁelds that would be used to determine the following:
Indicate the cache entry accessed and the cache byte value returned in hex.
If there is a cache miss, enter “–” for “Cache byte returned.”
Aswe have seen, the operation of a cache with respect to reads is straightforward.
First, look for a copy of the desired word w in the cache.
If there is a miss, fetch the block that contains w from the next lower level of the memory hierarchy, store the block in some cache line (possibly evicting a valid line), and then return w.
Suppose we write a word w that is already cached (a write hit)
After the cache updates its copy of w, what does it do about updating the copy of w in the next lower level of the hierarchy? The simplest approach, known aswrite-through, is to immediately writew’s cache block to the next lower level.
While simple, write-through has the disadvantage of causing bus trafﬁc with every write.
The cachemust maintain an additional dirty bit for each cache line that indicates whether or not the cache block has been modiﬁed.
One approach, known aswriteallocate, loads the corresponding block from the next lower level into the cache and then updates the cache block.
Write-allocate tries to exploit spatial locality of writes, but it has the disadvantage that every miss results in a block transfer from the next lower level to cache.
The alternative, known as no-write-allocate, bypasses the cache and writes the word directly to the next lower level.
Optimizing caches for writes is a subtle and difﬁcult issue, and we are only scratching the surface here.
The details vary from system to system and are often proprietary and poorly documented.
To the programmer trying to write reasonably cache-friendly programs, we suggest adopting a mental model that assumes write-back write-allocate caches.
As a rule, caches at lower levels of the memory hierarchy are more likely to use write-back instead of write-through because of the larger transfer times.
For example, virtual memory systems (which use main memory as a cache for the blocks stored on disk) use write-back exclusively.
But as logic densities increase, the increased complexity of write-back is becoming less of an impediment and we are seeing write-back caches at all levels of modern systems.
Another reason for assuming a write-back write-allocate approach is that it is symmetric to the way reads are handled, in that write-back write-allocate tries to exploit locality.
Thus, we can develop our programs at a high level to exhibit good spatial and temporal locality rather than trying to optimize for a particular memory system.
So far, we have assumed that caches hold only program data.
But in fact, caches can hold instructions as well as data.
A cache that holds instructions only is called an i-cache.
A cache that holds program data only is called a d-cache.
A cache that holds both instructions and data is known as a uniﬁed cache.
With two separate caches, the processor can read an instruction word and a data word at the same time.
The two caches are often optimized to different access patterns and can have different block sizes, associativities, and capacities.
Also, having separate caches ensures that data accesses do not create conﬂict misses with instruction accesses, and vice versa, at the cost of a potential increase in capacity misses.
The fraction of memory references during the execution of a program, or a part of a program, that miss.
The time to deliver a word in the cache to the CPU, including the time for set selection, line identiﬁcation, and word selection.
Hit time is on the order of several clock cycles for L1 caches.
Optimizing the cost and performance trade-offs of cache memories is a subtle exercise that requires extensive simulation on realistic benchmark codes and thus is beyond our scope.
However, it is possible to identify some of the qualitative trade-offs.
On the one hand, a larger cache will tend to increase the hit rate.
On the other hand, it is always harder to make large memories run faster.
As a result, larger caches tend to increase the hit time.
This is especially important for on-chip L1 caches that must have a short hit time.
On the one hand, larger blocks can help increase the hit rate by exploiting any spatial locality thatmight exist in a program.
However, for a given cache size, larger blocks imply a smaller number of cache lines, which can hurt the hit rate in programs with more temporal locality than spatial locality.
Larger blocks also have a negative impact on themiss penalty, since larger blocks cause larger transfer times.
The issue here is the impact of the choice of the parameter E, the number of cache lines per set.
The advantage of higher associativity (i.e., larger values of E) is that it decreases the vulnerability of the cache to thrashing due to conﬂictmisses.
Higher associativity is expensive to implement and hard to make fast.
It requires more tag bits per line, additional LRU state bits per line, and additional control logic.
Higher associativity can increase hit time, because of the increased complexity, and it can also increase the miss penalty because of the increased complexity of choosing a victim line.
The choice of associativity ultimately boils down to a trade-off between the hit time and themiss penalty.
Traditionally, high-performance systems that pushed the clock rates would opt for smaller associativity for L1 caches (where the miss penalty is only a few cycles) and a higher degree of associativity for the lower levels, where the miss penalty is higher.
Write-through caches are simpler to implement and can use a write buffer that works independently of the cache to update memory.
Furthermore, read misses are less expensive because they do not trigger a memory write.
On the other hand, write-back caches result in fewer transfers, which allows more bandwidth to memory for I/O devices that perform DMA.
Further, reducing the number of transfers becomes increasingly important as we move down the hierarchy and the transfer times increase.
In general, caches further down the hierarchy are more likely to use write-back than write-through.
It is easy to confuse the distinction between cache lines, sets, and blocks.
Let’s review these ideas and make sure they are clear:
A block is a ﬁxed-sized packet of information that moves back and forth between a cache andmain memory (or a lower-level cache)
A line is a container in a cache that stores a block, as well as other information such as the valid bit and the tag bits.
A set is a collection of one or more lines.
Sets in set associative and fully associative caches consist of multiple lines.
However, in associative caches, sets and lines are very different things and the terms cannot be used interchangeably.
Since a line always stores a single block, the terms “line” and “block” are often used interchangeably.
For example, systems professionals usually refer to the “line size” of a cache, when what they really mean is the block size.
This usage is very common, and shouldn’t cause any confusion, so long as you understand the distinction between blocks and lines.
In Section 6.2, we introduced the idea of locality and talked in qualitative terms about what constitutes good locality.
Now that we understand how cache memories work, we can be more precise.
Programs with better locality will tend to have lower miss rates, and programs with lower miss rates will tend to run faster than programs with higher miss rates.
Here is the basic approach we use to try to ensure that our code is cache friendly.
Make the common case go fast.Programs often spend most of their time in a few core functions.
These functions often spend most of their time in a few loops.
So focus on the inner loops of the core functions and ignore the rest.
Minimize the number of cachemisses in each inner loop.All other things being equal, such as the total number of loads and stores, loopswith bettermiss rates will run faster.
To see how this works in practice, consider the sumvec function from Section 6.2:
The reference to v[4] causes another miss as a new block is loaded into the cache, the next three references are hits, and so on.
In general, three out of four references will hit, which is the best we can do in this case with a cold cache.
To summarize, our simple sumvec example illustrates two important points about writing cache-friendly code:
Repeated references to local variables are good because the compiler can cache them in the register ﬁle (temporal locality)
Stride-1 referencepatterns are goodbecause caches at all levels of thememory hierarchy store data as contiguous blocks (spatial locality)
Spatial locality is especially important in programs that operate on multidimensional arrays.
For example, consider the sumarrayrows function from Section 6.2, which sums the elements of a two-dimensional array in row-major order:
Since C stores arrays in row-major order, the inner loop of this function has the same desirable stride-1 access pattern as sumvec.
For example, suppose we make the same assumptions about the cache as for sumvec.
Then the references to the array a will result in the following pattern of hits and misses:
But consider what happens if we make the seemingly innocuous change of permuting the loops:
In this case, we are scanning the array column by column instead of row by row.
If we are lucky and the entire array ﬁts in the cache, then we will enjoy the same miss rate of 1/4
However, if the array is larger than the cache (the more likely case), then each and every access of a[i][j] will miss!
Higher miss rates can have a signiﬁcant impact on running time.
For example, on our desktop machine, sumarrayrows runs twice as fast as sumarraycols.
To summarize, programmers should be aware of locality in their programs and try to write programs that exploit it.
Practice Problem 6.18 Transposing the rows and columns of a matrix is an important problem in signal processing and scientiﬁc computing applications.
It is also interesting from a locality point of view because its reference pattern is both row-wise and column-wise.
Assume this code runs on a machine with the following properties:
The cache has a total size of 16 data bytes and the cache is initially empty.
Accesses to the src and dst arrays are the only sources of read and write misses, respectively.
For each row and col, indicate whether the access to src[row][col] and dst[row][col] is a hit (h) or a miss (m)
Repeat the problem for a cache with 32 data bytes.
The only memory accesses are to the entries of the array grid.
What is the total number of reads that miss in the cache?
What is the total number of reads that miss in the cache?
What would the miss rate be if the cache were twice as big?
What is the total number of reads that miss in the cache?
What would the miss rate be if the cache were twice as big?
This section wraps up our discussion of the memory hierarchy by studying the impact that caches have on the performance of programs running on real machines.
The rate that a program reads data from the memory system is called the read throughput, or sometimes the read bandwidth.
If a program reads n bytes over a period of s seconds, then the read throughput over that period is n/s, typically expressed in units of megabytes per second (MB/s)
If we were to write a program that issued a sequence of read requests from a tight program loop, then the measured read throughput would give us some insight into the performance of the memory system for that particular sequence of reads.
Figure 6.42 shows a pair of functions that measure the read throughput for a particular read sequence.
The test function generates the read sequence by scanning the ﬁrst elems elements of an array with a stride of stride.
The run function is a wrapper that calls the test function and returns the measured read throughput.
The call to the test function in line 29 warms the cache.
Notice that the size argument to the run function is in units of bytes, while the corresponding elems argument to the test function is in units of array elements.
The size and stride arguments to the run function allow us to control the degree of temporal and spatial locality in the resulting read sequence.
Smaller values of size result in a smaller working set size, and thus better temporal locality.
If we call the run function repeatedly with different values of size and stride, then we can recover a fascinating two-dimensional function of read throughput versus temporal and spatial locality.
Every computer has a unique memory mountain that characterizes the capabilities of its memory system.
There is a feature of the L1 ridge that should be pointed out.
For very large strides, notice how the read throughput drops as the working set size approaches 2 KB (falling off the back side of the ridge)
It is an artifact of overheads of calling thetest function and settingup to execute the loop.
For large strides in small working set sizes, these overheads are not amortized, as they are with the larger sizes.
Figure 6.42 Functions that measure and compute read throughput.We can generate a memory mountain for a particular computer by calling the run function with different values of size (which corresponds to temporal locality) and stride (which corresponds to spatial locality)
Notice that even when the working set is too large to ﬁt in any of the caches, the highest point on themainmemory ridge is a factor of 7 higher than its lowest point.
So even when a program has poor temporal locality, spatial locality can still come to the rescue and make a signiﬁcant difference.
This is apparently due to a hardware prefetching mechanism in the Core i7 memory system that automatically identiﬁes memory referencing patterns and attempts to fetch those blocks into cache before they are accessed.
While the details of the particular prefetching algorithm are not documented, it is clear from thememorymountain that the algorithmworks best for small stridesyet another reason to favor sequential accesses in your code.
If we take a slice through the mountain, holding the stride constant as in Figure 6.44, we can see the impact of cache size and temporal locality on performance.
The only way to be sure is to perform a detailed cache simulation, but it.
Figure 6.44 Ridges of temporal locality in the memory mountain.
Slicing through the memory mountain in the opposite direction, holding the working set size constant, gives us some insight into the impact of spatial locality on the read throughput.
Notice how the read throughput decreases steadily as the stride increases from one to eight doublewords.
This is followed by some number of hits on the block in L2, depending on the stride.
Since misses are served more slowly than hits, the read throughput decreases.
To summarize our discussionof thememorymountain, the performanceof the memory system is not characterized by a single number.
Instead, it is a mountain of temporal and spatial locality whose elevations can vary by over an order of magnitude.
Wise programmers try to structure their programs so that they run in the peaks instead of the valleys.
Amatrixmultiply function is usually implemented using three nested loops, which are identiﬁed by their indexes i, j , and k.
If we permute the loops and make some other minor code changes, we can create the six functionally equivalent versions.
Each version is uniquely identiﬁed by the ordering of its loops.
Each version is uniquely identiﬁed by the ordering of its loops.
At a high level, the six versions are quite similar.
As we learned in Chapter 2, ﬂoating-point addition is commutative, but in general not associative.
In practice, if the matrices do not mix extremely large values with extremely small ones, as often is true when the matrices store physical properties, then the assumption of associativity is reasonable.
The six versions partition into three equivalence classes, denoted by the pair of arrays that are accessed in the inner loop.
Eachof then2 elements of A and B is read n times.
Each of the n2 elements of C is computed by summing n values.
However, if we analyze the behavior of the innermost loop iterations, we ﬁnd that there are differences in the number of accesses and the locality.
For the purposes of this analysis, we make the following assumptions:
The array size n is so large that a single matrix row does not ﬁt in the L1 cache.
The compiler stores local variables in registers, and thus references to local variables inside loops do not require any load or store instructions.
Figure 6.47 summarizes the results of our inner loop analysis.
Notice that the six versions pair up into three equivalence classes, which we denote by the pair of matrices that are accessed in the inner loop.
For example, versions ijk and jik are members of Class AB because they reference arrays A and B (but not C) in their innermost loop.
For each class, we have counted the number of loads (reads) and stores (writes) in each inner loop iteration, the number of references to A, B, and C that will miss in the cache in each loop iteration, and the total number of cache misses per iteration.
Since each cache block holds four doublewords, the miss rate forA is 0.25 misses per iteration.
On the other hand, the inner loop scans a column of B with a stride of n.
Since n is large, each access of array B results in a miss, for a total of 1.25 misses per iteration.
The inner loops in the Class AC routines (Figure 6.46(c) and (d)) have some problems.
Each iteration performs two loads and a store (as opposed to the Class AB routines, which perform two loads and no stores)
Second, the inner loop scans the columns of A and C with a stride of n.
The result is a miss on each load, for a total of two misses per iteration.
Notice that interchanging the loops has decreased the amount of spatial locality compared to the Class AB routines.
TheBC routines (Figure 6.46(e) and (f)) present an interesting trade-off:With two loads and a store, they require one more memory operation than the AB routines.
On the other hand, since the inner loop scans both B and C row-wise.
The graph plots the measured number of CPU cycles per inner loop iteration as a function of array size (n)
There are a number of interesting points to notice about this graph:
For large values of n, the fastest version runs almost 20 times faster than the slowest version, even though eachperforms the samenumber of ﬂoating-point arithmetic operations.
Pairs of versions with the same number of memory references and misses per iteration have almost identical measured performance.
The two versions with the worst memory behavior, in terms of the number of accesses and misses per iteration, run signiﬁcantly slower than the other four versions, which have fewer misses or fewer accesses, or both.
Miss rate, in this case, is a better predictor of performance than the total number of memory accesses.
For large values of n, the performance of the fastest pair of versions (kij and ikj) is constant.
Even though the array is much larger than any of the SRAM cache memories, the prefetching hardware is smart enough to recognize the stride-1 access pattern, and fast enough to keep up with memory accesses in the tight inner loop.
There is an interesting technique called blocking that can improve the temporal locality of inner loops.
The general idea of blocking is to organize the data structures in a program into large chunks called blocks.
In this context, “block” refers to an application-level chunk of data, not to a cache block.
The program is structured so that it loads a chunk into the L1 cache, does all the reads and writes that it needs to on that chunk, then discards the chunk, loads in the next chunk, and so on.
Unlike the simple loop transformations for improving spatial locality, blocking makes the code harder to read and understand.
For this reason, it is best suited for optimizing compilers or frequently executed library routines.
Still, the technique is interesting to study and understand because it is a general concept that can produce big performance gains on some systems.
As we have seen, the memory system is organized as a hierarchy of storage devices, with smaller, faster devices toward the top and larger, slower devices toward the bottom.
Because of this hierarchy, the effective rate that a program can access memory locations is not characterized by a single number.
Rather, it is a wildly varying function of program locality (what we have dubbed the memory mountain) that can vary by orders of magnitude.
Programs with good locality access most of their data from fast cache memories.
Programs with poor locality access most of their data from the relatively slow DRAM main memory.
Programmers who understand the nature of the memory hierarchy can exploit this understanding towritemore efﬁcient programs, regardless of the speciﬁc memory system organization.
Focus your attention on the inner loops, where the bulk of the computations and memory accesses occur.
Try to maximize the spatial locality in your programs by reading data objects sequentially, with stride 1, in the order they are stored in memory.
Try to maximize the temporal locality in your programs by using a data object as often as possible once it has been read from memory.
Static RAM (SRAM) is faster and more expensive, and is used for cache memories both on and off the CPU chip.
Dynamic RAM (DRAM) is slower and less expensive, and is used for the main memory and graphics frame buffers.
Nonvolatile memories, also called read-only memories (ROMs), retain their information even if the supply voltage is turned off, and they are used to store ﬁrmware.
Solid state disks (SSDs) based on nonvolatile ﬂash memory are becoming increasingly attractive alternatives to rotating disks for some applications.
In general, faster storage technologies are more expensive per bit and have smaller capacities.
The price and performance properties of these technologies are changing at dramatically different rates.
In particular, DRAM and disk access times are much larger than CPU cycle times.
Systems bridge these gaps by organizing memory as a hierarchy of storage devices, with smaller, faster devices at the top and larger, slower devices at the bottom.
Because well-written programs have good locality, most data are served from the higher levels, and the effect is a memory system that runs at the rate of the higher levels, but at the cost and capacity of the lower levels.
Programmers can dramatically improve the running times of their programs by writing programs with good spatial and temporal locality.
Programs that fetch data primarily from cache memories can run much faster than programs that fetch data primarily from memory.
In our experience, the best sources of technical information are the Web pages maintained by the manufacturers.
Companies such as Micron, Toshiba, and Samsung provide a wealth of current technical information on memory devices.
The pages for Seagate, Maxtor, and Western Digital provide similarly useful information about disks.
Hennessy and Patterson provide a comprehensive discussion of cache design issues [49]
Stricker introduced the idea of the memory mountain as a comprehensive characterization of the memory system in [111], and suggested the term “memory mountain” informally in later presentations of the work.
Carter and colleagues have proposed a cache-aware memory controller [18]
There is a large body of literature on building and using disk storage.
Systems such as Exokernel provide increased user-level control of disk and memory resources [55]
Schindler andGanger developed an interesting tool that automatically characterizes the geometry and performance of SCSI disk drives [92]
For each case below, suppose that a program reads the logical blocks of the ﬁle sequentially, one after the other, and that the time to position the head over the ﬁrst block is Tavg seek + Tavg rotation.
Best case: Estimate the optimal time (in ms) required to read the ﬁle over all possible mappings of logical blocks to disk sectors.
Random case: Estimate the time (in ms) required to read the ﬁle if blocks are mapped randomly to disk sectors.
The contents of the cache are as follows, with all addresses, tags, and values given in hexadecimal notation:
The following diagram shows the format of an address (one bit per box)
Indicate (by labeling the diagram) the ﬁelds that would be used to determine the following:
For each of the following memory accesses indicate if it will be a cache hit or miss when carried out in sequence as listed.
Also give the value of a read if it can be inferred from the information in the cache.
The Index column contains the set index for each set of four lines.
The Tag columns contain the tag value for each line.
The box that follows shows the format of an address (one bit per box)
Indicate (by labeling the diagram) the ﬁelds that would be used to determine the following:
Assume this code runs on a machine with the following properties:
The cache has a total size of 32 data bytes and the cache is initially empty.
Accesses to the src and dst arrays are the only sources of read and write misses, respectively.
For each row and col, indicate whether the access to src[row][col] and dst[row][col] is a hit (h) or a miss (m)
Array x begins at memory address 0x0 and is stored in row-major order.
The onlymemory accesses are to the entries of the array x.
Given these assumptions, estimate the miss rates for the following cases:
For Case 3, will a larger cache size help to reduce the miss rate?Why or why not?
For Case 3, will a larger block size help to reduce the miss rate?Why or why not?
Within the two loops, the code uses memory accesses only for the array data.
The loop indices and the value sum are held in registers.
The only memory accesses are to the entries of the array square.
What is the total number of writes that miss in the cache?
What is the total number of writes that miss in the cache?
What is the total number of writes that miss in the cache?
The only memory accesses are to the entries of the array buffer.
Variables i, j, cptr, and iptr are stored in registers.
What percentage of writes in the following code will miss in the cache?
Your job is to devise a transpose routine that runs as fast as possible.
Your job is to devise a conversion routine that runs as fast as possible.
In other words, the squarer the array, the fewer the address bits.
Putting it all together, the total estimated access time is.
First we need to determine a few basic properties of the ﬁle and the disk.
You can see now why it’s often a good idea to defragment your disk drive!
Then the following straightforward translation of units yields the following predicted times for each case:
Make sure you understand why this particular loop permutation results in a stride-1 access pattern.
Function clear3 not only hops around within each struct, but it also hops from struct to struct.
Not very exciting, but you need to understand how the cache organization induces these partitions in the address bits before you can really understand how caches work.
Here, the bad idea we are looking at is indexing the cache with the high-order bits instead of the middle bits.
With high-order bit indexing, each contiguous array chunk consists of 2t.
Clearly, using high-order bit indexing makes poor use of the cache.
Since there is only one valid line in the set, four addresses will hit.
Thus, the four hex addresses that hit in set 3 are.
The key to solving this problem is to visualize the picture in Figure 6.50
Notice that each cache line holds exactly one row of the array, that the cache is exactly large enough to hold one array, and that for all i, row i of src and dstmaps to the same cache line.
Because the cache is too small to hold both arrays, references to one arraykeep evicting useful lines from theother array.
So when we next read src[0][1], we have a miss.
When the cache is 32 bytes, it is large enough to hold both arrays.
Each loop visits these structures in memory order, reading one integer element each time.
So the pattern for each loop is miss, hit, miss, hit, and so on.
Notice that for this problem we could have predicted the miss rate without actually enumerating the total number of reads and misses.
What is the total number of read accesses? 512 reads.
What is the total number of read accesses that miss in the cache? 256 misses.
So the column-wise scan of the second half of the array evicts the lines that were loaded during the scan of the ﬁrst half.
So when we begin scanning the next column, the reference to the ﬁrst element of grid[0][1] misses.
What is the total number of read accesses? 512 reads.
What is the total number of read accesses that miss in the cache? 256 misses.
What would themiss rate be if the cache were twice as big? If the cache were.
What is the total number of read accesses? 512 reads.
What is the total number of read accesses that miss in the cache? 128 misses.
What would the miss rate be if the cache were twice as big? Increasing the cache size by any amount would not change the miss rate, since cold misses are unavoidable.
Our exploration of computer systems continues with a closer lookat the systems software that builds and runs application programs.The linker combines different parts of our programs into a single ﬁle that can be loaded into memory and executed by the processor.
Modern operating systems cooperate with the hardware to provide each program with the illusion that it has exclusive use of a processor and the main memory, when in reality, multiple programs are running on the system at any point in time.
In the ﬁrst part of this book, you developed a good understanding of the interaction between your programs and the hardware.
Part II of the book will broaden your view of systems by giving you a solid understanding of the interactions between your programs and the operating system.
You will learn how to use services provided by the operating system to build system-level programs such as Unix shells and dynamic memory allocation packages.
Linking is the process of collecting and combining various pieces of code and data into a single ﬁle that can be loaded (copied) into memory and executed.
Linking can be performed at compile time, when the source code is translated into machine code; at load time, when the program is loaded into memory and executed by the loader; and even at run time, by application programs.
On modern systems, linking is performed automatically by programs called linkers.
Linkers play a crucial role in software development because they enable separate compilation.
Instead of organizing a large application as one monolithic source ﬁle, we can decompose it into smaller, more manageable modules that can be modiﬁed and compiled separately.
When we change one of these modules, we simply recompile it and relink the application, without having to recompile the other ﬁles.
Linking is usually handled quietly by the linker, and is not an important issue for students who are building small programs in introductory programming classes.
Understanding linkers will help you avoid dangerous programming errors.The decisions that Unix linkers make when they resolve symbol references can silently affect the correctness of your programs.
Programs that incorrectly deﬁne multiple global variables pass through the linker without any warnings in the default case.
The resulting programs can exhibit bafﬂing run-time behavior and are extremely difﬁcult to debug.
We will show you how this happens and how to avoid it.
Understanding linking will help you understand how language scoping rules are implemented.For example, what is the difference between global and local variables? What does it really mean when you deﬁne a variable or function with the static attribute?
Understanding linking will help you understand other important systems concepts.The executable object ﬁles produced by linkers play key roles in important systems functions such as loading and running programs, virtual memory, paging, and memory mapping.
For many years, linking was considered to be fairly straightforward and uninteresting.
However, with the increased importance of shared libraries and dynamic linking in modern operating systems, linking is a sophisticated process that provides the knowledgeable programmer with signiﬁcant power.
For example, many software products use shared libraries to upgrade shrink-wrapped binaries at run time.Also, mostWeb servers rely on dynamic linking of shared libraries to serve dynamic content.
This chapter provides a thorough discussion of all aspects of linking, from traditional static linking, to dynamic linking of shared libraries at load time, to dynamic linking of shared libraries at run time.
We will describe the basic mechanisms using real examples, and we will identify situations in which linking issues can affect the performance and correctness of your programs.
To keep things concrete and understandable, we will couch our discussion in the context of an x86 system running Linux and using the standard ELF object ﬁle format.
Function main() calls swap, which swaps the two elements in the external global array buf.
Granted, this is a strange way to swap two numbers, but it will serve as a small running example throughout this chapter that will allow us to make some important points about how linking works.
Most compilation systems provide a compiler driver that invokes the language preprocessor, compiler, assembler, and linker, as needed on behalf of the user.
For example, to build the example program using the GNU compilation system, we might invoke the gcc driver by typing the following command to the shell:
Figure 7.2 summarizes the activities of the driver as it translates the example program from an ASCII source ﬁle into an executable object ﬁle.
If you want to see these steps for yourself, run gcc with the -v option.
The driver ﬁrst runs the C preprocessor (cpp), which translates the C source ﬁle main.c into anASCII intermediate ﬁle main.i:
Next, the driver runs the C compiler (cc1), which translates main.i into anASCII assembly language ﬁle main.s.
Then, thedriver runs the assembler (as), which translatesmain.s into a relocatable object ﬁle main.o:
The main function initializes a two-element array of ints, and then calls the swap function to swap the pair.
The driver goes through the same process to generate swap.o.
Finally, it runs the linker program ld, which combines main.o and swap.o, along with the necessary system object ﬁles, to create the executable object ﬁle p:
To run the executable p, we type its name on the Unix shell’s command line:
The shell invokes a function in the operating systemcalled the loader, which copies the code and data in the executable ﬁle p into memory, and then transfers control to the beginning of the program.
Static linkers such as the Unix ld program take as input a collection of relocatable object ﬁles and command-line arguments and generate as output a fully linked executable object ﬁle that can be loaded and run.
The input relocatable object ﬁles consist of various code and data sections.
Instructions are in one section, initialized global variables are in another section, and uninitialized variables are in yet another section.
To build the executable, the linker must perform two main tasks:
The purpose of symbol resolution is to associate each symbol reference with exactly one symbol deﬁnition.
The linker relocates these sections by associating a memory location with each symbol deﬁnition, and then modifying all of the references to those symbols so that they point to this memory location.
The sections that follow describe these tasks in more detail.
As you read, keep in mind some basic facts about linkers: Object ﬁles are merely collections of blocks of bytes.
Some of these blocks contain program code, others contain program data, and others contain data structures that guide the linker and loader.
A linker concatenates blocks together, decides on run-time locations for the concatenated blocks, and modiﬁes various locations within the code and data blocks.
The compilers and assemblers that generate the object ﬁles have already done most of the work.
Contains binary code and data in a form that can be combined with other relocatable object ﬁles at compile time to create an executable object ﬁle.
Contains binary code and data in a form that can be copied directly into memory and executed.
Shared object ﬁle.A special type of relocatable object ﬁle that can be loaded into memory and linked dynamically, at either load time or run time.
The ﬁrst Unix systems from Bell Labs used the a.out format.
To this day, executables are still referred to as a.outﬁles.
Windows NT uses a variant of COFF called the Portable Executable (PE) format.
Although our discussion will focus on ELF, the basic concepts are similar, regardless of the particular format.
Figure 7.3 shows the format of a typical ELF relocatable object ﬁle.
The ELF header begins with a 16-byte sequence that describes the word size and byte ordering of the system that generated the ﬁle.
The rest of the ELF header contains information that allows a linker to parse and interpret the object ﬁle.
This includes the size of the ELF header, the object ﬁle type (e.g., relocatable, executable, or shared), the machine type (e.g., IA32), the ﬁle offset of the section header table, and the size and number of entries in the section header table.
The locations and sizes of the various sections are described by the section header table, which contains a ﬁxed sized entry for each section in the object ﬁle.
Sandwiched between the ELF header and the section header table are the sections themselves.
A typical ELF relocatable object ﬁle contains the following sections:
Local C variables are maintained at run time on the stack, and do not appear in either the .data or .bss sections.
This section occupies no actual space in the object ﬁle; it is merely a place holder.
Object ﬁle formats distinguish between initialized and uninitialized variables for space efﬁciency: uninitialized variables do not have to occupy any actual disk space in the object ﬁle.
Some programmers mistakenly believe that a program must be compiled with the -g option to get symbol table information.
In fact, every relocatable object ﬁle has a symbol table in .symtab.
However, unlike the symbol table inside a compiler, the .symtab symbol table does not contain entries for local variables.
In general, any instruction that calls an external function or references a global variable will need to be modiﬁed.
On the other hand, instructions that call local functions do not need to be modiﬁed.
Note that relocation information is not needed in executable object ﬁles, and is usually omitted unless the user explicitly instructs the linker to include it.
In general, any initialized global variable whose initial value is the address of a global variable or externally deﬁned function will need to be modiﬁed.
It is only present if the compiler driver is invoked with the -g option.
It is only present if the compiler driver is invoked with the -g option.
A string table is a sequence of null-terminated character strings.
The use of the term .bss to denote uninitialized data is universal.
A simple way to remember the difference between the .data and .bss sections is to think of “bss” as an abbreviation for “Better Save Space!”
Each relocatable object module, m, has a symbol table that contains information about the symbols that are deﬁned and referenced bym.
In the context of a linker, there are three different kinds of symbols:
Global symbols that are deﬁned by module m and that can be referenced by othermodules.
Global linker symbols correspond to nonstaticC functions and global variables that are deﬁned without the C static attribute.
Global symbols that are referenced by module m but deﬁned by some other module.
Such symbols are called externals and correspond to C functions and variables that are deﬁned in other modules.
Some local linker symbols correspond to C functions and global variables that are deﬁned with the static attribute.
These symbols are visible anywhere within module m, but cannot be referenced by other modules.
The sections in an object ﬁle and the name of the source ﬁle that corresponds to module m also get local symbols.
It is important to realize that local linker symbols are not the same as local program variables.
The symbol table in .symtab does not contain any symbols that correspond to local nonstatic program variables.
These are managed at run time on the stack and are not of interest to the linker.
Interestingly, local procedure variables that are deﬁned with the C static attribute are not managed on the stack.
Instead, the compiler allocates space in .data or .bss for each deﬁnition and creates a local linker symbol in the symbol table with a unique name.
For example, suppose a pair of functions in the same module deﬁne a static local variable x:
In this case, the compiler allocates space for two integers in .data and exports a pair of unique local linker symbols to the assembler.
New to C? Hiding variable and function names with static.
Any global variable or function declared with the static attribute is private to that module.
Similarly, any global variable or function declared without the static attribute is public and can be accessed by any other module.
It is good programming practice to protect your variables and functions with the static attribute wherever possible.
Symbol tables are built by assemblers, using symbols exported by the compiler into the assembly-language .s ﬁle.
An ELF symbol table is contained in the .symtab section.
The name is a byte offset into the string table that points to the null-terminated string name of the symbol.
For relocatable modules, the value is an offset from the beginning of the section where the object is deﬁned.
For executable object ﬁles, the value is an absolute run-time address.
The size is the size (in bytes) of the object.
The symbol table can also contain entries for the individual sections and for the path name of the original source ﬁle.
So there are distinct types for these objects as well.
The binding ﬁeld indicates whether the symbol is local or global.
Each symbol is associated with some section of the object ﬁle, denoted by the section ﬁeld, which is an index into the section header table.
There are three special pseudo sections that don’t have entries in the section header table: ABS is for symbols that should not be relocated.
For COMMON symbols, the value ﬁeld gives the alignment requirement, and size gives the minimum size.
For example, here are the last three entries in the symbol table for main.o, as displayed by the GNU readelf tool.
The ﬁrst eight entries, which are not shown, are local symbols that the linker uses internally.
In this example, we see an entry for the deﬁnition of global symbol buf, an 8byte object located at an offset (i.e., value) of zero in the .data section.
This is followed by the deﬁnition of the global symbol main, a 17-byte function located at an offset of zero in the .text section.
The last entry comes from the reference for the external symbol swap.Readelf identiﬁes each section by an integer index.
The next symbol comes from the reference to the external buf symbol in the initialization code for bufp0
This is followed by the global symbol swap, a 39-byte function at an offset of zero in .text.
For each symbol that is deﬁned or referenced in swap.o, indicate whether or not it will have a symbol table entry in the .symtab section in module swap.o.
If so, indicate the module that deﬁnes the symbol (swap.oor main.o), the symbol type (local, global, or extern), and the section (.text, .data, or .bss) it occupies in that module.
Symbol swap.o .symtab entry? Symbol type Module where deﬁned Section.
The linker resolves symbol references by associating each reference with exactly one symbol deﬁnition from the symbol tables of its input relocatable object ﬁles.
Symbol resolution is straightforward for references to local symbols that are deﬁned in the samemodule as the reference.
The compiler allows only one deﬁnition of each local symbol per module.
The compiler also ensures that static local variables, which get local linker symbols, have unique names.
When the compiler encounters a symbol (either a variable or function name) that is not deﬁned in the current module, it assumes that it is deﬁned in some other module, generates a linker symbol table entry, and leaves it for the linker to handle.
If the linker is unable to ﬁnd a deﬁnition for the referenced symbol in any of its input modules, it prints an (often cryptic) error message and terminates.
Symbol resolution for global symbols is also tricky because the same symbol might be deﬁned bymultiple object ﬁles.
In this case, the linker must either ﬂag an error or somehow choose one of the deﬁnitions and discard the rest.
The approach adopted by Unix systems involves cooperation between the compiler, assembler, and linker, and can introduce some bafﬂing bugs to the unwary programmer.
BothC++ and Java allow overloadedmethods that have the same name in the source code but different parameter lists.
So howdoes the linker tell the difference between these different overloaded functions? Overloaded functions in C++ and Java work because the compiler encodes each unique method and parameter list combination into a unique name for the linker.
This encoding process is calledmangling, and the inverse process demangling.
A mangled class name consists of the integer number of characters in the name followed by the original name.
A method is encoded as the original method name, followed by __, followed.
Similar schemes are used to mangle global variable and template names.
At compile time, the compiler exports each global symbol to the assembler as either strong or weak, and the assembler encodes this information implicitly in the symbol table of the relocatable object ﬁle.
Given this notion of strong and weak symbols, Unix linkers use the following rules for dealing with multiply deﬁned symbols:
Rule 2: Given a strong symbol and multiple weak symbols, choose the strong symbol.
Rule 3: Given multiple weak symbols, choose any of the weak symbols.
For example, supposeweattempt to compile and link the following twoCmodules:
In this case, the linker will generate an error message because the strong symbol main is deﬁned multiple times (rule 1):
Similarly, the linker will generate an error message for the following modules because the strong symbol x is deﬁned twice (rule 1):
However, if x is uninitialized in onemodule, then the linkerwill quietly choose the strong symbol deﬁned in the other (rule 2):
The same thing can happen if there are two weak deﬁnitions of x (rule 3):
Consider the following example, in which x is deﬁned as an int in one module and a double in another:
This is a subtle and nasty bug, especially because it occurs silently, with no warning from the compilation system, and because it typically manifests itself much later in the execution of the program, far away from where the error occurred.
In a large system with hundreds of modules, a bug of this kind is extremely hard to ﬁx, especially because many programmers are not aware of how linkers work.
When in doubt, invoke the linker with a ﬂag such as the gcc -fno-common ﬂag, which triggers an error if it encounters multiply deﬁned global symbols.
For each example that follows, use this notation to indicate how the linker would resolve references to themultiply deﬁned symbol in eachmodule.
So far, we have assumed that the linker reads a collection of relocatable object ﬁles and links them together into an output executable ﬁle.
In practice, all compilation systems provide a mechanism for packaging related object modules into a single ﬁle called a static library, which can then be supplied as input to the linker.
When it builds the output executable, the linker copies only the object modules in the library that are referenced by the application program.
Why do systems support the notion of libraries? Consider ANSI C, which deﬁnes an extensive collection of standard I/O, string manipulation, and integer math functions such as atoi, printf, scanf, strcpy, and rand.
They are available to every C program in the libc.a library.
Consider the different approaches that compiler developers might use to provide these functions to users without the beneﬁt of static libraries.
One approach would be to have the compiler recognize calls to the standard functions and to generate the appropriate code directly.
Pascal, which provides a small set of standard functions, takes this approach, but it is not feasible for C, because of the large number of standard functions deﬁned by the C standard.
It would add signiﬁcant complexity to the compiler and would require a new compiler version each time a function was added, deleted, or modiﬁed.
To application programmers, however, this approach would be quite convenient because the standard functions would always be available.
Another approach would be to put all of the standard C functions in a single relocatable object module, say, libc.o, that application programmers could link into their executables:
This approach has the advantage that it would decouple the implementation of the standard functions from the implementation of the compiler, and would still be reasonably convenient for programmers.
However, a big disadvantage is that every executable ﬁle in a system would now contain a complete copy of the collection of standard functions, which would be extremely wasteful of disk space.
Worse, each running program would now contain its own copy of these functions in memory, which would be extremely wasteful of memory.
Another big disadvantage is that any change to any standard function, no matter how small, would require the library developer to recompile the entire source ﬁle, a timeconsuming operation that would complicate the development and maintenance of the standard functions.
We could address some of these problems by creating a separate relocatable ﬁle for each standard function and storing them in a well-known directory.
However, this approach would require application programmers to explicitly link the appropriate object modules into their executables, a process that would be error prone and time consuming:
The notion of a static library was developed to resolve the disadvantages of these various approaches.
Related functions can be compiled into separate object modules and then packaged in a single static library ﬁle.
Application programs can then use any of the functions deﬁned in the library by specifying a single ﬁle name on the command line.
For example, a program that uses functions from the standard C library and the math library could be compiled and linked with a command of the form.
At link time, the linker will only copy the object modules that are referenced by the program, which reduces the size of the executable on disk and in memory.
On the other hand, the application programmer only needs to include the names of a few library ﬁles.
In fact, C compiler drivers always pass libc.a to the linker, so the reference to libc.a mentioned previously is unnecessary.
On Unix systems, static libraries are stored on disk in a particular ﬁle format known as an archive.
An archive is a collection of concatenated relocatable object ﬁles, with a header that describes the size and location of each member object ﬁle.
To make our discussion of libraries concrete, suppose thatwewant toprovide the vector routines inFigure 7.5 in a static library called libvector.a.
To create the library, we would use the ar tool as follows:
The include (header) ﬁle vector.h deﬁnes the function prototypes for the routines in libvector.a.
To build the executable, we would compile and link the input ﬁles main.o and libvector.a:
The -static argument tells the compiler driver that the linker should build a fully linked executable object ﬁle that can be loaded into memory and run without any further linking at load time.
When the linker runs, it determines that the addvec symbol deﬁned by addvec.o is referenced by main.o, so it copies addvec.o into the executable.
Since the program doesn’t reference any symbols deﬁned by multvec.o, the linker does not copy this module into the executable.
The linker also copies the printf.o module from libc.a, along with a number of other modules from the C run-time system.
While static libraries are useful and essential tools, they are also a source of confusion to programmers because of theway theUnix linker uses them to resolve external references.
During the symbol resolution phase, the linker scans the relocatable object ﬁles and archives left to right in the same sequential order that they appear on the compiler driver’s command line.
The driver automatically translates any .c ﬁles on the command line into .o ﬁles.
During this scan, the linker maintains a set E of relocatable object ﬁles that will be merged to form the executable, a set U of unresolved symbols (i.e., symbols referred to, but not yet deﬁned), and a set D of symbols that have been deﬁned in previous input ﬁles.
For each input ﬁle f on the command line, the linker determines if f is an object ﬁle or an archive.
If f is an object ﬁle, the linker adds f to E, updates U and D to reﬂect the symbol deﬁnitions and references in f , and proceeds to the next input ﬁle.
If f is an archive, the linker attempts to match the unresolved symbols in U against the symbols deﬁned by the members of the archive.
If some archive member, m, deﬁnes a symbol that resolves a reference in U , then m is added to E, and the linker updates U and D to reﬂect the symbol deﬁnitions and references in m.
This process iterates over the member object ﬁles in the archive until a ﬁxed point is reached where U and D no longer change.
At this point, any member object ﬁles not contained in E are simply discarded and the linker proceeds to the next input ﬁle.
If U is nonempty when the linker ﬁnishes scanning the input ﬁles on the command line, it prints an error and terminates.
Otherwise, it merges and relocates the object ﬁles in E to build the output executable ﬁle.
Unfortunately, this algorithm can result in some bafﬂing link-time errors because the ordering of libraries and object ﬁles on the command line is signiﬁcant.
If the library that deﬁnes a symbol appears on the command line before the object ﬁle that references that symbol, then the reference will not be resolved and linking will fail.
What happened? When libvector.a is processed, U is empty, so no member object ﬁles from libvector.a are added to E.
Thus, the reference to addvec is never resolved and the linker emits an error message and terminates.
The general rule for libraries is to place them at the end of the command line.
If the members of the different libraries are independent, in that no member references a symbol deﬁned by another member, then the libraries can be placed at the end of the command line in any order.
If, on the other hand, the libraries are not independent, then they must be ordered so that for each symbol s that is referenced externally by a member of an archive, at least one deﬁnition of s follows a reference to s on the command line.
For example, suppose foo.c calls functions in libx.a and libz.a that call functions in liby.a.
Then libx.a and libz.amust precede liby.a on the command line:
Libraries can be repeated on the command line if necessary to satisfy the dependence requirements.
For example, suppose foo.c calls a function in libx.a that calls a function in liby.a that calls a function in libx.a.
Alternatively, we could combine libx.a and liby.a into a single archive.
Once the linker has completed the symbol resolution step, it has associated each symbol reference in the code with exactly one symbol deﬁnition (i.e., a symbol table entry in one of its input object modules)
At this point, the linker knows the exact sizes of the code and data sections in its input object modules.
It is now ready to begin the relocation step, where it merges the input modules and assigns run-time addresses to each symbol.
In this step, the linker merges all sections of the same type into a new aggregate section of the same type.
For example, the .data sections from the input modules are all merged into one section that will become the .data section for the output executable object ﬁle.
The linker then assigns run-time memory addresses to the new aggregate sections, to each section deﬁned by the input modules, and to each symbol deﬁned by the input modules.
When this step is complete, every instruction and global variable in the program has a unique run-time memory address.
In this step, the linker modiﬁes every symbol reference in the bodies of the code and data sections so that they point to the correct run-time addresses.
To perform this step, the linker relies ondata structures in the relocatable objectmodules knownas relocation entries, which we describe next.
When an assembler generates an object module, it does not know where the code and datawill ultimately be stored inmemory.Nor does it know the locations of any externally deﬁned functions or global variables that are referenced by themodule.
So whenever the assembler encounters a reference to an object whose ultimate.
Figure 7.8 shows the format of an ELF relocation entry.
The offset is the section offset of the reference that will need to be modiﬁed.
The symbol identiﬁes the symbol that the modiﬁed reference should point to.
The type tells the linker how to modify the new reference.
We are concerned with only the two most basic relocation types:
Recall from Section 3.6.3 that a PC-relative address is an offset from the current run-time value of the program counter (PC).When the CPU executes an instruction using PC-relative addressing, it forms the effective address (e.g., the target of the call instruction) by adding the 32-bit value encoded in the instruction to the current run-time value of the PC, which is always the address of the next instruction in memory.
With absolute addressing, the CPU directly uses the 32-bit value encoded in the instruction as the effective address, without further modiﬁcations.
Figure 7.9 shows the pseudo code for the linker’s relocation algorithm.
Also, assume that when the algorithm runs, the linker has already chosen run-time addresses for each section (denoted ADDR(s)) and each symbol (denoted ADDR(r.symbol))
If the reference uses absolute addressing, then it is relocated by lines 11–13
Recall from our running example in Figure 7.1(a) that the main routine in the .text section of main.o calls the swap routine, which is deﬁned in swap.o.
Here is the disassembled listing for the call instruction, as generated by the GNU objdump tool:
In the resulting executable object ﬁle, the call instruction has the following relocated form:
At run time, the call instruction will be stored at address 0x80483ba.
When the CPU executes the call instruction, the PC has a value of 0x80483bf, which is the address of the instruction immediately following the call instruction.
To execute the instruction, the CPU performs the following steps:
Thus, the next instruction to execute is the ﬁrst instruction of the swap routine, which of course is what we want!
Since bufp0 is an initialized data object, it will be stored in the .data section of the swap.o relocatable object module.
Since it is initialized to the address of a global array, it will need to be relocated.
Here is the disassembled listing of the .data section from swap.o:
In the resulting executable object ﬁle, the reference has the following relocated form:
The .text section in the swap.omodule contains ﬁve absolute references that are relocated in a similar way (see Problem 7.12)
Figure 7.10 shows the relocated .text and .data sections in the ﬁnal executable object ﬁle.
What is the hex address of the relocated reference to swap in line 5?
What is the hex value of the relocated reference to swap in line 5?
What would the hex value of the relocated reference in line 5 be in this case?
Figure 7.10 Relocated .text and .data sections for executable ﬁle p.
We have seen how the linker merges multiple object modules into a single executable object ﬁle.
Our C program, which began life as a collection of ASCII text ﬁles, has been transformed into a single binary ﬁle that contains all of the information needed to load the program into memory and run it.
Figure 7.11 summarizes the kinds of information in a typical ELF executable ﬁle.
The format of an executable object ﬁle is similar to that of a relocatable object ﬁle.
The ELF header describes the overall format of the ﬁle.
It also includes the program’s entry point, which is the address of the ﬁrst instruction to execute when the program runs.
The .text, .rodata, and .data sections are similar to those in a relocatable object ﬁle, except that these sections have been relocated to their eventual run-time memory addresses.
The .init section deﬁnes a small function, called _init, that will be called by the program’s initialization code.
Since the executable is fully linked (relocated), it needs no .rel sections.
Figure 7.12 shows the segment header table for our example executable p, as displayed by objdump.
From the segment header table, we see that two memory segments will be initialized with the contents of the executable object ﬁle.
Symbol table and debugging info are not loaded into memory.
Figure 7.12 Segment header table for the example executable p.
Legend: off: ﬁle offset, vaddr/paddr: virtual/physical address, align: segment alignment, filesz: segment size in the object ﬁle, memsz: segment size in memory, flags: run-time permissions.
The remaining bytes in the segment correspond to .bss data that will be initialized to zero at run time.
To run an executable object ﬁle p, we can type its name to the Unix shell’s command line:
Since p does not correspond to a built-in shell command, the shell assumes that p is an executable object ﬁle, which it runs for us by invoking some memoryresident operating system code known as the loader.
Any Unix program can invoke the loader by calling the execve function, whichwewill describe in detail in Section 8.4.5
The loader copies the code anddata in the executable object ﬁle from disk into memory, and then runs the program by jumping to its ﬁrst instruction, or entry point.
This process of copying the program into memory and then running it is known as loading.
Every Unix program has a run-time memory image similar to the one in Figure 7.13
The data segment follows at the next 4 KB aligned address.
The run-time heap follows on the ﬁrst 4 KB aligned address past the read/write segment and grows up via calls to the malloc library.
We will describe malloc and the heap in detail in Section 9.9
There is also a segment that is reserved for shared libraries.
The user stack always starts at the largest legal user address and grows down (toward lower memory addresses)
When the loader runs, it creates the memory image shown in Figure 7.13
Guided by the segment header table in the executable, it copies chunks of the executable into the code and data segments.
Next, the loader jumps to the program’s entry point, which is always the address of the _start symbol.
Figure 7.14 shows the speciﬁc sequence of calls in the startup code.
After calling initialization routines from the .text and .init sections, the startup code calls the atexit routine, which appends a list of routines that should be called when the application terminates normally.
The exit function runs the functions registered by atexit, and then returns control to the operating system.
Note: The code that pushes the arguments for each function is not shown.
Next, the startup code calls the application’s main routine, which begins executing our C code.
After the application returns, the startup code calls the _exit routine, which returns control to the operating system.
For the impatient reader, here is a preview of how loading really works: Each program in a Unix system runs in the context of a processwith its own virtual address space.When the shell runs a program, the parent shell process forks a child process that is a duplicate of the parent.
The child process invokes the loader via the execve system call.
The loader deletes the child’s existing virtual memory segments, and creates a new set of code, data, heap, and stack segments.
The new stack and heap segments are initialized to zero.
The new code and data segments are initialized to the contents of the executable ﬁle by mapping pages in the virtual address space to page-sized chunks of the executable ﬁle.
Finally, the loader jumps to the _start address, which eventually calls the application’s main routine.
Aside from some header information, there is no copying of data from disk to memory during loading.
The copying is deferred until theCPU references amapped virtual page, at which point the operating system automatically transfers the page from disk to memory using its paging mechanism.
Why does every C program need a routine called main?
Have you ever wondered why a C main routine can end with a call to exit, a return statement, or neither, and yet the program still terminates properly? Explain.
The static libraries that we studied in Section 7.6.2 address many of the issues associated with making large collections of related functions available to application programs.
Static libraries, like all software, need to be maintained and updated periodically.
If application programmers want to use the most recent version of a library, they must somehow become aware that the library has changed, and then explicitly relink their programs against the updated library.
Another issue is that almost everyCprogramuses standard I/O functions such as printf and scanf.
At run time, the code for these functions is duplicated in the text segment of each running process.
An interesting property of memory is that it is always a scarce resource, regardless of how much there is in a system.
Disk space and kitchen trash cans share this same property.
Shared libraries are modern innovations that address the disadvantages of static libraries.
A shared library is an object module that, at run time, can be loaded at an arbitrary memory address and linked with a program in memory.
This process is known as dynamic linking and is performed by a program called a dynamic linker.
Shared libraries are also referred to as shared objects, and on Unix systems are typically denoted by the .so sufﬁx.
Microsoft operating systems make heavy use of shared libraries, which they refer to as DLLs (dynamic link libraries)
First, in any given ﬁle system, there is exactly one .so ﬁle for a particular library.
The code and data in this.soﬁle are sharedby all of the executable object ﬁles that reference the library, as opposed to the contents of static libraries, which are copied and embedded in the executables that reference them.
Second, a single copy of the .text section of a shared library in memory can be shared by different running processes.
To build a shared library libvector.so of our example vector arithmetic routines in Figure 7.5, we would invoke the compiler driver with the following special directive to the linker:
The -shared ﬂag directs the linker to create a shared object ﬁle.
Once we have created the library, we would then link it into our example program in Figure 7.6:
This creates an executable object ﬁle p2 in a form that can be linked with libvector.so at run time.
The basic idea is to do some of the linking statically when the executable ﬁle is created, and then complete the linking process dynamically when the program is loaded.
It is important to realize that none of the code or data sections from libvector.so are actually copied into the executable p2 at this point.
Instead, the linker copies some relocation and symbol table information that will allow references to code and data in libvector.so to be resolved at run time.
Instead of passing control to the application, as it would normally do, the loader loads and runs the dynamic linker.
The dynamic linker then ﬁnishes the linking task by performing the following relocations:
Relocating the text and data of libc.so into some memory segment.
Relocating the text and data of libvector.so into anothermemory segment.
Relocating any references in p2 to symbols deﬁned by libc.so and libvector.so.
From this point on, the locations of the shared libraries are ﬁxed and do not change during execution of the program.
Up to this point, we have discussed the scenario in which the dynamic linker loads and links shared libraries when an application is loaded, just before it executes.
However, it is also possible for an application to request the dynamic linker to load and link arbitrary shared libraries while the application is running, without having to link in the applications against those libraries at compile time.
Developers of Microsoft Windows applications frequently use shared libraries to distribute software updates.
They generate a new copy of a shared library, which users can then download and use as a replacement for the current version.
The next time they run their application, it will automatically link and load the new shared library.
The idea is to package each function that generates dynamic content in a shared library.
When a request arrives from a Web browser, the server dynamically loads and links the appropriate function and then calls it directly, as opposed to using fork and execve to run the function in the context of a child process.
The function remains cached in the server’s address space, so subsequent requests can be handled at the cost of a simple function call.
This can have a signiﬁcant impact on the throughput of a busy site.
Further, existing functions can be updated and new functions can be added at run time, without stopping the server.
Linux systems provide a simple interface to the dynamic linker that allows application programs to load and link shared libraries at run time.
The dlopen function loads and links the shared library filename.
The external symbols in filename are resolvedusing libraries previously openedwith the RTLD_ GLOBAL ﬂag.
If the current executable was compiled with the -rdynamic ﬂag, then its global symbols are also available for symbol resolution.
Either of these values can be or’d with the RTLD_GLOBAL ﬂag.
The dlsym function takes a handle to a previously opened shared library and a symbol name, and returns the address of the symbol, if it exists, or NULL otherwise.
The dlclose function unloads the shared library if no other shared libraries are still using it.
Returns: error msg if previous call to dlopen, dlsym, or dlclose failed, NULL if previous call was OK.
The dlerror function returns a string describing the most recent error that occurred as a result of calling dlopen, dlsym, or dlclose, or NULL if no error occurred.
To compile the program, we would invoke gcc in the following way:
Java deﬁnes a standard calling convention called Java Native Interface (JNI) that allows “native” C and C++ functions to be called from Java programs.
The basic idea of JNI is to compile the native C function, say, foo, into a shared library, say foo.so.
When a running Java program attempts to invoke function foo, the Java interpreter uses the dlopen interface (or something like it) to dynamically link and load foo.so, and then call foo.
Get a pointer to the addvec() function we just loaded */
Now we can call addvec() just like any other function */
Figure 7.16 An application program that dynamically loads and links the shared library libvector.so.
A key purpose of shared libraries is to allow multiple running processes to share the same library code in memory and thus save precious memory resources.
So how canmultiple processes share a single copy of a program?One approachwould be to assign a priori a dedicated chunk of the address space to each shared library, and then require the loader to always load the shared library at that address.
It would be an inefﬁcient use of the address space because portions of the space would be allocated even if a process didn’t use the library.
We would have to ensure that none of the chunks overlapped.
Every time a library were modiﬁed, we would have to make sure that it still ﬁt in its assigned chunk.
If not, then we would have to ﬁnd a new chunk.
And if we created a new library, we would have to ﬁnd room for it.
Over time, given the hundreds of libraries and versions of libraries in a system, it would be difﬁcult to keep the address space from fragmenting into lots of small unused but unusable holes.
Even worse, the assignment of libraries to memory would be different for each system, thus creating even more management headaches.
A better approach is to compile library code so that it can be loaded and executed at any address without being modiﬁed by the linker.
Users direct GNU compilation systems to generate PIC code with the -fPIC option to gcc.
However, calls to externally deﬁned procedures and references to global variables are not normally PIC, since they require relocation at link time.
Compilers generate PIC references to global variables by exploiting the following interesting fact: No matter where we load an object module (including shared object modules) in memory, the data segment is always allocated immediately after the code segment.
Thus, the distance between any instruction in the code segment and any variable in the data segment is a run-time constant, independent of the absolute memory locations of the code and data segments.
To exploit this fact, the compiler creates a table called the global offset table (GOT) at the beginning of the data segment.
The GOT contains an entry for each global data object that is referenced by the object module.
The compiler also generates a relocation record for each entry in the GOT.
At load time, the dynamic linker relocates each entry in theGOT so that it contains the appropriate absolute address.
Each object module that references global data has its own GOT.
At run time, each global variable is referenced indirectly through the GOT using code of the form.
In this fascinating piece of code, the call to L1 pushes the return address (which happens to be the address of the popl instruction) on the stack.
The net effect of these two instructions is to move the value of the PC into register %ebx.
The addl instruction adds a constant offset to %ebx so that it points to the appropriate entry in the GOT, which contains the absolute address of the data item.
At this point, the global variable can be referenced indirectly through the GOT entry contained in %ebx.
In this example, the two movl instructions load the contents of the global variable (indirectly through the GOT) into register %eax.
Each global variable reference now requires ﬁve instructions instead of one, with an additional memory reference to the GOT.
Also, PIC code uses an additional register to hold the address of the GOT entry.
On machines with large register ﬁles, this is not a major issue.
On register-starved IA32 systems, however, losing even one register can trigger spilling of the registers onto the stack.
It would certainly be possible for PIC code to use the same approach for resolving external procedure calls:
However, this approach would require three additional instructions for each runtime procedure call.
Instead, ELF compilation systems use an interesting technique, called lazy binding, that defers the binding of procedure addresses until the ﬁrst time the procedure is called.
There is a nontrivial run-time overhead the ﬁrst time the procedure is called, but each call thereafter only costs a single instruction and a memory reference for the indirection.
Lazy binding is implemented with a compact yet somewhat complex interaction between two data structures: theGOT and the procedure linkage table (PLT)
If an object module calls any functions that are deﬁned in shared libraries, then it has its own GOT and PLT.
The ﬁrst three GOT entries are special: GOT[0] contains the address of the .dynamic segment, which contains information that the dynamic linker uses to bind procedure addresses, such as the location of the symbol table.
GOT[2] contains an entry point into the lazy binding code of the dynamic linker.
For the example program, we have shown theGOT entries for printf, which is deﬁned in libc.so, and addvec, which is deﬁned in libvector.so.
The ﬁrst entry, PLT[0], is a special entry that jumps into the dynamic linker.
Each called procedure has an entry in the PLT, starting at PLT[1]
Initially, after the program has been dynamically linked and begins executing, procedures printf and addvec are bound to the ﬁrst instruction in their respective PLT entries.
Initially, each GOT entry contains the address of the pushl entry in the corresponding PLT entry.
So the indirect jump in the PLT simply transfers control back to the next instruction in PLT[2]
This instruction pushes an ID for the addvec symbol onto the stack.
The next time addvec is called in the program, control passes to PLT[2] as before.
However, this time the indirect jump through GOT[4] transfers control to addvec.
The only additional overhead from this point on is the memory reference for the indirect jump.
There are a number of tools available on Unix systems to help you understand and manipulate object ﬁles.
In particular, the GNU binutils package is especially helpful and runs on every Unix platform.
Can display all of the information in an object ﬁle.
Itsmost useful function is disassembling the binary instructions in the .text section.
Unix systems also provide the ldd program for manipulating shared libraries:
Linking can be performed at compile time by static linkers, and at load time and run time by dynamic linkers.
Linkers manipulate binary ﬁles called object ﬁles, which come in three different forms: relocatable, executable, and shared.
Relocatable object ﬁles are combined by static linkers into an executable object ﬁle that can be loaded into memory and executed.
Shared object ﬁles (shared libraries) are linked and loaded by dynamic linkers at run time, either implicitly when the calling program is loaded and begins executing, or on demand, when the program calls functions from the dlopen library.
The twomain tasks of linkers are symbol resolution, where each global symbol in an object ﬁle is bound to a unique deﬁnition, and relocation, where the ultimate memory address for each symbol is determined and where references to those objects are modiﬁed.
Static linkers are invoked by compiler drivers such as gcc.
They combine multiple relocatable object ﬁles into a single executable object ﬁle.Multiple object ﬁles can deﬁne the same symbol, and the rules that linkers use for silently resolving these multiple deﬁnitions can introduce subtle bugs in user programs.
Multiple object ﬁles can be concatenated in a single static library.
Linkers use libraries to resolve symbol references in other object modules.
The left-toright sequential scan thatmany linkers use to resolve symbol references is another source of confusing link-time errors.
Loaders map the contents of executable ﬁles into memory and run the program.
Linkers can also produce partially linked executable object ﬁles with unresolved references to the routines and data deﬁned in a shared library.
At load time, the loader maps the partially linked executable into memory and then calls a dynamic linker, which completes the linking task by loading the shared library and relocating the references in the program.
Applications can also use the dynamic linker at run time in order to load, link, and access the functions and data in shared libraries.
Linking is not well documented in the computer systems literature.
Since it lies at the intersection of compilers, computer architecture, and operating systems, linking requires understanding of code generation, machine-language programming, program instantiation, and virtual memory.
It does not ﬁt neatly into any of the usual computer systems specialties and thus is not well covered by the classic texts in these areas.
However, Levine’s monograph provides a good general reference on the subject [66]
The original speciﬁcations for ELF and DWARF (a speciﬁcation for the contents of the .debug and .line sections) are described in [52]
Binary translation can be used for three different purposes [64]: to emulate one system on another system, to observe program behavior, or to perform system-dependent optimizations that are not possible at compile time.
Commercial products such as VTune, Purify, and BoundsChecker use binary translation to provide programmers with detailed observations of their programs.
The Atom system provides a ﬂexible mechanism for instrumenting Alpha executable object ﬁles and shared libraries with arbitrary C functions [103]
Atom has been used to build a myriad of analysis tools that trace procedure calls, proﬁle instruction counts and memory referencing patterns, simulate memory system behavior, and isolate memory referencing errors.
The Shade system uses binary translation for instruction proﬁling [23]
For each symbol that is deﬁned and referenced in swap.o, indicate if it will have a symbol table entry in the .symtab section in module swap.o.
If so, indicate the module that deﬁnes the symbol (swap.o or main.o), the symbol type (local, global, or extern), and the section (.text, .data, or .bss) it occupies in that module.
Symbol swap.o .symtab entry? Symbol type Module where deﬁned Section.
Determinewhich instructions in .textwill need to bemodiﬁed by the linker when the module is relocated.
For each such instruction, list the information in its relocation entry: section offset, relocation type, and symbol name.
Determinewhich data objects in .datawill need to bemodiﬁed by the linker when the module is relocated.
For each such instruction, list the information in its relocation entry: section offset, relocation type, and symbol name.
Feel free to use tools such as objdump to help you solve this problem.
Determinewhich instructions in .textwill need to bemodiﬁed by the linker when the module is relocated.
For each such instruction, list the information in its relocation entry: section offset, relocation type, and symbol name.
Determine which data objects in .rodata will need to be modiﬁed by the linker when the module is relocated.
For each such instruction, list the information in its relocation entry: section offset, relocation type, and symbol name.
Feel free to use tools such as objdump to help you solve this problem.
How many object ﬁles are contained in the versions of libc.a and libm.a on your system?
What shared libraries does the gcc driver on your system use?
Notice that theC local variable temp does not have a symbol table entry.
Symbol swap.o .symtab entry? Symbol type Module where deﬁned Section.
Understanding these rules can help you avoid some nasty programming bugs.
This is an ERROR, because each module deﬁnes a strong symbol main (rule 1)
However, once you understand how linkers use static libraries to resolve references, it’s pretty straightforward.
Our purpose here is to give you some practice reading disassembly listings and to check your understanding of PC-relative addressing.
Remember that the disassembly listing shows the value of the reference in little-endian byte order.
Thekey observation here is that nomatterwhere the linker locates the .text section, the distance between the reference and the swap function is always the same.
Thus, because the reference is a PC-relative address, its value will be 0x9, regardless of where the linker locates the .text section.
You can answer them by referring to the C startup code in Figure 7.14
Every program needs a main function, because the C startup code, which is common to every C program, jumps to a function called main.
If main terminates with a return statement, then control passes back to the startup routine, which returns control to the operating system by calling _exit.
The same behavior occurs if the user omits the return statement.
If main terminates with a call to exit, then exit eventually returns control to the operating system by calling _exit.
The net effect is the same in all three cases: when main has ﬁnished, control passes back to the operating system.
From the time you ﬁrst apply power to a processor until the time you shut it off, the program counter assumes a sequence of values.
Each transition from ak to ak+1 is called a control transfer.
A sequence of such control transfers is called the ﬂow of control, or control ﬂow of the processor.
The simplest kind of control ﬂow is a “smooth” sequence where each Ik and Ik+1 are adjacent in memory.
Typically, abrupt changes to this smooth ﬂow, where Ik+1 is not adjacent to Ik, are causedby familiar program instructions suchas jumps, calls, and returns.
But systems must also be able to react to changes in system state that are not captured by internal program variables and are not necessarily related to the execution of the program.
For example, a hardware timer goes off at regular intervals and must be dealt with.
Packets arrive at the network adapter and must be stored in memory.
Programs request data from a disk and then sleep until they are notiﬁed that the data are ready.
Parent processes that create child processes must be notiﬁed when their children terminate.
Modern systems react to these situations by making abrupt changes in the control ﬂow.
In general, we refer to these abrupt changes as exceptional control ﬂow (ECF)
Exceptional control ﬂow occurs at all levels of a computer system.
For example, at the hardware level, events detected by the hardware trigger abrupt control transfers to exception handlers.
At the operating systems level, the kernel transfers control from one user process to another via context switches.
At the application level, a process can send a signal to another process that abruptly transfers control to a signal handler in the recipient.
An individual program can react to errors by sidestepping the usual stack discipline and making nonlocal jumps to arbitrary locations in other functions.
As programmers, there are a number of reasons why it is important for you to understand ECF:
Before you can really understand these important ideas, you need to understand ECF.
For example, writing data to adisk, readingdata fromanetwork, creating anewprocess, and terminating the current process are all accomplished by application programs invoking system calls.
Understanding the basic system call mechanism will help you understand how these services are provided to applications.
Understanding ECF will help you write interesting new application programs.
If you understand these ECF mechanisms, then you can use them to write interesting programs such as Unix shells and Web servers.
Understanding ECF will help you understand how software exceptions work.
Languages such as C++ and Java provide software exception mechanisms via try, catch, and throw statements.
Software exceptions allow the program to make nonlocal jumps (i.e., jumps that violate the usual call/return stack discipline) in response to error conditions.
Nonlocal jumps are a form of application-level ECF, and are provided in C via the setjmp and longjmp functions.
Understanding these low-level functions will help you understand how higher-level software exceptions can be implemented.
Up to this point in your study of systems, you have learned how applications interact with the hardware.
This chapter is pivotal in the sense that you will begin to learn how your applications interact with the operating system.
We describe the various forms of ECF that exist at all levels of a computer system.
We start with exceptions, which lie at the intersection of the hardware and the operating system.We also discuss system calls, which are exceptions that provide applications with entry points into the operating system.
We then move up a level of abstraction and describe processes and signals, which lie at the intersection of applications and the operating system.
Finally, we discuss nonlocal jumps, which are an application-level form of ECF.
Exceptions are a form of exceptional control ﬂow that are implemented partly by the hardware and partly by the operating system.
Because they are partly implemented in hardware, the details vary from system to system.
However, the basic ideas are the same for every system.
Our aim in this section is to give you a general understanding of exceptions and exception handling, and to help demystify what is often a confusing aspect of modern computer systems.
An exception is an abrupt change in the control ﬂow in response to some change in the processor’s state.
In the ﬁgure, the processor is executing some current instruction Icurr when a signiﬁcant change in the processor’s state occurs.
The state is encoded in various bits and signals inside the processor.
A change in the processor’s state (event) triggers an abrupt control transfer (an exception) from the application program to an exception handler.
After it ﬁnishes processing, the handler either returns control to the interrupted program or aborts.
For example, a virtual memory page fault occurs, an arithmetic overﬂowoccurs, or an instruction attempts a divide by zero.
On the other hand, the event might be unrelated to the execution of the current instruction.
For example, a system timer goes off or an I/O request completes.
In any case, when the processor detects that the event has occurred, it makes an indirect procedure call (the exception), througha jump table called an exception table, to an operating system subroutine (the exception handler) that is speciﬁcally designed to process this particular kind of event.
When the exception handler ﬁnishes processing, one of three things happens, depending on the type of event that caused the exception:
The handler returns control to the current instruction Icurr , the instruction that was executing when the event occurred.
The handler returns control to Inext , the instruction that would have executed next had the exception not occurred.
C++ and Java programmers will have noticed that the term “exception” is also used to describe the application-level ECF mechanism provided by C++ and Java in the form of catch, throw, and try statements.
Ifwewanted tobeperfectly clear, wemight distinguish between“hardware” and“software” exceptions, but this is usually unnecessary because the meaning is clear from the context.
Exceptions can be difﬁcult to understand because handling them involves close cooperation between hardware and software.
It is easy to get confused about which component performs which task.
Let’s look at the division of labor between hardware and software in more detail.
The exception table is a jump table where entry k contains the address of the handler code for exception k.
Each type of possible exception in a system is assigned a unique nonnegative integer exception number.
Some of these numbers are assigned by the designers of the processor.
Other numbers are assigned by the designers of the operating system kernel (the memory-resident part of the operating system)
Examples of the former include divide by zero, page faults, memory access violations, breakpoints, and arithmetic overﬂows.
Examples of the latter include system calls and signals from external I/O devices.
At system boot time (when the computer is reset or powered on), the operating system allocates and initializes a jump table called an exception table, so that entry k contains the address of the handler for exception k.
At run time (when the system is executing some program), the processor detects that an event has occurred and determines the corresponding exception number k.
The processor then triggers the exception by making an indirect procedure call, through entry k of the exception table, to the corresponding handler.
Figure 8.3 shows how the processor uses the exception table to form the address of the appropriate exception handler.
The exception number is an index into the exception table, whose starting address is contained in a special CPU register called the exception table base register.
An exception is akin to a procedure call, but with some important differences.
As with a procedure call, the processor pushes a return address on the stack before branching to the handler.
The exception number is an index into the exception table.
The processor also pushes some additional processor state onto the stack that will be necessary to restart the interrupted programwhen the handler returns.
For example, an IA32 system pushes the EFLAGS register containing, among other things, the current condition codes, onto the stack.
If control is being transferred from a user program to the kernel, all of these items are pushed onto the kernel’s stack rather than onto the user’s stack.
Exception handlers run in kernelmode (Section 8.2.4), whichmeans they have complete access to all system resources.
Once the hardware triggers the exception, the rest of the work is done in software by the exception handler.
After the handler has processed the event, it optionally returns to the interrupted program by executing a special “return from interrupt” instruction, which pops the appropriate state back into the processor’s control and data registers, restores the state to user mode (Section 8.2.4) if the exception interrupted a user program, and then returns control to the interrupted program.
Exceptions can be divided into four classes: interrupts, traps, faults, and aborts.
The table in Figure 8.4 summarizes the attributes of these classes.
Interrupts occur asynchronously as a result of signals from I/O devices that are external to the processor.
Hardware interrupts are asynchronous in the sense that they are not caused by the execution of any particular instruction.
Exception handlers for hardware interrupts are often called interrupt handlers.
I/O devices such as network adapters, disk controllers, and timer chips trigger interrupts by signaling a pin on the processor chip and placing onto the system bus the exception number that identiﬁes the device that caused the interrupt.
Asynchronous exceptions occur as a result of events in I/O devices that are external to the processor.
Synchronous exceptions occur as a direct result of executing an instruction.
The interrupt handler returns control to the next instruction in the application program’s control ﬂow.
After the current instruction ﬁnishes executing, the processor notices that the interrupt pin has gone high, reads the exception number from the system bus, and then calls the appropriate interrupt handler.
When the handler returns, it returns control to the next instruction (i.e., the instruction that would have followed the current instruction in the control ﬂowhad the interrupt not occurred)
The effect is that the program continues executing as though the interrupt had never happened.
The remaining classes of exceptions (traps, faults, and aborts) occur synchronously as a result of executing the current instruction.
Traps are intentional exceptions that occur as a result of executing an instruction.
Like interrupt handlers, trap handlers return control to the next instruction.
The most important use of traps is to provide a procedure-like interface between user programs and the kernel known as a system call.
User programs often need to request services from the kernel such as reading a ﬁle (read), creating a new process (fork), loading a new program (execve), or terminating the current process (exit)
To allow controlled access to such kernel services, processors provide a special “syscall n” instruction that user programs can execute when theywant to request service n.
Executing the syscall instruction causes a trap to an exception handler that decodes the argument and calls the appropriate kernel routine.
From a programmer’s perspective, a system call is identical to a.
The trap handler returns control to the next instruction in the application program’s control ﬂow.
Depending on whether the fault can be repaired or not, the fault handler either reexecutes the faulting instruction or aborts.
Regular functions run in user mode, which restricts the types of instructions they can execute, and they access the same stack as the calling function.
A system call runs inkernelmode, which allows it to execute instructions, and accesses a stack deﬁned in the kernel.
Section 8.2.4 discusses user and kernel modes in more detail.
Faults result from error conditions that a handler might be able to correct.
When a fault occurs, the processor transfers control to the fault handler.
If the handler is able to correct the error condition, it returns control to the faulting instruction, thereby reexecuting it.
Otherwise, the handler returns to an abort routine in the kernel that terminates the application program that caused the fault.
A classic example of a fault is the page fault exception, which occurs when an instruction references a virtual address whose corresponding physical page is not resident in memory and must therefore be retrieved from disk.
The page fault handler loads the appropriate page from disk and then returns control to the instruction that caused the fault.
When the instruction executes again, the appropriate physical page is resident in memory and the instruction is able to run to completion without faulting.
Aborts result from unrecoverable fatal errors, typically hardware errors such as parity errors that occur when DRAM or SRAM bits are corrupted.
As shown in Figure 8.8, the handler returns control to an abort routine that terminates the application program.
To help make things more concrete, let’s look at some of the exceptions deﬁned for IA32 systems.
The abort handler passes control to a kernel abort routine that terminates the application program.
Divide error.A divide error (exception 0) occurs when an application attempts to divide by zero, or when the result of a divide instruction is too big for the destination operand.
Unix does not attempt to recover from divide errors, opting instead to abort the program.
General protection fault.The infamous general protection fault (exception 13) occurs for many reasons, usually because a program references an undeﬁned area of virtual memory, or because the program attempts to write to a read-only text segment.
Linux shells typically report general protection faults as “Segmentation faults.”
Page fault.A page fault (exception 14) is an example of an exception where the faulting instruction is restarted.
The handler maps the appropriate page of physical memory on disk into a page of virtual memory, and then restarts the faulting instruction.
Machine check.A machine check (exception 18) occurs as a result of a fatal hardware error that is detected during the execution of the faulting instruction.
Machine check handlers never return control to the application program.
Linux provides hundreds of system calls that application programs use when they want to request services from the kernel, such as reading a ﬁle, writing a ﬁle, or creating a new process.
Figure 8.10 shows some of the more popular Linux system calls.
Each system call has a unique integer number that corresponds to an offset in a jump table in the kernel.
The standard C library provides a set of convenient wrapper functions for most system calls.
The wrapper functions package up the arguments, trap to the kernel with the appropriate system call number, and then pass the return status of the system call back to the calling program.
Throughout this text, we will refer to system calls and their associated wrapper functions interchangeably as system-level functions.
It is quite interesting to study how programs can use the int instruction to invoke Linux system calls directly.
All parameters to Linux system calls are passed through general purpose registers rather than the stack.
By convention, register %eax contains the syscall number, and registers %ebx, %ecx, %edx, %esi, %edi, and %ebp contain up to six arbitrary arguments.
The stack pointer %esp cannot be used because it is overwritten by the kernel when it enters kernel mode.
For example, consider the following version of the familiar hello program, written using the write system-level function:
Figure 8.11 Implementing the hello program directly with Linux system calls.
The ﬁrst argument to write sends the output to stdout.
The second argument is the sequence of bytes to write, and the third argument gives the number of bytes to write.
Figure 8.11 shows an assembly language version of hello that uses the int instruction to invoke the write and exit system calls directly.
Then line 13 uses the int instruction to invoke the system call.
The terminology for the various classes of exceptions varies from system to system.
Processor macroarchitecture speciﬁcations often distinguish between asynchronous “interrupts” and synchronous “exceptions,” yet providenoumbrella term to refer to these very similar concepts.
Toavoidhaving to constantly refer to “exceptions and interrupts” and “exceptions or interrupts,” we use the word “exception” as the general term and distinguish between asynchronous exceptions (interrupts) and synchronous exceptions (traps, faults, and aborts) only when it is appropriate.
As we have noted, the basic ideas are the same for every system, but you should be aware that some manufacturers’ manuals use the word “exception” to refer only to those changes in control ﬂow caused by synchronous events.
Exceptions are the basic building blocks that allow theoperating system toprovide the notion of a process, one of themost profound and successful ideas in computer science.
When we run a program on a modern system, we are presented with the illusion that our program is the only one currently running in the system.
Our program appears to have exclusive use of both the processor and the memory.
The processor appears to execute the instructions in our program, one after the other, without interruption.
Finally, the code and data of our program appear to be the only objects in the system’s memory.
These illusions are provided to us by the notion of a process.
The classic deﬁnition of a process is an instance of a program in execution.
Each program in the system runs in the context of some process.
The context consists of the state that the program needs to run correctly.
This state includes the program’s code and data stored in memory, its stack, the contents of its generalpurpose registers, its program counter, environment variables, and the set of open ﬁle descriptors.
Each time a user runs a program by typing the name of an executable object ﬁle to the shell, the shell creates a new process and then runs the executable object ﬁle in the context of this new process.
Application programs can also create new processes and run either their own code or other applications in the context of the new process.
A detailed discussion of how operating systems implement processes is beyond our scope.
Instead, we will focus on the key abstractions that a process provides to the application:
An independent logical control ﬂow that provides the illusion that our program has exclusive use of the processor.
Aprivate address space that provides the illusion that our program has exclusive use of the memory system.
A process provides each program with the illusion that it has exclusive use of the processor, even though many other programs are typically running concurrently on the system.
If we were to use a debugger to single step the execution of our program, we would observe a series of program counter (PC) values that corresponded exclusively to instructions contained in our program’s executable object ﬁle or in shared objects linked into our program dynamically at run time.
This sequence of PC values is known as a logical control ﬂow, or simply logical ﬂow.
Consider a system that runs three processes, as shown in Figure 8.12
The single physical control ﬂow of the processor is partitioned into three logical ﬂows, one for each process.
Each vertical line represents a portion of the logical ﬂow for.
Processes provide each program with the illusion that it has exclusive use of the processor.
Each vertical bar represents a portion of the logical control ﬂow for a process.
In the example, the execution of the three logical ﬂows is interleaved.
Process A runs for a while, followed by B, which runs to completion.
Process C then runs for awhile, followed by A, which runs to completion.
The key point in Figure 8.12 is that processes take turns using the processor.
Each process executes a portion of its ﬂow and then is preempted (temporarily suspended) while other processes take their turns.
To a program running in the context of one of these processes, it appears to have exclusive use of the processor.
The only evidence to the contrary is that if we were to precisely measure the elapsed time of each instruction, we would notice that the CPU appears to periodically stall between the execution of some of the instructions in our program.
However, each time the processor stalls, it subsequently resumes execution of our program without any change to the contents of the program’s memory locations or registers.
Exception handlers, processes, signal handlers, threads, and Java processes are all examples of logical ﬂows.
A logical ﬂow whose execution overlaps in time with another ﬂow is called a concurrent ﬂow, and the two ﬂows are said to run concurrently.
More precisely, ﬂows X and Y are concurrent with respect to each other if and only if X begins after Y begins and before Y ﬁnishes, or Y begins after X begins and before X ﬁnishes.
For example, in Figure 8.12, processes A and B run concurrently, as do A and C.
On the other hand, B and C do not run concurrently, because the last instruction of B executes before the ﬁrst instruction of C.
The general phenomenon of multiple ﬂows executing concurrently is known as concurrency.
The notion of a process taking turns with other processes is also known as multitasking.
Each time period that a process executes a portion of its ﬂow is called a time slice.
For example, in Figure 8.12, the ﬂow for Process A consists of two time slices.
Notice that the idea of concurrent ﬂows is independent of the number of processor cores or computers that the ﬂows are running on.
If two ﬂows overlap in time, then they are concurrent, even if they are running on the same processor.
However, wewill sometimes ﬁnd it useful to identify a proper subset of concurrent.
If two ﬂows are running concurrently on different processor cores or computers, then we say that they are parallel ﬂows, that they are running in parallel, and have parallel execution.
Practice Problem 8.1 Consider three processes with the following starting and ending times:
For each pair of processes, indicate whether they run concurrently (y) or not (n):
Although the contents of the memory associated with each private address space is different in general, each such space has the same general organization.
The bottom portion of the address space is reserved for the user program, with the usual text, data, heap, and stack segments.
The top portion of the address space is reserved for the kernel.
This part of the address space contains the code, data, and stack that the kernel uses when it executes instructions on behalf of the process (e.g., when the application program executes a system call)
In order for the operating system kernel to provide an airtight process abstraction, the processor must provide a mechanism that restricts the instructions that an application can execute, as well as the portions of the address space that it can access.
Processors typically provide this capability with a mode bit in some control register that characterizes the privileges that the process currently enjoys.
When the mode bit is set, the process is running in kernel mode (sometimes called supervisor mode)
A process running in kernel mode can execute any instruction in the instruction set and access any memory location in the system.
When the mode bit is not set, the process is running in user mode.
A process in user mode is not allowed to execute privileged instructions that do things such as halt the processor, change the mode bit, or initiate an I/O operation.
Nor is it allowed to directly reference code or data in the kernel area of the address space.
User programs must instead access kernel code and data indirectly via the system call interface.
A process running application code is initially in user mode.
The only way for the process to change from user mode to kernel mode is via an exception such as an interrupt, a fault, or a trapping system call.
When the exception occurs, and control passes to the exception handler, the processor changes themode fromuser mode to kernel mode.
When it returns to the application code, the processor changes the mode from kernel mode back to user mode.
Linux provides a clever mechanism, called the /proc ﬁlesystem, that allows user mode processes to access the contents of kernel data structures.
The /proc ﬁlesystemexports the contents ofmanykernel data structures as ahierarchyof text ﬁles that can be read by user programs.
The 2.6 version of the Linux kernel introduced a /sys ﬁlesystem, which exports additional low-level information about system buses and devices.
The operating system kernel implements multitasking using a higher-level form of exceptional control ﬂow known as a context switch.
The context switch mechanism is built on top of the lower-level exception mechanism that we discussed in Section 8.1
The context is the state that the kernel needs to restart a preempted process.
It consists of the values of objects such as the general purpose registers, the ﬂoating-point registers, the program counter, user’s stack, status registers, kernel’s stack, and various kernel data structures such as a page table that characterizes the address space, a process table that contains information about the current process, and a ﬁle table that contains information about the ﬁles that the process has opened.
At certain points during the execution of a process, the kernel can decide to preempt the current process and restart a previously preempted process.
This decision is known as scheduling, and is handled by code in the kernel called the scheduler.
When the kernel selects a new process to run, we say that the kernel has scheduled that process.
A context switch canoccurwhile the kernel is executing a systemcall onbehalf of the user.
If the system call blocks because it is waiting for some event to occur, then the kernel can put the current process to sleep and switch to another process.
For example, if a read system call requires a disk access, the kernel can opt to perform a context switch and run another process instead of waiting for the data to arrive from the disk.
Another example is the sleep system call, which is an explicit request to put the calling process to sleep.
In general, even if a system call does not block, the kernel can decide to perform a context switch rather than return control to the calling process.
A context switch can also occur as a result of an interrupt.
Each time a timer interrupt occurs, the kernel can decide that the current process has run long enough and switch to a new process.
Figure 8.14 shows an example of context switching between a pair of processes AandB.
In this example, initially processA is running in usermode until it traps to the kernel by executing a read system call.
The trap handler in the kernel requests a DMA transfer from the disk controller and arranges for the disk to interrupt the processor after the disk controller has ﬁnished transferring the data from disk to memory.
The disk will take a relatively long time to fetch the data (on the order of tens of milliseconds), so instead of waiting and doing nothing in the interim, the kernel performs a context switch from processA toB.
During the ﬁrst part of the switch, the kernel is executing instructions in kernel mode on behalf of process A.
Then at some point it begins executing instructions (still in kernel mode) on behalf of process B.
And after the switch, the kernel is executing instructions in user mode on behalf of process B.
Process B then runs for a while in user mode until the disk sends an interrupt to signal that data has been transferred from disk to memory.
The kernel decides that process B has run long enough and performs a context switch from process B to A, returning control in process A to the instruction immediately following the read system call.
Process A continues to run until the next exception occurs, and so on.
In general, hardware cache memories do not interact well with exceptional control ﬂows such as interrupts and context switches.
If the current process is interrupted brieﬂy by an interrupt, then the cache is cold for the interrupt handler.
If the handler accesses enough items from main memory, then the cache will also be cold for the interrupted process when it resumes.
In this case, we say that the handler has polluted the cache.
When a process resumes after a context switch, the cache is cold for the application program and must be warmed up again.
The strerror function returns a text string that describes the error associated with a particular value of errno.
We can simplify this code somewhat by deﬁning the following error-reporting function:
Given this function, our call to fork reduces from four lines to two lines:
We can simplify our code even further by using error-handling wrappers.
For a given base function foo, we deﬁne a wrapper function Foo with identical arguments, but with the ﬁrst letter of the name capitalized.
The wrapper calls the base function, checks for errors, and terminates if there are any problems.
For example, here is the error-handling wrapper for the fork function:
Given this wrapper, our call to fork shrinks to a single compact line:
We will use error-handling wrappers throughout the remainder of this book.
They allow us to keep our code examples concise, without giving you themistaken impression that it is permissible to ignore error checking.
Note that when we discuss system-level functions in the text, we will always refer to them by their lowercase base names, rather than by their uppercase wrapper names.
See Appendix A for a discussion of Unix error handling and the errorhandling wrappers used throughout this book.
The wrappers are deﬁned in a ﬁle called csapp.c, and their prototypes are deﬁned in a header ﬁle called csapp.h; these are available online from the CS:APP Web site.
Unix provides a number of system calls for manipulating processes from C programs.
This section describes the important functions and gives examples of how they are used.
Each process has a unique positive (nonzero) process ID (PID)
The getpid function returns the PID of the calling process.
The getppid function returns the PID of its parent (i.e., the process that created the calling process)
The getpid and getppid routines return an integer value of type pid_t, which on Linux systems is deﬁned in types.h as an int.
From a programmer’s perspective, we can think of a process as being in one of three states:
The process is either executing on the CPU or is waiting to be executed and will eventually be scheduled by the kernel.
Stopped.The execution of the process is suspended and will not be scheduled.
A process stops as a result of receiving a SIGSTOP, SIGTSTP, SIGTTIN, or SIGTTOU signal, and it remains stopped until it receives a SIGCONT signal, at which point it can begin running again.
A signal is a form of software interrupt that is described in detail in Section 8.5
The exit function terminates the process with an exit status of status.
The other way to set the exit status is to return an integer value from the main routine.
A parent process creates a new running child process by calling the fork function.
The newly created child process is almost, but not quite, identical to the parent.
The child gets an identical (but separate) copy of the parent’s user-level virtual address space, including the text, data, and bss segments, heap, and user stack.
The child also gets identical copies of any of the parent’s open ﬁle descriptors, which means the child can read and write any ﬁles that were open in the parent when it called fork.
The most signiﬁcant difference between the parent and the newly created child is that they have different PIDs.
The fork function is interesting (and often confusing) because it is called once but it returns twice: once in the calling process (the parent), and once in the newly created child process.
In the parent, fork returns the PID of the child.
Since the PID of the child is always nonzero, the return value provides an unambiguous way to tell whether the program is executing in the parent or the child.
Figure 8.15 shows a simple example of a parent process that usesfork to create a child process.
When we run the program on our Unix system, we get the following result:
Call once, return twice.The fork function is called once by the parent, but it returns twice: once to the parent and once to the newly created child.
This is fairly straightforward for programs that create a single child.
But programs with multiple instances of fork can be confusing and need to be reasoned about carefully.
The parent and the child are separate processes that run concurrently.
The instructions in their logical control ﬂows can be interleaved by the kernel in an arbitrary way.
When we run the program on our system, the parent process completes its printf statement ﬁrst, followed by the child.
In general, as programmers we can nevermake assumptions about the interleaving of the instructions in different processes.
If we could halt both the parent and the child immediately after the fork function returned in each process, we would see that the address space of each process is identical.
Each process has the same user stack, the same local variable values, the same heap, the sameglobal variable values, and the same code.Thus, in our example program, local variable x has a value of 1 in both the parent and the child when the fork function returns in line 8.However, since the parent and the child are separate processes, they each have their own private address spaces.
Any subsequent changes that a parent or child makes to x are private and are not reﬂected in thememory of the other process.
This is why the variable xhas different values in the parent and child when they call their respective printf statements.
Shared ﬁles.When we run the example program, we notice that both parent and child print their output on the screen.
The reason is that the child inherits all of the parent’s open ﬁles.
When the parent calls fork, the stdout ﬁle is open and directed to the screen.
The child inherits this ﬁle and thus its output is also directed to the screen.
When you are ﬁrst learning about the fork function, it is often helpful to sketch the process graph, where each horizontal arrow corresponds to a process that executes instructions from left to right, and each vertical arrow corresponds to the execution of a fork function.
Figure 8.16 Examples of fork programs and their process graphs.
Each of these calls printf once, so the program prints two output lines.
Thus, there are four processes, each of which calls printf, so the program generates four output lines.
Each process calls printf, so the program produces eight output lines.
When a process terminates for any reason, the kernel does not remove it from the system immediately.
Instead, the process is kept around in a terminated state until it is reaped by its parent.
When the parent reaps the terminated child, the kernel passes the child’s exit status to the parent, and then discards the terminated process, atwhichpoint it ceases to exist.A terminatedprocess that has not yet been reaped is called a zombie.
In folklore, a zombie is a living corpse, an entity that is half alive and half dead.
A zombie process is similar in the sense that while it has already terminated, the kernel maintains some of its state until it can be reaped by the parent.
If the parent process terminates without reaping its zombie children, the kernel arranges for the init process to reap them.
Even though zombies are not running, they still consume system memory resources.
A process waits for its children to terminate or stop by calling the waitpid function.
By default (when options = 0), waitpid suspends execution of the calling process until a child process in its wait set terminates.
If a process in the wait set has already terminated at the time of the call, then waitpid returns immediately.
In either case, waitpid returns the PID of the terminated child that caused waitpid to return, and the terminated child is removed from the system.
The members of the wait set are determined by the pid argument:
If pid = -1, then the wait set consists of all of the parent’s child processes.
The waitpid function also supports other kinds of wait sets, involving Unix process groups, that we will not discuss.
The default behavior can be modiﬁed by setting options to various combinations of the WNOHANG and WUNTRACED constants:
WNOHANG: Return immediately (with a return value of 0) if none of the child processes in the wait set has terminated yet.
The default behavior suspends the calling process until a child terminates.
This option is useful in those cases where you want to continue doing useful work while waiting for a child to terminate.
WUNTRACED: Suspend execution of the calling process until a process in the wait set becomes either terminated or stopped.
Return the PID of the terminated or stopped child that caused the return.
This option is useful when you want to check for both terminated and stopped children.
If the status argument is non-NULL, then waitpid encodes status information about the child that caused the return in the status argument.
The wait.h include ﬁle deﬁnes several macros for interpreting the status argument:
WIFEXITED(status): Returns true if the child terminated normally, via a call to exit or a return.
WTERMSIG(status): Returns the number of the signal that caused the child process to terminate.
WSTOPSIG(status): Returns the number of the signal that caused the child to stop.
Constants such as WNOHANG and WUNTRACED are deﬁned by system header ﬁles.
For example, WNOHANG and WUNTRACED are deﬁned (indirectly) by the wait.h header ﬁle:
In order to use these constants, you must include the wait.h header ﬁle in your code:
The man page for each Unix function lists the header ﬁles to include whenever you use that function in your code.
Also, in order to check return codes such as ECHILD and EINTR, you must include errno.h.
To simplify our code examples, we include a single header ﬁle called csapp.h that includes the header ﬁles for all of the functions used in the book.
The csapp.h header ﬁle is available online from the CS:APP Web site.
Practice Problem 8.3 List all of the possible output sequences for the following program:
Because the waitpid function is somewhat complicated, it is helpful to look at a few examples.
Figure 8.17 shows a program that uses waitpid to wait, in no particular order, for all of its N children to terminate.
Before moving on, make sure you understand why line 12 is executed by each of the children, but not the parent.
The only normal termination is if there are no more children */
Figure 8.17 Using the waitpid function to reap zombie children in no particular order.
Notice that the program reaps its children in no particular order.
The order that they were reaped is a property of this speciﬁc computer system.
The only normal termination is if there are no more children */
Figure 8.18 Using waitpid to reap zombie children in the order they were created.
This is an example of the nondeterministic behavior that canmake reasoning about concurrency so difﬁcult.
Either of the two possible outcomes is equally correct, and as a programmer you may never assume that one outcome will always occur, no matter how unlikely the other outcome appears to be.
The only correct assumption is that each possible outcome is equally likely.
Figure 8.18 shows a simple change that eliminates this nondeterminism in the output order by reaping the children in the same order that they were created by the parent.
In line 11, the parent stores the PIDs of its children in order, and then waits for each child in this same order by calling waitpid with the appropriate PID in the ﬁrst argument.
The sleep function suspends a process for a speciﬁed period of time.
Sleep returns zero if the requested amount of time has elapsed, and the number of seconds still left to sleep otherwise.
The latter case is possible if the sleep function returns prematurely because it was interrupted by a signal.
Another function that we will ﬁnd useful is the pause function, which puts the calling function to sleep until a signal is received by the process.
Practice Problem 8.5 Write a wrapper function for sleep, called snooze, with the following interface:
The snooze function behaves exactly as the sleep function, except that it prints a message describing how long the process actually slept:
The execve function loads and runs a new program in the context of the current process.
The execve function loads and runs the executable object ﬁle filename with the argument list argv and the environment variable list envp.
Execve returns to the calling program only if there is an error such as not being able to ﬁnd filename.
So unlike fork, which is called once but returns twice, execve is called once and never returns.
The argument list is represented by the data structure shown in Figure 8.19
The argv variable points to a null-terminated array of pointers, each of which.
The list of environment variables is represented by a similar data structure, shown inFigure 8.20
Theenvp variable points to a null-terminated array of pointers to environment variable strings, each of which is a name-value pair of the form “NAME=VALUE”
After execve loads filename, it calls the startup code described in Section 7.9
The startup code sets up the stack and passes control to the main routine of the new program, which has a prototype of the form.
Let’s work our way from the bottom of the stack (the highest address) to the top (the lowest address)
Figure 8.21 Typical organization of the user stack when a new program starts.
These are followed further up the stack by a nullterminated array of pointers, each of which points to an environment variable string on the stack.
The global variable environpoints to the ﬁrst of these pointers, envp[0]
The environment array is followed immediately by the null-terminated argv[] array, with each element pointing to an argument string on the stack.
Returns: ptr to name if exists, NULL if no match.
The getenv function searches the environment array for a string “name=value”
If found, it returns a pointer to value, otherwise it returns NULL.
If the environment array contains a string of the form “name=oldvalue”, then unsetenv deletes it and setenv replaces oldvalue with newvalue, but only if overwrite is nonzero.
If name does not exist, then setenv adds “name=newvalue” to the array.
This is a good place to pause and make sure you understand the distinction between a program and a process.
A program is a collection of code and data; programs can exist as object modules on disk or as segments in an address space.
A process is a speciﬁc instance of a program in execution; a program always runs in the context of some process.
Understanding this distinction is important if you want to understand the fork and execve functions.
The fork function runs the same program in a new child process that is a duplicate of the parent.
The execve function loads and runs a new program in the.
While it overwrites the address space of the current process, it does not create a new process.
The new program still has the same PID, and it inherits all of the ﬁle descriptors that were open at the time of the call to the execve function.
Practice Problem 8.6 Write a program called myecho that prints its command line arguments and environment variables.
Programs such as Unix shells and Web servers (Chapter 11) make heavy use of the fork and execve functions.
A shell is an interactive application-level program that runs other programs on behalf of the user.
The original shell was the sh program, which was followed by variants such as csh, tcsh, ksh, and bash.
A shell performs a sequence of read/evaluate steps, and then terminates.
The read step reads a command line from the user.
The evaluate step parses the command line and runs programs on behalf of the user.
Figure 8.22 shows the main routine of a simple shell.
The shell prints a command-line prompt, waits for the user to type a command line on stdin, and then evaluates the command line.
Figure 8.23 shows the code that evaluates the command line.
Its ﬁrst task is to call the parseline function (Figure 8.24), which parses the space-separated command-line arguments and builds the argv vector thatwill eventually be passed to execve.
The ﬁrst argument is assumed to be either the name of a built-in shell command that is interpreted immediately, or an executable object ﬁle that will be loaded and run in the context of a new child process.
Figure 8.22 The main routine for a simple shell program.
Otherwise it returns 0, indicating that the program should be run in the foreground (the shell waits for it to complete)
After parsing the command line, the eval function calls the builtin_command function, which checks whether the ﬁrst command line argument is a built-in shell command.
Our simple shell has just one built-in command, the quit command, which terminates the shell.
Real shells have numerous commands, such as pwd, jobs, and fg.
If the user has asked for the program to run in the background, then the shell returns to the top of the loop and waits for the next command line.
Otherwise the shell uses the waitpid function to wait for the job to terminate.
When the job terminates, the shell goes on to the next iteration.
Notice that this simple shell is ﬂawed because it does not reap any of its background children.
Correcting this ﬂaw requires the use of signals, which we describe in the next section.
If first arg is a builtin command, run it and return true */
Figure 8.24 parseline: Parses a line of input for the shell.
To this point in our study of exceptional control ﬂow, we have seen how hardware and software cooperate to provide the fundamental low-level exception mechanism.
We have also seen how the operating system uses exceptions to support a form of exceptional control ﬂow known as the process context switch.
In this section, we will study a higher-level software form of exceptional control ﬂow, known as a Unix signal, that allows processes and the kernel to interrupt other processes.
Notes: (1) Years ago, main memory was implemented with a technology known as core memory.
A signal is a small message that notiﬁes a process that an event of some type has occurred in the system.
Typing “man 7 signal” on the shell command line gives the list.
Each signal type corresponds to some kind of system event.
Low-level hardware exceptions are processed by the kernel’s exception handlers and would not.
Signals provide a mechanism for exposing the occurrence of such exceptions to user processes.
For example, if a process attempts to divide by zero, then the kernel sends it a SIGFPE signal (number 8)
If a process executes an illegal instruction, the kernel sends it a SIGILL signal (number 4)
If a process makes an illegal memory reference, the kernel sends it a SIGSEGV signal (number 11)
Other signals correspond to higher-level software events in the kernel or in other user processes.
For example, if you type a ctrl-c (i.e., press the ctrl key and the c key at the same time) while a process is running in the foreground, then the kernel sends a SIGINT (number 2) to the foreground process.
A process can forcibly terminate another process by sending it a SIGKILL signal (number 9)
When a child process terminates or stops, the kernel sends a SIGCHLD signal (number 17) to the parent.
The transfer of a signal to a destination process occurs in two distinct steps:
Sending a signal.The kernel sends (delivers) a signal to a destination process by updating some state in the context of the destination process.
The signal is delivered for one of two reasons: (1) The kernel has detected a system event such as a divide-by-zero error or the termination of a child process.
A process has invoked the kill function (discussed in the next section) to explicitly request the kernel to send a signal to the destination process.
Receiving a signal.A destination process receives a signal when it is forced by the kernel to react in some way to the delivery of the signal.
The process can either ignore the signal, terminate, or catch the signal by executing a user-level function called a signal handler.
Figure 8.26 shows the basic idea of a handler catching a signal.
A signal that has been sent but not yet received is called a pending signal.
At any point in time, there can be at most one pending signal of a particular type.
If a process has a pending signal of type k, then any subsequent signals of type k sent to that process are not queued; they are simply discarded.
A process can selectively block the receipt of certain signals.
When a signal is blocked, it can be delivered, but the resulting pending signal will not be received until the process unblocks the signal.
Receipt of a signal triggers a control transfer to a signal handler.
After it ﬁnishes processing, the handler returns control to the interrupted program.
For each process, the kernel maintains the set of pending signals in the pending bit vector, and the set of blocked signals in the blocked bit vector.
The kernel sets bit k in pending whenever a signal of type k is delivered and clears bit k in pending whenever a signal of type k is received.
Unix systems provide a number of mechanisms for sending signals to processes.
All of the mechanisms rely on the notion of a process group.
Every process belongs to exactly one process group, which is identiﬁed by a positive integer process group ID.
The getpgrp function returns the process group ID of the current process.
By default, a child process belongs to the same process group as its parent.
A process can change the process group of itself or another process by using the setpgid function:
The setpgid function changes the process group of process pid to pgid.
If pid is zero, the PID of the current process is used.
If pgid is zero, the PID of the process speciﬁed by pid is used for the process group ID.
For example, if process 15213 is the calling process, then.
A negative PID causes the signal to be sent to every process in process group PID.
Note that we use the complete path /bin/kill here because some Unix shells have their own built-in kill command.
Unix shells use the abstraction of a job to represent the processes that are created as a result of evaluating a single command line.
At any point in time, there is at most one foreground job and zero or more background jobs.
The shell creates a separate process group for each job.
Typically, the process group ID is taken from one of the parent processes in the job.
For example, Figure 8.27 shows a shell with one foreground job and two background jobs.
Typing ctrl-c at the keyboard causes a SIGINT signal to be sent to the shell.
The shell catches the signal (see Section 8.5.3) and then sends a SIGINT to every process in the foreground process group.
Similarly, typing crtl-z sends a SIGTSTP signal to the shell, which catches it and sends a SIGTSTP signal to every process in the foreground process group.
In the default case, the result is to stop (suspend) the foreground job.
Processes send signals to other processes (including themselves) by calling the kill function.
If pid is greater than zero, then the kill function sends signal number sig to process pid.
If pid is less than zero, then kill sends signal sig to every process in process group abs(pid)
Figure 8.28 shows an example of a parent that uses the kill function to send a SIGKILL signal to its child.
Figure 8.28 Using the kill function to send a signal to a child.
A process can send SIGALRM signals to itself by calling the alarm function.
Returns: remaining secs of previous alarm, or 0 if no previous alarm.
The alarm function arranges for the kernel to send a SIGALRM signal to the calling process in secs seconds.
If secs is zero, then no new alarm is scheduled.
In any event, the call to alarm cancels any pending alarms, and returns the number of seconds remaining until any pending alarm was due to be delivered (had not this call to alarm canceled it), or 0 if there were no pending alarms.
Figure 8.29 shows a program called alarm that arranges to be interrupted by a SIGALRM signal every second for ﬁve seconds.
When we run the program in Figure 8.29, we get the followingoutput: a “BEEP”every second for ﬁve seconds, followedbya “BOOM” when the program terminates.
Notice that the program in Figure 8.29 uses the signal function to install a signal handler function (handler) that is called asynchronously, interrupting the inﬁnite while loop in main, whenever the process receives a SIGALRM signal.
When the handler function returns, control passes back to main, which picks up where it was interrupted by the arrival of the signal.
Installing and using signal handlers can be quite subtle, and is the topic of the next few sections.
If this set is empty (the usual case), then the kernel passes control to the next instruction (Inext) in the logical control ﬂow of p.
However, if the set is nonempty, then the kernel chooses some signal k in the set (typically the smallest k) and forces p to receive signal k.
The receipt of the signal triggers some action by the process.
Once the process completes the action, then control passes back to the next instruction (Inext) in the logical control ﬂow of p.
Each signal type has a predeﬁned default action, which is one of the following:
Figure 8.29 Using the alarm function to schedule periodic events.
Figure 8.25 shows the default actions associated with each type of signal.
For example, the default action for the receipt of a SIGKILL is to terminate the receiving process.
On the other hand, the default action for the receipt of a SIGCHLD is to ignore the signal.
A process can modify the default action associated with a signal by using the signal function.
The only exceptions are SIGSTOP and SIGKILL, whose default actions cannot be changed.
Returns: ptr to previous handler if OK, SIG_ERR on error (does not set errno)
The signal function can change the action associated with a signal signum in one of three ways:
If handler is SIG_IGN, then signals of type signum are ignored.
If handler is SIG_DFL, then the action for signals of type signum reverts to the default action.
Otherwise, handler is the address of a user-deﬁned function, called a signal handler, that will be called whenever the process receives a signal of type signum.
Changing the default action by passing the address of a handler to the signal function is known as installing the handler.
The invocation of the handler is called catching the signal.
The execution of the handler is referred to as handling the signal.
When a process catches a signal of type k, the handler installed for signal k is invoked with a single integer argument set to k.
This argument allows the same handler function to catch different types of signals.
When thehandler executes itsreturn statement, control (usually) passes back to the instruction in the control ﬂow where the process was interrupted by the receipt of the signal.We say “usually” because in some systems, interrupted system calls return immediately with an error.
Figure 8.30 shows a program that catches the SIGINT signal sent by the shell whenever the user types ctrl-c at the keyboard.
The default action for SIGINT is to immediately terminate the process.
In this example, we modify the default behavior to catch the signal, print a message, and then terminate the process.
Signal handlers are yet another example of concurrency in a computer system.
The execution of the signal handler interrupts the execution of themainC routine, akin to theway that a low-level exception handler interrupts the control ﬂowof the current application program.
Since the logical control ﬂow of the signal handler overlaps the logical control ﬂow of the main routine, the signal handler and the main routine run concurrently.
Write your program so that the user can interrupt the snooze function by typing ctrl-c at the keyboard.
Figure 8.30 A program that uses a signal handler to catch a SIGINT signal.
Signal handling is straightforward for programs that catch a single signal and then terminate.
However, subtle issues arise when a program catches multiple signals.
Unix signal handlers typically block pending signals of the type currently being processed by the handler.
For example, suppose a process has caught a SIGINT signal and is currently running its SIGINT handler.
If another SIGINT signal is sent to the process, then the SIGINT will become pending, but will not be received until after the handler returns.
There can be at most one pending signal of any particular type.
Thus, if two signals of type k are sent to a destination process while signal k is blocked because the destination process is currently executing a handler for signal k, then the second signal is simply discarded; it is not queued.
The key idea is that the existence of a pending signal merely indicates that at least one signal has arrived.
System calls such as read, wait, and accept that can potentially block the process for a long period of time are called slow system calls.
On some systems, slow system calls that are interrupted when a handler catches a signal do not resume when the signal handler returns, but instead return immediately to the user with an error condition and errno set to EINTR.
Let’s look more closely at the subtleties of signal handling, using a simple application that is similar in nature to real programs such as shells and Web servers.
The basic structure is that a parent process creates some children that run independently for a while and then terminate.
The parent must reap the children to avoid leaving zombies in the system.Butwe alsowant the parent to be free to do other work while the children are running.
So we decide to reap the children with a SIGCHLD handler, instead of explicitly waiting for the children to terminate.
Recall that the kernel sends a SIGCHLD signal to the parent whenever one of its children terminates or stops.
The parent installs a SIGCHLD handler, and then creates three children, each of which runs for 1 second and then terminates.
In the meantime, the parent waits for a line of input from the terminal and then processes it.
When each child terminates, the kernel notiﬁes the parent by sending it a SIGCHLD signal.
The parent catches the SIGCHLD, reaps one child, does some additional cleanup work (modeled by the sleep(2) statement), and then returns.
When we run it on our Linux system, however, we get the following output:
From the output, we note that although three SIGCHLD signals were sent to the parent, only two of these signals were received, and thus the parent only reaped two children.
If we suspend the parent process, we see that, indeed, child process 10321 was never reaped and remains a zombie (indicated by the string “defunct” in the output of the ps command):
Whatwent wrong? The problem is that our code failed to account for the facts that signals can block and that signals are not queued.
Here’s what happened: The ﬁrst signal is received and caught by the parent.
While the handler is still processing the ﬁrst signal, the second signal is delivered and added to the set of pending signals.
Parent waits for terminal input and then processes it */
Shortly thereafter, while the handler is still processing the ﬁrst signal, the third signal arrives.
Since there is already a pending SIGCHLD, this third SIGCHLD signal is discarded.
Sometime later, after the handler has returned, the kernel notices that there is a pending SIGCHLD signal and forces the parent to receive the signal.
The parent catches the signal and executes the handler a second time.
After the handler ﬁnishes processing the second signal, there are no more pending SIGCHLD signals, and there never will be, because all knowledge of the third SIGCHLD has been lost.
The crucial lesson is that signals cannot be used to count the occurrence of events in other processes.
To ﬁx the problem, we must recall that the existence of a pending signal only implies that at least one signal has been delivered since the last time the process received a signal of that type.
So we must modify the SIGCHLD handler to reap as many zombie children as possible each time it is invoked.
When we run signal2 on our Linux system, it now correctly reaps all of the zombie children:
If we run the signal2 program on an older version of the Solaris operating system, it correctly reaps all of the zombie children.
However, now the blocked read system call returns prematurely with an error, before we are able to type in our input on the keyboard:
What went wrong? The problem arises because on this particular Solaris system, slow system calls such as read are not restarted automatically after they are interrupted by the delivery of a signal.
Instead, they return prematurely to the calling application with an error condition, unlike Linux systems, which restart interrupted system calls automatically.
In order to write portable signal handling code, we must allow for the possibility that system calls will return prematurely and then restart them manually.
Parent waits for terminal input and then processes it */
However, it does not allow for the possibility that system calls can be interrupted.
The EINTR return code in errno indicates that the read system call returned prematurely after it was interrupted.
When we run our new signal3 program on a Solaris system, the program runs correctly:
Practice Problem 8.8 What is the output of the following program?
Manually restart the read call if it is interrupted */
The differences in signal handling semantics from system to system—such as whether or not an interrupted slow system call is restarted or aborted prematurely—is an ugly aspect of Unix signal handling.
To deal with this problem, the Posix standard deﬁnes the sigaction function, which allows users on Posixcompliant systems such as Linux and Solaris to clearly specify the signal handling semantics they want.
The sigaction function is unwieldy because it requires the user to set the entries of a structure.
Richard Stevens [109], is to deﬁne a wrapper function, called Signal, that calls sigaction for us.
Figure 8.34 shows the deﬁnition of Signal, which is invoked in the same way as the signal function.
The Signalwrapper installs a signal handler with the following signal handling semantics:
Only signals of the type currently being processed by the handler are blocked.
Figure 8.34 Signal: A wrapper for sigaction that provides portable signal handling on Posix-compliant systems.
Some older Unix systems restore the signal action to its default action after a signal has been processed by a handler.
The only difference is that we have installed the handler with a call to Signal rather than a call to signal.
The program now runs correctly on both our Solaris and Linux systems, and we no longer need to manually restart interrupted read system calls.
Applications can explicitly block and unblock selected signals using the sigprocmask function:
The sigprocmask function changes the set of currently blocked signals (the blocked bit vector described in Section 8.5.1)
If oldset is non-NULL, the previous value of the blocked bit vector is stored in oldset.
Signal sets such as set are manipulated using the following functions.
Parent waits for terminal input and then processes it */
The problem of how to program concurrent ﬂows that read and write the same storage locations has challenged generations of computer scientists.
In general, the number of potential interleavings of the ﬂows is exponential in the number of instructions.
Some of those interleavings will produce correct answers, and others will not.
The fundamental problem is to somehow synchronize the concurrent ﬂows so as to allow the largest set of feasible interleavings such that each of the feasible interleavings produces a correct answer.
However, we can use what you’ve learned about exceptional control ﬂow in this chapter to give you a sense of the interesting intellectual challenges associated with concurrency.
For example, consider the program in Figure 8.36, which captures the structure of a typical Unix shell.
The parent keeps track of its current children using entries in a job list, with one entry per job.
The addjob and deletejob functions add and remove entries from the job list, respectively.
After the parent creates a new child process, it adds the child to the job list.
When the parent reaps a terminated (zombie) child in the SIGCHLD signal handler, it deletes the child from the job list.
The parent executes the fork function and the kernel schedules the newly created child to run instead of the parent.
Before the parent is able to run again, the child terminates and becomes a zombie, causing the kernel to deliver a SIGCHLD signal to the parent.
Later, when the parent becomes runnable again but before it is executed, the kernel notices the pending SIGCHLD and causes it to be received by running the signal handler in the parent.
The signal handler reaps the terminated child and calls deletejob, which does nothing because the parent has not added the child to the list yet.
After the handler completes, the kernel then runs the parent, which returns from fork and incorrectly adds the (nonexistent) child to the job list by calling addjob.
Thus, for some interleavings of the parent’smain routine and signal handling ﬂows, it is possible for deletejob to be called before addjob.
This results in an incorrect entry on the job list, for a job that no longer exists and that will never be removed.
On the other hand, there are also interleavings where events occur in the correct order.
For example, if the kernel happens to schedule the parent to run when the fork call returns instead of the child, then the parent will correctly add the child to the job list before the child terminates and the signal handler removes the job from the list.
This is an example of a classic synchronization error known as a race.
In this case, the race is between the call to addjob in the main routine and the call to deletejob in the handler.
If addjob wins the race, then the answer is correct.
Figure 8.36 A shell program with a subtle synchronization error.
If the child terminates before the parent is able to run, then addjob and deletejob will be called in the wrong order.
Such errors are enormously difﬁcult to debug because it is often impossible to test every interleaving.
You may run the code a billion times without a problem, but then the next test results in an interleaving that triggers the race.
By blocking SIGCHLD signals before the call to fork and then unblocking them only after we have called addjob, we guarantee that the child will be reaped after it is added to the job list.
Notice that children inherit the blocked set of their parents, sowemust be careful to unblock the SIGCHLD signal in the child before calling execve.
In this example, the parent ensures that addjob executes before the corresponding deletejob.
Generate a different seed each time the function is called */
Determine whether to sleep in parent of child and for how long */
Randomly decide to sleep in the parent or the child */
Figure 8.38 A wrapper for fork that randomly determines the order in which the parent and child execute.
The parent and child ﬂip a coin to determine which will sleep, thus giving the other process a chance to be scheduled.
Aside A handy trick for exposing races in your code.
Races such as those in Figure 8.36 are difﬁcult to detect because they depend on kernel-speciﬁc scheduling decisions.
After a call to fork, some kernels schedule the child to run ﬁrst, while other kernels schedule the parent to run ﬁrst.
If you were to run the code in Figure 8.36 on one of the latter systems, it would never fail, no matter how many times you tested it.
But as soon as you ran it on one of the former systems, then the race would be exposed and the code would fail.
Figure 8.38 shows a wrapper function that can help expose such hidden assumptions about the execution ordering of parent and child processes.
The basic idea is that after each call to fork, the parent and child ﬂip a coin to determine which of them will sleep for a bit, thus giving the other process the opportunity to run ﬁrst.
If we were to run the code multiple times, then with high probability we would exercise both orderings of child and parent executions, regardless of the particular kernel’s scheduling policy.
Nonlocal jumps are provided by the setjmp and longjmp functions.
The calling environment includes the program counter, stack pointer, and general purpose registers.
The longjmp function restores the calling environment from the env buffer and then triggers a return from the most recent setjmp call that initialized env.
The setjmp then returns with the nonzero return value retval.
The interactions between setjmp and longjmp can be confusing at ﬁrst glance.
On the other hand, the longjmp function is called once, but never returns.
An important application of nonlocal jumps is to permit an immediate return from a deeply nested function call, usually as a result of detecting some error condition.
If an error condition is detected deep in a nested function call, we can use a nonlocal jump to return directly to a common localized error handler instead of laboriously unwinding the call stack.
Figure 8.39 shows an example of how this might work.
The main routine ﬁrst calls setjmp to save the current calling environment, and then calls function foo, which in turn calls function bar.
If foo or bar encounter an error, they return immediately from the setjmp via a longjmp call.
The nonzero return value of the setjmp indicates the error type, which can then be decoded and handled in one place in the code.
Another important application of nonlocal jumps is to branch out of a signal handler to a speciﬁc code location, rather than returning to the instruction thatwas interrupted by the arrival of the signal.
Figure 8.40 shows a simple program that illustrates this basic technique.
The program uses signals and nonlocal jumps to do a soft restart whenever the user types ctrl-c at the keyboard.
The sigsetjmp and siglongjmp functions are versions of setjmp and longjmp that can be used by signal handlers.
The initial call to the sigsetjmp function saves the calling environment and signal context (including the pending and blocked signal vectors) when the program ﬁrst starts.
When the user types ctrl-c, the shell sends a SIGINT signal to the process, which catches it.
Instead of returning from the signal handler, which would pass control back to the interrupted processing loop, the handler performs a nonlocal jump back to the beginning of the main program.
When we ran the program on our system, we got the following output:
You can think of a catch clause inside a try statement as being akin to a setjmp function.
Similarly, a throw statement is similar to a longjmp function.
This example shows the framework for using nonlocal jumps to recover from error conditions in deeply nested functions without having to unwind the entire stack.
Figure 8.40 A program that uses nonlocal jumps to restart itself when the user types ctrl-c.
Linux systems provide a number of useful tools for monitoring and manipulating processes:
Compile your programwith-static to get a cleaner tracewithout a lot of output related to shared libraries.
Exceptional control ﬂow (ECF) occurs at all levels of a computer system and is a basic mechanism for providing concurrency in a computer system.
At the hardware level, exceptions are abrupt changes in the control ﬂow that are triggered by events in the processor.
The control ﬂow passes to a software handler, which does some processing and then returns control to the interrupted control ﬂow.
There are four different types of exceptions: interrupts, faults, aborts, and traps.
Interrupts occur asynchronously (with respect to any instructions) when an external I/O device such as a timer chip or a disk controller sets the interrupt pin on the processor chip.
Faults and aborts occur synchronously as the result of the execution of an instruction.
Fault handlers restart the faulting instruction, while abort handlers never return control to the interrupted ﬂow.
Finally, traps are like function calls that are used to implement the system calls that provide applications with controlled entry points into the operating system code.
At the operating system level, the kernel uses ECF to provide the fundamental notion of a process.
At the interface between the operating system and applications, applications can create child processes, wait for their child processes to stop or terminate, run new programs, and catch signals from other processes.
The semantics of signal handling is subtle and can vary from system to system.However, mechanisms exist on Posix-compliant systems that allow programs to clearly specify the expected signal handling semantics.
Finally, at the application level, C programs can use nonlocal jumps to bypass the normal call/return stack discipline and branch directly from one function to another.
The Intel macroarchitecture speciﬁcation contains a detailed discussion of exceptions and interrupts on Intel processors [27]
Richard Stevens [110] is a valuable and highly readable description of how to work with processes and signals from application programs.
Bovet and Cesati [11] give a wonderfully clear description of the Linux kernel, including details of the process and signal implementations.
For each pair of processes, indicate whether they run concurrently (y) or not (n):
Note: The atexit function takes a pointer to a function and adds it to a list of functions (initially empty) that will be called when the exit function is called.
The ls program gets the width of the screen from the COLUMNS environment variable.
If COLUMNS is unset, then ls assumes that the screen is 80 columns wide.
Thus, you can check your handling of the environment variables by setting the COLUMNS environment to something smaller than 80:
The mysystem function executes command by calling “/bin/sh -c command”, and then returns after command has completed.
If command exits normally (by calling the exit function or executing a return statement), then mysystem returns the command exit status.
Otherwise, if command terminates abnormally, then mysystem returns the status returned by the shell.
Each child terminates abnormally after attempting to write to a location in the read-only text segment.
The parent prints output that is identical (except for the PIDs) to the following:
The command line typed by the user consists of a name and zero ormore arguments, all separated by one or more spaces.
If name is a built-in command, the shell handles it immediately and waits for the next command line.
Otherwise, the shell assumes that name is an executable ﬁle, which it loads and runs in the context of an initial child process (job)
The process group ID for the job is identical to the PID of the child.
Each job is identiﬁed by either a process ID (PID) or a job ID (JID), which is a small arbitrary positive integer assigned by the shell.
If the command line ends with an ampersand, then the shell runs the job in the background.
Typing ctrl-c (ctrl-z) causes the shell to send a SIGINT (SIGTSTP) signal to every process in the foreground process group.
The <job> argument can be either a PID or a JID.
If any job terminates because it receives a signal that was not caught, then the shell prints a message to the terminal with the job’s PID and a description of the offending signal.
Processes A and C are not concurrent, because their executions do not overlap; A ﬁnishes before C begins.
However, in this program, the parent and child execute non-disjoint sets of instructions, which is possible because the parent and child have identical code segments.
This can be a difﬁcult conceptual hurdle, so be sure you understand the solution to this problem.
The key idea here is that the child executes both printf statements.
Each time we run this program, it generates six output lines.
The ordering of the output lines will vary from system to system, depending on the how the kernel interleaves the instructions of the parent and the child.
In general, any topological sort of the following graph is a valid ordering:
For example, when we run the program on our system, we get the following output:
But since the default action upon receipt of a SIGINT is to terminate the process (Figure 8.25), we must install a SIGINT handler to allow the sleep function to return.
The handler simply catches the SIGNALand returns control to the sleep function, which returns immediately.
The parent starts by printing “2”, then forks the child, which spins in an inﬁnite loop.
The parent then sends a signal to the child, and waits for it to terminate.
Processes in a system share the CPU and main memory with other processes.
As demand on the CPU increases, processes slow down in some reasonably smooth way.
But if too many processes need too much memory, then some of them will simply not be able to run.
When a program is out of space, it is out of luck.
If some process inadvertently writes to the memory usedby another process, that processmight fail in somebewildering fashion totally unrelated to the program logic.
In order to manage memory more efﬁciently and with fewer errors, modern systems provide an abstraction of main memory known as virtual memory (VM)
Virtual memory is an elegant interaction of hardware exceptions, hardware address translation, main memory, disk ﬁles, and kernel software that provides each process with a large, uniform, and private address space.
With one clean mechanism, virtual memory provides three important capabilities.
It uses main memory efﬁciently by treating it as a cache for an address space stored on disk, keeping only the active areas in main memory, and transferring data back and forth between disk and memory as needed.
It simpliﬁes memory management by providing each process with a uniform address space.
It protects the address space of each process from corruption by other processes.
Virtual memory is one of the great ideas in computer systems.
Amajor reason for its success is that it works silently and automatically, without any intervention from the application programmer.
Since virtual memory works so well behind the scenes, whywould a programmer need to understand it?There are several reasons.
Virtual memory pervades all levels of computer systems, playing key roles in the design of hardware exceptions, assemblers, linkers, loaders, shared objects, ﬁles, and processes.
Understanding virtual memory will help you better understand how systems work in general.
Virtual memory is powerful.Virtual memory gives applications powerful capabilities to create and destroy chunks of memory, map chunks of memory to portions of disk ﬁles, and share memory with other processes.
For example, did you know that you can read ormodify the contents of a disk ﬁle by reading and writing memory locations? Or that you can load the contents of a ﬁle into memory without doing any explicit copying? Understanding virtual memory will help you harness its powerful capabilities in your applications.
If virtualmemory is used improperly, applications can suffer from perplexing and insidious memory-related bugs.
For example, a program with a bad pointer can crash immediately with a “Segmentation fault” or a “Protection fault,” run silently for hours before crashing, or scariest of all, run to completion with incorrect results.
Understanding virtual memory, and the allocation packages such as malloc that manage it, can help you avoid these errors.
The ﬁrst half of the chapter describes how virtual memory works.
There is no avoiding the fact that VM is complicated, and the discussion reﬂects this in places.
The good news is that if you work through the details, you will be able to simulate the virtual memory mechanism of a small system by hand, and the virtual memory idea will be forever demystiﬁed.
The second half builds on this understanding, showing you how to use and manage virtual memory in your programs.
You will learn how to manage virtual memory via explicit memorymapping and calls to dynamic storage allocators such as the malloc package.
You will also learn about a host of common memoryrelated errors in C programs and how to avoid them.
The main memory of a computer system is organized as an array of M contiguous byte-sized cells.
Given this simple organization, the most natural way for a CPU to access memory would be to use physical addresses.
When theCPUexecutes the load instruction, it generates an effective physical address and passes it to main memory over the memory bus.
Early PCs used physical addressing, and systems such as digital signal processors, embedded microcontrollers, and Cray supercomputers continue to do so.
However, modern processors use a form of addressing known as virtual addressing, as shown in Figure 9.2
With virtual addressing, the CPU accesses main memory by generating a virtual address (VA), which is converted to the appropriate physical address before being sent to the memory.
The task of converting a virtual address to a physical one is known as address translation.
Dedicated hardware on the CPU chip called the memory management unit (MMU) translates virtual addresses on the ﬂy, using a look-up table stored inmain memory whose contents are managed by the operating system.
An address space is an ordered set of nonnegative integer addresses.
A system also has a physical address space that corresponds to the M bytes of physical memory in the system:
The concept of an address space is important because it makes a clean distinction between data objects (bytes) and their attributes (addresses)
Once we recognize this distinction, then we can generalize and allow each data object to have multiple independent addresses, each chosen from a different address space.
Figure 9.3 How a VM system uses main memory as a cache.
Each byte of main memory has a virtual address chosen from the virtual address space, and a physical address chosen from the physical address space.
Practice Problem 9.1 Complete the following table, ﬁlling in the missing entries and replacing each question mark with the appropriate integer.
Conceptually, a virtualmemory is organized as an array ofN contiguous byte-sized cells stored on disk.
Each byte has a unique virtual address that serves as an index into the array.
The contents of the array on disk are cached in main memory.
As with any other cache in the memory hierarchy, the data on disk (the lower level) is partitioned into blocks that serve as the transfer units between the disk and the mainmemory (the upper level)
Each virtual page is P = 2p bytes in size.
Similarly, physical memory is partitioned into physical pages (PPs), also P bytes in size.
At any point in time, the set of virtual pages is partitioned into three disjoint subsets:
Unallocated: Pages that have not yet been allocated (or created) by the VM system.
Unallocated blocks do not have any data associated with them, and thus do not occupy any space on disk.
Cached: Allocated pages that are currently cached in physical memory.
Uncached: Allocated pages that are not cached in physical memory.
The example in Figure 9.3 shows a small virtual memory with eight virtual pages.
The position of the DRAM cache in the memory hierarchy has a big impact on the way that it is organized.
Further, the cost of reading the ﬁrst byte from a disk sector is about 100,000 times slower than reading successive bytes in the sector.
The bottom line is that the organization of the DRAM cache is driven entirely by the enormous cost of misses.
The replacement policy onmisses also assumes greater importance, because the penalty associated with replacing the wrong virtual page is so high.
Thus, operating systems use much more sophisticated replacement algorithms for DRAM caches than the hardware does for SRAM caches.
Finally, because of the large access time of disk, DRAM caches always use write-back instead of write-through.
As with any cache, the VM system must have some way to determine if a virtual page is cached somewhere in DRAM.
If so, the system must determine which physical page it is cached in.
If there is a miss, the system must determine where the virtual page is stored on disk, select a victim page in physical memory, and copy the virtual page from disk to DRAM, replacing the victim page.
These capabilities are provided by a combination of operating system software, address translation hardware in theMMU (memorymanagement unit), and a data structure stored in physical memory known as a page table that maps virtual pages to physical pages.
The address translation hardware reads the page table each time it converts a virtual address to a physical address.
Figure 9.4 shows the basic organization of a page table.Apage table is an array of page table entries (PTEs)
Each page in the virtual address space has a PTE at a ﬁxed offset in the page table.
For our purposes, we will assume that each PTE consists of a valid bit and an n-bit address ﬁeld.
The valid bit indicates whether the virtual page is currently cached in DRAM.
If the valid bit is set, the address ﬁeld indicates the start of the corresponding physical page in DRAM where the virtual page is cached.
If the valid bit is not set, then a null address indicates that the virtual page has not yet been allocated.
Otherwise, the address points to the start of the virtual page on disk.
The example in Figure 9.4 shows a page table for a system with eight virtual pages and four physical pages.
An important point to notice about Figure 9.4 is that because the DRAM cache is fully associative, any physical page can contain any virtual page.
Practice Problem 9.2 Determine the number of page table entries (PTEs) that are needed for the following combinations of virtual address size (n) and page size (P ):
The reference to a word in VP 2 is a hit.
Since the valid bit is set, the address translation hardware knows that VP 2 is cached in memory.
So it uses the physical memory address in the PTE (which points to the start of the cached page in PP 1) to construct the physical address of the word.
In virtual memory parlance, a DRAM cache miss is known as a page fault.
Figure 9.6 shows the state of our example page table before the fault.
The CPU has referenced a word in VP 3, which is not cached in DRAM.
If VP 4 has been modiﬁed, then the kernel copies it back to disk.
When the handler returns, it restarts the faulting instruction, which resends the faulting virtual address to the address translation hardware.
But now, VP 3 is cached in main memory, and the page hit is handled normally by the address translation hardware.
Figure 9.7 shows the state of our example page table after the page fault.
Virtual memory was invented in the early 1960s, long before the widening CPU-memory gap spawned SRAM caches.
The reference to a word in VP 3 is a miss and triggers a page fault.
After the page fault handler restarts the faulting instruction, it will read the word from memory normally, without generating an exception.
The activity of transferring a page between disk and memory is known as swapping or paging.
Pages are swapped in (paged in) from disk to DRAM, and swapped out (paged out) fromDRAM to disk.
The strategy of waiting until the last moment to swap in a page, when a miss occurs, is known as demand paging.
Other approaches, such as trying to predict misses and swap pages in before they are actually referenced, are possible.
Figure 9.8 shows the effect on our example page table when the operating system allocates a new page of virtual memory, for example, as a result of calling malloc.
When many of us learn about the idea of virtual memory, our ﬁrst impression is often that it must be terribly inefﬁcient.
Given the large miss penalties, we worry that paging will destroy program performance.
In practice, virtual memory works well, mainly because of our old friend locality.
Although the total number of distinct pages that programs referenceduring an entire run might exceed the total size of physical memory, the principle of locality promises that at any point in time they will tend to work on a smaller set of active pages known as the working set or resident set.
After an initial overhead where the working set is paged into memory, subsequent references to the working set result in hits, with no additional disk trafﬁc.
As long as our programs have good temporal locality, virtual memory systems work quite well.
But of course, not all programs exhibit good temporal locality.
If the working set size exceeds the size of physical memory, then the program can produce an unfortunate situation known as thrashing, where pages are swapped in and out continuously.
Although virtual memory is usually efﬁcient, if a program’s performance slows to a crawl, the wise programmer will consider the possibility that it is thrashing.
You can monitor the number of page faults (and lots of other information) with the Unix getrusage function.
In the last section, we saw how virtualmemory provides amechanism for using the DRAM to cache pages from a typically larger virtual address space.
Interestingly, some early systems such as the DEC PDP-11/70 supported a virtual address space that was smaller than the available physical memory.
Yet virtual memory was still a useful mechanism because it greatly simpliﬁed memory management and provided a natural way to protect memory.
Thus far, we have assumed a single page table that maps a single virtual address space to the physical address space.
In fact, operating systems provide a separate page table, and thus a separate virtual address space, for each process.
Notice that multiple virtual pages can be mapped to the same shared physical page.
The combination of demand paging and separate virtual address spaces has a profound impact on the way that memory is used and managed in a system.
In particular, VM simpliﬁes linking and loading, the sharing of code and data, and allocating memory to applications.
Simplifying linking.A separate address space allows each process to use the same basic format for its memory image, regardless of where the code and data actually reside in physicalmemory.
For example, as we saw in Figure 8.13, every process on a given Linux system has a similar memory format.
The data and bss sections follow immediately after the text section.
The stack occupies the highest portion of the process address space and grows downward.
Such uniformity greatly simpliﬁes the design and implementation of linkers, allowing them to produce fully linked executables that are independent of the ultimate location of the code and data in physical memory.
Virtual memory also makes it easy to load executable and shared object ﬁles into memory.
Figure 9.9 How VM provides processes with separate address spaces.
The operating systemmaintains a separate page table for each process in the system.
The interesting point is that the loader never actually copies any data from disk into memory.
The data is paged in automatically and on demand by the virtual memory system the ﬁrst time each page is referenced, either by the CPU when it fetches an instruction, or by an executing instruction when it references a memory location.
This notion of mapping a set of contiguous virtual pages to an arbitrary location in an arbitrary ﬁle is known as memory mapping.
Unix provides a system call called mmap that allows application programs to do their own memory mapping.
We will describe application-level memory mapping in more detail in Section 9.8
Separate address spaces provide the operating system with a consistent mechanism for managing sharing between user processes and the operating system itself.
In general, each process has its own private code, data, heap, and stack areas that are not sharedwith any other process.
In this case, the operating system creates page tables thatmap the corresponding virtual pages to disjoint physical pages.
However, in some instances it is desirable for processes to share code and data.
For example, every process must call the same operating system kernel code, and every C program makes calls to routines in the standard C library such as printf.
Rather than including separate copies of the kernel and standard C library in each process, the operating system can arrange for multiple processes to share a single copy of this code by mapping the appropriate virtual pages in different processes to the same physical pages, as we saw in Figure 9.9
When a program running in a user process requests additional heap space (e.g., as a result of calling malloc), the operating system allocates an appropriate number, say, k, of contiguous virtualmemory pages, andmaps them to k arbitrary physical pages located anywhere in physical memory.
Because of the way page tables work, there is no need for the operating system to locate k contiguous pages of physical memory.
Any modern computer system must provide the means for the operating system to control access to the memory system.
A user process should not be allowed to modify its read-only text section.
Nor should it be allowed to read or modify any of the code and data structures in the kernel.
It should not be allowed to read or write the private memory of other processes, and it should not be allowed to.
As we have seen, providing separate virtual address spaces makes it easy to isolate the private memories of different processes.
But the address translation mechanism can be extended in a natural way to provide even ﬁner access control.
Since the address translation hardware reads a PTE each time the CPU generates an address, it is straightforward to control access to the contents of a virtual pageby adding some additional permission bits to the PTE.
The READ and WRITE bits control read and write access to the page.
If an instruction violates these permissions, then the CPU triggers a general protection fault that transfers control to an exception handler in the kernel.
Unix shells typically report this exception as a “segmentation fault.”
Our aim is to give you an appreciation of the hardware’s role in supporting virtual memory, with enough detail so that you can work through some concrete examples by hand.
However, keep inmind that we are omitting a number of details, especially related to timing, that are important to hardware designers but are beyond our scope.
Notice that since the physical and virtual pages are both P bytes, the physical page offset (PPO) is identical to the VPO.
Figure 9.13(a) shows the steps that the CPU hardware performs when there is a page hit.
Step 1: The processor generates a virtual address and sends it to the MMU.
Step 2: The MMU generates the PTE address and requests it from the cache/main memory.
Step 3: The cache/main memory returns the PTE to the MMU.
Step 3: The MMU constructs the physical address and sends it to cache/main memory.
Step 4: The cache/main memory returns the requested data word to the processor.
Unlike a page hit, which is handled entirely by hardware, handling a page fault requires cooperation between hardware and the operating system kernel (Figure 9.13(b))
Step 4: The valid bit in the PTE is zero, so the MMU triggers an exception, which transfers control in the CPU to a page fault exception handler in the operating system kernel.
Step 5: The fault handler identiﬁes a victim page in physical memory, and if that page has been modiﬁed, pages it out to disk.
Step 6: The fault handler pages in the new page and updates the PTE in memory.
Step 7: The fault handler returns to the original process, causing the faulting instruction to be restarted.
The CPU resends the offending virtual address to the MMU.
Because the virtual page is now cached in physical memory, there is a hit, and after the MMU performs the steps in Figure 9.13(b), the main memory returns the requested word to the processor.
In any system that uses both virtual memory and SRAM caches, there is the issue of whether to use virtual or physical addresses to access the SRAM cache.
Although a detailed discussion of the trade-offs is beyond our scope here, most systems opt for physical addressing.With physical addressing, it is straightforward for multiple processes to have blocks in the cache at the same time and to share blocks from the same virtual pages.
Further, the cache does not have to deal with protection issues because access rights are checked as part of the address translation process.
Figure 9.14 shows how a physically addressed cache might be integrated with virtual memory.
The main idea is that the address translation occurs before the cache lookup.
Notice that page table entries can be cached, just like any other data words.
As we have seen, every time the CPU generates a virtual address, the MMUmust refer to a PTE in order to translate the virtual address into a physical address.
In the worst case, this requires an additional fetch from memory, at a cost of tens to hundreds of cycles.
If the PTE happens to be cached in L1, then the cost goes down to one or two cycles.
However, many systems try to eliminate even this cost by including a small cache of PTEs in the MMU called a translation lookaside buffer (TLB)
A TLB is a small, virtually addressed cache where each line holds a block consisting of a single PTE.
As shown in Figure 9.15, the index and tag ﬁelds that are used for set selection and line matching are extracted from the virtual page number in the virtual address.
If the TLB has T = 2t sets, then the TLB index (TLBI) consists of the t least signiﬁcant bits of the VPN, and the TLB tag (TLBT) consists of the remaining bits in the VPN.
Figure 9.15 Components of a virtual address that are used to access the TLB.
Figure 9.16(a) shows the steps involved when there is a TLB hit (the usual case)
The key point here is that all of the address translation steps are performed inside the on-chip MMU, and thus are fast.
Step 4:TheMMUtranslates the virtual address to a physical address and sends it to the cache/main memory.
Step 5: The cache/main memory returns the requested data word to the CPU.
The newly fetched PTE is stored in the TLB, possibly overwriting an existing entry.
To this pointwehave assumed that the systemuses a single page table todoaddress translation.
The problem is compounded for systems with 64-bit address spaces.
The common approach for compacting the page table is to use a hierarchy of page tables instead.
The idea is easiest to understand with a concrete example.
Figure 9.17 shows how we might construct a two-level page table hierarchy for this virtual address space.
If every page in chunk i is unallocated, then level 1 PTE i is null.
Figure 9.16 Operational view of a TLB hit and miss.
This represents a signiﬁcant potential savings, since most of the 4 GB virtual address space for a typical program is unallocated.
Second, only the level 1 table needs to be inmainmemory at all times.
Only the most heavily used level 2 page tables need to be cached in main memory.
Accessing k PTEs may seem expensive and impractical at ﬁrst glance.
However, the TLB comes to the rescue here by caching PTEs from the page tables at the different levels.
In practice, address translation with multi-level page tables is not signiﬁcantly slower than with single-level page tables.
In this section, we put it all together with a concrete example of end-to-end address translation on a small system with a TLB and L1 d-cache.
The TLB is four-way set associative with 16 total entries.
Figure 9.19 shows the formats of the virtual and physical addresses.
The high-order 8 bits of the virtual address serve as the VPN.
The high-order 6 bits of the physical address serve as the PPN.
Above the ﬁgures of the TLB and cache, we have also shown how the bits of the virtual and physical addresses are partitioned by the hardware as it accesses these devices.
Figure 9.20 TLB, page table, and cache for small memory system.
All values in the TLB, page table, and cache are in hexadecimal notation.
TLB: The TLB is virtually addressed using the bits of the VPN.
Since the TLB has four sets, the 2 low-order bits of the VPN serve as the set index (TLBI)
The remaining 6 high-order bits serve as the tag (TLBT) that distinguishes the different VPNs that might map to the same TLB set.
However, we are only interested in the ﬁrst sixteen of these.
For convenience, we have labeled each PTE with the VPN that indexes it; but keep in mind that these VPNs are not part of the page table and not stored in memory.
Also, notice that the PPN of each invalid PTE is denoted with a dash to reinforce the idea that whatever bit values might happen to be stored there are not meaningful.
The direct-mapped cache is addressed by the ﬁelds in the physical address.
Given this initial setup, let’s see what happens when the CPU executes a load instruction that reads the byte at address 0x03d4
Recall that our hypothetical CPU reads one-byte words rather than four-byte words.
To begin this kind of manual simulation, we ﬁnd it helpful to write down the bits in the virtual address, identify the various ﬁelds we will need, and determine their hex values.
The hardware performs a similar task when it decodes the address.
If theTLBhadmissed, then theMMUwould need to fetch thePTE frommain memory.However, in this casewe got lucky and had aTLBhit.
For example, if the TLBmisses, then theMMUmust fetch the PPN from a PTE in the page table.
If the resulting PTE is invalid, then there is a page fault and the kernel must page in the appropriate page and rerun the load instruction.
Another possibility is that the PTE is valid, but the necessary memory block misses in the cache.
For the given virtual address, indicate the TLB entry accessed, physical address, and cache byte value returned.
Indicatewhether theTLBmisses, whether a page fault occurs, andwhether a cache miss occurs.
If there is a cache miss, enter “–” for “Cache byte returned.” If there is a page fault, enter “–” for “PPN” and leave parts C and D blank.
We conclude our discussion of virtual memory mechanisms with a case study of a real system: an Intel Core i7 running Linux.
The processor package includes four cores, a large L3 cache shared by all of the cores, and a.
Each core contains a hierarchy of TLBs, a hierarchy of data and instruction caches, and a set of fast point-to-point links, based on the Intel QuickPath technology, for communicating directly with the other cores and the external I/O bridge.
When a Linux process is running, the page tables associated with allocated pages are all memory-resident, although the Core i7 architecture allows these page tables to be swapped in and out.
The value of CR3 is part of each process context, and is restored during each context switch.
For simplicity, the i-caches, i-TLB, and L2 uniﬁed TLB are not shown.
R/W Read-only or read-write access permission for all reachable pages.
U/S User or supervisor (kernel) mode access permission for all reachable pages.
A Reference bit (set by MMU on reads and writes, cleared by software)
Base addr 40 most signiﬁcant bits of physical base address of child page table.
Notice that this imposes a 4 KB alignment requirement on page tables.
Again, this imposes a 4 KB alignment requirement on physical pages.
The U/S bit, which determines whether the page can be accessed in user mode, protects code and data in the operating system kernel from user programs.
The XD (execute disable) bit, which was introduced in 64-bit systems, can be used to disable instruction fetches from individual memory pages.
This is an important new feature that allows the operating system kernel to reduce the risk of buffer overﬂow attacks by restricting execution to the read-only text segment.
As theMMUtranslates each virtual address, it also updates two other bits that can be used by the kernel’s page fault handler.
The MMU sets the A bit, which is known as a reference bit, each time a page is accessed.
The kernel can use the reference bit to implement its page replacement algorithm.
The MMU sets the D bit, or dirty bit, each time the page is written to.
A page that has been modiﬁed is sometimes called a dirty page.
The dirty bit tells the kernel whether or not it must write-back a victim page before it copies in a replacement page.
The kernel can call a special kernel-mode instruction to clear the reference or dirty bits.
U/S User or supervisor mode (kernel mode) access permission for child page.
A Reference bit (set by MMU on reads and writes, cleared by software)
Base addr 40 most signiﬁcant bits of physical base address of child page.
However, real hardware implementations use a neat trick that allows these steps to be partially overlapped, thus speeding up accesses to the L1 cache.
While the MMU is requesting a page table entry from the TLB, the L1 cache is busy using the VPO bits to ﬁnd the appropriate set and read out the eight tags and corresponding data words in that set.
When the MMU gets the PPN back from the TLB, the cache is ready to try to match the PPN to one of these eight tags.
The Linux names for the four levels of page tables are also shown.
A virtual memory system requires close cooperation between the hardware and the kernel.
Details vary from version to version, and a complete description is beyond our scope.
Nonetheless, our aim in this section is to describe enough of theLinux virtualmemory system to give you a sense of howa real operating system organizes virtual memory and how it handles page faults.
Linux maintains a separate virtual address space for each process of the form shown in Figure 9.26
We have seen this picture a number of times already, with its familiar code, data, heap, shared library, and stack segments.
Now that we understand address translation, we can ﬁll in some more details about the kernel virtual memory that lies above the user stack.
The kernel virtualmemory contains the code and data structures in the kernel.
Some regions of the kernel virtual memory are mapped to physical pages that are shared by all processes.
For example, each process shares the kernel’s code and global data structures.
Interestingly, Linux also maps a set of contiguous virtual pages (equal in size to the total amount of DRAM in the system) to the corresponding set of contiguous physical pages.
Run-time heap (via malloc) Uninitialized data (.bss) Initialized data (.data) Program text (.text)
Other regions of kernel virtual memory contain data that differs for each process.
Examples include page tables, the stack that the kernel uses when it is executing code in the context of the process, and various data structures that keep track of the current organization of the virtual address space.
Linux organizes the virtual memory as a collection of areas (also called segments)
An area is a contiguous chunk of existing (allocated) virtual memory whose pages are related in some way.
For example, the code segment, data segment, heap, shared library segment, and user stack are all distinct areas.
Each existing virtual page is contained in some area, and any virtual page that is not part of some area does not exist and cannot be referenced by the process.
The notion of an area is important because it allows the virtual address space to have gaps.
The kernel does not keep track of virtual pages that do not exist, and such pages do not consume any additional resources in memory, on disk, or in the kernel itself.
Figure 9.27 highlights the kernel data structures that keep track of the virtual memory areas in a process.
The kernel maintains a distinct task structure (task_ struct in the source code) for each process in the system.
One of the entries in the task structure points to an mm_struct that characterizes the current state of the virtual memory.
When the kernel runs this process, it stores pgd in the CR3 control register.
For our purposes, the area struct for a particular area contains the following ﬁelds:
Suppose the MMU triggers a page fault while trying to translate some virtual address A.
The exception results in a transfer of control to the kernel’s page fault handler, which then performs the following steps:
If the instruction is not legal, then the fault handler triggers a segmentation fault, which terminates the process.
Because a process can create an arbitrary number of new virtual memory areas (using the mmap function described in the next section), a sequential search of the list of area structs might be very costly.
So in practice, Linux superimposes a tree on the list, using some ﬁelds that we have not shown, and performs the search on this tree.
Is the attempted memory access legal? In other words, does the process have permission to read, write, or execute the pages in this area? For example, was the page fault the result of a store instruction trying to write to a read-only page in the text segment? Is the page fault the result of a process running in user mode that is attempting to read a word from kernel virtual memory? If the attempted access is not legal, then the fault handler triggers a protection exception, which terminates the process.
Protection exception: e.g., violating permission by writing to a read-only page.
At this point, the kernel knows that the page fault resulted from a legal operation on a legal virtual address.
It handles the fault by selecting a victim page, swapping out the victim page if it is dirty, swapping in the new page, and updating the page table.
When the page fault handler returns, the CPU restarts the faulting instruction, which sends A to the MMU again.
This time, the MMU translates A normally, without generating a page fault.
Linux (alongwith other forms ofUnix) initializes the contents of a virtualmemory area by associating it with an object on disk, a process known asmemorymapping.
Areas can be mapped to one of two types of objects:
Regular ﬁle in the Unix ﬁle system: An area can be mapped to a contiguous section of a regular disk ﬁle, such as an executable object ﬁle.
The ﬁle section is divided into page-sized pieces, with each piece containing the initial contents of a virtual page.
Because of demand paging, none of these virtual pages is actually swapped into physical memory until the CPU ﬁrst touches the page (i.e., issues a virtual address that falls within that page’s region of the address space)
If the area is larger than the ﬁle section, then the area is padded with zeros.
Anonymous ﬁle: An area can also be mapped to an anonymous ﬁle, created by the kernel, that contains all binary zeros.
The ﬁrst time the CPU touches a virtual page in such an area, the kernel ﬁnds an appropriate victim page in physical memory, swaps out the victim page if it is dirty, overwrites the victim page with binary zeros, and updates the page table to mark the page as resident.
Notice that no data is actually transferred between disk and memory.
For this reason, pages in areas that are mapped to anonymous ﬁles are sometimes called demand-zero pages.
In either case, once a virtual page is initialized, it is swapped back and forth between a special swap ﬁle maintained by the kernel.
The swap ﬁle is also known as the swap space or the swap area.
An important point to realize is that at any point in time, the swap space bounds the total amount of virtual pages that can be allocated by the currently running processes.
The idea of memory mapping resulted from a clever insight that if the virtual memory system could be integrated into the conventional ﬁle system, then it could provide a simple and efﬁcient way to load programs and data into memory.
As we have seen, the process abstraction promises to provide each process with its own private virtual address space that is protected from errant writes or reads by other processes.
For example, each process that runs the Unix shell program tcsh has the same text area.
For example, every C program requires functions from the standard C library such as printf.
It would be extremely wasteful for each process to keep duplicate copies of these commonly used codes in physical memory.
Fortunately, memory mapping provides us with a clean mechanism for controlling how objects are shared by multiple processes.
An object can be mapped into an area of virtual memory as either a shared object or aprivate object.
If a processmaps a shared object into an area of its virtual address space, then any writes that the process makes to that area are visible to any other processes that have also mapped the shared object into their virtual memory.
Further, the changes are also reﬂected in the original object on disk.
Changes made to an area mapped to a private object, on the other hand, are not visible to other processes, and any writes that the process makes to the area are not reﬂected back to the object on disk.
A virtual memory area into which a shared object is mapped is often called a shared area.
The key point is that only a single copy of the shared object needs to be stored in physical memory, even though the object is mapped into multiple shared areas.
For convenience, we have shown the physical pages as being contiguous, but of course this is not true in general.
Private objects are mapped into virtual memory using a clever technique known as copy-on-write.
A private object begins life in exactly the same way as a.
For example, Figure 9.30(a) shows a case where two processes have mapped a private object into different areas of their virtual memories but share the same physical copy of the object.
For each process thatmaps the private object, the page table entries for the corresponding private area are ﬂagged as read-only, and the area struct is ﬂagged as private copy-on-write.
So long as neither process attempts to write to its respective private area, they continue to share a single copy of the object in physical memory.
However, as soon as a process attempts to write to some page in the private area, the write triggers a protection fault.
When the fault handler notices that the protection exception was caused by the process trying to write to a page in a private copy-on-write area, it creates a new copy of the page in physical memory, updates the page table entry to point to the new copy, and then restores write permissions to the page, as shown in Figure 9.30(b)
When the fault handler returns, the CPU reexecutes the write, which now proceeds normally on the newly created page.
By deferring the copying of the pages in private objects until the last possible moment, copy-on-write makes the most efﬁcient use of scarce physical memory.
Now that we understand virtual memory andmemorymapping, we can get a clear idea of how the fork function creates a new process with its own independent virtual address space.
When the fork function is called by the current process, the kernel creates various data structures for the new process and assigns it a unique PID.
To create the virtual memory for the new process, it creates exact copies of the current.
It ﬂags each page in both processes as read-only, and ﬂags each area struct in both processes as private copyon-write.
When the fork returns in the new process, the new process now has an exact copy of the virtual memory as it existed when the fork was called.
When either of the processes performs any subsequent writes, the copy-on-write mechanism creates new pages, thus preserving the abstraction of a private address space for each process.
Virtualmemory andmemorymapping also play key roles in the process of loading programs into memory.
Now that we understand these concepts, we can understand how the execve function really loads and executes programs.
Suppose that the program running in the current process makes the following call:
As you learned in Chapter 8, the execve function loads and runs the program contained in the executable object ﬁle a.outwithin the current process, effectively replacing the current program with the a.out program.
Delete existing user areas.Delete the existing area structs in the user portion of the current process’s virtual address.
Create new area structs for the text, data, bss, and stack areas of the new program.
The text and data areas are mapped to the text and data sections of the a.out ﬁle.
The bss area is demand-zero, mapped to an anonymous ﬁle whose size is contained in a.out.
The stack and heap area are also demand-zero, initially of zero-length.
Figure 9.31 summarizes the different mappings of the private areas.
If the a.out program was linked with shared objects, such as the standard C library libc.so, then these objects are dynamically linked into the program, and thenmapped into the shared region of the user’s virtual address space.
The last thing that execve does is to set the program counter in the current process’s context to point to the entry point in the text area.
The next time this process is scheduled, it will begin execution from the entry point.
Linux will swap in code and data pages as needed.
Unix processes can use the mmap function to create new areas of virtual memory and to map objects into these areas.
Figure 9.31 How the loader maps the areas of the user address space.
The mmap function asks the kernel to create a new virtual memory area, preferably one that starts at address start, and to map a contiguous chunk of the object speciﬁed by ﬁle descriptor fd to the new area.
The contiguous object chunk has a size of length bytes and starts at an offset of offset bytes from the beginning of the ﬁle.
The start address is merely a hint, and is usually speciﬁed as NULL.
The prot argument contains bits that describe the access permissions of the newly mapped virtual memory area (i.e., the vm_prot bits in the corresponding area struct)
PROT_EXEC: Pages in the area consist of instructions that may be executed by the CPU.
The flags argument consists of bits that describe the type of the mapped object.
If the MAP_ANON ﬂag bit is set, then the backing store is an anonymous object and the corresponding virtual pages are demand-zero.
If the call is successful, then bufp contains the address of the new area.
The munmap function deletes the area starting at virtual address start and consisting of the next length bytes.
Subsequent references to the deleted region result in segmentation faults.
The name of the input ﬁle should be passed as a command line argument.
While it is certainly possible to use the low-level mmap and munmap functions to create and delete areas of virtual memory, C programmers typically ﬁnd it more.
A dynamic memory allocator maintains an area of a process’s virtual memory known as the heap (Figure 9.33)
Details vary from system to system, but without loss of generality, we will assume that the heap is an area of demand-zero memory that begins immediately after the uninitialized bss area and grows upward (toward higher addresses)
For each process, the kernel maintains a variable brk (pronounced “break”) that points to the top of the heap.
An allocator maintains the heap as a collection of various-sized blocks.
Each block is a contiguous chunk of virtual memory that is either allocated or free.
An allocatedblockhas been explicitly reserved for use by the application.A free block is available to be allocated.
A free block remains free until it is explicitly allocated by the application.
An allocated block remains allocated until it is freed, either explicitly by the application, or implicitly by the memory allocator itself.
They differ about which entity is responsible for freeing allocated blocks.
Explicit allocators require the application to explicitly free any allocated blocks.
For example, the C standard library provides an explicit allocator called the malloc package.
Implicit allocators, on the other hand, require the allocator to detect when an allocated block is no longer being used by the program and then free the block.
Implicit allocators are also known as garbage collectors, and the.
For example, higher-level languages such as Lisp,ML, and Java rely on garbage collection to free allocated blocks.
For concreteness, our discussion focuses on allocators that manage heap memory.
However, you should be aware that memory allocation is a general idea that arises in a variety of contexts.
Programs allocate blocks from the heap by calling the malloc function.
Returns: ptr to allocated block if OK, NULL on error.
The malloc function returns a pointer to a block of memory of at least size bytes that is suitably aligned for any kind of data object that might be contained in the block.
On the Unix systems that we are familiar with, malloc returns a block that is aligned to an 8-byte (double word) boundary.
If malloc encounters a problem (e.g., the program requests a block ofmemory that is larger than the available virtual memory), then it returns NULL and sets errno.
Applications that want initialized dynamic memory can use calloc, a thin wrapper around the malloc function that initializes the allocated memory to zero.
Applications that want to change the size of a previously allocated block can use the realloc function.
Dynamic memory allocators such as malloc can allocate or deallocate heap memory explicitly by using the mmap and munmap functions, or they can use the sbrk function:
Programs free allocated heap blocks by calling the free function.
The ptr argument must point to the beginning of an allocated block that was obtained from malloc, calloc, or realloc.
Even worse, since it returns nothing, free gives no indication to the application that something is wrong.
As we shall see in Section 9.11, this can produce some bafﬂing run-time errors.
The heavy-lined rectangles correspond to allocated blocks (shaded) and free blocks (unshaded)
Initially, the heap consists of a single 16-word doubleword aligned free block.
Malloc responds by carving out a four-word block from the front of the free block and returning a pointer to the ﬁrst word of the block.
Malloc responds by allocating a six-word block from the front of the free block.
In this example, malloc pads the block with an extra word in order to keep the free block aligned on a double-word boundary.
Figure 9.34(c): The program requests a six-word block and malloc responds by carving out a six-word block from the free block.
Notice that after the call to free returns, the pointer p2 still points to the freed block.
It is the responsibility of the application not to use p2 again until it is reinitialized by a new call to malloc.
In this case, malloc allocates a portion of the block that was freed in the previous step and returns a pointer to this new block.
The most important reason that programs use dynamic memory allocation is that often they do not know the sizes of certain data structures until the program actually runs.
For example, suppose we are asked to write a C program that reads a list of n ASCII integers, one integer per line, from stdin into a C array.
The input consists of the integer n, followed by the n integers to be read and stored into the array.
The simplest approach is to deﬁne the array statically with some hard-coded maximum array size:
Allocating arrays with hard-coded sizes like this is often a bad idea.
The value ofMAXN is arbitrary and has no relation to the actual amount of available virtual memory on the machine.
Further, if the user of this program wanted to read a ﬁle that was larger thanMAXN, the only recourse would be to recompile the program with a larger value of MAXN.
While not a problem for this simple example, the presence of hard-coded array bounds can become a maintenance nightmare for large software products with millions of lines of code and numerous users.
A better approach is to allocate the array dynamically, at run time, after the value of n becomes known.
With this approach, the maximum size of the array is limited only by the amount of available virtual memory.
Dynamic memory allocation is a useful and important programming technique.
However, in order to use allocators correctly and efﬁciently, programmers need to have an understanding of how theywork.Wewill discuss some of the gruesome errors that can result from the improper use of allocators in Section 9.11
Handling arbitrary request sequences.An application can make an arbitrary sequence of allocate and free requests, subject to the constraint that each.
Thus, the allocator cannot make any assumptions about the ordering of allocate and free requests.
For example, the allocator cannot assume that all allocate requests are accompanied by a matching free request, or that matching allocate and free requests are nested.
Thus, the allocator is not allowed to reorder or buffer requests in order to improve performance.
In order for the allocator to be scalable, any non-scalar data structures used by the allocator must be stored in the heap itself.
The allocator must align blocks in such a way that they can hold any type of data object.
On most systems, this means that the block returned by the allocator is aligned on an 8-byte (doubleword) boundary.
In particular, they are not allowed tomodify or move blocks once they are allocated.
Thus, techniques such as compaction of allocated blocks are not permitted.
Working within these constraints, the author of an allocator attempts to meet the often conﬂicting performance goals of maximizing throughput and memory utilization.
Goal 1: Maximizing throughput.Given some sequence of n allocate and free requests.
In general, we can maximize throughput by minimizing the average time to satisfy allocate and free requests.
As we’ll see, it is not too difﬁcult to develop allocators with reasonably good performance where the worst-case running time of an allocate request is linear in the number of free blocks and the running time of a free request is constant.
In fact, the total amount of virtual memory allocated by all of the processes in a system is limited by the amount of swap space on disk.
Good programmers know that virtual memory is a ﬁnite resource that must be used efﬁciently.
This is especially true for a dynamic memory allocator that might be asked to allocate and free large blocks of memory.
There are a number of ways to characterize how efﬁciently an allocator uses the heap.
In our experience, the most useful metric is peak utilization.
If an application requests a block of p bytes, then the resulting allocated block has a payload of p bytes.
After request Rk has completed, let the aggregate payload, denoted Pk, be the sum of the payloads of the currently allocated blocks, and let Hk denote the current (monotonically nondecreasing) size of the heap.
Then the peak utilization over the ﬁrst k requests, denoted by Uk, is given by.
We could relax the monotonically nondecreasing assumption in our deﬁnition ofUk and allow the heap to grow up and down by letting Hk be the highwater mark over the ﬁrst k requests.
The primary cause of poor heap utilization is a phenomenon known as fragmentation, which occurs when otherwise unused memory is not available to satisfy allocate requests.
There are two forms of fragmentation: internal fragmentation and external fragmentation.
For example, the implementation of an allocator might impose a minimum size on allocated blocks that is greater than some requested payload.
Or, as we saw in Figure 9.34(b), the allocator might increase the block size in order to satisfy alignment constraints.
It is simply the sum of the differences between the sizes of the allocated blocks and their payloads.
Thus, at any point in time, the amount of internal fragmentation depends only on the pattern of previous requests and the allocator implementation.
External fragmentation occurs when there is enough aggregate free memory to satisfy an allocate request, but no single free block is large enough to handle the request.
For example, if the request in Figure 9.34(e)were for sixwords rather than two words, then the request could not be satisﬁed without requesting additional virtual memory from the kernel, even though there are six free words remaining.
The problem arises because these six words are spread over two free blocks.
External fragmentation is much more difﬁcult to quantify than internal fragmentation because it depends not only on the pattern of previous requests and the allocator implementation, but also on the pattern of future requests.
For example, suppose that after k requests all of the free blocks are exactly four words in size.
Does this heap suffer from external fragmentation? The answer depends on the pattern of future requests.
If all of the future allocate requests are for blocks that are smaller than or equal to four words, then there is no external fragmentation.
On the other hand, if one or more requests ask for blocks larger than four words, then the heap does suffer from external fragmentation.
Since external fragmentation is difﬁcult to quantify and impossible to predict, allocators typically employ heuristics that attempt to maintain small numbers of larger free blocks rather than large numbers of smaller free blocks.
The simplest imaginable allocator would organize the heap as a large array of bytes and a pointer p that initially points to the ﬁrst byte of the array.
To allocate size bytes, malloc would save the current value of p on the stack, increment p by size, and return the old value of p to the caller.
Free would simply return to the caller without doing anything.
This naive allocator is an extreme point in the design space.
Since each malloc and free execute only a handful of instructions, throughput would be extremely good.
Free block organization: How do we keep track of free blocks?
Placement: How do we choose an appropriate free block in which to place a newly allocated block?
Splitting: After we place a newly allocated block in some free block, what do we do with the remainder of the free block?
Coalescing: What do we do with a block that has just been freed?
The rest of this section looks at these issues in more detail.
Since the basic techniques of placement, splitting, and coalescing cut across many different free block organizations, we will introduce them in the context of a simple free block organization known as an implicit free list.
Any practical allocator needs some data structure that allows it to distinguish block boundaries and to distinguish between allocated and free blocks.
The block size includes the header, payload, and any padding.
In this case, a block consists of a one-word header, the payload, and possibly some additional padding.
The header encodes the block size (including the header and any padding) as well as whether the block is allocated or free.
If we impose a double-word alignment constraint, then the block size is always a multiple of eight and the 3 low-order bits of the block size are always zero.
In this case, we are using the least signiﬁcant of these bits (the allocated bit) to indicate whether the block is allocated or free.
The header is followed by the payload that the application requested when it called malloc.
The payload is followed by a chunk of unused padding that can be any size.
For example, the padding might be part of an allocator’s strategy for combating external fragmentation.
Or it might be needed to satisfy the alignment requirement.
Figure 9.36 Organizing the heap with an implicit free list.
We call this organization an implicit free list because the free blocks are linked implicitly by the size ﬁelds in the headers.
The allocator can indirectly traverse the entire set of free blocks by traversing all of the blocks in the heap.
Notice that we need some kind of specially marked end block, in this example a terminating headerwith the allocated bit set and a size of zero.
Aswewill see in Section 9.9.12, setting the allocated bit simpliﬁes the coalescing of free blocks.
A signiﬁcant disadvantage is that the cost of any operation, such as placing allocated blocks, that requires a search of the free list will be linear in the total number of allocated and free blocks in the heap.
It is important to realize that the system’s alignment requirement and the allocator’s choice of block format impose a minimum block size on the allocator.
No allocated or free block may be smaller than this minimum.
For example, if we assume a double-word alignment requirement, then the size of each block must be a multiple of two words (8 bytes)
Thus, the block format in Figure 9.35 induces a minimum block size of two words: one word for the header, and another to maintain the alignment requirement.
Even if the application were to request a single byte, the allocator would still create a two-word block.
Practice Problem 9.6 Determine the block sizes and header values that would result from the following sequence of malloc requests.
Block sizes are rounded up to the nearest multiple of 8 bytes.
When an application requests a block of k bytes, the allocator searches the free list for a free block that is large enough to hold the requested block.
The manner in which the allocator performs this search is determined by the placement policy.
Some common policies are ﬁrst ﬁt, next ﬁt, and best ﬁt.
First ﬁt searches the free list from the beginning and chooses the ﬁrst free block that ﬁts.
Next ﬁt is similar to ﬁrst ﬁt, but instead of starting each search at the beginning of the list, it starts each search where the previous search left off.
Best ﬁt examines every free block and chooses the free block with the smallest size that ﬁts.
An advantage of ﬁrst ﬁt is that it tends to retain large free blocks at the end of the list.
A disadvantage is that it tends to leave “splinters” of small free blocks.
Figure 9.37 Splitting a free block to satisfy a three-word allocation request.
Next ﬁt was ﬁrst proposed by Donald Knuth as an alternative to ﬁrst ﬁt, motivated by the idea that if we found a ﬁt in some free block the last time, there is a good chance that we will ﬁnd a ﬁt the next time in the remainder of the block.
Next ﬁt can run signiﬁcantly faster than ﬁrst ﬁt, especially if the front of the list becomes littered with many small splinters.
However, some studies suggest that next ﬁt suffers from worse memory utilization than ﬁrst ﬁt.
Studies have found that best ﬁt generally enjoys better memory utilization than either ﬁrst ﬁt or next ﬁt.
However, the disadvantage of using best ﬁt with simple free list organizations such as the implicit free list, is that it requires an exhaustive search of the heap.
Later, we will look at more sophisticated segregated free list organizations that approximate a best-ﬁt policy without an exhaustive search of the heap.
Once the allocator has located a free block that ﬁts, it must make another policy decision about how much of the free block to allocate.
Although simple and fast, the main disadvantage is that it introduces internal fragmentation.
If the placement policy tends to produce good ﬁts, then some additional internal fragmentation might be acceptable.
However, if the ﬁt is not good, then the allocator will usually opt to split the free block into two parts.
The ﬁrst part becomes the allocated block, and the remainder becomes a new free block.
What happens if the allocator is unable to ﬁnd a ﬁt for the requested block? One option is to try to create some larger free blocks by merging (coalescing) free blocks that are physically adjacent in memory (next section)
However, if this does not yield a sufﬁciently large block, or if the free blocks are alreadymaximally coalesced, then the allocator asks the kernel for additional heapmemory by calling the sbrk function.
The allocator transforms the additional memory into one large free block, inserts the block into the free list, and then places the requested block in this new free block.
When the allocator frees an allocated block, there might be other free blocks that are adjacent to the newly freed block.
Such adjacent free blocks can cause a phenomenon known as false fragmentation, where there is a lot of available free memory chopped up into small, unusable free blocks.
The result is two adjacent free blocks with payloads of three words each.
As a result, a subsequent request for a payload of four words would fail, even though the aggregate size of the two free blocks is large enough to satisfy the request.
To combat false fragmentation, any practical allocator must merge adjacent free blocks in a process known as coalescing.
This raises an important policy decision about when to perform coalescing.
The allocator can opt for immediate coalescing bymerging any adjacent blocks each time a block is freed.
Or it can opt for deferred coalescing by waiting to coalesce free blocks at some later time.
For example, the allocator might defer coalescing until some allocation request fails, and then scan the entire heap, coalescing all free blocks.
Immediate coalescing is straightforward and can be performed in constant time, but with some request patterns it can introduce a form of thrashing where a block is repeatedly coalesced and then split soon thereafter.
For example, in Figure 9.38 a repeated pattern of allocating and freeing a three-word block would introduce a lot of unnecessary splitting and coalescing.
In our discussion of allocators, we will assume immediate coalescing, but you should be aware that fast allocators often opt for some form of deferred coalescing.
How does an allocator implement coalescing? Let us refer to the block we want to free as the current block.
Then coalescing the next free block (in memory) is straightforward and efﬁcient.
The header of the current block points to the header of the next block, which can be checked to determine if the next block is free.
If so, its size is simply added to the size of the current header and the blocks are coalesced in constant time.
But how would we coalesce the previous block? Given an implicit free list of blocks with headers, the only option would be to search the entire list, remembering the location of the previous block, until we reached the current block.With an.
Figure 9.39 Format of heap block that uses a boundary tag.
Even with more sophisticated free list organizations, the search time would not be constant.
Knuth developed a clever and general technique, known as boundary tags, that allows for constant-time coalescing of the previous block.
The idea, which is shown in Figure 9.39, is to add a footer (the boundary tag) at the end of each block, where the footer is a replica of the header.
If each block includes such a footer, then the allocator can determine the starting location and status of the previous block by inspecting its footer, which is always one word away from the start of the current block.
Consider all the cases that can exist when the allocator frees the current block:
The previous block is allocated and the next block is free.
The previous block is free and the next block is allocated.
Figure 9.40 shows howwewould coalesce each of the four cases.
In case 1, both adjacent blocks are allocated and thus no coalescing is possible.
So the status of the current block is simply changed from allocated to free.
In case 2, the current block is merged with the next block.
The header of the current block and the footer of the next block are updated with the combined sizes of the current and next blocks.
In case 3, the previous block is merged with the current block.
The header of the previous block and the footer of the current block are updated with the combined sizes of the two blocks.
In case 4, all three blocks are merged to form a single free block, with the header of the previous block and the footer of the next block updated with the combined sizes of the three blocks.
In each case, the coalescing is performed in constant time.
The idea of boundary tags is a simple and elegant one that generalizes to many different types of allocators and free list organizations.
Requiring each block to contain both a header and a footer can introduce signiﬁcant memory overhead if an application manipulates.
For example, if a graph application dynamically creates and destroys graphnodes bymaking repeated calls tomalloc andfree, and each graph node requires only a couple of words of memory, then the header and the footer will consume half of each allocated block.
Fortunately, there is a clever optimization of boundary tags that eliminates the need for a footer in allocated blocks.
Recall that when we attempt to coalesce the current block with the previous and next blocks in memory, the size ﬁeld in the footer of the previous block is only needed if the previous block is free.
If we were to store the allocated/free bit of the previous block in one of the excess loworder bits of the current block, then allocated blocks would not need footers, and we could use that extra space for payload.
Practice Problem 9.7 Determine the minimum block size for each of the following combinations of alignment requirements and block formats.
Assumptions: Implicit free list, zerosized payloads are not allowed, and headers and footers are stored in 4-bytewords.
Single word Header and footer Header and footer Single word Header, but no footer Header and footer Double word Header and footer Header and footer Double word Header, but no footer Header and footer.
The design space is large, with numerous alternatives for block format and free list format, as well as placement, splitting, and coalescing policies.
Another challenge is that you are often forced to program outside the safe, familiar conﬁnes of the type system, relying on the error-prone pointer casting and pointer arithmetic that is typical of low-level systems programming.
While allocators do not require enormous amounts of code, they are subtle and unforgiving.
Students familiar with higher-level languages such as C++ or Java often hit a conceptual wall when they ﬁrst encounter this style of programming.
To help you clear this hurdle, we will work through the implementation of a simple allocator based on an implicit free list with immediate boundary-tag coalescing.
Our allocator uses a model of the memory system provided by the memlib.c package shown in Figure 9.41
The purpose of the model is to allow us to run our allocator without interfering with the existing system-level malloc package.
The mem_init function models the virtual memory available to the heap as a large, double-word aligned array of bytes.
The allocator requests additional heap memory by calling the mem_sbrk function, which has the same interface as the system’s sbrk function, as well as the same semantics, except that it rejects requests to shrink the heap.
The allocator itself is contained in a source ﬁle (mm.c) that users can compile and link into their applications.
The free list is organized as an implicit free list, with the invariant form shown in Figure 9.42
The ﬁrst word is an unused padding word aligned to a double-word boundary.
The padding is followed by a special prologue block, which is an 8-byte allocated block consisting of only a header and a footer.
The prologue block is created during initialization and is never freed.
Following the prologue block are zero or more regular blocks that are created by calls to malloc or free.
The prologue and epilogue blocks are tricks that eliminate the edge conditions during coalescing.
The allocator uses a single private (static) global variable (heap_listp) that always points to the prologue block.
As a minor optimization, we could make it point to the next block instead of the prologue block.
Figure 9.43 shows some basic constants and macros that we will use throughout the allocator code.
Lines 2–4 deﬁne some basic size constants: the sizes of words (WSIZE) and double words (DSIZE), and the size of the initial free block and the default size for expanding the heap (CHUNKSIZE)
Manipulating the headers and footers in the free list can be troublesome because it demands extensive use of casting and pointer arithmetic.
Thus, we ﬁnd it helpful to deﬁne a small set of macros for accessing and traversing the free list (lines 9–25)
The PACK macro (line 9) combines a size and an allocate bit and returns a value that can be stored in a header or footer.
The GET macro (line 12) reads and returns the word referenced by argument p.
The argument p is typically a (void *) pointer, which cannot be dereferenced directly.
Similarly, the PUT macro (line 13) stores val in the word pointed at by argument p.
The remaining macros operate on block pointers (denoted bp) that point to the ﬁrst payload byte.
Given a block pointer bp, the HDRP and FTRPmacros (lines 20–21) return pointers to the block header and footer, respectively.
The macros can be composed in various ways to manipulate the free list.
For example, given a pointer bp to the current block, we could use the following line of code to determine the size of the next block in memory:
Pack a size and allocated bit into a word */
Read the size and allocated fields from address p */
Given block ptr bp, compute address of its header and footer */
Given block ptr bp, compute address of next and previous blocks */
Figure 9.43 Basic constants and macros for manipulating the free list.
At this point, the allocator is initialized and ready to accept allocate and free requests from the application.
The heap begins on a double-word aligned boundary, and every call to extend_ heap returns a blockwhose size is an integral number of double words.
Extend the empty heap with a free block of CHUNKSIZE bytes */
Allocate an even number of words to maintain alignment */
Finally, in the likely case that the previous heap was terminated by a free block, we call the coalesce function to merge the two free blocks and return the block pointer of the merged blocks (line 17)
The code in the coalescehelper function is a straightforward implementation of the four cases outlined in Figure 9.40
The free list format we have chosen—with its prologue and epilogue blocks that are alwaysmarked as allocated—allows us to ignore the potentially troublesome edge conditions where the requested block bp is at the beginning or end of the heap.
Without these special blocks, the code would be messier, more error prone, and slower, because we would have to check for these rare edge conditions on each and every free request.
After checking for spurious requests, the allocator must adjust the requested block size to allow room for the header and the footer, and to satisfy the double-word alignment requirement.
Once the allocator has adjusted the requested size, it searches the free list for a suitable free block (line 18)
If there is a ﬁt, then the allocator places the requested block and optionally splits the excess (line 19), and then returns the address of the newly allocated block.
Your solution should perform a ﬁrst-ﬁt search of the implicit free list.
Practice Problem 9.9 Implement a place function for the example allocator.
Your solution should place the requested block at the beginning of the free block, splitting only if the size of the remainder would equal or exceed the minimum block size.
Figure 9.48 Format of heap blocks that use doubly linked free lists.
The implicit free list provides us with a simple way to introduce some basic allocator concepts.
However, because block allocation time is linear in the total number of heap blocks, the implicit free list is not appropriate for a generalpurpose allocator (although it might be ﬁne for a special-purpose allocator where the number of heap blocks is known beforehand to be small)
A better approach is to organize the free blocks into some form of explicit data structure.
Since by deﬁnition the body of a free block is not needed by the program, the pointers that implement the data structure can be stored within the bodies of the free blocks.
For example, the heap can be organized as a doubly linked free list by including a pred (predecessor) and succ (successor) pointer in each free block, as shown in Figure 9.48
Using a doubly linked list instead of an implicit free list reduces the ﬁrst ﬁt allocation time from linear in the total number of blocks to linear in the number of free blocks.
However, the time to free a block can be either linear or constant, depending on the policy we choose for ordering the blocks in the free list.
One approach is to maintain the list in last-in ﬁrst-out (LIFO) order by inserting newly freed blocks at the beginning of the list.
With a LIFO ordering and a ﬁrst ﬁt placement policy, the allocator inspects the most recently used blocks ﬁrst.
In this case, freeing a block can be performed in constant time.
If boundary tags are used, then coalescing can also be performed in constant time.
Another approach is to maintain the list in address order, where the address of each block in the list is less than the address of its successor.
In this case, freeing a block requires a linear-time search to locate the appropriate predecessor.
The trade-off is that address-ordered ﬁrst ﬁt enjoys better memory utilization than LIFO-ordered ﬁrst ﬁt, approaching the utilization of best ﬁt.
A disadvantage of explicit lists in general is that free blocks must be large enough to contain all of the necessary pointers, as well as the header and possibly a footer.
This results in a larger minimum block size, and increases the potential for internal fragmentation.
As we have seen, an allocator that uses a single linked list of free blocks requires time linear in the number of free blocks to allocate a block.Apopular approach for reducing the allocation time, known generally as segregated storage, is to maintain multiple free lists, where each list holds blocks that are roughly the same size.
The general idea is to partition the set of all possible block sizes into equivalence classes called size classes.
For example, we might partition the block sizes by powers of two:
The dynamic storage allocation literature describes dozens of variants of segregated storage that differ in how they deﬁne size classes, when they perform coalescing, when they request additional heap memory from the operating system, whether they allow splitting, and so forth.
To give you a sense of what is possible, we will describe two of the basic approaches: simple segregated storage and segregated ﬁts.
To allocate a block of some given size, we check the appropriate free list.
If the list is not empty, we simply allocate the ﬁrst block in its entirety.
If the list is empty, the allocator requests a ﬁxed-sized chunk of additional memory from the operating system (typically a multiple of the page size), divides the chunk into equal-sized blocks, and links the blocks together to form the new free list.
To free a block, the allocator simply inserts the block at the front of the appropriate free list.
There are a number of advantages to this simple scheme.
Further, the combination of the same-sized blocks in each chunk, no splitting, and no coalescing means that there is very little per-block memory overhead.
Since each chunk has only samesized blocks, the size of an allocated block can be inferred from its address.
Since there is no coalescing, allocated blocks do not need an allocated/free ﬂag in the header.
Since allocate and free operations insert and delete blocks at the beginning of the free list, the list need only be singly linked instead of doubly linked.
The bottom line is that the only required ﬁeld in any block is a one-word succ pointer in each free block, and thus the minimum block size is only one word.
A signiﬁcant disadvantage is that simple segregated storage is susceptible to internal and external fragmentation.
Internal fragmentation is possible because free blocks are never split.
Worse, certain reference patterns can cause extreme external fragmentation because free blocks are never coalesced (Problem 9.10)
Practice Problem 9.10 Describe a reference pattern that results in severe external fragmentation in an allocator based on simple segregated storage.
With this approach, the allocator maintains an array of free lists.
Each free list is associated with a size class and is organized as some kind of explicit or implicit list.
Each list contains potentially different-sized blocks whose sizes are members of the size class.
To allocate a block, we determine the size class of the request and do a ﬁrstﬁt search of the appropriate free list for a block that ﬁts.
If we ﬁnd one, then we (optionally) split it and insert the fragment in the appropriate free list.
If we cannot ﬁnd a block that ﬁts, then we search the free list for the next larger size class.
If none of the free lists yields a block that ﬁts, then we request additional heap memory from the operating system, allocate the block out of this new heap memory, and place the remainder in the appropriate size class.
To free a block, we coalesce and place the result on the appropriate free list.
The segregated ﬁts approach is a popular choice with production-quality allocators such as the GNU malloc package provided in the C standard library because it is both fast and memory efﬁcient.
Search times are reduced because searches are limited to particular parts of the heap instead of the entire heap.
Memory utilization can improve because of the interesting fact that a simple ﬁrstﬁt search of a segregated free list approximates a best-ﬁt search of the entire heap.
As we perform this splitting, each remaining half (known as a buddy) is placed on the appropriate free list.
To free a block of size 2k, we continue coalescing with the free.Whenwe encounter an allocated buddy, we stop the coalescing.
A key fact about buddy systems is that given the address and size of a block, it is easy to compute the address of its buddy.
For example, a block of size 32 byes with address.
In other words, the addresses of a block and its buddy differ in exactly one bit position.
The major advantage of a buddy system allocator is its fast searching and coalescing.
The major disadvantage is that the power-of-two requirement on the block size can cause signiﬁcant internal fragmentation.
For this reason, buddy system allocators are not appropriate for general-purpose workloads.
With an explicit allocator such as the C malloc package, an application allocates and frees heap blocks by making calls to malloc and free.
It is the application’s responsibility to free any allocated blocks that it no longer needs.
Failing to free allocated blocks is a common programming error.
For example, consider the following C function that allocates a block of temporary storage as part of its processing:
It remains allocated for the lifetime of the program, needlessly occupying heap space that could be used to satisfy subsequent allocation requests.
A garbage collector is a dynamic storage allocator that automatically frees allocated blocks that are no longer needed by the program.
Such blocks are known as garbage (hence the term garbage collector)
The process of automatically reclaiming heap storage is known as garbage collection.
In the context of a C program, the application calls malloc, but never calls free.
Instead, the garbage collector periodically identiﬁes the garbage blocks and makes the appropriate calls to free to place those blocks back on the free list.
Garbage collection dates back to Lisp systems developed by John McCarthy atMIT in the early 1960s.
It is an important part of modern language systems such as Java,ML, Perl, andMathematica, and it remains an active and important area of research.
The literature describes an amazing number of approaches for garbage collection.
We will limit our discussion to McCarthy’s original Mark&Sweep algorithm, which is interesting because it can be built on top of an existing malloc package to provide garbage collection for C and C++ programs.
We say that a node p is reachable if there exists a directed path from any root node to p.
At any point in time, the unreachable nodes correspond to garbage that can never be used again by the application.
The role of a garbage collector is to maintain some representation of the reachability graph and periodically reclaim the unreachable nodes by freeing them and returning them to the free list.
Garbage collectors for languages like ML and Java, which exert tight control over how applications create and use pointers, can maintain an exact representation of the reachability graph, and thus can reclaim all garbage.
However, collectors for languages like C and C++ cannot in general maintain exact representations of the reachability graph.
They are conservative in the sense that each reachable block.
Figure 9.49 A garbage collector’s view of memory as a directed graph.
Figure 9.50 Integrating a conservative garbage collector and a C malloc package.
Collectors can provide their service on demand, or they can run as separate threads in parallel with the application, continuously updating the reachability graph and reclaiming garbage.
For example, consider how wemight incorporate a conservative collector for C programs into an existing malloc package, as shown in Figure 9.50
The application calls malloc in the usual manner whenever it needs heap space.
If malloc is unable to ﬁnd a free block that ﬁts, then it calls the garbage collector in hopes of reclaiming some garbage to the free list.
The collector identiﬁes the garbage blocks and returns them to the heap by calling the free function.
The key idea is that the collector calls free instead of the application.
When the call to the collector returns, malloc tries again to ﬁnd a free block that ﬁts.
If that fails, then it can ask the operating system for additional memory.
Eventually malloc returns a pointer to the requested block (if successful) or the NULL pointer (if unsuccessful)
A Mark&Sweep garbage collector consists of a mark phase, which marks all reachable and allocated descendants of the root nodes, followed by a sweep phase, which frees each unmarked allocated block.
Typically, one of the spare low-order bits in the block header is used to indicate whether a block is marked or not.
Themark phase calls the mark function shown in Figure 9.51(a) once for each root node.
Each call to the mark function marks any unmarked and reachable descendants of some root node.
At the end of the mark phase, any allocated block that is not marked is guaranteed to be unreachable and, hence, garbage that can be reclaimed in the sweep phase.
The sweep phase is a single call to the sweep function shown in Figure 9.51(b)
The sweep function iterates over each block in the heap, freeing any unmarked allocated blocks (i.e., garbage) that it encounters.
Each block has a one-word header, which is either marked or unmarked.
Note that the arrows in this example denote memory references, and not free list pointers.
Initially, the heap in Figure 9.52 consists of six allocated blocks, each of which is unmarked.
After the sweep phase, the two unreachable blocks are reclaimed to the free list.
Mark&Sweep is an appropriate approach for garbage collecting C programs because it works in place withoutmoving any blocks.
However, the C language poses some interesting challenges for the implementation of the isPtr function.
First, C does not tag memory locations with any type information.
Thus, there is no obviousway for isPtr to determine if its input parameter p is a pointer or not.
Second, even if we were to know that p was a pointer, there would be no obvious way for isPtr to determine whether p points to some location in the payload of an allocated block.
One solution to the latter problem is to maintain the set of allocated blocks as a balanced binary tree that maintains the invariant that all blocks in the left subtree are located at smaller addresses and all blocks in the right subtree are located in larger addresses.
As shown in Figure 9.53, this requires two additional ﬁelds (left and right) in the header of each allocated block.
Each ﬁeld points to the header of some allocated block.
The isPtr(ptr p) function uses the tree to perform a binary search of the allocated blocks.
At each step, it relies on the size ﬁeld in the block header to determine if p falls within the extent of the block.
The balanced tree approach is correct in the sense that it is guaranteed tomark all of the nodes that are reachable from the roots.
This is a necessary guarantee, as application users would certainly not appreciate having their allocated blocks prematurely returned to the free list.
However, it is conservative in the sense that it may incorrectly mark blocks that are actually unreachable, and thus it may fail to free some garbage.
While this does not affect the correctness of application programs, it can result in unnecessary external fragmentation.
The fundamental reason that Mark&Sweep collectors for C programs must be conservative is that the C language does not tag memory locations with type information.
Thus, scalars like ints or floats can masquerade as pointers.
For example, suppose that some reachable allocated block contains an int in its payload whose value happens to correspond to an address in the payload of some other allocated block b.
There is no way for the collector to infer that the data is really an int and not a pointer.
Therefore, the allocator must conservatively mark block b as reachable, when in fact it might not be.
Figure 9.53 Left and right pointers in a balanced tree of allocated blocks.
We conclude our discussion of virtual memory with a discussion of some of the common memoryrelated bugs.
Aswe learned in Section 9.7.2, there are large holes in the virtual address space of a process that are not mapped to any meaningful data.
If we attempt to dereference a pointer into one of these holes, the operating systemwill terminate our program with a segmentation exception.
Attempting towrite to oneof these areas terminates the programwith a protection exception.
A common example of dereferencing a bad pointer is the classic scanf bug.
Suppose we want to use scanf to read an integer from stdin into a variable.
The correct way to do this is to pass scanf a format string and the address of the variable:
In this case, scanf will interpret the contents of val as an address and attempt to write aword to that location.
In the best case, the program terminates immediately with an exception.
In the worst case, the contents of val correspond to some valid read/write area of virtual memory, and we overwrite memory, usually with disastrous and bafﬂing consequences much later.
While bss memory locations (such as uninitialized global C variables) are always initialized to zeros by the loader, this is not true for heap memory.
A common error is to assume that heap memory is initialized to zero:
In this example, the programmer has incorrectly assumed that vector y has been initialized to zero.
A correct implementation would explicitly zero y[i], or use calloc.
Aswe saw inSection 3.12, a programhas abuffer overﬂowbug if itwrites to a target buffer on the stack without examining the size of the input string.
For example, the following function has a buffer overﬂow bug because the gets function copies an arbitrary length string to the buffer.
To ﬁx this, we would need to use the fgets function, which limits the size of the input string.
One common mistake is to assume that pointers to objects are the same size as the objects they point to:
The intent here is to create an array of n pointers, each of which points to an array of m ints.
This code will run ﬁne on machines where ints and pointers to ints are the same size.
But if we run this code on a machine like the Core i7, where a pointer is.
Since one of these words will likely be the boundary tag footer of the allocated block, we may not discover the error until we free the block much later in the program, at which point the coalescing code in the allocator will fail dramatically and for no apparent reason.
This is an insidious example of the kind of “action at a distance” that is so typical of memory-related programming bugs.
This is another version of the program in the previous section.
If we are not careful about the precedence and associativity of C operators, then we incorrectly manipulate a pointer instead of the object it points to.
In line 6, the intent is todecrement the integer valuepointed toby thesizepointer.
If we are lucky, the program will crash immediately; but more likely we will be left scratching our heads when the program produces an incorrect answermuch later in its execution.
Themoral here is to use parentheses whenever in doubt about precedence and associativity.
Another common mistake is to forget that arithmetic operations on pointers are performed in units that are the size of the objects they point to, which are not necessarily bytes.
For example, the intent of the following function is to scan an array of ints and return a pointer to the ﬁrst occurrence of val:
Naive C programmers who do not understand the stack discipline will sometimes reference local variables that are no longer valid, as in the following example:
This function returns a pointer (say, p) to a local variable on the stack and then pops its stack frame.Although p still points to a validmemory address, it no longer points to a valid variable.When other functions are called later in the program, the memory will be reused for their stack frames.
Later, if the program assigns some value to *p, then it might actually be modifying an entry in another function’s stack frame, with potentially disastrous and bafﬂing consequences.
A similar error is to reference data in heap blocks that have already been freed.
As with many memory-related bugs, the error will only become evident later in the program when we notice that the values in y are corrupted.
Memory leaks are slow, silent killers that occur when programmers inadvertently create garbage in the heap by forgetting to free allocated blocks.
For example, the following function allocates a heap block x and then returns without freeing it:
If leak is called frequently, then the heap will gradually ﬁll up with garbage, in the worst case consuming the entire virtual address space.
Memory leaks are particularly serious for programs such as daemons and servers, which by deﬁnition never terminate.
Processors that support virtual memory reference main memory using a form of indirection known as virtual addressing.
The processor generates a virtual address, which is translated into a physical address before being sent to the main memory.
The translation of addresses from a virtual address space to a physical address space requires close cooperation between hardware and software.
Dedicated hardware translates virtual addresses using page tables whose contents are supplied by the operating system.
First, it automatically caches recently used contents of the virtual address space stored on disk in main memory.
The block in a virtual memory cache is known as a page.
A reference to a page on disk triggers a page fault that transfers control to a fault handler in the operating system.
The fault handler copies the page from disk to the main memory cache, writing back the evicted page if necessary.
Second, virtual memory simpliﬁes memory management, which in turn simpliﬁes linking, sharing data between processes, the allocation of memory for processes, and program loading.
Finally, virtual memory simpliﬁes memory protection by incorporating protection bits into every page table entry.
The process of address translation must be integrated with the operation of any hardware caches in the system.
Modern systems initialize chunks of virtual memory by associating them with chunks of ﬁles on disk, a process known as memory mapping.
Dynamic memory allocators are application-level programs with a system-level feel, directly manipulating memory without much help from the type system.
Explicit allocators require applications to explicitly free their memory blocks.
Implicit allocators (garbage collectors) free any unused and unreachable blocks automatically.
Managing and usingmemory is a difﬁcult and error-prone task for C programmers.
Examples of common errors include dereferencing bad pointers, reading uninitialized memory, allowing stack buffer overﬂows, assuming that pointers and the objects they point to are the same size, referencing a pointer instead of the object it points to, misunderstanding pointer arithmetic, referencing nonexistent variables, and introducing memory leaks.
Kilburn and his colleagues published the ﬁrst description of virtual memory [60]
Architecture texts contain additional details about the hardware’s role in virtual memory [49]
Since that time there has been a tremendous amount of work in the area.Wilson, Johnstone, Neely, and Boles have written a beautiful survey and performance evaluation of explicit allocators [117]
The general comments in this book about the throughput and utilization of different allocator strategies are paraphrased from their survey.
Jones and Lins provide a comprehensive survey of garbage collection [54]
Kernighan and Ritchie [58] show the complete code for a simple allocator based on an explicit free list with a block size and successor pointer in each free block.
The code is interesting in that it uses unions to eliminate a lot of the complicated pointer arithmetic, but at the expense of a linear-time (rather than constant-time) free operation.
Single word Header and footer Header and footer Single word Header, but no footer Header and footer Double word Header and footer Header and footer Double word Header, but no footer Header and footer.
Mark & sweep garbage collectors are called conservative if: (a) They coalesce freed memory only when a memory request cannot be.
At one point in time, a 32-bit address space seemed impossibly large.
But now there are database and scientiﬁc applications that need more, and you can expect this trend to continue.
At some point in your lifetime, expect to ﬁnd yourself complaining about the cramped 64-bit address space on your personal computer!
You might ﬁnd it helpful to write out all the bits in the addresses, and then draw boxes around the different bit ﬁelds, such as VPN, TLBI, etc.
In this particular problem, there are no misses of any kind: the TLB has a copy of the PTE and the cache has a copy of the requested data words.
We haven’t discussed the open, fstat, or write functions, so you’ll need to read their man pages to see how they work.
The general approach for determining the block size is to round the sum of the requested payload and the header size to the nearest multiple of the alignment requirement (in this case 8 bytes)
Thus, it is good to understand the minimum block sizes associated with different allocator designs and alignment requirements.
The tricky part is to realize that the same block can be allocated or free at different points in time.
Thus, the minimum block size is the maximum of the minimum allocated block size and.
So the minimum block size for this allocator is 8 bytes.
But the solution requires you to understand how the rest of our simple implicit-list allocator works and how to manipulate and traverse blocks.
Notice that for this allocator the minimum block size is 16 bytes.
For each size class, the allocator creates a lot of memory that is never reclaimed because the allocator doesn’t coalesce, and because the application never requests blocks from that size class again.
Tothis point in our study of computer systems, we have assumed thatprograms run in isolation, with minimal input and output.
How-ever, in the real world, application programs use services provided by the operating system to communicate with I/O devices and with other programs.
This part of the book will give you an understanding of the basic I/O services provided by Unix operating systems, and how to use these services to build applications such as Web clients and servers that communicate with each other over the Internet.
You will learn techniques for writing concurrent programs such as Web servers that can service multiple clients at the same time.
Writing concurrent application programs can also allow them to execute faster on modern multi-core processors.
When you ﬁnish this part, you will be well on your way to becoming a power programmer with a mature understanding of computer systems and their impact on your programs.
Input/output (I/O) is the process of copying data between main memory and external devices such as disk drives, terminals, and networks.
An input operation copies data from an I/O device to main memory, and an output operation copies data from memory to a device.
All language run-time systems provide higher-level facilities for performing I/O.
For example, ANSIC provides the standard I/O library, with functions such as printf and scanf that perform buffered I/O.
On Unix systems, these higher-level I/O functions are implemented using system-level Unix I/O functions provided by the kernel.
Most of the time, the higher-level I/O functions work quite well and there is no need to use Unix I/O directly.
Understanding Unix I/O will help you understand other systems concepts.
I/O is integral to the operation of a system, and because of this we often encounter circular dependences between I/O and other systems ideas.
For example, I/O plays a key role in process creation and execution.
Conversely, process creation plays a key role in how ﬁles are shared by different processes.
Thus, to really understand I/O you need to understand processes, and vice versa.
We have already touched on aspects of I/O in our discussions of the memory hierarchy, linking and loading, processes, and virtual memory.
Now that you have a better understanding of these ideas, we can close the circle and delve into I/O in more detail.
Sometimes you have no choice but to use Unix I/O.There are some important cases where using higher-level I/O functions is either impossible or inappropriate.
For example, the standard I/O library provides no way to access ﬁle metadata such as ﬁle size or ﬁle creation time.
Further, there areproblemswith the standard I/O library that make it risky to use for network programming.
This chapter introduces you to the general concepts of Unix I/O and standard I/O, and shows you how to use them reliably from your C programs.
Besides serving as a general introduction, this chapter lays a ﬁrm foundation for our subsequent study of network programming and concurrency.
All I/O devices, such as networks, disks, and terminals, are modeled as ﬁles, and all input and output is performed by reading andwriting the appropriate ﬁles.
This elegant mapping of devices to ﬁles allows the Unix kernel to export a simple, lowlevel application interface, known as Unix I/O, that enables all input and output to be performed in a uniform and consistent way:
An application announces its intention to access an I/O device by asking the kernel to open the corresponding ﬁle.
The kernel keeps track of all information about the open ﬁle.
The kernel maintains a ﬁle position k, initially 0, for each open ﬁle.
The ﬁle position is a byte offset from the beginning of a ﬁle.
An application can set the current ﬁle position k explicitly by performing a seek operation.
When an application has ﬁnished accessing a ﬁle, it informs the kernel by asking it to close the ﬁle.
The kernel responds by freeing the data structures it created when the ﬁle was opened and restoring the descriptor to a pool of available descriptors.
When a process terminates for any reason, the kernel closes all open ﬁles and frees their memory resources.
A process opens an existing ﬁle or creates a new ﬁle by calling the open function:
The open function converts a filename to a ﬁle descriptor and returns the descriptor number.
The descriptor returned is always the smallest descriptor that is not currently open in the process.
The flags argument indicates how the process intends to access the ﬁle:
For example, here is how to open an existing ﬁle for reading:
The flags argument can also be or’d with one or more bit masks that provide additional instructions for writing:
O_CREAT: If the ﬁle doesn’t exist, then create a truncated (empty) version of it.
O_APPEND: Before each write operation, set the ﬁle position to the end of the ﬁle.
For example, here is how you might open an existing ﬁle with the intent of appending some data:
The mode argument speciﬁes the access permission bits of new ﬁles.
The symbolic names for these bits are shown in Figure 10.1
As part of its context, each process has a umask that is set by calling the umask function.
For example, suppose we are given the following default values for mode and umask:
Then the following code fragment creates a new ﬁle in which the owner of the ﬁle has read and write permissions, and all other users have read permissions:
Finally, a process closes an open ﬁle by calling the close function.
Closing a descriptor that is already closed is an error.
Practice Problem 10.1 What is the output of the following program?
Applications perform input and output by calling the read and write functions, respectively.
Figure 10.2 Copies standard input to standard output one byte at a time.
The write function copies at most n bytes from memory location buf to the current ﬁle position of descriptor fd.
Applications can explicitly modify the current ﬁle position by calling the lseek function, which is beyond our scope.
In some situations, read and write transfer fewer bytes than the application requests.
Then the next read will return a short count of 20, and the read after that will signal EOF by returning a short count of zero.
Reading text lines from a terminal.If the open ﬁle is associated with a terminal (i.e., a keyboard and display), then each read function will transfer one text line at a time, returning a short count equal to the size of the text line.
Reading and writing network sockets.If the open ﬁle corresponds to a network socket (Section 11.3.3), then internal buffering constraints and long network delays can cause read and write to return short counts.
Short counts can also occur when you call read and write on a Unix pipe, an interprocess communication mechanism that is beyond our scope.
In practice, you will never encounter short counts when you read from disk ﬁles except on EOF, and you will never encounter short counts when you write to disk ﬁles.
However, if you want to build robust (reliable) network applications such as Web servers, then you must deal with short counts by repeatedly calling read and write until all requested bytes have been transferred.
In this section, we will develop an I/O package, called the Rio (Robust I/O) package, that handles these short counts for you automatically.
The Rio package provides convenient, robust, and efﬁcient I/O in applications such as network programs that are subject to short counts.
Unbuffered input and output functions.These functions transfer data directly between memory and a ﬁle, with no application-level buffering.
They are especially useful for reading and writing binary data to and from networks.
Buffered input functions.These functions allowyou to efﬁciently read text lines and binary data from a ﬁle whose contents are cached in an application-level buffer, similar to the one provided for standard I/O functions such as printf.
For example, you can read some text lines from a descriptor, then some binary data, and then some more text lines.
First, we will be using them in the network applications we develop in the next two chapters.
Second, by studying the code for these routines, you will gain a deeper understanding of Unix I/O in general.
The rio_readn function transfers up to n bytes from the current ﬁle position of descriptor fd to memory location usrbuf.
The rio_readn function can only return a short count if it encounters EOF.
Notice that each function manually restarts the read or write function if it is interrupted by the return from an application signal handler.
To be as portable as possible, we allow for interrupted system calls and restart them when necessary.
See Section 8.5.4 for a discussion on interrupted system calls.
A text line is a sequence of ASCII characters terminated by a newline character.
Suppose we wanted to write a program that counts the number of text lines in a text ﬁle.
How might we do this? One approach is to use the read function to transfer 1 byte at a time from the ﬁle to the user’s memory, checking each byte for the newline character.
The disadvantage of this approach is that it is inefﬁcient, requiring a trap to the kernel to read each byte in the ﬁle.
A better approach is to call a wrapper function (rio_readlineb) that copies the text line from an internal read buffer, automatically making a read call to reﬁll the buffer whenever it becomes empty.
It associates the descriptor fd with a read buffer of type rio_t at address rp.
The rio_readlineb function reads the next text line from ﬁle rp (including the terminating newline character), copies it to memory location usrbuf, and terminates the text line with the null (zero) character.
Text lines that exceed maxlen-1 bytes are truncated and terminated with a null character.
However, calls to these buffered functions should not be interleaved with calls to the unbuffered rio_readn function.
You will encounter numerous examples of the Rio functions in the remainder of this text.
Figure 10.4 shows how to use the Rio functions to copy a text ﬁle from standard input to standard output, one line at a time.
Figure 10.4 Copying a text ﬁle from standard input to standard output.
The rio_readinitb function sets up an empty read buffer and associates an open ﬁle descriptor with that buffer.
The rio_read function is a buffered version of the Unix read function.
If the buffer is empty, then it is replenished with a call to read.
Receiving a short count from this invocation of read is not an error, and simply has the effect of partially ﬁlling the read buffer.
First, because readline is buffered and readn is not, these two functions cannot be used together on the same descriptor.
Second, because it uses a static buffer, the Stevens readline function is not thread-safe, which required Stevens to introduce a different threadsafe version called readline_r.
An application can retrieve information about a ﬁle (sometimes called the ﬁle’s metadata) by calling the stat and fstat functions.
The stat function takes as input a ﬁle name and ﬁlls in the members of a stat structure shown in Figure 10.8
A regular ﬁle contains some sort of binary or text data.
To the kernel there is no difference between text ﬁles and binary ﬁles.
A socket is a ﬁle that is used to communicate with another process across a network (Section 11.4)
Unix provides macro predicates for determining the ﬁle type from the st_ mode member.
Unix ﬁles can be shared in a number of different ways.
Unless you have a clear picture of how the kernel represents open ﬁles, the idea of ﬁle sharing can be quite confusing.
The kernel represents open ﬁles using three related data structures:
Descriptor table.Each process has its own separate descriptor table whose entries are indexed by the process’s open ﬁle descriptors.
Each open descriptor entry points to an entry in the ﬁle table.
File table.The set of open ﬁles is represented by a ﬁle table that is shared by all processes.
Each ﬁle table entry consists of (for our purposes) the current ﬁle position, a reference count of the number of descriptor entries that currently point to it, and a pointer to an entry in the v-node table.
Closing a descriptor decrements the reference count in the associated ﬁle table entry.
The kernel will not delete the ﬁle table entry until its reference count is zero.
Like the ﬁle table, the v-node table is shared by all processes.
This is the typical situation, where ﬁles are not shared, and where each descriptor corresponds to a distinct ﬁle.
Multiple descriptors can also reference the same ﬁle through different ﬁle table entries, as shown in Figure 10.12
This might happen, for example, if you were to call the open function twice with the same filename.
The key idea is that each descriptor has its own distinct ﬁle position, so different reads on different descriptors can fetch data from different locations in the ﬁle.
This example shows two descriptors sharing the same disk ﬁle through two open ﬁle table entries.
Figure 10.13 How a child process inherits the parent’s open ﬁles.
We can also understand how parent and child processes share ﬁles.
Suppose that before a call to fork, the parent process has the open ﬁles shown in Figure 10.11
Then Figure 10.13 shows the situation after the call to fork.
The child gets its own duplicate copy of the parent’s descriptor table.
Parent and child share the same set of open ﬁle tables, and thus share the same ﬁle position.An important consequence is that the parent and child must both close their descriptors before the kernel will delete the corresponding ﬁle table entry.
Practice Problem 10.2 Suppose the disk ﬁle foobar.txt consists of the six ASCII characters “foobar”
Practice Problem 10.3 As before, suppose the disk ﬁle foobar.txt consists of the six ASCII characters “foobar”
As we will see in Section 11.5, a Web server performs.
So how does I/O redirection work? One way is to use the dup2 function.
Thedup2 function copies descriptor table entryoldfd to descriptor table entry newfd, overwriting the previous contents of descriptor table entry newfd.
If newfd was already open, then dup2 closes newfd before it copies oldfd.
Both descriptors now point to ﬁle B; ﬁle A has been closed and its ﬁle table and v-node table entries deleted; and the reference count for ﬁle B has been incremented.
From this point on, any data written to standard output is redirected to ﬁle B.
Practice Problem 10.5 Assuming that the disk ﬁle foobar.txt consists of the six ASCII characters “foobar”, what is the output of the following program?
ANSICdeﬁnes a set of higher level input and output functions, called the standard I/O library, that provides programmers with a higher-level alternative to Unix I/O.
The standard I/O librarymodels an open ﬁle as a stream.
To the programmer, a stream is a pointer to a structure of type FILE.
EveryANSI C program begins with three open streams, stdin, stdout, and stderr, which correspond to standard input, standard output, and standard error, respectively:
A stream of type FILE is an abstraction for a ﬁle descriptor and a stream buffer.
The purpose of the stream buffer is the same as the Rio read buffer: to minimize the number of expensiveUnix I/O system calls.
For example, supposewe have a program thatmakes repeated calls to the standard I/O getc function, where each invocation returns the next character from a ﬁle.When getc is called the ﬁrst time, the library ﬁlls the stream buffer with a single call to the read function, and then returns the ﬁrst byte in the buffer to the application.
Figure 10.15 summarizes the various I/O packages that we have discussed in this chapter.
It is available to applications through functions such as open, close, lseek, read, write, and stat functions.
The higher-level Rio and standard I/O functions are implemented “on top of” (using) the Unix I/O functions.
TheRio functions are robust wrappers for read and write that were developed speciﬁcally for this textbook.
They automatically deal with short counts and provide an efﬁcient buffered approach for reading text lines.
The standard I/O functions provide a more complete buffered alternative to the Unix I/O functions, including formatted I/O routines.
So which of these functions should you use in your programs? The standard I/O functions are the method of choice for I/O on disk and terminal devices.
Most C programmers use standard I/O exclusively throughout their careers, never bothering with the lower-level Unix I/O functions.
Unfortunately, standard I/O poses some nasty problems when we attempt to use it for input and output on networks.
As we will see in Section 11.4, the Unix abstraction for a network is a type of ﬁle called a socket.
Like any Unix ﬁle, sockets are referenced by ﬁle descriptors, known in this case as socket descriptors.
Application processes communicate with processes running on other computers by reading and writing socket descriptors.
Standard I/O streams are full duplex in the sense that programs can perform input and output on the same stream.
However, there are poorly documented restrictions on streams that interact badly with restrictions on sockets:
An input function cannot followanoutput functionwithout an intervening call to fflush, fseek, fsetpos, or rewind.
The latter three functions use the Unix I/O lseek function to reset the current ﬁle position.
Restriction 2: Output functions following input functions.An output function cannot follow an input functionwithout an intervening call to fseek, fsetpos, or rewind, unless the input function encounters an end-of-ﬁle.
These restrictions pose a problem for network applications because it is illegal to use the lseek function on a socket.
The ﬁrst restriction on stream I/O can be worked around by adopting a discipline of ﬂushing the buffer before every input operation.
However, the only way to work around the second restriction is to open two streams on the same open socket descriptor, one for reading and one for writing:
But this approach has problems as well, because it requires the application to call fclose on both streams in order to free the memory resources associated with each stream and avoid a memory leak:
Eachof theseoperations attempts to close the sameunderlying socket descriptor, so the second close operation will fail.
This is not a problem for sequential programs, but closing an already closed descriptor in a threaded program is a recipe for disaster (see Section 12.7.4)
Thus, we recommend that you not use the standard I/O functions for input and output on network sockets.
If you need formatted output, use the sprintf function to format a string inmemory, and then send it to the socket using rio_writen.
If you need formatted input, use rio_ readlineb to read an entire text line, and then use sscanf to extract different ﬁelds from the text line.
Unix provides a small number of system-level functions that allow applications to open, close, read, and write ﬁles; fetch ﬁle metadata; and perform I/O redirection.
Unix read and write operations are subject to short counts that applications must anticipate and handle correctly.
Instead of calling the Unix I/O functions directly, applications should use the Rio package, which deals with short counts automatically by repeatedly performing read and write operations until all of the requested data have been transferred.
The Unix kernel uses three related data structures to represent open ﬁles.
Entries in a descriptor table point to entries in the open ﬁle table, which point.
Each process has its own distinct descriptor table, while all processes share the same open ﬁle and v-node tables.
Understanding the general organization of these structures clariﬁes our understanding of both ﬁle sharing and I/O redirection.
The standard I/O library is implemented on top of Unix I/O and provides a powerful set of higher-level I/O routines.
Formost applications, standard I/O is the simpler, preferred alternative to Unix I/O.
However, because of some mutually incompatible restrictions on standard I/O and network ﬁles, Unix I/O, rather than standard I/O, should be used for network applications.
Stevens wrote the standard reference text for Unix I/O [110]
Kernighan and Ritchie give a clear and complete discussion of the standard I/O functions [58]
You might expect that this invocation of fstatcheck would fetch and display metadata for ﬁle foo.txt.
However, when we run it on our system, it fails with a “bad ﬁle descriptor.” Given this behavior, ﬁll in the pseudo-code that the shell must be executing between the fork and execve calls:
Thus, the read from fd2 reads the ﬁrst byte of foobar.txt, and the output is.
Thus, the descriptor fd in both the parent and child points to the same open ﬁle table entry.
When the child reads the ﬁrst byte of the ﬁle, the ﬁle position increases by one.
Thus, the parent reads the second byte, and the output is.
Any time you browse the Web, send an email message, or pop up an X window, you are using a network application.
Interestingly, all network applications are based on the same basic programming model, have similar overall logical structures, and rely on the same programming interface.
Network applications rely on many of the concepts that you have already learned in our study of systems.
For example, processes, signals, byte ordering, memory mapping, and dynamic storage allocation all play important roles.
We will need to understand the basic clientserver programming model and how to write client-server programs that use the services provided by the Internet.
At the end, we will tie all of these ideas together by developing a small but functional Web server that can serve both static and dynamic content with text and graphics to real Web browsers.
A server manages some resource, and it provides some service for its clients bymanipulating that resource.
For example, aWeb servermanages a set of disk ﬁles that it retrieves and executes on behalf of clients.
An FTP server manages a set of disk ﬁles that it stores and retrieves for clients.
Similarly, an email server manages a spool ﬁle that it reads and updates for clients.
The fundamental operation in the client-server model is the transaction (Figure 11.1)
When a client needs service, it initiates a transaction by sending a request to the server.
For example, when a Web browser needs a ﬁle, it sends a request to a Web server.
The server receives the request, interprets it, and manipulates its resources in the appropriate way.
For example, when aWeb server receives a request from a browser, it reads a disk ﬁle.
The server sends a response to the client, and then waits for the next request.
For example, a Web server sends the ﬁle back to a client.
For example, after aWeb browser receives a page from the server, it displays it on the screen.
It is important to realize that clients and servers are processes and not machines, or hosts as they are often called in this context.
A single host can run many different clients and servers concurrently, and a client and server transaction can be on the same or different hosts.
The client-server model is the same, regardless of the mapping of clients and servers to hosts.
Client-server transactions are not database transactions and do not share any of their properties, such as atomicity.
In our context, a transaction is simply a sequence of steps carried out by a client and a server.
Clients and servers often run on separate hosts and communicate using the hardware and software resources of a computer network.
Networks are sophisticated systems, and we can only hope to scratch the surface here.
Our aim is to give you a workable mental model from a programmer’s perspective.
To a host, a network is just another I/O device that serves as a source and sink for data, as shown in Figure 11.2
An adapter plugged into an expansion slot on the I/O bus provides the physical interface to the network.
Data received from the network is copied from the adapter across the I/O andmemory buses intomemory, typically by a DMA transfer.
Similarly, data can also be copied from memory to the network.
Physically, a network is a hierarchical system that is organized by geographical proximity.
At the lowest level is a LAN (Local Area Network) that spans a building or a campus.
The most popular LAN technology by far is Ethernet, which was developed in the mid-1970s at Xerox PARC.
An Ethernet segment consists of some wires (usually twisted pairs of wires) and a small box called a hub, as shown in Figure 11.3
Ethernet segments typically span small areas, such as a room or a ﬂoor in a building.
One end is attached to an adapter on a host, and the other end is attached to a port on the hub.
A hub slavishly copies every bit that it receives on each port to every other port.
Each Ethernet adapter has a globally unique 48-bit address that is stored in a non-volatile memory on the adapter.
A host can send a chunk of bits called a frame to any other host on the segment.
Each frame includes some ﬁxed number of header bits that identify the source and destination of the frame and the frame length, followed by a payload of data bits.
Every host adapter sees the frame, but only the destination host actually reads it.
Multiple Ethernet segments can be connected into larger LANs, called bridged Ethernets, using a set of wires and small boxes called bridges, as shown in Figure 11.4
In a bridged Ethernet, some wires connect bridges to bridges, and others connect bridges to hubs.
Bridges make better use of the available wire bandwidth than hubs.
Using a clever distributed algorithm, they automatically learn over time which hosts are reachable from which ports, and then selectively copy frames from one port to another only when it is necessary.
For example, if host A sends a frame to host B, which is on the segment, then bridge X will throw away the frame when it arrives at its input port, thus saving bandwidth on the other segments.
However, if host A sends a frame to host C on a different segment, then bridge X will copy the frame only to the port connected to bridge Y, which will copy the frame only to the port connected to bridge C’s segment.
To simplify our pictures of LANs, we will draw the hubs and bridges and the wires that connect them as a single horizontal line, as shown in Figure 11.5
At a higher level in the hierarchy, multiple incompatible LANs can be connected by specialized computers called routers to form an internet (interconnected network)
We will always use lowercase internet to denote the general concept, and uppercase Internet to denote a speciﬁc implementation, namely the global IP Internet.
Each router has an adapter (port) for each network that it is connected to.
Routers can also connect high-speed point-to-point phone connections, which are examples of networks known asWANs (Wide-Area Networks), so called because they span larger geographical areas than LANs.
In general, routers can be used to build internets from arbitrary collections of LANs and WANs.
For example, Figure 11.6 shows an example internet with a pair of LANs andWANs connected by three routers.
The crucial property of an internet is that it can consist of different LANs and WANs with radically different and incompatible technologies.
Each host is physically connected to every other host, but how is it possible for some source host to send data bits to another destination host across all of these incompatible networks?
The solution is a layer of protocol software running on each host and router that smoothes out the differences between the different networks.
Two LANs and two WANs are connected by three routers.
Naming scheme.Different LAN technologies have different and incompatible ways of assigning addresses to hosts.
The internet protocol smoothes these differences by deﬁning a uniform format for host addresses.
Each host is then assigned at least one of these internet addresses that uniquely identiﬁes it.
Different networking technologies have different and incompatible ways of encoding bits on wires and of packaging these bits into frames.
The internet protocol smoothes these differences by deﬁning a uniform way to bundle up data bits into discrete chunks called packets.
A packet consists of a header, which contains the packet size and addresses of the source and destination hosts, and a payload, which contains data bits sent from the source host.
Figure 11.7 shows an example of how hosts and routers use the internet protocol to transfer data across incompatibleLANs.
The example internet consists of two LANs connected by a router.
The client on hostA invokes a system call that copies the data from the client’s virtual address space into a kernel buffer.
Notice that the payload of the LAN1 frame is an internet packet, whose payload is the actual user data.
This kind of encapsulation is one of the fundamental insights of internetworking.
When the frame reaches the router, the router’s LAN1 adapter reads it from the wire and passes it to the protocol software.
The router fetches the destination internet address from the internet packet header and uses this as an index into a routing table to determine where to forward the packet, which in this case is LAN2
Figure 11.7 How data travels from one host to another on an internet.
The router’s LAN2 adapter copies the frame to the network.
When the frame reaches host B, its adapter reads the frame from the wire and passes it to the protocol software.
Finally, the protocol software on host B strips off the packet header and frame header.
The protocol software will eventually copy the resulting data into the server’s virtual address space when the server invokes a system call that reads the data.
Of course, we are glossing over many difﬁcult issues here.
What if different networks have different maximum frame sizes? How do routers know where to forward frames? How are routers informed when the network topology changes? What if a packet gets lost? Nonetheless, our example captures the essence of the internet idea, and encapsulation is the key.
The global IP Internet is the most famous and successful implementation of an internet.
While the internal architecture of the Internet is complex and constantly changing, the organization of client-server applications has remained remarkably stable since the early 1980s.
Figure 11.8 shows the basic hardware and software organization of an Internet.
Figure 11.8 Hardware and software organization of an Internet application.
Each Internet host runs software that implements the TCP/IP protocol (Transmission Control Protocol/Internet Protocol), which is supported by almost every modern computer system.
Internet clients and servers communicate using a mix of sockets interface functions and Unix I/O functions.
The sockets functions are typically implemented as system calls that trap into the kernel and call various kernel-mode functions in TCP/IP.
TCP/IP is actually a family of protocols, each of which contributes different capabilities.
For example, the IP protocol provides the basic naming scheme and a delivery mechanism that can send packets, known as datagrams, from one Internet host to any other host.
To simplify our discussion, we will treat TCP/IP as a single monolithic protocol.
We will not discuss its inner workings, and we will only discuss some of the basic capabilities that TCP and IP provide to application programs.
Froma programmer’s perspective, we can think of the Internet as aworldwide collection of hosts with the following properties:
The set of hosts is mapped to a set of 32-bit IP addresses.
The set of IP addresses is mapped to a set of identiﬁers called Internet domain names.
A process on one Internet host can communicate with a process on any other Internet host over a connection.
The next three sections discuss these fundamental Internet ideas in more detail.
Network programs store IP addresses in the IP address structure shown in Figure 11.9
Aside Why store the scalar IP address in a structure?
Storing a scalar address in a structure is an unfortunate artifact from the early implementations of the sockets interface.
It would make more sense to deﬁne a scalar type for IP addresses, but it is too late to change now because of the enormous installed base of applications.
Because Internet hosts can have different host byte orders, TCP/IP deﬁnes a uniform network byte order (big-endian byte order) for any integer data item, such as an IP address, that is carried across the network in a packet header.Addresses in IP address structures are always stored in (big-endian) network byte order, even if the host byte order is little-endian.
Unix provides the following functions for converting between network and host byte order:
The htonl function converts a 32-bit integer from host byte order to network byte order.
The ntohl function converts a 32-bit integer from network byte order to host byte order.
The htons and ntohs functions perform corresponding conversions for 16-bit integers.
The inet_aton function converts a dotted-decimal string (cp) to an IP address in network byte order (inp)
Similarly, the inet_ntoa function converts an IP address in network byte order to its corresponding dotted-decimal string.
The set of domain names forms a hierarchy, and each domain name encodes its position in the hierarchy.
Figure 11.10 shows a portion of the domain name hierarchy.
The nodes of the tree represent domain names that are formed by the path back to the root.
The ﬁrst level in the hierarchy is an unnamed root node.
At the next level are second-level domain names such as cmu.edu, which are assigned on a ﬁrst-come ﬁrst-serve basis by various authorized agents of ICANN.
Once an organization has received a second-level domain name, then it is free to create any other new domain name within its subdomain.
The Internet deﬁnes a mapping between the set of domain names and the set of IP addresses.
Until 1988, this mapping was maintained manually in a single text ﬁle called HOSTS.TXT.
Since then, the mapping has been maintained in a distributed world-wide database known as DNS (Domain Name System)
Conceptually, the DNS database consists of millions of the host entry structures shown in Figure 11.11, each of which deﬁnes themapping between a set of domain names (an ofﬁcial name and a list of aliases) and a set of IP addresses.
In a mathematical sense, you can think of each host entry as an equivalence class of domain names and IP addresses.
Internet applications retrieve arbitrary host entries from the DNS database by calling the gethostbyname and gethostbyaddr functions.
Returns: non-NULL pointer if OK, NULL pointer on error with h_errno set.
Returns: non-NULL pointer if OK, NULL pointer on error with h_errno set.
The gethostbyname function returns the host entry associated with the domain name name.
The gethostbyaddr function returns the host entry associated with the IP address addr.
The second argument gives the length in bytes of an IP.
We can explore someof the properties of theDNSmappingwith the hostinfo program in Figure 11.12, which reads a domain name or dotted-decimal address from the command line and displays the corresponding host entry.
Each Internet host has the locally deﬁned domain name localhost, which always maps to the loopback address 127.0.0.1:
The localhost name provides a convenient and portable way to reference clients and servers that are running on the same machine, which can be especially useful for debugging.
We can use hostname to determine the real domain name of our local host:
In the simplest case, there is a one-to-one mapping between a domain name and an IP address:
However, in some cases, multiple domain names are mapped to the same IP address:
In the most general case, multiple domain names can be mapped to multiple IP addresses:
Finally, we notice that some valid domain names are notmapped to any IP address:
The survey, which estimates the number of Internet hosts by counting the number of IP addresses that have been assigned a domain name, reveals an amazing trend.
What do you notice about the ordering of the IP addresses in the three host entries?
Internet clients and servers communicate by sending and receiving streams of bytes over connections.
A connection is point-to-point in the sense that it connects a pair of processes.
It is full-duplex in the sense that data can ﬂow in both directions at the same time.
Each socket has a corresponding socket address that consists of an Internet address and a 16-bit integer port, and is denoted by address:port.
The port in the client’s socket address is assigned automatically by the kernel when the client makes a connection request, and is known as an ephemeral port.
However, the port in the server’s socket address is typically some well-known port that is associated with the service.
A connection is uniquely identiﬁed by the socket addresses of its two endpoints.
This pair of socket addresses is known as a socket pair and is denoted by the tuple.
For example, Figure 11.13 shows a connection between a Web client and a Web server.
Given these client and server socket addresses, the connection between the client and server is uniquely identiﬁed by the socket pair.
The Internet is one of themost successful examples of government, university, and industry partnership.
Many factors contributed to its success, but we think two are particularly important: a sustained 30year investment by the United States government, and a commitment by passionate researchers to what Dave Clarke at MIT has dubbed “rough consensus and working code.”
The seeds of the Internet were sown in 1957, when, at the height of the Cold War, the Soviet Union shocked theworld by launching Sputnik, the ﬁrst artiﬁcial earth satellite.
In response, theUnited States government created the Advanced Research Projects Agency (ARPA), whose charter was to reestablish the U.S.
In 1967, Lawrence Roberts at ARPA published plans for a new network called the ARPANET.
The sockets interface is a set of functions that are used in conjunctionwith theUnix I/O functions to build network applications.
It has been implemented on most modern systems, including all Unix variants, Windows, and Macintosh systems.
Figure 11.14 gives an overview of the sockets interface in the context of a typical client-server transaction.
You should use this picture as a road map when we discuss the individual functions.
The sockets interface was developed by researchers at University of California, Berkeley, in the early 1980s.
For this reason, it is often referred to as Berkeley sockets.
The Berkeley researchers developed the sockets interface to work with any underlying protocol.
The ﬁrst implementation was for TCP/IP, which they included in the Unix 4.2BSD kernel and distributed to numerous universities and labs.
Almost overnight, thousands of people had access to TCP/IP and its source codes.
It generated tremendous excitement and sparked a ﬂurry of new research in networking and internetworking.
From the perspective of the Unix kernel, a socket is an end point for communication.
From the perspective of a Unix program, a socket is an open ﬁle with a corresponding descriptor.
The IP address and port number are always stored in network (big-endian) byte order.
Generic socket address structure (for connect, bind, and accept) */
Today we would use the generic void * pointer, which did not exist in C at that time.
The solution was to deﬁne sockets functions to expect a pointer to a generic sockaddr structure, and then require applications to cast pointers to protocol-speciﬁc structures to this generic structure.
To simplify our code examples, we follow Stevens’s lead and deﬁne the following type:
We then use this type whenever we need to cast a sockaddr_in structure to a generic sockaddr structure.
Clients and servers use the socket function to create a socket descriptor.
In our codes, we will always call the socket function with the arguments.
The clientfd descriptor returned by socket is only partially opened and cannot yet be used for reading and writing.
How we ﬁnish opening the socket depends on whether we are a client or a server.
The next section describes how we ﬁnish opening the socket if we are a client.
A client establishes a connection with a server by calling the connect function.
The connect function blocks until either the connection is successfully established or an error occurs.
If successful, the sockfd descriptor is now ready for reading and writing, and the resulting connection is characterized by the socket pair.
We ﬁnd it convenient to wrap the socket and connect functions into a helper function called open_clientfd that a client can use to establish a connection with a server.
The open_clientfd function establishes a connection with a server running on host hostname and listening for connection requests on the well-known port port.
It returns an open socket descriptor that is ready for input and output using the Unix I/O functions.
When the connect function returns, we return the socket descriptor to the client, which can immediately begin using Unix I/O to communicate with the server.
The remaining sockets functions—bind, listen, and accept—are used by servers to establish connections with clients.
The bind function tells the kernel to associate the server’s socket address in my_addr with the socket descriptor sockfd.
Servers are passive entities that wait for connection requests from clients.
By default, the kernel assumes that a descriptor created by the socket function corresponds to an active socket that will live on the client end of a connection.
A server calls the listen function to tell the kernel that the descriptor will be used by a server instead of a client.
The listen function converts sockfd from an active socket to a listening socket that can accept connection requests fromclients.
The backlog argument is a hint about the number of outstanding connection requests that the kernel should queue up before it starts to refuse requests.
The exact meaning of the backlog argument requires an understanding of TCP/IP that is beyond our scope.
We ﬁnd it helpful to combine the socket, bind, and listen functions into a helper function called open_listenfd that a server can use to create a listening descriptor.
The open_listenfd function opens and returns a listening descriptor that is ready to receive connection requests on the well-known port port.
After we create the listenfd socket descriptor, we use the setsockopt function (not described here) to conﬁgure the server so that it can be terminated and restarted immediately.
Make it a listening socket ready to accept connection requests */
Next, we initialize the server’s socket address structure in preparation for calling the bind function.
Notice that we use the htonl and htons functions to convert the IP address and port number from host byte order to network byte order.
Finally, we convert listenfd to a listening descriptor (line 25) and return it to the caller.
Servers wait for connection requests from clients by calling the accept function:
The accept function waits for a connection request from a client to arrive on the listening descriptor listenfd, then ﬁlls in the client’s socket address in addr, and returns a connected descriptor that can be used to communicate with the client using Unix I/O functions.
The distinction between a listening descriptor and a connected descriptor confuses many students.
The listening descriptor serves as an end point for client connection requests.
It is typically created once and exists for the lifetime of the server.
The connected descriptor is the end point of the connection that is established between the client and the server.
It is created each time the server accepts a connection request and exists only as long as it takes the server to service a client.
Figure 11.18 outlines the roles of the listening and connected descriptors.
Recall that descriptors 0–2 are reserved for the standard ﬁles.
In Step 2, the client calls the connect function, which sends a connection request to listenfd.
In Step 3, the accept function opens a new connected.
Server blocks in accept, waiting for connection request on listening descriptor listenfd.
Client makes connection request by calling and blocking in connect.
Figure 11.18 The roles of the listening and connected descriptors.
The client also returns from the connect, and from this point, the client and server can pass data back and forth by reading and writing clientfd and connfd, respectively.
You might wonder why the sockets interface makes a distinction between listening and connected descriptors.
At ﬁrst glance, it appears to be an unnecessary complication.
However, distinguishing between the two turns out to be quite useful, because it allows us to build concurrent servers that can process many client connections simultaneously.
For example, each time a connection request arrives on the listening descriptor, we might fork a new process that communicates with the client over its connected descriptor.
The best way to learn the sockets interface is to study example code.
After establishing a connection with the server, the client enters a loop that repeatedly reads a text line from standard input, sends the text line to the server, reads the echo line from the server, and prints the result to standard output.
The loop terminates when fgets encounters EOFon standard input, either because the user typed ctrl-d at the keyboard or because it has exhausted the text lines in a redirected input ﬁle.
This results in an EOF notiﬁcation being sent to the server, which it detects when it receives a return code of zero from its rio_readlineb function.
Since the client’s kernel automatically closes all open descriptors when a process terminates, the close in line 24 is not necessary.
However, it is good programming practice to explicitly close any descriptors we have opened.
Figure 11.20 shows the main routine for the echo server.
After opening the listening descriptor, it enters an inﬁnite loop.
Each iteration waits for a connection request from a client, prints the domain name and IP address of the connected client, and calls the echo function that services the client.
After the echo routine returns, the main routine closes the connected descriptor.
Once the client and server have closed their respective descriptors, the connection is terminated.
Notice that our simple echo server can only handle one client at a time.
A server of this type that iterates through clients, one at a time, is called an iterative server.
In Chapter 12, we will learn how to build more sophisticated concurrent servers that can handle multiple clients simultaneously.
The idea of EOF is often confusing to students, especially in the context of Internet connections.
First, we need to understand that there is no such thing as an EOF character.
Rather, EOF is a condition that is detected by the kernel.
An application ﬁnds out about the EOF condition when it receives a zero return code from the read function.
For disk ﬁles, EOF occurs when the current ﬁle position exceeds the ﬁle length.
For Internet connections, EOF occurs when a process closes its end of the connection.
The process at the other end of the connection detects the EOF when it attempts to read past the last byte in the stream.
Determine the domain name and IP address of the client */
Figure 11.21 echo function that reads and echoes text lines.
So far we have discussed network programming in the context of a simple echo server.
In this section, we will show you how to use the basic ideas of network programming to build your own small, but quite functional, Web server.
Web clients and servers interact using a text-based application-level protocol known as HTTP (Hypertext Transfer Protocol)
A Web client (known as a browser) opens an Internet connection to a server and requests some content.
The server responds with the requested content and then closes the connection.
The browser reads the content and displays it on the screen.
What distinguishesWeb services from conventional ﬁle retrieval services such as FTP? The main difference is that Web content can be written in a language known as HTML (Hypertext Markup Language)
An HTML program (page) contains instructions (tags) that tell the browser how to display various text and graphical objects in the page.
However, the real power ofHTML is that a page can contain pointers (hyperlinks) to content stored on any Internet host.
If the user clicks on the highlighted text object, the browser requests the corresponding HTML ﬁle from the CMU server and displays it.
In 1989, Berners-Lee wrote an internal memo proposing a distributed hypertext system that would connect a “web of notes with links.” The intent of the proposed system was to help CERN scientists share andmanage information.
Over the next 2 years, after Berners-Lee implemented the ﬁrst Web server and Web browser, the Web developed a small following within CERN and a few other sites.
A pivotal event occurred in 1993, whenMarc Andreesen (who later founded Netscape) and his colleagues at NCSA released a graphical browser called mosaic for all three major platforms: Unix, Windows, and Macintosh.
After the release of mosaic, interest in the Web exploded, with the number of Web sites increasing by a factor of 10 or more each year.
ToWebclients and servers, content is a sequenceof byteswith an associatedMIME (Multipurpose Internet Mail Extensions) type.
Web servers provide content to clients in two different ways:
Fetch a disk ﬁle and return its contents to the client.
The disk ﬁle is known as static content and the process of returning the ﬁle to the client is known as serving static content.
Run an executable ﬁle and return its output to the client.
The output produced by the executable at run time is known as dynamic content, and the process of running the program and returning its output to the client is known as serving dynamic content.
Every piece of content returned by a Web server is associated with some ﬁle that it manages.
Each of these ﬁles has a unique name known as aURL (Universal Resource Locator)
URLs for executable ﬁles can include program arguments after the ﬁle name.
Clients and servers use different parts of the URL during a transaction.
There are several points to understand about how servers interpret the sufﬁx of a URL:
There are no standard rules for determining whether a URL refers to static or dynamic content.
Each server has its own rules for the ﬁles it manages.
A common approach is to identify a set of directories, such as cgi-bin, where all executables must reside.
The initial ‘/’ in the sufﬁx does not denote the Unix root directory.
Rather, it denotes the home directory for whatever kind of content is being requested.
For example, a server might be conﬁgured so that all static content is stored in directory /usr/httpd/html and all dynamic content is stored in directory /usr/httpd/cgi-bin.
The minimal URL sufﬁx is the ‘/’ character, which all servers expand to some default home page such as /index.html.
This explains why it is possible to fetch the homepage of a site by simply typing a domain name into the browser.
Figure 11.23 Example of an HTTP transaction that serves static content.
Since HTTP is based on text lines transmitted over Internet connections, we can use the Unix telnet program to conduct transactions with any Web server on the Internet.
The telnet program is very handy for debugging servers that talk to clients with text lines over connections.
For example, Figure 11.23 uses telnet to request the home page from the AOL Web server.
In line 1, we run telnet from a Unix shell and ask it to open a connection to the AOL Web server.
Telnet prints three lines of output to the terminal, opens the connection, and then waits for us to enter text (line 5)
Each time we enter a text line and hit the enter key, telnet reads the line, appends carriage return and line feed characters (“\r\n” in C notation), and sends the line to the server.
This is consistent with the HTTP standard, which requires every text line to be terminated by a carriage return and line feed pair.
To initiate the transaction, we enter an HTTP request (lines 5–7)
The GET method instructs the server to generate and return the content identiﬁed by theURI (Uniform Resource Identiﬁer)
The URI is the sufﬁx of the correspondingURL that includes the ﬁle name and optional arguments.1
The <version> ﬁeld in the request line indicates the HTTP version to which the request conforms.
HTTP/1.1 deﬁnes additional headers that provide support for advanced features such as caching and security, as well as a mechanism that allows a client and server to perform multiple transactions over the same persistent connection.
To summarize, the request line in line 5 asks the server to fetch and return the HTML ﬁle /index.html.
It also informs the server that the remainder of the request will be in HTTP/1.1 format.
Request headers provide additional information to the server, such as the brand name of the browser or the MIME types that the browser understands.
The Host header is used by proxy caches, which sometimes serve as intermediaries between a browser and the origin server that manages the requested ﬁle.
Multiple proxies can exist between a client and an origin server in a so-called proxy chain.
The data in the Host header, which identiﬁes the domain name of the origin server, allows a proxy in the middle of a proxy chain to determine if it might have a locally cached copy of the requested content.
Actually, this is only true when a browser requests content.
If a proxy server requests content, then the URI must be the complete URL.
Moved permanently Content has moved to the hostname in the Location header.
Bad request Request could not be understood by the server.
The version ﬁeld describes the HTTP version that the response conforms to.
The status code is a three-digit positive integer that indicates the disposition of the request.
The status message gives the English equivalent of the error code.
Figure 11.24 lists some common status codes and their corresponding messages.
The response headers in lines 9–13 provide additional information about the response.
The empty text line in line 14 that terminates the response headers is followed by the response body, which contains the requested content.
If we stop to think for a moment how a server might provide dynamic content to a client, certain questions arise.
For example, how does the client pass any program arguments to the server? How does the server pass these arguments to the child process that it creates? How does the server pass other information to the child that it might need to generate the content? Where does the child send its output? These questions are addressed by a de facto standard called CGI (Common Gateway Interface)
Spaces are not allowed in arguments and must be represented with the “%20” string.
Arguments for HTTP POST requests are passed in the request body rather than in the URI.
Programs like the adder program are often referred to as CGI programs because they obey the rules of the CGI standard.
And since many CGI programs are written as Perl scripts, CGI programs are often called CGI scripts.
A CGI program sends its dynamic content to the standard output.
Before the child process loads and runs the CGI program, it uses the Unix dup2 function to redirect standard output to the connected descriptor that is associated with the client.
Thus, anything that the CGI program writes to standard output goes directly to the client.
Notice that since the parent does not know the type or size of the content that the child generates, the child is responsible for generating the Content-type and Content-length response headers, as well as the empty line that terminates the headers.
Figure 11.26 shows a simple CGI program that sums its two arguments and returns an HTML ﬁle with the result to the client.
Figure 11.27 shows an HTTP transaction that serves dynamic content from the adder program.
For POST requests, the child would also need to redirect standard input to the connected descriptor.
The CGI program would then read the arguments in the request body from standard input.
Figure 11.27 An HTTP transaction that serves dynamic HTML content.
Yet the CGI program in Figure 11.26 is able to use standard I/O without any problems.
We conclude our discussion of network programming by developing a small but functioning Web server called Tiny.
It combines many of the ideas that we have learned about, such as process control, Unix I/O, the sockets interface, and HTTP, in only 250 lines of code.
While it lacks the functionality, robustness, and security of a real server, it is powerful enough to serve both static and dynamic content to real Web browsers.
We encourage you to study it and implement it yourself.
Tiny is an iterative server that listens for connection requests on the port that is passed in the command line.
The doit function in Figure 11.29 handles one HTTP transaction.
First, we read and parse the request line (lines 11–12)
If the client requests another method (such as POST), we send it an error message and return to the main routine (lines 13–17), which then closes the connection and awaits the next connection request.
Otherwise, we read and (as we shall see) ignore any request headers (line 18)
Next, we parse the URI into a ﬁle name and a possibly empty CGI argument string, and we set a ﬂag that indicates whether the request is for static or dynamic content (line 21)
If the ﬁle does not exist on disk, we immediately send an error message to the client and return.
Finally, if the request is for static content, we verify that the ﬁle is a regular ﬁle and that we have read permission (line 29)
If so, we serve the static content (line 34) to the client.
Tiny lacks many of the error handling features of a real server.
However, it does check for some obvious errors and reports them to the client.
The clienterror function in Figure 11.30 sends anHTTP response to the clientwith the appropriate.
Figure 11.30 Tiny clienterror: Sends an error message to the client.
Recall that an HTML response should indicate the size and type of the content in the body.
Thus, we have opted to build the HTML content as a single string so that we can easily determine its size.
Tinydoes not use anyof the information in the request headers.
Tiny assumes that the home directory for static content is its current directory, and that the home directory for executables is ./cgi-bin.
Any URI that contains the string cgi-bin is assumed to denote a request for dynamic content.
It parses the URI into a ﬁle name and an optional CGI argument string.
Tiny serves four different types of static content: HTML ﬁles, unformatted text ﬁles, and images encoded in GIF and JPEG formats.
These ﬁle types account for the majority of static content served over the Web.
Next, we send the response body by copying the contents of the requested ﬁle to the connected descriptor fd.
The code here is somewhat subtle and needs to be studied carefully.
Line 15 opens filename for reading and gets its descriptor.
In line 16, the Unix mmap function maps the requested ﬁle to a virtual memory area.
Recall from our discussion of mmap in Section 9.8 that the call to mmap maps the.
Once we have mapped the ﬁle to memory, we no longer need its descriptor, so we close the ﬁle (line 17)
Failing to do this would introduce a potentially fatal memory leak.
Line 18 performs the actual transfer of the ﬁle to the client.
This is important to avoid a potentially fatal memory leak.
Tiny serves any type of dynamic content by forking a child process, and then running a CGI program in the context of the child.
The CGI program is responsible for sending the rest of the response.
Notice that this is not as robust as we might wish, since it doesn’t allow for the possibility that the CGI program might encounter some error.
After sending the ﬁrst part of the response, we fork a new child process (line 11)
Notice that a real server would set the otherCGI environment variables here aswell.
Also, we note that Solaris systems use the putenv function instead of the setenv function.
Since the CGI program runs in the context of the child, it has access to the same open ﬁles and environment variables that existed before the call to the execve function.
Thus, everything that the CGI program writes to standard output goes directly to the client process, without any intervention from the parent process.
Meanwhile, the parent blocks in a call to wait, waiting to reap the child when it terminates (line 17)
With this model, an application consists of a server and one or more clients.
The server manages resources, providing a service for its clients bymanipulating the resources in some way.
The basic operation in the client-server model is a client-server transaction, which consists of a request from a client, followed by a response from the server.
Clients and servers communicate over a global networkknownas the Internet.
The set of IP addresses is mapped to a set of Internet domain names.
Processes on different Internet hosts can communicate with each other over connections.
Clients and servers establish connections by using the sockets interface.
A socket is an end point of a connection that is presented to applications in the form of a ﬁle descriptor.
The sockets interface provides functions for opening and closing socket descriptors.
Clients and servers communicate with each other by reading and writing these descriptors.
Web servers and their clients (such as browsers) communicate with each other using the HTTP protocol.
A browser requests either static or dynamic content from the server.
A request for static content is served by fetching a ﬁle from the.
A request for dynamic content is served by running a program in the context of a child process on the server and returning its output to the client.
The CGI standard provides a set of rules that govern how the client passes program arguments to the server, how the server passes these arguments and other information to the child process, and how the child sends its output back to the client.
A simple but functioning Web server that serves both static and dynamic content can be implemented in a few hundred lines of C code.
The ofﬁcial source of information for the Internet is contained in a set of freely available numbered documents known as RFCs (Requests for Comments)
A searchable index of RFCs is available on the Web at.
RFCs are typically written for developers of Internet infrastructure, and thus are usually too detailed for the casual reader.
Serious students of Unix systems programming will want to study all of them.
Use your favorite browser to make a request to Tiny for static content.
Check your work by using a real browser to request the form from Tiny, submit the ﬁlled-in form to Tiny, and then display the dynamic content generated by adder.
The different ordering of the addresses in different DNS queries is known asDNS round-robin.
It can be used to load-balance requests to a heavily used domain name.
When the child terminates, the kernel closes all descriptors automatically.
As we learned in Chapter 8, logical control ﬂows are concurrent if they overlap in time.
This general phenomenon, known as concurrency, shows up at many different levels of a computer system.
Hardware exception handlers, processes, and Unix signal handlers are all familiar examples.
Thus far, we have treated concurrency mainly as a mechanism that the operating system kernel uses to run multiple application programs.
It can play an important role in application programs as well.
For example, we have seen how Unix signal handlers allow applications to respond to asynchronous events such as the user typing ctrl-c or the program accessing an undeﬁned area of virtual memory.
Accessing slow I/O devices.When an application is waiting for data to arrive from a slow I/O device such as a disk, the kernel keeps the CPU busy by running other processes.
Individual applications can exploit concurrency in a similar way by overlapping useful work with I/O requests.
Interactingwith humans.Peoplewho interactwith computers demand the ability to perform multiple tasks at the same time.
For example, they might want to resize a window while they are printing a document.
Each time the user requests some action (say, by clicking the mouse), a separate concurrent logical ﬂow is created to perform the action.
Reducing latency by deferring work.Sometimes, applications can use concurrency to reduce the latency of certain operations by deferring other operations and performing them concurrently.
For example, a dynamic storage allocator might reduce the latency of individual free operations by deferring coalescing to a concurrent “coalescing” ﬂow that runs at a lower priority, soaking up spare CPU cycles as they become available.
Servicing multiple network clients.The iterative network servers that we studied in Chapter 11 are unrealistic because they can only service one client at a time.
Thus, a single slow client can deny service to every other client.
For a real server that might be expected to service hundreds or thousands of clients per second, it is not acceptable to allow one slow client to deny service to the others.Abetter approach is to build a concurrent server that creates a separate logical ﬂow for each client.
This allows the server to service multiple clients concurrently, and precludes slow clients from monopolizing the server.
Many modern systems are equipped with multi-core processors that contain multiple CPUs.
Applications that use application-level concurrency are known as concurrent programs.
Modern operating systems provide three basic approaches for building concurrent programs:
With this approach, each logical control ﬂow is a process that is scheduled andmaintained by the kernel.
Since processes have separate virtual address spaces, ﬂows thatwant to communicatewith eachothermust use some kind of explicit interprocess communication (IPC) mechanism.
Logical ﬂows are modeled as state machines that the main program explicitly transitions from state to state as a result of data arriving on ﬁle descriptors.
Since the program is a single process, all ﬂows share the same address space.
Threads.Threads are logical ﬂows that run in the context of a single process and are scheduled by the kernel.
You can think of threads as a hybrid of the other two approaches, scheduled by the kernel like process ﬂows, and sharing the same virtual address space like I/O multiplexing ﬂows.
To keep our discussion concrete, we will work with the same motivating application throughout—a concurrent version of the iterative echo server from Section 11.4.9
The simplest way to build a concurrent program is with processes, using familiar functions such as fork, exec, and waitpid.
For example, a natural approach for building a concurrent server is to accept client connection requests in the parent, and then create a new child process to service each new client.
To see how this might work, suppose we have two clients and a server that is listening for connection requests on a listening descriptor (say, 3)
After accepting the connection request, the server forks a child, which gets a complete copy of the server’s descriptor table.
This gives us the situation in Figure 12.2, where the child process is busy servicing the client.
Since the connected descriptors in the parent and child each point to the same ﬁle table entry, it is crucial for the parent to close.
Otherwise, the ﬁle table entry for connected descriptor 4 will never be released, and the resulting memory leak will eventually consume the available memory and crash the system.
At this point, the parent is waiting for the next connection request and the two children are servicing their respective clients concurrently.
Figure 12.5 shows the code for a concurrent echo server based on processes.
There are several important points to make about this server:
First, servers typically run for long periods of time, so we must include a SIGCHLD handler that reaps zombie children (lines 4–9)
Since SIGCHLD signals are blocked while the SIGCHLD handler is executing, and since Unix signals are not queued, the SIGCHLD handler must be prepared to reap multiple zombie children.
As we have mentioned, this is especially important for the parent, which must close its copy of the connected descriptor to avoid a memory leak.
Finally, because of the reference count in the socket’s ﬁle table entry, the connection to the client will not be terminated until both the parent’s and child’s copies of connfd are closed.
Processes have a clean model for sharing state information between parents and children: ﬁle tables are shared and user address spaces are not.
Having separate address spaces for processes is both an advantage and a disadvantage.
It is impossible for one process to accidentally overwrite the virtual memory of another process, which eliminates a lot of confusing failures—an obvious advantage.
On the other hand, separate address spaces make it more difﬁcult for processes to share state information.
To share information, they must use explicit IPC (interprocess communications) mechanisms.
Another disadvantage of process-based designs is that they tend to be slower because the overhead for process control and IPC is high.
You have already encountered several examples of IPC in this text.
The waitpid function and Unix signals from Chapter 8 are primitive IPC mechanisms that allow processes to send tiny messages to processes running on the same host.
The sockets interface from Chapter 11 is an important form of IPC that allows processes on different hosts to exchange arbitrary byte streams.
However, the term Unix IPC is typically reserved for a hodge-podge of techniques that allow processes to communicate with other processes that are running on the same host.
The parent forks a child to handle each new connection request.
Suppose you are asked to write an echo server that can also respond to interactive commands that the user types to standard input.
If we are waiting for a connection request in accept, then we cannot respond to input commands.
Similarly, if we are waiting for an input command in read, thenwe cannot respond to any connection requests.
One solution to this dilemma is a technique called I/Omultiplexing.
The basic idea is to use the select function to ask the kernel to suspend the process, returning control to the application only after one or more I/O events have occurred, as in the following examples:
Timeout if 152.13 seconds have elapsed waiting for an I/O event to occur.
Select is a complicated function with many different usage scenarios.
We will only discuss the ﬁrst scenario: waiting for a set of descriptors to be ready for reading.
The select function manipulates sets of type fd_set, which are known as descriptor sets.
Logically, we think of a descriptor set as a bit vector (introduced in Section 2.1) of size n:
For our purposes, the select function takes two inputs: a descriptor set (fdset) called the read set, and the cardinality (n) of the read set (actually the maximum cardinality of any descriptor set)
The select function blocks until at least one descriptor in the read set is ready for reading.
A descriptor k is ready for reading if and only if a request to read 1 byte from that descriptor would not block.
As a side effect, selectmodiﬁes the fd_set pointed to by argument fdset to indicate a subset of the read set called the ready set, consisting of the descriptors in the read set that are ready for reading.
The value returned by the function indicates the cardinality of the ready set.
Note that because of the side effect, we must update the read set every time select is called.
The best way to understand select is to study a concrete example.
Figure 12.6 shows how we might use select to implement an iterative echo server that also accepts user commands on the standard input.
But instead of waiting for a connection request by calling the accept function, we call the select function, which blocks until either the listening descriptor or standard input is ready for reading (line 25)
For example, here is the value of ready_set that select would return if the user hit the enter key, thus causing the standard input descriptor to become ready for reading:
Figure 12.6 An iterative echo server that uses I/O multiplexing.
The server uses select to wait for connection requests on a listening descriptor and commands on standard input.
Once select returns, we use the FD_ISSET macro to determine which descriptors are ready for reading.
If standard input is ready (line 26), we call the command function, which reads, parses, and responds to the command before returning to the main routine.
While this program is a good example of using select, it still leaves something to be desired.
The problem is that once it connects to a client, it continues echoing input lines until the client closes its end of the connection.
Thus, if you type a command to standard input, you will not get a response until the server is ﬁnished with the client.
A better approach would be to multiplex at a ﬁner granularity, echoing (at most) one text line each time through the server loop.
Practice Problem 12.3 In most Unix systems, typing ctrl-d indicates EOF on standard input.
What happens if you type ctrl-d to the program in Figure 12.6 while it is blocked in the call to select?
I/O multiplexing can be used as the basis for concurrent event-driven programs, where ﬂows make progress as a result of certain events.
The general idea is to model logical ﬂows as state machines.
Informally, a state machine is a collection of states, input events, and transitions that map states and input events to states.
Each transition maps an (input state, input event) pair to an output state.
A self-loop is a transition between the same input and output state.
State machines are typically drawn as directed graphs, where nodes represent states, directed arcs represent transitions, andarc labels represent input events.A statemachinebegins execution in some initial state.
Each input event triggers a transition from the current state to the next state.
For each new client k, a concurrent server based on I/O multiplexing creates a new state machine sk and associates it with connected descriptor dk.
As shown in Figure 12.7, each state machine sk has one state (“waiting for descriptor dk to be ready for reading”), one input event (“descriptor dk is ready for reading”), and one transition (“read a text line from descriptor dk”)
The server uses the I/O multiplexing, courtesy of the select function, to detect the occurrence of input events.
As each connected descriptor becomes ready for reading, the server executes the transition for the corresponding state machine, in this case reading and echoing a text line from the descriptor.
Figure 12.8 shows the complete example code for a concurrent event-driven server based on I/O multiplexing.
The set of active clients is maintained in a pool structure (lines 3–11)
Figure 12.7 State machine for a logical ﬂow in a concurrent event-driven echo server.
We then add the connected descriptor to the select read set (line 12), and we update some global properties of the pool.
The maxfd variable (lines 15–16) keeps track of the largest ﬁle descriptor for select.
If we are successful in reading a text line from the descriptor, then we echo that line back to the client (lines 15–18)
Notice that in line 15 we are maintaining a cumulative count of total bytes received from all clients.
The check_clients function performs state transitions by echoing input lines, and it also deletes the state machine when the client has ﬁnished sending text lines.
If listening descriptor ready, add new client to pool */
Echo a text line from each ready connected descriptor */
Each server iteration echoes a text line from each ready descriptor.
Initially, listenfd is only member of select read set */
If the descriptor is ready, echo a text line from it */
For example, we can imagine writing an event-driven concurrent server that gives preferred service to some clients, which would be difﬁcult for a concurrent server based on processes.
Another advantage is that an event-driven server based on I/O multiplexing runs in the context of a single process, and thus every logical ﬂow has access to the entire address space of the process.
A related advantage of running as a single process is that you can debug your concurrent server as you would any sequential program, using a familiar debugging tool such as gdb.
Finally, event-driven designs are often signiﬁcantly more efﬁcient than process-based designs because they do not require a process context switch to schedule a new ﬂow.
Our event-driven concurrent echo server requires three times more code than the process-based server.
Unfortunately, the complexity increases as the granularity of the concurrency decreases.
By granularity, we mean the number of instructions that each logical ﬂow executes per time slice.
For instance, in our example concurrent server, the granularity of concurrency is the number of instructions required to read an entire text line.
As long as some logical ﬂow is busy reading a text line, no other logical ﬂow can make progress.
This is ﬁne for our example, but it makes our event-driver server vulnerable to a malicious client that sends only a partial text line and then halts.
Modifying an event-driven server to handle partial text lines is a nontrivial task, but it is handled cleanly and automatically by a processbased design.
Another signiﬁcant disadvantage of event-based designs is that they cannot fully utilize multi-core processors.
To this point, we have looked at two approaches for creating concurrent logical ﬂows.
With the ﬁrst approach, we use a separate process for each ﬂow.
Each process has its own private address space, which makes it difﬁcult for ﬂows to share data.
With the second approach, we create our own logical ﬂows and use I/O multiplexing to explicitly schedule the ﬂows.
Because there is only one process, ﬂows share the entire address space.
This section introduces a third approach—based on threads—that is a hybrid of these two.
A thread is a logical ﬂow that runs in the context of a process.
Thus far in this book, our programs have consisted of a single thread per process.
But modern systems also allowus towrite programs that havemultiple threads running concurrently in a single process.
Each thread has its own thread context, including a unique integer thread ID (TID), stack, stack pointer, program counter, general-purpose registers, and condition codes.
All threads running in a process share the entire virtual address space of that process.
Logical ﬂows based on threads combine qualities of ﬂows based on processes and I/O multiplexing.
Like processes, threads are scheduled automatically by the kernel and are known to the kernel by an integer ID.
The execution model for multiple threads is similar in some ways to the execution model for multiple processes.
Each process begins life as a single thread called themain thread.At somepoint, themain thread creates a peer thread, and from this point in time the two threads run concurrently.
Eventually, control passes to the peer thread via a context switch, because the main thread executes a slow system call such as read or sleep, or because it is interrupted by the system’s interval timer.
The peer thread executes for a while before control passes back to the main thread, and so on.
Because a thread context is much smaller than a process context, a thread context switch is faster than a process context switch.Another difference is that threads, unlike processes, are not organized in a rigid parent-child hierarchy.
The threads associated with a process form a pool of peers, independent of which threads were created by which other threads.
The main thread is distinguished from other threads only in the sense that it is always the ﬁrst thread to run in the process.
The main impact of this notion of a pool of peers is that a thread can kill any of its peers, or wait for any of its peers to terminate.
Further, each peer can read and write the same shared data.
Posix threads (Pthreads) is a standard interface for manipulating threads from C programs.
It was adopted in 1995 and is available on most Unix systems.
Pthreads deﬁnes about 60 functions that allow programs to create, kill, and reap threads, to share data safely with peer threads, and to notify peers about changes in the system state.
The main thread creates a peer thread and then waits for it to terminate.
When the main thread detects that the peer thread has terminated, it terminates the process by calling exit.
This is the ﬁrst threaded program we have seen, so let us dissect it carefully.
The code and local data for a thread is encapsulated in a thread routine.
As shown by the prototype in line 2, each thread routine takes as input a single generic pointer and returns a generic pointer.
If you want to pass multiple arguments to a thread routine, then you should put the arguments into a structure and pass a pointer to the structure.
Similarly, if youwant the thread routine to returnmultiple arguments, you can return a pointer to a structure.
Line 4 marks the beginning of the code for the main thread.
The main thread declares a single local variable tid, which will be used to store the thread ID of the peer thread (line 6)
When the call to pthread_create returns, the main thread and the newly created peer thread are running concurrently, and tid contains the ID of the new thread.
Finally, the main thread calls exit (line 9), which terminates all threads (in this case just the main thread) currently running in the process.
Lines 12–16 deﬁne the thread routine for the peer thread.
The pthread_create function creates a new thread and runs the thread routine f in the context of the new thread and with an input argument of arg.
The attr argument can be used to change the default attributes of the newly created thread.
Changing these attributes is beyond our scope, and in our examples, we will always call pthread_create with a NULL attr argument.
When pthread_create returns, argument tid contains the ID of the newly created thread.
The new thread can determine its own thread ID by calling the pthread_self function.
The thread terminates implicitly when its top-level thread routine returns.
Some peer thread calls the Unix exit function, which terminates the process and all threads associated with the process.
Another peer thread terminates the current thread by calling the pthread_ cancel function with the ID of the current thread.
Threads wait for other threads to terminate by calling the pthread_join function.
Notice that, unlike the Unix wait function, the pthread_join function can only wait for a speciﬁc thread to terminate.
There is no way to instruct pthread_ wait to wait for an arbitrary thread to terminate.
This can complicate our code by forcing us to use other, less intuitive mechanisms to detect process termination.
Indeed, Stevens argues convincingly that this is a bug in the speciﬁcation [109]
At any point in time, a thread is joinable or detached.
A joinable thread can be reaped and killed by other threads.
Its memory resources (such as the stack) are not freed until it is reaped by another thread.
In contrast, a detached thread cannot be reaped or killed by other threads.
Itsmemory resources are freed automatically by the system when it terminates.
In order to avoidmemory leaks, each joinable thread should either be explicitly reaped by another thread, or detached by a call to the pthread_detach function.
Although some of our examples will use joinable threads, there are good reasons to use detached threads in real programs.
For example, a high-performance Web server might create a new peer thread each time it receives a connection request from a Web browser.
Since each connection is handled independently by a separate thread, it is unnecessary—and indeed undesirable—for the server to explicitly wait for each peer thread to terminate.
In this case, each peer thread should detach itself before it begins processing the request so that its memory resources can be reclaimed after it terminates.
The pthread_once function allows you to initialize the state associated with a thread routine.
The pthread_once function is useful whenever you need to dynamically initialize global variables that are shared by multiple threads.
Figure 12.14 shows the code for a concurrent echo server based on threads.
The main thread repeatedly waits for a connection request and then creates a peer thread to handle the request.
While the code looks simple, there are a couple of general and somewhat subtle issues we need to look at more closely.
The ﬁrst issue is how to pass the connected descriptor to the peer thread when we call pthread_create.
The obvious approach is to pass a pointer to the descriptor, as in the following:
Then we have the peer thread dereference the pointer and assign it to a local variable, as follows:
This would be wrong, however, because it introduces a race between the assignment statement in the peer thread and the accept statement in the main thread.
If the assignment statement completes before the next accept, then the local connfd variable in the peer thread gets the correct descriptor value.
However, if the assignment completes after the accept, then the local connfd variable in the peer thread gets the descriptor number of the next connection.
The unhappy result is that two threads are now performing input and output on the same descriptor.
In order to avoid the potentially deadly race, we must assign each connected descriptor returned by accept to its own dynamically allocated memory block, as shown in lines 21–22
We will return to the issue of races in Section 12.7.4
Another issue is avoiding memory leaks in the thread routine.
Since we are not explicitly reaping threads, we must detach each thread so that its memory resources will be reclaimed when it terminates (line 31)
Further, we must be careful to free the memory block that was allocated by the main thread (line 32)
However, in the threadsbased server in Figure 12.14, we only closed the connected descriptor in one place: the peer thread.
From a programmer’s perspective, one of the attractive aspects of threads is the ease with whichmultiple threads can share the same program variables.
In order to write correctly threaded programs, we must have a clear understanding of what we mean by sharing and how it works.
To keep our discussion of sharing concrete, we will use the program in Figure 12.15 as a running example.
Although somewhat contrived, it is nonetheless useful to study because it illustrates a number of subtle points about sharing.
The example program consists of a main thread that creates two peer threads.
Figure 12.15 Example program that illustrates different aspects of sharing.
A pool of concurrent threads runs in the context of a process.
Each thread has its own separate thread context, which includes a thread ID, stack, stack pointer, program counter, condition codes, and general-purpose register values.
Each thread shares the rest of the process context with the other threads.
This includes the entire user virtual address space, which consists of read-only text (code), read/write data, the heap, and any shared library code and data areas.
The threads also share the same set of open ﬁles.
In an operational sense, it is impossible for one thread to read or write the register values of another thread.
On the other hand, any thread can access any location in the shared virtual memory.
If some threadmodiﬁes amemory location, then every other thread will eventually see the change if it reads that location.
Thus, registers are never shared, whereas virtual memory is always shared.
The memory model for the separate thread stacks is not as clean.
These stacks are contained in the stack area of the virtual address space, and are usually accessed independently by their respective threads.
We say usually rather than always, because different thread stacks are not protected from other threads.
So if a thread somehow manages to acquire a pointer to another thread’s stack, then it can read and write any part of that stack.
Our example program shows this in line 26, where the peer threads reference the contents of the main thread’s stack indirectly through the global ptr variable.
Variables in threaded C programs are mapped to virtual memory according to their storage classes:
Global variables.A global variable is any variable declared outside of a function.
At run time, the read/write area of virtual memory contains exactly one instance of each global variable that can be referenced by any thread.
For example, the global ptr variable declared in line 5 has one run-time instance in the read/write area of virtual memory.
When there is only one instance of a variable, we will denote the instance by simply using the variable name—in this case, ptr.
Local automatic variables.A local automatic variable is one that is declared inside a function without the static attribute.
At run time, each thread’s stack contains its own instances of any local automatic variables.
This is true even if multiple threads execute the same thread routine.
For example, there is one instance of the local variable tid, and it resides on the stack of themain thread.
A local static variable is one that is declared inside a function with the static attribute.
As with global variables, the read/write area of virtual memory contains exactly one instance of each local static variable declared in a program.
For example, even though each peer thread in our example program declares cnt in line 25, at run time there is only one instance of cnt residing in the read/write area of virtual memory.
We say that a variable v is shared if and only if one of its instances is referenced by more than one thread.
On the other hand, myid is not shared because each of its two instances is referenced by exactly one thread.
However, it is important to realize that local automatic variables such as msgs can also be shared.
Using the analysis from Section 12.4, ﬁll each entry in the following table.
Given the analysis in Part A, which of the variables ptr, cnt, i, msgs, and myid are shared?
We will ﬁnd it helpful to partition the loop code for thread i into ﬁve parts:
Hi: The block of instructions at the head of the loop.
Li: The instruction that loads the shared variable cnt into register %eaxi, where %eaxi denotes the value of register %eax in thread i.
Si: The instruction that stores the updated value of %eaxi back to the shared variable cnt.
Ti: The block of instructions at the tail of the loop.
Notice that the head and tail manipulate only local stack variables, while Li, Ui, and Si manipulate the contents of the shared counter variable.
When the two peer threads in badcnt.c run concurrently on a uniprocessor, the machine instructions are completed one after the other in some order.
Thus, each concurrent execution deﬁnes some total ordering (or interleaving) of the instructions in the two threads.
Unfortunately, some of these orderings will produce correct results, but others will not.
Here is the crucial point: In general, there is no way for you to predict whether the operating system will choose a correct ordering for your threads.For example, Figure 12.18(a) shows the step-by-step operation of a correct instruction ordering.
After each thread has updated the shared variable cnt, its value in memory is 2, which is the expected result.
On the other hand, the ordering in Figure 12.18(b) produces an incorrect value for cnt.
We can clarify these notions of correct and incorrect instruction orderings with the help of a device known as a progress graph, which we introduce in the next section.
Figure 12.18 Instruction orderings for the ﬁrst loop iteration in badcnt.c.
Practice Problem 12.7 Complete the table for the following instruction ordering of badcnt.c:
Does this ordering result in a correct value for cnt?
The origin of the graph corresponds to the initial state where none of the threads has yet completed an instruction.
Figure 12.19 shows the two-dimensional progress graph for the ﬁrst loop iteration of the badcnt.c program.
Figure 12.19 Progress graph for the ﬁrst loop iteration of badcnt.c.
A progress graph models instruction execution as a transition from one state to another.
A transition is represented as a directed edge from one point to an adjacent point.
Two instructions cannot complete at the same time—diagonal transitions are not allowed.
Programs never run backwards, so transitions that move down or to the left are not legal either.
The execution history of a program is modeled as a trajectory through the state space.
Figure 12.20 shows the trajectory that corresponds to the following instruction ordering:
The intersection of the critical regions forms an unsafe region.
Trajectories that skirt the unsafe region correctly update the counter variable.
In other words, we want to ensure that each thread has mutually exclusive access to the shared variable while it is executing the instructions in its critical section.
On the progress graph, the intersection of the two critical sections deﬁnes a region of the state space known as an unsafe region.
Figure 12.21 shows the unsafe region for the variable cnt.
Notice that the unsafe region abuts, but does not include, the states along its perimeter.
A trajectory that skirts the unsafe region is known as a safe trajectory.
Conversely, a trajectory that touches any part of the unsafe region is an unsafe trajectory.
Figure 12.21 shows examples of safe and unsafe trajectories through the state space of our example badcnt.c program.
The upper trajectory skirts the unsafe region along its left and top sides, and thus is safe.
The lower trajectory crosses the unsafe region, and thus is unsafe.
In order to guarantee correct execution of our example threaded program—and indeed any concurrent program that shares global data structures—we must somehow synchronize the threads so that they always have a safe trajectory.
A classic approach is based on the idea of a semaphore, which we introduce next.
EdsgerDijkstra, a pioneer of concurrent programming, proposed a classic solution to the problem of synchronizing different execution threads based on a special type of variable called a semaphore.
A semaphore, s, is a global variable with a nonnegative integer value that can only bemanipulated by two special operations, called P and V :
P(s): If s is nonzero, thenP decrements s and returns immediately.
If s is zero, then suspend the thread until s becomes nonzero and the process is restarted by a V operation.
After restarting, the P operation decrements s and returns control to the caller.
If there are any threads blocked at a P operation waiting for s to become nonzero, then the V operation restarts exactly one of these threads, which then completes its P operation by decrementing s.
The test and decrement operations in P occur indivisibly, in the sense that once the semaphore s becomes nonzero, the decrement of s occurs without interruption.
The increment operation in V also occurs indivisibly, in that it loads, increments, and stores the semaphore without interruption.
Notice that the deﬁnition of V does not deﬁne the order in which waiting threads are restarted.
The only requirement is that the V must restart exactly one waiting thread.Thus, when several threads are waiting at a semaphore, you cannot predict which one will be restarted as a result of the V.
The deﬁnitions of P and V ensure that a running program can never enter a state where a properly initialized semaphore has a negative value.
This property, known as the semaphore invariant, provides a powerful tool for controlling the trajectories of concurrent programs, as we shall see in the next section.
The Posix standard deﬁnes a variety of functions for manipulating semaphores.
Each semaphore must be initialized before it can be used.
For conciseness, we prefer to use the following equivalent P and V wrapper functions instead:
The names P and V come from the Dutch words Proberen (to test) and Verhogen (to increment)
Semaphores provide a convenient way to ensure mutually exclusive access to shared variables.
The basic idea is to associate a semaphore s, initially 1, with each shared variable (or related set of shared variables) and then surround the corresponding critical section with P(s) and V (s) operations.
Binary semaphores whose purpose is to provide mutual exclusion are often called mutexes.
Performing a P operation on a mutex is called locking the mutex.
Similarly, performing the V operation is called unlocking the mutex.
A thread that has locked but not yet unlocked a mutex is said to be holding the mutex.
A semaphore that is used as a counter for a set of available resources is called a counting semaphore.
The progress graph in Figure 12.22 shows how we would use binary semaphores to properly synchronize our example counter program.
Each state is labeled with the value of semaphore s in that state.
And since the forbidden region completely encloses the unsafe region, no feasible trajectory can touch any part of the unsafe region.
Thus, every feasible trajectory is safe, and regardless of the ordering of the instructions at run time, the program correctly increments the counter.
In an operational sense, the forbidden region created by the P and V operations makes it impossible for multiple threads to be executing instructions in the enclosed critical region at any point in time.
In other words, the semaphore operations ensure mutually exclusive access to the critical region.
Putting it all together, to properly synchronize the example counter program in Figure 12.16 using semaphores, we ﬁrst declare a semaphore called mutex:
Finally, we protect the update of the shared cnt variable in the thread routine by surrounding it with P and V operations:
Whenwe run the properly synchronized program, it now produces the correct answer each time.
Progress graphs give us a nice way to visualize concurrent program execution on uniprocessors and to understand why we need synchronization.
However, they do have limitations, particularly with respect to concurrent execution on multiprocessors, where a set of CPU/cache pairs share the same main memory.
Multiprocessors behave in ways that cannot be explained by progress graphs.
Another important use of semaphores, besides providing mutual exclusion, is to schedule accesses to shared resources.
In this scenario, a thread uses a semaphore operation to notify another thread that some condition in the program state has become true.
Two classical and useful examples are the producer-consumer and readers-writers problems.
A producer and consumer thread share a bounded buffer with n slots.
The producer thread repeatedly produces new items and inserts them in the buffer.
The consumer thread repeatedly removes items from the buffer and then consumes (uses) them.
Since inserting and removing items involves updating shared variables, we must guarantee mutually exclusive access to the buffer.
If the buffer is full (there are no empty slots), then the producer must wait until a slot becomes available.
Similarly, if the buffer is empty (there are no available items), then the consumer must wait until an item becomes available.
For example, in a multimedia system, the producer might encode video frames while the consumer decodes and renders them on the screen.
The purpose of the buffer is to reduce jitter in the video stream caused by data-dependent differences in the encoding anddecoding times for individual frames.
Thebuffer provides a reservoir of slots to the producer and a reservoir of encoded frames to the consumer.
Another common example is the design of graphical user interfaces.
The producer generates items and inserts them into a bounded buffer.
The consumer removes items from the buffer and then consumes them.
The consumer removes the events from the buffer in some priority-based manner and paints the screen.
In this section, we will develop a simple package, called Sbuf, for building producer-consumer programs.
In the next section, we look at how to use it to build an interesting concurrent server based on prethreading.
Items are stored in a dynamically allocated integer array (buf) with n items.
The front and rear indices keep track of the ﬁrst and last items in the array.
Semaphores slots and items are counting semaphores that count the number of empty slots and available items, respectively.
The sbuf_init function allocates heap memory for the buffer, sets front and rear to indicate an empty buffer, and assigns initial values to the three semaphores.
This function is called once, before calls to any of the other three functions.
The sbuf_deinit function frees the buffer storage when the application is through using it.
The sbuf_insert function waits for an available slot, locks the mutex, adds the item, unlocks the mutex, and then announces the availability of a new item.
After waiting for an available buffer item, it locks the mutex, removes the item from the front of the buffer, unlocks the mutex, and then signals the availability of a new slot.
Practice Problem 12.9 Let p denote the number of producers, c the number of consumers, and n the buffer size in units of items.
Create an empty, bounded, shared FIFO buffer with n slots */
Insert item onto the rear of shared buffer sp */
Remove and return the first item from buffer sp */
Figure 12.25 Sbuf: A package for synchronizing concurrent access to bounded buffers.
Some threads only read the object, while others modify it.
Writers must have exclusive access to the object, but readers may share the object with an unlimited number of other readers.
In general, there are an unbounded number of concurrent readers and writers.
For example, in an online airline reservation system, an unlimited number of customers are allowed to concurrently inspect the seat assignments, but a customer who is booking a seat must have exclusive access to the database.
As another example, in a multithreaded caching Web proxy, an unlimited number of threads can fetch existing pages from the shared page cache, but any thread that writes a new page to the cache must have exclusive access.
The readers-writers problem has several variations, each based on the priorities of readers and writers.
The ﬁrst readers-writers problem, which favors readers, requires that no reader be kept waiting unless a writer has already been granted permission to use the object.
In other words, no reader should wait simply because a writer is waiting.
The second readers-writers problem, which favors writers, requires that once a writer is ready to write, it performs its write as soon as possible.
Unlike the ﬁrst problem, a reader that arrives after a writer must wait, even if the writer is also waiting.
Figure 12.26 shows a solution to the ﬁrst readers-writers problem.
Like the solutions to many synchronization problems, it is subtle and deceptively simple.
The w semaphore controls access to the critical sections that access the shared object.
The mutex semaphore protects access to the shared readcnt variable, which counts the number of readers currently in the critical section.
A writer locks the wmutex each time it enters the critical section, and unlocks it each time it leaves.
This guarantees that there is at most one writer in the critical section at any point in time.
On the other hand, only the ﬁrst reader to enter the critical section locks w, and only the last reader to leave the critical section unlocks it.
The wmutex is ignored by readers who enter and leave while other readers are present.
This means that as long as a single reader holds the w mutex, an unbounded number of readers can enter the critical section unimpeded.
A correct solution to either of the readers-writers problems can result in starvation, where a thread blocks indeﬁnitely and fails to make progress.
For example, in the solution in Figure 12.26, a writer could wait indeﬁnitely while a stream of readers arrived.
Describe a scenario where this weak priority would allow a collection of writers to starve a reader.
For example, Java threads are synchronized with amechanism called a Javamonitor [51], which provides a higher level abstraction of the mutual exclusion and scheduling capabilities of semaphores; in fact monitors can be implemented with semaphores.
As another example, the Pthreads interface deﬁnes a set of synchronization operations on mutex and condition variables.
Condition variables are used for scheduling accesses to shared resources, such as the bounded buffer in a producer-consumer program.
We have seen how semaphores can be used to access shared variables and to schedule accesses to shared resources.
To help you understand these ideas more clearly, let us apply them to a concurrent server based on a technique called prethreading.
A set of existing threads repeatedly remove and process connected descriptors from a bounded buffer.
In the concurrent server in Figure 12.14, we created a new thread for each new client.
A disadvantage of this approach is that we incur the nontrivial cost of creating a new thread for each new client.
A server based on prethreading tries to reduce this overhead by using the producer-consumer model shown in Figure 12.27
The server consists of a main thread and a set of worker threads.
The main thread repeatedly accepts connection requests from clients and places the resulting connected descriptors in a bounded buffer.
Each worker thread repeatedly removes a descriptor from the buffer, services the client, and thenwaits for the next descriptor.
Figure 12.28 shows how we would use the Sbuf package to implement a prethreaded concurrent echo server.
Then it enters the inﬁnite server loop, accepting connection requests and inserting the resulting connected descriptors in sbuf.
This is interesting code to study because it shows you a general technique for initializing packages that are called from thread routines.
In our case, we need to initialize the byte_cnt counter and the mutex semaphore.
One approach, which we used for the Sbuf and Rio packages, is to require the main thread to explicitly call an initialization function.
The advantage of this approach is that it makes the package easier to use.
The server uses a producerconsumer model with one producer and multiple consumers.
I/O multiplexing is not the only way to write an event-driven program.
For example, you might have noticed that the concurrent prethreaded server that we just developed is really an event-driven server with simple state machines for the main and worker threads.
The main thread has two states (“waiting for connection request” and “waiting for available buffer slot”), two I/O events (“connection request arrives” and “buffer slot becomes available”), and two transitions (“accept connection request” and “insert buffer item”)
Similarly, each worker thread has one state (“waiting for available buffer item”), one I/O event (“buffer item becomes available”), and one transition (“remove buffer item”)
Thus far in our study of concurrency, we have assumed concurrent threads executing on uniprocessor systems.
Concurrent programs often run faster on such machines because the operating system kernel schedules the concurrent threads in parallel on multiple cores, rather than sequentially on a single core.
Exploiting such parallelism is critically important in applications such as busy Web servers, database servers, and large scientiﬁc codes, and it is becoming increasingly useful in mainstream applications such as Web browsers, spreadsheets, and document processors.
Figure 12.30 shows the set relationships between sequential, concurrent, and parallel programs.
The set of all programs can be partitioned into the disjoint sets of sequential and concurrent programs.
A sequential program is written as a single logical ﬂow.
A parallel program is a concurrent program running onmultiple processors.
Thus, the set of parallel programs is a proper subset of the set of concurrent programs.
The most straightforward approach is to partition the sequence into t disjoint regions, and then assign each of t different threads to work on its own region.
For simplicity, assume that n is a multiple of t , such that each region has n/t elements.
The main thread creates t peer threads, where each peer thread k runs in parallel on its own processor core and computes sk, which is the sum of the elements in region k.
Once the peer threads have completed, the main thread computes the ﬁnal result by summing each sk.
Figure 12.31 shows how we might implement this simple parallel sum algorithm.
In lines 27–32, the main thread creates the peer threads and then waits for them to terminate.
Notice that the main thread passes a small integer to each peer thread that serves as a unique thread ID.
Each peer thread will use its thread ID to determine which portion of the sequence it should work on.
This idea of passing a small unique thread ID to the peer threads is a general technique that is used in many parallel applications.
After the peer threads have terminated, the psum vector contains the partial sums computed by each peer thread.
Figure 12.30 Relationships between the sets of sequential, concurrent, and parallel programs.
Create peer threads and wait for them to finish */
Add up the partial sums computed by each thread */
Figure 12.31 Simple parallel program that uses multiple threads to sum the elements of a sequence.
Figure 12.32 shows the function that each peer thread executes.
Notice that we are careful to give each peer thread a unique memory location to update, and thus it is not necessary to synchronize access to the psum array with semaphoremutexes.
The only necessary synchronization in this particular case is that the main thread must wait for each of the children to ﬁnish so that it knows that each entry in psum is valid.
In each case, the program runs on a system with four processor cores and sums a sequence of n = 231 elements.
We see that running time decreases as we increase the number of threads, up to four threads, at which point it levels off and even starts to increase a little.
In the ideal case, we would expect the running time to decrease linearly with the number of cores.
That is, we would expect running time to drop by half each time we double the number of threads.
Running time actually increases a bit as we increase the number of threads because of the overhead of context switching multiple threads on the same core.
For this reason, parallel programs are often written so that each core runs exactly one thread.
Although absolute running time is the ultimate measure of any program’s performance, there are some useful relative measures, known as speedup and efﬁciency, that can provide insight into how well a parallel program is exploiting.
The speedup of a parallel program is typically deﬁned as.
When T1 is the execution time of a sequential version of the program, then Sp is called the absolute speedup.
When T1 is the execution time of the parallel version of the program running on one core, then Sp is called the relative speedup.
Absolute speedup is a truer measure of the beneﬁts of parallelism than relative speedup.
Parallel programs often suffer from synchronization overheads, even when they run on one processor, and these overheads can artiﬁcially inﬂate the relative speedup numbers because they increase the size of the numerator.
On the other hand, absolute speedup is more difﬁcult to measure than relative speedup because measuring absolute speedup requires two different versions of the program.
For complex parallel codes, creating a separate sequential version might not be feasible, either because the code is too complex or the source code is not available.
Efﬁciency is a measure of the overhead due to parallelization.
Programs with high efﬁciency are spending more time doing useful work and less time synchronizing and communicating than programs with low efﬁciency.
Figure 12.34 shows the different speedup and efﬁciency measures for our example parallel sum program.
Efﬁciencies over 90% such as these are very good, but do not be fooled.Wewere able to achieve high efﬁciency because our problem was trivially easy to parallelize.
Parallel programming has been an active area of research for decades.
With the advent of commodity multi-core machines whose core count is doubling every few years, parallel programming continues to be a deep, difﬁcult, and active area of research.
There is another view of speedup, known as weak scaling, which increases the problem size along with the number of processors, such that the amount of work performed on each processor is held constant as the number of processors increases.
With this formulation, speedup and efﬁciency are expressed in terms of the total amount of work accomplished per unit time.
For example, if we can double the number of processors and do twice the amount of work per hour, then we are enjoying linear speedup and 100% efﬁciency.
Weak scaling is often a truer measure than strong scaling because it more accurately reﬂects our desire to use bigger machines to do more work.
This is particularly true for scientiﬁc codes, where the problem size can be easily increased, and where bigger problem sizes translate directly to better predictions of nature.
However, there exist applications whose sizes are not so easily increased, and for these applications strong scaling is more appropriate.
For example, the amount of work performed by real-time signal processing applications is often determined by the properties of the physical sensors that are generating the signals.
Changing the total amount of work requires using different physical sensors, whichmight not be feasible or necessary.
For these applications, we typically want to use parallelism to accomplish a ﬁxed amount of work as quickly as possible.
Practice Problem 12.11 Fill in the blanks for the parallel program in the following table.
You probably noticed that life got much more complicated once we were asked to synchronize accesses to shared data.
So far, we have looked at techniques for mutual exclusion and producer-consumer synchronization, but this is only the tip of the iceberg.
Synchronization is a fundamentally difﬁcult problem that raises issues that simply do not arise in ordinary sequential programs.
This section is a survey (by no means complete) of some of the issues you need to be aware of when you write concurrent programs.
To keep things concrete, we will couch our discussion in terms of threads.
Keep inmind, however, that these are typical of the issues that arise when concurrent ﬂows of any kind manipulate shared resources.
When we program with threads, we must be careful to write functions that have a property called thread safety.A function is said to be thread-safe if andonly if it will always produce correct results when called repeatedly from multiple concurrent threads.
If a function is not thread-safe, then we say it is thread-unsafe.
We have already encountered this problem with the thread function in Figure 12.16, which increments an unprotected global counter variable.
This class of thread-unsafe function is relatively easy to make thread-safe: protect the shared variables with synchronization operations such as P and V.
An advantage is that it does not require any changes in the calling program.
A disadvantage is that the synchronization operations will slow down the function.
A pseudorandom number generator is a simple example of this class of thread-unsafe function.
The rand function is thread-unsafe because the result of the current invocation depends on an intermediate result from the previous iteration.
When we call rand repeatedly from a single thread after seeding it with a call to srand, we can expect a repeatable sequence of numbers.
However, this assumption no longer holds if multiple threads are calling rand.
The only way to make a function such as rand thread-safe is to rewrite it so that it does not use any static data, relying instead on the caller to pass the state information in arguments.
The disadvantage is that the programmer is now forced to change the code in the calling routine as well.
In a large program where there are potentially hundreds of different call sites, making such modiﬁcations could be nontrivial and prone to error.
Class 3: Functions that return a pointer to a static variable.Some functions, such as ctime and gethostbyname, compute a result in a static variable and then return a pointer to that variable.
If we call such functions from concurrent threads, then disaster is likely, as results being used by one thread are silently overwritten by another thread.
There are two ways to deal with this class of thread-unsafe functions.
One option is to rewrite the function so that the caller passes the address of the variable in which to store the results.
This eliminates all shared data, but it requires the programmer to have access to the function source code.
If the thread-unsafe function is difﬁcult or impossible to modify (e.g., the code is very complex or there is no source code available), then another option is to use the lock-and-copy technique.
The basic idea is to associate a mutex with the thread-unsafe function.
At each call site, lock the mutex, call the thread-unsafe function, copy the result returned by the function to a private memory location, and then unlock the mutex.
To minimize changes to the caller, you should deﬁne a thread-safe wrapper function that performs the lock-and-copy, and then replace all calls to the thread-unsafe function with calls to the wrapper.
For example, Figure 12.36 shows a thread-safe wrapper for ctime that uses the lock-and-copy technique.
If a function f calls a thread-unsafe function g, is f thread-unsafe? It depends.
If g is a class 2 function that relies on state across multiple invocations, then f is also threadunsafe and there is no recourse short of rewriting g.
We see a good example of this in Figure 12.36, where we use lock-and-copy to write a thread-safe function that calls a thread-unsafe function.
There is an important class of thread-safe functions, known as reentrant functions, that are characterized by the property that they do not reference any shared data.
Figure 12.36 Thread-safe wrapper function for the C standard library ctime function.
Uses the lock-and-copy technique to call a class 3 thread-unsafe function.
Figure 12.37 Relationships between the sets of reentrant, thread-safe, and nonthread-safe functions.
Although the terms thread-safe and reentrant are sometimes used (incorrectly) as synonyms, there is a clear technical distinction that is worth preserving.
Figure 12.37 shows the set relationships between reentrant, thread-safe, and thread-unsafe functions.
The set of all functions is partitioned into the disjoint sets of thread-safe and thread-unsafe functions.
The set of reentrant functions is a proper subset of the thread-safe functions.
Reentrant functions are typically more efﬁcient than nonreentrant threadsafe functions because they require no synchronization operations.
Furthermore, the only way to convert a class 2 thread-unsafe function into a thread-safe one is to rewrite it so that it is reentrant.
The key idea is that we have replaced the static next variable with a pointer that is passed in by the caller.
If all function arguments are passed by value (i.e., no pointers) and all data references are to local automatic stack variables (i.e., no references to static or global variables), then the function is explicitly reentrant, in the sense that we can assert its reentrancy regardless of how it is called.
However, if we loosen our assumptions a bit and allow some parameters in our otherwise explicitly reentrant function to be passed by reference (that is, we allow them to pass pointers) then we have an implicitly reentrant function, in the sense that it is only reentrant if the calling threads are careful to pass pointers.
We always use the term reentrant to include both explicit and implicit reentrant functions.
However, it is important to realize that reentrancy is sometimes a property of both the caller and the callee, and not just the callee alone.
Most Unix functions, including the functions deﬁned in the standard C library (such as malloc, free, realloc, printf, and scanf), are thread-safe, with only a few exceptions.
The asctime, ctime, and localtime functions are popular functions for converting back and forth between different time and date formats.
The strtok function is a deprecated function (one whose use is discouraged) for parsing strings.
With the exceptions of rand and strtok, all of these thread-unsafe functions are of the class 3 variety that return a pointer to a static variable.
If we need to call one of these functions in a threaded program, the least disruptive approach to the caller is to lock-and-copy.
Second, functions such as gethostbyname that return pointers to complex structures of structures require a deep copy of the structures in order to copy the entire structure hierarchy.
Third, the lock-and-copy approach will not work for a class 2 thread-unsafe function such as rand that relies on static state across calls.
Therefore, Unix systems provide reentrant versions of most thread-unsafe functions.
The names of the reentrant versions always end with the “_r” sufﬁx.
For example, the reentrant version of gethostbyname is called gethostbyname_r.
A race occurs when the correctness of a program depends on one thread reaching point x in its control ﬂow before another thread reaches point y.
Races usually occur because programmers assume that threads will take some particular trajectory through the execution state space, forgetting the golden rule that threaded programs must work correctly for any feasible trajectory.
An example is the easiest way to understand the nature of races.
The main thread creates four peer threads and passes a pointer to a unique integer ID to each one.
Each peer thread copies the ID passed in its argument to a local variable (line 21), and then prints a message containing the ID.
It looks simple enough, but when we run this program on our system, we get the following incorrect result:
The problem is caused by a race between each peer thread and the main thread.
When the main thread creates a peer thread in line 12, it passes a pointer to the local stack variable i.
Otherwise, it will contain the ID of some other thread.
The scary thing is that whether we get the correct answer depends on how the kernel schedules the execution of the threads.
On our system it fails, but on other systems it might work correctly, leaving the programmer blissfully unaware of a serious bug.
To eliminate the race, we can dynamically allocate a separate block for each integer ID, and pass the thread routine a pointer to this block, as shown in.
Notice that the thread routine must free the block in order to avoid a memory leak.
When we run this program on our system, we now get the correct result:
In Figure 12.41, we eliminated the race by allocating a separate block for each integer ID.
Outline a different approach that does not call the malloc or free functions.
Semaphores introduce the potential for a nasty kind of run-time error, called deadlock, where a collection of threads are blocked, waiting for a condition that.
Figure 12.42 Progress graph for a program that can deadlock.
The progress graph is an invaluable tool for understanding deadlock.
For example, Figure 12.42 shows the progress graph for a pair of threads that use two semaphores formutual exclusion.
From this graph, we can glean some important insights about deadlock:
The programmer has incorrectly ordered the P and V operations such that the forbidden regions for the two semaphores overlap.
If some execution trajectory happens to reach the deadlock state d , then no further progress is possible because the overlapping forbidden regions block progress in every legal direction.
In other words, the program is deadlocked because each thread is waiting for the other to do a V operation that will never occur.
The overlapping forbidden regions induce a set of states called the deadlock region.
If a trajectory happens to touch a state in the deadlock region, then deadlock is inevitable.
Trajectories can enter deadlock regions, but they can never leave.
Deadlock is an especially difﬁcult issue because it is not always predictable.
Some lucky execution trajectories will skirt the deadlock region, while others will be trapped by it.
Or the program might work ﬁne on one machine but deadlock on another.
Worst of all, the error is often not repeatable because different executions have different trajectories.
Programs deadlock for many reasons and avoiding them is a difﬁcult problem in general.
However, when binary semaphores are used for mutual exclusion, as in Figure 12.42, then you can apply the following simple and effective rule to avoid deadlocks:
Mutex lock ordering rule: A program is deadlock-free if, for each pair of mutexes (s, t) in the program, each thread that holds both s and t simultaneously locks them in the same order.
For example, we can ﬁx the deadlock in Figure 12.42 by locking s ﬁrst, then t in each thread.
Practice Problem 12.15 Consider the following program, which attempts to use a pair of semaphores for mutual exclusion.
If so, what simple change to the initial semaphore values will eliminate the potential for deadlock?
A concurrent program consists of a collection of logical ﬂows that overlap in time.
In this chapter, we have studied three different mechanisms for building concurrent programs: processes, I/O multiplexing, and threads.
We used a concurrent network server as the motivating application throughout.
Processes are scheduled automatically by the kernel, and because of their separate virtual address spaces, they require explicit IPC mechanisms in order to share data.
Event-driven programs create their own concurrent logical ﬂows, which aremodeled as statemachines, and use I/Omultiplexing to explicitly schedule the ﬂows.
Because the program runs in a single process, sharing data between ﬂows is fast and easy.
Like ﬂows based on processes, threads are scheduled automatically by the kernel.
Like ﬂows based on I/O multiplexing, threads run in the context of a single process, and thus can share data quickly and easily.
The P and V operations on semaphores have been developed to help deal with this problem.
Semaphore operations can be used to providemutually exclusive access to shared data, as well as to schedule access to resources such as the bounded buffers in producer-consumer systems and shared objects in readers-writers systems.A concurrent prethreaded echo server provides a compelling example of these usage scenarios for semaphores.
Functions that are called by threads must have a property known as thread safety.
We have identiﬁed four classes of thread-unsafe functions, along with suggestions for making them thread-safe.
Reentrant functions are the proper subset of thread-safe functions that do not access any shared data.
Reentrant functions are often more efﬁcient than nonreentrant functions because they do not require any synchronization primitives.
Deadlocks occur when a ﬂow is waiting for an event that will never happen.
The book by Butenhof [16] is a comprehensive description of the Posix threads interface.
The paper by Birrell [7] is an excellent introduction to threads programming and its pitfalls.
The bookbyReinders [86] describes aC/C++ library that simpliﬁes the design and implementation of threaded programs.
Pugh identiﬁes weaknesses with the way that Java threads interact through memory and proposes replacement memory models [84]
Gustafson proposed the weak scaling speedup model [46] as an alternative to strong scaling.
However, when we run it on our system, nothing prints.
You can ﬁx this bug by replacing the exit function in line 9 with one of two different Pthreads function calls.
When the server has ﬁnished interacting with the client, close both streams as follows:
However, if you try this approach in a concurrent server based on threads, you will create a deadly race condition.
For each thread, list the pairs of mutexes that it holds simultaneously.
For these threads, show a new lock ordering that guarantees freedom from deadlock.
Test your solution using the driver program from Problem 12.31
In the ﬁrst part of the lab, you will set up the proxy to accept requests, parse the HTTP, forward the requests to the server, and return the results back to the browser.
Your proxy should log the URLs of all requests in a log ﬁle on disk, and it should also block requests to any URL contained in a ﬁlter ﬁle on disk.
In the second part of the lab, you will upgrade your proxy to deal with multiple open connections at onceby spawning a separate thread todealwith each request.
While your proxy is waiting for a remote server to respond to a request so that it can serve one browser, it should be working on a pending request from another browser.
Since the kernel will not close a ﬁle until the reference counter in its ﬁle table goes to 0, the child’s end of the connection stays open.
Thus, the child’s copy of the connected ﬁle descriptor will be closed automatically when the child exits.
If EOF becomes true on a descriptor, then the descriptor is ready for reading because the read operation will return immediately with a zero return code indicating EOF.
Thus, typing ctrl-d causes the select function to return with descriptor 0 in the ready set.
Thus, a single close operation is sufﬁcient to free the memory resources associated with the connected descriptor when we are through with it.
Static variables such as cnt are a little tricky because the sharing is limited to the functions within their scope—in this case, the thread routine.
Even though its value is passed to the peer threads, the peer threads never reference it on the stack, and thus it is not shared.
Variables ptr, cnt, and msgs are referenced by more than one thread, and thus are shared.
Trajectories such as A and C that skirt the critical region are safe and will produce correct results.
When the buffer contains an item, the producer is blocked.
So at any point in time, only a single thread can access the buffer, and thusmutual exclusion is guaranteedwithout using themutex.
Similarly, the V operation pops the top thread ID from the stack and restarts that thread.
Given this stack implementation, an adversarial writer in its critical section could simply wait until another writer blocks on the semaphore before releasing the semaphore.
In this scenario, a waiting reader might wait forever as two writers passed control back and forth.
Notice that although it might seemmore intuitive to use a FIFO queue rather than a LIFO stack, using such a stack is not incorrect and does not violate the semantics of the P and V operations.
However, it is threadsafe because the accesses to the shared variable are protected by P and V operations, and thus are mutually exclusive.
Another approach is to pass the integer i directly, rather than passing a pointer to i:
In the thread routine, we cast the argument back to an int and assign it to myid:
The advantage is that it reduces overhead by eliminating the calls to malloc and free.
A signiﬁcant disadvantage is that it assumes that pointers are at least as large as ints.
While this assumption is true for all modern systems, it might not be true for legacy or future systems.
The progress graph for the original program is shown in Figure 12.46
The program always deadlocks, since any feasible trajectory is eventually trapped in a deadlock state.
The progress graph for the corrected program is shown in Figure 12.47
Programmers should always check the error codes returned by system-level functions.
There aremany subtleways that things can gowrong, and it onlymakes sense to use the status information that the kernel is able to provide us.
Unfortunately, programmers are often reluctant to do error checking because it clutters their code, turning a single line of code into a multi-line conditional statement.
Error checking is also confusing because different functions indicate errors in different ways.
On the one hand, we would like our code examples to be concise and simple to read.
On the other hand, we do not want to give students the wrong impression that it is OK to skip error checking.
To resolve these issues, we have adopted an approach based on error-handling wrappers that was pioneered byW.
The idea is that given some base system-level function foo, we deﬁne a wrapper function Foowith identical arguments, but with the ﬁrst letter capitalized.
If it detects an error, the wrapper prints an informative message and terminates the process.
Notice that if there are no errors, thewrapper behaves exactly like the base function.
Put another way, if a program runs correctly with wrappers, it will run correctly if we render the ﬁrst letter of each wrapper in lowercase and recompile.
The wrappers are packaged in a single source ﬁle (csapp.c) that is compiled and linked into each program.
A separate header ﬁle (csapp.h) contains the function prototypes for the wrappers.
This appendix gives a tutorial on the different kinds of error handling in Unix systems, and gives examples of the different styles of error-handling wrappers.
Copies of the csapp.h and csapp.c ﬁles are available on the CS:APP Web page.
The systems-level function calls that we will encounter in this book use three different styles for returning errors: Unix-style, Posix-style, and DNS-style.
The strerror function returns a text description for a particular value of errno.
Many of the newer Posix functions such as Pthreads use the return value only to indicate success (0) or failure (nonzero)
Any useful results are returned in function arguments that are passed by reference.
For example, the Posix-style pthread_create function indicates success or failure with its return value and returns the ID of the newly created thread (the useful result) by reference in its ﬁrst argument.
The gethostbyname and gethostbyaddr functions that retrieve DNS (Domain Name System) host entries have yet another approach for returning errors.
These functions return a NULL pointer on failure and set the global h_errno variable.
Thoughout this book, we use the following error-reporting functions to accommodate different error-handling styles.
The app_error function is included as a convenience for application errors.
Figure A.2 shows the wrapper for the Unix-style wait function.
If the wait returns with an error, the wrapper prints an informative message and then exits.
Figure A.3 shows the wrapper for the Unix-style kill function.
Notice that this function, unlike Wait, returns void on success.
Like most Posix-style functions, it does not overload useful results with error-return codes, so the wrapper returns void on success.
Figure A.5 shows the error-handling wrapper for the DNS-style gethostbyname function.
Buffer overﬂows: Attacks and defenses for the vulnerability of the decade.
Improving cache performance of dynamic applications through data and computation reorganizations at run time.
Online maintenance of very large random samples on ﬂash storage.
Patterson, G.Gibson, andR.Katz.A case for redundant arrays of inexpensive disks (RAID)
The Omega test: A fast and practical integer programming algorithm for dependence analysis.
Coda: A highly available ﬁle system for a distributed workstation environment.
Global address space, non-uniform bandwidth: A memory system performance characterization of parallel systems.
Entries that belong to a hardware or software systemare followedby a tag in brackets that identiﬁes the system, along with a brief description to jog your memory.
