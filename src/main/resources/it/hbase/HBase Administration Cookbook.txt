No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
In 2009, he led the development of his previous company's display advertisement data infrastructure using Hadoop and Hive.
In 2010, he joined his current employer, where he designed and implemented the Hadoop- and HBase-based, large-scale item ranking system.
He is also one of the members of the Hadoop team in the company, which operates several Hadoop/HBase clusters.
Little did I know, when I was first asked by Packt Publishing whether I would be interested in writing a book about HBase administration on September 2011, how much work and stress (but also a lot of fun) it was going to be.
Now that the book is finally complete, I would like to thank those people without whom it would have been impossible to get done.
First, I would like to thank the HBase developers for giving us such a great piece of software.
Thanks to all of the people on the mailing list providing good answers to my many questions, and all the people working on tickets and documents.
I would also like to thank the team at Packt Publishing for contacting me to get started with the writing of this book, and providing support, guidance, and feedback.
Many thanks to Rakuten, my employer, who provided me with the environment to work on HBase and the chance to write this book.
Thank you to Michael Stack for helping me with a quick review of the book.
To Yotaro Kagawa: Thank you for supporting me and my family from the very start and ever since.
To Xinping and Lingyin: Thank you for your support and all your patience—I love you!
Tatsuya Kawano is an HBase contributor and evangelist in Japan.
He is currently working for Gemini Mobile Technologies as a Research & Development software engineer.
He is also developing Cloudian, a fully S3 API-complaint cloud storage platform, and Hibari DB, an open source, distributed, key-value store.
He has studied graphic design in New York, in the late 1990s.
He loves playing with 3D computer graphics as much as he loves developing high-availability, scalable, storage systems.
His areas of research are mostly related to large-scale systems and emerging technologies dedicated to solving scalability, performance, and high availability issues.
I would like to thank my wife and my little angel for their love and support.
He has more than 7 years of experience in software and middleware (Apache, Tomcat, PostgreSQL, Hadoop eco system) engineering.
Shinicha has written a few books on Hadoop in Japan.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Why Subscribe? f Fully searchable across every book published by Packt.
Preface As an open source, distributed, big data store, HBase scales to billions of rows, with millions of columns and sits on top of the clusters of commodity machines.
If you are looking for a way to store and access a huge amount of data in real time, then look no further than HBase.
HBase Administration Cookbook provides practical examples and simple step-by-step instructions for you to administrate HBase with ease.
The recipes cover a wide range of processes for managing a fully distributed, highly available HBase cluster on the cloud.
Working with such a huge amount of data means that an organized and manageable process is key, and this book will help you to achieve that.
The recipes in this practical cookbook start with setting up a fully distributed HBase cluster and moving data into it.
You will learn how to use all the tools for day-to-day administration tasks, as well as for efficiently managing and monitoring the cluster to achieve the best performance possible.
Understanding the relationship between Hadoop and HBase will allow you to get the best out of HBase; so this book will show you how to set up Hadoop clusters, configure Hadoop to cooperate with HBase, and tune its performance.
Chapter 2, Data Migration: In this chapter, we will start with the simple task of importing data from MySQL to HBase, using its Put API.
We will then describe how to use the importtsv and bulk load tools to load TSV data files into HBase.
We will also use a MapReduce sample to import data from other file formats.
This includes putting data directly into an HBase table and writing to HFile format files on Hadoop Distributed File System (HDFS)
The last recipe in this chapter explains how to precreate regions before loading data into HBase.
This chapter ships with several sample sources written in Java.
It assumes that you have basic Java knowledge, so it does not explain how to compile and package the sample Java source in the recipes.
Chapter 3, Using Administration Tools: In this chapter, we describe the usage of various administration tools such as HBase web UI, HBase Shell, HBase hbck, and others.
We explain what the tools are for, and how to use them to resolve a particular task.
Chapter 4, Backing Up and Restoring HBase Data: In this chapter, we will describe how to back up HBase data using various approaches, their pros and cons, and which approach to choose depending on your dataset size, resources, and requirements.
Chapter 5, Monitoring and Diagnosis: In this chapter, we will describe how to monitor and diagnose HBase cluster with Ganglia, OpenTSDB, Nagios, and other tools.
We will start with a simple task to show the disk utilization of HBase tables.
We will install and configure Ganglia to monitor an HBase metrics and show an example usage of Ganglia graphs.
We will also set up OpenTSDB, which is similar to Ganglia, but more scalable as it is built on the top of HBase.
We will set up Nagios to check everything we want to check, including HBase-related daemon health, Hadoop/HBase logs, HBase inconsistencies, HDFS health, and space utilization.
In the last recipe, we will describe an approach to diagnose and fix the frequently asked hot spot region issue.
Chapter 6, Maintenance and Security: In the first six recipes of this chapter we will learn about the various HBase maintenance tasks, such as finding and correcting faults, changing cluster size, making configuration changes, and so on.
In the last three recipes, we will install Kerberos and then set up HDFS security with Kerberos, and finally set up secure HBase client access.
Chapter 7, Troubleshooting: In this chapter, we will look through several of the most confronted issues.
We will describe the error messages of these issues, why they happen, and how to fix them with the troubleshooting tools.
Chapter 8, Basic Performance Tuning: In this chapter, we will describe how to tune HBase to gain better performance.
We will also include recipes to tune other tuning points such as Hadoop configurations, the JVM garbage collection settings, and the OS kernel parameters.
Chapter 9, Advanced Configurations and Tuning: This is another chapter about performance tuning in the book.
The previous chapter describes some recipes to tune Hadoop, OS setting, Java, and HBase itself, to improve the overall performance of the HBase cluster.
In this chapter, we will describe more specific recipes, some of which are for write-heavy clusters, while some are aimed at improving the read performance of the cluster.
What you need for this book Everything you need is listed in each recipe.
The basic list of software required for this book are as follows:
Who this book is for This book is for HBase administrators, developers, and will even help Hadoop administrators.
You are not required to have HBase experience, but are expected to have a basic understanding of Hadoop and MapReduce.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: "HBase can be stopped using its stop-hbase.sh script."
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: " Verify the startup from AWS Management Console"
Warnings or important notes appear in a box like this.
Let us know what you think about this bookwhat you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the codewe would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded to our website, or added to any list of existing errata, under the Errata section of that title.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Introduction This chapter explains how to set up HBase cluster, from a basic standalone HBase instance to a fully distributed, highly available HBase cluster on Amazon EC2
Use HBase when you need random, real-time, read/write access to your Big Data.
This project's goal is the hosting of very large tables—billions of rows X millions of columns—atop clusters of commodity hardware.
In most cases, a fully distributed HBase cluster runs on an instance of HDFS, so we will explain how to set up Hadoop before proceeding.
Apache ZooKeeper is an open source software providing a highly reliable, distributed coordination service.
HBase, which is a database that runs on Hadoop, keeps a lot of files open at the same time.
We need to change some Linux kernel settings to run HBase smoothly.
A fully distributed HBase cluster has one or more master nodes (HMaster), which coordinate the entire cluster, and many slave nodes (RegionServer), which handle the actual data storage and request.
HBase can run multiple master nodes at the same time, and use ZooKeeper to monitor and failover the masters.
But as HBase uses HDFS as its low-layer filesystem, if HDFS is down, HBase is down too.
The master node of HDFS, which is called NameNode, is the Single Point Of Failure (SPOF) of HDFS, so it is the SPOF of an HBase cluster.
However, NameNode as a software is very robust and stable.
Moreover, the HDFS team is working hard on a real HA NameNode, which is expected to be included in Hadoop's next major release.
The first seven recipes in this chapter explain how we can get HBase and all its dependencies working together, as a fully distributed HBase cluster.
The last recipe explains an advanced topic on how to avoid the SPOF issue of the cluster.
We will start by setting up a standalone HBase instance, and then demonstrate setting up a distributed HBase cluster on Amazon EC2
Quick start HBase has two run modes—standalone mode and distributed mode.
In standalone mode, HBase uses a local filesystem instead of HDFS, and runs all HBase daemons and an HBase-managed ZooKeeper instance, all in the same JVM.
It leads you through installing HBase, starting it in standalone mode, creating a table via HBase Shell, inserting rows, and then cleaning up and shutting down the standalone HBase instance.
Getting ready You are going to need a Linux machine to run the stack.
As HBase is written in Java, you will need to have Java installed first.
HBase runs on Oracle's JDK only, so do not use OpenJDK for the setup.
We will add a user with the name hadoop, as the owner of all HBase/Hadoop daemons and files.
We will have all HBase files and data stored under /usr/local/hbase:
At the time of writing this book, the current stable release was 0.92.1
You can set up a standalone HBase instance by following these instructions:
HBASE_HOME environment variable to make the setup easier, by using the following commands:
Set JAVA_HOME in HBase's environment setting file, by using the following command:
Start HBase in standalone mode by using the following command:
Connect to the running HBase via HBase Shell, using the following command:
Verify HBase's installation by creating a table and then inserting some values.
In order to list the newly created table, use the following command:
In order to insert some values into the newly created table, use the following commands:
Verify the data we inserted into HBase by using the scan command:
Now clean up all that was done, by using the disable and drop commands:
In order to disable the table test, use the following command:
In order to drop the the table test, use the following command:
We have used a symbolic link named current for it, so that version upgrading in the future is easy to do.
In order to inform HBase where Java is installed, we will set JAVA_HOME in hbase-env.
You will see some Java heap and HBase daemon settings in it too.
We will discuss these settings in the last two chapters of this book.
In step 1, we created a directory on the local filesystem, for HBase to store its data.
For a fully distributed installation, HBase needs to be configured to use HDFS, instead of a local filesystem.
The HBase master daemon (HMaster) is started on the server where starthbase.sh is executed.
As we did not configure the region server here, HBase will start a single slave daemon (HRegionServer) on the same JVM too.
As we mentioned in the Introduction section, HBase depends on ZooKeeper as its coordination service.
You may have noticed that we didn't start ZooKeeper in the previous steps.
This is because HBase will start and manage its own ZooKeeper ensemble, by default.
Using HBase Shell, you can manage your cluster, access data in HBase, and do many other jobs.
Here, we just created a table called test, we inserted data into HBase, scanned the test table, and then disabled and dropped it, and exited the shell.
By using Amazon EC2, we can practice HBase on a fully distributed mode easily, at low cost.
All the servers that we will use to demonstrate HBase in this book are running on Amazon EC2
This recipe describes the setup of the Amazon EC2 environment, as a preparation for the installation of HBase on it.
We will set up a name server and client on Amazon EC2
You can also use other hosting services such as Rackspace, or real servers to set up your HBase cluster.
Getting ready You will need to sign up, or create an Amazon Web Service (AWS) account at http://aws.
We will use EC2 command-line tools to manage our instances.
You can download and set up the tools by following the instructions available at the following page:
You need a public/private key to log in to your EC2 instances.
You can generate your key pairs and upload your public key to EC2, using these instructions:
Before you can log in to an instance, you must authorize access.
The following link contains instructions for adding rules to the default security group:
After all these steps are done, review the following checklist to make sure everything is ready:
You can check this at your account's Security Credentials page.
As an example, the following snippet shows my settings; make sure you are using the right EC2_URL for your region:
If everything has been set up properly, the command will show your instances similarly to how you had configured them in the previous command.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.
An AMI is a preconfigured operating system and software, which is used to create a virtual machine within EC2
For the purpose of practicing HBase, a 32-bit, EBS-backed AMI is the most cost effective AMI to use.
A small instance is good for practicing HBase on EC2 because it's cheap.
For production, we recommend you to use at least High-Memory Extra Large Instance with EBS, or a real server.
Follow these instructions to get your EC2 instances ready for HBase.
We will start two EC2 instances; one is a DNS/NTP server, and the other one is the client:
You should see two instances from the output of the command.
Log in to the instances via SSH, using the following command:
Update the package index files before we install packages on the server, by using the following command:
Change your instances' time zone to your local timezone, using the following command:
Install the NTP server daemon on the DNS server, using the following command:
Install the NTP client on the client/server, using the following command:
Because there is no HBase-specific configuration for the NTP setup, we will skip the details.
You can find the sample ntp.conf files for both the server and client, from the sample source of this book.
You will need to configure BIND9 to run as a primary master server for internal lookup, and run as a caching server for external lookup.
You also need to configure the DNS server, to allow other EC2 instances to update their record on the DNS server.
We will skip the details as this is out of the scope of this book.
For sample BIND9 configuration, please refer to the source, shipped with this book.
Set up hostname to the EC2 instance's user data of the client.
Create a script to update the client's record on the DNS server, using user data:
Finally, to run this at boot time from rc.local, add the following script to the rc.local file:
First we started two instances, a micro instance for DNS/NTP server, and a small one for client.
To provide a name service to other instances, the DNS name server has to be kept running.
In step 3, we set up the NTP server and client.
We will run our own NTP server on the same DNS server, and NTP clients on all other servers.
Note: Make sure that the clocks on the HBase cluster members are in basic alignment.
EC2 instances can be started and stopped on demand; we don't need to pay for stopped instances.
But, restarting an EC2 instance will change the IP address of the instance, which makes it difficult to run HBase.
We can resolve this issue by running a DNS server to provide a name service to all EC2 instances in our HBase cluster.
We can update name records on the DNS server every time other EC2 instances are restarted.
Also, we will get the private IP address of the instance via EC2 API.
With this data, we can then send a DNS update command to our DNS server every time the instance is restarted.
As a result, we can always use its fixed hostname to access the instance.
You can stop all other instances whenever you do not need to run your HBase cluster.
As a fully distributed HBase cluster installation, its master daemon (HMaster) typically runs on the same server as the master node of HDFS (NameNode), while its slave daemon (HRegionServer) runs on the same server as the slave node of HDFS, which is called DataNode.
We will cover the setup of MapReduce in this recipe too, in case you like to run MapReduce on HBase.
For a small Hadoop cluster, we usually have a master daemon of MapReduce (JobTracker) run on the NameNode server, and slave daemons of MapReduce (TaskTracker) run on the DataNode servers.
We will have one master node (master1) run NameNode and JobTracker on it.
Getting ready You will need four small EC2 instances, which can be obtained by using the following command:
All these instances must be set up properly, as described in the previous recipe, Getting ready on Amazon EC2
Besides the NTP and DNS setups, Java installation is required by all servers too.
We will use the hadoop user as the owner of all Hadoop daemons and files.
All Hadoop files and data will be stored under /usr/local/hadoop.
Add the hadoop user and create a /usr/local/hadoop directory on all the servers, in advance.
We will set up one Hadoop client node as well.
We will use client1, which we set up in the previous recipe.
Therefore, the Java installation, hadoop user, and directory should be prepared on client1 too.
Here are the steps to set up a fully distributed Hadoop cluster:
In order to SSH log in to all nodes of the cluster, generate the hadoop user's public key on the master node:
On all slave and client nodes, add the hadoop user's public key to allow SSH login from the master node:
Get the latest, stable, HBase-supported Hadoop release from Hadoop's official site, http://www.apache.org/dyn/closer.cgi/hadoop/common/
While this chapter was being written, the latest HBase-supported, stable Hadoop release was 1.0.2
Download the tarball and decompress it to our root directory for Hadoop, then add a symbolic link, and an environment variable:
You can skip the following steps if you don't use MapReduce:
You can access your HDFS by typing the following command:
You can also view your HDFS admin page from the browser.
Now you can access your MapReduce admin page from the browser.
To stop HDFS, execute the following command from the master node:
To stop MapReduce, execute the following command from the master node:
To start/stop the daemon on remote slaves from the master node, a passwordless SSH login of the hadoop user is required.
HBase must run on a special HDFS that supports a durable sync implementation.
If HBase runs on an HDFS that has no durable sync implementation, it may lose data if its slave servers go down.
Before starting Hadoop, all Hadoop directories and settings need to be synced with the slave servers.
The first time you start Hadoop (HDFS), you need to format NameNode.
Note that you should only do this at the initial installation.
At this point, you can start/stop Hadoop using its start/stop script.
Here we started/stopped HDFS and MapReduce separately, in case you don't require MapReduce.
All HBase cluster nodes and clients need to be able to access the ZooKeeper ensemble.
This recipe describes how to set up a ZooKeeper cluster.
We will only set up a standalone ZooKeeper node for our HBase cluster, but in production it is recommended that you run a ZooKeeper ensemble of at least three nodes.
Also, make sure to run an odd number of nodes.
We will cover the setting up of a clustered ZooKeeper in the There's more...
Getting ready First, make sure Java is installed in your ZooKeeper server.
We will use the hadoop user as the owner of all ZooKeeper daemons and files.
So, the Java installation, hadoop user, and directory should be prepared on client1 as well.
To set up a standalone ZooKeeper installation, follow these instructions:
Get the latest stable ZooKeeper release from ZooKeeper's official site, http://ZooKeeper.apache.org/releases.html#download.
Download the tarball and decompress it to our root directory for ZooKeeper.
We will set a ZK_HOME environment variable to make the setup easier.
As of this writing, ZooKeeper 3.4.3 is the latest stable version:
Create directories for ZooKeeper to store its snapshot and transaction log:
Start ZooKeeper from the master node by executing this command:
Connect to the running ZooKeeper, and execute some commands to verify the installation:
Stop ZooKeeper from the master node by executing the following command:
In this recipe, we set up a basic standalone ZooKeeper instance.
As you can see, the setting is very simple; all you need to do is to tell ZooKeeper where to find Java and where to save its data.
In step 4, we created a file named java.env and placed the Java settings in this file.
You must use this filename as ZooKeeper, which by default, gets its Java settings from this file.
You can copy the settings from the sample file shipped with ZooKeeper.
As ZooKeeper always acts as a central role in a cluster system, it should be set up properly to gain the best performance.
To connect to a running ZooKeeper ensemble, use its command-line tool, and specify the ZooKeeper server and port you want to connect to.
You don't need to specify it, if you are using the default port setting.
ZooKeeper provides commands to access or update Znode from its command-line tool; type help for more information.
As HBase relays ZooKeeper as its coordination service, the ZooKeeper service must be extremely reliable.
In production, you must run a ZooKeeper cluster of at least three nodes.
Also, make sure to run an odd number of nodes.
The procedure to set up a clustered ZooKeeper is basically the same as shown in this recipe.
You can follow the previous steps to set up each cluster node at first.
Add the following settings to each node's zoo.cfg, so that every node knows about every other node in the ensemble:
Note that clocks on all ZooKeeper nodes must be synchronized.
You can use Network Time Protocol (NTP) to have the clocks synchronized.
Then, you can connect to the cluster from your client, by using the following command:
ZooKeeper will function as long as more than half of the nodes in the ZooKeeper cluster are alive.
This means, in a three node cluster, only one server can die.
Changing the kernel settings HBase is a database running on Hadoop, and just like other databases, it keeps a lot of files open at the same time.
Linux limits the number of file descriptors that any one process may open; the default limits are 1024 per process.
To run HBase smoothly, you need to increase the maximum number of open file descriptors for the user, who started HBase.
The nproc setting specifies the maximum number of processes that can exist simultaneously for the user.
If nproc is too low, an OutOfMemoryError error may happen.
We will describe how to show and change the kernel settings, in this recipe.
Getting ready Make sure you have root privileges on all of your servers.
You will need to make the following kernel setting changes to all servers of the cluster:
To confirm the current open file limits, log in as the hadoop user and execute the following command:
To show the setting for maximum processes, use the -u option of the ulimit command:
Log in as the root user to increase open file and nproc limits.
Log out and back in again, as the hadoop user, and confirm the setting values again; you should see the above changes have been applied:
With this change of the kernel setting, HBase can keep enough files open at the same time and also run smoothly.
Setting up HBase A fully distributed HBase instance has one or more master nodes (HMaster), and many slave nodes (RegionServer) running on HDFS.
It uses a reliable ZooKeeper ensemble to coordinate all the components of the cluster, including masters, slaves, and clients.
It's not necessary to run HMaster on the same server of HDFS NameNode, but, for a small cluster, it's typical to have them run on the same server, just for ease of management.
RegionServers are usually configured to run on servers of HDFS DataNode.
Running RegionServer on the DataNode server has the advantage of data locality too.
Eventually, DataNode running on the same server, will have a copy on it of all the data that RegionServer requires.
This recipe describes the setup of a fully distributed HBase.
We will also set up an HBase client on client1
Getting ready First, make sure Java is installed on all servers of the cluster.
We will use the hadoop user as the owner of all HBase daemons and files, too.
All HBase files and data will be stored under /usr/local/hbase.
Create this directory on all servers of your HBase cluster, in advance.
Therefore, the Java installation, hadoop user, and directory should be prepared on client1 too.
You can ensure it started properly by accessing HDFS, using the following command:
MapReduce does not need to be started, as HBase does not normally use it.
We assume that you are managing your own ZooKeeper, in which case, you can start it and confirm if it is running properly.
You can ensure it is running properly by sending the ruok command to its client port:
To set up our fully distributed HBase cluster, we will download and configure HBase on the master node first, and then sync to all slave nodes and clients.
At the time of writing this book, the current stable release was 0.92.1
Remove it first if you have created it for your standalone HBase installation:
To tell HBase where the Java installation is, set JAVA_HOME in the HBase environment setting file (hbase-env.sh):
Tell HBase whether it should manage it's own instance of ZooKeeper or not.
Link the HDFS configuration file (hdfs-site.xml) to HBase's configuration folder (conf), so that HBase can see the HDFS's client configuration on your Hadoop cluster:
Sync all the HBase files under /usr/local/hbase from master, to the same directory as client and slave nodes.
You can also access the HBase web UI from your browser.
Our HBase cluster is configured to use /hbase as its root directory on HDFS, by specifying the hbase.rootdir property.
Because it is the first time HBase was started, it will create the directory automatically.
You can see the files HBase created on HDFS from the client:
You can use clustered ZooKeeper by listing all the servers of the ensemble, such as zoo1,zoo2,zoo3
When starting the cluster, HBase will SSH into each region server configured here, and start the HRegionServer daemon on that server.
By linking hdfs-site.xml under the $HBASE_HOME/conf directory, HBase will use all the client configurations you made for your HDFS in hdfs-site.xml, such as the dfs.replication setting.
HBase ships with its prebuilt hadoop-core and ZooKeeper JAR files.
They may be out of date, compared to what you used in your Hadoop and ZooKeeper installation.
Make sure HBase uses the same version of .jar files with Hadoop and ZooKeeper, to avoid any unexpected problems.
There are some basic settings we should tune, before moving forward.
These are very basic and important Hadoop (HDFS), ZooKeeper, and HBase settings that you should consider to change immediately after setting up your cluster.
Some of these settings take effect due to data durability or cluster availability, which must be configured, while some are recommended configurations for running HBase smoothly.
Configuration settings depend on your hardware, data, and cluster size.
You may need to change the settings to fit your environment.
Every time you make changes, you need to sync to all clients and slave nodes, then restart the respective daemon to apply the changes.
The configurations that should be considered for change are as follows:
It must be set to true, or you may lose data if the region server crashes:
Increase ZooKeeper's heap memory size so that it does not swap:
Increase ZooKeeper's maximum client connection number to handle more concurrent requests:
It's better to change the log directory out of the installation folder.
With this feature enabled, a writer of HDFS can guarantee that data will be persisted by invoking a flush call.
So, HBase can guarantee that when a region server dies, data can be recovered and replayed on other region servers using its Write-Ahead Log (WAL)
To verify if the HDFS append is supported or not, see your HMaster log of the HBase startup.
If the append is not turned to on, you will find a log like the following:
Its default value is 256, which is too low for running HBase on HDFS.
ZooKeeper is very sensitive to swapping, which will seriously degrade its performance.
ZooKeeper has an upper bound on the number of connections it will serve at any one time.
Its default is 10, which is too low for HBase, especially when running MapReduce on it.
HBase ships with a heap size of 1 GB, which is too low for modern machines.
In step 6, we changed the ZooKeeper's session timeout to a lower value.
Lower timeout means HBase can find crashed region servers faster, and thus, recover the crashed regions on other servers in a short time.
On the other hand, with a very short session timeout, there is a risk that the HRegionServer daemon may kill itself when the cluster is in heavy load, because it may not be able to send a heartbeat to the ZooKeeper before getting a timeout.
Hadoop and HBase are designed to handle the failover of their slave nodes automatically.
Because there may be many nodes in a large cluster, a hardware failure of a server or shut down of a slave node are considered as normal in the cluster.
A ZooKeeper ensemble is typically clustered with three or more servers; as long as more than half of the servers in the cluster are online, ZooKeeper can provide its service normally.
HBase saves its active master node, root region server location, and other important running data in ZooKeeper.
Therefore, we can just start two or more HMaster daemons on separate servers and the one started first will be the active master server of the HBase cluster.
But, NameNode of HDFS is the SPOF of the cluster.
NameNode keeps the entire HDFS's filesystem image in its local memory.
As you may notice, there is a Secondary NameNode of HDFS.
Note that Secondary NameNode is not a standby of NameNode, it just provides a checkpoint function to NameNode.
So, the challenge of a highly available cluster is to make NameNode highly available.
In this recipe, we will describe the setup of two highly available master nodes, which will use Heartbeat to monitor each other.
Heartbeat is a widely used HA solution to provide communication and membership for a Linux cluster.
Heartbeat needs to be combined with a Cluster Resource Manager (CRM) to start/stop services for that cluster.
We will set up a Virtual IP (VIP) address using Heartbeat and Pacemaker, and then associate it with the active master node.
We will focus on setting up NameNode and HBase; you can simply use a similar method to set up two JobTracker nodes as well.
Getting ready You should already have HDFS and HBase installed.
We will set up a standby master node (master2), as you need another server ready to use.
Sync your Hadoop and HBase root directory from the active master (master1) to the standby master.
Make sure the hadoop user has write permission to the NFS directory.
We will set up VIP for the two masters, and assume you have the following IP addresses and DNS mapping:
It is the VIP that will be set up later.
The following instructions describe how to set up two highly available master nodes.
Install and configure Heartbeat and Pacemaker First, we will install Heartbeat and Pacemaker, and make some basic configurations:
Create and install a NameNode resource agent Pacemaker depends on a resource agent to manager the cluster.
A resource agent is an executable that manages a cluster resource.
In our case, the VIP address and the HDFS NameNode service is the cluster resource we want to manage, using Pacemaker.
Pacemaker ships with an IPaddr resource agent to manage VIP, so we only need to create our own namenode resource agent:
Create a standard Open Clustering Framework (OCF) resource agent file called namenode, with the following content.
The namenode resource agent starts with including standard OCF functions such as the following:
Add a meta_data() function as shown in the following code.
The meta_data() function dumps the resource agent metadata to standard output.
Every resource agent must have a set of XML metadata describing its own purpose and supported parameters:
This function is used by Pacemaker to actually start the NameNode daemon on the server.
In the namenode_start() function, we firstly check whether NameNode is already started on the server; if it is not started, we invoke hadoop-daemon.sh from the hadoop user to start it:
This function is used by Pacemaker to actually stop the NameNode daemon on the server.
In the namenode_stop() function, we first check whether NameNode is already stopped on the server; if it is running, we invoke hadoop-daemon.sh from the hadoop user to stop it:
This function is used by Pacemaker to monitor the status of the NameNode daemon on the server.
In the namenode_status() function, we use the jps command to show all running Java processes owned by the hadoop user, and the grep name of the NameNode daemon to see whether it has started:
Here, we will simply call the previous functions to implement the required standard OCF resource agent actions:
Make sure all the tests are passed before proceeding to the next step, or the HA cluster will behave unexpectedly.
Configure highly available NameNode We are ready to configure highly available NameNode using Heartbeat and Pacemaker.
We will set up a VIP address and configure Hadoop and HBase to use this VIP address as their master node.
NameNode will be started on the active master where VIP is assigned.
If active master has crashed, Heartbeat and Pacemaker will detect it and assign the VIP address to the standby master node, and then start NameNode there.
Make the following changes to configure Hadoop to use our VIP address.
Sync to all masters, clients, and slaves after you've made the changes:
Make the following changes to configure HBase to use our VIP address.
Sync to all masters, clients, and slaves after you've made the changes:
To configure Hadoop to write its metadata to a local disk and NFS, make the following changes and sync to all masters, clients, and slaves:
Configure the VIP resource and the NAMENODE resource as a resource group:
Configure colocation of a VIP resource and the NAMENODE resource:
Configure the resource order of the VIP resource and the NAMENODE resource:
Verify the previous Heartbeat and resource configurations by using the crm_mon command.
If everything is configured properly, you should see an output like the following :
Make sure that the VIP and NAMENODE resources are started on the same server.
Start DataNode, HBase cluster, and backup HBase master We have confirmed that our HA configuration works as expected, so we can start HDFS and HBase now.
Note that NameNode has already been started by Pacemaker, so we need only start DataNode here:
Start standby HMaster from the standby master server, master2 in this case:
The previous steps finally leave us with a cluster structure like the following diagram:
At first, we installed Heartbeat and Pacemaker on the two masters and then configured Heartbeat to enable Pacemaker.
In step 2 of the Create and install a NameNode resource agent section, we created the namenode script, which is implemented as a standard OCF resource agent.
The most important function of the namenode script is namenode_status, which monitors the status of the NameNode daemon.
Here we use the jps command to show all running Java processes owned by the hadoop user, and the grep name of the NameNode daemon to see if it has started.
The namenode resource agent is used by Pacemaker to start/stop/monitor the NameNode daemon.
You can find a full list of the code from the source shipped with this book.
We started Heartbeat after our namenode resource agent was tested and installed.
Then, we made some changes to the default crm configurations.
By using VIP in their configuration, Hadoop and HBase can switch to communicate with the standby master if the active one is down.
In step 6 of the same section, we configured Hadoop (HDFS NameNode) to write its metadata to both the local disk and NFS.
If an active master is down, NameNode will be started from the standby master.
Because they were mounted on the same NFS directory, NameNode started from the standby master can apply the latest metadata from NFS, and restore HDFS to the status before the original active master is down.
Because Pacemaker will start NameNode for us via the namenode resource agent, we need to start DataNode separately, which is what we did in step 1 of the Start DataNode, HBase cluster, and backup HBase master section.
After starting HBase normally, we started our standby HBase master (HMaster) on the standby master server.
If you check your HBase master log, you will find output like the following, which shows itself as a standby HMaster:
Finally, we got NameNode and HMaster running on two servers with an active-standby configuration.
The single point of failure of the cluster was avoided.
However, it leaves us with lots of works to do in production.
You need to test your HA cluster in all rare cases, such as a server power off, unplug of a network cable, shut down of network switch, or anything else you can think of.
On the other hand, SPOF of the cluster may not be as critical as you think.
Based on our experience, almost all of the downtime of the cluster is due to an operational miss or software upgrade.
An alternative way is to use an Elastic IP address.
An Elastic IP address is the role of a static IP address on EC2 while it is associated with your account, not a particular instance.
We can use Heartbeat to associate EIP to the standby master automatically, if the active one is down.
Then, we configure Hadoop and HBase to use an instance's public DNS associated with EIP, to find an active master.
Also, in the namenode resource agent, we have to start/stop not only NameNode, but also all DataNodes.
This is because the IP address of an active master has changed, but DataNode cannot find the new active master unless it is restarted.
We will skip the details because it's out of the scope of this book.
We created an elastic-ip resource agent to achieve this purpose.
You can find it in the source shipped with this book.
Introduction There are several ways to move data into HBase:
For most situations however, it is not always the most efficient method.
This is especially true when a large amount of data needs to be transferred into HBase within a limited time period.
The volume of data to be taken care of is usually huge, and that's probably why you will be using HBase rather than another database.
You have to think about how to move all that data into HBase carefully at the beginning of your HBase project; otherwise you might run into serious performance problems.
HBase has the bulk load feature to support loading of huge volumes of data efficiently into HBase.
The bulk load feature uses a MapReduce job to load data into a specific HBase table by generating HBase's internal HFile data format files and then loading the data files directly into a running cluster.
The easiest way to use this bulk load feature is to use the importtsv tool.
It will run MapReduce jobs to read data from TSV files and will then write output directly into either HBase tables or HBase internal data format files.
Although the importtsv tool can be very useful if you are going to load textual data into HBase, there are cases, such as importing data from other formats, where you might want to generate data programmatically.
MapReduce is the most efficient way to process a huge amount of data.
It might be the only practical way to load a large amount of data into HBase.
We can, of course, use MapReduce to import data into HBase.
However, the MapReduce job might be very heavy, as the data set is huge.
If you don't treat it properly, the heavy MapReduce job might run with poor throughput.
Data migration is a write-heavy task over HBase, unless we generate the internal data files and load them directly into HBase.
Even though HBase writes are always fast, writes can be blocked often during the migration process if it is not configured properly.
Another issue with write-heavy tasks is that all writes may go to the same region server, this is especially true when loading a large amount of data into a new HBase installation.
As all loads go to the same server, the cluster is unbalanced and writing speed will slow down significantly.
We are going to address these issues in this chapter.
We will start with the simple task of importing data from MySQL to HBase, using its Put API.
We will then describe how to use the importtsv and bulk load tools to load TSV data files into HBase.
We will also have a MapReduce sample to import data from other file formats.
This includes putting data directly into an HBase table and writing to HFile format files on Hadoop Distributed File System (HDFS)
The last recipe in this chapter explains how to precreate regions before loading data into HBase.
This chapter ships with several sample sources written in Java.
We assume you have basic Java knowledge, so we will skip explaining how to compile and package the sample Java source in the recipes, but we will put an introduction in the sample source.
Importing data from MySQL via single client The most usual case of data migration might be importing data from an existing RDBMS into HBase.
For this kind of task, the most simple and straightforward way could be to fetch the data from a single client and then put it into HBase, using the HBase Put API.
It works well if there is not too much data to transfer.
This recipe describes importing data from MySQL into HBase using its Put API.
All the operations will be executed on a single client.
This recipe leads you through creating an HBase table via HBase Shell, connecting to the cluster from Java, and then putting data into HBase.
Getting ready Public data sets are an ideal data source to practice HBase data migration.
There are many public data sets available on the internet.
We will use the NOAA'S 1981-2010 CLIMATE NORMALS public data set in this book.
This is climate statistics data generated by the National Oceanic and Atmospheric Administration (NOAA)
In this recipe, we will use its hourly temperature data under products | hourly, which you will find at the aforementioned link.
Create an hly_temp_normal table in your MySQL database, using the following SQL statements:
This book ships with some scripts to help you load data into your MySQL table.
You will need to change your host, user, password, and database name, in the script.
To compile the Java source that will be mentioned in the following section, you will need the following libraries:
You can add them to your classpath manually, or you can use the sample source available with this book.
Before starting to import data, make sure your HDFS, ZooKeeper, and HBase clusters are running properly.
To import data from MySQL to HBase via a single client:
Connect to your HBase cluster through HBase Shell, from your HBase client server:
Write a Java source to import data from MySQL into HBase.
The following steps are to import data using Java source:
Create a connectHBase() method to connect to a specific HBase table from Java:
Create a connectDB() method to connect to MySQL from Java:
In this method, we get data from MySQL and put this data into HBase:
The script to run the JAR file looks as follows:
Verify if the data has been imported to the target table in HBase:
The table name is hly_temp, and it has a single column family n.
The reason we gave it a name one character long is that the column family name will be stored in every Key/Value in HBase.
Using a very short name makes storing and caching of data efficient.
We only need one version of our data, which is specified by the VERSION property for the column family.
In the Java source, to connect to HBase, we first create a Configuration object and then use it with the table name to create an HTable instance.
As you can see, we don't set any ZooKeeper or HBase connection configurations in the source.
By doing this, the HBase client API will load configurations from the file hbase-site.xml from classpath.
After fetching data from MySQL using JDBC, we looped through the result set and mapped one row in MySQL to one row in the HBase table.
Here, we use stationid, month, and day, to compose the row key of the HBase data.
We also added a left padding of 0 to the month and day data.
We created one Put object for one row using the row key.
Again, we took a very short column name here to make the stored data efficient.
After all data has been set, calling HTable.put() will put the data into the table.
We closed the MySQL and HBase connections in the final block of the source to make sure it will be called even if an exception occurs during the importing.
You can verify the importing by comparing the row counts of MySQL and HBase tables.
As you can see in the scan result, data was accurately imported into HBase.
Importing data from TSV files using the bulk load tool.
HBase has an importtsv tool to support importing data from TSV files into HBase.
Using this tool to load text data into HBase is very efficient, because it runs a MapReduce job to perform the importing.
Even if you are going to load data from an existing RDBMS, you can dump data into a text file somehow and then use importtsv to import dumped data into HBase.
This approach works well when importing a huge amount of data, as dumping data is much faster than executing SQL on RDBMS.
The importtsv tool does not only load data directly into an HBase table, it also supports generating HBase internal format (HFile) files, so that you can use the HBase bulk load tool to load generated files directly into a running HBase cluster.
This way, you reduce network traffic that was generated from the data transfers and your HBase load, during the migration.
This recipe describes usage of the importtsv and bulk load tools.
We first demonstrate loading data from TSV files into the HBase table, using the importtsv tool.
We will also cover how to generate HBase internal format files and how to load generated files into HBase directly.
Getting ready We will use the NOAA CLIMATE NORMALS data in this recipe.
The downloaded data cannot be loaded directly from the importtsv tool, as its format is not supported.
We provide scripts to help you convert the data into TSV files.
Besides the actual data, the TSV file to be loaded must contain a field representing the row key of the HBase table row.
The to_tsv_hly.py script shipped with this book reads data from an hourly NOAA data file and generates the row key and output data to a TSV file on the local file system:
As the importtsv tool runs a MapReduce job to perform the importing, we need to get MapReduce running on our cluster.
Start the MapReduce daemons by executing the following command from your master node:
On HDFS, create the home directory for the hac user:
Also make sure the hac user has write permission to the MapReduce temporary directory on HDFS:
To load data from the TSV file into the HBase table using MapReduce, carry out the following steps:
Create a directory on HDFS and copy the TSV file from the local file system to HDFS:
Connect to HBase and add the hly_temp table to it:
If the table exists (we created it in the first recipe of this chapter), add a new column family to it:
Add HBase dependency JARs to the Hadoop classpath by editing the hadoop-env.
Run the importtsv tool by running the following script by the hac user:
Check the job status via the MapReduce admin page—http://master1:50030/ jobtracker.jsp:
Verify the data imported in the target table in HBase.
Here, we get row count in the hly_temp table and we also scan some sample data in the table.
The row count should be equal to the number of lines in the TSV file.
The row key in the table should be equal to the first field in the file.
The importtsv tool will only read data from HDFS, so we started by copying the TSV files from the local file system to HDFS, using the hadoop fs -copyFromLocal command.
If the table already exists, we can alter it to just add our column family to it.
All data will be loaded into the newly added column family; data in the existing column family will not be touched.
In order to run the MapReduce job, the JAR file containing the class files needs to be executed by the hadoop jar command.
In order to pass our HBase configuration to the command, we link hbase-site.xml under the $HADOOP_HOME/conf directory; all files under this directory will be added to the classpath of the Java processes that are kicked by the hadoop command.
Besides the ZooKeeper library, the guava-r09.jar file is required by the importtsv tool.
The importtsv tool itself is a Java class included in the HBase JAR file.
In step 6, we run the tool by executing the hadoop jar command.
This command will start the Java process for us and add all dependencies to it.
HBASE_ROW_KEY is a constant word specifying the row key field.
Imports the given input directory of TSV data into the specified table.
In the map phase of the job, it reads and parses rows from TSV files under the specified input directory, and puts rows into the HBase table using the column mapping information.
The Read and Put operations are executed in parallel on multiple servers, so it is much faster than loading data from a single client.
There is no reduce phase in the job, by default.
We can check the job progress, counters, and other MapReduce information on the Admin page of MapReduce.
To view data inserted in the table, we can use the scan command of HBase Shell.
We specified COLUMNS => 't:' to only scan the t column family in the table.
The importtsv tool, by default, uses the HBase Put API to insert data into the HBase table using TableOutputFormat in its map phase.
Therefore, we can then use the completebulkload tool to load the generated files into a running cluster.
The following steps are to use the bulk output and load tools:
Create a directory on HDFS to put the generated files in:
The completebulkload tool looks through the generated files, determines the regions in which they belong, and then contacts the appropriate region server.
The region server will move the adopted HFile into its storage directories and create the data online for clients.
Although the importtsv tool is very useful for loading text files into HBase, in many cases, for full control of the loading process you may want to write your own MapReduce job to import data into HBase.
For example, the importtsv tool does not work if you are going to load files of other formats.
HBase provides TableOutputFormat for writing data into an HBase table from a MapReduce job.
You can also generate its internal HFile format files in your MapReduce job by using the HFileOutputFormat class, and then load the generated files into a running HBase cluster, using the completebulkload tool we described in the previous recipe.
In this recipe, we will explain the steps for loading data using your own MapReduce job.
You don't need to do any formatting to the downloaded data file.
We assume your environment is already prepared for running MapReduce on HBase.
If it still isn't, you can refer to the Importing from TSV files using the bulk load tool recipe for details.
Follow these instructions to load data into HBase using your own MapReduce job:
Copy the raw data file from the local file system to HDFS:
Write your MapReduce Java source and then package it as a JAR file.
This class is the mapper class of the MapReduce job:
In order to run the MapReduce job, package the Java source into a JAR file, and run it from the client by using the hadoop jar command:
The output of the MapReduce job should be as shown in the following screenshot:
Map input records should be equal to the total lines of files under your input path; the Map output records value is supposed to be equal to the input record count.
You can also check the result using the HBase count/scan command.
After the instance is created, we set an input path, input format, and mapper class to the job.
The setup includes adding HBase configurations, setting up TableOutputFormat, and adding dependencies to the job.
TableMapReduceUtil is a useful utility class for writing a MapReduce program over HBase.
The running job will read all files under the input path and pass the data line by line to the specified mapper class (HourlyImporter)
In the map method of the class, we parse the line, compose a row key, create the Put object, and then add parsed data to the corresponding column by calling Put.add()
Finally, we write the data into the HBase table by calling context.write()
As you can see, writing a customized MapReduce job to insert data into HBase is very simple.
The program is similar to the one using HBase API from a single client, which we described in the Importing data from MySQL using a single client recipe.
For a large amount of data, we recommend that you use MapReduce to load the data into HBase.
Using a customized MapReduce job to load data into HBase makes sense for many situations.
However, if your data is voluminous, this approach might be not efficient enough.
There are ways to make the data migration more efficient.
Generating HFile files in MapReduce Instead of writing data directly into an HBase table, we can also generate the internal HFile format files in our MapReduce job and then load them into our cluster using the completebulkload tool that we described in the second recipe.
This approach will use less CPU and network resources than simply using the TableOutputFormat API:
Compile and package the source, and then add the output path to the command of running the job:
In step 1, we changed the source of the job configuration.
We set the job to use the PutSortReducer reducer class, which is provided by HBase.
This class will sort columns of a row before writing them out.
After the job completes running in step 2, the internal HFile format files are generated under the output path we specified.
Files under 2-3/n, which is the column family directory, will be loaded by the completebulkload tool into our HBase cluster.
During the MapReduce job execution, if you open the HBase admin page from your browser, you will find that no request comes to HBase.
This indicates that data is not written into the HBase table directly.
Important configurations affecting data migration If you write data directly into the HBase table using TableOutputFormat from your MapReduce job, it could be a very write-heavy job on HBase.
Although HBase is designed to be able to handle writes quickly, there are some important configurations you might want to tune, such as the following:
You will need basic knowledge of HBase architecture to understand how these configurations affect the write performance of HBase.
There are several types of logs generated by the Hadoop and HBase clusters.
Checking the logs gives you hints to find out the bottleneck of your cluster during the MapReduce data loading job.
When we create a table in HBase, the table starts with a single region.
All data inserted into the table goes to the single region, first.
Data keeps being inserted, and when it reaches a threshold, the region will be split into two halves.
Split regions will be distributed to other region servers, so that the load can be balanced among the clusters.
As you can imagine, if we can initialize the table with precreated regions, using an appropriate algorithm, the load of the data migration will be balanced over the entire cluster, which increases data load speed significantly.
We will describe how to create a table with precreated regions in this recipe.
This command calls the RegionSplitter class with the following parameters:
Open the HBase admin page from your browser, and click hly_temp2 in the User Tables section.
If you run the data load job discussed in the Writing your own MapReduce job to import data recipe, you may expect data writes to be distributed to all region servers in the cluster, but the result is different.
From the admin page, you will find that all requests go to the same server during the MapReduce job execution time.
This is because the default splitting algorithm (MD5StringSplit) does not fit our case.
All of our rows drop into the same region, thus all the API requests go to the region server that holds the region.
We need to provide our custom algorithm to split regions properly.
Presplitting regions also changes the behaviour of the MapReduce job that generates the internal HFile format files.
Run the job in the Writing your own MapReduce job to import data recipe, with the option for generating HFile files turned on in the hly_temp2 table.
This is because the reducer count of the job is based on the region count of the target table.
If the reducer count increases, it normally means that the load was distributed to multiple servers, and thus the job will be much faster.
Introduction Everyone expects their HBase administrator to keep the cluster running smoothly, storing a huge amount of data in it, handling maybe millions of requests simultaneously, quickly, and reliably.
Keeping a large amount of data in HBase accessible, manageable, and easy to query, is a critical task for an administrator.
Besides a solid knowledge of the cluster you are operating, just as important are the tools you use.
HBase ships with several administration tools to make life easier.
There is a web-based administration page; on this page you can view the status of the cluster and execute simple administration tasks such as region splitting.
This command-line tool has features to create and manage HBase tables, to insert and view data in the tables, and also has methods to manage the cluster itself.
HBase also provides a bunch of Java utilities with its installation.
You can import and use these utilities directly from your HBase Shell prompt, which makes administration tasks much more efficient.
We have HBase tools to deal with its internal Write Ahead Log (WAL) and HFile files, too.
These tools are useful for advanced users, who need to dig deeper into how data is stored in HBase.
HBase hbck is the tool that can be used to check the health of the cluster.
In production, you might want to run hbck frequently, so that you can find out problems quickly.
Although HBase has its own commands to access its data, the functionality is very limited.
We can map HBase tables and columns to a Hive table so that it is possible to perform complex queries over HBase data using HQL.
In this chapter, we will describe the usage of these tools.
We will explain what the tools are for, and how to use them to resolve a particular task.
HBase Master web UI The HBase Master web UI is a simple but useful tool, to get an overview of the current status of the cluster.
From its page, you can get the version of the running HBase, its basic configuration, including the root HDFS path and ZooKeeper quorum, load average of the cluster, and a table, region, and region server list.
Furthermore, you can manually split a region using a particular boundary row key.
This is useful when you turn off the automatic region splitting of your cluster.
Getting ready Make sure the port for your master page, which has a default value of 60010, is opened to your client computer from your network firewall.
As you can see, the Attributes section shows information about the HBase and Hadoop version, the HBase root directory on HDFS, load average of the cluster, and the ZooKeeper quorum.
The HBase root directory and ZooKeeper quorum data are the values set in your HBase configuration file hbase-site.xml.
The load average is the average number of regions per region server.
A single region server's load is shown in the Region Server section.
There are two tables shown in the Catalog Table section, -ROOT- and .META..
The -ROOT- table holds references of the region servers, where all the .META.
The User Table section displays a list of all user tables and their column families' properties.
Clicking the link of the table name from the list will bring you to the table's details page.
You will see a list of regions of that table, displayed on this page.
You can also compact or split regions manually on the table's details page.
The region key is optional for the compaction and splitting of regions.
If specified, HBase will only take action on the region where the key drops in.
If it is not specified, all the regions are targeted by this action.
The Region Server section shows all online region servers and their load.
If you are sending lots of requests to the cluster, you can see which server the requests went to, from this section.
Clicking the link of the region server address will show the details page of the clicked server.
On the details page, you can see the metrics data of the server and all the regions deployed on that server, in detail.
The region server's details page is a simple, but very important interface for knowing the insight information of a region server.
As shown in the previous screenshot, it displays very important metrics of the region server.
Using HBase Shell to manage tables HBase Shell is a command-line tool shipped with HBase.
It provides basic functions to manage tables, access data in HBase, and manage the cluster.
The group for managing tables is called Data Definition Language (DDL)
Using DDL group commands, you can create, drop, and change HBase tables.
The following steps will show you how to use DDL commands to manage HBase tables:
Execute the following command from the client node, to start an HBase Shell prompt:
Enter the following commands to disable the table again and drop it:
HBase Shell is started by using the hbase shell command.
This command uses the HBase configuration file (hbase-site.xml) for the client to find the cluster to connect to.
After connecting to the cluster, it starts a prompt, waiting for commands.
As shown in the following code, you can also use the --config option, which allows you to pass a different configuration for HBase Shell to use:
The list command is used to show all tables in the cluster.
After the table is created, we can show its properties by using the describe command.
The describe command also displays whether the table is enabled.
In order to disable a table, use the disable command.
We can only change a table's properties after it is disabled.
All properties shown by the describe command can be changed using the alter command.
We can use the enable command to enable a table in HBase.
To drop a table in HBase, first disable it and then use the drop command to drop the disabled table.
A single help command prints all the available commands and general usage of HBase Shell.
To show the detailed description of a command, pass its name to the help command.
For example, help 'create' displays the detailed use of the create command:
Create table; pass table name, a dictionary of specifications per.
The DML group includes the commands count, delete, deleteall, get, get_counter, incr, put, scan, and truncate.
Just as their names express, these commands provide basic access and update operations on data in HBase.
HBase has a feature called counter, which is useful to build a metrics gathering system on HBase.
The count, scan, and truncate commands may take time to finish when running them on a huge amount of data in HBase.
To count a big table, you should use the rowcounter MapReduce job, which is shipped with HBase.
We will describe it in the Row counter recipe, later in this chapter.
Getting ready Start your HBase cluster, connect to the cluster from your client, and create a table called t1, if it does not exist.
The following steps are demonstrations of how to use DML commands for accessing data in HBase:
Insert the following data into the table by using the put command:
Get a row count of the table by running the count command:
Don't forget to specify the LIMIT property when invoking scan on a table with a huge amount of rows:
Execute the get command again; you will see that the f1:c1 cell has been deleted from the row:
Delete all the cells in a given row using the deleteall command:
Execute the get command again; you will see that the entire row1 row has been deleted from the table:
Get the new counter value by using the get_counter command:
Use single quotes around the value if you want HBase Shell to treat it as a string value; otherwise, HBase Shell will try to guess the data type of the value you put.
You will have a numeric value of 0 put in the table:
You will get a string of 0000 in the table:
The count command counts the number of rows in a table.
It counts an HBase table on row basis and shows the row number on its output.
This command may take a while to finish, when counting a large amount of data in HBase.
In this case, a better option is to use the Row Counter utility provided by HBase, which will run a MapReduce job on HBase for counting it.
You can specify the row range, columns to include, time range, and filter for the scanning.
The help command has a very detailed explanation of the usage of this command; type help 'scan' for more information.
An alternative way of scanning an HBase table is to use Hive over HBase; it is the easier option, for scanning a table with complex conditions.
The get command is very straightforward; you must give the exact row key you want, to get to the command.
While you can optionally specify the range of columns, timestamp, or versions you want to include in the result, you can also restrict the output length by specifying the MAXLENGTH parameter.
The delete command marks a cell as deleted, while the deleteall command deletes all the cells in a given row.
HBase counters are distributed, atomic counters, which are stored as the value of a specified cell.
At specified table/row/column coordinates, use the incr command to increase a counter, and use the get_counter command to get the counter value.
The truncate command acts as a sequence for disabling, dropping, and recreating a table.
Using HBase Shell to manage the cluster There are a bunch of HBase Shell commands for managing the cluster.
Warning Many of these commands are for advanced users only, as their misuse can cause unexpected damages to an HBase installation.
The tool's group commands provide an interface to manage HBase regions manually.
Although HBase does all these operations automatically, by default, there are situations wherein you may want to balance your region server's load manually.
This is especially true when the default, balancing algorithm does not work well for your data access pattern.
In this recipe, we will describe how to manually flush, compact, split, balance, and move HBase regions.
Getting ready Start your HBase cluster, create a table, and put some data into it.
We assume that the table you are using already has several regions in it.
As shown in the following screenshot, you can view the table's region list by clicking the table's link shown on your HBase web UI:
We will start by manually flushing and compacting regions, and then split the regions manually.
Finally, we will rebalance these regions to make them well-balanced within the cluster.
Flush all regions in the table using the flush command:
You can also flush an individual region by passing the region name to the flush command:
You can find the region's name under the Table Regions section of the table's administration page:
Compact all the regions in a table by running the compact command:
Run a major compaction on a table by running the major_compact command:
Split a region in a table by running the split command:
In the table's administration page, you will find that the region has been split into two regions, so the total region count becomes four:
Balance the load of the cluster by using the balancer command:
The output true indicates that a balancing call has been triggered successfully.
It will run in the background on the master server.
Move a region to a specific region server by using the move command:
An HBase data edit will first be written to the Write Ahead Log (WAL) and then to the MemStore of a region server.
The edits will not be flushed to disk until the size of MemStore has reached a threshold.
The flush command flushes these pending records to disk synchronously.
It takes either a table name, or a region name as its parameter.
A region name consists of its table name, region start row key, region server start code, and a hash-encoded value of itself.
You can find region names under the Table Regions section of the table's administration page.
Just like the flush command, if instead of a table name, a region name is passed to both compact and major_compact commands, the compaction will be performed on the specified region only.
Executing these commands queues the tables or regions for compaction, which will be executed in the background by the server hosting the specified region, or all regions of the table.
You might want to execute major compaction manually, only at a low load time.
This is because lots of disk IO and network traffic might happen during the compaction, and it might take a long time to complete.
The split command allows you to split a specific region or all regions in a table, if the table name is given to the command.
When running HBase with very high write load, we recommend you turn off HBase's automatic region split feature, precreate regions using your own algorithm, and use the split command to split a specific region if it becomes too large.
We will cover how to presplit tables in Chapter 9, Advanced Configurations and Tuning.
In order to balance the cluster, HBase runs its balancing functionality periodically in the background.
On the other hand, you can also explicitly enable the balancer by running the balance_switch command, and then trigger a balancing operation by using the balancer command.
It is also useful to stop the balancer when you need to stop a region server for maintenance.
As you can see, the second region server holds almost all the regions (five of all the six regions) in the cluster, which means most of the cluster load will go to that server:
After the balancing operation was executed, our cluster became much more balanced as regions were distributed to all the region servers in the cluster:
The default, load-balancing algorithm just simply causes each region server to take a large number of regions deployed on it.
If this algorithm does not fit your data access pattern, you can manually move a region to a specific server by using the move command.
The move command takes an encoded region name and the target server name as its parameters.
An encoded region name is the hash code suffix of a region name.
A move call will close the region from its originally deployed server, open it on the target server, and then finally update the record of the region in the .META.
See also f Precreating regions using your own algorithm recipe, in Chapter 9, Advanced.
HBase ships with many Java utility classes; the ability to execute Java methods from HBase Shell makes it possible to import and use these utilities directly from HBase Shell.
We will demonstrate two examples of how to call Java method from HBase Shell, in this recipe.
The first one converts the timestamp of the HBase Shell output into a readable date format.
The second one imports an HBase filter class, and performs the filtering on the scanner of the scan command.
Getting ready Start your HBase cluster, create a table, and put some data into it.
Connect to your cluster via HBase Shell, before you start.
To convert the timestamp of an HBase Shell output into a readable date format:
Enter the following command to get a row from HBase:
Import the Java Date class to convert the output of timestamp, and then create an instance of the Date class using the timestamp of the get command:
To use the HBase filter on a scan, import the following classes into HBase Shell:
As you can see from the output of step 1, the displayed timestamp is not human-friendly to read.
We want to convert it to a normal date format.
The conversion can be easily done by using the java.util.Date class.
In step 2, we import it into our shell session by using the import command.
The import command is a basic JRuby feature, and it is not HBase-specific.
After the Date class is imported, we create an instance of the class using the timestamp we want to convert.
Finally, we call its toString() method to convert and print the timestamp to a normal date format.
This class is very useful, because all data in HBase is stored as byte arrays.
Using this class with the scan command makes it only pass the data that has a specific row key prefix to the client.
The prefix is specified by passing its byte array representation to the constructor of the PrefixFilter class.
We specified the prefix AQW00061705010, converted it to a byte array using the Bytes class, created an instance of the PrefixFilter class using the converted byte array, and then set it as the filter of the scan command.
There are many other useful Java classes shipped with HBase.
Have a glance at the following packages of HBase API:
Row counter The count command in HBase Shell is a straightforward way to count the row numbers on an HBase table.
However, running the count command on a table with a huge amount of data might take a long time to complete.
A better approach for this case is to use the RowCounter class.
This class will kick a MapReduce job to count the row number on a table, which is much more efficient than the count command.
We will describe the usage of RowCounter in this recipe.
Getting ready Make sure your Hadoop and HBase clusters are running.
MapReduce is also required; if it is not running, start it by using the following command on your JobTracker server:
To run a row counter MapReduce job on the hly_temp table, follow these steps:
Add a ZooKeeper JAR file to the Hadoop class path on your client node:
When we run a JAR file by using the hadoop jar command, the dependencies of the JAR file must be included in Hadoop's class path.
Parameters after the name of the JAR file are passed to the main class of the JAR file.
To run a row counter on the hly_temp table, we pass a fix string of rowcounter and the table name to the command.
The RowCounter class will take hly_temp as its parameter and finally kick the MapReduce job for us.
You can also specify which column or column family to count, by passing the space separated column (family) names after the table name.
Besides row counter, there are other MapReduce jobs you can execute from the command line.
An example program must be given as the first argument.
When running RowCounter against large datasets, we recommend you to tune the scanner's caching setting on your client, as shown in the following snippet:
This makes HBase fetch 1000 rows for each next call on the scanner.
Setting it to a higher value increases the scanner's speed, but consumes more memory.
An HBase edit will first be written to a region server's Write Ahead Log (WAL)
After the log is written successfully, MemStore of the region server will be updated.
As WAL is a sequence file on HDFS, it will be automatically replicated to the two other DataNode servers by default, so that a single region server crash will not cause a loss of the data stored on it.
As WAL is shared by all regions deployed on the region server, the WAL needs to first be split so that it can be replayed on each relative region, in order to recover from a region server crash.
HBase handles region server failover automatically by using this algorithm.
HBase has a WAL tool providing manual WAL splitting and dumping facilities.
We will describe how to use this tool in this recipe.
Getting ready We will need to put some data into an HBase table to have HBase generate WAL files for our demonstration.
Again, we will use the hly_temp table in this recipe.
We will put the following data into the hly_temp table using the HBase Shell Put command:
These commands will have HBase add four entries to its WAL on HDFS.
Also, you will need to find out which region holds the row we put in the previous commands, and which region server holds that region.
In the Table Regions section, you will find a Start Key and an End Key for each region.
As the data in HBase is sorted by row key, in lexicographical order, a row belongs to the region that has the closest start/end key of the row key.
The following table is transferred from the Table Regions section of our HBase web UI.
Follow these instructions to manually dump and split WAL logs:
The WAL file of our previous put operations will be generated under the following directory on HDFS:
Execute the WAL tool with the dump option to dump the WAL file:
To manually split the WAL files, switch to the user who started Hadoop and HBase; in our case it is the hadoop user:
Execute the WAL tool with the split option to split the WAL files:
Do not manually split WAL if the hosting region server is online, as splitting WAL is only for closing a region server, or for recovery from a region server crash.
When a region server receives an edit, it will append it (a KeyValue instance) to its WAL files before actually updating its MemStore data.
If the size of MemStore reaches the threshold (128MB by default), edits will be persisted onto HDFS by flushing them to the HFile files.
You can also dump the JSON format WAL file by passing -j to the command.
Here is a sample JSON output of a WAL dump:
When passing the --dump option to the HLog class, it internally invokes the HLogPrettyPrinter class to dump the WAL file.
There are also options to filter by region or sequence number.
The dump output has information about the table name, region, row, column, timestamp, and value of the edit.
There's also a sequence number for each entry of WAL, which is used to keep a sequential order of edits.
The highest sequence number of edits, which have been persisted, are written to a meta field of each HBase storage file (HFile)
Passing --split to HLog makes it kick a splitting task over a specific WAL directory.
We must pass the directory of WAL files to the split command, as all WAL files have to be split and replayed, to recovery from a region server crash.
All the edits that have been recorded in the WAL files but not yet been persisted, will be replayed during the splitting.
HFile files containing the edits will be generated at proper directories after the splitting.
The following is the HFile file generated by our WAL splitting:
You can use the HFile tool to view the content of the generated file:
The content is exactly the same as what was put in the Getting ready section of this recipe.
All edits are replayed successfully; no data is lost during the splitting.
For almost all situations, you won't need to split WAL manually, as the master server will automatically start the splitting if a region server crashes.
For a case where you have many nodes going down uncleanly, you could use this manually splitting option in parallel on many machines, to speed up the recovery.
As of HBase 0.92, there is a new feature called Distributed Log Splitting, with which this WAL splitting is automatically distributed.
See also f The HFile tool—viewing textualized HFile content recipe, in this chapter.
HFile is the internal file format for HBase to store its data.
These are the first two lines of the description of HFile from its source code:
We don't need to know the details of HFile for our administration tasks.
However, by using the HFile tool, we can get some useful information from HFile.
The HFile tool provides the facility to view a textualized version of HFile content.
We can also get the metadata of an HFile file by using this tool.
Some metadata, such as entry count and average Key/Value size, are important indicators of performance tuning.
We will describe how to use an HFile tool to show textualized content and metadata of HFile files.
We will use the hly_temp table in this recipe, for demonstration purposes.
Follow these instructions to view textualized content or metadata of HFile files:
Type the following to show the textualized Key/Value content of an HFile file:
To show the metadata of an HFile file, use the -m option in the HFile tool:
Get the total entry count of a specific region, with the following command:
The total entry count of the region is the sum of entries from the output.
We can also use this class as a tool for the HFile files.
In step 1, we simply show the textualized content of an HFile file.
The -p flag tells the command to include the content of the Key/Value pair to its output as well.
The HFile tool simply scans all the blocks of the specific HFile file, and prints all Key/Value entries to its output.
Note that the deleted cell also has an entry in the HFile file until a major compaction is executed on that region.
In step 2, we pass the -m flag to the HFile tool to display the metadata of a specified HFile file.
As you can see from the output, metadata has very useful information about the HFile file.
This information includes the block index size, average Key/Value length, entry count, and so on.
A typical use case is to get the total entry count of each region in a table, so that we are able to know whether our data is well-balanced among all regions in the table.
Passing a region name with the -r option to the HFile tool makes it perform tasks on each HFile file belonging to the region.
Collaborated with the -m flag, it displays the metadata for each HFile file in the region.
Having displayed the metadata for each HFile file, we can simply sum the output entry count to get a total Key/Value entry count for that region.
HBase provides the hbck command to check for various inconsistencies.
The name hbck comes from the HDFS fsck command, which is the tool to check HDFS for inconsistencies.
The following is a very easy-to-understand description from the source of hbck:
Check consistency among the in-memory states of the master and the region server(s) and the state of data in HDFS.
HBase hbck not only has the facility to check inconsistencies, but also the functionality to fix an inconsistency.
In production, we recommend you run hbck frequently so that inconsistencies can be found earlier and fixed easily.
In this recipe, we will describe how to use hbck to check inconsistencies.
We will also make some inconsistencies to the cluster and then demonstrate how to use hbck to fix it.
Getting ready Start up your HBase cluster, and log in to your HBase client node.
The instructions to check and fix the inconsistencies of an HBase cluster using hbck are as follows:
Check the health of the cluster with the default hbck command option:
At the end of the command's output it prints Status: OK, which indicates the cluster is in consistent status.
To demonstrate the fixing feature of hbck, make some inconsistencies by typing the following in HBase Shell:
Replace the last parameter with your region name, which can be found on the HBase web UI.
Run hbck again; you will find that, at the end of its output it reports the status of the cluster as inconsistent:
Use hbck with the -fix option, to fix the inconsistencies:
In this code, we skipped some output of the command to make it easy to understand.
In step 1, we run the hbck command without any parameter.
This will simply have hbck check the inconsistencies of the cluster.
If no inconsistency is found, it will report Status: OK at the end of its output; otherwise, inconsistencies will be reported.
The close_region command logs in via SSH into the region server that holds the specific region, and closes it directly.
The master server will not be notified in this case.
That's why, our cluster became inconsistent by the execution of the close_region command.
When we run hbck again in step 3, the command will find the inconsistent status of our cluster.
As you can see from the output of the command, the region we closed in step 2 was reported as not being deployed on any region server—this is the result we expected.
If you run a get command to get a row in that region, HBase will report an error to you, as the region is offline.
In step 4, we execute hbck with the -fix option.
As you can see from the output of step 4, specifying the -fix option caused hbck to first check the inconsistencies of the cluster; if inconsistencies are found, it will try to fix it automatically.
After that, a check is performed again to ensure the fixing works properly.
Now, you can get data from the fixed region again.
There are other options of the hbck command, such as displaying a full report of all regions, checking ROOT and META tables only, and so on.
See also f Simple scripts to report status of the cluster recipe, in Chapter 5, Monitoring.
HBase supports several interfaces to access data in its tables, such as the following:
HBase Shell is straightforward, but a little too simple to perform complex queries on.
Other interfaces need programming, which is not suitable for ad hoc queries.
As data keeps growing, people might want an easy way to analyze the large amount of data stored in HBase.
The analysis should be efficient, ad hoc, and it should not require programming.
Hive is used for ad hoc querying, and analyzing a large data set without having to write a MapReduce program.
Hive supports a SQL-like query language called HiveQL (HQL) to access data in its table.
We can integrate HBase and Hive, so that we can use HQL statements to access HBase tables, both to read and write.
In this recipe, we will describe how to install and run Hive over HBase tables.
We will show simple demonstrations of reading/writing data from/into HBase tables using Hive.
Getting ready Hive stores its metadata in RDBMS such as MySQL or PostgreSQL.
We will use MySQL in this recipe, so a running MySQL installation is required.
Here is the online MySQL 5.5 Reference Manual to help you install MySQL:
Hive only needs to be installed on your client node.
We assume you have installed Hive on the same node of an HBase client (client1 in our case)
We will use the user hadoop as the owner of all Hive files.
All Hive files and data will be stored under /usr/local/hive.
Create a /usr/local/hive directory on your client node in advance; change the owner of the directory to the hadoop user.
Besides a running HBase installation, you will need to start MapReduce to use Hive to query the HBase data.
We will load data from a TSV file into Hive, and then write the data from Hive to the HBase table.
Just as a demonstration, our TSV file (hly_temp.tsv) will only contain the following data:
Put the hly_temp.tsv file under /home/hac; we will load it into Hive later.
To install Hive and run it over HBase, follow these instructions:
While this book was being written, the latest release was 0.8.0
Download the tarball and decompress it to our root directory for Hive.
Add dependencies to the Hive library: Download the JDBC driver for MySQL from http://www.mysql.com/downloads/connector/j/
Replace the HBase and ZooKeeper JAR files with the versions we are using:
You will need to adjust the MySQL server, password, and the ZooKeeper quorum setting to your environment:
We will use the hac user to run the Hive job in this recipe.
Switch to the user and test the Hive installation from the Hive Command Line Interface (CLI)
Here we just create, show, and drop a Hive table to confirm our Hive installation:
Query HBase data from the table we mapped in Hive:
Create a Hive table (hive_hly_temp2) to store data from the TSV file:
Create a Hive-managed HBase table and map it with a Hive table:
Confirm the data in HBase, which we inserted via HQL in the previous step:
We also need to create a /tmp directory on HDFS, so that Hive can save its temporary files under it.
In step 5, we add the JDBC driver for MySQL to the Hive library folder.
We need to do this because we will configure Hive to store its metadata in MySQL.
The JDBC driver is required to access MySQL from Hive.
As HBase and ZooKeeper JAR files, shipped with Hive, are out of date, we replace them with the files from our HBase installation.
When Hive starts a MapReduce job, all JAR files under this directory will be shipped to TaskTrackers via Hadoop Distributed Cache, and added to the job task's class path.
The JAR files we put under $HIVE_HOME/auxlib are dependencies of all Hive MapReduce jobs.
We recommend you to limit this user's privileges to prevent it from creating or altering tables in the metastore database schema.
We need to set up HADOOP_HOME in Hive's environment setting file (hive-env.sh) to notify to Hive the location where Hadoop is installed.
Metastore database and ZooKeeper quorum settings are set in Hive's configuration file (hive-site.xml)
Hive will use these settings to connect to the Metastore database and HBase cluster.
If everything is set up properly, you should be able to connect to Hive using its Command Line Interface (CLI)
In step 12, we create an external Hive table to map an existing HBase table.
An external Hive table is defined by the create external table statement.
When dropping an external table, the data is not deleted from HBase.
There must be exactly one :key entry for mapping HBase row keys.
It is not necessary to map every HBase column, but only mapped columns are accessible from Hive.
If not specified, then the table will have the same name in Hive and in HBase.
For most of the cases, a Hive query will be translated into a MapReduce job.
Note that this table is not an external table as we have not specified the external keyword in the creator.
Dropping this table from Hive will drop the table from HBase, too.
After executing the command in step 15, you should be able to see the new (empty) table in HBase Shell.
As the table was created, we can insert data into it using the insert overwrite HQL statement.
As you can see, integrating Hive with HBase provides a powerful data access interface on top of HBase.
While the drawback is that some HBase features, such as timestamp/version support, composite row key mapping, and updating on a single HBase row from Hive, are not supported.
Although it is not meant for latency-sensitive systems, as it is still a MapReduce job each time, Hive integrating is a good add-on to HBase, for ad hoc querying capabilities.
Introduction If you are thinking about using HBase in production, you will probably want to understand the backup options and practices of HBase.
The challenge is that the dataset you need to back up might be huge, so the backup solution must be efficient.
It is expected to be able to scale to hundreds of terabytes of storage, and finish restoring the data in a reasonable time frame.
A full shutdown backup has to stop HBase (or disable all tables) at first, then use Hadoop's distcp command to copy the contents of an HBase directory to either another directory on the same HDFS, or to a different HDFS.
To restore from a full shutdown backup, just copy the backed up files, back to the HBase directory using distcp.
The CopyTable utility could be used to copy data from one table to either another one on the same cluster, or to a different cluster.
The Export utility dumps the data of a table to HDFS, which is on the same cluster.
As a set of Export, the Import utility is used to restore the data of the dump files.
The benefit of a full shutdown backup is that there is no chance of writing data into the cluster during the backup process, so it is possible to ensure a consistent status of the backup.
As for the live cluster backup approach, since the cluster is up, there is a risk that edits could be missed during the backup process.
Furthermore, as HBase edits are atomic only at the row level, if your tables depend on each other, your backup might end up with inconsistencies if the tables are being modified while Export or CopyTable is being performed.
Making a snapshot of HBase tables is not supported in the current Apache Release.
It is a way to copy data between the different HBase deployments.
Cluster replication can be considered as a disaster recovery solution at the HBase level.
Besides the tables, what you will likely want to back up are your HDFS metadata and HBase region starting keys.
A metadata corruption might damage the entire HDFS metadata; it is suggested to back up the metadata frequently.
Backing up the region starting keys makes it possible to restore not only the data, but also the data distribution.
If we split the tables in advance, using well-distributed region starting keys, the speed of restoring data by using the CopyTable/Import utility could be increased dramatically.
In this chapter, we will describe how to back up HBase data using the aforementioned approaches, their pros and cons, and which approach to choose, depending on your dataset size, resources, and requirements.
Full shutdown backup using distcp distcp (distributed copy) is a tool provided by Hadoop for copying a large dataset on the same, or different HDFS cluster.
It uses MapReduce to copy files in parallel, handle error and recovery, and report the job status.
As HBase stores all its files, including system files on HDFS, we can simply use distcp to copy the HBase directory to either another directory on the same HDFS, or to a different HDFS, for backing up the source HBase cluster.
The distcp tool works because the HBase cluster is shut down (or all tables are disabled) and there are no edits to files during the process.
Therefore, this solution is for the environment that can tolerate a periodic full shutdown of their HBase cluster.
For example, a cluster that is used for backend batch processing and not serving frontend requests.
We will describe how to use distcp to back up a fully shut down HBase cluster to a different HDFS cluster.
Backing up on a different cluster can serve as a disaster recovery solution and can contribute to higher availability of data.
In production, it is recommended to back up a fully shut down HBase cluster to a different HDFS cluster.
We will also demonstrate how to restore data from the backup, later in this chapter.
Getting ready You will need a secondary cluster if you are going to use distcp to back up your HBase data to a different cluster.
You could also back up HBase data to the same cluster as your source HBase cluster.
In this recipe we will use a different cluster for backup.
This is because we will use this cluster for our performance tuning recipes later, while small instance specification is too low for that usage.
The distcp tool uses MapReduce to copy files in parallel, so you will need to start MapReduce on the source cluster as well.
MapReduce can be started from the JobTracker node, using the following command:
We will back up the HBase directory under the /backup directory, on the backup cluster.
Create this directory in advance, from the Hadoop client (l-client1) of the backup cluster, using the following command:
Follow these instructions to back up/restore HBase data using distcp:
Shut down the source and backup HBase clusters, if they are running:
Make sure HBase has shut down on the source cluster by checking if the HMaster daemon has started:
Make sure the HMaster daemon is not listed in the output.
If it is set to final and true, remove the setting.
This is a client-side change; it will only affect the MapReduce jobs submitted from that client.
Use distcp to copy the HBase root directory from the source cluster to the backup cluster.
The HBase root directory is set by the hbase.rootdir property in the HBase configuration file (hbase-site.xml)
You will find a running MapReduce job of the source cluster from the admin page of your JobTracker:
After the distcp job is completed, you should be able to find the copied HBase directory under the /backup directory on the backup cluster:
The steps for restoring data from the previous backup are as follows:
Start MapReduce on the backup cluster by using the following command:
Make sure HBase is not running on the source and backup clusters.
Make sure there are no files under the hbase.rootdir directory on the source cluster.
You may remove the files if you are sure they are not required.
Copy the HBase directory from the backup cluster to the source cluster:
It shows all the Java processes owned by the executed user.
As all HBase-related daemons are started by the hadoop user in our case, we can use the jps command to see whether HBase's master daemon (HMaster) is running.
But a final property cannot be changed in a single MapReduce job.
In step 3, we remove the final setting to enable distcp to configure this property in its MapReduce job.
The parameters include the full path of the source directory, including its HDFS schema, and the HDFS schema and path of the destination directory on the backup cluster.
As you can see from its output, distcp starts a MapReduce job to copy data in parallel.
The MapReduce job started by distcp copies data only in its Map phase.
The maximum number of maps is specified by the -m option.
Note that this is just a hint to the MapReduce job; more maps may not always increase the number of simultaneous copies or the overall throughput.
In order to restore from the backup, we copy the data back from the backup cluster to the source cluster and then start HBase on the source cluster.
For a full list of the distcp options, type the following command:
Using CopyTable to copy data from one table to another CopyTable is a utility to copy the data of one table to another table, either on the same cluster, or on a different HBase cluster.
You can copy to a table that is on the same cluster; however, if you have another cluster that you want to treat as a backup, you might want to use CopyTable as a live backup option to copy the data of a table to the backup cluster.
CopyTable is configurable with a start and an end timestamp.
If specified, only the data with a timestamp in the specific time frame will be copied.
This feature makes it possible for incremental backup of an HBase table in some situations.
Note: Since the cluster keeps running, there is a risk that edits could be missed during the copy process.
In this recipe, we will describe how to use CopyTable to copy the data of a table to another one, on a different HBase cluster.
Getting ready On the client node, you will need to add the HBase configuration file (hbase-site.xml) to Hadoop's class path, so that a MapReduce job can access the HBase cluster.
You can do it by linking hbase-site.xml under the Hadoop configuration directory, as follows:
Also, add HBase dependency JARs to Hadoop's class path, by editing hadoop-env.sh:
The aforementioned steps are the minimum requirements to run MapReduce over HBase.
And since this is a common process, we will skip its details henceforth, in this book.
If you do not have another cluster, you could also copy a table to another table on the local cluster.
However, it is not recommended for backup purposes in production.
We will assume that the ZooKeeper quorum of the backup cluster is l-master1:2181, and the HBase root directory is /hbase.
CopyTable uses MapReduce to copy data in parallel; you will need to start MapReduce on the source cluster, using the following command:
Connect to the backup HBase cluster via HBase Shell and create the destination table if it does not exist:
You will find a running MapReduce job of the source cluster from the admin page of your JobTracker:
Type the following command to only copy the data that has a specific timestamp range:
The backup cluster is specified by the --peer.adr option, by specifying the ZooKeeper quorum address, whose format is hbase.zookeeper.
The --families option is used to specify a comma-separated list of families to be copied.
The destination table is specified by the --new.name option; skip this option if the two tables have the same names.
As you can see from the output, data is copied in a MapReduce job.
The hadoop jar command is used to run a JAR file in MapReduce.
The MapReduce job scans the source table, reads data entries belonging to the target families, and then writes them into the destination table on the backup cluster using its normal client API.
HBase uses timestamps to order edits; edits with a bigger timestamp have the newer version for the cell.
Data out of the specific time frame will be skipped in the scanning phase of the copy job.
The minimum timestamp value, which is specified by the --starttime option, is inclusive.
The maximum timestamp value, which is specified by the --endtime option, is exclusive.
Using CopyTable with a specific timestamp range is not identical to an incremental backup.
This is because, when putting data into HBase, the timestamp can be specified explicitly by the client, and there is no insurance that the newly inserted data has a larger timestamp.
There is no way to know if data was deleted.
We recommend you to implement your own incremental backup solution at the application level.
For a full description of CopyTable usage, type the following command:
The HBase export utility dumps the contents of a table to the same HDFS cluster.
The dump file is in a Hadoop sequence file format.
Exporting data to Hadoop sequence files has merits for data backup, because the Hadoop sequence file format supports several compression types and algorithms.
With it we can choose the best compression options to fit our environment.
Like the copytable utility we mentioned in the previous recipe, export is configurable with a start and an end timestamp, so that only the data within a specific time frame will be dumped.
This feature enables export to incrementally export an HBase table to HDFS.
As the cluster is running, there is a risk that edits could be missed during the export process.
In this recipe, we will describe how to use the export utility to export a table to HDFS on the same cluster.
We will introduce the import utility in the next recipe, which is used to restore the data from an export dump.
You will need to create a /backup directory in advance.
Add HBase's configurable file (hbase-site.xml) and dependency JAR files to Hadoop's class path on your client node.
Follow these instructions to export an HBase table to HDFS using the export utility:
Run the following command from your client node to export all the data in the hly_ temp table:
You will find the export MapReduce job from the admin page of your JobTracker:
Execute the following command to only export the data that has a specific timestamp range:
Running an HBase JAR file with the export argument will execute the export utility to dump the contents of an HBase table.
The last parameter specifies the maximum version of the data to export.
HBase supports storing multiple versions of data within a cell.
The maximum version a cell can store is determined by its column family's VERSIONS property, which is specified at the time of creating a table.
To dump a table with multi-version column families, we must pass the maximum version we want to export to the export command, otherwise only the latest version will be exported.
Many -D properties have been passed to the export command.
These are set to the export MapReduce job controlling its runtime properties.
We have set it to use the BZip2 codec, which has the highest compression ratio.
When this book was being written, these were the widely used codecs for Hadoop.
Which codec to choose, depends on your requirement; all of them have a space/time trade-off:
For most cases, you should choose BLOCK as it compresses sequences of records together in blocks, which is the most efficient way for compression.
As you can see from the output, data is exported via MapReduce.
This is done by the hadoop jar command, which runs a JAR file in MapReduce.
The MapReduce job scans the table, reads all the data entries, and then writes them to the Hadoop sequence files under the output path on the same HDFS cluster.
Data out of the specific time frame will be skipped in the scanning phase of the export job.
As data is exported to HDFS on the same cluster, there is a risk that backups will not be available if the HDFS crashes.
We recommend you to copy the exported files to a different HDFS cluster, using the Hadoop distcp tool.
If your data size is not very large, an even better option is to copy the dumped files to tapes, for offline backup.
See also f Restoring HBase data by importing dump files from HDFS recipe, in this chapter.
The HBase Import utility is used to load data that has been exported by the Export utility into an existing HBase table.
It is the process to restore data from the Export utility backup solution.
We will look at the usage of the Import utility in this recipe.
We will import the files that we exported in the previous recipe into our hly_temp table.
If you do not have those dump files, refer to the Exporting HBase table to dump files on HDFS recipe, to generate the dump files in advance.
We assume the dump files are saved in the /backup/hly_temp directory.
Add the HBase configurable file (hbase-site.xml) and dependency JAR files to Hadoop class path on your client node.
Connect to your HBase cluster via HBase Shell and create the target table if it does not exist:
Run the following command from your client node to import data into the hly_temp table:
You will find the import MapReduce job from the admin page of your JobTracker:
As you can see from the output of step 2, data is imported via MapReduce.
This was done with the hadoop jar command, which executes the MapReduce code that is packaged in a JAR file.
Data in the dump files, including the timestamp of each entry, will be restored by the Import utility.
After that, we recommend you to run a major compaction on the table and perform cluster balancing manually, to cause the cluster to run in the best status.
This can be done via HBase Shell by using the following command:
If you are importing data into a newly created table, you may find that the importing speed is not as fast as you had expected.
This is because the table starts with only one region, and all the edits will go to that region.
This makes the hosting region server busy to handle those requests.
At the same time, other region servers are free of load.
The cluster keeps running on this unbalanced status until the data in the first region reaches a threshold and the region is split into two regions.
It will take a long time before a new table becomes well balanced by this automatic region splitting.
The solution is to precreate enough regions at the time of creating the table.
The regions need to be created with proper boundaries so that edits can be distributed well to all these precreated regions.
In order to determine proper boundaries, you might also want to back up your region starting keys, which can also be used to restore the region distribution of an HBase table.
Since HBase 0.94, the Import utility can generate the HBase internal files (HFiles) for bulk load.
With this feature, we are able to restore the dump files in a shorter time frame.
Backing up NameNode metadata As HBase runs within HDFS, in addition to taking care of the HBase cluster, it is also important to keep your HDFS running on a healthy status.
NameNode is the most important component in an HDFS cluster.
The metadata of an HDFS cluster, including the filesystem image and edit log, is managed by NameNode.
We need to protect our NameNode metadata for two situations:
For the first situation, we can set up NameNode to write its metadata to its local disk, along with an NFS mount.
As described in the Setting up multiple, highly available (HA) masters recipe, in Chapter 1, Setting Up HBase Cluster, we can even set up multiple NameNode nodes to achieve high availability.
Our solution for the second situation, is to back up the metadata frequently so that we can restore the NameNode state in case of metadata corruption.
We will describe how to back up and restore NameNode metadata in this recipe.
Getting ready Start your HDFS and log in to your client node.
Make sure the port is opened for your client node.
The dfs.name.dir property specifies where the NameNode metadata has been saved.
Grep the HDFS configuration file (hdfs-site.xml) to find out the value of this property, which is required to perform the restoration task.
We assume the value is /usr/local/hadoop/ var/dfs/name, in the demonstration.
We will save the backup under /backup/namenode on the client node and use /restore/ namenode on the NameNode server as a working directory for the restoring.
Create these directories and make sure that the user running the backup task has write permission on them.
Make a tarball for the fetched metadata by using the following command:
In order to restore the NameNode state from a metadata backup:
Stop the NameNode daemon if it is running from your NameNode server:
If the previous command does not work for any reason, just kill the Java process of NameNode.
Run hadoop fsck to check inconsistencies after the NameNode server has been restarted:
If many files have been corrupted, as you can see from the web UI, NameNode might continue in safe mode until you explicitly turn it off:
Turn off safe mode by explicitly using the following command:
The web UI will not state that safe mode is turned off; instead, it may show a warning message on the page:
NameNode supports getting its filesystem image and edit log from its web UI.
If there are edits that happened after our latest backup, the metadata we restored will not match the actual data on DataNodes, in which case our HDFS becomes inconsistent.
If the path and the -files option are provided, fsck will check the inconsistencies under the specified path and print the file it is checking.
If corrupt files are detected, we need to delete them to make HDFS consistent.
If many files have been corrupted, NameNode will not be able to reach the threshold of the reported blocks ratio that it will keep in safe mode.
Before we can delete the corrupt files, we need to turn off the NameNode safe mode explicitly.
Running it with the -safemode leave option turns off the NameNode safe mode explicitly.
As the safe mode is turned off, we can use the fsck command with the -move option to move the corrupt files to /lost+found.
After that, fsck should report the filesystem as healthy, and you should be able to access the HDFS.
Restoring the NameNode state from a metadata backup can take a long time to complete.
This is because the restart of a NameNode daemon will at first load the filesystem image from disk, and then replay the edit log to reconstruct the final system state.
The edit log can be huge and the replaying can take a long time.
Note that SecondaryNameNode is not a backup daemon for the NameNode.
It periodically (default 1 hour) compacts the edit log into a checkpoint, so that a restart of NameNode then loads the latest checkpoint, and a much smaller edit log that contains only the edits since the checkpoint.
With this compaction capability, SecondaryNameNode makes the backup and restoring of NameNode metadata much more efficient.
SecondaryNameNode, by default, runs on the same node as NameNode.
We recommend you to run it on a separate server from the NameNode server, for scalability and durability.
It is configured in the masters file under Hadoop's configuration directory:
In order to start SecondaryNameNode in this file, add the hostname you want, and restart HDFS to apply the change.
Backing up region starting keys Besides the tables in HBase, we should back up the region starting keys for each table.
Region starting keys determine the data distribution in a table, as regions are split by region starting keys.
A region is the basic unit for load balancing and metrics gathering in HBase.
There is no need to back up the region starting keys if you are performing full shutdown backups using distcp, because distcp also copies region boundaries to the backup cluster.
But for the live backup options, backing up region starting keys is as important as the table data, which is especially true if your data distribution is difficult to calculate in advance or your regions are manually split.
It is important because live backup options, including the CopyTable and Export utilities use the normal HBase client API to restore data in a MapReduce job.
The restoring speed can be improved dramatically if we precreate well-split regions before running the restore MapReduce job.
We will describe how to back up an HBase table's region starting keys in this recipe.
We will create a script to get the region starting keys from an HBase table.
Getting ready Start your HBase cluster, create a table, and put some data in it.
The table needs to have several regions in it so that we can verify whether our script works well or not.
If your table has only one region, manually split it into several regions from the table page of the HBase web UI.
We will use the hly_temp table in this recipe; its web UI page looks like the web page shown in the following screenshot:
Create this directory and grant write privilege to the user who will run the backup script.
Follow these instructions to back up the hly_temp table's region starting keys:
To get the region starting keys of an HBase table, we need to access the HBase Java client API from the script.
Our script created in step 1 was written in JRuby.
We chose JRuby because, as HBase Shell is written in JRuby, there is no need to introduce another language and its dependencies.
In the script, we create an HTable instance to access our hly_temp table.
The HTable class is used to communicate with a single HBase table.
We only pass the table name (hly_temp) to the constructor of the HTable class.
The ZooKeeper quorum connection string for the HBase cluster is set to the default value, which is achieved from HBase's configuration file (hbase-site.xml) on the client, by using the hbase command.
If the HTable instance was created successfully, a connection session to the ZooKeeper quorum will be established.
After that, we can communicate to our hly_temp table via the HTable instance.
Next, we call the getStartKeys() method of the HTable instance, which returns an array of region starting keys for the table.
A region starting key is a row key represented by a byte array in HBase.
In the next step of the script, we convert the keys to strings using the Bytes class.
The Bytes class is a utility class converting other objects from/to a byte array.
This class is useful because all Key/Values are stored as byte array in HBase, regardless of their original data type.
In the last step of the script, we write the converted strings to the output file, and finally close the connection to the cluster.
In step 2, we execute the script by running the hbase command.
The hbase command is used to run a Java class in the current HBase context.
Our script is passed to the org.jruby.Main class and run in the HBase context.
The table name and output file's path are also passed to the script via the command line.
As you can see from the output of step 3, the region starting keys we wrote to the output file are the same as what we have seen on the table's administration web page.
See also f Precreating regions using your own algorithm recipe, in Chapter 9, Advanced.
Cluster replication HBase supports cluster replication, which is a way to copy data between the HBase clusters.
For example, it can be used as a way to easily ship edits from a real-time frontend cluster to a batch purpose cluster on the backend.
The basic architecture of an HBase replication is very practical.
The master cluster captures write ahead log (WAL), and puts replicable Key/Values (edits of the column family with replication support) from the log into the replication queue.
The replication message is then sent to the peer cluster, and then replayed on that cluster using its normal HBase client API.
The master cluster also keeps the current position of the WAL being replicated in ZooKeeper for failure recovery.
Because the HBase replication is done asynchronously, the clusters participating in the replication can be geographically distant.
It is not a problem if the connections between them are offline for some time, as the master cluster will track the replication, and recover it after connections are online again.
This means that the HBase replication can serve as a disaster recovery solution at the HBase layer.
We will look at how to enable the replication of a table between two clusters, in this recipe.
Getting ready You will need two HBase clusters—one is the master, and the other is the replication peer (slave) cluster.
Check the HBASE_MANAGES_ZK setting in your hbase-env.sh file, and make sure it is set to false.
All machines, including the ZooKeeper clusters and HBase clusters, need to be able to reach other machines.
Make sure both clusters have the same HBase and Hadoop major version.
Add the following code to HBase's configuration file (hbase-site.xml) to enable replication on the master cluster:
Sync the change to all the servers, including the client nodes in the cluster, and restart HBase.
Connect to HBase Shell on the master cluster and enable replication on the table you want to replicate:
If you are using an existing table, alter it to support replication:
This includes enabling replication, restarting HBase, and creating an identical copy of the table.
Add a peer replication cluster via HBase Shell from the master cluster:
Start replication on the master cluster by running the following command:
You should be able to see the data appear in the peer cluster table in a short while.
Connect to HBase Shell on the peer cluster and do a scan on the table to see if the data has been replicated:
Verify the replicated data on the two clusters by invoking the verifyrep command on the master cluster:
We skipped some output of the verifyrep command to make it clearer.
Stop the replication on the master cluster by running the following command:
Remove the replication peer from the master cluster by using the following command:
Replication is still considered an experimental feature, and it is disabled by default.
For the peer cluster, we did the same procedure in step 4—enabling replication support and creating an identical table with the exact same name— for those replicated families.
After that, we start the actual shipping of edit records to the peer cluster.
To test our replication setting, we put some data into the table, and after a while, as you can see from the output of the scan command on the peer cluster, data has been shipped to the peer cluster correctly.
While this is easy to do when looking at only a few rows, the better way is to use the verifyrep command to do a comparison between the two tables.
The following is the help description of the verifyrep command:
Running verifyrep from the hadoop jar command, with parameters of the peer ID (the one used to establish a replication stream in step 5) and the table name, will start a MapReduce job to compare each cell in the original and replicated tables.
Good rows means that the rows between the two tables were an exact match, while the bad rows are the rows that did not match.
As our data was replicated successfully, we got the following output:
If you got some bad rows, check the MapReduce job's map log to see the reason.
Finally, we stop the replication and remove the peer from the master cluster.
Stopping the replication will still complete shipping all the queued edits to the peer, but not accept further processing.
In order to dig deeper into the architecture used for HBase cluster replication, refer to the following document—http://hbase.apache.org/replication.html.
Introduction It is vital to monitor the status of an HBase cluster to ensure that it is operating as expected.
The challenge of monitoring a distributed system, besides taking the case of each server separately, is that you will also need to look at the overall status of the cluster.
It exposes a large amount of metrics, giving the insight information of the cluster.
These metrics are subsequently configured to expose other monitoring systems, such as Ganglia or OpenTSDB, to gather and make them visible through graphs.
Ganglia/OpenTSDB graphs help us understand the insight of the cluster, both for a single server and the entire cluster.
Graphs are good for getting an overview of the historical status, but we also need a mechanism to check the current state of the cluster, and send us notifications or take some automatic actions if the cluster has some problem.
A good solution for this kind of monitoring task is Nagios.
Nagios sits at the center of a monitoring system to watch cluster resources and alert users.
We will describe how to monitor and diagnose an HBase cluster with Ganglia, OpenTSDB, Nagios, and other tools.
We will start with a simple task to show the disk utilization of HBase tables.
We will install and configure Ganglia to monitor HBase metrics and show an example using Ganglia graphs.
We will also set up OpenTSDB, which is similar to Ganglia but more scalable, as it is built on top of HBase.
We will set up Nagios to check everything we want to check, including HBase-related daemon health, Hadoop/HBase logs, HBase inconsistencies, HDFS health, and space utilization.
In the last recipe, we will describe an approach to diagnose and fix the frequently asked hot spot region issue.
Showing the disk utilization of HBase tables In this recipe, we will show the answer to the following simple question:
How much space is HBase or a single HBase table using on HDFS?
It is a really simple task, but you might need to answer this question frequently.
We will give you a tip to make it a bit easier.
Getting ready Start your HBase cluster and log in to your HBase client node.
We assume your HBase root directory on HDFS is /hbase.
The instructions to show the disk utilization of HBase tables are as follows:
Show the disk utilization for all HBase objects by executing the following command:
Show the disk utilization of a particular HBase table (hly_temp) by executing the following command:
Show a list of the regions of an HBase table and their disk utilization, by executing the following command:
All HBase objects are stored under the HBase root directory on HDFS.
The HBase root directory is configured by the hbase.rootdir property in the HBase configuration file (hbase-site.xml)
Thus the disk utilization of all HBase objects equals the HDFS usage under the HBase root directory.
In step 1, to show the HDFS usage, we ran the hadoop fs -dus command passing our HBase root directory (/hbase) to it.
The amount of space under the /hbase directory is shown in the output in bytes.
We ran the hadoop fs -du command to show a list of the regions of the hly_temp table and their disk utilization.
The hadoop fs -du command is similar to the hadoop fs -dus command, but displays the amount of space for each directory/file under a specific path.
As you can see from the output, the disk utilization is displayed in bytes, which is not humanfriendly, especially when it is a large number.
The following is a simple JRuby script, used to convert the output to a humanly readable format:
Run dus.rb to show HBase's disk utilization with a humanly readable output:
In dus.rb, we simply execute the same hadoop fs -dus command to determine the amount of space HBase is using.
One of the most important parts of HBase operation tasks is to monitor the cluster and make sure it is running as expected.
It exposes a lot of metrics, which gives the insight information of the cluster's current status, including region-based statistics, RPC details, and the Java Virtual Machine (JVM) memory and garbage collection data.
These metrics are then subsequently configured to expose to JMX and Ganglia, which makes the metrics visible through graphs.
Ganglia itself is a scalable, distributed system; it is said to be able to handle clusters with 2000 nodes.
We will describe how to use Ganglia to monitor an HBase cluster in this recipe.
We will install Ganglia Monitoring Daemon (Gmond) on each node in the cluster, which will gather the server and HBase metrics of that node.
These metrics are then subsequently polled to Ganglia Meta Daemon (Gmetad) servers, where the metrics are computed and saved in round-robin, time-series databases using round-robin database tool (RRDtool)
We will set up only one Gmetad node here, but it is possible to scale out to multiple Gmetad nodes, where at each Gmetad node it aggregates its assigned Gmond nodes' results.
We will also install a PHP web frontend on the same Gmetad server, so that we can access Ganglia from web browsers.
Finally, we will describe how to configure HBase to expose its metrics to Ganglia.
Getting ready Besides the servers in the cluster, you will need a Gmetad server to run the Gmetad daemon on it.
In our demonstration, we will use master2 as the Gmetad server.
Add a ganglia user as the owner of the Ganglia daemons:
On all nodes you want to monitor, download Ganglia-3.0.7 from: http://downloads.
You will need root privileges on all the servers to install Ganglia.
The instructions to set up Gmond are as follows; they need to be performed on all the nodes you want to monitor:
Copy the sample configuration file (gmetad.conf) to its default location:
Find the following settings and change them, as shown in the next snippet:
Create a directory for the round-robin databases, to store collected data:
The following are instructions to set up the Ganglia web frontend.
Copy the PHP files of the Ganglia web frontend to the location where Apache's web files are stored:
You should be able to access your Ganglia web frontend page at: http://master2/ganglia/
The Ganglia frontend page has all the graphs of metrics that Ganglia has collected, as shown in the following screenshot:
Lastly, the following instructions are for HBase to export its metrics to Ganglia:
You will find that HBase metrics are now collected by Ganglia automatically, as shown in the following screenshot:
When this book was being written, HBase only supported Ganglia 3.0.x versions.
This is because the network protocol changed in the newer 3.1.x releases.
That's why we have to install Ganglia from the source rather than using a package management system.
The file is created at /etc/gmond.conf, which is the default configuration file the Gmond daemon will read.
In step 4, we set the owner of the Gmond daemon as the ganglia user and the name of the cluster to hbase-cookbook.
Instead of the default UDP multicast communication method we use between Gmonds, we have configured it to use unicast messages.
This is done by commenting out the multicast address and time to live (TTL) settings, and adding a dedicated master Gmond node (master2) in the gmond.conf file.
We chose unicast because multicast is not supported in the Amazon EC2 environment.
Another reason is that using unicast is good for a large cluster, say a cluster with more than a hundred nodes.
Gmond needs to be installed and started on all the nodes you want to monitor.
Gmond, by default, monitors some basic server metrics on that node, such as load averages and CPU/ memory/disk usage.
To install Gmetad, we added the --with-gmetad option to compile Gmetad from the source.
Note that when using unicast, you need to set data_source to the dedicated Gmond server configured in the Gmond setup.
After that, we started the Gmetad daemon on the Gmetad server (master2)
The last part of the Ganglia setup is the PHP web frontend.
It is usually set up on the same Gmetad node, so that it can access the round-robin databases created by the Gmetad daemon.
You will only see the basic graphs of the Gmond nodes, because HBase is not yet configured to expose its metrics to Ganglia.
Do not change the filename, as HBase inherits its monitoring APIs from Hadoop.
GangliaContext class to send its metrics collected by the server process to the Gmond daemon running on master2:8649, which is the dedicated Gmond node we specified earlier.
There are some graphs you should pay more attention to, such as the following:
For example, the compaction queue size indicates how many Stores have been queued for compaction in a region server.
Usually this should be quite low (up to several tens of a RegionServer)
When the server is overloaded, or has I/O issues, you will see a spike on the graph.
The following screenshot shows the CPU usage, JVM GC time, and HBase compaction queue size of a region server, after several rounds of heavy writes rush.
As you can see, the CPU usage and long garbage collection time indicates that the server is overloaded.
The setup is similar to what we described in this recipe.
OpenTSDB is an extremely scalable Time Series Database (TSDB) built on top of HBase.
Like Ganglia, OpenTSDB can be used to monitor various systems including HBase.
As compared to Ganglia, which stores its data in RRDtool, OpenTSDB leverages HBase's scalability to monitor it at a larger scale.
The following is an introduction from the OpenTSDB homepage (http://opentsdb.net/):
Thanks to HBase's scalability, OpenTSDB allows you to collect many thousands of metrics from thousands of hosts and applications, at a high rate (every few seconds)
OpenTSDB will never delete or downsample data and can easily store billions of data points.
To use OpenTSDB, we need to write little scripts to collect data from our systems, and push them into OpenTSDB every few seconds.
It is interesting that OpenTSDB uses HBase (to store metrics) to monitor HBase itself.
In this recipe, we will describe how to set up OpenTSDB and Tcollector and use them to monitor our HBase cluster.
We will only start one Time Series Daemon (TSD) in the demonstration.
However, it is possible to run more TSDs on multiple servers as they are independent from each other.
Getting ready You will need a server to run TSD on.
In our demonstration, we will use master2 as the TSD server.
On the TSD server (master2), add a tsdb user as the owner of the TSD daemon.
You need root privileges on the TSD server and on all the nodes you want to monitor, such as Hadoop DataNodes and HBase RegionServers.
The instructions for setting up OpenTSDB and Tcollector are as follows:
This step requires the HBase cluster to support LZO compression.
If your cluster does not support LZO, don't invoke the first command.
Refer to the Using compression recipe in Chapter 8, Basic Performance Tuning, on how to add LZO support to HBase.
To verify our installation, create OpenTSDB's own metrics, by executing the following command:
The following steps need to be executed on every node you want to monitor:
In Tcollector's startup script, add the DNS name of the server where the TSD daemon is running:
On your TSD server, restart the TSD daemon with the --auto-metric option:
Start Tcollector to collect metrics and send them to TSD:
After a while, you should be able to view the metrics collected by Tcollector on the OpenTSDB web UI.
The following screenshot shows our HBase cluster's metrics collected by Tcollector:
As OpenTSDB is built on top of HBase, we need to create OpenTSDB's tables in HBase first.
In step 3, we configured OpenTSDB to create its tables with LZO compression support.
You can also use other HBase-supported compressions such as Snappy and Gzip, but just make sure that compression is available on your HBase cluster.
After that, we specify where HBase is installed by setting the HBASE_HOME environment variable.
Then we invoke create_table.sh to actually create the tables in HBase.
This script comes with OpenTSDB; it creates two tables—tsdb and tsdb-uid.
You will find these tables on your HBase master UI.
In step 4, we created the necessary directories and started OpenTSDB with options to use those directories.
By default, we need to make a metric with the tsdb mkmetric command before we can store that metric in OpenTSDB.
This is a design to prevent making too many metrics, which makes the metric namespace a mess.
As we have shown in step 11, starting the TSD daemon with the --auto-metric option configures OpenTSDB to automatically make the metrics that it received.
The TSD daemon accepts the stats command to expose its own metrics.
In step 6, we created a simple script to periodically get the TSD daemon's metrics with the stats command, and then put them into OpenTSDB by using the put command.
As shown in step 7, the collected OpenTSDB's own metrics are displayed as graphs on its web UI.
Tcollector collects metrics from Hadoop, HBase, and so on and sends them to the TSD daemons.
In Tcollector's startup script (startstop), we have set TSD_HOST to the DNS name of the TSD server to specify which server we should send the metrics to.
In step 11, we restarted our TSD daemon with the --auto-metric option, so that metrics received from Tcollector will automatically be created in OpenTSDB.
As we mentioned earlier, the automatic creation of metrics is not considered a good practice.
You should be aware of what metrics the system is collecting, and create those metrics manually to assure that the metric namespace is clean.
Finally, we started Tcollector to start collecting metrics and sending them to the TSD daemon.
Metrics will be stored in HBase and shown as graphs on the OpenTSDB's web UI.
TSDash provides the same features as the OpenTSDB web UI, but the user interface is a little different.
The following screenshot shows TSDash's UI and an example graph:
Monitoring HBase-related processes in the cluster is an important part of operating HBase.
A basic monitoring is done by running health checks on the HBase processes and notifying the administrators if any process is down.
Nagios is a popular, open source monitoring software used to watch hosts, services, and resources, and alert users when something goes wrong and when it gets recovered again.
Nagios can be easily extended by custom-modules, which are called plugins.
We can use this plugin to send a ping to a Hadoop/HBase daemon's RPC port, to check whether the daemon is alive.
In this recipe, we will set up a monitor server running Nagios to watch all the HBase-related processes in the entire cluster.
Getting ready You will need a monitor server to run Nagios on.
We assume that you are using a Debian machine with minimal setup.
The monitor server needs to be able to communicate with all machines in the cluster via proper TCP ports.
In our demonstration, we will use master2 as the monitor server.
Start the HBase cluster you want to monitor, and then log in to the monitor server.
Make sure you have root privileges on the monitor server.
The instructions to install and configure Nagios to monitor your HBase cluster are as follows:
Install Nagios on the monitor server using your Linux distribution's package management tool:
During the installation process, you are asked for a nagiosadmin password and Samba workgroup settings; enter your password and let the Samba workgroup settings be set as default.
You should be able to access the Nagios admin page from your web browser using the URL http://<monitor_host>/nagios3/
You will be asked for an account and a password; use nagiosadmin and the password you set in the installation to log in to the Nagios admin page.
Add all hosts in the HBase cluster to the Nagios hosts configuration file:
Add the masters and slaves host groups to Nagios by executing the following command:
Configure Nagios to monitor master daemons on the master node.
We assume that the NameNode, ZooKeeper, and HMaster daemons are running on the same master node (master1)
Configure Nagios to monitor slave daemons on all the slave nodes:
Change the notification e-mail address; set it to your own:
Install the Postfix e-mail server to let Nagios send out e-mails:
During the installation, configure Postfix as Internet Site and set a domain from where the Nagios mails will be received.
If everything is fine, restart Nagios to apply the configuration changes:
As shown in the following screenshot of the Nagios admin page, Nagios starts checking the HBase-related processes we just configured.
All the hosts, the Hadoop/HBase/ ZooKeeper daemons, and their statuses are now shown on the Nagios admin page:
To test our Nagios settings, we stop the HRegionServer daemon from one of the slave nodes:
After a while, the region server that is down will be detected by Nagios and a CRITICAL status will be shown on the Nagios admin page, as shown in the following screenshot:
You will also receive an alert e-mail from Nagios reporting that a region server is down, as shown in the following screenshot:
Now restart the HRegionServer daemon again, to test Nagios' recovery notification:
You will find that the status has changed to OK again.
It is easy to install Nagios using your distribution's package management system.
Nagios has an admin web page, from where you will see the current status of the cluster you are monitoring.
You can see the status as per host, host group, service, and service group basis.
You can also generate very detailed availability and trend reports on the admin page.
We also set up two host groups in step 4, the masters and slaves host groups.
This setting allows us to monitor servers on a host group basis.
In a similar way, we defined services to monitor the slave daemons running on the slave nodes.
Note that we used the host group (slaves) we added previously in these service definitions.
After that, we installed the Postfix e-mail server to let Nagios send out e-mails.
Nagios periodically sends a ping to the RPC port of the daemon, which it is monitoring to check if the daemon is alive.
If Nagios cannot get a response from the daemon, it will mark the status of the daemon as CRITICAL on its admin page, and send an alert e-mail to the address we specified.
This e-mail contains details about the hostname, state of the daemon, when it detected the CRITICAL status, and other additional information.
The following table shows a quick reference of the Hadoop daemons you might want to monitor and their default RPC/HTTP ports:
These logs include information about normal operations, as well as warning/error output, and internal diagnostic data.
It is ideal to have a system gathering and processing all these logs to extract useful insight information of the cluster.
A most basic task is to check these logs and get notified if anything abnormal is shown in them.
The NRPE and check_log Nagios plugins can be used to achieve this simple goal, with a few simple steps.
Using NRPE, we can remotely execute the check_log Nagios plugin on a cluster node to check the Hadoop/HBase logs generated by that node.
The check_log plugin greps a particular query word incrementally in a specified logfile.
If any line in the logfile matches the query word, check_log reports a critical status to the NRPE server running on the same node.
As the scope of the book is limited, we will only demonstrate setting up Nagios to check HBase's master daemon log in this recipe.
You can easily copy it and make a few changes to set up Nagios to monitor other logs as well.
Getting ready We assume that you have set up a Nagios monitor server properly, as described in the previous recipe.
The check_log plugin will save the previously checked logs so that it only needs to check the newly appended ones.
Create a directory on the master node for the check_log plugin to save its previously checked logs:
To use Nagios to check Hadoop/HBase logs, perform the following steps:
Install the NRPE server and plugin on the master node of the HBase cluster:
Set NRPE on the master node to allow the monitor server to talk to the NRPE daemon running on it.
Change the allowed_hosts setting for the monitor server in the nrpe.
Check the NRPE service before we continue to change the configuration.
Configure the check_log command on the master node to check the HBase's master daemon log.
Add the following command to the nrpe.cfg file on the master node.
You will need to change the logfile path of the command definition to fit your environment.
Restart the NRPE daemon on the master node to apply our changes:
Add the following service definitions on the monitor server to check the master logs:
You will find that the added services are shown on your Nagios admin page.
Nagios starts checking the HMaster daemon's log on the master node.
Test the setup by adding a FATAL entry to the master daemon log:
After a while, this will be detected by Nagios and a CRITICAL status will be shown on the Nagios admin page, as shown in the following screenshot:
You will also receive an alert e-mail from Nagios reporting that something is wrong in the HBase's master daemon log, as shown in the following screenshot:
First of all, we need to install the NRPE plugin on the monitor server, and the NRPE server and Nagios plugins on all nodes in the HBase cluster.
This is added to the NRPE configuration file (nrpe.cfg) on the node we want to check (the HBase master node)
The following syntax shows the usage of the check_log plugin:
We passed the full path of the HBase master logfile, a temporary old logfile name, and the regular expression for matching the abnormal status log.
We only searched for ERROR or FATAL in the logfile, which are the keywords when log4j logs errors or fatal information.
These are necessary because of the way the check_log plugin operates.
Restart the NRPE server on the HBase master node and the Nagios daemon on the monitor server.
You will find that Nagios starts monitoring the HBase master log and sends notifications to us if ERROR or FATAL appears in the logfile.
The following are some other logs that you might want monitored by Nagios:
You can simply copy the settings we made in this recipe and change the logfile path to configure Nagios to monitor these logs.
Your Nagios admin page should look like the following screenshot, if all these logs are monitored by Nagios:
Besides the health of the HBase-related daemons and their logs, what you might want to monitor is the overview of the current status of the cluster.
In this recipe, we will create a check_hbase Nagios plugin to perform the task of monitoring.
We will install our check_hbase plugin on the master node of the cluster, and remotely execute it with Nagios from the monitor server using the NRPE Nagios plugin.
Getting ready We assume that you have installed and configured the Nagios NRPE plugin on your monitor and master server.
If you have not installed it yet, refer to the previous recipe for detailed installation instructions.
The following are instructions to get the status of the HBase cluster and monitor it by Nagios:
Copy the script to the Nagios plugin folder on your master node and change its execution permission:
Add the check_hbase command to the NRPE configuration on the master node, by editing the nrpe.cfg file:
Restart the NRPE daemon on the master node to apply the changes:
You will find that this service has been added to your Nagios admin page, as shown in the following screenshot:
Change the warning/critical threshold or abnormal query words to test the plugin.
Nagios will detect this CRITICAL status if your remaining HDFS space is lower than 40 percent, as shown in the following screenshot:
In the check_hbase script, we firstly execute the hbase hbck command to get a report of the current status of the HBase deployment.
We also run the hadoop fsck /hbase command to check the HDFS health under the /hbase directory, which is the default root directory for HBase.
The output of these commands is redirected to a temporary file.
The remaining HDFS space is obtained from the HDFS admin web URL.
We access the URL and extract the remaining space value from the HTML output using regular expression.
Lastly, we compare this value to the warning/critical threshold and generate the proper output and exit status of the script.
The output and exit status will subsequently be used as the monitoring result of Nagios.
In step 2, we just copied the script to the Nagios plugin directory and made it executable enough to install as a Nagios plugin.
As you can see, Nagios starts monitoring the cluster's status using our check_hbase plugin, sending out notifications if either the hbck result, fsck result, or the HDFS usage has anything abnormal.
You might also want to monitor the following statuses of the cluster:
These can be done in a similar way to what we have just discussed.
See also f Setting up Nagios to monitor HBase processes in this chapter.
Hot region—write diagnosis As the data keeps growing, the HBase cluster may become unbalanced due to poorly designed table schema or row keys, or for some other reasons.
Many requests may go to a small part of the regions of a table.
There are two types of hot spot region issues—hot write and hot read issues.
Hot write is generally more important for us, because hot read would benefit greatly from the HBase internal cache mechanism.
A solution for the hot write region issue is to find out the hot regions, split them manually, and then distribute the split regions to other region servers.
An HBase edit will firstly be written to the region server's Write-ahead-Log (WAL)
The actual update to the table data occurs once the WAL is successfully appended.
This architecture makes it possible to get an approximate write diagnosis easily.
This information can be used to find out the hot spot write regions in an HBase cluster for some situations.
Getting ready Start your HBase cluster and log in to your HBase client node.
The main() method entry of the previous Java source is shown as follows:
You will get an output as shown in the following screenshot:
For each WAL file, we call the init() method of the instance to read data from the WAL file.
An entry of the WAL file is represented by the HLog.Entry class.
For each entry, we get the table name and region name the entry belongs to, from its key.
We also get the number of edits from the entry using the getEdit().size() method of the HLog.Entry class.
With all this information, we can aggregate the total number of edits on a region basis.
Package the Java source as a JAR file and run it with the hadoop jar command.
As you can see from the output, there are many write requests for the hly_temp table, while one region in the table handles twice the requests of the other two regions.
You will want to take action on the hot spot region we just found, such as manually splitting it into two.
Note that the number of edits shown in the output is an approximate value.
This is because WAL may get deleted by a background thread, if all the edits in WAL have been persisted; there is no way of knowing the number of edits in that deleted WAL.
However, an approximate write diagnosis is a good enough solution for many situations.
It's especially useful when you turn off the automatic region splitting of your cluster.
Introduction After a cluster is delivered for operation, maintenance will be a necessary ongoing task while the cluster is in use.
Typical maintenance tasks include finding out and correcting faults, changing cluster size, making configuration changes, and so on.
One of the most important HBase features is that it is extremely easy to scale in and out.
As your service and data keeps growing, you might need to add nodes to the cluster.
Graceful node decommissioning and rolling restart will also become necessary.
Minimizing the offline time during the decommission and restart is an important task.
What is important is to keep the data distribution the same as what it was before the restart, to retain data locality.
There are many ways to deploy your HBase to the cluster.
The simplest way is to use a script-based approach to sync HBase installations and configurations across the cluster.
We will cover these topics in the first six recipes of this chapter.
What we will also look at in this chapter is security.
As HBase becomes more and more popular, different users and groups may store more data in a shared HBase cluster.
You may not like all the users to have full permissions to every HBase table.
This has risks to your data; for example, security risks or missing data operations.
You might want to verify users' identities and have access control for the HBase tables based on their identities.
Before the release of Hadoop 0.20.203, there were no mechanisms to verify a user identity in Hadoop.
Hadoop uses the user's current login as their Hadoop username (that is, the equivalent of whoami)
Newer releases of Hadoop (0.20.203 and above) support the optional Kerberos authentication of clients.
With this security support, it is allowed to store sensitive data such as financial data on a shared HDFS cluster.
HBase leverages HDFS security to provide its client with secure access.
Only authenticated users are allowed to access a secured HBase.
It is also possible to add a table basis or column family basis access control in HBase.
In the last three recipes of this chapter, we will install Kerberos and then set up HDFS security with Kerberos, and finally set up secure HBase client access.
Enabling HBase RPC DEBUG-level logging Hadoop and HBase use the log4j library to write their logs.
In production, the logging level is usually set to the INFO level, which is good for many situations.
However, there will be cases where you might want to see the debug information of a particular Hadoop/HBase daemon.
HBase inherits its online logging level change capability from Hadoop.
It is possible to change an HBase daemon's logging level from its web UI without restarting the daemon.
This feature is useful when you need to know the debug information of an HBase daemon but cannot restart it.
A typical situation is to troubleshoot a production HBase cluster.
We will describe how to enable HBase RPC DEBUG-level logging in this recipe.
Getting ready Start the HBase cluster and open the HBase web UI from the following URL:
The instructions to enable HBase RPC DEBUG-level logging without restarting an HBase daemon are as follows:
Show the region server web UI from the HBase web UI by clicking on the region server link.
Click on the Log Level link at the top left of the region server web UI.
Get the current logging level of the specific package or class.
You will get an output as shown in the following screenshot:
Enter the package name and logging level, and then click on the Set Log Level button to set the package's logging level (for example, DEBUG)
Effective level: DEBUG, which means that the package's logging level has been changed to DEBUG:
Now you should be able to check the debug log in the region server's logfile, as follows:
It is possible to get or set a Hadoop daemon's logging level from its web UI.
This is useful to debug a production cluster that does not allow restarting.
To enable the DEBUG-level logging of a specific HBase daemon on a particular region server, we locate the region server's web UI and then show its log level page.
On this page, we can get or set a particular Java package's logging level.
As a result, the HRegionServer daemon starts writing its IPC debug information to its logfile.
A Hadoop/HBase daemon may generate a huge amount of debug logs in a short time.
Do not forget to set it back to the INFO level, soon after you have got enough debug information.
We can also get/set the logging level with the hadoop daemonlog command.
The following command gets the IPC log level of the HMaster daemon running on localhost:
Executing hadoop daemonlog without an argument prints the command's usage.
Graceful node decommissioning We will describe how to stop a region server gracefully in this recipe.
It is possible to simply invoke the following command to stop the RegionServer daemon on a region server:
However, this approach has the disadvantage that regions deployed on the stopped region server will go offline for a while during the stopping process.
In production, especially for clusters handling online requests, it is expected to gracefully stop a region server to minimize the region's offline time.
We will describe how HBase supports its graceful node decommissioning feature in this recipe.
Getting ready Start your HBase cluster and log into the master node as the user (the hadoop user in our demonstration) who starts the cluster.
The instructions to gracefully decommission a region server are as follows:
Gracefully stop a region server by invoking the following command:
As the graceful_stop.sh script will turn off HBase's load balancing before it actually stops the region server daemon, explicitly enable it again if needed:
It closes the regions deployed on the region server and stops itself.
The master only learns of the region server's disappearance via the removal of the region server's znode in ZooKeeper, which is invoked by the region server as the last thing it does on its way out.
There is a large window of unavailability while the closes are going on and until the master learns of the region server's removal.
So to minimize this window, we use a graceful shutdown.
It moves the regions off the region server one at a time (gracefully)
The graceful_stop.sh script moves regions off a region server and then stops it.
As regions are firstly moved to other region servers, the region offline time is avoided during the stopping process.
Before moving the regions deployed on the specific region server, the script will turn off the cluster's load balancer first.
This is important because the balancer might balance regions in the cluster at the same time the graceful_stop.sh script is moving regions.
We must avoid this situation by turning off the balancer.
As you can see from the output of step 1, after the load balancer is turned off, the script moved two regions from the specified server to other region servers, then stopped the region server at last.
As the graceful_stop.sh script turned off the load balancer, you might want to turn it on again after the node decommission.
The full usage of the graceful_stop.sh script is shown as follows:
You may have noticed that the graceful_stop.sh script has the restart and reload options.
These options are used to rolling restart an HBase cluster.
We will describe it in the Rolling restart recipe later.
Adding nodes to the cluster One of the most important HBase features is that it is extremely scalable.
HBase lineally scales out by adding nodes to the cluster.
It is easy to start at a small cluster and scale it out when your service and data grows.
Adding a region server to an HBase cluster would be an important maintenance task for administrators.
An HBase cluster can only have one active master node.
However, we can add a backup master node to the cluster to make the HBase master highly available (HA)
In this recipe, we will describe how to add a backup master node to an HBase cluster.
We will also describe adding region servers to a cluster after that.
Getting ready Download and install HBase on the new master or region server first.
Make sure the HBase configuration on that node is synced with other nodes in the cluster.
You might want to install Hadoop and start DataNode and TaskTracker on that node too.
We assume that you have got all Hadoop/HBase directories, configurations, and OS/user settings ready on that new node.
The instructions to add a backup master node to the cluster are as follows:
Start the HBase master daemon on the backup master node:
From the master log, you will find that the newly started master is waiting to become the next active master:
Add the new region server's hostname into the regionservers file, under the $HBASE_HOME/conf directory.
For example, to add "slave4" to the cluster, we invoke the following command:
Log in to the new server and start the region server daemon there:
Optionally, trigger a load balancer manually from HBase Shell to move some regions to the new region server.
You can also wait for the next run of the balancer, which runs every five minutes by default.
In step 1, we simply started the HBase master daemon on the backup master node.
All master nodes compete for creating a / hbase/master znode in ZooKeeper.
The node which won the election (created the znode successfully) becomes the active master for the cluster.
As now there is already a master present, the new master node goes into idle mode and waits to become the next active master if the current one is down.
Next time we start the cluster, the region server daemon will be started automatically on that node.
The region server will query ZooKeeper to find and join the cluster.
As the new region server has fewer regions deployed on it, we can trigger a load balancing explicitly to move some regions to it.
The balancer runs every five minutes by default, so we can also just wait for the next run of the balancer.
We can also start backup masters at the cluster's start time through the following steps:
Start HBase normally; you will find that a backup HBase master will be started at the backup master node:
The master log on the backup master indicates that it is a backup master:
Rolling restart You might want to invoke a rolling restart when upgrading to a new HBase version, or when you want to apply some configuration changes.
As described in the Graceful node decommissioning recipe, a rolling restart minimizes downtime because we only take a single region offline at a time rather than a whole cluster.
A rolling restart keeps the region distribution the same as what it was before the restart.
This will change in HBase 0.96 when you will be able to have old clients talk to new servers and vice versa.
Please check the following link for details about upgrading from one version to another:
A rolling restart contains steps involving restarting the master, gracefully restarting region servers, and finally checking the data consistency after the restart.
Log into the master node as the user who started the cluster.
The instructions to rolling restart an HBase cluster are as follows:
Make the script executable, and then execute it to gracefully restart each region server daemon in the cluster.
Run hbck again to ensure that the cluster is consistent:
In step 1, we ran the hbck command to ensure that the cluster is consistent before the restart.
In step 3, before restarting the region servers, we turned off the balancer to avoid the situation in which balancing happens during the restart process.
HBaseConfTool is a Java class used to get the runtime HBase configurations.
If zparent is not set, we use the default /hbase value.
Each online region server has its znode under $zparent/rs in ZooKeeper.
We use this to get the list of online region servers by invoking the hbase zkcli ls command.
We did not fetch the region server list from the conf/ regionservers file.
This is because, in some environments such as a cluster using its own DNS service in Amazon EC2, the hostname in the file may be different from what is was given by HBase.
The graceful_stop.sh script only accepts the hostname used in HBase, which is shown on the HBase web UI.
For each region server in the list, we invoke the graceful_stop.sh command passing the --restart --reload --debug options, and the server's hostname to gracefully restart the region server on that node.
Finally in step 7, we executed hbck again to ensure that the cluster is running consistently.
We did not use this script because we wanted to gracefully restart our region servers, which was not implemented in the script.
Simple script for managing HBase processes When the nodes in the cluster keep growing, you might want to find tools to show and manage the HBase-related processes running in the cluster.
As the hadoop user is configured to be able to SSH from the master node to each slave node in the cluster without a password, it is easy for us to write a simple script to achieve this task with SSH login for every node and show/manage the running HBase processes on that node.
As Hadoop/HBase processes run in a Java Virtual Machine (JVM), our task is to manage these Java processes in the cluster.
In this recipe, we will create a simple script to show all the running Java processes owned by the hadoop user in an HBase cluster.
Log in to the master node as the user who started the cluster.
We assume that you are running HDFS and HBase as the same user (the hadoop user here)
The instructions to create a simple script to manage HBase processes are as follows:
Run the script as the user who started Hadoop/HBase on the master node:
You will get an output as shown in the following screenshot:
To show the master processes on the master node, invoke the following command:
In this script, we will get the region server list from the conf/regionservers file.
Then for each region server, SSH log into the server and run the jps command on it.
Jps is a command shipped along with the JDK installation to show running Java processes owned by the user.
The most basic information we need, the process ID (PID) and name of each Java process, is contained in the output of the jps command.
The rest of the script just gathers the jps command output from each node and shows the result in a human-friendly format.
As we did in step 4, we can also use the jps command to show Java processes running on the master node.
We skipped the logic that shows the master's Java processes, in this script.
As described in the Setting up Nagios to monitor HBase processes recipe of Chapter 5, Monitoring and Diagnosis, you will need to make sure these Java processes are monitored carefully.
Simple script for making deployment easier There are many ways to deploy your HBase to the cluster.
As Hadoop and HBase are written in Java, most of the deployment is done by simply copying all the files to the nodes in the cluster.
The simplest way is to use a script-based approach to sync HBase installation and configurations across the cluster.
It may not be as cool compared to other modern deployment management tools, but it works well for small or even medium-sized clusters.
In this recipe, we will create a simple script to sync an HBase installation from its master node to all region servers in the cluster.
This approach can be used to deploy Hadoop as well.
Getting ready Log in to the master node as the user who starts the cluster.
We assume that you have set up a non-password SSH from the master node to the region servers, for the user.
The instructions to create a simple script to make HBase deployment easier are as follows:
Run the script as the user who starts Hadoop/HBase on the master node:
The script uses the fact that all region servers are listed in the conf/regionservers file.
It reads every region server's hostname from the file, and then invokes the rsync command to sync the HBase installation directory from master node to that region server.
We add the --delete option to the rsync command because we want to remove unnecessary files from the region server.
To use this script, make some HBase configuration changes on the master node, and then run the script with the HBase owner user.
When the cluster becomes larger, you might want to automate the deployment as much as possible, such as OS and user settings for a new node, fetching and deploying the package, and setting files from a central version managed configuration center.
You might want to try Puppet or Chef for these kinds of deployment tasks.
There are some examples of Puppet and Chef configurations for HBase available on the Internet, the URLs of which are as follows:
Another tool you might want to give a try is Apache Bigtop, which can be found at the following location:
Bigtop is a project for the development of packaging and tests of the Hadoop ecosystem.
It is not yet ready for prime time, but it is a great tool for easily building and deploying Hadoop and HBase.
With security enabled, only authenticated users can access a Hadoop and HBase cluster.
The authentication is provided by a separate authentication service managed by trusted administrators.
This makes HBase a considerable option to store sensitive, big data such as financial data.
Hadoop relies on the Kerberos authentication service for its security support.
A secure HBase must run on HDFS with security support, so HBase also relies on Kerberos to provide it with security support.
Kerberos is a computer network authentication protocol which works on the basis of "tickets" to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner.
We will describe how to install and set up MIT Kerberos in this recipe.
The installation includes setting up a Kerberos admin server and a Key Distribution Center (KDC)
We assume that the hostname of the server is master2 in this recipe.
Create the Kerberos logging directory on the server in advance, using the following command:
During the installation, you will be asked to enter your default realm name.
The standard name for a Kerberos realm is your domain name in uppercase.
The installation will also ask you to enter the Kerberos server and administrative server.
Enter the fully qualified domain name of your server here.
Configure Kerberos by editing the krb5.conf file, shown as follows:
Grant full privileges to admin users by uncommenting the following line in the kadm5.acl file:
Restart the Kerberos admin server and KDC to apply the changes:
Quit the kadmin.local console and test the authentication for the test user by obtaining a Kerberos ticket:
We installed our Kerberos admin server and KDC on the same master2 server.
Only a root user on the Kerberos admin server can enter the kadmin.local console and execute Kerberos admin commands there.
A unique identity (user, host, and so on) in Kerberos is called a principal.
In step 7, we added an admin user (the root user) to Kerberos.
By entering its Kerberos password (not the OS password), the root user from a Kerberos client will be able to execute Kerberos's admin commands via the kadmin console.
The kadmin console is similar to kadmin.local, but can be entered from other Kerberos clients using an admin user.
It is useful when you need to grant admin users, but do not want them to have root access to the Kerberos admin server.
Kerberos users need a ticket to access Kerberos's authenticated services.
A ticket is issued with a Ticket Granting Ticket (TGT), which is issued by the KDC.
To gain a TGT, users are asked to provide their Kerberos password to prove their identity.
A Kerberos TGT can be cached and will get expired after the configured time.
In step 9, we have shown the hac user's cached TGTs using the klist command.
After that, the klist command shows the cached TGT and its expiration date.
We will configure HDFS and HBase to use this Kerberos installation to provide security support in the following two recipes.
The details of Kerberos are out of the scope of this book.
Configuring HDFS security with Kerberos Newer releases of Hadoop (0.20.203 and above) support an optional Kerberos authentication of clients.
This security support includes secure HDFS and secure MapReduce configurations.
The motivation for Hadoop security is not to defend against hackers, as all large Hadoop clusters are behind firewalls that only allow employees to access them.
Its purpose is simply to allow storing sensitive data such as financial data on a shared cluster.
Prior releases of Hadoop already had file ownership and permissions in HDFS; the limitation was that they had no mechanisms for verifying user identity.
With this Kerberos security support, user identities are verified by Kerberos, and only authenticated users are allowed to access the HDFS cluster.
As a secure HBase access is expected to be running on top of a secured HDFS cluster, setting up HDFS security is a prerequisite for HBase security configuration.
In this recipe, we will focus on how to configure HDFS security with Kerberos.
Getting ready We assume that you have a working Kerberos Key Distribution Center (KDC) and have realm set up.
For more information about installing and configuring Kerberos, see the Kerberos authentication for Hadoop and HBase recipe in this chapter.
We assume that the root user on every node has administrative privileges in Kerberos.
With this assumption, we will use the kadmin console to execute the Kerberos admin commands.
If this assumption is not true, you will need to operate Kerberos via the kadmin.local console on the Kerberos admin server.
On every node of the cluster, create a directory to store the Kerberos keytab file, using the following command:
Shut down your cluster before executing the steps discussed in this recipe.
To configure HDFS security with Kerberos, execute the following steps on every node of the cluster:
Replace the hostname part of the principal with your host's FQDN:
Quit from the kadmin console, and confirm if the keytab file has been created correctly:
Make sure the keytab file is only readable by the hadoop user:
Install it by following the README file present in the downloaded file.
Enable Hadoop security by adding the following configurations to the core-site.
Enable HDFS security by adding the following configuration to the hdfs-site.xml file:
You will find logs like the following in the NameNode logfile:
Change the data store directory permission to only readable by its owner:
Configure the Hadoop secure DataNode user by adding the following command to the hadoop-env.sh file:
For 32-bit systems only, build jsvc from source, and then replace the commonsdaemon JAR file in the Hadoop lib directory, as described in the following steps:
You should be able to find logs if everything is configured well, which are shown as follows:
After all DataNodes in the cluster have been started, test our secure HDFS using the following instructions:
Click on the Browse the filesystem link; an error will show on the screen, as seen in the following screenshot:
Test the secure HDFS from the command line on your client node:
Authenticate a user to access the secure HDFS from the kadmin console:
Obtain a Kerberos ticket for the user with the following command:
Test the authentication from the command line on your client node again:
First of all, we installed the Kerberos client software on each HDFS node.
After that, we entered the kadmin console to add Kerberos principals for the hadoop user of that node, and for the node itself.
As we will only use these principals from their keytab files, we added the -randkey option to the addprinc command.
As we will only set up HDFS security in this recipe, we are using the hadoop user here.
If you will also enable secure MapReduce, we recommend you use different users for HDFS and MapReduce, for example the hdfs and mapreduce users respectively.
Another point here is to add different principals for the hadoop user on each node.
Although it is possible to add a single principal in Kerberos for the hadoop user on all nodes, this is not recommended.
A keytab is a file containing pairs of Kerberos principals and encrypted keys.
Using this file, Hadoop can log in to Kerberos without being prompted for a password.
We set the keytab file path and principals in this step.
The _HOST setting value will be replaced with the hostname of the node, at runtime.
As you can see from the output of step 13, if everything was configured well, the hadoop user and the master node will get authenticated by Kerberos during the NameNode starting process.
A secure DataNode must be started by the root user and run by the hadoop user.
This is simply because the 32-bit jsvc binary file was not included in the current Hadoop release's tarball.
After all the DataNodes in the cluster have been started by the root user, we are now able to test the secured HDFS from its web UI and from the command-line interface as well.
Identities including components of HDFS itself, and clients, must be authenticated by Kerberos before they are able to communicate with other identities in the realm.
In step 21, we added a test principal for the hac user on our HDFS client node.
After obtaining a Kerberos ticket (authenticated) in step 22, the user was able to access the secured HDFS successfully.
As the scope of the book is limited, we have omitted some steps here, including:
HBase security configuration As HBase becomes more and more popular, different users and groups may store more data in a shared HBase cluster.
You might not like all users having full permission to every HBase table.
This adds risks to your data, for example, security risks or missed data operation.
Newer HBase releases (0.92 and above) have Kerberos-based security support.
With this, user identities are verified by Kerberos, and only authenticated users are allowed to access data in a secured HBase cluster.
We will describe how to configure secure client access to HBase in this recipe.
Getting ready Make sure you are using the security-enabled HBase release.
We assume that you have a working Kerberos Key Distribution Center (KDC) and have realm set up.
For more information about installing and configuring Kerberos, see the Kerberos authentication for Hadoop and HBase recipe in this chapter.
As a secure HBase access is expected to be running on top of a secured HDFS cluster, you will also need to set up HDFS security in advance.
Refer to the Configuring HDFS security with Kerberos recipe in this chapter, for details.
As we have used the same user (the hadoop user) to run HDFS and HBase in this book, we are able to share the keytab files.
Generate the HBase owner's own keytab file if you are using a different user for HDFS and HBase.
On every node in the cluster, add the following, most basic configuration to the hbase-site.xml file:
On every server node in the cluster, add the following configuration to the hbasesite.xml file:
On every HBase client node, add the following configuration to the hbase-site.xml file:
If everything is configured well, you will find logs in the HBase logfile on the master node, as follows:
Start the region server daemon on every region server node:
You should be able to find logs in the HBase logfile on the master node, shown as follows:
Clear the Kerberos cache and test the secure HBase configuration on your client node.
In step 1, we configured the Kerberos principal and its keytab file for the HBase master and region servers in the hbase-site.xlm file.
It is the common configuration for both the server nodes and client nodes.
Like the Hadoop security configuration, the _HOST setting value will be replaced with the hostname of the node at runtime.
We have enabled the HBase authentication and authorization in the configuration.
We have set HBase to use the Kerberos authentication, and HBase coprocessors-based authorization.
HBase coprocessors is a new feature in the 0.92 release, based on Google's BigTable coprocessors.
It is an arbitrary code that runs at each region in the region server.
An example usage of HBase Coprocessors is to implement access control for HBase tables.
On the client side in step 3, we only need to configure Kerberos's authentication.
Identities, including server-side components and clients, must be authenticated by Kerberos before they are able to communicate with other identities in the realm.
We have set up secure client access to HBase in this recipe.
Besides that, you might also want to configure access control for the HBase tables.
Using the previous setup, implement the following configurations to enable access control:
As the scope of the book is limited, we have omitted these configurations.
Introduction Everyone expects their HBase cluster to run smoothly and steadily, but the cluster does not work as expected sometimes, especially when a cluster has not been well configured.
This chapter describes things you can do to troubleshoot a cluster that is running in an unexpected status.
Before you start troubleshooting a cluster, it is better to get familiar with the tools that will help us restore the cluster.
Useful tools are as important as a deep knowledge of HBase and the cluster you are operating.
We will introduce several recommended tools and their sample usage, in the first recipe.
Problems usually occur on a cluster that is missing the basic setup.
If you encounter problems with your cluster, the first thing you should do is analyze the master log file, as the master acts as the coordinator service of the cluster.
Hopefully, you will be able to identify the root cause of the error once you have found the WARN or ERROR level logs in the log file.
The region server log file is another source you need to check.
The region server log file usually contains load-related error logs, as region servers handle the actual data storing and accessing for the cluster.
HBase runs on the top of HDFS, and it relies on ZooKeeper as its coordination service.
Sometimes, it is necessary to investigate the HDFS, MapReduce, and ZooKeeper logs, as well.
All these log files are, by default, stored under the logs directory of the installation folder.
Of course, it is configurable in the log4j property file.
If you find an error message, search the online resource at http://search-hadoop.
There is a great HBase community that you can always ask for help.
However, before you ask, do not forget to subscribe first—http://hbase.apache.org/mail-lists.html.
In this chapter, we will look through several of the most confronted issues.
We will describe the error messages of these issues, why they happened, and how to fix them with the troubleshooting tools.
Troubleshooting tools In order to troubleshoot an HBase cluster, besides a solid knowledge of the cluster you are operating, the tools you use are also important.
We will describe sample usage of these tools, in this recipe.
The following command sorts the processes by memory usage, in descending order:
In order to install ClusterSSH (cssh) on a Ubuntu desktop, run the following command:
Start the ClusterSSH tool to monitor a small HBase cluster.
For example, on Mac OS X, run the following command to start multiple SSH sessions on each node of the HBase cluster:
A master window (the second one in the following screenshot) will also be created.
In the red master window, type the following to run the top command on every node:
You will get a screen similar to the following screenshot:
To show the Java processes on a slave node for the hadoop user, run the following command:
In order to print the aforementioned HRegionServer process' Java heap summary, use the following command:
Run the following command to show the summary of the garbage collection statistics of a HRegionServer process:
The ps command has a k option, which we can use to specify the sorting order.
This causes the ps command to sort processes by their resident set size, in descending order.
We use this method to find the top process that used the most memory.
The Cluster SSH tool is convenient for managing small clusters.
You can use it to invoke the same commands simultaneously on multiple servers.
Just as we have earlier, we open one session to the master and every slave node in the cluster.
Then, we invoke the top command on each server, via the master window.
This acts as a simple monitoring system for the cluster.
Step 4 shows the output of the jps command on a slave node running the DataNode, RegionServer, and TaskTracker daemons.
Hadoop and HBase are written in Java; this makes jps a useful tool for us.
The output shows that the young generation collection keeps occurring.
Handling the XceiverCount error In this recipe, we will describe how to troubleshoot the following XceiverCount error shown in the DataNode logs:
The following are the steps to fix the XceiverCount error:
Add the following snippet to the HDFS setting file (hdfs-site.xml):
Its default value is 256, which is too small to run HBase on HDFS.
If your DataNode reaches this upper bound, you will see an error log in the DataNode logs notifying you that the Xciever count has been exceeded.
We recommend setting it to a much higher value, 4096 for example, on modern machines.
After changing this setting, you will need to sync the modified hdfs-site.xml file across the cluster and then restart HDFS to apply the changes.
Handling the "too many open files" error In this recipe, we will describe how to troubleshoot the error shown in the following DataNode logs:
Getting ready To fix this issue, you will need root privileges on every node of the cluster.
We assume you use the hadoop user to start your HDFS cluster.
To fix the "too many open files" error, execute the following steps on every node of the cluster:
Log out and then log in again as the hadoop user.
Confirm that the open file limit has been increased, by running the following command:
HBase is a database running on Hadoop, and just like the other databases, it keeps lots of files open at the same time.
On the other hand, Linux limits the number of file descriptors that a process may open.
The default limitation is 1024, which is too small for HBase.
If the hadoop user's open file number exceeds this upper bound, you will see an error in the DataNode logs indicating that too many files are open.
To run HBase smoothly, you will need to increase the maximum number of open file descriptors for the user running HDFS and HBase; in our case, that is the hadoop user.
The soft limit (soft nofile) is the value that the OS kernel enforces for the number of open file descriptors.
The hard limit (hard nofile) acts as a ceiling for the soft limit.
As you can see in step 4, the ulimit -n command shows the open file limit for the hadoop user.
Do not forget to restart the HDFS cluster to apply the changes.
You may also want to know the current open file number of the hadoop user.
In order to get this information, use the lsof command:
In this recipe, we will describe how to troubleshoot the error shown in the following RegionServer logs:
Getting ready To fix this error, you will need root privileges on every node of the cluster.
We assume you use the hadoop user to start your HDFS and HBase clusters.
To fix the "unable to create new native thread" error, execute the following steps on every node of the cluster:
Log out and then log in again as the hadoop user.
Make sure the open process limit has been increased, by running the following command:
Linux limits the number of processes that a user may execute simultaneously.
In a high load HBase cluster, a low nproc setting could manifest as an OutOfMemoryError exception, as shown earlier.
If the running process numbers of the hadoop user exceed the nproc limit, you will see an error in the RegionServer logs to the tune of "unable to create new native thread"
To run HBase smoothly, you will need to increase the nproc limit for the user who runs HDFS and HBase.
The soft limit (soft nproc) is the value that the OS kernel enforces for the number of processes.
The hard limit (hard nproc) acts as a ceiling for the soft limit.
After logging out, we log back in again and run the ulimit -u program, to show the limit of the current user's process number.
Finally, we restart HDFS and HBase to apply the changes.
In order to view the current thread number of the hadoop user, type the following command:
The THCNT column of the output is the thread number of each process by the hadoop user.
You may have noticed that HBase ignores your HDFS client configuration, for example the dfs.replication setting.
In the following example, we have set a replication factor of 2 for our HDFS client:
However, the HBase files on HDFS show a factor of 3, which is the default replication factor of HDFS:
We will describe why this happens and how to fix it, in this recipe.
Getting ready Log in to your master node as the user who starts HDFS and HBase.
We assume you are using the hadoop user for HDFS and HBase.
The following are the steps to apply your HDFS client configurations to HBase:
Add a symbolic link of the HDFS setting file (hdfs-site.xml) under the HBase configuration directory:
Now, the newly created HBase files will have the HDFS client configurations.
The actual replication factor value will be 2, as we expected:
To apply the HDFS client configuration to HBase, we need to add the settings in this file, and add the hdfs-site.xml file to the HBase's classpath..
The easiest way to do this is to create a symbolic link of the hdfs-site.xml file under the HBase configuration directory and then sync it across the cluster.
Once HBase has been restarted, you will notice that the HDFS client configurations will be applied.
In this recipe, we will describe how to troubleshoot the ZooKeeper client connection error shown in the following RegionServer logs:
The following are the steps to fix the ZooKeeper client connection error:
Add the following to the ZooKeeper configuration file (zoo.cfg) on every ZooKeeper quorum node:
This error usually occurs when running a MapReduce job over an HBase cluster.
ZooKeeper has a maxClientCnxns setting to limit the number of concurrent connections that a single client may make to a single member of the ZooKeeper ensemble.
Every region server is a ZooKeeper client; if a region server's concurrent connection number exceeds this maximum client connection limit, it will be not able to create a new connection to the ZooKeeper ensemble.
To fix this error, we need to set a higher value to the maxClientCnxns setting and restart ZooKeeper for the changes to be applied.
To view the current number of connections from a client to a specific ZooKeeper quorum node, run the following command:
In this recipe, we will describe how to troubleshoot the following ZooKeeper session expired error shown in the RegionServer logs:
This issue is critical, because master or region servers will shut themselves down if they lose connection to the ZooKeeper quorum.
Getting ready Log in to the server where this error occurred.
The following are the steps to fix the ZooKeeper session expired issue:
Check hbase-env.sh; make sure you give plenty of RAM to the HBase daemons.
The default of 1 GB won't be enough for heavy clusters:
Run the vmstat command to show the virtual memory statistics:
Check the swap columns called si (swap in) and so (swap out), and make sure you don't swap.
Use the jstat command to show the Java Garbage Collection (GC) statistics:
Check the FGCT column, and make sure RegionServer doesn't run into a long GC pause.
Make sure the RegionServer thread has plenty of CPU resources.
MapReduce may use a lot of CPU power, causing the RegionServer to starve and run into a long GC pause.
Consider reducing the map/reduce slots number on the region server, if MapReduce consumes too much CPU resource.
Consider increasing the ZooKeeper session timeout by editing the hbase-site.xml file:
Increase the ZooKeeper maximum session timeout on every ZooKeeper quorum node.
Restart ZooKeeper on every ZooKeeper quorum node, if you made changes to the zoo.cfg file:
An HBase Master and RegionServer daemon acts as a ZooKeeper client; if the client cannot communicate with the ZooKeeper quorum in a configured time, the connection will time out and this error will occur.
The two most likely reasons why a ZooKeeper connection gets timed out are as follows:
The default size of 1 GB won't be enough for heavy clusters.
The reason we recommend setting the HBase heap size lower than 16 GB is, if we use a very big heap size here, the GC will take a very long time to complete, which is what we must avoid.
The vmstat command can be used to see whether a swap happened or not.
There is a vm.swappiness kernel setting you should change, to avoid swap.
The FGCT column of the output is the total, full GC time; check this column to see whether a long GC pause happened or not.
Another reason for a long GC pause is that the RegionServer process may not have enough CPU resources.
This is especially true when running heavy MapReduce jobs on an HBase cluster.
In this case, if you are running heavy MapReduce jobs, use the MapReduce configurations mapred.
Note that setting a higher timeout means that the cluster will take at least that much time to failover a failed RegionServer.
You will need to think about whether it is acceptable to your system or not.
Handling the HBase startup error on EC2 In this recipe, we will describe how to troubleshoot the HBase startup error shown in the following Master logs:
This error usually happens after stopping and restarting EC2 instances.
The reason is that HBase stores region locations in its "system" -ROOT- and .META.
The location information contains the internal EC2 DNS name in it.
Due to the DNS name change, HBase will not be able to resolve the old DNS name in its system table to the new one, thus causing the aforementioned error message.
Getting ready Make sure you have already set up your Amazon EC2 environment for HBase.
You will need an account that has root privileges on every node of the cluster, to run the scripts we will create later.
The following are the steps to fix the HBase startup error on EC2:
YOUR_PASSWORD in the script with the password of the account you are using to run the script:
Replace the value of NS_HOSTNAME with the hostname of your name server:
The /etc/hosts file on every node has been updated by the script, using EC2 API.
The old DNS entry of each slave node will be appended to the /etc/hosts file on the master node.
Log in to your master node as the hadoop user and start HBase on this node:
As a simple solution, every time we stop and restart the instances before starting HBase, we update the /etc/hosts file on every node in the cluster and then append the old DNS entry of each slave node to the master node's /etc/hosts file.
With this, HBase Master is able to resolve the old DNS entries in its system tables to the updated, new IP address and update them with the new DNS entries in the system tables, too.
These are the new DNS entries we need to update to the /etc/hosts file of each node.
The script also backs up the original /etc/hosts file before overwriting it.
We write the account's password in the script so that we don't need to enter it on every node.
That's why it was noted in step 3, that the script should only readable to yourself.
As you can see, these scripts are simple but useful, if you want to start your HBase EC2 instances only when you need to run HBase on them.
You may also like to update the public DNS name of your EC2 instances to the /etc/hosts file on your local PC, so that you can access the HBase web UI from your web browser.
The script will ask you to enter the password of your account, because it needs root privileges to update the /etc/hosts file:
Now, open your web browser and enter the URL http://master1:60010/ master.jsp.
Note that you will need a Mac OS X or Linux PC to use this script.
Introduction Performance is one of the most interesting characteristics of an HBase cluster's behavior.
It is a challenging operation for administrators, because performance tuning requires deep understanding of not only HBase but also of Hadoop, Java Virtual Machine Garbage Collection (JVM GC), and important tuning parameters of an operating system.
The structure of a typical HBase cluster is shown in the following diagram:
There are several components in the cluster—the ZooKeeper cluster, the HBase master node, region servers, the Hadoop Distributed File System (HDFS) and the HBase client.
The ZooKeeper cluster acts as a coordination service for the entire HBase cluster, handling master selection, root region server lookup, node registration, and so on.
Its job includes region allocation and failover, log splitting, and load balancing.
Region servers hold the actual regions; they handle I/O requests to the hosting regions, flush the in-memory data store (MemStore) to HDFS, and split and compact regions.
We usually have an HBase region server running on the same machine as the HDFS DataNode, but it is not mandatory.
The HBase client provides APIs to access the HBase cluster.
To communicate with the cluster, clients need to find the region server holding a specific row key range; this is called region lookups.
HBase has two system tables to support region lookups—the -ROOT- table and the .META.
With this background knowledge, we will describe how to tune HBase to gain better performance, in this chapter.
Besides HBase itself, other tuning points include Hadoop configurations, the JVM garbage collection settings, and the OS kernel parameters.
We will also include recipes to tune these configurations, in this chapter.
Setting up Hadoop to spread disk I/O Modern servers usually have multiple disk devices to provide large storage capacities.
These disks are usually configured as RAID arrays, as their factory settings.
This is good for many cases but not for Hadoop.
The Hadoop slave node stores HDFS data blocks and MapReduce temporary files on its local disks.
These local disk operations benefit from using multiple independent disks to spread disk I/O.
In this recipe, we will describe how to set up Hadoop to use multiple disks to spread its disk I/O.
Getting ready We assume you have multiple disks for each DataNode node.
In order to set up Hadoop to spread disk I/O, follow these instructions:
On each DataNode node, create directories on each disk for HDFS to store its data blocks:
Add the following code to the HDFS configuration file (hdfs-site.xml):
We recommend JBOD or RAID0 for the DataNode disks, because you don't need the redundancy of RAID, as HDFS ensures its data redundancy using replication between nodes.
So, there is no data loss when a single disk fails.
Which one to choose, JBOD or RAID0? You will theoretically get better performance from a JBOD configuration than from a RAID configuration.
This is because, in a RAID configuration, you have to wait for the slowest disk in the array to complete before the entire write operation can complete, which makes the average I/O time equivalent to the slowest disk's I/O time.
In a JBOD configuration, operations on a faster disk will complete independently of the slower ones, which makes the average I/O time faster than the slowest one.
You might want to benchmark your JBOD and RAID0 configurations before deciding which one to go with.
For both JBOD and RAID0 configurations, you will have the disks mounted at different paths.
The key point here is to set the dfs.data.dir property to all the directories created on each disk.
The dfs.data.dir property specifies where the DataNode should store its local blocks.
By setting it to comma-separated multiple directories, DataNode stores its blocks across all the disks in round robin fashion.
This causes Hadoop to efficiently spread disk I/O to all the disks.
Warning Do not leave blanks between the directory paths in the dfs.data.dir property value, or it won't work as expected.
You will need to sync the changes across the cluster and restart HDFS to apply them.
If you run MapReduce, as MapReduce stores its temporary files on TaskTracker's local file system, you might also like to set up MapReduce to spread its disk I/O:
On each TaskTracker node, create directories on each disk for MapReduce to store its intermediate data files:
Sync the modified mapred-site.xml file across the cluster and restart MapReduce.
MapReduce generates a lot of temporary files on TaskTrackers' local disks during its execution.
Like HDFS, setting up multiple directories on different disks helps spread MapReduce disk I/O significantly.
Administrators are able to define the rack of each DataNode in the cluster.
In this recipe, we will describe how to make Hadoop rack-aware and why it is important.
Getting ready You will need to know the rack to which each of your slave nodes belongs.
Log in to the master node as the user who started Hadoop.
Create a topology.sh script and store it under the Hadoop configuration directory.
Change the path for topology.data, in line 3, to fit your environment:
Don't forget to set the execute permission on the script file:
Create a topology.data file, as shown in the following snippet; change the IP addresses and racks to fit your environment:
Add the following to the Hadoop core configuration file (core-site.xml):
Sync the modified files across the cluster and restart HDFS and MapReduce.
If everything works well, you should be able to find something like the following snippet in your NameNode log file:
If everything works well, you should be able to find something like the following snippet in your JobTracker log file:
The following diagram shows the concept of Hadoop rack awareness:
Each block of the HDFS files will be replicated to multiple DataNodes, to prevent loss of all the data copies due to failure of one machine.
However, if all copies of data happen to be replicated on DataNodes in the same rack, and that rack fails, all the data copies will be lost.
So to avoid this, the NameNode needs to know the network topology in order to use that information to make intelligent data replication.
As shown in the previous diagram, with the default replication factor of three, two data copies will be placed on the machines in the same rack, and another one will be put on a machine in a different rack.
This ensures that a single rack failure won't result in the loss of all data copies.
Normally, two machines in the same rack have more bandwidth and lower latency between them than two machines in different racks.
With the network topology information, Hadoop is able to maximize network performance by reading data from proper DataNodes.
If data is available on the local machine, Hadoop will read data from it.
If not, Hadoop will try reading data from a machine in the same rack, and if it is available on neither, data will be read from machines in different racks.
The script takes DNS names as arguments and returns network topology (rack) names as the output.
If an entry is not found in the topology.data file, the script returns /default/rack as a default rack name.
Note that we use IP addresses, and not hostnames in the topology.
There is a known bug that Hadoop does not correctly process hostnames that start with letters "a" to "f"
This indicates that the HDFS and MapReduce rack awareness work well with the aforementioned settings.
If you mount the disks as noatime, the access timestamps are not updated when a file is read on the filesystem.
In the case of the nodiratime attribute, mounting disks does not update the directory inode access times on the filesystem.
As there is no more disk I/O for updating the access timestamps, this speeds up filesystem reads.
In this recipe, we will describe why the noatime and nodiratime options are recommended for Hadoop, and how to mount disks with noatime and nodiratime.
Getting ready You will need root privileges on your slave nodes.
We assume you have two disks for only Hadoop—/dev/xvdc and /dev/xvdd.
To mount disks with noatime and nodiratime, execute the following instructions on every slave node in the cluster:
Unmount the disks and mount them again to apply the changes:
As Hadoop (HDFS) manages the metadata (inode) of its filesystem with NameNode, any access time information kept by Hadoop is independent of the atime attribute of individual blocks.
So, the access timestamps in DataNode's local filesystem makes no sense here.
That's why we recommend that you mount disks with noatime and nodiratime, if the disks are used purely for Hadoop.
Mounting disks with noatime and nodiratime saves the write I/O every time a local file is accessed.
Do not forget to unmount and mount the disks again, for the changes to be applied.
With these options enabled, an improvement in the performance of the HDFS read is expected.
As HBase stores its data on HDFS, the HBase read performance is also expected to improve.
By default, some of the filesystem blocks are reserved for use by privileged processes.
This is to avoid situations wherein user processes fill up the disk spaces that are required by the system daemons, in order to keep working.
This is very important for the disks hosting the operating system but less useful for the disks only used by Hadoop.
Usually these Hadoop-only disks have a very large storage space.
Reducing the percentage of the reserved blocks can add quite a few storage capacities to the HDFS cluster.
Warning: Do not reduce the reserved blocks on the disks hosting the operating system.
To achieve this, run the following command on each disk of each slave node in the cluster:
Setting vm.swappiness to 0 to avoid swap Linux moves the memory pages that have not been accessed for some time to the swap space, even if there is enough free memory available.
On the other hand, reading swapped out data from the swap space to memory is called swap in.
Swapping is necessary in many situations, but as Java Virtual Machine (JVM) does not behave well under swapping, HBase may run into trouble if swapped.
The ZooKeeper session expiring is a typical problem that may be introduced by a swap.
In this recipe, we will describe how to tune the Linux vm.swappiness parameter to avoid swap.
Getting ready Make sure you have root privileges on your nodes in the cluster.
To tune the Linux parameter to avoid swapping, invoke the following on each node in the cluster:
This change will persist until the next reboot of the server.
Add the following to the /etc/sysctl.conf file, so that the setting will be enabled whenever the system boots:
The vm.swappiness parameter can be used to define how aggressively memory pages are swapped to disk.
This is good for HBase, because HBase processes consume a large amount of memory.
A higher vm.swappiness value will make HBase swap a lot and encounter very slow garbage collection.
This is likely to result in the RegionServer process being killed off, as the ZooKeeper session times out.
Note that the value set by the sysctl command only persists until the next reboot of the server.
You need to set vm.swappiness in the /etc/sysctl.conf file, so that the setting is enabled whenever the system reboots.
Java GC and HBase heap settings As HBase runs within JVM, the JVM Garbage Collection (GC) settings are very important for HBase to run smoothly, with high performance.
In addition to the general guideline for configuring HBase heap settings, it's also important to have the HBase processes output their GC logs and then tune the JVM settings based on the GC logs' output.
We will describe the most important HBase JVM heap settings as well as how to enable and understand GC logging, in this recipe.
We will also cover some general guidelines to tune Java GC settings for HBase.
The following are the recommended Java GC and HBase heap settings:
Give HBase enough heap size by editing the hbase-env.sh file.
For example, the following snippet configures a 8000-MB heap size for HBase:
In step 1, we configure the HBase heap memory size.
By default, HBase uses a heap size of 1GB, which is too low for modern machines.
Basic knowledge about JVM memory allocation and garbage collection is required to understand the log output.
The following is a diagram of the JVM generational garbage collection system:
There are three heap generations: the Perm (or Permanent) generation, the Old Generation (or Tenured) generation, and the Young Generation.
Usually, objects are allocated in the Eden space of the young generation.
If an allocation fails (the Eden is full), all Java threads are halted and a young generation GC (Minor GC) is invoked.
If the S1 space is full, objects are copied (promoted) to the old generation.
The old generation is collected (Major/Full GC) when a promotion fails.
The permanent generation is used to hold class and method definitions for objects.
Back to our sample in step 5, the minor GC output for the aformentioned options is produced in the following form:
HBase uses CMS GC as its default garbage collector for the old generation.
During the concurrent marking and sweeping phases, the CMS thread runs along with the application's threads.
Note that it is a concurrent marking; Java was not paused.
The pause here is 0.0413960 seconds to remark the heap.
The tuning point here is to keep all these pauses low.
To keep the pauses low, you may need to adjust the young generation space's size via the -XX:NewSize and -XX:MaxNewSize JVM flags, to set them to relative small values (for example, up to several hundred MB)
If the server has more CPU power, we recommend using the Parallel New Collector by setting the -XX:+UseParNewGC option.
For the old generation, the concurrent collection (CMS) generally cannot be sped up, but it can be started earlier.
For some situations, especially during loading, if the CMS starts too late, HBase may run into full garbage collection.
When using CMS for an old generation, the default young generation GC will be set to the Parallel New Collector.
If you are using an HBase version prior to 0.92, consider enabling the MemStore-Local Allocation Buffer to prevent old generation heap fragmentation under heavy write loads:
Using compression One of the most important features of HBase is the use of data compression.
Our suggestion is to use the LZO compression algorithm because of its fast data decompression and low CPU usage.
As a better compression ratio is preferred for the system, you should consider GZip.
Unfortunately, HBase cannot ship with LZO because of a license issue.
We will use the hadoop-lzo library, which brings splittable LZO compression to Hadoop.
In this recipe, we will describe how to install LZO and how to configure HBase to use LZO compression.
Getting ready Make sure Java is installed on the machine on which hadoop-lzo is to be built.
All nodes in the cluster need to have native LZO library installed.
We will use the hadoop-lzo library to add LZO compression support to HBase:
For example, to build 32bit binaries, run the following commands:
HBase ships with a tool to test whether compression is set up properly.
Use this tool to test the LZO setup on each node of the cluster.
If everything is configured accurately, you will get the SUCCESS output:
By adding LZO compression support, HBase StoreFiles (HFiles) will use LZO compression on blocks as they are written.
HBase uses the native LZO library to perform the compression, while the native library is loaded by HBase via the hadoop-lzo Java library that we built.
This setting will cause a failed startup of a region server if LZO is not installed properly.
If you see logs such as "Could not load native gpl library", there is an issue with the LZO installation.
In order to fix it, make sure that the native LZO libraries are installed and the path is configured properly.
A compression algorithm is specified on a per-column family basis.
Although it adds a read-time penalty as the data blocks probably is decompressed when reading, LZO is fast enough as a real-time compression library.
We recommend using LZO as the default compression algorithm in production HBase.
Another compression option is to use the recently released Snappy compression library and Hadoop Snappy integration.
As the setup is basically the same as what we did before, we will skip the details.
Check the following URL to know how to add Snappy compression to HBase:
Managing compactions An HBase table has the following physical storage structure:
While a region may have several Stores, each holds a single column family.
An edit first writes to the hosting region store's in-memory space, which is called MemStore.
When the size of MemStore reaches a threshold, it is flushed to StoreFiles on HDFS.
As data increases, there may be many StoreFiles on HDFS, which is not good for its performance.
Thus, HBase will automatically pick up a couple of the smaller StoreFiles and rewrite them into a bigger one.
For certain situations, or when triggered by a configured interval (once a day by default), major compaction runs automatically.
Major compaction will drop the deleted or expired cells and rewrite all the StoreFiles in the Store into a single StoreFile; this usually improves the performance.
However, as major compaction rewrites all of the Stores' data, lots of disk I/O and network traffic might occur during the process.
You might want to run it at a lower load time of your system.
In this recipe, we will describe how to turn off this automatic major compaction feature, and run it manually.
Getting ready Log in to your HBase master server as the user who starts the cluster.
The following steps describe how to disable automatic major compaction:
With the aforementioned setting, automatic major compaction will be disabled; you will now need to run it explicitly.
The default value is 86400000, which means once a day.
This will prevent a major compaction from running during a heavy load time, for example when the MapReduce jobs are running over the HBase cluster.
On the other hand, major compaction is required to help performance.
In step 4, we've shown an example of how to manually trigger a major compaction on a particular region, via HBase Shell.
In this example, we have passed a region name to the major_compact command to invoke the major compaction only on a single region.
It is also possible to run major compaction on all regions of a table, by passing the table name to the command.
The major_compact command queues the specified tables or regions for major compaction; this will be executed in the background by the region server hosting them.
As we mentioned earlier, you might want to execute major compaction manually only during a low load time.
This can be done easily by invoking the major_compact command from a cron job.
It is easy to call this API in Java, thus you can manage complex major compaction scheduling from Java.
Managing a region split Usually an HBase table starts with a single region.
However, as data keeps growing and the region reaches its configured maximum size, it is automatically split into two halves, so that they can handle more data.
This mechanism works well for many cases, however there are situations wherein it encounters problems, such as the split/ compaction storms issue.
With a roughly uniform data distribution and growth, eventually all the regions in the table will need to be split at the same time.
Immediately following a split, compactions will run on the daughter regions to rewrite their data into separate files.
This causes a large amount of disk I/O and network traffic.
In order to avoid this situation, you can turn off automatic splitting and manually invoke it.
As you can control at what time to invoke the splitting, it helps spread the I/O load.
Another advantage is that, manually splitting lets you have better control of the regions, which helps you trace and fix region-related issues.
In this recipe, we will describe how to turn off automatic region splitting and invoke it manually.
Getting ready Log in to your HBase master server as the user who starts the cluster.
To turn off automatic region splitting and invoke it manually, follow these steps:
With the aforementioned setting, region splitting will not happen until a region's size reaches the configured 100GB threshold.
You will need to explicitly trigger it on selected regions.
To run a region split through HBase Shell, use the following command:
As splitting won't happen until regions reach the 100GB upper boundary, we need to invoke it explicitly.
In step 4, we invoke splitting on a specified region via HBase Shell, using the split command.
A region is the basic unit of data distribution and balancing in HBase.
Regions should be split into proper size and at low load time.
On the other hand, too much splitting is not good.
Having too many regions on a region server lowers its performance.
You might also want to trigger major compaction and balancing, after manually splitting regions.
The setting that we set previously causes the entire cluster to have a default maximum region size of 100GB.
Besides changing the entire cluster, it is also possible to specify the MAX_FILESIZE property on a column family basis, when creating a table:
Like major compaction, you can also use the split API provided by the org.apache.
See also f Precreating regions using your own algorithm recipe, Chapter 9, Advanced.
In Chapter 8, Basic Performance Tuning, we described some recipes to tune Hadoop, OS settings, Java, and HBase itself to improve the overall performance of the HBase cluster.
In this chapter, we will describe more "specific" recipes; some of them are for write-heavy clusters, while some are aimed to improve read performance of the cluster.
Before tuning a HBase cluster, you will need to know how its performance is.
Therefore, we will start by introducing how to use Yahoo! Cloud Serving Benchmark (YCSB) to measure (benchmark) performance of a HBase cluster.
In the recipe Precreating regions before moving data into HBase in Chapter 2, we introduced how to use HBase's RegionSplitter utility to create a table with precreated regions to improve data loading speed.
We will describe an approach to precreate regions with any boundaries you would like to specify.
There are basically two load type HBase clusters, the write-heavy cluster and the read-heavy cluster.
Many tuning options are trade-off between write performance and read performance.
We will have several recipes describing how to tune HBase cluster to gain better write performance; meanwhile, we will also introduce recipes to tune read-heavy HBase cluster.
These recipes include server-side configuration tuning, client-side setting, and table schema selection.
You need to think carefully about the performance requirements of your system, and tune your cluster to gain the best balance between write and read performance.
We assume you have basic knowledge about HBase architecture and have done general tunings for your cluster.
Benchmarking HBase cluster with YCSB Measuring the performance of a HBase cluster, or benchmarking the cluster, is as important as tuning the cluster itself.
The performance characteristics of a HBase cluster that we should measure include at least the following:
Therefore, you can use YCSB to benchmark for both write-heavy and read-heavy HBase clusters.
The record count to load, operations to perform, proportion of read and write, and many other properties are configurable for each test, so it is easy to use YCSB to test different load scenarios of the cluster.
A common use of YCSB is to benchmark multiple systems and compare their performance.
In this recipe, we will describe how to install YCSB and use it to test write-heavy and readheavy HBase clusters.
Getting ready Start your HBase cluster and log in to your HBase client node.
The following steps need to be followed to benchmark your HBase cluster with YCSB:
Download YCSB on your HBase client node and decompress it:
Add the HBase configuration file (hbase-site.xml) to YCSB HBase binding's classpath:
The latest YCSB version at the time of writing is 0.1.4
To use YCSB HBase binding, we added a link of our HBase cluster's configuration file (hbase-site.xml) to the hbase-binding/conf folder under YCSB installation.
This tells YCSB the connection information of our HBase cluster.
To achieve this, just copy the HBase and ZooKeeper JAR files to the hbase-binding/lib folder under the YCSB installation.
Before running the load test, we need to create the test table in HBase at first.
The test table's name is fixed: it must be usertable.
We also need to create a column family for the table.
After creating the test table, we can run a YCSB load test using the ycsb command.
Executing the ycsb command without parameters will show the command usage.
The workloads specify the data that will be loaded into the table, and the operations that will be executed against the data.
We invoked four client threads to run the test by passing -p threadcount=4 to the command.
The -s option makes YCSB report the status periodically to the output.
As you can see from the HBase web UI, YCSB started loading test data into the table we created previously in step 3:
The output also reports the insert operation number, overall latency, and detailed latency.
These inserts were probably being blocked for a while by the region servers due to high load.
As you can see, HBase writes are much faster than reads.
HBase is shipped with its own performance evaluation (PE) tool which can be used to benchmark HBase, too.
The following is the usage of the HBase PE tool:
This is an example of using HBase PE to test sequential write performance:
There is no need to specify a table and column family name, as the HBase PE tool will create a table called TestTable with a column family called info in its code.
Note that you will need to execute the write tests before using PE to run read tests, as read tests use the data inserted by the write tests.
Increasing region server handler count Region server keeps a number of running threads to answer incoming requests to user tables.
To prevent region server running out of memory, this number is set to very low by default.
For many situations, especially when you have lots of concurrent clients, you will need to increase this number to handle more requests.
We will describe how to tune the region server handler count in this recipe.
Getting ready Log in to the master node as the user who starts HBase.
The following steps need to be followed to increase region server handler count:
On the master node, add the following to your hbase-site.xml file:
This is a rather low value to prevent the region server running out of memory in some situations.
You should set it to a low value if your region server has low memory available.
A low value is also good for handling requests which require lots of memory, such as putting a big value into HBase or scanning data with a large cache configuration.
If your request requires only a litter memory, but needs high transactions per second (TPS), consider setting it to a bigger value so that a region server can handle more concurrent requests.
When tuning this value, we recommend you enable RPC-level logging, and monitor the memory usage of each RPC request and GC status.
You will need to sync the changes across the cluster and restart HBase to apply it.
When we create a table in HBase, the table starts with a single region.
All data inserted into that table goes to the single region.
As data keeps growing, when the size of the region reaches a threshold, Region Splitting happens.
The single region is split into two halves so that the table can handle more data.
In a write-heavy HBase cluster, this approach has several issues that need to be fixed:
As data grows uniformly, most of the regions are split at the same time, which causes huge disk I/O and network traffics.
Especially right after the table is created, all requests go to the same region server where the first region is deployed.
The split/compaction issue has been discussed in the Managing region split recipe in Chapter 8, Basic Performance Tuning.
We described how to use HBase RegionSplitter utility to precreate regions.
This works well for many cases; but there are situations where you might want to generate the keys using your own algorithm, so that the load is spread well among your cluster.
As HBase row key is fully controlled by applications which put data into HBase, for many situations, the range and distribution of row keys are predictable somehow.
Thus it is possible to calculate the region splitting keys and use them to create presplit regions.
We will describe how to achieve this goal in this recipe.
We will create a table with predefined regions, using regions' starting keys specified in a text file.
Getting ready Log in to your HBase client node and create a split-keys file there.
Put your region splitting keys into the file, one key per line.
As an example, we assume the file contains the following keys:
Follow these instructions to precreate regions using your own algorithm:
There are several other methods of the interface we need to implement to compile the Java class.
As we don't actually use them, just make an empty implementation to each of those methods, shown as follows (we skipped some methods here):
Run the following script to precreate regions at the table creation time:
HBase ships with a RegionSplitter utility class, which is used to:
First, we created a FileSplitAlgorithm Java class implementing the SplitAlgorithm interface.
SplitAlgorithm is a Java interface declared in the RegionSplitter class, used to define how RegionSplitter functions.
We also defined a SPLIT_KEY_FILE constant in the FileSplitAlgorithm class to refer to the file which contains our region starting keys.
The SplitAlgorithm interface defines several methods that the implementation class needs to implement.
This method is called by the RegionSplitter class to split an entire table.
In our implementation, it reads split keys from the file we prepared, one key per line.
Then, convert them to an array of byte[] representing the split keys for the initial regions of the table.
In step 4, we added the HBase jar to our classpath and then run the javac command to compile the Java code.
To use the class to split regions, we added our FileSplitAlgorithm class to HBASE_ CLASSPATH, and then invoked the RegionSplitter utility with the following parameters:
The split keys are exactly what we put in the split-keys file.
Other table attributes are all with default values; you might want to change some of them by using the alter command via HBase Shell.
Note that even when regions are split previously, you will also need to design your row key at the application layer, to avoid writing too many sequential row keys to a single region.
You need to choose your split algorithm carefully to fit your data access pattern.
Another useful scenario of this approach is to speed up importing data into HBase from exported backup HBase tables.
As mentioned in the recipe Backing up region starting keys in Chapter 4, Backing Up and Restoring HBase data, you can backup your region starting keys via a simple script.
With the FileSplitAlgorithm class, we can use those keys to previously restore the region boundaries of an HBase table, and then restore the data by importing from exported data files.
Compared to importing data from a single region, as regions are restored and balanced to many region servers firstly, the data restoring speed will be improved significantly.
On a write-heavy HBase cluster, you may observe an unstable write speed.
Most of the writes are very fast, while some are slow.
For an online system, this unstable write speed is not acceptable even when average speed is very fast.
This situation is probably caused by the following two reasons:
As we described in Chapter 8, Basic Performance Tuning you can avoid the split/compaction issues by disabling the automatic split/compaction and invoking them at low load time.
Grep your region server logs, if you find many messages saying "Blocking updates for ...", it is possible that many updates were blocked, and those updates might have poor response time.
To fix this issue, we need to tune both server side and client side configurations to gain a stable write speed.
We will describe the most important server side tuning to avoid update blocking in this recipe.
Getting ready Log in to your master node by the user who starts HBase.
Sync the changes across the cluster and restart HBase to apply the changes.
An edit first writes to RegionServer's HLog (Write Ahead Log), where the edit is persisted on HDFS.
After that, the edit goes to the hosting HRegion, and then its column family HStore's in-memory space, which is called MemStore.
When the size of MemStore reaches a threshold, it is flushed to a StoreFile on HDFS.
StoreFiles save the data using the HFile file format internally.
To update/delete any old data, instead of overwriting it, HBase adds a newer version to the data.
This makes HBase write very fast because all writes are sequential operations.
When there are many small StoreFiles on HDFS, HBase starts compaction to rewrite them into fewer but bigger ones.
A region will be split into two halves if its size reaches a threshold.
To prevent long time compact/split and out of memory error, HBase blocks updates if a region's MemStore size reaches a threshold, which is defined by:
This value is too small in write-heavy clusters during spikes in update traffic, so we need to increase the blocking threshold.
We increased it to 20, which is a reasonably big value for write-heavy clusters.
The side effects are that there will be more files to compact.
This tuning normally reduces the chances that updates blocking happens.
While, as we just mentioned, it has side effects, too.
We recommend you to tune these settings carefully, and watch writes throughputs and latencies during the tuning to find the best configuration values.
Tuning memory size for MemStores As we described in recipe Avoiding update blocking on write-heavy clusters, HBase write operations are applied in the hosting region's MemStore at first, and then flushed to HDFS to save memory space when MemStore size reaches a threshold.
MemStore flush runs on background threads using a snapshot of the MemStore.
Thus HBase keeps handling writes even when the MemStores are being flushed.
If the write spike is so high that the MemStore flush cannot catch up, the speed writes fill MemStores and memory used by MemStores will keep growing.
If the size of all MemStores in a region server reaches a configurable threshold, updates are blocked and flushes are forced.
We will describe how to tune this total MemStore memory size to avoid update blocking in this recipe.
Getting ready Log in to your master node as the user who starts HBase.
The following steps need to be carried out to tune memory size for MemStores:
Sync the changes across the cluster and restart HBase to apply the changes.
This is a configuration to prevent HBase from running out of memory by write spikes.
But, if you detected many log entries saying Flush of region xxxx due to global heap pressure in your region server logs, then you may need to tune this property to handle the high write rate.
The default value is 35% of the region server heap size.
On write-heavy clusters, increasing these two values helps reduce the chances that updates are blocked due to MemStore size limiting.
On the other hand, you will need to tune it carefully to avoid the situation of full garbage collection or out of memory error.
Normally, read performance is not so important as writes on write-heavy clusters.
Thus we can tune the cluster to optimize for writes.
One of the optimizations is to reduce the memory space allocated to HBase block cache, and make the room for MemStores.
See the recipe Increasing block cache size on read-heavy clusters for how to tune block cache size.
Client-side tuning for low latency systems We have introduced several recipes to avoid server side blocking.
Those recipes should help the cluster run stably and with high performance.
Cluster throughput and average latency will be improved significantly by server-side tuning.
However, in low latency and real-time systems, just server-side tuning is not enough.
Even if it only occurs slightly, long time pause is not acceptable in low latency systems.
There are client-side configurations we can tune to avoid long time pause.
In this recipe, we will describe how to tune those configurations and how they work.
Getting ready Log in to your HBase client node as the user who accesses HBase.
Follow these instructions to perform client side tuning for write-heavy clusters:
The sleep time between each retry is calculated with the following command:
Where RETRY_BACKOFF is a retry back off multiplier table, which has the following definition:
This setting disables Nagle's Algorithm for socket transmission between client and servers.
Nagle's Algorithm is a means to improve the efficiency of networks, by buffering a number of small outgoing messages, and sending them all at once.
Low latency systems should disable Nagle's Algorithm by setting hbase.ipc.
Configuring block cache for column families HBase supports block cache to improve read performance.
When performing a scan, if block cache is enabled and there is room remaining, data blocks read from StoreFiles on HDFS are cached in region server's Java heap space, so that next time, accessing data in the same block can be served by the cached block.
Block cache helps in reducing disk I/O for retrieving data.
Different column families can have different cache priorities or even disable the block cache.
Applications leverage this cache mechanism to fit different data sizes and access patterns.
In this recipe, we will describe how to configure block cache for column families and tips to leverage HBase block cache.
The following steps need to be carried out to configure block cache at column family level:
Execute the following to create a table with three column families:
For family f1, we didn't specify any properties, so all its properties were set to default.
HBase block cache contains three levels of block priority: single, multiple, and in-memory access.
A block is added with an in-memory flag (this means HBase will try to keep that block in memory more aggressively, but it is not guaranteed) if necessary, otherwise it becomes a single access priority.
Once a block is accessed again, it is marked as multiple access.
The column family f2 we created above, was marked as in-memory enabled.
Thus blocks belonging to that family are cached with in-memory priority.
For family f3, block cache is disabled, which means the column family's data blocks will not be cached.
As data is cached per-block basis, it is very efficient to access data in the same block.
Therefore, putting data which will be accessed at the same time in the same column family is a good practice for table schema design.
For example, when using HBase to store web pages crawled from the Internet, it is good to have a meta column family with in-memory enabled to hold metadata of a web page, and a raw column family to store the raw content of the page.
You can also change the block cache property for an existing column family by using the alter command via HBase Shell:
Change block size for the table using the alter command:
See also f Increasing block cache size on read-heavy clusters.
As described in recipe Tuning memory size for MemStores and Configuring block cache for column families, region server allocates a large amount of its Java heap space for MemStores to increase write performance.
It also uses lots of the heap space to cache StoreFile blocks to improve read performance.
While on read-heavy clusters, as read performance is more important, you might want to allocate more memory to block cache.
In this recipe, we will describe how to increase block cache size.
We will also include tips for finding out whether block cache is enough or not.
Getting ready Log in to your master node as the user who starts HBase.
The following steps need to be carried out to increase block cache size:
Sync the changes across the cluster and restart HBase to apply the changes.
This property specifies the percentage of maximum region server heap to allocate for block cache.
By default, it allocates 25% of the maximum heap size.
On read-heavy clusters, it is recommended to reduce the space for MemStores and allocate more memory to block cache.
MemStores and block cache normally consume approximately 60%~70% of the maximum region server heap size.
The total value of MemStore upper limit and block cache should not be higher than this level except when you are absolutely sure it will be fine.
Another indicator to determine how much memory we should allocate for block cache is to check the region server metrics, either on Ganglia or the HBase web UI.
For example, click the region server link on the HBase web UI; you will find the most important metrics of the region server:
Check the MemStore size, block cache size, hit ratio, and so forth on the page; you will find this information useful to help you tune the cluster.
If block cache hit ratio is very low, you may need to review your table schema and data access pattern; put columns together if they are always accessed at the same time.
Using Bloom Filter is another solution to increase block cache hit ratio.
If you are encountering many block evictions, consider increasing the block cache size to fit more blocks.
Client side scanner setting To achieve better read performance, besides server side tuning, what's important is the scanner setting at client application side.
Better client scanner setting makes the scan process much more efficient.
By contrast, a badly configured scanner will not only slow down the scan itself, but also have a negative effect on the region server.
So we need to configure the client side scanner settings carefully.
The most important scanner settings include scan caching, scan attribute selection, and scan block caching.
We will describe how to configure these settings properly in this recipe.
Getting ready Log in to your HBase client node by the user who accesses HBase.
The following steps need to be followed to change client side scanner settings:
Only fetch the columns you need by specifying the column families and qualifiers.
To disable block cache for a particular scan, add the following to your code.
Note that this does not disable block cache on the server side, but prevents the blocks scanned by the scanner to be cached.
This means the region server will transfer 500 rows at a time to the client to be processed.
For some situations, such as running MapReduce to read data from HBase, setting this value to a bigger one makes the scan process much more efficient than the default.
However, a higher caching value requires more memory to cache the rows for both the client and region server.
It also has the risk that the client may time out before it completes processing the date set and calls next() on the scanner.
We will introduce how to set per-scan basis caching in the There's more...
Step 2 shows the idea that if only a small subset of a column family is to be processed, then only the required data should be transferred to clients.
This is especially important when large amounts of rows are to be processed (for example, during MapReduce)
The overhead of nonneed data transfer becomes a performance penalty over large datasets.
A block will not be added to block cache if block cache is disabled on the scanner where the block is scanned.
As we did in step 3, disabling block cache for a particular scan is sometimes very important.
For example, when full scanning a table with MapReduce, you should disable block cache for the scan because all blocks in the table will be scanned, which fills up the block cache space with one time access blocks, and triggers a cache eviction process again and again.
All client sessions on that node will inherit this default caching row number after the change.
However, you can also specify a per-scan basis caching rows by using the HBase client Scan API.
The following code sets the scanner to fetch 1000 rows when next() is called:
HBase data are stored as StoreFile in the HFile format.
HFile block is the smallest unit of data that HBase reads from its StoreFiles.
It is also the basic element that region server caches in the block cache.
The size of the HFile block is an important tuning parameter.
To achieve better performance, we should select different block sizes, based on the average Key/Value size and disk I/O speed.
Like block cache and Bloom Filter, HFile block size is also configurable at the column family level.
We will describe how to show the average Key/Value size and tune block size to improve seek performance in this recipe.
The following steps need to be carried out to tune block size to improve seek performance:
Set block size for a particular column family via HBase Shell:
First of all, we want to know the average Key/Value size of a particular column family.
As described in recipe HFile tool, view textualized HFile content in Chapter 3, with -m option, we can get metadata, including average Key/Value size, of an HFile file using this tool.
In step 1, we passed the -f option and the file path to show a single HFile's metadata.
If your table has only one column family, you can also use the -r option and a region name to show metadata of each HFile that belongs to that region.
With this information, we are able to get the approximate average Key/Value size of a column family.
This is much smaller than the 64KB default block size.
Here we chose smaller block sizes for faster random access at the expense of larger block indices (more memory consumption)
On the other hand, if average Key/Value is large, or slow disks cause bottlenecks, we should select a larger block size, so that a single disk I/O can fetch more data.
You can also change block size for an existing column family by using the alter command from HBase Shell.
HBase supports Bloom Filter to improve the overall throughput of the cluster.
A HBase Bloom Filter is a space-efficient mechanism to test whether a StoreFile contains a specific row or row-col cell.
Without Bloom Filter, the only way to decide if a row key is contained in a StoreFile is to check the StoreFile's block index, which stores the start row key of each block in the StoreFile.
It is very likely that the row key we are finding will drop in between two block start keys; if it does then HBase has to load the block and scan from the block's start key to figure out if that row key actually exists.
The problem here is that there will be a number of StoreFiles that exist before a major compaction aggregates them into a single one.
Thus several StoreFiles may have some cells of the requested row key.
Think about the following example; it is an image showing how HBase stores data, block indices, and Bloom Filters in StoreFile:
A single column family has four StoreFiles, which are created by MemStore flushes.
Some row keys (for example, row-D) are localized to a few StoreFiles, while some (for example, row-F) are spread among many StoreFiles.
Bloom Filter is used to reduce these unnecessary disk I/O, and thus improve the overall throughput of the cluster.
In this recipe, we will describe how to enable Bloom Filter for a column family, and tips to leverage HBase Bloom Filter to improve the overall throughput.
The following steps need to be carried out to enable Bloom Filter for a column family:
Execute the following to create a table with three column families:
For family f1, we didn't specify any properties, so all its properties were set to default.
If Bloom Filter is enabled, HBase will add Bloom Filter along with data blocks when creating a StoreFile.
Bloom Filter is used to let HBase efficiently figure out whether a StoreFile contains a specific row or cell, without actually loading the file and scanning the block.
Bloom Filter can be False Positives, which means a query returns a row is contained in a file but it is actually not.
But False Negatives are not allowed for Bloom Filter, thus when a query returns a row is not in a file, the row will be definitely not in the file.
Reducing the error rate requires more space to store Bloom Filter.
It is configurable at column family basis, to enable a row level or row + column level Bloom Filter.
A row level Bloom Filter is used to test whether a row key is contained in a StoreFile, while a row + column Bloom Filter can tell HBase whether a StoreFile contains a specific cell.
Row + column level Bloom Filter takes more disk space because cell entries are much bigger than row entries.
Even when Bloom Filter is enabled, you may not gain immediate performance on individual get operations, as HBase reads data in parallel, and the latency is bound by disk I/O speed.
However, as the number of blocks loaded is greatly reduced, it improves the overall throughput significantly, especially in heavy load clusters.
Another advantage is that using Bloom Filter could improve block cache ratio.
With Bloom Filter enabled, HBase can load fewer blocks to fetch the data that clients requested.
Because unnecessary blocks are not loaded,  blocks containing the data that clients actually request, have more chance to remain in the block cache.
The downside is, every entry in Bloom Filter uses about one byte storage.
Therefore, we would recommend you to disable Bloom Filter for small cell column families, and always enable it for medium or large cell families.
As we mentioned above, HBase supports row level and row + column level Bloom Filters.
Which one to use, depends on your data access pattern.
Returning to our example HStore, only a few StoreFiles hold row-D.
If most of your cells in a row are updated together, row level filter is better.
It is obvious that row + column filter makes no sense if you always request the entire row.
For example, to load the entire row-F, a region server will need to load all four StoreFiles anyway.
When your read pattern is to load only a few columns of a row, for example, just the columns of row-F in StoreFile 4, then the row + column filter is useful, as the region server will skip loading the other StoreFiles.
If you have too many cells in your StoreFile, you might exceed this number.
You will need to use a row level filter in order to reduce the key number in the Bloom Filter.
Bloom Filter is an efficient way to improve the overall performance of your cluster.
The row level Bloom Filter works well for many cases; you should try it as your first choice, and consider the row + column Bloom Filter only when the row level filter does not fit your usage.
You can also change the Bloom Filter property for existing tables' column families with the alter command via HBase Shell.
It will be applied to StoreFiles created after the change.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Solve real-world PostgreSQL problems with over 100 simple yet incredibly effective recipes.
Monitor your database ensuring that it performs as quickly as possible.
Set up MySQL to perform administrative tasks such as efficiently managing data and database schema, improving the performance of MySQL servers, and managing user credentials.
Restrict access sensibly and regain access to your database in case of loss of administrative user credentials.
Accelerate your PostgreSQL system and avoid the common pitfals that can slow it down.
Avoid the common pitfalls that can slow your system down.
Over 150 recipes to design and optimize large-scale Apache Cassandra deployments.
Get the best out of Cassandra using this efficient recipe bank.
Well illustrated, step-by-step recipes to make all tasks look easy!
Chapter 2: Data Migration Introduction Importing data from MySQL via single client Importing data from TSV files using the bulk load tool Writing your own MapReduce job to import data Precreating regions before moving data into HBase.
Chapter 4: Backing Up and Restoring HBase Data Introduction Full shutdown backup using distcp Using CopyTable to copy data from one table to another Exporting an HBase table to dump files on HDFS Restoring HBase data by importing dump files from HDFS Backing up NameNode metadata Backing up region starting keys Cluster replication.
Chapter 5: Monitoring and Diagnosis Introduction Showing the disk utilization of HBase tables Setting up Ganglia to monitor an HBase cluster OpenTSDB—using HBase to monitor an HBase cluster Setting up Nagios to monitor HBase processes Using Nagios to check Hadoop/HBase logs Simple scripts to report the status of the cluster Hot region—write diagnosis.
Chapter 9: Advanced Configurations and Tuning Introduction Benchmarking HBase cluster with YCSB Increasing region server handler count Precreating regions using your own algorithm Avoiding update blocking on write-heavy clusters Tuning memory size for MemStores Client-side tuning for low latency systems Configuring block cache for column families Increasing block cache size on read-heavy clusters Client side scanner setting Tuning block size to improve seek performance Enabling Bloom Filter to improve the overall throughput.
