Some ancillaries, including electronic and print components, may not be available to customers outside the United States.
All credits appearing on page or at the end of the book are considered to be an extension of the copyright page.
The Internet addresses listed in the text were accurate at the time of publication.
The inclusion of a Web site does not indicate an endorsement by the authors of McGraw-Hill, and McGraw-Hill does not guarantee the accuracy of the information presented at these sites.
To my wife, Sita my children, Madhur and Advaith and my mother, Indira.
Database management has evolved from a specialized computer application to a central component of a modern computing environment, and, as a result, knowledge about database systems has become an essential part of an education in computer science.
In this text, we present the fundamental concepts of database management.
These concepts include aspects of database design, database languages, and database-system implementation.
This text is intended for a first course in databases at the junior or senior undergraduate, or first-year graduate, level.
In addition to basic material for a first course, the text contains advanced material that can be used for course supplements, or as introductory material for an advanced course.
We assume only a familiarity with basic data structures, computer organization, and a high-level programming language such as Java, C, or Pascal.
We present concepts as intuitive descriptions, many of which are based on our running example of a university.
Important theoretical results are covered, but formal proofs are omitted.
In place of proofs, figures and examples are used to suggest why a result is true.
Formal descriptions and proofs of theoretical results may be found in research papers and advanced texts that are referenced in the bibliographical notes.
The fundamental concepts and algorithms covered in the book are often based on those used in existing commercial or experimental database systems.
Our aim is to present these concepts and algorithms in a general setting that is not tied to one particular database system.
Details of particular database systems are discussed in Part 9, “Case Studies.”
In this, the sixth edition of Database System Concepts, we have retained the overall style of the prior editions while evolving the content and organization to reflect the changes that are occurring in thewaydatabases are designed,managed, and used.
We have also taken into account trends in the teaching of database concepts and made adaptations to facilitate these trends where appropriate.
The text is organized in nine major parts, plus five appendices.
Chapter 1 provides a general overview of the nature and purpose of database systems.
We explain how the concept of a database system has developed, what the common features of database systems are, what a database system does for the user, and how a database system interfaces with operating systems.
We also introduce an example database application: a university organization consisting of multiple departments, instructors, students, and courses.
This application is used as a running example throughout the book.
Chapter 2 introduces the relational model of data, covering basic concepts such as the structure of relational databases, database schemas, keys, schema diagrams, relational query languages, and relational operations.
Chapter 6 covers the formal relational query languages: relational algebra, tuple relational calculus, and domain relational calculus.
The chapters in this part describe data manipulation: queries, updates, insertions, anddeletions, assuminga schemadesignhas beenprovided.
The theory of functional dependencies and normalization is covered, with emphasis on the motivation and intuitive understanding of each normal form.
This chapter begins with an overview of relational design and relies on an intuitive understanding of logical implication of functional dependencies.
This chapter emphasizes the construction of database applications with Web-based interfaces.
Chapter 10 deals with storage devices, files, and data-storage structures.
A variety of data-access techniques are presented in Chapter 11, including B+-tree indices and hashing.
These chapters provide anunderstanding of the internals of the storage and retrieval components of a database.
It provides an overview of the methods used to ensure these properties, including locking and snapshot isolation.
Chapter 15 focuses on concurrency control and presents several techniques.
Alternatives to serializability are covered, most notably the widely-used snapshot isolation, which is discussed in detail.
Chapter 16 covers the primary techniques for ensuring correct transaction execution despite system crashes and storage failures.
Chapter 17 covers computer-system architecture, and describes the influence of the underlying computer system on the database system.
We discuss centralized systems, client–server systems, and parallel and distributed architectures in this chapter.
Chapter 18, on parallel databases, explores a variety of parallelization techniques, including I/O parallelism, interquery and intraquery parallelism, and interoperation and intraoperation parallelism.
Chapter 19 covers distributed database systems, revisiting the issues of database design, transaction management, and query evaluation and optimization, in the context of distributed databases.
The chapter also covers issues of system availability during failures, heterogeneous distributed databases, cloud-based databases, and distributed directory systems.
Chapter 20 introduces the concepts of data warehousing and data mining.
It can therefore be incorporated easily into a course that focuses on SQL and on database design.
The chapter describes the object-relational data model, which extends the relational data model to support complex data types, type inheritance, and object identity.
The chapter also describes database access from object-oriented programming languages.
Chapter 25 covers spatial and geographic data, temporal data, multimedia data, and issues in the management of mobile and personal databases.
In this part, we present case studies of four of the leading database systems, PostgreSQL, Oracle, IBM DB2, and Microsoft SQL Server.
These chapters outline unique features of each of these systems, and describe their internal structure.
They provide a wealth of interesting information about the respective products, and help you see how the various implementation techniques described in earlier parts are used in real systems.
They also cover several interesting practical aspects in the design of real systems.
We provide five appendices that cover material that is of historical nature or is advanced; these appendices are available only online on the Web site of the book (http://www.db-book.com)
An exception is Appendix A, which presents details of our university schema including the full schema, DDL, and all the tables.
Appendix C describes advanced relational database design, including the theory of multivalued dependencies, join dependencies, and the project-join and domain-key normal forms.
This appendix is for the benefit of individuals who wish to study the theory of relational database design in more detail, and instructors who wish to do so in their courses.
This appendix, too, is available only online, on the Web site of the book.
Although most new database applications use either the relational model or the object-relational model, the network and hierarchical data models are still in use in some legacy applications.
The production of this sixth edition has been guided by the many comments and suggestions we received concerning the earlier editions, by our own observations while teaching at Yale University, Lehigh University, and IIT Bombay, and by our analysis of the directions in which database technology is evolving.
We have replaced the earlier running example of bank enterprise with a university example.
This example has an immediate intuitive connection to students that assists not only in remembering the example, but, more importantly, in gaining deeper insight into the various design decisions that need to be made.
We have reorganized the book so as to collect all of our SQL coverage together and place it early in the book.
In Chapter 5, we present JDBC along with other means of accessing SQL from a general-purpose programming language.
We present triggers and recursion, and then conclude with coverage of online analytic processing (OLAP)
Introductory courses may choose to cover only certain sections of Chapter 5 or defer sections until after the coverage of database design without loss of continuity.
Beyond these two major changes, we revised the material in each chapter, bringing the older material up-to-date, adding discussions on recent developments in database technology, and improving descriptions of topics that students found difficult to understand.
In order to give students ample time for the projects, particularly for universities and colleges on the quarter system, it is essential to teach SQL as early as possible.
With this in mind, we have undertaken several changes in organization:
A new chapter on the relational model (Chapter 2) precedes SQL, laying the conceptual foundation, without getting lost in details of relational algebra.
These chapters also discuss variants supported by different database systems, to minimize problems that students face when they execute queries on actual database systems.
These chapters cover all aspects of SQL, including queries, data definition, constraint specification, OLAP, and the use of SQL from within a variety of languages, including Java/JDBC.
Formal languages (Chapter 6) have been postponed to after SQL, and can be omitted without affecting the sequencing of other chapters.
We adopted a new schema, which is based on university data, as a running example throughout the book.
This schema is more intuitive andmotivating for students than the earlier bank schema, and illustrates more complex design trade-offs in the database-design chapters.
To facilitate following our running example, we list the database schema and the sample relation instances for our university database together in Appendix A as well as where they are used in the various regular chapters.
In addition, we provide, on ourWeb site http://www.db-book.com, SQL data-definition statements for the entire example, along with SQL statements to create our example relation instances.
This encourages students to run example queries directly on a database system and to experiment with modifying those queries.
The E-R diagram notation in Chapter 7 has beenmodified tomake it more compatible with UML.
The chapter also makes good use of the new university database schema to illustrate more complex design trade-offs.
Chapter 8 now has a more readable style, providing an intuitive understanding of functional dependencies and normalization, before covering functional dependency theory; the theory is motivated much better as a result.
Expandedmaterial on application development and security.Chapter 9 has new material on application development, mirroring rapid changes in the field.
In particular, coverage of security has been expanded, considering its criticality in today’s interconnected world, with an emphasis on practical issues over abstract concepts.
Revised and updated coverage of data storage, indexing and query optimization.
Chapter 10 has been updated with new technology, including expanded coverage of flash memory.
Coverage of B+-trees in Chapter 11 has been revised to reflect practical implementations, including coverage of bulk loading, and the presentation has been improved.
Chapter 16 now has a simplified description of basic log-based recovery leading up to coverage of the ARIES algorithm.
We now cover cloud data storage, which is gaining significant interest for business applications.
Cloud storage offers enterprises opportunities for improved costmanagement and increased storage scalability, particularly for Web-based applications.
We examine those advantages along with the potential drawbacks and risks.
Multidatabases, which were earlier in the advanced transaction processing chapter, are now covered earlier as part of the distributed database chapter.
Postponed coverage of object databases andXML.Although object-oriented languages and XML are widely used outside of databases, their use in databases is still limited, making them appropriate for more advanced courses, or as supplementary material for an introductory course.
These topics, which were earlier part of a chapter on “other relational languages,” are now covered in online Appendix C.
All topics not listed above are updated from the fifth edition, though their overall organization is relatively unchanged.
Each chapter has a list of review terms, in addition to a summary, which can help readers review key topics covered in the chapter.
The exercises are divided into two sets: practice exercises and exercises.
The solutions for the practice exercises are publicly available on the Web site of the book.
Students are encouraged to solve the practice exercises on their own, and later use the solutions on the Web site to check their own solutions.
Solutions to the other exercises are available only to instructors (see “Instructor’s Note,” below, for information on how to get the solutions)
Many chapters have a tools section at the end of the chapter that provides information on software tools related to the topic of the chapter; some of these tools can be used for laboratory exercises.
The book contains both basic and advanced material, which might not be covered in a single semester.
We have marked several sections as advanced, using the symbol “**”
These sections may be omitted if so desired, without a loss of continuity.
Exercises that are difficult (and can be omitted) are also marked using the symbol “**”
It is possible to design courses by using various subsets of the chapters.
Some of the chapters can also be covered in an order different from their order in the book.
Chapter 5 (Advanced SQL) can be skipped or deferred to later without loss of continuity.
We expect most courses will cover at least Section 5.1.1 early, as JDBC is likely to be a useful tool in student projects.
We recommend covering Section 6.1 (relational algebra) if the course also covers query processing.
Alternatively, they can be used as an illustration of concepts when the earlier chapters are presented in class.
Model course syllabi, based on the text, can be found on the Web site of the book.
A Web site for the book is available at the URL: http://www.db-book.com.
Laboratory material, including SQL DDL and sample data for the university.
An instructor manual containing solutions to all exercises in the book.
We have endeavored to eliminate typos, bugs, and the like from the text.
But, as in new releases of software, bugs almost surely remain; an up-to-date errata list is accessible from the book’s Web site.
Wewould appreciate it if youwould notify us of any errors or omissions in the book that are not on the current list of errata.
We would be glad to receive suggestions on improvements to the book.
We also welcome any contributions to the book Web site that could be of use to other readers, such as programming exercises, project suggestions, online labs and tutorials, and teaching tips.
Many people have helped us with this sixth edition, as well as with the previous five editions from which it is derived.
Hakan Jakobsson (Oracle), for writing Chapter 28 on the Oracle database system.
Daniel Abadi for reviewing the table of contents of the fifth edition and helping with the new organization.
Judi Paige for her help in generating figures and presentation slides.
MarkWogahn for making sure that the software to produce the book, including LaTeX macros and fonts, worked properly.
Sarda for feedback that helpedus improve several chapters, in particular Chapter 11; VikramPudi formotivating us to replace the earlier bank schema; and Shetal Shah for feedback on several chapters.
Students at Yale, Lehigh, and IIT Bombay, for their comments on the fifth edition, as well as on preprints of the sixth edition.
Chen Li and Sharad Mehrotra for providing material on JDBC and security for the fifth edition.
Marilyn Turnamian andNandprasad Joshi provided secretarial assistance for the fifth edition, andMarilyn also prepared an early draft of the cover design for the fifth edition.
The idea of using ships as part of the cover concept was originally suggested to us by Bruce Stephan.
The following people offered suggestions and comments for the fifth and earlier editions of the book.
Sudarshan would like to acknowledge his wife, Sita, for her love and support, and children Madhur and Advaith for their love and joie de vivre.
Hank would like to acknowledge his wife, Joan, and his children, Abby and Joe, for their love and understanding.
Avi would like to acknowledge Valerie for her love, patience, and support during the revision of this book.
The collection of data, usually referred to as the database, contains information relevant to an enterprise.
The primary goal of a DBMS is to provide a way to store and retrieve database information that is both convenient and efficient.
Database systems are designed to manage large bodies of information.
Management of data involves both defining structures for storage of information and providing mechanisms for the manipulation of information.
In addition, the database systemmust ensure the safety of the information stored, despite system crashes or attempts at unauthorized access.
If data are to be shared among several users, the system must avoid possible anomalous results.
Because information is so important in most organizations, computer scientists have developed a large body of concepts and techniques for managing data.
These concepts and techniques form the focus of this book.
Human resources: For information about employees, salaries, payroll taxes, and benefits, and for generation of paychecks.
Manufacturing: For management of the supply chain and for tracking productionof items in factories, inventories of items inwarehouses and stores, and orders for items.
Online retailers: For sales data noted above plus online order tracking, generation of recommendation lists, and maintenance of online product evaluations.
Credit card transactions: For purchases on credit cards and generation of monthly statements.
Finance: For storing information about holdings, sales, and purchases of financial instruments such as stocks and bonds; also for storing real-time market data to enable online trading by customers and automated trading by the firm.
Universities: For student information, course registrations, and grades (in addition to standard enterprise information such as human resources and accounting)
Airlines were among the first to use databases in a geographically distributed manner.
Telecommunication: For keeping records of calls made, generating monthly bills, maintaining balances on prepaid calling cards, and storing information about the communication networks.
As the list illustrates, databases form an essential part of every enterprise today, storing not only types of information that are common to most enterprises, but also information that is specific to the category of the enterprise.
Over the course of the last four decades of the twentieth century, use of databases grew in all enterprises.
In the early days, very few people interacted directly with database systems, although without realizing it, they interacted with databases indirectly—through printed reports such as credit card statements, or through agents such as bank tellers and airline reservation agents.
Then automated teller machines came along and let users interact directly with databases.
Phone interfaces to computers (interactive voice-response systems) also allowed users to deal directly with databases—a caller could dial a number, and press phone keys to enter information or to select alternative options, to find flight arrival/departure times, for example, or to register for courses in a university.
The Internet revolution of the late 1990s sharply increaseddirect user access to databases.
Organizations converted many of their phone interfaces to databases into Web interfaces, and made a variety of services and information available online.
For instance, when you access an online bookstore and browse a book or music collection, you are accessing data stored in a database.
When you enter an order online, your order is stored in a database.
Furthermore, data about your Web accesses may be stored in a database.
Thus, although user interfaces hide details of access to a database, and most people are not even aware they are dealing with a database, accessing databases forms an essential part of almost everyone’s life today.
The importance of database systems can be judged in another way—today, database system vendors like Oracle are among the largest software companies in the world, and database systems form an important part of the product line of Microsoft and IBM.
Database systems arose in response to early methods of computerized management of commercial data.
As an example of such methods, typical of the 1960s, consider part of a university organization that, among other data, keeps information about all instructors, students, departments, and course offerings.
One way to keep the information on a computer is to store it in operating system files.
To allow users to manipulate the information, the system has a number of application programs that manipulate the files, including programs to:
Assign grades to students, compute grade point averages (GPA), and generate.
System programmers wrote these application programs to meet the needs of the university.
New application programs are added to the system as the need arises.
For example, suppose that a university decides to create a new major (say, computer science)
As a result, the university creates a newdepartment and creates newpermanent files (or adds information to existing files) to record information about all the instructors in the department, students in that major, course offerings, degree requirements, etc.
The university may have to write new application programs to deal with rules specific to the new major.
New application programs may also have to be written to handle new rules in the university.
Thus, as time goes by, the system acquires more files and more application programs.
This typical file-processing system is supported by a conventional operating system.
The system stores permanent records in various files, and it needs different application programs to extract records from, and add records to, the appropriate files.
Before database management systems (DBMSs) were introduced, organizations usually stored information in such systems.
Keeping organizational information in a file-processing system has a number of major disadvantages:
For example, if a student has a double major (say, music and mathematics) the address and telephone number of that student may appear in a file that consists of student records of students in the Music department and in a file that consists of student records of students in the Mathematics department.
In addition, it may lead to data inconsistency; that is, the various copies of the samedatamayno longer agree.
For example, a changed student address may be reflected in the Music department records but not elsewhere in the system.
Suppose that one of the university clerks needs to find out the names of all students who live within a particular postal-code area.
The clerk asks the data-processing department to generate such a list.
Because the designers of the original system did not anticipate this request, there is no application program on hand to meet it.
There is, however, an application program to generate the list of all students.
The university clerk has now two choices: either obtain the list of all students and extract the needed information manually or ask a programmer to write the necessary application program.
Suppose that such a program is written, and that, several days later, the same clerk needs to trim that list to include only those students who have taken at least 60 credit hours.
As expected, a program to generate such a list does not exist.
Again, the clerk has the preceding two options, neither of which is satisfactory.
The point here is that conventional file-processing environments do not allow needed data to be retrieved in a convenient and efficient manner.
Because data are scattered in various files, and files may be in different formats, writing new application programs to retrieve the appropriate data is difficult.
The data values stored in the database must satisfy certain types of consistency constraints.
Suppose the university maintains an account for each department, and records the balance amount in each account.
Suppose also that the university requires that the account balance of a department may never fall below zero.
Developers enforce these constraints in the system by adding appropriate code in the various application programs.
However, when new constraints are added, it is difficult to change the programs to enforce them.
The problem is compoundedwhen constraints involve several data items from different files.
A computer system, like any other device, is subject to failure.
In many applications, it is crucial that, if a failure occurs, the data.
Consider a program to transfer $500 from the account balance of department A to the account balance of department B.
If a system failure occurs during the execution of the program, it is possible that the $500 was removed from the balance of departmentA but was not credited to the balance of department B, resulting in an inconsistent database state.
Clearly, it is essential to database consistency that either both the credit and debit occur, or that neither occur.
That is, the funds transfer must be atomic—it must happen in its entirety or not at all.
It is difficult to ensure atomicity in a conventional file-processing system.
For the sake of overall performance of the system and faster response, many systems allow multiple users to update the data simultaneously.
Indeed, today, the largest Internet retailers may have millions of accesses per day to their data by shoppers.
In such an environment, interaction of concurrent updates is possible and may result in inconsistent data.
Suppose that the programs executing on behalf of eachwithdrawal read the old balance, reduce that value by the amount beingwithdrawn, andwrite the result back.
To guard against this possibility, the system must maintain some form of supervision.
But supervision is difficult to provide because datamay be accessed bymany different application programs that have not been coordinated previously.
As another example, suppose a registration program maintains a count of students registered for a course, in order to enforce limits on the number of students registered.
When a student registers, the program reads the current count for the courses, verifies that the count is not already at the limit, adds one to the count, and stores the count back in the database.
Not every user of the database system should be able to access all the data.
For example, in a university, payroll personnel need to see only that part of the database that has financial information.
They do not need access to information about academic records.
But, since application programs are added to the file-processing system in an ad hoc manner, enforcing such security constraints is difficult.
These difficulties, among others, prompted the development of database systems.
In what follows, we shall see the concepts and algorithms that enable database systems to solve the problems with file-processing systems.
In most of this book, we use a university organization as a running example of a typical data-processing application.
A database system is a collection of interrelated data and a set of programs that allow users to access and modify these data.
A major purpose of a database system is to provide users with an abstract view of the data.
That is, the system hides certain details of how the data are stored and maintained.
The need for efficiency has led designers to use complex data structures to represent data in the database.
Since many database-system users are not computer trained, developers hide the complexity from users through several levels of abstraction, to simplify users’ interactions with the system:
The lowest level of abstraction describes how the data are actually stored.
The physical level describes complex low-level data structures in detail.
The next-higher level of abstraction describes what data are stored in the database, and what relationships exist among those data.
The logical level thus describes the entire database in terms of a small number of relatively simple structures.
Although implementation of the simple structures at the logical level may involve complex physical-level structures, the user of the logical level does not need to be aware of this complexity.
Database administrators, who must decide what information to keep in the database, use the logical level of abstraction.
The highest level of abstraction describes only part of the entire database.
Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database.
Many users of the database system do not need all this information; instead, they need to access only a part of the database.
The view level of abstraction exists to simplify their interaction with the system.
The system may provide many views for the same database.
Figure 1.1 shows the relationship among the three levels of abstraction.
An analogy to the concept of data types in programming languages may.
This code defines a new record type called instructor with four fields.
Each field has a name and a type associated with it.
A university organization may have several such record types, including.
At the physical level, an instructor, department, or student record can be described as a block of consecutive storage locations.
Similarly, the database system hides many of the lowest-level storage details from database programmers.
Database administrators, on the other hand, may be aware of certain details of the physical organization of the data.
The actual type declaration depends on the language being used.
Java does not have such a declaration, but a simple class can be defined to the same effect.
At the logical level, each such record is described by a type definition, as in the previous code segment, and the interrelationship of these record types is defined as well.
Programmers using a programming language work at this level of abstraction.
Similarly, database administrators usually work at this level of abstraction.
Finally, at the view level, computer users see a set of application programs that hide details of the data types.
At the view level, several views of the database are defined, and a database user sees some or all of these views.
In addition to hiding details of the logical level of the database, the views also provide a security mechanism to prevent users from accessing certain parts of the database.
For example, clerks in the university registrar office can see only that part of the database that has information about students; they cannot access information about salaries of instructors.
Databases change over time as information is inserted and deleted.
The collection of information stored in the database at a particular moment is called an instance of the database.
The overall design of the database is called the database schema.
Each variable has a particular value at a given instant.
The values of the variables in a program at a point in time correspond to an instance of a database schema.
Database systems have several schemas, partitioned according to the levels of abstraction.
The physical schema describes the database design at the physical level, while the logical schema describes the database design at the logical level.
A database may also have several schemas at the view level, sometimes called subschemas, that describe different views of the database.
Application programs are said to exhibit physical data independence if they do not depend on the physical schema, and thus need not be rewritten if the physical schema changes.
We study languages for describing schemas after introducing the notion of data models in the next section.
Underlying the structure of adatabase is thedatamodel: a collectionof conceptual tools for describing data, data relationships, data semantics, and consistency constraints.
A data model provides a way to describe the design of a database at the physical, logical, and view levels.
There are a number of different data models that we shall cover in the text.
The data models can be classified into four different categories:
The relational model uses a collection of tables to represent both data and the relationships among those data.
Each table has multiple columns, and each column has a unique name.
The relational model is an example of a record-based model.
Record-based models are so named because the database is structured in fixed-format records of several types.
Each record type defines a fixed number of fields, or attributes.
The columns of the table correspond to the attributes of the record type.
The relational data model is the most widely used data model, and a vast majority of current database systems are based on the relational model.
An entity is a “thing” or “object” in the real world that is distinguishable from other objects.
This led to the development of an object-oriented data model that can be seen as extending the E-R model with notions of encapsulation, methods (functions), and object identity.
The object-relational data model combines features of the object-oriented data model and relational data model.
The semistructured data model permits the specification of data where individual data items of the same type may have different sets of attributes.
This is in contrast to the data models mentioned earlier, where every data item of a particular type must have the same set of attributes.
The Extensible Markup Language (XML) is widely used to represent semistructured data.
Historically, the network data model and the hierarchical data model preceded the relational datamodel.
Thesemodelswere tied closely to the underlying implementation, and complicated the task of modeling data.
As a result they are used little now, except in old database code that is still in service in some places.
They are outlined online in Appendices D and E for interested readers.
In practice, the data-definition and data-manipulation languages are not two separate languages; instead they simply form parts of a single database language, such as the widely used SQL language.
A data-manipulation language (DML) is a language that enables users to access or manipulate data as organized by the appropriate data model.
Procedural DMLs require a user to specify what data are needed and how to get those data.
Declarative DMLs (also referred to as nonprocedural DMLs) require a user to specify what data are needed without specifying how to get those data.
Declarative DMLs are usually easier to learn and use than are procedural DMLs.
However, since a user does not have to specify how to get the data, the database system has to figure out an efficient means of accessing data.
A query is a statement requesting the retrieval of information.
The portion of a DML that involves information retrieval is called a query language.
Although technically incorrect, it is common practice to use the terms query language and data-manipulation language synonymously.
There are a number of database query languages in use, either commercially or experimentally.
The levels of abstraction that we discussed in Section 1.3 apply not only to defining or structuring data, but also to manipulating data.
At the physical level, we must define algorithms that allow efficient access to data.
At higher levels of abstraction, we emphasize ease of use.
The goal is to allow humans to interact efficiently with the system.
We specify a database schema by a set of definitions expressed by a special language called a data-definition language (DDL)
The DDL is also used to specify additional properties of the data.
We specify the storage structure and access methods used by the database system by a set of statements in a special type of DDL called a data storage and definition language.
These statements define the implementation details of the database schemas, which are usually hidden from the users.
The data values stored in the database must satisfy certain consistency constraints.
For example, suppose the university requires that the account balance of a department must never be negative.
The database system checks these constraints every time the database is updated.
In general, a constraint can be an arbitrary predicate pertaining to the database.
Thus, database systems implement integrity constraints that can be testedwithminimal overhead:
A domain of possible values must be associated with every attribute (for example, integer types, character types, date/time types)
Declaring an attribute to be of a particular domain acts as a constraint on the values that it can take.
Domain constraints are the most elementary form of integrity constraint.
They are tested easily by the system whenever a new data item is entered into the database.
There are cases where we wish to ensure that a value that appears in one relation for a given set of attributes also appears in a certain set of attributes in another relation (referential integrity)
For example, the department listed for each course must be one that actually exists.
More precisely, the dept name value in a course record must appear in the dept name attribute of some record of the department relation.
An assertion is any condition that the database must always satisfy.
However, there are many constraints that we cannot express by using only these special forms.
For example, “Every department must have at least five courses offered every semester”must be expressed as an assertion.
When an assertion is created, the system tests it for validity.
If the assertion is valid, then any future modification to the database is allowed only if it does not cause that assertion to be violated.
We may want to differentiate among the users as far as the type of access they are permitted onvariousdata values in thedatabase.
These differentiations are expressed in terms of authorization, the most common being: read authorization, which allows reading, but not modification, of data; insert authorization, which allows insertion of new data, but not modification of existing data; update authorization, which allows modification, but not deletion, of data; and delete authorization, which allows deletion of data.
We may assign the user all, none, or a combination of these types of authorization.
The DDL, just like any other programming language, gets as input some instructions (statements) and generates some output.
The data dictionary is considered to be a special type of table that can only be accessed and updated by the database system itself (not a regular user)
The database system consults the data dictionary before reading or modifying actual data.
A relational database is based on the relational model and uses a collection of tables to represent both data and the relationships among those data.
In Chapter 2 we present a gentle introduction to the fundamentals of the relational model.
Each table has multiple columns and each column has a unique name.
Figure 1.2 presents a sample relational database comprising two tables: one shows details of university instructors and the other shows details of the various university departments.
The second table, department, shows, for example, that the Biology department is located in the Watson building and has a budget of $90,000
Of course, a real-world university would have many more departments and instructors.
We use small tables in the text to illustrate concepts.
A larger example for the same schema is available online.
The relational model is an example of a record-based model.
Record-based models are so named because the database is structured in fixed-format records of several types.
Each record type defines a fixed number of fields, or attributes.
The columns of the table correspond to the attributes of the record type.
It is not hard to see how tables may be stored in files.
For instance, a special character (such as a comma) may be used to delimit the different attributes of a record, and another special character (such as a new-line character) may be used to delimit records.
The relational model hides such low-level implementation details from database developers and users.
We also note that it is possible to create schemas in the relational model that have problems such as unnecessarily duplicated information.
For example, suppose we store the department budget as an attribute of the instructor record.
Then, whenever the value of a particular budget (say that one for the Physics department) changes, that change must to be reflected in the records of all instructors.
In Chapter 8, we shall study how to distinguish good schema designs from bad schema designs.
A query takes as input several tables (possibly only one) and always returns a single table.
Here is an example of an SQL query that finds the names of all instructors in the History department:
The query specifies that those rows from the table instructorwhere the dept name is Historymust be retrieved, and the name attribute of these rowsmust be displayed.
More specifically, the result of executing this query is a table with a single column.
If the query is run on the table in Figure 1.2, the result will consist of two rows, one with the name El Said and the other with the name Califieri.
For instance, the following query finds the instructor ID and department name of all instructors associated with a department with budget of greater than $95,000
For instance, the following SQL DDL statement defines the department table:
Execution of the above DDL statement creates the department table with three columns: dept name, building, and budget, each of which has a specific data type associated with it.
In addition, the DDL statement updates the data dictionary, which contains metadata (see Section 1.4.2)
The schema of a table is an example of metadata.
Such computations and actions must be written in a host language, such as C, C++, or Java, with embedded SQL queries that access the data in the database.
Application programs are programs that are used to interact with the database in this fashion.
Examples in a university system are programs that allow students to register for courses, generate class rosters, calculate student GPA, generate payroll checks, etc.
To access the database, DML statements need to be executed from the host language.
By providing an application program interface (set of procedures) that can be used to send DML and DDL statements to the database and retrieve the results.
The Open Database Connectivity (ODBC) standard for use with the C language is a commonly used application program interface standard.
The Java Database Connectivity (JDBC) standard provides corresponding features to the Java language.
By extending the host language syntax to embed DML calls within the host language program.
Usually, a special character prefaces DML calls, and a preprocessor, called the DML precompiler, converts the DML statements to normal procedure calls in the host language.
These large bodies of information do not exist in isolation.
They are part of the operation of some enterprise whose end product may be information from the database or may be some device or service for which the database plays only a supporting role.
Database design mainly involves the design of the database schema.
The design of a complete database application environment that meets the needs of the enterprise being modeled requires attention to a broader set of issues.
In this text, we focus initially on the writing of database queries and the design of database schemas.
Ahigh-level datamodel provides the database designerwith a conceptual framework in which to specify the data requirements of the database users, and how the database will be structured to fulfill these requirements.
The initial phase of database design, then, is to characterize fully the data needs of the prospective database users.
The database designer needs to interact extensively with domain experts andusers to carry out this task.
The outcome of this phase is a specification of user requirements.
Next, the designer chooses a data model, and by applying the concepts of the chosen data model, translates these requirements into a conceptual schema of the database.
The schema developed at this conceptual-design phase provides a detailed overview of the enterprise.
The designer reviews the schema to confirm that all data requirements are indeed satisfied and are not in conflict with one another.
The designer can also examine the design to remove any redundant.
The focus at this point is on describing the data and their relationships, rather than on specifying physical storage details.
A fully developed conceptual schema indicates the functional requirements of the enterprise.
In a specification of functional requirements, users describe the kinds of operations (or transactions) that will be performed on the data.
Example operations include modifying or updating data, searching for and retrieving specific data, and deleting data.
At this stage of conceptual design, the designer can review the schema to ensure it meets functional requirements.
The process of moving from an abstract data model to the implementation of the database proceeds in two final design phases.
In the logical-design phase, the designer maps the high-level conceptual schema onto the implementation data model of the database system that will be used.
The designer uses the resulting system-specific database schema in the subsequent physical-design phase, in which the physical features of the database are specified.
To illustrate the design process, let us examine how a database for a university could be designed.
The initial specification of user requirements may be based on interviews with the database users, and on the designer’s own analysis of the organization.
The description that arises from this design phase serves as the basis for specifying the conceptual structure of the database.
Each department is identified by a unique name (dept name), is located in a particular building, and has a budget.
Each course has associatedwith it a course id, title, dept name, and credits, and may also have have associated prerequisites.
Each instructor has name, associated department (dept name), and salary.
Each student has a name, an associatedmajor department (dept name), and tot cred (total credit hours the student earned thus far)
The university maintains a list of classrooms, specifying the name of the building, room number, and room capacity.
The university maintains a list of all classes (sections) taught.
Each section is identified by a course id, sec id, year, and semester, and has associated with it a semester, year, building, room number, and time slot id (the time slot when the class meets)
The department has a list of teaching assignments specifying, for each instructor, the sections the instructor is teaching.
The university has a list of all student course registrations, specifying, for each student, the courses and the associated sections that the student has taken (registered for)
A real university database would be much more complex than the preceding design.Howeverwe use this simplifiedmodel to help youunderstand conceptual ideas without getting lost in details of a complex design.
An entity is a “thing” or “object” in the real world that is distinguishable from other objects.
For example, each person is an entity, and bank accounts can be considered as entities.
Entities are described in a database by a set of attributes.
For example, the attributes dept name, building, and budgetmay describe one particular department in a university, and they form attributes of the department entity set.
Similarly, attributes ID, name, and salary may describe an instructor entity.2
The extra attribute ID is used to identify an instructor uniquely (since it may be possible to have two instructors with the same name and the same salary)
A unique instructor identifier must be assigned to each instructor.
For example, amember relationship associates an instructor with her department.
The set of all entities of the same type and the set of all relationships of the same type are termed an entity set and relationship set, respectively.
There are several ways in which to draw these diagrams.
One of the most popular is to use the Unified Modeling Language (UML)
In the notation we use, which is based on UML, an E-R diagram is represented as follows:
The astute reader will notice that we dropped the attribute dept name from the set of attributes describing the instructor entity set; this is not an error.
In Chapter 7 we shall provide a detailed explanation of why this is the case.
Entity sets are represented by a rectangular box with the entity set name in the header and the attributes listed below it.
Relationship sets are represented by a diamond connecting a pair of related entity sets.
The name of the relationship is placed inside the diamond.
As an illustration, consider part of a university database consisting of instructors and the departments with which they are associated.
The E-R diagram indicates that there are two entity sets, instructor and department, with attributes as outlined earlier.
The diagram also shows a relationship member between instructor and department.
In addition to entities and relationships, the E-R model represents certain constraints to which the contents of a database must conform.
One important constraint is mapping cardinalities, which express the number of entities to which another entity can be associated via a relationship set.
For example, if each instructor must be associated with only a single department, the E-R model can express that constraint.
Another method for designing a relational database is to use a process commonly known as normalization.
The goal is to generate a set of relation schemas that allows us to store information without unnecessary redundancy, yet also allows us to retrieve information easily.
The approach is to design schemas that are in an appropriate normal form.
To determine whether a relation schema is in one of the desirable normal forms, we need additional information about the real-world enterprise that we are modeling with the database.
The most common approach is to use functional dependencies, which we cover in Section 8.4
To understand the need for normalization, let us look at what can go wrong in a bad database design.
Among the undesirable properties that a bad design may have are:
We shall discuss these problems with the help of a modified database design for our university example.
Suppose that instead of having the two separate tables instructor and department, we have a single table, faculty, that combines the information from the two tables (as shown in Figure 1.4)
Notice that there are two rows in faculty that contain repeated information about the History department, specifically, that department’s building and budget.
The repetition of information in our alternative design is undesirable.
This change must be reflected in the two rows; contrast this with the original design, where this requires an update to only a single row.
Thus, updates are more costly under the alternative design than under the original design.
When we perform the update in the alternative database, we must ensure that every tuple pertaining to the History department is updated, or else our database will show two different budget values for the History department.
Now, let us shift our attention to the issue of “inability to represent certain information.” Suppose we are creating a new department in the university.
In the alternativedesign above,we cannot represent directly the information concerning a department (dept name, building, budget) unless that department has at least one instructor at the university.
This is because rows in the faculty table require values for ID, name, and salary.
This means that we cannot record information about the newly created department until the first instructor is hired for the new department.
One solution to this problem is to introduce null values.
The null value indicates that the value does not exist (or is not known)
An unknown value may be eithermissing (the value does exist, but we do not have that information) or not known (we do not know whether or not the value actually exists)
If we are not willing to deal with null values, then we can create a particular item of department information only when the department has at least one instructor associated with the department.
Furthermore, we would have to delete this information when the last instructor in the department departs.
Clearly, this situation is undesirable, since, under our original database design, the department information would be available regardless of whether or not there is an instructor associated with the department, and without resorting to null values.
An extensive theory of normalization has been developed that helps formally define what database designs are undesirable, and how to obtain desirable designs.
A database system is partitioned into modules that deal with each of the responsibilities of the overall system.
The functional components of a database system can be broadly divided into the storage manager and the query processor components.
The storage manager is important because databases typically require a large amount of storage space.
Corporate databases range in size from hundreds of gigabytes to, for the largest databases, terabytes of data.
Since the main memory of computers cannot store this much information, the information is stored on disks.
Data are moved between disk storage and main memory as needed.
Since the movement of data to and from disk is slow relative to the speed of the central processing unit, it is imperative that the database system structure the data so as to minimize the need to move data between disk and main memory.
The query processor is important because it helps the database system to simplify and facilitate access to data.
The query processor allows database users to obtain good performance while being able to work at the view level and not be burdenedwith understanding the physical-level details of the implementation of the system.
It is the job of the database system to translate updates and queries written in a nonprocedural language, at the logical level, into an efficient sequence of operations at the physical level.
The storage manager is the component of a database system that provides the interface between the low-level data stored in the database and the application programs and queries submitted to the system.
The storage manager is responsible for the interaction with the file manager.
The raw data are stored on the disk using the file system provided by the operating system.
The storage manager translates the various DML statements into low-level file-system commands.
Thus, the storage manager is responsible for storing, retrieving, and updating data in the database.
Authorization and integrity manager, which tests for the satisfaction of integrity constraints and checks the authority of users to access data.
Transaction manager, which ensures that the database remains in a consistent (correct) state despite system failures, and that concurrent transaction executions proceed without conflicting.
File manager, which manages the allocation of space on disk storage and the data structures used to represent information stored on disk.
Buffermanager, which is responsible for fetching data from disk storage into main memory, and deciding what data to cache in main memory.
The buffer manager is a critical part of the database system, since it enables the database to handle data sizes that are much larger than the size of main memory.
The storage manager implements several data structures as part of the physical system implementation:
Like the index in this textbook, a database index provides pointers to those data items that hold a particular value.
For example, we could use an index to find the instructor record with a particular ID, or all instructor records with a particular name.
Hashing is an alternative to indexing that is faster in some but not all cases.
A query can usually be translated into any of a number of alternative evaluationplans that all give the same result.
TheDMLcompiler alsoperforms query optimization; that is, it picks the lowest cost evaluation plan from among the alternatives.
Query evaluation engine, which executes low-level instructions generated by the DML compiler.
Often, several operations on the database form a single logical unit of work.
An example is a funds transfer, as in Section 1.2, in which one department account (say A) is debited and another department account (say B) is credited.
Clearly, it is essential that either both the credit and debit occur, or that neither occur.
That is, the funds transfer must happen in its entirety or not at all.
In addition, it is essential that the execution of the funds transfer preserve the consistency of the database.
That is, the value of the sum of the balances of A and B must be preserved.
Finally, after the successful execution of a funds transfer, the new values of the balances of accounts A and B must persist, despite the possibility of system failure.
A transaction is a collection of operations that performs a single logical function in a database application.
Each transaction is a unit of both atomicity and consistency.
That is, if the database was consistent when a transaction started, the database must be consistent when the transaction successfully terminates.
However, during the execution of a transaction, it may be necessary temporarily to allow inconsistency, since either the debit of A or the credit of B must be done before the other.
This temporary inconsistency, although necessary, may lead to difficulty if a failure occurs.
It is the programmer’s responsibility to define properly the various transactions, so that each preserves the consistency of the database.
For example, the transaction to transfer funds from the account of department A to the account of department B could be defined to be composed of two separate programs: one that debits account A, and another that credits account B.
In the absence of failures, all transactions complete successfully, and atomicity is achieved easily.
However, because of various types of failure, a transaction may not always complete its execution successfully.
If we are to ensure the atomicity property, a failed transaction must have no effect on the state of the database.
Thus, the database must be restored to the state in which it was before the transaction in question started executing.
The database system must therefore perform failure recovery, that is, detect system failures and restore the database to the state that existed prior to the occurrence of the failure.
Finally, when several transactions update the database concurrently, the consistency of datamayno longer be preserved, even though each individual transaction is correct.
The concept of a transaction has been applied broadly in database systems and applications.
While the initial use of transactions was in financial applications, the concept is now used in real-time applications in telecommunication, as well as in the management of long-duration activities such as product design or administrative workflows.
We are now in a position to provide a single picture (Figure 1.5) of the various components of a database system and the connections among them.
The architecture of a database system is greatly influenced by the underlying computer system on which the database system runs.
Database systems can be centralized, or client-server, where one server machine executes work on behalf ofmultiple clientmachines.
Database systems can also be designed to exploit parallel computer architectures.
In Chapter 17 we cover the general structure of modern computer systems.
Chapter 18 describes how various actions of a database, in particular query processing, can be implemented to exploit parallel processing.
Chapter 19 presents a number of issues that arise in a distributed database, and describes how to deal with each issue.
The issues include how to store data, how to ensure atomicity of transactions that execute at multiple sites, how to perform concurrency control, and how to provide high availability in the presence of failures.
Distributed query processing and directory systems are also described in this chapter.
Most users of a database system today are not present at the site of the database system, but connect to it through a network.
We can therefore differentiate between clientmachines, on which remote database users work, and server machines, on which the database system runs.
Database applications are usually partitioned into two or three parts, as in Figure 1.6
In a two-tier architecture, the application resides at the client machine, where it invokes database system functionality at the server machine through.
Application program interface standards like ODBC and JDBC are used for interaction between the client and the server.
In contrast, in a three-tier architecture, the client machine acts as merely a front end and does not contain any direct database calls.
Instead, the client end communicates with an application server, usually through a forms interface.
The application server in turn communicates with a database system to access data.
The business logic of the application, which says what actions to carry out under what conditions, is embedded in the application server, instead of being distributed across multiple clients.
Three-tier applications are more appropriate for large applications, and for applications that run on the World Wide Web.
The termdatamining refers loosely to the process of semiautomatically analyzing large databases to find useful patterns.
Like knowledge discovery in artificial intelligence (also called machine learning) or statistical analysis, data mining attempts to discover rules and patterns from data.
However, data mining differs from machine learning and statistics in that it deals with large volumes of data, stored primarily on disk.
That is, data mining deals with “knowledge discovery in databases.”
Some types of knowledge discovered from a database can be represented by a set of rules.
The following is an example of a rule, stated informally: “Young womenwith annual incomesgreater than$50,000 are themost likelypeople to buy small sports cars.” Of course such rules are not universally true, but rather have.
There are a variety of possible types of patterns that may be useful, and different techniques are used to find different types of patterns.
In Chapter 20 we study a few examples of patterns and see how theymay be automatically derived from a database.
Usually there is amanual component todatamining, consisting of preprocessing data to a form acceptable to the algorithms, and postprocessing of discovered patterns to find novel ones that could be useful.
There may also be more than one type of pattern that can be discovered from a given database, and manual interaction may be needed to pick useful types of patterns.
For this reason, data mining is really a semiautomatic process in real life.
However, in our description we concentrate on the automatic aspect of mining.
Businesses have begun to exploit the burgeoning data online to make better decisions about their activities, such as what items to stock and how best to target customers to increase sales.
Many of their queries are rather complicated, however, and certain types of information cannot be extracted even by using SQL.
Several techniques and tools are available to help with decision support.
Several tools for data analysis allow analysts to view data in different ways.
Other analysis tools precompute summaries of very large amounts of data, in order to give fast responses to queries.
The SQL standard contains additional constructs to support data analysis.
Large companies havediverse sources of data that theyneed touse formaking business decisions.
To execute queries efficiently on such diverse data, companies have built data warehouses.
Data warehouses gather data from multiple sources under a unified schema, at a single site.
Thus, they provide the user a single uniform interface to data.
Textual data is unstructured, unlike the rigidly structured data in relational databases.
Querying of unstructured textual data is referred to as information retrieval.
Information retrieval systems have much in common with database systems—in particular, the storage and retrieval of data on secondary storage.
However, the emphasis in the field of information systems is different from that in database systems, concentrating on issues such as querying based on keywords; the relevance of documents to the query; and the analysis, classification, and indexing of documents.
Several application areas for database systems are limited by the restrictions of the relational datamodel.As a result, researchers havedeveloped several datamodels to deal with these application domains, including object-based data models and semistructured data models.
This led to the development of an object-oriented data model that can be seen as extending the E-R model with notions of encapsulation, methods (functions), and object identity.
Inheritance, object identity, and encapsulation (information hiding), with methods to provide an interface to objects, are among the key concepts of object-oriented programming that have found applications in data modeling.
The object-oriented data model also supports a rich type system, including structured and collection types.
In the 1980s, several database systems based on the object-oriented data model were developed.
The major database vendors presently support the object-relational data model, a datamodel that combines features of the object-oriented datamodel and relational data model.
It extends the traditional relational model with a variety of features such as structured and collection types, as well as object orientation.
Semistructured data models permit the specification of data where individual data items of the same typemay have different sets of attributes.
This is in contrast with the data models mentioned earlier, where every data item of a particular type must have the same set of attributes.
The XML language was initially designed as a way of adding markup information to text documents, but has become important because of its applications in data exchange.
Chapter 23 describes the XML language, different ways of expressing queries on data represented in XML, and transforming XML data from one form to another.
A primary goal of a database system is to retrieve information from and store new information into the database.
People who work with a database can be categorized as database users or database administrators.
There are four different types of database-system users, differentiated by the way they expect to interact with the system.
Different types of user interfaces have been designed for the different types of users.
This program asks the clerk for the name of the new instructor, her new ID, the name of the department (that is, A), and the salary.
As another example, consider a student, who during class registration period, wishes to register for a class by using a Web interface.
Such a user connects to a Web application program that runs at a Web server.
The application first verifies the identity of the user, and allows her to access a form where she enters the desired information.
The form information is sent back to the Web application at the server, which then determines if there is room in the class (by retrieving information from the database) and if so adds the student information to the class roster in the database.
Application programmers can choose frommany tools to develop user interfaces.Rapid application development (RAD) tools are tools that enable an application programmer to construct forms and reports withminimal programming effort.
Instead, they form their requests either using a database query language or by using tools such as data analysis software.
Analysts who submit queries to explore data in the database fall in this category.
Specialized users are sophisticated users who write specialized database applications that do not fit into the traditional data-processing framework.
One of themain reasons for using DBMSs is to have central control of both the data and the programs that access those data.
A person who has such central control over the system is called a database administrator (DBA)
The DBA creates the original database schema by executing a set of data definition statements in the DDL.
By granting different types of authorization, the database administrator can regulate which parts of the database various users can access.
Periodically backing up the database, either onto tapes or onto remote servers, to prevent loss of data in case of disasters such as flooding.
Ensuring that enough free disk space is available for normal operations, and upgrading disk space as required.
Monitoring jobs running on the database and ensuring that performance is not degraded by very expensive tasks submitted by some users.
Information processing drives the growth of computers, as it has from the earliest days of commercial computers.
Punched cards, invented by Herman Hollerith, were used at the very beginning of the twentieth century to record U.S.
Punched cards were later widely used as a means of entering data into computers.
Techniques for data storage and processing have evolved over the years:
Data processing tasks such as payroll were automated, with data stored on tapes.
Processing of data consisted of reading data from one or more tapes and writing data to a new tape.
Data could also be input from punched card decks, and output to printers.
For example, salary raises were processed by entering the raises on punched cards and reading the punched card deck in synchronization with a tape containing the master salary details.
The records had to be in the same sorted order.
The salary raises would be added to the salary read from the master tape, and written to a new tape; the new tape would become the new master tape.
Tapes (and card decks) could be read only sequentially, and data sizes were.
The position of data on disk was immaterial, since any location on disk could be accessed in just tens of milliseconds.
With disks, network and hierarchical databases could be created that allowed data structures such as lists and trees to be stored on disk.
A landmark paper by Codd [1970] defined the relational model and nonprocedural ways of querying data in the relational model, and relational databaseswere born.
The simplicity of the relationalmodel and thepossibility of hiding implementation details completely from the programmer were enticing indeed.
That changed with System R, a groundbreaking project at IBMResearch that developed techniques for the construction of an efficient relational database system.
Excellent overviews of System R are provided by Astrahan et al.
The fully functional System R prototype led to IBM’s first relational database product, SQL/DS.
At the same time, the Ingres system was being developed at the University of California at Berkeley.
It led to a commercial product of the same name.
Initial commercial relational database systems, such as IBM DB2, Oracle, Ingres, and DEC Rdb, played a major role in advancing techniques for efficient processing of declarative queries.
By the early 1980s, relational databases had become competitive with network and hierarchical database systems even in the area of performance.
Relational databases were so easy to use that they eventually replaced network and hierarchical databases; programmers using such databases were forced to deal with many low-level implementation details, and had to code their queries in a procedural fashion.
Most importantly, they had to keep efficiency in mind when designing their programs, which involved a lot of effort.
In contrast, in a relational database, almost all these low-level tasks are carried out automatically by the database, leaving the programmer free to work at a logical level.
Since attaining dominance in the 1980s, the relational model has reigned supreme among data models.
The 1980s also saw much research on parallel and distributed databases, as well as initial work on object-oriented databases.
Decision support and querying re-emerged as a major application area for databases.
Tools for analyzing large amounts of data saw large growths in usage.
Many database vendors introduced parallel database products in this period.
Database vendors also began to add object-relational support to their databases.
Although XML is widely used for data exchange, as well as for storing certain complex data types, relational databases still form the core of a vast majority of large-scale database applications.
This period also saw a significant growth in use of open-source database systems, particularly PostgreSQL and MySQL.
The latter part of the decade has seen growth in specialized databases for data analysis, in particular column-stores, which in effect store each column of a table as a separate array, and highly parallel database systems designed for analysis of very large data sets.
Several novel distributed data-storage systems have been built to handle the datamanagement requirements of very large Web sites such as Amazon, Facebook, Google, Microsoft and Yahoo!, and some of these are now offered as Web services that can be used by application developers.
There has also been substantialwork onmanagement and analysis of streaming data, such as stock-market ticker data or computer networkmonitoring data.
The primary goal of a DBMS is to provide an environment that is both convenient and efficient for people to use in retrieving and storing information.
Database systems are ubiquitous today, and most people interact, either directly or indirectly, with databases many times every day.
Database systems are designed to store large bodies of information.
Themanagement of data involves both the definition of structures for the storage of information and the provision of mechanisms for the manipulation of information.
In addition, the database system must provide for the safety of the information stored, in the face of system crashes or attempts at unauthorized access.
If data are to be shared among several users, the system must avoid possible anomalous results.
A major purpose of a database system is to provide users with an abstract view of the data.
That is, the system hides certain details of how the data are stored and maintained.
Underlying the structure of a database is the data model: a collection of conceptual tools for describing data, data relationships, data semantics, and data constraints.
The relational datamodel is themost widely deployedmodel for storing data in databases.
Other data models are the object-oriented model, the objectrelational model, and semistructured data models.
A data-manipulation language (DML) is a language that enables users to access or manipulate data.
Nonprocedural DMLs, which require a user to specify only what data are needed, without specifying exactly how to get those data, are widely used today.
A data-definition language (DDL) is a language for specifying the database schema and as well as other properties of the data.
Database design mainly involves the design of the database schema.
It provides a convenient graphical representation to view data, relationships, and constraints.
The query processor subsystem compiles and executes DDL and DML statements.
Transaction management ensures that the database remains in a consistent (correct) state despite system failures.
The transaction manager ensures that concurrent transaction executions proceed without conflicting.
The architecture of a database system is greatly influenced by the underlying computer system on which the database system runs.
Database systems can be centralized, or client-server, where one server machine executes work on behalf of multiple client machines.
Database systems can also be designed to exploit parallel computer architectures.
Database applications are typicallybrokenup into a front-endpart that runs at client machines and a part that runs at the back end.
In two-tier architectures, the front end directly communicates with a database running at the back end.
In three-tier architectures, the back end part is itself broken up into an application server and a database server.
There are four different types of database-system users, differentiated by the way they expect to interact with the system.
Different types of user interfaces have been designed for the different types of users.
Java or C++ differs from the data definition language used in a database.
Consider each of the points listed in Section 1.2, as disadvantages of keeping data in a file-processing system.
Discuss the relevance of each of these points to the storage of actual video data, and to metadata about the video, such as title, the user who uploaded it, tags, and which users viewed it.
List key differences between the two, in terms of the way the queries are specified, and in terms of what is the result of a query.
For each responsibility, explain the problems that would arise if the responsibility were not discharged.
There are a large number of commercial database systems in use today.
We list below general-purpose books, research paper collections, and Web sites on databases.
Subsequent chapters provide references to material on each topic outlined in this chapter.
Codd [1970] is the landmark paper that introduced the relational model.
A book containing a collection of research papers on database management is offered by Hellerstein and Stonebraker [2005]
A review of accomplishments in database management and an assessment of future research challenges appears in Silberschatz et al.
Database vendor Web sites (see the Tools section above) provide details about their respective products.
A data model is a collection of conceptual tools for describing data, data relationships, data semantics, and consistency constraints.
The relationalmodel, which is covered in Chapter 2, uses a collection of tables to represent both data and the relationships among those data.
Its conceptual simplicity has led to its widespread adoption; today a vast majority of database products are based on the relationalmodel.
The relationalmodel describes data at the logical and view levels, abstracting away low-level details of data storage.
To make data from a relational database available to users, we have to address several issues.
The most important issue is how users specify requests for retrieving and updating data; several query languages have been developed for this task.
A second, but still important, issue is data integrity and protection; databases need to protect data from damage by user actions, whether unintentional or intentional.
Chapter 4 also covers integrity constraints which are enforced by the database, and authorization mechanisms, which control what access and update actions can be carried out by a user.
Chapter 5 covers more advanced topics, including access to SQL from programming languages, and the use of SQL for data analysis.
Chapter 6 covers three formal query languages, the relational algebra, the tuple relational calculus and the domain relational calculus, which are declarative query languages based on mathematical logic.
These formal languages form the basis for SQL, and for two other user-friendly languages, QBE and Datalog, which are described in Appendix B (available online at db-book.com)
The relational model is today the primary data model for commercial dataprocessing applications.
It attained its primary position because of its simplicity, which eases the job of the programmer, compared to earlier data models such as the network model or the hierarchical model.
In this chapter, we first study the fundamentals of the relational model.
A relational database consists of a collection of tables, each of which is assigned a unique name.
For example, consider the instructor table of Figure 2.1, which stores information about instructors.
The table has four column headers: ID, name, dept name, and salary.
Each row of this table records information about an instructor, consisting of the instructor’s ID, name, dept name, and salary.
Similarly, the course table of Figure 2.2 stores information about courses, consisting of a course id, title, dept name, and credits, for each course.
Note that each instructor is identified by the value of the column ID, while each course is identified by the value of the column course id.
Figure 2.3 shows a third table, prereq, which stores the prerequisite courses for each course.
The table has two columns, course id and prereq id.
Each row consists of a pair of course identifiers such that the second course is a prerequisite for the first course.
Thus, a row in the prereq table indicates that two courses are related in the sense that one course is a prerequisite for the other.
As another example, we consider the table instructor, a row in the table can be thought of as representing.
In general, a row in a table represents a relationship among a set of values.
Since a table is a collection of such relationships, there is a close correspondence between the concept of table and the mathematical concept of relation, fromwhich the relational data model takes its name.
In mathematical terminology, a tuple is simply a sequence (or list) of values.
A relationship between n values is represented mathematically by an n-tuple of values, i.e., a tuple with n values, which corresponds to a row in a table.
Thus, in the relationalmodel the term relation is used to refer to a table, while the term tuple is used to refer to a row.
Similarly, the term attribute refers to a column of a table.
ExaminingFigure 2.1,we can see that the relation instructorhas four attributes: ID, name, dept name, and salary.
We use the term relation instance to refer to a specific instance of a relation, i.e., containing a specific set of rows.
They do not include all the data an actual university database would contain, in order to simplify our presentation.
The order in which tuples appear in a relation is irrelevant, since a relation is a set of tuples.
For ease of exposition, we will mostly show the relations sorted by their first attribute.
For each attribute of a relation, there is a set of permitted values, called the domain of that attribute.
Thus, the domain of the salary attribute of the instructor relation is the set of all possible salary values, while the domain of the name attribute is the set of all possible instructor names.
We require that, for all relations r, the domains of all attributes of r be atomic.
A domain is atomic if elements of the domain are considered to be indivisible units.
For example, suppose the table instructor had an attribute phone number, which can store a set of phone numbers corresponding to the instructor.
Then the domain of phone numberwould not be atomic, since an element of the domain is a set of phone numbers, and it has subparts, namely the individual phone numbers in the set.
The important issue is not what the domain itself is, but rather how we use domain elements in our database.
Suppose now that the phone number attribute stores a single phone number.
Even then, if we split the value from the phone number attribute into a country code, an area code and a local number, we would be treating it as a nonatomic value.
If we treat each phone number as a single indivisible unit, then the attribute phone number would have an atomic domain.
In Chapter 22, we shall discuss extensions to the relational data model to permit nonatomic domains.
The null value is a special value that signifies that the value is unknown or does not exist.
For example, suppose as before that we include the attribute phone number in the instructor relation.
It may be that an instructor does not have a phone number at all, or that the telephone number is unlisted.
We would then have to use the null value to signify that the value is unknown or does not exist.
We shall see later that null values cause a number of difficulties when we access or update the database, and thus should be eliminated if at all possible.
We shall assume null values are absent initially, and in Section 3.6 we describe the effect of nulls on different operations.
When we talk about a database, we must differentiate between the database schema, which is the logical design of the database, and the database instance, which is a snapshot of the data in the database at a given instant in time.
In general, a relation schema consists of a list of attributes and their corresponding domains.
In contrast, the schema of a relation does not generally change.
Although it is important to know the difference between a relation schema and a relation instance, we often use the same name, such as instructor, to refer to both the schema and the instance.
Where required, we explicitly refer to the schema or to the instance, for example “the instructor schema,” or “an instance of the instructor relation.” However, where it is clear whether we mean the schema or the instance, we simply use the relation name.
Note that the attribute dept name appears in both the instructor schema and the department schema.
Rather, using common attributes in relation schemas is one way of relating tuples of distinct relations.
For example, suppose we wish to find the information about all the instructors who work in the Watson building.
We look first at the department relation to find the dept name of all the departments housed in Watson.
Then, for each such department, we look in the instructor relation to find the information about the instructor associated with the corresponding dept name.
Each course in a university may be offered multiple times, across different.
We need a relation to describe each individual offering, or section, of the class.
Figure 2.6 shows a sample instance of the section relation.
We need a relation to describe the association between instructors and the.
Figure 2.7 shows a sample instance of the teaches relation.
As you can imagine, there are many more relations maintained in a real university database.
In addition to those relations we have listed already, instructor, department, course, section, prereq, and teaches, we use the following relations in this text:
We must have a way to specify how tuples within a given relation are distinguished.
That is, the values of the attribute values of a tuple must be such that they can uniquely identify the tuple.
In other words, no two tuples in a relation are allowed to have exactly the same value for all attributes.
A superkey is a set of one or more attributes that, taken collectively, allow us to identify uniquely a tuple in the relation.
For example, the ID attribute of the relation instructor is sufficient to distinguish one instructor tuple from another.
The name attribute of instructor, on the other hand, is not a superkey, because several instructors might have the same name.
For example, the combination of ID and name is a superkey for the relation instructor.
If K is a superkey, then so is any superset of K.
We are often interested in superkeys for which no proper subset is a superkey.
We shall use the term primary key to denote a candidate key that is chosen by the database designer as the principal means of identifying tuples within a relation.
A key (whether primary, candidate, or super) is a property of the entire relation, rather than of the individual tuples.
Any two individual tuples in the relation are prohibited from having the same value on the key attributes at the same time.
The designation of a key represents a constraint in the real-world enterprise being modeled.
As we noted, the name of a person is obviously not sufficient, because there may be many people with the same name.
In the United States, the social-security number attribute of a person would be a candidate key.
An alternative is to use some unique combination of other attributes as a key.
The primary key should be chosen such that its attribute values are never, or very rarely, changed.
For instance, the address field of a person should not be part of the primary key, since it is likely to change.
Social-security numbers, on the other hand, are guaranteed never to change.
Unique identifiers generated by enterprises generally donot change, except if two enterprisesmerge; in such a case the same identifier may have been issued by both enterprises, and a reallocation of identifiers may be required to make sure they are unique.
It is customary to list the primary key attributes of a relation schema before the other attributes; for example, the dept name attribute of department is listed first, since it is the primary key.
For example, the attribute dept name in instructor is a foreign key from instructor, referencing department, since dept name is the primary key of department.
In any database instance, given any tuple, say ta , from the instructor relation, there must be some tuple, say tb , in the department relation such that the value of the dept name attribute of ta is the same as the value of the primary key, dept name, of tb.
It would be reasonable to require that if a section exists for a course, it must be taught by at least one instructor; however, it could possibly be taught by more than one instructor.
To enforce this constraint, we would require that if a particular (course id, sec id, semester, year) combination appears in section, then the same combination must appear in teaches.
However, this set of values does not form a primary key for teaches, since more than one instructor may teach one such section.
As a result, we cannot declare a foreign key constraint from section to teaches (although we can define a foreign key constraint in the other direction, from teaches to section)
The constraint from section to teaches is an example of a referential integrity constraint; a referential integrity constraint requires that the values appearing in specified attributes of any tuple in the referencing relation also appear in specified attributes of at least one tuple in the referenced relation.
A database schema, along with primary key and foreign key dependencies, can be depicted by schema diagrams.
Figure 2.8 shows the schema diagram for our university organization.
Each relation appears as a box, with the relation name at the top in blue, and the attributes listed inside the box.
Foreign key dependencies appear as arrows from the foreign key attributes of the referencing relation to the primary key of the referenced relation.
Referential integrity constraints other than foreign key constraints are not shown explicitly in schema diagrams.
Entityrelationship diagrams let us represent several kinds of constraints, including general referential integrity constraints.
Many database systems provide design tools with a graphical user interface for creating schema diagrams.
The enterprise that we use in the examples in later chapters is a university.
Figure 2.9 gives the relational schema that we use in our examples, with primarykey attributes underlined.
As we shall see in Chapter 3, this corresponds to the approach to defining relations in the SQL data-definition language.
A query language is a language in which a user requests information from the database.
These languages are usually on a level higher than that of a standard programming language.
Query languages can be categorized as either procedural or nonprocedural.
In a procedural language, the user instructs the system to perform a sequence of operations on the database to compute the desired result.
In a nonprocedural language, the user describes the desired information without giving a specific procedure for obtaining that information.
Query languages used in practice include elements of both the procedural and the nonprocedural approaches.
There are a number of “pure” query languages: The relational algebra is procedural, whereas the tuple relational calculus and domain relational calculus are nonprocedural.
These query languages are terse and formal, lacking the “syntactic sugar” of commercial languages, but they illustrate the fundamental techniques for extracting data from the database.
In Chapter 6, we examine in detail the relational algebra and the two versions of the relational calculus, the tuple relational calculus and domain relational calculus.
The relational algebra consists of a set of operations that take one or two relations as input and produce a new relation as their result.
The relational calculus uses predicate logic to define the result desired without giving any specific algebraic procedure for obtaining that result.
All procedural relational query languages provide a set of operations that can be applied to either a single relation or a pair of relations.
These operations have the nice and desired property that their result is always a single relation.
This property allows one to combine several of these operations in a modular way.
Specifically, since the result of a relational query is itself a relation, relational operations can be applied to the results of queries as well as to the given set of relations.
The specific relational operations are expressed differently depending on the language, but fit the general framework we describe in this section.
In Chapter 3, we show the specific way the operations are expressed in SQL.
Another frequent operation is to select certain attributes (columns) from a relation.
The result is a new relation having only those selected attributes.
Each tuple in the result is derived from a tuple of the instructor relation but with only selected attributes shown.
The join operation allows the combining of two relations by merging pairs of tuples, one from each relation, into a single tuple.
There are a number of different ways to join relations (as we shall see in Chapter 3)
Figure 2.12 shows an example of joining the tuples from the instructor and department tables with the new tuples showing the information about each instructor and the department in which she is working.
This result was formed by combining each tuple in the instructor relation with the tuple in the department relation for the instructor’s department.
In the form of join shown in Figure 2.12, which is called a natural join, a tuple from the instructor relation matches a tuple in the department relation if the values.
Figure 2.11 Result of query selecting attributes ID and salary from the instructor relation.
Figure 2.12 Result of natural join of the instructor and department relations.
All such matching pairs of tuples are present in the join result.
In general, the natural join operation on two relations matches tupleswhose values are the same on all attribute names that are common to both relations.
TheCartesian product operation combines tuples from two relations, but unlike the join operation, its result contains all pairs of tuples from the two relations, regardless of whether their attribute values match.
Because relations are sets, we can perform normal set operations on relations.
The union operation performs a set union of two “similarly structured” tables (say a table of all graduate students and a table of all undergraduate students)
For example, one can obtain the set of all students in a department.
Other set operations, such as intersection and set difference can be performed as well.
As we noted earlier, we can perform operations on the results of queries.
For example, if wewant to find the ID and salary for those instructors who have salary greater than $85,000, we would perform the first two operations in our example above.
Just as algebraic operations on numbers take one or more numbers as input and return a number as output, the relational algebra operations typically take one or two relations as input and return a relation as output.
Relational algebra is covered in detail in Chapter 6, but we outline a few of the operations below.
In this example, we could have performed the operations in either order, but that is not the case for all situations, as we shall see.
Certain relational languages adhere strictly to themathematical definition of a set and remove duplicates.
Others, in consideration of the relatively large amount of processing required to remove duplicates from large result relations, retain duplicates.
In these latter cases, the relations are not truly relations in the pure mathematical sense of the term.
Of course, data in a database must be changed over time.
A relation can be updated by inserting new tuples, deleting existing tuples, or modifying tuples by.
The relational data model is based on a collection of tables.
The user of the database system may query these tables, insert new tuples, delete tuples, and update (modify) tuples.
The schema of a relation refers to its logical design, while an instance of the relation refers to its contents at a point in time.
The schema of a database and an instance of a database are similarly defined.
The schema of a relation includes its attributes, and optionally the types of the attributes and constraints on the relation such as primary and foreign key constraints.
A superkey of a relation is a set of one or more attributes whose values are guaranteed to identify tuples in the relation uniquely.
A candidate key is a minimal superkey, that is, a set of attributes that forms a superkey, but none of whose subsets is a superkey.
One of the candidate keys of a relation is chosen as its primary key.
A foreign key is a set of attributes in a referencing relation, such that for each tuple in the referencing relation, the values of the foreign key attributes are guaranteed to occur as the primary key value of a tuple in the referenced relation.
A schema diagram is a pictorial depiction of the schema of a database that shows the relations in the database, their attributes, and primary keys and foreign keys.
The relational query languages define a set of operations that operate on tables, and output tables as their results.
These operations can be combined to get expressions that express desired queries.
The relational algebra provides a set of operations that take one or more relations as input and return a relation as anoutput.
Practical query languages such as SQL are based on the relational algebra, but add a number of useful syntactic features.
Give examples of inserts and deletes to these relations, which can cause a violation of the foreign key constraint.
Given that a particular time slot can meet more than once in a week, explain why day and start time are part of the primary key of this relation, while end time is not.
From this, can we conclude that name can be used as a superkey (or primary key) of instructor?
For each expression, explain in words what the expression does.
Give an expression in the relational algebra to express each of the following queries:
Find the names of all employees who live in city “Miami”
Find the names of all employeeswhose salary is greater than $100,000
Find the names of all employees who live in “Miami” and whose salary is greater than $100,000
Give an expression in the relational algebra for each of the following queries.
Find the names of all borrowers who have a loan in branch “Downtown”
Given your choice of primary keys, identify appropriate foreign keys.
Then, would s id still be a primary key of the advisor relation? If not, what should the primary key of advisor be?
Give an expression in the relational algebra to express each of the following queries:
Find the names of all employees who work for “First Bank Corporation”
Find the names and cities of residence of all employees who work for “First Bank Corporation”
Find the names, street address, and cities of residence of all employees who work for “First Bank Corporation” and earn more than $10,000
Give an expression in the relational algebra for each of the following queries:
Find all loan numbers with a loan value greater than $10,000
Find the names of all depositors who have an account with a value greater than $6,000
Find the names of all depositors who have an account with a value greater than $6,000 at the “Uptown” branch.
Microsoft Access is a single-user database product that is part of the Microsoft Office suite.
There are a number of database query languages in use, either commercially or experimentally.
Although we refer to the SQL language as a “query language,” it can do much more than just query a database.
It can define the structure of the data, modify data in the database, and specify security constraints.
It is not our intention to provide a complete users’ guide for SQL.
Individual implementations of SQL may differ in details, or may support only a subset of the full language.
The Sequel language has evolved since then, and its name has changed to SQL (Structured Query Language)
The SQL DDL provides commands for defining relation schemas, deleting relations, and modifying relation schemas.
The SQL DML provides the ability to query information from the database and to insert tuples into, delete tuples from, and modify tuples in the database.
The SQL DDL includes commands for specifying integrity constraints that the data stored in the database must satisfy.
Embedded and dynamic SQL define how SQL statements can be embedded within general-purpose programming languages, such as C, C++, and Java.
The SQL DDL includes commands for specifying access rights to relations and views.
In this chapter, we present a survey of basic DML and the DDL features of SQL.
Features described here have been part of the SQL standard since SQL-92
In Chapter 4, we provide amore detailed coverage of the SQL query language, including (a) various join expressions; (b) views; (c) transactions; (d) integrity constraints; (e) type system; and (f) authorization.
Although most SQL implementations support the standard features we describe here, you should be aware that there are differences between implementations.
Most implementations support some nonstandard features, while omitting support for some of the more advanced features.
In case you find that some language features described here do not work on the database system that you use, consult the user manuals for your database system to find exactly what features it supports.
The set of relations in a database must be specified to the system by means of a data-definition language (DDL)
The SQL DDL allows specification of not only a set of relations, but also information about each relation, including:
The set of indices to be maintained for each relation.
The SQL standard supports a variety of built-in types, including:
The number consists of p digits (plus a sign), and d of the p digits are to the right of the decimal point.
Each type may include a special value called the null value.
In certain cases, we may wish to prohibit null values from being entered, as we shall see shortly.
In contrast, if attribute B were of type varchar(10), and we store “Avi” in attribute B, no spaces would be added.
When comparing two values of type char, if they are of different lengths extra spaces are automatically added to the shorter one to make them the same size, before comparison.
When comparing a char typewith a varchar type, onemay expect extra spaces to be added to the varchar type to make the lengths equal, before comparison; however, this may or may not be done, depending on the database system.
As a result, even if the same value “Avi” is stored in the attributes A and B above, a comparison A=B may return false.
We recommend you always use the varchar type instead of the char type to avoid these problems.
However, many databases allow Unicode (in the UTF-8 representation) to be stored even in varchar types.
We define an SQL relation by using the create table command.
The following command creates a relation department in the database.
The create table command also specifies that the dept name attribute is the primary key of the department relation.
The semicolon shown at the end of the create table statements, as well as at the end of other SQL statements later in this chapter, is optional in many SQL implementations.
In this section, we discuss only a few of them:
The primarykey attributes are required to be nonnull and unique; that is, no tuple can have a null value for a primary-key attribute, and no two tuples in the relation can be equal on all the primary-key attributes.
The definition of the course table has a declaration “foreign key (dept name) references department”
This foreign-key declaration specifies that for each course tuple, the department name specified in the tuple must exist in the primary key attribute (dept name) of the department relation.
Without this constraint, it is possible for a course to specify a nonexistent department name.
Figure 3.1 also shows foreignkey constraints on tables section, instructor and teaches.
For example, in Figure 3.1, the not null constraint on the name attribute of the instructor relation ensures that the name of an instructor cannot be null.
More details on the foreign-key constraint, aswell as on other integrity constraints that the create table command may include, are provided later, in Section 4.4
For example, if a newly inserted ormodified tuple in a relation has null values for any primary-key attribute, or if the tuple has the same value on the primary-key attributes as does another tuple in the relation, SQL flags an error and prevents the update.
Similarly, an insertion of a course tuple with a dept name value that does not appear in the department relation would violate the foreign-key constraint on course, and SQL prevents such an insertion from taking place.
We can use the insert command to load data into the relation.
The values are specified in the order in which the corresponding attributes are listed in the relation schema.
The insert command has a number of useful features, and is covered in more detail later, in Section 3.9.2
Figure 3.1 SQL data definition for part of the university database.
Other forms of the delete command allow specific tuples to be deleted; the delete command is covered in more detail later, in Section 3.9.1
To remove a relation from an SQL database, we use the drop table command.
The drop table command deletes all information about the dropped relation from the database.
The latter retains relation r, but deletes all tuples in r.
The former deletes not only all tuples of r, but also the schema for r.
After r is dropped, no tuples can be inserted into r unless it is re-created with the create table command.
We use the alter table command to add attributes to an existing relation.
All tuples in the relation are assigned null as the value for the new attribute.
We can drop attributes from a relation by the command.
Many database systems do not support dropping of attributes, although they will allow an entire table to be dropped.
The basic structure of an SQL query consists of three clauses: select, from, and where.
The query takes as its input the relations listed in the from clause, operates on them as specified in thewhere and select clauses, and then produces a relation as the result.
We introduce the SQL syntax through examples, and describe the general structure of SQL queries later.
Let us consider a simple query using our university example, “Find the names of all instructors.” Instructor names are found in the instructor relation, so we.
The instructor’s name appears in the name attribute, so we put that in the select clause.
The result is a relation consisting of a single attribute with the heading name.
Now consider another query, “Find the department names of all instructors,” which can be written as:
Since more than one instructor can belong to a department, a department name could appear more than once in the instructor relation.
The result of the above query is a relation containing the department names, shown in Figure 3.3
In the formal, mathematical definition of the relational model, a relation is a set.
Therefore, SQL allows duplicates in relations as well as in the results of SQL expressions.
Thus, the preceding SQL query lists each department name once for every tuple in which it appears in the instructor relation.
In those cases where we want to force the elimination of duplicates, we insert the keyword distinct after select.
The result of the above querywould contain each department name at most once.
Since duplicate retention is the default, we shall not use all in our examples.
To ensure the elimination of duplicates in the results of our example queries, we shall use distinctwhenever it is necessary.
This shows what would result if we gave a 10% raise to each instructor; note, however, that it does not result in any change to the instructor relation.
The where clause allows us to select only those rows in the result relation of the from clause that satisfy a specified predicate.
Consider the query “Find the names of all instructors in the Computer Science department who have salary greater than $70,000.” This query can be written in SQL as:
We shall explore other features ofwhere clause predicates later in this chapter.
So far our example queries were on a single relation.
An an example, suppose we want to answer the query “Retrieve the names of all instructors, along with their department names and department building name.”
Looking at the schema of the relation instructor, we realize that we can get the department name from the attribute dept name, but the department building name is present in the attribute building of the relation department.
To answer the query, each tuple in the instructor relation must be matched with the tuple in the department relation whose dept name value matches the dept name value of the instructor tuple.
In SQL, to answer the above query,we list the relations that need to be accessed in the from clause, and specify the matching condition in the where clause.
Figure 3.5 The result of “Retrieve the names of all instructors, along with their department names and department building name.”
In contrast, the attributes name and building appear in only one of the relations, and therefore do not need to be prefixed by the relation name.
This naming convention requires that the relations that are present in the from clause have distinct names.
This requirement causes problems in some cases, such as when information from two different tuples in the same relation needs to be combined.
In Section 3.4.1, we see how to avoid these problems by using the rename operation.
We now consider the general case of SQL queries involvingmultiple relations.
As we have seen earlier, an SQL query can contain three types of clauses, the select clause, the from clause, and the where clause.
The select clause is used to list the attributes desired in the result of a query.
The from clause is a list of the relations to be accessed in the evaluation of.
The where clause is a predicate involving attributes of the relation in the from clause.
Each Ai represents an attribute, and each ri a relation.P is a predicate.
If thewhere clause is omitted, the predicate P is true.
Although the clauses must be written in the order select, from, where, the easiest way to understand the operations specified by the query is to consider the clauses in operational order: first from, then where, and then select.1
The from clause by itself defines a Cartesian product of the relations listed in the clause.
It is defined formally in terms of set theory, but is perhaps best understood as an iterative process that generates tuples for the result relation of the from clause.
The result relation has all attributes from all the relations in the from clause.
Since the same attribute name may appear in both ri and r j , as we saw earlier, we prefix the the name of the relation from which the attribute originally came, before the attribute name.
For example, the relation schema for the Cartesian product of relations instructor and teaches is:
For those attributes that appear in only one of the two schemas, we shall usually drop the relation-name prefix.
The Cartesian product by itself combines tuples from instructor and teaches that are unrelated to each other.
Each tuple in instructor is combined with every tuple in teaches, even those that refer to a different instructor.
The result can be an extremely large relation, and it rarely makes sense to create such a Cartesian product.
In practice, SQL may convert the expression into an equivalent form that can be processed more efficiently.
Note that we renamed instructor.ID as inst.ID to reduce the width of the table in Figure 3.6
Figure 3.6 The Cartesian product of the instructor relation with the teaches relation.
Instead, the predicate in thewhere clause is used to restrict the combinations created by the Cartesian product to those that are meaningful for the desired answer.
We would expect a query involving instructor and teaches to combine a particular tuple t in instructor with only those tuples in teaches that refer to the same instructor towhich t refers.
That is,wewish only tomatch teaches tupleswith instructor tuples that have the same ID value.
The following SQL query ensures this condition, and outputs the instructor name and course identifiers from such matching tuples.
Note that the above query outputs only instructors who have taught some course.
Instructors who have not taught any course are not output; if we wish to output such tuples, we could use an operation called the outer join, which is described in Section 4.1.2
Observe that instructors Gold, Califieri, and Singh, who have not taught any course, do not appear in the above result.
If we only wished to find instructor names and course identifiers for instructors in the Computer Science department, we could add an extra predicate to the where clause, as shown below.
Note that since the dept name attribute occurs only in the instructor relation, we could have used just dept name, instead of instructor.dept name in the above query.
In general, the meaning of an SQL query can be understood as follows:
Figure 3.7 Result of “For all instructors in the university who have taught some course, find their names and the course ID of all courses they taught.”
For each tuple in the result of Step 2, output the attributes (or results of expressions) specified in the select clause.
The above sequence of steps helps make clear what the result of an SQL query should be, not how it should be executed.
A real implementation of SQL would not execute the query in this fashion; it would instead optimize evaluation by generating (as far as possible) only elements of the Cartesian product that satisfy the where clause predicates.
In our example query that combined information from the instructor and teaches table, thematching condition required instructor.ID to be equal to teaches.ID.
These are the only attributes in the two relations that have the same name.
In fact this is a common case; that is, the matching condition in the from clause most often requires all attributes with matching names to be equated.
To make the life of an SQL programmer easier for this common case, SQL supports an operation called the natural join, which we describe below.
In fact SQL supports several other ways in which information from two or more relations can be joined together.
We have already seen how a Cartesian product along with a where clause predicate can be used to join information from multiple relations.
Otherways of joining information frommultiple relations are discussed in Section 4.1
The natural join operation operates on two relations and produces a relation as the result.
Unlike the Cartesian product of two relations, which concatenates each tuple of the first relationwith every tuple of the second, natural join considers only those pairs of tuples with the same value on those attributes that appear in the schemas of both relations.
So, going back to the example of the relations instructor and teaches, computing instructor natural join teaches considers only those pairs of tuples where both the tuple from instructor and the tuple from teaches have the same value on the common attribute, ID.
Figure 3.8 The natural join of the instructor relation with the teaches relation.
Notice that we do not repeat those attributes that appear in the schemas of both relations; rather they appear only once.
Notice also the order in which the attributes are listed: first the attributes common to the schemas of both relations, second those attributes unique to the schema of the first relation, and finally, those attributes unique to the schema of the second relation.
Consider the query “For all instructors in the university who have taught some course, find their names and the course ID of all courses they taught”, which we wrote earlier as:
This query can be written more concisely using the natural-join operation in SQL as:
Aswe saw earlier, the result of the natural join operation is a relation.
Conceptually, expression “instructor natural join teaches” in the from clause is replaced.
A from clause in an SQL query can have multiple relations combined using natural join, as shown here:
More generally, a from clause can be of the form.
For example, suppose we wish to answer the query “List the names of instructors along with the the titles of courses that they teach.” The query can be written in SQL as follows:
The natural join of instructor and teaches is first computed, as we saw earlier, and a Cartesian product of this result with course is computed, from which thewhere clause extracts only those tuples where the course identifier from the join result matches the course identifier from the course relation.
Note that teaches.course id in thewhere clause refers to the course id field of the natural join result, since this field in turn came from the teaches relation.
In contrast the following SQL query does not compute the same result:
To seewhy, note that thenatural joinof instructor and teaches contains the attributes (ID, name, dept name, salary, course id, sec id), while the course relation contains the attributes (course id, title, dept name, credits)
As a result, the natural join of these two would require that the dept name attribute values from the two inputs be the same, in addition to requiring that the course id values be the same.
This query would then omit all (instructor name, course title) pairs where the instructor teaches a course in a department other than the instructor’s own department.
The previous query, on the other hand, correctly outputs such pairs.
To provide the benefit of natural join while avoiding the danger of equating attributes erroneously, SQL provides a form of the natural join construct that allows you to specify exactly which columns should be equated.
Thus, in the preceding SQL query, the join construct permits teaches.dept name and course.dept name to differ, and the SQL query gives the correct answer.
There are number of additional basic operations that are supported in SQL.
The result of this query is a relation with the following attributes:
The names of the attributes in the result are derived from the names of the attributes in the relations in the from clause.
We cannot, however, always derive names in this way, for several reasons: First, two relations in the from clause may have attributes with the same name, in which case an attribute name is duplicated in the result.
Second, if we used an arithmetic expression in the select clause, the resultant attribute does not have a name.
Third, even if an attribute name can be derived from the base relations as in the preceding example, we may want to change the attribute name in the result.
Hence, SQL provides a way of renaming the attributes of a result relation.
The as clause can appear in both the select and from clauses.4 For example, if wewant the attribute name name to be replacedwith the name.
One reason to rename a relation is to replace a long relation name with a shortened version that is more convenient to use elsewhere in the query.
To illustrate, we rewrite the query “For all instructors in the university who have taught some course, find their names and the course ID of all courses they taught.”
Another reason to rename a relation is a case where we wish to compare tuples in the same relation.
We then need to take the Cartesian product of a relation with itself and, without renaming, it becomes impossible to distinguish one tuple from the other.
Suppose that we want to write the query “Find the names of all instructors whose salary is greater than at least one instructor in the Biology department.” We can write the SQL expression:
In the abovequery,T andS canbe thought of as copies of the relation instructor, butmore precisely, they are declared as aliases, that is as alternative names, for the relation instructor.
An identifier, such as T and S, that is used to rename a relation is referred to as a correlation name in the SQL standard, but is also commonly referred to as a table alias, or a correlation variable, or a tuple variable.
Note that a betterway to phrase the previous query in Englishwould be “Find the names of all instructors who earn more than the lowest paid instructor in the Biology department.” Our original wording fits more closely with the SQL that we wrote, but the latter wording is more intuitive, and can in fact be expressed directly in SQL as we shall see in Section 3.8.2
Early versions of SQL did not include the keyword as.
As a result, some implementations of SQL, notably Oracle, do not permit the keyword as in the from clause.
In Oracle, “old-name as new-name” is written instead as “old-name new-name” in the from clause.
The keyword as is permitted for renaming attributes in the select clause, but it is optional and may be omitted in Oracle.
This default behavior can, however, be changed, either at the database level or at the level of specific attributes.
There are variations on the exact set of string functions supported by different database systems.
See your database system’s manual for more details on exactly what string functions it supports.
Pattern matching can be performed on strings, using the operator like.
Patterns are case sensitive; that is, uppercase characters do not match lowercase characters, or vice versa.
Comp%’ matches any string containing “Comp” as a substring, for example, ’Intro.
Consider the query “Find the names of all departments whose building name includes the substring ‘Watson’.” This query can be written as:
For patterns to include the special pattern characters (that is, % and ), SQL allows the specification of an escape character.
The escape character is used immediately before a special pattern character to indicate that the special pattern character is to be treated like a normal character.
We define the escape character for a like comparison using the escape keyword.
To illustrate, consider the following patterns, which use a backslash (\) as the escape character:
Some databases provide variants of the like operation which do not distinguish lower and upper case.
SQL:1999 also offers a similar to operation, which provides more powerful pattern matching than the like operation; the syntax for specifying patterns is similar to that used in Unix regular expressions.
A select clause of the form select * indicates that all attributes of the result relation of the from clause are selected.
The order by clause causes the tuples in the result of a query to appear in sorted order.
To list in alphabetic order all instructors in the Physics department, we write:
By default, the order by clause lists items in ascending order.
To specify the sort order, we may specify desc for descending order or asc for ascending order.
Suppose that we wish to list the entire instructor relation in descending order of salary.
We can extend the preceding query that finds instructor names along with.
We show below the modified form of the SQL query that does not use natural join.
The comparison operators can be used on tuples, and the ordering is defined lexicographically.
Thus, the preceding SQL query can be rewritten as follows:5
The set of all courses taught in the Fall 2009 semester:
The set of all courses taught in the Spring 2010 semester:
Although it is part of the SQL-92 standard, some SQL implementations may not support this syntax.
If we want to retain all duplicates, we must write union all in place of union:
The parentheses we include around each select-from-where statement are optional, but useful for ease of reading.
If we want to retain all duplicates, we must write intersect all in place of intersect:
The result of this query is shown in Figure 3.13
The except operation7 outputs all tuples from its first input that do not occur in the second input; that is, it performs set difference.
The operation automatically eliminates duplicates in the inputs before performing set difference.
If we want to retain duplicates, we must write except all in place of except:
Some SQL implementations, notably Oracle, use the keywordminus in place of except.
Null values present special problems in relational operations, including arithmetic operations, comparison operations, and set operations.
It would be wrong to say this is true since we do not knowwhat the null value represents.
This creates a third logical value in addition to true and false.
Since the predicate in a where clause can involve Boolean operations such as and, or, and not on the results of comparisons, the definitions of the Boolean operations are extended to deal with the value unknown.
If thewhere clause predicate evaluates to either false or unknown for a tuple, that tuple is not added to the result.
Thus, to find all instructors who appear in the instructor relation with null values for salary, we write:
The predicate is not null succeeds if the value on which it is applied is not null.
Some implementations of SQL also allowus to testwhether the result of a comparison is unknown, rather than true or false, by using the clauses is unknown and is not unknown.
The above approach of treating tuples as identical if they have the same values for all attributes, even if some of the values are null, is also used for the set operations union, intersection and except.
Aggregate functions are functions that take a collection (a set or multiset) of values as input and return a single value.
The input to sum and avgmust be a collection of numbers, but the other operators can operate on collections of nonnumeric data types, such as strings, as well.
Consider the query “Find the average salary of instructors in the Computer Science department.” We write this query as follows:
The result of this query is a relation with a single attribute, containing a single tuple with a numerical value corresponding to the average salary of instructors in the Computer Science department.
The database systemmay give an arbitrary name to the result relation attribute that is generated by aggregation; however, we can give a meaningful name to the attribute by using the as clause as follows:
Suppose the Computer Science department adds a fourth instructor whose salary happens to be $75,000
There are cases where we must eliminate duplicates before computing an aggregate function.
If we do want to eliminate duplicates, we use the keyword distinct in the aggregate expression.
An example arises in the query “Find the total number of instructors who teach a course in the Spring 2010 semester.” In this case, an instructor counts only once, regardless of the number of course sections that the instructor teaches.
The required information is contained in the relation teaches, and we write this query as follows:
Because of the keyword distinct preceding ID, even if an instructor teaches more than one course, she is counted only once in the result.
We use the aggregate function count frequently to count the number of tuples in a relation.
The notation for this function in SQL is count (*)
Thus, to find the number of tuples in the course relation, we write.
Figure 3.14 Tuples of the instructor relation, grouped by the dept name attribute.
It is legal to use distinct with max and min, even though the result does not change.
We can use the keyword all in place of distinct to specify duplicate retention, but, since all is the default, there is no need to do so.
There are circumstances where we would like to apply the aggregate function not only to a single set of tuples, but also to a group of sets of tuples; we specify this wish in SQL using the group by clause.
The attribute or attributes given in the group by clause are used to form groups.
Tuples with the same value on all attributes in the group by clause are placed in one group.
As an illustration, consider the query “Find the average salary in each department.” We write this query as follows:
Figure 3.14 shows the tuples in the instructor relation grouped by the dept name attribute, which is the first step in computing the query result.
The specified aggregate is computed for each group, and the result of the query is shown in Figure 3.15
In contrast, consider the query “Find the average salary of all instructors.”We write this query as follows:
Figure 3.15 The result relation for the query “Find the average salary in each department”
In this case the group by clause has been omitted, so the entire relation is treated as a single group.
As another example of aggregation on groups of tuples, consider the query “Find the number of instructors in each department who teach a course in the Spring 2010 semester.” Information about which instructors teach which course sections in which semester is available in the teaches relation.
However, this information has to be joined with information from the instructor relation to get the department name of each instructor.
When an SQL query uses grouping, it is important to ensure that the only.
In other words, any attribute that is not present in the group by clause must appear only inside an aggregate function if it appears in the select clause, otherwise the query is treated as erroneous.
For example, the following query is erroneous since ID does not appear in the group by clause, and yet it appears in the select clause without being aggregated:
Each instructor in a particular group (defined by dept name) can have a different ID, and since only one tuple is output for each group, there is no unique way of choosing which ID value to output.
At times, it is useful to state a condition that applies to groups rather than to tuples.
For example, we might be interested in only those departments where the average salary of the instructors is more than $42,000
This condition does not apply to a single tuple; rather, it applies to each group constructed by the group by clause.
To express such a query, we use the having clause of SQL.
Aswas the case for the select clause, any attribute that is present in the having.
The meaning of a query containing aggregation, group by, or having clauses is defined by the following sequence of operations:
As was the case for queries without aggregation, the from clause is first evaluated to get a relation.
If a where clause is present, the predicate in thewhere clause is applied on the result relation of the from clause.
Tuples satisfying the where predicate are then placed into groups by the group by clause if it is present.
If the group by clause is absent, the entire set of tuples satisfying thewhere predicate is treated as being in one group.
The having clause, if it is present, is applied to each group; the groups that do not satisfy the having clause predicate are removed.
The select clause uses the remaining groups to generate tuples of the result of the query, applying the aggregate functions to get a single result tuple for each group.
Note that all the required information for the preceding query is available from the relations takes and student, and that although the query pertains to sections, a join with section is not needed.
Null values, when they exist, complicate the processing of aggregate operators.
For example, assume that some tuples in the instructor relation have a null value for salary.
The values to be summed in the preceding query include null values, since some tuples have a null value for salary.
Rather than say that the overall sum is itself null, the SQL standard says that the sum operator should ignore null values in its input.
In general, aggregate functions treat nulls according to the following rule: All aggregate functions except count (*) ignore null values in their input collection.
As a result of null values being ignored, the collection of valuesmay be empty.
The count of an empty collection is defined to be 0, and all other aggregate operations.
The effect of null values on some of the more complicated SQL constructs can be subtle.
A Boolean data type that can take values true, false, and unknown, was introduced in SQL:1999
The aggregate functions some and every, which mean exactly what you would intuitively expect, can be applied on a collection of Boolean values.
A subquery is a select-fromwhere expression that is nested within another query.
A common use of subqueries is to perform tests for set membership, make set comparisons, and determine set cardinality, by nesting subqueries in the where clause.
In Section 3.8.5, we study nesting of subqueries in the from clause.
In Section 3.8.7, we see how a class of subqueries called scalar subqueries can appear wherever an expression returning a value can occur.
The in connective tests for set membership, where the set is a collection of values produced by a select clause.
The not in connective tests for the absence of set membership.
Clearly, this formulation generates the same results as the previous one did, but it leads us towrite our query using the in connective of SQL.
We begin by finding all courses taught in Spring 2010, and we write the subquery.
We then need to find those courses that were taught in the Fall 2009 and that appear in the set of courses obtained in the subquery.
We do so by nesting the subquery in thewhere clause of an outer query.
This example shows that it is possible to write the same query several ways in SQL.
This flexibility is beneficial, since it allows a user to think about the query in the way that seems most natural.
We shall see that there is a substantial amount of redundancy in SQL.
We use the not in construct in a way similar to the in construct.
The in and not in operators can also be used on enumerated sets.
The following query selects the names of instructors whose names are neither “Mozart” nor “Einstein”
In the preceding examples, we tested membership in a one-attribute relation.
It is also possible to test for membership in an arbitrary relation in SQL.
For example, we canwrite the query “find the total number of (distinct) students who have taken course sections taught by the instructor with ID 110011” as follows:
As an example of the ability of a nested subquery to compare sets, consider the query “Find the names of all instructors whose salary is greater than at least one instructor in the Biology department.” In Section 3.4.1, we wrote this query as follows:
The phrase “greater than at least one” is represented in SQL by> some.
This construct allows us to rewrite the query in a form that resembles closely our formulation of the query in English.
The> some comparison in thewhere clause of the outer select is true if the salary value of the tuple is greater than at least one member of the set of all salary values for instructors in Biology.
Let us find the names of all instructors that have a salary value greater than that of each instructor in the Biology department.
The construct> all corresponds to the phrase “greater than all.”Using this construct, we write the query as follows:
As an exercise, verify that <> all is identical to not in, whereas = all is not the same as in.
As another example of set comparisons, consider the query “Find the departments that have the highest average salary.” We begin by writing a query to find all average salaries, and then nest it as a subquery of a larger query that finds.
Later versions added the alternative some to avoid the linguistic ambiguity of the word any in English.
The exists construct returns the value true if the argument subquery is nonempty.
The above query also illustrates a feature of SQL where a correlation name from an outer query (S in the above query), can be used in a subquery in the where clause.
A subquery that uses a correlation name from an outer query is called a correlated subquery.
In queries that contain subqueries, a scoping rule applies for correlation names.
In a subquery, according to the rule, it is legal to use only correlation names defined in the subquery itself or in any query that contains the subquery.
If a correlation name is defined both locally in a subquery and globally in a containing query, the local definition applies.
This rule is analogous to the usual scoping rules used for variables in programming languages.
To illustrate the not exists operator, consider the query “Find all studentswho have taken all courses offered in the Biology department.” Using the except construct, we can write the query as follows:
Thus, the outer select takes each student and tests whether the set of all courses that the student has taken contains the set of all courses offered in the Biology department.
The unique construct9 returns the value true if the argument subquery contains no duplicate tuples.
Using the unique construct, we can write the query “Find all courses that were offered at most once in 2009” as follows:
Note that if a course is not offered in 2009, the subquery would return an empty result, and the unique predicate would evaluate to true on the empty set.
An equivalent version of the above query not using the unique construct is:
We can test for the existence of duplicate tuples in a subquery by using the not unique construct.
To illustrate this construct, consider the query “Find all courses that were offered at least twice in 2009” as follows:
The key concept appliedhere is that any select-from-where expression returns a relation as a result and, therefore, can be inserted into another select-from-where anywhere that a relation can appear.
We can now rewrite this query, without using the having clause, by using a subquery in the from clause, as follows:
The subquery generates a relation consisting of the names of all departments and their corresponding average instructors’ salaries.
The attributes of the subquery result can be used in the outer query, as can be seen in the above example.
Note that we do not need to use the having clause, since the subquery in the from clause computes the average salary, and the predicate that was in the having clause earlier is now in the where clause of the outer query.
We can give the subquery result relation a name, and rename the attributes, using the as clause, as illustrated below.
The subquery result relation is named dept avg, with the attributes dept name and avg salary.
Nested subqueries in the from clause are supported by most but not all SQL implementations.
However, some SQL implementations, notably Oracle, do not support renaming of the result relation in the from clause.
As another example, suppose we wish to find the maximum across all departments of the total salary at each department.
The having clause does not help us in this task, but we can write this query easily by using a subquery in the from clause, as follows:
We note that nested subqueries in the from clause cannot use correlation variables from other relations in the from clause.
However, SQL:2003 allows a subquery in the from clause that is prefixed by the lateral keyword to access attributes of preceding tables or subqueries in the from clause.
For example, if we wish to print the names of each instructor, along with their salary and the average salary in their department, we could write the query as follows:
Without the lateral clause, the subquery cannot access the correlation variable I1 from the outer query.
Currently, only a few SQL implementations, such as IBM DB2, support the lateral clause.
Thewith clause provides away of defining a temporary relationwhose definition is available only to the query in which the with clause occurs.
Consider the following query, which finds those departments with the maximum budget.
The with clause defines the temporary relation max budget, which is used in the immediately following query.
The with clause, introduced in SQL:1999, is supported by many, but not all, database systems.
We could have written the above query by using a nested subquery in either the from clause or the where clause.
However, using nested subqueries would have made the query harder to read and understand.
The with clause makes the query logic clearer; it also permits a view definition to be used in multiple places within a query.
For example, suppose we want to find all departments where the total salary is greater than the average of the total salary at all departments.
We can write the query using the with clause as follows.
We can, of course, create an equivalent querywithout thewith clause, but itwould be more complicated and harder to understand.
The subquery in the above example is guaranteed to return only a single value since it has a count(*) aggregate without a group by.
The example also illustrates the usage of correlation variables, that is, attributes of relations in the from clause of the outer query, such as department.dept name in the above example.
Scalar subqueries can occur in select, where, and having clauses.
It is not always possible to figure out at compile time if a subquery can return more than one tuple in its result; if the result has more than one tuple when the subquery is executed, a run-time error occurs.
Note that technically the type of a scalar subquery result is still a relation, even if it contains a single tuple.
However, when a scalar subquery is used in an expression where a value is expected, SQL implicitly extracts the value from the single attribute of the single tuple in the relation, and returns that value.
We have restricted our attention until now to the extraction of information from the database.Now,we showhow to add, remove, or change informationwith SQL.
Adelete request is expressed inmuch the sameway as a query.We can delete only whole tuples; we cannot delete values on only particular attributes.
The delete statement first finds all tuples t in r for which P(t) is true, and then deletes them from r.
The where clause can be omitted, in which case all tuples in r are deleted.
Ifwewant todelete tuples from several relations, we must use one delete command for each relation.
The predicate in the where clause may be as complex as a select command’s where clause.
At the other extreme, the where clause may be empty.
The instructor relation itself still exists, but it is empty.
Delete all tuples in the instructor relation pertaining to instructors in the Finance department.
Delete all tuples in the instructor relation for those instructors associatedwith a department located in the Watson building.
This delete request first finds all departments located in Watson, and then deletes all instructor tuples pertaining to those departments.
Note that, although we may delete tuples from only one relation at a time, we may reference any number of relations in a select-from-where nested in the where clause of a delete.
The delete request can contain a nested select that references the relation fromwhich tuples are to be deleted.
For example, suppose that we want to delete the records of all instructors with salary below the average at the university.
The delete statement first tests each tuple in the relation instructor to check whether the salary is less than the average salary of instructors in the university.
Then, all tuples that fail the test—that is, represent an instructor with a lower-than-average salary—are deleted.
Performing all the tests before performing any deletion is important—if some tuples are deleted before other tuples.
To insert data into a relation, we either specify a tuple to be inserted or write a query whose result is a set of tuples to be inserted.
Obviously, the attribute values for inserted tuples must be members of the corresponding attribute’s domain.
Similarly, tuples inserted must have the correct number of attributes.
The simplest insert statement is a request to insert one tuple.
In this example, the values are specified in the order in which the corresponding attributes are listed in the relation schema.
For the benefit of users who may not remember the order of the attributes, SQL allows the attributes to be specified as part of the insert statement.
For example, the following SQL insert statements are identical in function to the preceding one:
More generally, we might want to insert tuples on the basis of the result of a query.
Instead of specifying a tuple as we did earlier in this section, we use a select to specify a set of tuples.
Each tuple has an ID, a name, a dept name (Music), and an salary of $18,000
It is important that we evaluate the select statement fully before we carry out any insertions.
If we carry out some insertions even as the select statement is being evaluated, a request such as:
Without the primary key constraint, the request would insert the first tuple in student again, creating a second copy of the tuple.
Since this second copy is part of student now, the select statement may find it, and a third copy would be inserted into student.
The select statement may then find this third copy and insert a fourth copy, and so on, forever.
Evaluating the select statement completely before performing insertions avoids such problems.
Thus, the above insert statement would simply duplicate every tuple in the student relation, if the relation did not have a primary key constraint.
Our discussion of the insert statement considered only examples in which a value is given for every attribute in inserted tuples.
It is possible for inserted tuples to be given values on only some attributes of the schema.
The remaining attributes are assigned a null value denoted by null.
The tuple inserted by this request specified that a student with ID “3003” is in the Finance department, but the tot cred value for this student is not known.
Most relational database products have special “bulk loader” utilities to insert a large set of tuples into a relation.
These utilities allow data to be read from formatted text files, and can execute much faster than an equivalent sequence of insert statements.
In certain situations, we may wish to change a value in a tuple without changing all values in the tuple.
As we could for insert and delete, we can choose the tuples to be updated by using a query.
Suppose that annual salary increases are being made, and salaries of all instructors are to be increased by 5 percent.
The preceding update statement is applied once to each of the tuples in instructor relation.
If a salary increase is to be paid only to instructors with salary of less than $70,000, we can write:
In general, the where clause of the update statement may contain any construct legal in the where clause of the select statement (including nested selects)
As with insert and delete, a nested selectwithin an update statement may reference the relation that is being updated.As before, SQL first tests all tuples in the relation to see whether they should be updated, and carries out the updates afterward.
For example, we can write the request “Give a 5 percent salary raise to instructors whose salary is less than average” as follows:
Note that the order of the two update statements is important.
The general form of the case statement is as follows.
Case statements can be used in any place where a value is expected.
Scalar subqueries are also useful in SQL update statements, where they can be used in the set clause.
Consider an update where we set the tot cred attribute of each student tuple to the sum of the credits of courses successfully completed by the student.
We assume that a course is successfully completed if the student has a grade that is not ’F’ or null.
To specify this update, we need to use a subquery in the set clause, as shown below:
In case a student has not successfully completed any course, the above update statement would set the tot cred attribute value to null.
Data-definition language (DDL), which provides commands for defining relation schemas, deleting relations, and modifying relation schemas.
Data-manipulation language (DML), which includes a query language and commands to insert tuples into, delete tuples from, andmodify tuples in the database.
The SQL data-definition language is used to create relations with specified schemas.
In addition to specifying the names and types of relation attributes, SQL also allows the specification of integrity constraints such as primary-key constraints and foreign-key constraints.
These include the select, from, andwhere clauses, and support for the natural join operation.
It also supports scalar subqueries, wherever an expression returning a value is permitted.
We suggest you actually run these queries on a database, using the sample data that we provide on the Web site of the book, db-book.com.
Instructions for setting up a database, and loading sample data, are provided on the above Web site.
Find the IDs of all students who were taught by an instructor named Einstein; make sure there are no duplicates in the result.
Find all instructors earning the highest salary (there may be more than one with the same salary)
Given the above relation, and our university schema, write each of the following queries in SQL.
You can assume for simplicity that no takes tuple has the null value for grade.
Find the grade-point average (GPA) for the above student, that is, the total grade-points divided by the total credits for the associated courses.
Find the ID and the grade-point average of every student.
Delete all courses that have never been offered (that is, do not occur in the section relation)
Find the total number of people who owned cars that were involved.
Add a new accident to the database; assume any values for required attributes.
Display the grade for each student, based on the marks relation.
To show how, write a query that finds departments whose names contain the string “sci” as a substring, regardless of the case.
Find all customers of the bank who have an account but not a loan.
Find the names of all customers who live on the same street and in the same city as “Smith”
Find the names of all branches with customers who have an account in the bank and who live in “Harrison”
Give an expression in SQL for each of the following queries.
Find the names and cities of residence of all employees who work for “First Bank Corporation”
Find the names, street addresses, and cities of residence of all employees who work for “First Bank Corporation” and earn more than $10,000
Find all employees in the database who do not work for “First Bank Corporation”
Assume that the companies may be located in several cities.
Find all companies located in every city in which “Small Bank Corporation” is located.
Find those companies whose employees earn a higher salary, on average, than the average salary at “First Bank Corporation”
Give an expression in SQL for each of the following queries.
Modify the database so that “Jones” now lives in “Newtown”
Find the names of all students who have taken at least one Comp.
For each department, find the maximum salary of instructors in that department.
You may assume that every department has at least one instructor.
Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.
Delete enrollments in the above section where the student’s name is Chavez.
What will happen if you run this delete statement without first deleting offerings (sections) of this course.
Delete all takes tuples corresponding to any section of any coursewith the word “database” as a part of the title; ignore case when matching the word with the title.
Make any reasonable assumptions about data types, and be sure to declare primary and foreign keys.
Find the number of accidents in which the cars belonging to “John Smith” were involved.
Find all customers who have an account at all the branches located in “Brooklyn”
Find out the total sum of all loan amounts in the bank.
Find the names of all branches that have assets greater than those of at least one branch located in “Brooklyn”
Give an expression in SQL for each of the following queries.
Find the names of all employees who work for “First Bank Corporation”
Find all employees in the database who live in the same cities as the companies for which they work.
Find all employees in the database who live in the same cities and on the same streets as do their managers.
Find all employees who earn more than the average salary of all employees of their company.
Give an expression in SQL for each of the following queries.
Give all employees of “First Bank Corporation” a 10 percent raise.
Give all managers of “First Bank Corporation” a 10 percent raise.
Delete all tuples in the works relation for employees of “Small Bank Corporation”
Choose an appropriate domain for each attribute and an appropriate primary key for each relation schema.
Print the names of members who have borrowed any book published by “McGraw-Hill”
Print the names of members who have borrowed all books published by “McGraw-Hill”
For each publisher, print the names of members who have borrowed more than five books of that publisher.
Take into account that if an member does not borrow any books, then that member does not appear in the borrowed relation at all.
Explain why joining section as well in the from clause would not change the result.
In addition several database systems can be downloaded and used free of charge, including PostgreSQL, MySQL (free except for certain kinds of commercial use), and Oracle Express edition.
Most database systems provide a command line interface for submitting SQL commands.
The NetBeans IDE provides a GUI front end that works with a number of different databases, but with limited functionality, while the Eclipse IDE supports similar functionality through several different plugins such as the Data Tools Platform (DTP) and JBuilder.
The SQL constructs discussed in this chapter are part of the SQL standard, but certain features are not supported by some databases.
The Web site lists these incompatibilities, which you will need to take into account when executing queries on those databases.
The original version of SQL, called Sequel 2, is described by Chamberlin et al.
Donahoo and Speegle [2005] covers SQL from a developers’ perspective.
The standards documents are densely packed with information and hard to read, and of use primarily for database system implementers.
The standards documents are available from the Web site http://webstore.ansi.org, but only for purchase.
Many database products support SQL features beyond those specified in the standard, and may not support some features of the standard.
More information on these featuresmay be found in the SQL usermanuals of the respectiveproducts.
We consider more complex forms of SQL queries, view definition, transactions, integrity constraints, more details regarding SQL data definition, and authorization.
We shall discuss these forms of join in this section.
The null value indicates that the grade has not been awarded yet.
In Section 3.3.3, we saw how to express natural joins, and we saw the join.
The on condition allows a general predicate over the relations being joined.
This predicate is written like a where clause predicate except for the use of the keywordon rather thanwhere.
Like theusing condition, theon condition appears at the end of the join expression.
Consider the following query, which has a join expression containing the on condition.
The on condition above specifies that a tuple from student matches a tuple from takes if their ID values are equal.
The join expression in this case is almost the same as the join expression student natural join takes, since the natural join operation.
The one difference is that the result has the ID attribute listed twice, in the join result, once for student and once for takes, even though their ID values must be the same.
In fact, the above query is equivalent to the following query (in other words, they generate exactly the same results):
Aswehave seenearlier, the relationname isused todisambiguate the attribute name ID, and thus the two occurrences can be referred to as student.ID and takes.ID respectively.
A version of this query that displays the ID value only once is as follows:
The result of the above query is shown in Figure 4.3
The on condition can express any SQL predicate, and thus a join expressions.
However, as illustrated by our preceding example, a query using a join expression with an on condition can be replaced by an equivalent expression without the on condition, with the predicate in the on clause moved to thewhere clause.
Thus, it may appear that the on condition is a redundant feature of SQL.
However, there are two good reasons for introducing the on condition.
First, we shall see shortly that for a kind of join called an outer join, on conditions do behave in a manner different from where conditions.
Second, an SQL query is often more readable by humans if the join condition is specified in the on clause and the rest of the conditions appear in the where clause.
Suppose we wish to display a list of all students, displaying their ID, and name, dept name, and tot cred, alongwith the courses that they have taken.
The following SQL query may appear to retrieve the required information:
Unfortunately, the above query does not work quite as intended.
Suppose that there is some student who takes no courses.
Then the tuple in the student relation for that particular student would not satisfy the condition of a natural join with any tuple in the takes relation, and that student’s data would not appear in the result.Wewould thus not see any information about studentswho have not taken.
Figure 4.3 The result of student join takes on student.ID= takes.ID with second occurrence of ID omitted.
Snow appears in student, but Snow’s ID number does not appear in the ID column of takes.
Thus, Snow does not appear in the result of the natural join.
More generally, some tuples in either or both of the relations being joinedmay be “lost” in this way.
The outer join operation works in a manner similar to the join operations we have already studied, but preserve those tuples that would be lost in a join, by creating tuples in the result containing null values.
For example, to ensure that the student named Snow fromour earlier example appears in the result, a tuple could be added to the join result with all attributes from the student relation set to the corresponding values for the student Snow, and all the remaining attributes which come from the takes relation, namely course id, sec id, semester, and year, set to null.
Thus the tuple for the student Snow is preserved in the result of the outer join.
The left outer join preserves tuples only in the relation named before (to the left of) the left outer join operation.
The right outer join preserves tuples only in the relation named after (to the right of) the right outer join operation.
Wenowexplain exactly how each formof outer join operates.We can compute the left outer-join operation as follows.
First, compute the result of the inner join as before.
Then, for every tuple t in the left-hand-side relation that does not match any tuple in the right-hand-side relation in the inner join, add a tuple r to the result of the join constructed as follows:
The attributes of tuple r that are derived from the left-hand-side relation are filled in with the values from tuple t.
The remaining attributes of r are filled with null values.
That result includes student Snow (ID 70557), unlike the result of an inner join, but the tuple for Snow includes nulls for the attributes that appear only in the schema of the takes relation.
As another example of the use of the outer-join operation, we can write the query “Find all students who have not taken a course” as:
The right outer join is symmetric to the left outer join.
Tuples from the righthand-side relation that do not match any tuple in the left-hand-side relation are padded with nulls and are added to the result of the right outer join.
Thus, if we rewrite our above query using a right outer join and swapping the order in which we list the relations as follows:
The full outer join is a combination of the left and right outer-join types.
After the operation computes the result of the inner join, it extends with nulls those tuples from the left-hand-side relation that did not match with any from the.
Figure 4.4 Result of student natural left outer join takes.
Similarly, it extends with nulls those tuples from the right-hand-side relation that did not match with any tuples from the left-hand-side relation and adds them to the result.
Figure 4.5 The result of takes natural right outer join student.
The following query is identical to the first query we saw using “student natural left outer join takes,” except that the attribute ID appears twice in the result.
The reason for this is that outer join adds null-padded tuples only for those tuples that do not contribute to the result of the corresponding inner join.
The on condition is part of the outer join specification, but awhere clause is not.
In our example, the case of the student tuple for student “Snow” with ID 70557, illustrates this distinction.
Suppose we modify the preceding query by moving the on clause predicate to thewhere clause, and instead using an on condition of true.
In the latter query, however, every tuple satisfies the join condition true, so no null-padded tuples are generated by the outer join.
The outer join actually generates the Cartesian product of the two relations.
Since there is no tuple in takeswith ID = 70557, every time a tuple appears in the outer join with name = “Snow”, the values for student.ID and takes.ID must be different, and such tuples would be eliminated by the where clause predicate.
Thus student Snow never appears in the result of the latter query.
To distinguish normal joins from outer joins, normal joins are called inner joins in SQL.
A join clause can thus specify inner join instead of outer join to specify that a normal join is to be used.
The default join type, when the join clause is used without the outer prefix is the inner join.
Figure 4.6 shows a full list of the various types of join that we have discussed.
As can be seen from the figure, any form of join (inner, left outer, right outer, or full outer) can be combined with any join condition (natural, using, or on)
In our examples up to this point, we have operated at the logical-model level.
That is, we have assumed that the relations in the collection we are given are the actual relations stored in the database.
It is not desirable for all users to see the entire logical model.
Security considerations may require that certain data be hidden from users.
Consider a clerk who needs to know an instructor’s ID, name and department name, but does not have authorization to see the instructor’s salary amount.
This person should see a relation described in SQL, by:
Aside from security concerns, we may wish to create a personalized collection of relations that is better matched to a certain user’s intuition than is the logical model.
We may want to have a list of all course sections offered by the Physics department in the Fall 2009 semester, with the building and room number of each section.
The relation that we would create for obtaining such a list is:
It is possible to compute and store the results of the above queries and then make the stored relations available to users.
However, if we did so, and the underlying data in the relations instructor, course, or section changes, the stored query results would then no longer match the result of reexecuting the query on the relations.
In general, it is a bad idea to compute and store query results such as those in the above examples (although there are some exceptions, which we study later)
Instead, SQL allows a “virtual relation” to be defined by a query, and the relation conceptually contains the result of the query.
The virtual relation is not precomputed and stored, but instead is computed by executing the query whenever the virtual relation is used.
Any such relation that is not part of the logical model, but is made visible to a user as a virtual relation, is called a view.
It is possible to support a large number of views on top of any given set of actual relations.
We define a view in SQL by using the create view command.
To define a view, we must give the view a name and must state the query that computes the view.
Consider again the clerkwho needs to access all data in the instructor relation, except salary.
The clerk should not be authorized to access the instructor relation (we see later, in Section 4.6, how authorizations can be specified)
Instead, a view relation faculty can be made available to the clerk, with the view defined as follows:
As explained earlier, the view relation conceptually contains the tuples in the query result, but is not precomputed and stored.
Thus, the view relation is created whenever needed, on demand.
To create a view that lists all course sections offered by the Physics department in the Fall 2009 semester with the building and room number of each section, we write:
Once we have defined a view, we can use the view name to refer to the virtual relation that the view generates.
View names may appear in a query any place where a relation name may appear, The attribute names of a view can be specified explicitly as follows:
The preceding view gives for each department the sum of the salaries of all the instructors at that department.
Since the expression sum(salary) does not have a name, the attribute name is specified explicitly in the view definition.
Intuitively, at any given time, the set of tuples in the view relation is the result of evaluation of the query expression that defines the view.
Thus, if a view relation is computed and stored, it may become out of date if the relations used to define it are modified.
When we define a view, the database system stores the definition of the view itself, rather than the result of evaluation of the query expression that defines the view.
Wherever a view relation appears in a query, it is replaced by the stored query expression.
Thus, whenever we evaluate the query, the view relation is recomputed.
One viewmay be used in the expression defining another view.
Certain database systems allow view relations to be stored, but they make sure that, if the actual relations used in the view definition change, the view is kept up-to-date.
If the above view is materialized, its results would be stored in the database.
However, if an instructor tuple is added to or deleted from the instructor relation, the result of the query defining the view would change, and as a result the materialized view’s contents must be updated.
Similarly, if an instructor’s salary is updated, the tuple in departments total salary corresponding to that instructor’s department must be updated.
The process of keeping the materialized view up-to-date is called materialized view maintenance (or often, just view maintenance) and is covered in Section 13.5
View maintenance can be done immediately when any of the relations on which the view is defined is updated.
Some database systems, however, perform view maintenance lazily, when the view is accessed.
Some systems update materialized views only periodically; in this case, the contents of the materialized viewmay be stale, that is, not up-to-date, when it is used, and should not be used if the application needs up-to-date data.
And some database systems permit the database administrator to control which of the above methods is used for each materialized view.
Applications that use a view frequently may benefit if the view is materialized.
Applications that demand fast response to certain queries that compute aggregates over large relations can also benefit greatly by creating materialized views corresponding to the queries.
In this case, the aggregated result is likely to be much smaller than the large relations on which the view is defined; as a result the materialized view can be used to answer the query very quickly, avoiding reading the large underlying relations.
Of course, the benefits to queries from the materialization of a view must be weighed against the storage costs and the added overhead for updates.
Some database systems always keepmaterialized views up-to-date when the underlying relations change, while others permit them to become out of date, and periodically recompute them.
Although views are a useful tool for queries, they present serious problems if we express updates, insertions, or deletions with them.
The difficulty is that a modification to the database expressed in terms of a viewmust be translated to a modification to the actual relations in the logical model of the database.
Suppose the view faculty, which we saw earlier, is made available to a clerk.
Since we allow a view name to appear wherever a relation name is allowed, the clerk can write:
This insertionmust be represented by an insertion into the relation instructor, since instructor is the actual relation fromwhich thedatabase systemconstructs the view faculty.
However, to insert a tuple into instructor, we must have some value for salary.
There are two reasonable approaches to dealing with this insertion:
Reject the insertion, and return an error message to the user.
Insert a tuple (’30765’, ’Green’, ’Music’, null) into the instructor relation.
Another problem with modification of the database through views occurs with a view such as:
This view lists the ID, name, and building-nameof each instructor in the university.
Suppose there is no instructor with ID 69987, and no department in the Taylor building.
Then the only possible method of inserting tuples into the instructor and department relations is to insert (’69987’, ’White’, null, null) into instructor and (null, ’Taylor’, null) into department.
However, this update does not have the desired effect, since the view relation instructor info still does not include the tuple (’69987’, ’White’, ’Taylor’)
Thus, there is no way to update the relations instructor and department by using nulls to get the desired update on instructor info.
Because of problems such as these, modifications are generally not permitted on view relations, except in limited cases.
Different database systems specify different conditions under which they permit updates on view relations; see the database system manuals for details.
The general problem of database modification through views has been the subject of substantial research, and the bibliographic notes provide pointers to some of this research.
In general, an SQL view is said to be updatable (that is, inserts, updates or deletes can be applied on the view) if the following conditions are all satisfied by the query defining the view:
The select clause contains only attribute names of the relation, and does not.
Any attribute not listed in the select clause can be set to null; that is, it does not have a not null constraint and is not part of a primary key.
The query does not have a group by or having clause.
Under these constraints, the update, insert, and delete operations would be allowed on the following view:
Figure 4.7 Relations instructor and department after insertion of tuples.
Evenwith the conditions on updatability, the following problem still remains.
This tuple can be inserted into the instructor relation, but it would not appear in the history instructors view since it does not satisfy the selection imposed by the view.
By default, SQL would allow the above update to proceed.
However, views can be defined with awith check option clause at the end of the view definition; then, if a tuple inserted into the view does not satisfy the view’s where clause condition, the insertion is rejected by the database system.
Updates are similarly rejected if the new value does not satisfy thewhere clause conditions.
SQL:1999 has a more complex set of rules about when inserts, updates, and deletes can be executed on a view, that allows updates through a larger class of views; however, the rules are too complex to be discussed here.
A transaction consists of a sequence of query and/or update statements.
The SQL standard specifies that a transaction begins implicitly when an SQL statement is executed.
One of the following SQL statements must end the transaction:
Commit work commits the current transaction; that is, it makes the updates performed by the transaction become permanent in the database.
After the transaction is committed, a new transaction is automatically started.
Rollback work causes the current transaction to be rolled back; that is, it undoes all the updates performed by the SQL statements in the transaction.
Thus, the database state is restored to what it was before the first statement of the transaction was executed.
Transaction rollback is useful if some error condition is detected during execution of a transaction.
Commit is similar, in a sense, to saving changes to a document that is being edited, while rollback is similar to quitting the edit session without saving changes.
Once a transaction has executed commit work, its effects can no longer be undone by rollback work.
The database system guarantees that in the event of some failure, such as an error in one of the SQL statements, a power outage, or a system crash, a transaction’s effects will be rolled back if it has not yet executed commit work.
In the case of power outage or other system crash, the rollback occurs when the system restarts.
To do so, we need to update two account balances, subtracting the amount transferred from one, and adding it to the other.
If the system crashes after subtracting the amount from the first account, but before adding it to the second account, the bank balances would be inconsistent.
A similar problem would occur, if the second account is credited before subtracting the amount from the first account, and the system crashes just after crediting the amount.
As another example, consider our running example of a university application.
We assume that the attribute tot cred of each tuple in the student relation is kept up-to-date by modifying it whenever the student successfully completes a course.
To do so, whenever the takes relation is updated to record successful completion of a course by a student (by assigning an appropriate grade) the corresponding student tuple must also be updated.
If the application performing these two updates crashes after one update is performed, but before the second one is performed, the data in the database would be inconsistent.
By either committing the actions of a transaction after all its steps are completed, or rolling back all its actions in case the transaction could not complete all its actions successfully, the database provides an abstraction of a transaction as being atomic, that is, indivisible.
Either all the effects of the transaction are reflected in the database, or none are (after rollback)
Applying the notion of transactions to the above applications, the update statements should be executed as a single transaction.An errorwhile a transaction executes one of its statements would result in undoing of the effects of the earlier statements of the transaction, so that the database is not left in a partially updated state.
If a program terminates without executing either of these commands, the updates are either committed or rolled back.
The standard does not specify which of the two happens, and the choice is implementation dependent.
In many SQL implementations, by default each SQL statement is taken to be a transaction on its own, and gets committed as soon as it is executed.
Automatic commit of individual SQL statementsmust be turned off if a transaction consisting of multiple SQL statements needs to be executed.
A better alternative, which is part of the SQL:1999 standard (but supported by only some SQL implementations currently), is to allow multiple SQL statements to be enclosed between the keywords begin atomic.
All the statements between the keywords then form a single transaction.
Integrity constraints ensure that changes made to the database by authorized users do not result in a loss of data consistency.
Thus, integrity constraints guard against accidental damage to the database.
Every department name in the course relation must have a matching department name in the department relation.
The budget of a department must be greater than $0.00
In general, an integrity constraint can be an arbitrary predicate pertaining to the database.
Thus, most database systems allow one to specify integrity constraints that can be testedwith minimal overhead.
We have already seen some forms of integrity constraints in Section 3.2.2
We study some more forms of integrity constraints in this section.
Integrity constraints are usually identified as part of the database schema design process, and declared as part of the create table command used to create relations.
However, integrity constraints can also be added to an existing relation by using the command alter table table-name add constraint, where constraint can be any constraint on the relation.
When such a command is executed, the system first ensures that the relation satisfies the specified constraint.
If it does, the constraint is added to the relation; if not, the command is rejected.
We described in Section 3.2 how to define tables using the create table command.
In addition to the primary-key constraint, there are a number of other ones that can be included in the create table command.
We cover each of these types of constraints in the following sections.
As we discussed in Chapter 3, the null value is a member of all domains, and as a result is a legal value for every attribute in SQL by default.
Consider a tuple in the student relation where name is null.
Such a tuple gives student information for an unknown student; thus, it does not contain useful information.
Similarly, we would not want the department budget to be null.
In cases such as this, we wish to forbid null values, and we can do so by restricting the domain of the attributes name and budget to exclude null values, by declaring it as follows:
The not null specification prohibits the insertion of a null value for the attribute.
Any database modification that would cause a null to be inserted in an attribute declared to be not null generates an error diagnostic.
There are many situations where we want to avoid null values.
In particular, SQL prohibits null values in the primary key of a relation schema.
Thus, in our university example, in the department relation, if the attribute dept name is declared.
As a result it would not need to be declared explicitly to be not null.
Ajm form a candidate key; that is, no two tuples in the relation can be equal on all the listed attributes.
However, candidate key attributes are permitted to be null unless they have explicitly been declared to be not null.
Recall that a null value does not equal any other value.
The treatment of nulls here is the same as that of the unique construct defined in Section 3.8.4
When applied to a relation declaration, the clause check(P) specifies a predicate P that must be satisfied by every tuple in a relation.
A common use of the check clause is to ensure that attribute values satisfy specified conditions, in effect creating a powerful type system.
Here, we use the check clause to simulate an enumerated type, by specifying that semester must be one of ’Fall’, ’Winter’, ’Spring’, or ’Summer’
The predicate in the check clause can, according to the SQL standard, be an arbitrary predicate that can include a subquery.
However, currently none of the widely used database products allows the predicate to contain a subquery.
Often, we wish to ensure that a value that appears in one relation for a given set of attributes also appears for a certain set of attributes in another relation.
The definition of the course table has a declaration “foreign key (dept name) references department”
This foreign-key declaration specifies that for each course tuple, the department name specified in the tuplemust exist in the department relation.
Without this constraint, it is possible for a course to specify a nonexistent department name.
By default, in SQL a foreign key references the primary-key attributes of the referenced table.
The specified list of attributes must, however, be declared as a candidate key of the referenced relation, using either a primary key constraint, or a unique constraint.
The SQL standard specifies other constructs that can be used to implement such constraints; they are described in Section 4.4.7
We canuse the following short formas part of an attribute definition to declare that the attribute forms a foreign key:
Figure 4.8 SQL data definition for part of the university database.
Consider this definition of an integrity constraint on the relation course:
Instead, the delete “cascades” to the course relation, deleting the tuple that refers to the department that was deleted.
Similarly, the system does not reject an update to a field referenced by the constraint if it violates the constraint; instead, the system updates the field dept name in the referencing tuples in course to the new value as well.
If there is a chain of foreign-key dependencies across multiple relations, a deletion or update at one end of the chain can propagate across the entire chain.
An interesting case where the foreign key constraint on a relation references the same relation appears in Practice Exercises 4.9
If a cascading update or delete causes a constraint violation that cannot be handled by a further cascading operation, the system aborts the transaction.
As a result, all the changes caused by the transaction and its cascading actions are undone.
Attributes of foreign keys are allowed to be null, provided that they have not otherwise been declared to be not null.
If all the columns of a foreign key are nonnull in a given tuple, the usual definition of foreign-key constraints is used for that tuple.
If any of the foreign-key columns is null, the tuple is defined automatically to satisfy the constraint.
This definition may not always be the right choice, so SQL also provides constructs that allow you to change the behavior with null values; we do not discuss the constructs here.
Transactions may consist of several steps, and integrity constraints may be violated temporarily after one step, but a later step may remove the violation.
For instance, suppose we have a relation person with primary key name, and an attribute spouse, and suppose that spouse is a foreign key on person.
That is, the constraint says that the spouse attribute must contain a name that is present in the person table.
Suppose we wish to note the fact that John and Mary are married to each other by inserting two tuples, one for John and one forMary, in the above relation, with the spouse attributes set toMary and John, respectively.
The insertion of the first tuple would violate the foreign-key constraint, regardless of which of.
After the second tuple is inserted the foreign-key constraint would hold again.
To handle such situations, the SQL standard allows a clause initially deferred to be added to a constraint specification; the constraint would then be checked at the end of a transaction, and not at intermediate steps.
A constraint can alternatively be specified as deferrable, which means it is checked immediately by default, but can be deferred when desired.
For constraints declared as deferrable, executing a statement set constraints constraint-list deferred as part of a transaction causes the checking of the specified constraints to be deferred to the end of that transaction.
However, you shouldbe aware that thedefault behavior is to check constraints immediately, and many database implementations do not support deferred constraint checking.
We can work around the problem in the above example in another way, if the spouse attribute can be set to null: We set the spouse attributes to null when inserting the tuples for John and Mary, and we update them later.
However, this technique requires more programming effort, and does not work if the attributes cannot be set to null.
The SQL standard supports additional constructs for specifying integrity constraints that are described in this section.
However, you should be aware that these constructs are not currently supported by most database systems.
As defined by the SQL standard, the predicate in the check clause can be an arbitrary predicate, which can include a subquery.
The check condition verifies that the time slot id in each tuple in the section relation is actually the identifier of a time slot in the time slot relation.
Thus, the condition has to be checked not only when a tuple is inserted or modified in section, but also when the relation time slot changes (in this case, when a tuple is deleted or modified in relation time slot)
Another natural constraint on our university schemawould be to require that every section has at least one instructor teaching the section.
In an attempt to enforce this, we may try to declare that the attributes (course id, sec id, semester, year) of the section relation form a foreign key referencing the corresponding attributes of the teaches relation.
Unfortunately, these attributes do not form a candidate key of the relation teaches.
A check constraint similar to that for the time slot attribute can be used to enforce this constraint, if check constraints with subqueries were supported by a database system.
Complex check conditions can be useful when we want to ensure integrity of data, but may be costly to test.
An assertion is a predicate expressing a condition that we wish the database always to satisfy.
We have paid substantial attention to these forms of assertions because they are easily tested and apply to a wide range of database applications.
However, there are many constraints that we cannot express by using only these special forms.
For each tuple in the student relation, the value of the attribute tot cred must equal the sum of credits of courses that the student has completed successfully.
An instructor cannot teach in two different classrooms in a semester in the same time slot.1
In Figure 4.9, we show how the first example of constraints can be written in SQL.
Since SQL does not provide a “for all X, P(X)” construct (where P is a predicate), we are forced to implement the constraint by an equivalent construct, “not exists X such that not P(X)”, that can be expressed in SQL.
We leave the specification of the second constraint as an exercise.
When an assertion is created, the system tests it for validity.
This testing may introduce a significant amount of overhead if complex assertions have been made.
The high overhead of testing and maintaining assertions has led some system developers to omit support for general assertions, or to provide specialized forms of assertion that are easier to test.
We assume that lectures are not displayed remotely in a second classroom! An alternative constraint that specifies that “an instructor cannot teach two courses in a given semester in the same time slot” may not hold since courses are sometimes cross-listed; that is, the same course is given two identifiers and titles.
Currently, none of the widely used database systems supports either subqueries in the check clause predicate, or the create assertion construct.
However, equivalent functionality can be implemented using triggers, which are described in Section 5.3, if they are supported by the database system.
Section 5.3 also describes how the referential integrity constraint on time slot id can be implemented using triggers.
In Chapter 3, we covered a number of built-in data types supported in SQL, such as integer types, real types, and character types.
There are additional built-in data types supported by SQL, which we describe below.We also describe how to create basic user-defined types in SQL.
In addition to the basic data types we introduced in Section 3.2, the SQL standard supports several data types relating to dates and times:
A variant, time(p), can be used to specify the number of fractional digits for seconds (the default being 0)
It is also possible to store time-zone information along with the time by specifying time with timezone.
A variant, timestamp(p), can be used to specify the number of fractional digits for seconds (the default here being 6)
Dates must be specified in the format year followed by month followed by day, as shown.
The seconds field of time or timestamp can have a fractional part, as in the timestamp above.
We can use an expression of the form cast e as t to convert a character string (or string valued expression) e to the type t, where t is one of date, time, or timestamp.
The string must be in the appropriate format as illustrated at the beginning of this paragraph.
When required, time-zone information is inferred from the system settings.
To extract individual fields of a date or time value d, we can use extract (field from d), where field can be one of year,month, day, hour,minute, or second.
Timezone information can be extracted using timezone hour and timezone minute.
For example, current date returns the current date, current time returns the current time (with time zone), and localtime returns the current local time (without time zone)
Timestamps (date plus time) are returned by current timestamp (with time zone) and localtimestamp (local date and time without time zone)
The following insert statement illustrates how an insertion can omit the value for the tot cred attribute.
Many queries reference only a small proportion of the records in a file.
For example, a query like “Find all instructors in the Physics department” or “Find the tot cred value of the student with ID 22201” references only a fraction of the student records.
It is inefficient for the system to read every record and to check ID field for the ID “32556,” or the building field for the value “Physics”
An index on an attribute of a relation is a data structure that allows the database system to find those tuples in the relation that have a specified value for that attribute efficiently, without scanning through all the tuples of the relation.
We study later, in Chapter 11, how indices are actually implemented, including a particularly widely used kind of index called a B+-tree index.
The above statement creates an index named studentID index on the attribute ID of the relation student.
When a user submits an SQL query that can benefit from using an index, the SQL query processor automatically uses the index.
For example, given an SQL query that selects the student tuple with ID 22201, the SQL query processor would use the index studentID index defined above to find the required tuple without reading the whole relation.
Many current-generation database applications need to store attributes that can be large (of the order of many kilobytes), such as a photograph, or very large (of the order of many megabytes or even gigabytes), such as a high-resolution medical image or video clip.
The letters “lob” in these data types stand for “Large OBject.” For example, we may declare attributes.
For result tuples containing large objects (multiplemegabytes to gigabytes), it is inefficient or impractical to retrieve an entire large object into memory.
Instead, an application would usually use an SQL query to retrieve a “locator” for a large object and then use the locator to manipulate the object from the host language in which the application itself is written.
For instance, the JDBC application program interface (described in Section 5.1.1) permits a locator to be fetched instead of the entire large object; the locator can then be used to fetch the large object in small pieces, rather than all at once, much like reading data from an operating system file using a read function call.
The first form, which we cover here, is called distinct types.
It is possible for several attributes to have the same data type.
It is perhaps less clear whether name and dept name should have the samedomain.At the implementation level, both instructor names and department names are character strings.
However, we would normally not consider the query “Find all instructorswhohave the samenameas adepartment” to be a meaningful query.
Thus, if we view the database at the conceptual, rather than the physical, level, name and dept name should have distinct domains.
More importantly, at a practical level, assigning an instructor’s name to a department name is probably a programming error; similarly, comparing a monetary value expressed in dollars directly with a monetary value expressed in pounds is also almost surely a programming error.
A good type system should be able to detect such assignments or comparisons.
To support such checks, SQL provides the notion of distinct types.
The create type clause can be used to define new types.
The keyword final isn’t reallymeaningful in this context but is requiredby the SQL:1999 standard for reasonswewon’t get into here; some implementations allow thefinalkeyword to be omitted.
The newly created types can then be used, for example, as types of attributes of relations.
An attempt to assign a value of type Dollars to a variable of type Pounds results in a compile-time error, although both are of the same numeric type.
Such an assignment is likely to be due to a programmer error, where the programmer forgot about the differences in currency.
Declaring different types for different currencies helps catch such errors.
Values of one type can be cast (that is, converted) to another domain, as illustrated below:
We could do addition on the numeric type, but to save the result back to an attribute of typeDollars we would have to use another cast expression to convert the type back to Dollars.
For example, we could define a domain DDollars as follows.
The domain DDollars can be used as an attribute type, just as we used the type Dollars.
However, there are two significant differences between types and domains:
Domains can have constraints, such as not null, specified on them, and can have default values defined for variables of the domain type, whereas userdefined types cannot have constraints or default values specified on them.
User-defined types are designed to be used not just for specifying attribute types, but also in procedural extensions to SQL where it may not be possible to enforce constraints.
As a result, values of one domain type can be assigned to values of another domain type as long as the underlying types are compatible.
When applied to a domain, the check clause permits the schema designer to specify a predicate that must be satisfied by any attribute declared to be from this domain.
For instance, a check clause can ensure that an instructor’s salary domain allows only values greater than a specified value:
The domain YearlySalary has a constraint that ensures that the YearlySalary is greater than or equal to $29,000.00
The clause constraint salary value test is optional, and is used to give the name salary value test to the constraint.
The name is used by the system to indicate the constraint that an update violated.
As another example, a domain can be restricted to contain only a specified set of values by using the in clause:
Although the create type and create domain constructs described in this section are part of the SQL standard, the forms of these constructs described here are not fully supported by most database implementations.
PostgreSQL supports the create domain construct, but its create type construct has a different syntax and interpretation.
Microsoft SQL Server implements a version of create type construct that supports domain constraints, similar to the SQL create domain construct.
Oracle, IBM DB2, PostgreSQL, and SQL Server all support objectoriented type systems using different forms of the create type construct.
Applications often require creation of tables that have the same schema as an existing table.
The above statement creates a new table temp instructor that has the same schema as instructor.
When writing a complex query, it is often useful to store the result of a query as a new table; the table is usually temporary.
Two statements are required, one to create the table (with appropriate columns) and the second to insert the query result into the table.
SQL:2003 provides a simpler technique to create a table containing the results of a query.
For example the following statement creates a table t1 containing the results of a query.
By default, the names and data types of the columns are inferred from the query result.
Names can be explicitly given to the columns by listing the column names after the relation name.
As defined by the SQL:2003 standard, if the with data clause is omitted, the table is created but not populated with data.
However many implementations populate the table with data by default even if the with data clause is omitted.
Note that several implementations support the functionality of create table.
The main difference is that the contents of the table are set when the table is created, whereas the contents of a view always reflect the current query result.
To understand the motivation for schemas and catalogs, consider how files are named in a file system.
Early file systems were flat; that is, all files were stored in a single directory.
Current file systems, of course, have a directory (or, synonymously, folder) structure, with files stored within subdirectories.
Like early file systems, early database systems also had a single name space for all relations.
Users had to coordinate to make sure they did not try to use the same name for different relations.
Contemporary database systems provide a three-level hierarchy for naming relations.
The top level of the hierarchy consists of catalogs, each of which can contain schemas.
Some database implementations use the term “database” in place of the term catalog.
In order to perform any actions on a database, a user (or a program) must first connect to the database.
The user must provide the user name and usually, a password for verifying the identity of the user.
Each user has a default catalog and schema, and the combination is unique to the user.
When a user connects to a database system, the default catalog and schema are set up for the connection; this corresponds to the current directory being set to the user’s home directory when the user logs into an operating system.
We may omit the catalog component, in which case the catalog part of the name is considered to be the default catalog for the connection.
Thus if catalog5 is the default catalog, we can use univ schema.course to identify the same relation uniquely.
If a user wishes to access a relation that exists in a different schema than the default schema for that user, the name of the schemamust be specified.
However, if a relation is in the default schema for a particular user, then even the schema namemay be omitted.
Thus we can use just course if the default catalog is catalog5 and the default schema is univ schema.
With multiple catalogs and schemas available, different applications and different users can work independently without worrying about name clashes.
Moreover, multiple versions of an application—one a production version, other test versions—can run on the same database system.
The default catalog and schema are part of an SQL environment that is set up for each connection.
The environment additionally contains the user identifier (also referred to as the authorization identifier)
All the usual SQL statements, including the DDL and DML statements, operate in the context of a schema.
We can create and drop schemas bymeans of create schema and drop schema statements.
In most database systems, schemas are also created automatically when user accounts are created, with the schema name set to the user account name.
The schema is created in either a default catalog, or a catalog specified in creating the user account.
The newly created schema becomes the default schema for the user account.
Creation and dropping of catalogs is implementation dependent and not part of the SQL standard.
We may assign a user several forms of authorizations on parts of the database.
Each of these types of authorizations is called a privilege.
We may authorize the user all, none, or a combination of these types of privileges on specified parts of a database, such as a relation or a view.
When a user submits a query or an update, the SQL implementation first checks if the query or update is authorized, based on the authorizations that the user has been granted.
If the query or update is not authorized, it is rejected.
In addition to authorizations on data, users may also be granted authorizations on the database schema, allowing them, for example, to create, modify, or drop relations.
A user who has some form of authorization may be allowed to pass on (grant) this authorization to other users, or to withdraw (revoke) an authorization that was granted earlier.
In this section, we see how each of these authorizations can be specified in SQL.
The ultimate form of authority is that given to the database administrator.
The database administratormay authorize newusers, restructure the database, and so on.
This form of authorization is analogous to that of a superuser, administrator, or operator for an operating system.
The SQL standard includes the privileges select, insert, update, and delete.
A user who creates a new relation is given all privileges on that relation automatically.
The SQL data-definition language includes commands to grant and revoke privileges.
The privilege list allows the granting of several privileges in one command.
The notion of roles is covered later, in Section 4.6.2
The select authorization on a relation is required to read tuples in the relation.
The following grant statement grants database users Amit and Satoshi select authorization on the department relation:
This allows those users to run queries on the department relation.
The update authorization on a relation allows a user to update any tuple.
The update authorization may be given either on all attributes of the relation or on only some.
If update authorization is included in a grant statement, the list of attributes on which update authorization is to be granted optionally appears in parentheses immediately after the update keyword.
If the list of attributes is omitted, the update privilege will be granted on all attributes of the relation.
This grant statement gives users Amit and Satoshi update authorization on the budget attribute of the department relation:
The insert authorization on a relation allows a user to insert tuples into the relation.
The insert privilege may also specify a list of attributes; any inserts to the relation must specify only these attributes, and the system either gives each of the remaining attributes default values (if a default is defined for the attribute) or sets them to null.
The delete authorization on a relation allows a user to delete tuples from a relation.
The user name public refers to all current and future users of the system.
Thus, privileges granted to public are implicitly granted to all current and future users.
By default, a user/role that is granted a privilege is not authorized to grant that privilege to another user/role.
We describe this feature in more detail in Section 4.6.5
It is worth noting that the SQL authorization mechanism grants privileges on an entire relation, or on specified attributes of a relation.
However, it does not permit authorizations on specific tuples of a relation.
It takes a form almost identical to that of grant:
Thus, to revoke the privileges that we granted previously, we write.
Revocation of privileges is more complex if the user fromwhom the privilege is revoked has granted the privilege to another user.
Consider the real-world roles of various people in a university.
A better approachwould be to specify the authorizations that every instructor is to be given, and to identify separatelywhich database users are instructors.
The system can use these two pieces of information to determine the authorizations of each instructor.When a new instructor is hired, a user identifiermust be allocated to him, and he must be identified as an instructor.
Individual permissions given to instructors need not be specified again.
Authorizations can be granted to roles, in exactly the same fashion as they are granted to individual users.
Each database user is granted a set of roles (which may be empty) that she is authorized to perform.
In our university database, examples of roles could include instructor, teaching assistant, student, dean, and department chair.
A less preferable alternative would be to create an instructor userid, and permit each instructor to connect to the database using the instructor userid.
The problem with this approach is that it would not be possible to identify exactly which instructor carried out a database update, leading to security risks.
The use of roles has the benefit of requiring users to connect to the database with their own userid.
Any authorization that can be granted to a user can be granted to a role.
Roles can then be granted privileges just as the users can, as illustrated in this statement:
Roles can be granted to users, as well as to other roles, as these statements show:
Thus the privileges of a user or a role consist of:
All privileges granted to roles that have been granted to the user/role.
Note that there can be a chain of roles; for example, the role teaching assistant may be granted to all instructors.
In turn the role instructor is granted to all deans.
Thus, the dean role inherits all privileges granted to the roles instructor and to teaching assistant in addition to privileges granted directly to dean.
When a user logs in to the database system, the actions executed by the user during that session have all the privileges granted directly to the user, as well as all privileges granted to roles that are granted (directly or indirectly via other roles) to that user.
Thus, if a user Amit has been granted the role dean, user Amit holds all privileges granted directly to Amit, as well as privileges granted to dean, plus privileges granted to instructor, and teaching assistant if, as above, those roles were granted (directly or indirectly) to the role dean.
It is worth noting that the concept of role-based authorization is not specific to SQL, and role-based authorization is used for access control in a wide variety of shared applications.
In our university example, consider a staff member who needs to know the salaries of all faculty in a particular department, say the Geology department.
This staff member is not authorized to see information regarding faculty in other departments.
Thus, the staffmembermust be denied direct access to the instructor relation.
But, if he is to have access to the information for theGeology department, he might be granted access to a view that we shall call geo instructor, consisting.
Suppose that the staff member issues the following SQL query:
Clearly, the staff member is authorized to see the result of this query.
However, when the query processor translates it into a query on the actual relations in the database, it produces a query on instructor.
Thus, the system must check authorization on the clerk’s query before it begins query processing.
A user who creates a view does not necessarily receive all privileges on that view.
She receives only those privileges that provide no additional authorization beyond those that she already had.
For example, a user who creates a view cannot be given update authorization on a view without having update authorization on the relations used to define the view.
If a user creates a view on which no authorization can be granted, the system will deny the view creation request.
In our geo instructor view example, the creator of the view must have select authorization on the instructor relation.
As we will see later, in Section 5.2, SQL supports the creation of functions and procedures, which may in turn contain queries and updates.
The execute privilege can be granted on a function or procedure, enabling a user to execute the function/procedure.
By default, just like views, functions and procedures have all the privileges that the creator of the function or procedure had.
In effect, the function or procedure runs as if it were invoked by the user who created the function.
Although this behavior is appropriate in many situations, it is not always appropriate.
Starting with SQL:2003, if the function definition has an extra clause sql security invoker, then it is executed under the privileges of the user who invokes the function, rather than the privileges of the definer of the function.
This allows the creation of libraries of functions that can run under the same authorization as the invoker.
The SQL standard specifies a primitive authorization mechanism for the database schema: Only the owner of the schema can carry out any modification to the schema, such as creating or deleting relations, adding or dropping attributes of relations, and adding or dropping indices.
However, SQL includes a references privilege that permits a user to declare foreign keys when creating relations.
The SQL references privilege is granted on specific attributes in a manner like that for the update privilege.
The following grant statement allows user Mariano to create relations that reference the key branch name of the branch relation as a foreign key:
Initially, it may appear that there is no reason ever to prevent users from creating foreign keys referencing another relation.
However, recall that foreignkey constraints restrict deletion and update operations on the referenced relation.
Suppose Mariano creates a foreign key in a relation r referencing the dept name attribute of the department relation and then inserts a tuple into r pertaining to the Geology department.
It is no longer possible to delete the Geology department from the department relationwithout alsomodifying relation r.
Thus, thedefinition of a foreign key byMariano restricts future activity by other users; therefore, there is a need for the references privilege.
Continuing to use the example of the department relation, the references privilege on department is also required to create a check constraint on a relation r if the constraint has a subquery referencing department.
This is reasonable for the same reason as the one we gave for foreign-key constraints; a check constraint that references a relation limits potential updates to that relation.
A user who has been granted some form of authorization may be allowed to pass on this authorization to other users.
By default, a user/role that is granted a privilege is not authorized to grant that privilege to another user/role.
If we wish to grant a privilege and to allow the recipient to pass the privilege on to other users,we append thewith grant option clause to the appropriate grant command.
For example, ifwewish to allowAmit the select privilege on department and allow Amit to grant this privilege to others, we write:
Consider, as an example, the granting of update authorization on the teaches relation of the university database.
The passing of a specific authorization from one user to another can be represented by an authorization graph.
A user has an authorization if and only if there is a path from the root of the authorization graph (the node representing the database administrator) down to the node representing the user.
Suppose that the database administrator decides to revoke the authorization of userU1
A pair of devious users might attempt to defeat the rules for revocation of authorization by granting authorization to each other.
Thus, SQL ensures that the authorization is revoked from both the users.
As we just saw, revocation of a privilege from a user/role may cause other users/roles also to lose that privilege.
However, the revoke statement may specify restrict in order to prevent cascading revocation:
In this case, the system returns an error if there are any cascading revocations, and does not carry out the revoke action.
The keyword cascade canbeused instead of restrict to indicate that revocation should cascade; however, it can be omitted, as we have done in the preceding examples, since it is the default behavior.
The following revoke statement revokes only the grant option, rather than the actual select privilege:
Note that some database implementations do not support the above syntax; instead, the privilege itself can be revoked, and then granted again without the grant option.
To deal with the above situation, SQL permits a privilege to be granted by a role rather than by a user.
By default, the current role associated with a session is null (except in some special cases)
The current role associated with a session can be set by executing set role role name.
The specified role must have been granted to the user, else the set role statement fails.
To grant a privilege with the grantor set to the current role associated with a session, we can add the clause:
Suppose the granting of the role instructor (or other privileges) to Amit is.
Then, revoking of roles/privileges (including the role dean) from Satoshi will not result in revoking of privileges that had the grantor set to the role dean, even if Satoshi was the user who executed the grant; thus, Amit would retain the instructor role even after Satoshi’s privileges are revoked.
View relations can be defined as relations containing the result of queries.
Views are useful for hiding unneeded information, and for collecting together information from more than one relation into a single view.
Transactions are a sequence of queries and updates that together carry out a task.
Transactions can be committed, or rolled back; when a transaction is rolled back, the effects of all updates performed by the transaction are undone.
Integrity constraints ensure that changes made to the database by authorized users do not result in a loss of data consistency.
Domain constraints specify the set of possible values that may be associated with an attribute.
Such constraints may also prohibit the use of null values for particular attributes.
Assertions are declarative expressions that state predicates that we require always to be true.
The SQL data-definition language provides support for defining built-in domain types such as date and time, as well as user-defined domain types.
A user who has been granted some form of authority may be allowed to pass on this authority to other users.
However, we must be careful about how authorization can be passed among users if we are to ensure that such authorization can be revoked at some future time.
Roles help to assign a set of privileges to a user according to the role that the user plays in the organization.
Display a list of all instructors, showing their ID, name, and the number of sections that they have taught.
Make sure to show the number of sections as 0 for instructors who have not taught any section.
Your query should use an outerjoin, and should not use scalar subqueries.
Write the same query as above, but using a scalar subquery, without outerjoin.
Display the list of all course sections offered in Spring 2010, along with the names of the instructors teaching the section.
If a section has more than one instructor, it should appear as many times in the result as it has instructors.
If it does not have any instructor, it should still appear in the result with the instructor name set to “—”
Display the list of all departments,with the total number of instructors in each department, without using scalar subqueries.
To illustrate this fact, show how to rewrite each of the following SQL queries without using the outer join expression.
Give instances of relations r , s and t such that in the result of the second expression, attribute C has a null value but attribute D has a non-null value.
Is the above pattern, with C null and D not null possible in the result of the first expression? Explain why or why not.
In Section 3.3.3 we saw an example of an erroneous SQL query which was intended to find which courses had been taught by each instructor; the query computed the natural join of instructor, teaches, and course, and as a result unintentionally equated the dept name attribute of instructor and course.
Give an example of a dataset that would help catch this particular error.
When creating test databases, it is important to create tuples in referenced relations that do not have anymatching tuple in the referencing relation, for each foreign key.
Explain why, using an example query on the university database.
When creating test databases, it is important to create tupleswith null values for foreign key attributes, provided the attribute is nullable (SQL allows foreign key attributes to take on null values, as long as they are not part of the primary key, and have not been declared as not null)
Explain why, using an example query on the university database.
Make sure your view definition correctly handles the case of null values for the grade attribute of the takes relation.
Write an SQL query that returns all (instructor, section) combinations that violate this constraint.
Write an SQL assertion to enforce this constraint (as discussed in Section 4.4.7, current generation database systems do not support such assertions, although they are part of the SQL standard)
Here, employee name is a key to the table manager, meaning that each employee has at most onemanager.
The foreign-key clause requires that every manager also be an employee.
Explain exactly what happens when a tuple in the relation manager is deleted.
Let a and b be relations with the schemas A(name, address, title), and B(name, address, salary), respectively.
Show how to express a natural full outer join b using the full outer-join operation with an on condition and the coalesce operation.
Make sure that the result relation does not contain two copies of the attributes name and address, and that the solution is correct even if some tuples in a and b have null values for attributesname or address.
Note that an employee may simply have no manager listed or may have a null manager.
Write your query using an outer join and then write it again using no outer join at all.
Consider a database that includes the relations shown in Figure 4.12
Suppose thatwewish to require that everyname that appears in address appears in either salaried worker or hourly worker, but not necessarily in both.
Discuss the actions that the system must take to enforce a constraint of this form.
Suppose user B then grants select on r to A.
Does this cause a cycle in the authorization graph? Explain why.
Discuss an advantage and a disadvantage of such an approach.
See the bibliographic notes of Chapter 3 for SQL reference material.
The rules used by SQL to determine the updatability of a view, and how.
In this chapter, we cover some of the more advanced features of SQL.1 We address the issue of how to access SQL from a general-purpose programming language, which is very important for building applications that use a database to store and retrieve data.
We describe how procedural code can be executed within the database, either by extending the SQL language to support procedural actions, or by allowing functions defined in procedural languages to be executed within the database.
We describe triggers, which can be used to specify actions that are to be carried out automatically on certain events such as insertion, deletion, or update of tuples in a specified relation.
We discuss recursive queries and advanced aggregation features supported by SQL.
Finally, we describe online analytic processing (OLAP) systems, which support interactive analysis of very large datasets.
Writing queries in SQL is usually much easier than coding the same queries in a general-purpose programming language.
However, a database programmer must have access to a general-purpose programming language for at least two reasons:
Not all queries can be expressed in SQL, since SQL does not provide the full expressive power of a general-purpose language.
That is, there exist queries that can be expressed in a language such as C, Java, or Cobol that cannot be expressed in SQL.
To write such queries, we can embed SQL within a more powerful language.
It is quite possible to study database design first, and study this chapter later.
However, for courses with a programming emphasis, a richer variety of laboratory exercises is possible after studying Section 5.1, and we recommend that it be covered before database design for such courses.
Nondeclarative actions—such as printing a report, interacting with a user, or sending the results of a query to a graphical user interface—cannot be done from within SQL.
Applications usually have several components, and querying or updating data is only one component; other components are written in general-purpose programming languages.
For an integrated application, there must be a means to combine SQL with a general-purpose programming language.
There are two approaches to accessing SQL from a general-purpose programming language:
Dynamic SQL: A general-purpose program can connect to and communicate with a database server using a collection of functions (for procedural languages) or methods (for object-oriented languages)
Dynamic SQL allows the program to construct an SQL query as a character string at runtime, submit the query, and then retrieve the result into program variables a tuple at a time.
The dynamic SQL component of SQL allows programs to construct and submit SQL queries at runtime.
In this chapter, we look at two standards for connecting to an SQL database.
One, JDBC (Section 5.1.1), is an application program interface for the Java language.
Embedded SQL: Like dynamic SQL, embedded SQL provides a means by which a program can interact with a database server.
However, under embedded SQL, the SQL statements are identified at compile time using a preprocessor.
A major challenge in mixing SQL with a general-purpose language is the mismatch in the ways these languages manipulate data.
In SQL, the primary type of data is the relation.
Programming languages normally operate on a variable at a time, and those variables correspond roughly to the value of an attribute in a tuple in a relation.
Thus, integrating these two types of languages into a single application requires providing a mechanism to return the result of a query in a manner that the program can handle.
The JDBC standard defines an application program interface (API) that Java programs can use to connect to database servers.
Figure 5.1 shows an example Java program that uses the JDBC interface.
It illustrates how connections are opened, how statements are executed and results processed, and how connections are closed.
The Java program must import java.sql.*, which contains the interface definitions for the functionality provided by JDBC.
The first step in accessing a database from a Java program is to open a connection to the database.
Only after opening a connection can a Java program execute SQL statements.
A connection is opened using the getConnection method of the DriverManager class (within java.sql)
The first parameter to the getConnection call is a string that specifies the URL, ormachine name,where the server runs (in our example, db.yale.edu), along with possibly some other information such as the protocol to be used to communicate with the database (in our example, jdbc:oracle:thin:; we shall shortly see why this is required), the port number the database system uses for communication (in our example, 2000), and the specific database on the server to be used (in our example, univdb)
Note that JDBC specifies only the API, not the communication protocol.
A JDBC drivermay supportmultiple protocols, and we must specify one supported by both the database and the driver.
The second parameter to getConnection is a database user identifier, which is a string.
The third parameter is a password, which is also a string.
Note that the need to specify a password within the JDBC code presents a security risk if an unauthorized person accesses your Java code.
In our example in the figure, we have created a Connection object whose handle is conn.
Each database product that supports JDBC (all the major database vendors do) provides a JDBC driver that must be dynamically loaded in order to access the database from Java.
In fact, loading the driver must be done first, before connecting to the database.
This interface provides for the translation of productindependent JDBC calls into the product-specific calls needed by the specific database management system being used.
The actual protocol used to exchange information with the database depends on the driver that is used, and is not defined by the JDBC standard.
There are multiple versions of the getConnection method, which differ in the parameters that they accept.
Sun also offers a “bridge driver” that converts JDBC calls to ODBC.
This should be used only for vendors that support ODBC but not JDBC.
In our example, when opening a connection with the database, the string jdbc:oracle:thin: specifies a particular protocol supported by Oracle.
Once a database connection is open, the program can use it to send SQL statements to the database system for execution.
This is done via an instance of the class Statement.
A Statement object is not the SQL statement itself, but rather an object that allows the Java program to invokemethods that ship an SQL statement given as an argument for execution by the database system.
Our example creates a Statement handle (stmt) on the connection conn.
It retrieves the set of tuples in the result into a ResultSet object rset and fetches them one tuple at a time.
The next method on the result set tests whether or not there remains at least one unfetched tuple in the result set and if so, fetches it.
The return value of the next method is a Boolean indicating whether it fetched a tuple.
Attributes from the fetched tuple are retrieved using various methods whose names begin with get.
The method getString can retrieve any of the basic SQL data types (converting the value to a Java String object), but more restrictive methods such as getFloat can be used as well.
The argument to the various get methods can either be an attribute name specified as a string, or an integer indicating the position of the desired attribute within the tuple.
The statement and connection are both closed at the end of the Java program.
Note that it is important to close the connection because there is a limit imposed on the number of connections to the database; unclosed connections may cause that limit to be exceeded.
If this happens, the application cannot open any more connections to the database.
We can create a prepared statement in which some values are replaced by “?”, thereby specifying that actual values will be provided later.
The database system compiles the query when it is prepared.
Each time the query is executed (with new values to replace the “?”s), the database system can reuse the previously compiled form of the query and apply the new values.
The code fragment in Figure 5.2 shows how prepared statements can be used.
The prepareStatement method of the Connection class submits an SQL statement for compilation.
The executeQuery and executeUpdate methods of PreparedStatement class do that.
But before they can be invoked, we must use methods of class PreparedStatement that assign values for the “?” parameters.
The setStringmethod and other similarmethods such as setInt for other basic SQL types allow us to specify the values for the parameters.
In the example in the figure, we prepare an insert statement, set the “?” parameters, and then invoke executeUpdate.
The final two lines of our example show that parameter assignments remain unchanged until we specifically reassign them.
Prepared statements allow for more efficient execution in cases where the same query can be compiled once and then run multiple times with different parameter values.
However, there is an even more significant advantage to prepared statements that makes them the preferredmethod of executing SQL queries whenever a user-entered value is used, even if the query is to be run only once.
Suppose that we read in a user-entered value and then use Java string manipulation to construct the SQL statement.
If the user enters certain special characters, such as a single quote, the resulting SQL statement may be syntactically incorrect unless we take extraordinary care in checking the input.
The setStringmethod does this for us automatically and inserts the needed escape characters to ensure syntactic correctness.
In our example, suppose that the values for the variables ID, name, dept name, and salary have been entered by a user, and a corresponding row is to be inserted into the instructor relation.
Suppose that, instead of using a prepared statement, a query is constructed by concatenating the strings using the following Java expression:
Now, if the user typed a single quote in the ID or name fields, the query stringwould have a syntax error.
It is quite possible that an instructor name may have a quotation mark in its name (for example, “O’Henry”)
While the above example might be considered an annoyance, the situation can be much worse.
A technique called SQL injection can be used by malicious hackers to steal data or damage the database.
Suppose a Java program inputs a string name and constructs the query:
In the resulting query, the where clause is always true and the entire instructor relation is returned.
More clever malicious users could arrange to output even more data.
Use of a prepared statement would prevent this problem because the input string would have escape characters inserted, so the resulting query becomes:
Older systems allow multiple statements to be executed in a single call, with.
This feature is being eliminated because the SQL injection technique was used by malicious hackers to insert whole SQL statements.
Because these statements run with the privileges of the owner of the.
Java program, devastating SQL statements such as drop table could be executed.
Developers of SQL applications need to be wary of such potential security holes.
These play the same role for functions and procedures as prepareStatement does for queries.
As we noted earlier, a Java application program does not include declarations for data stored in the database.
Therefore, a Java program that uses JDBCmust either have assumptions about the database schema hard-coded into the program or determine that information directly from the database system at runtime.
The latter approach is usually preferable, since it makes the application program more robust to changes in the database schema.
Recall that when we submit a query using the executeQuery method, the result of the query is contained in a ResultSet object.
The interface ResultSet has a method, getMetaData(), that returns a ResultSetMetaData object that containsmetadata about the result set.
ResultSetMetaData, in turn, hasmethods to find metadata information, such as the number of columns in the result, the name of a specified column, or the type of a specified column.
In this way, we can execute a query even if we have no idea of the schema of the result.
The Java code segment below uses JDBC to print out the names and types of all columns of a result set.
The variable rs in the code below is assumed to refer to a ResultSet instance obtained by executing a query.
The getColumnCount method returns the arity (number of attributes) of the result relation.
Arguments to getColumns: Catalog, Schema-pattern, Table-pattern, // and Column-Pattern // Returns: One row for each column; row has a number of attributes // such as COLUMN NAME, TYPE NAME.
For each attribute, we retrieve its name and data type using the methods getColumnName and getColumnTypeName, respectively.
The DatabaseMetaData interface provides a way to find metadata about the database.
The interface Connection has a method getMetaData that returns a DatabaseMetaData object.
The DatabaseMetaData interface in turn has a very large number of methods to get metadata about the database and the database system to which the application is connected.
For example, there are methods that return the product name and version number of the database system.
Other methods allow the application to query the database system about its supported features.
The code in Figure 5.3 illustrates how to find information about columns (attributes) of relations in a database.
The variable conn is assumed to be a handle for an already opened database connection.
The method getColumns takes four arguments: a catalog name (null signifies that the catalog name is to be ignored), a schema name pattern, a table name pattern, and a column name pattern.
The schema name, table name, and column name patterns can be used to specify a name or a pattern.
Patterns can use the SQL string matching special characters “%” and “ ”; for instance, the pattern “%” matches all names.
Only columns of tables of schemas satisfying the specified name or pattern are retrieved.
Each row in the result set contains information about one column.
The rows have a number of columns such as the name of the catalog, schema, table and column, the type of the column, and so on.
The metadata interfaces can be used for a variety of tasks.
For example, they can be used to write a database browser that allows a user to find the tables in a database, examine their schema, examine rows in a table, apply selections to see desired rows, and so on.
Similarly, it is possible to write code that takes a query string, executes the query, and prints out the results as a formatted table; the code can work regardless of the actual query submitted.
It can create an updatable result set from a query that performs a selection and/or a projection on a database relation.
An update to a tuple in the result set then results in an update to the corresponding tuple of the database relation.
Recall from Section 4.3 that a transaction allows multiple actions to be treated as a single atomic unit which can be committed or rolled back.
By default, each SQL statement is treated as a separate transaction that is committed automatically.
The method setAutoCommit() in the JDBC Connection interface allows this behavior to be turned on or off.
Transactions must then be committed or rolled back explicitly using either conn.commit() or conn.rollback()
To fetch large objects, the ResultSet interface provides methods getBlob() and getClob() that are similar to the getString() method, but return objects of type Blob and Clob, respectively.
These objects do not store the entire large object, but instead store “locators” for the large objects, that is, logical pointers to the actual large object in the database.
Fetching data from these objects is very much like fetching data from a file or an input stream, and can be performed using methods such as getBytes and getSubString.
Conversely, to store large objects in the database, the PreparedStatement class permits a database column whose type is blob to be linked to an input stream (such as a file that has been opened) using the method setBlob(int parameterIndex, InputStream inputStream)
When the prepared statement is executed, data are read from the input stream, and written to the blob in the database.
Similarly, a clob column can be set using the setClob method, which takes as arguments a parameter index and a character stream.
Row sets can be scanned both backward and forward and can be modified.
Because row sets are not part of the database itself once they are downloaded, we do not cover details of their use here.
The Open Database Connectivity (ODBC) standard defines an API that applications can use to open a connectionwith a database, send queries and updates, and get back results.
Each database system supportingODBC provides a library thatmust be linked with the client program.
When the client program makes an ODBC API call, the code in the library communicateswith the server to carry out the requested action, and fetch results.
Figure 5.4 shows an example of C code using the ODBC API.
The first step in using ODBC to communicate with a server is to set up a connection with the server.
To do so, the program first allocates an SQL environment, then a.
The program then opens the database connection by using SQLConnect.
This call takes several parameters, including the connection handle, the server to which to connect, the user identifier, and the password for the database.
The constant SQL NTS denotes that the previous argument is a null-terminated string.
Once the connection is set up, the program can send SQL commands to the database by using SQLExecDirect.
TheSQLBindCol function does this task; the second argument identifies the position of the attribute in the query result, and the third argument indicates the type conversion required from SQL to C.
For variable-length types like character arrays, the last two arguments give the maximum length of the variable and a location where the actual length is to be stored when a tuple is fetched.
A negative value returned for the length field indicates that the value is null.
For fixed-length types such as integer or float, the maximum length field is ignored, while a negative value returned for the length field indicates a null value.
The SQLFetch statement is in a while loop that is executed until SQLFetch returns a value other than SQL SUCCESS.
On each fetch, the program stores the values in C variables as specified by the calls on SQLBindCol and prints out these values.
At the end of the session, the program frees the statement handle, disconnects from the database, and frees up the connection and SQL environment handles.
Good programming style requires that the result of every function call must be checked to make sure there are no errors; we have omitted most of these checks for brevity.
The question marks are placeholders for values which will be supplied later.
The above statement can be “prepared,” that is, compiled at the database, and repeatedly executed by providing actual values for the placeholders—in this case, by providing an department name, building, and budget for the relation department.
By default, each SQL statement is treated as a separate transaction that is committed automatically.
The ODBC standard defines conformance levels, which specify subsets of the functionality defined by the standard.
Level 1 requires support for fetching information about the catalog, such as information about what relations are present and the types of their attributes.
The ADO.NET API, designed for the Visual Basic .NET and C# languages, provides functions to access data, which at a high level are not dissimilar to the JDBC functions, although details differ.
Like JDBC and ODBC, the ADO.NET API allows access to results of SQL queries, as well as to metadata, but is considerably simpler to use than ODBC.
A database that supports ODBC can be accessed using the ADO.NET API, and the ADO.NET calls are translated into ODBC calls.
The ADO.NET API can also be used with some kinds of nonrelational data sources such as Microsoft’s OLE-DB, XML (covered in Chapter 23), and more recently, the Entity Framework developed by Microsoft.
Level 2 requires further features, such as the ability to send and retrieve arrays of parameter values and to retrieve more detailed catalog information.
The SQL standard defines a call level interface (CLI) that is similar to the ODBC interface.
A language in which SQL queries are embedded is referred to as a host language, and the SQL structures permitted in the host language constitute embedded SQL.
Programs written in the host language can use the embedded SQL syntax to access and update data stored in a database.
An embedded SQL program must be processed by a special preprocessor prior to compilation.
The preprocessor replaces embedded SQL requests with host-language declarations and procedure calls that allow runtime execution of the database accesses.
Then, the resulting program is compiled by the host-language compiler.
This is the main distinction between embedded SQL and JDBC or ODBC.
In JDBC, SQL statements are interpreted at runtime (even if they are prepared first using the prepared statement feature)
When embedded SQL is used, some SQL-related errors (including data-type errors) may be caught at compile time.
To identify embedded SQL requests to the preprocessor, we use the EXEC SQL statement; it has the form:
The exact syntax for embedded SQL requests depends on the language in which SQL is embedded.
In some languages, such as Cobol, the semicolon is replaced with END-EXEC.
We place the statement SQL INCLUDE SQLCA in the program to identify the place where the preprocessor should insert the special variables used for communication between the program and the database system.
Before executing any SQL statements, the program must first connect to the database.
Here, server identifies the server to which a connection is to be established.
Variables used as abovemust be declaredwithin a DECLARE section, as illustrated below.
The syntax for declaring the variables, however, follows the usual host language syntax.
Embedded SQL statements are similar in form to regular SQL statements.
There are, however, several important differences, as we note here.
To write a relational query, we use the declare cursor statement.
Rather, the program must use the open and fetch commands (discussed later in this section) to obtain the result tuples.
As we shall see, use of a cursor is analogous to iterating through a result set in JDBC.
Assume that we have a host-language variable credit amount in our program, declared as we saw earlier, and that we wish to find the names of all students who have taken more than credit amount credit hours.
The variable c in the preceding expression is called a cursor for the query.
We then use the open statement, which causes the query to be evaluated.
The open statement for our sample query is as follows:
This statement causes the database system to execute the query and to save the resultswithin a temporary relation.
The query uses the value of the host-language variable (credit amount) at the time the open statement is executed.
If the SQL query results in an error, the database system stores an error diagnostic in the SQL communication-area (SQLCA) variables.
We thenuse a series of fetch statements, each ofwhich causes the values of one tuple to be placed in host-language variables.
The fetch statement requires one host-language variable for each attribute of the result relation.
For our example query, we need one variable to hold the ID value and another to hold the name value.
Suppose that those variables are si and sn, respectively, and have been declared within a DECLARE section.
The program can then manipulate the variables si and sn by using the features of the host programming language.
Wemust use the close statement to tell the database system to delete the temporary relation that held the result of the query.
Embedded SQL expressions for database modification (update, insert, and delete) do not return a result.
If an error condition arises in the execution of the statement, a diagnostic is set in the SQLCA.
For example, if we want to add 100 to the salary attribute of every instructor in theMusic department, we could declare a cursor as follows.
The Java embedding of SQL, called SQLJ, provides the same features as other embedded SQL implementations, but using a different syntax that more closely matches features already present in Java, such as iterators.
For example, SQLJ uses the syntax #sql instead of EXEC SQL, and instead of cursors, uses the Java iterator interface to fetch query results.
Thus the result of executing a query is a Java iterator, and the next() method of the Java iterator interface can be used to step through the result tuples, just as the preceding examples use fetch on the cursor.
The iterator must have attributes declared, whose types match the types of the attributes in the SQL query result.
The translator can connect to the database in order to check the syntactic correctness of queries at compile time, and to ensure that the SQL types of query results are compatible with the Java types of variables they are assigned to.
As of early 2009, SQLJ is not supported by other database systems.
We do not describe SQLJ in detail here; see the bibliographic notes for more information.
We then iterate through the tuples by performing fetch operations on the cursor (as illustrated earlier), and after fetching each tuplewe execute the following code:
Queries in embedded SQL are normally definedwhen the program is written.
There are rare situations where a query needs to be defined at runtime.
For example, an application interface may allow a user to specify selection conditions on one or more attributes of a relation, and may construct thewhere clause of an SQL query at runtime, with conditions on only those attributes for which the user specifies selections.
Wehave already seen several functions that are built into the SQL language.
In this section, we show how developers can write their own functions and procedures, store them in the database, and then invoke them from SQL statements.
Functions are particularly useful with specialized data types such as images and geometric objects.
Procedures and functions allow “business logic” to be stored in the database, and executed from SQL statements.
For example, universities usually have many rules about how many courses a student can take in a given semester, the minimum number of courses a full-time instructor must teach in a year, the maximum number of majors a student can be enrolled in, and so on.
For example, it allowsmultiple applications to access the procedures, and it allows a single point of change in case the business rules change, without changing other parts of the application.
Application code can then call the stored procedures, instead of directly updating database relations.
These can be defined either by the procedural component of SQL, or by an external programming language such as Java, C, or C++
We look at definitions in SQL first, and then see how to use definitions in external languages in Section 5.2.3
Although the syntax we present here is defined by the SQL standard, most databases implement nonstandard versions of this syntax.
For example, the procedural languages supported by Oracle (PL/SQL), Microsoft SQL Server (TransactSQL), and PostgreSQL (PL/pgSQL) all differ from the standard syntaxwepresent.
We illustrate some of the differences, for the case of Oracle, later (page 178)
Although parts of the syntax we present here may not be supported on such systems, the concepts we describe are applicable across implementations, although with a different syntax.
Suppose that we want a function that, given the name of a department, returns the count of the number of instructors in that department.
The function returns a table containing all the instructors of a particular department.
Note that the function’s parameter is referenced by prefixing it with the name of the function (instructor of.dept name)
The function can be used in a query as follows:
In the above simple case it is straightforward towrite this querywithout using table-valued functions.
In general, however, table-valued functions can be thought of as parameterized views that generalize the regular notion of views by allowing parameters.
If you are entering your own functions or procedures, you should write “create or replace” rather than create so that it is easy to modify your code (by replacing the function) during debugging.
The keywords in and out indicate, respectively, parameters that are expected to have values assigned to them and parameters whose values are set in the procedure in order to return results.
Procedures can be invoked either from an SQL procedure or from embedded SQL by the call statement:
Procedures and functions can be invoked from dynamic SQL, as illustrated by the JDBC syntax in Section 5.1.1.4
The name, along with the number of arguments, is used to identify the procedure.
The part of the SQL standard that dealswith these constructs is called the Persistent Storage Module (PSM)
Variables are declared using a declare statement and can have any valid SQL data type.
Local variables can be declared within a compound statement, as we have seen in Section 5.2.1
SQL:1999 supports the while statements and the repeat statements by the following syntax:
There is also a for loop that permits iteration over all results of a query:
The program fetches the query results one row at a time into the for loop variable (r, in the above example)
The statement leave can be used to exit the loop, while iterate starts on the next tuple, from the beginning of the loop, skipping the remaining statements.
The conditional statements supported by SQL include if-then-else statements by using this syntax:
Figure 5.7 Procedure to register a student for a course section.
Figure 5.7 provides a larger example of the use of procedural constructs in SQL.
The function registerStudent defined in the figure, registers a student in a course section, after verifying that the number of students in the section does not exceed the capacity of the room allocated to the section.
The function returns an error code, with a value greater than or equal to 0 signifying success, and a negative value signifying an error condition, and amessage indicating the reason for the failure is returned as an out parameter.
Although the SQL standard defines the syntax for procedures and functions, most databases do not follow the standard strictly, and there is considerable variation in the syntax supported.
One of the reasons for this situation is that these databases typically introduced support for procedures and functions before the syntax was standardized, and they continue to support their original syntax.
It is not possible to list the syntax supported by each database here, but we illustrate a few of the differences in the case of Oracle’s PL/SQL, by showing below a version of the function from Figure 5.5, as it would be defined in PL/SQL.
While the two versions are similar in concept, there are a number of minor syntactic differences, some of which are evident when comparing the two versions of the function.
Although not shown here, the syntax for control flow in PL/SQL also has several differences from the syntax presented here.
Observe that PL/SQL allows a type to be specified as the type of an attribute of a relation, by adding the suffix %type.
On the other hand, PL/SQL does not directly support the ability to return a table, although there is an indirect way of implementing this functionality by creating a table type.
The procedural languages supported by other databases also have a number of syntactic and semantic differences.
The SQL procedural language also supports the signaling of exception conditions, and declaring of handlers that can handle the exception, as in this code:
The statements between thebegin and the end can raise an exception by executing signal out of classroom seats.
The handler says that if the condition arises, the action to be taken is to exit the enclosing begin end statement.
Alternative actionswould be continue, which continues execution from the next statement following the one that raised the exception.
In addition to explicitly defined conditions, there are also predefined conditions such as sqlexception, sqlwarning, and not found.
Although the procedural extensions to SQL can be very useful, they are unfortunately not supported in a standard way across databases.
Even the most basic features have different syntax or semantics in different database products.
As a result, programmers have to essentially learn a new language for each database product.
An alternative that is gaining in support is to define procedures in an imperative programming language, but allow them to be invoked from SQL queries and trigger definitions.
Functions defined in this fashion can be more efficient than functions defined in SQL, and computations that cannot be carriedout in SQL canbe executed by these functions.
External procedures and functions can be specified in this way (note that the exact syntax depends on the specific database system you use):
In general, the external language procedures need to deal with null values in parameters (both in and out) and return values.
They also need to communicate failure/success status, to deal with exceptions.
This information can be communicated by extra parameters: an sqlstate value to indicate failure/success status, a parameter to store the return value of the function, and indicator variables for each parameter/function result to indicate if the value is null.
Other mechanisms are possible to handle null values, for example by passing pointers instead of values.
Functions defined in a programming language and compiled outside the database system may be loaded and executed with the database-system code.
However, doing so carries the risk that a bug in the program can corrupt the database internal structures, and can bypass the access-control functionality of the database system.
Database systems that are concerned more about efficient performance than about securitymay execute procedures in such a fashion.
Database systems that are concerned about security may execute such code as part of a separate process, communicate the parameter values to it, and fetch results back, via interprocess communication.
However, the time overhead of interprocess communication is quite high; on typical CPU architectures, tens to hundreds of thousands of instructions can execute in the time taken for one interprocess communication.
If the code is written in a “safe” language such as Java or C#, there is another possibility: executing the code in a sandboxwithin the database query execution process itself.
The sandbox allows the Java or C# code to access its own memory area, but prevents the code from reading or updating the memory of the query execution process, or accessing files in the file system.
Creating a sandbox is not possible for a language such as C, which allows unrestricted access to memory through pointers.
Several database systems today support external language routines running in a sandbox within the query execution process.
For example, Oracle and IBM DB2 allow Java functions to run as part of the database process.
Microsoft SQL Server allows procedures compiled into the Common Language Runtime (CLR) to execute within the database process; such procedures could have been written, for example, in C# or Visual Basic.
PostgreSQL allows functions defined in several languages, such as Perl, Python, and Tcl.
A trigger is a statement that the system executes automatically as a side effect of a modification to the database.
To design a trigger mechanism, we must meet two requirements:
This is broken up into an event that causes the trigger to be checked and a condition that must be satisfied for trigger execution to proceed.
Specify the actions to be taken when the trigger executes.
Oncewe enter a trigger into thedatabase, the database system takes on the responsibility of executing it whenever the specified event occurs and the corresponding condition is satisfied.
Triggers can be used to implement certain integrity constraints that cannot be specified using the constraint mechanism of SQL.
As an illustration, we could design a trigger that, whenever a tuple is inserted into the takes relation, updates the tuple in the student relation for the student taking the course by adding the number of credits for the course to the student’s total credits.
As another example, suppose a warehouse wishes to maintain a minimum inventory of each item; when the inventory level of an item falls below the minimum level, an order can be placed automatically.
On an update of the inventory level of an item, the trigger compares the current inventory level with the minimum inventory level for the item, and if the level is at or below the minimum, a new order is created.
Instead, we add an order to a relation holding reorders.Wemust create a separate permanently running system process that periodically scans that relation and places orders.
Some database systems provide built-in support for sending email from SQL queries and triggers, using the above approach.
Although the syntax we present here may not be supported on such systems, the concepts we describe are applicable across implementations.
We discuss nonstandard trigger implementations later in this section (page 184)
Figure 5.8 showshow triggers canbeused to ensure referential integrity on the time slot id attribute of the section relation.
The first trigger definition in the figure specifies that the trigger is initiated after any insert on the relation section and it ensures that the time slot id value being inserted is valid.
An SQL insert statement could insert multiple tuples of the relation, and the for each row clause in the trigger code would then explicitly iterate over each inserted row.
The referencing new row as clause creates a variable nrow (called a transition variable) that stores the value of an inserted row after the insertion.
The system executes the rest of the trigger body only for tuples that satisfy the condition.
In our example, though, there is only one statement, which rolls back the transaction that caused the trigger to get executed.
Thus any transaction that violates the referential integrity constraint gets rolled back, ensuring the data in the database satisfies the constraint.
It is not sufficient to check referential integrity on inserts alone,we also need to consider updates of section, as well as deletes and updates to the referenced table time slot.
The second trigger definition in Figure 5.8 considers the case of deletes to time slot.
This trigger checks that the time slot id of the tuple being deleted is either still present in time slot, or that no tuple in section contains that particular time slot id value; otherwise, referential integrity would be violated.
To ensure referential integrity, wewould also have to create triggers to handle updates to section and time slot; we describe next how triggers can be executed on updates, but leave the definition of these triggers as an exercise to the reader.
For updates, the trigger can specify attributeswhose update causes the trigger to execute; updates to other attributes would not cause it to be executed.
For example, to specify that a trigger executes after an update to the grade attribute of the takes relation, we write:
The referencing old row as clause can be used to create a variable storing the old value of an updated or deleted row.
The referencing new row as clause can be used with updates in addition to inserts.
Figure 5.9 shows how a trigger can be used to keep the tot cred attribute value of student tuples up-to-date when the grade attribute is updated for a tuple in the takes relation.
The trigger is executed only when the grade attribute is updated from a value that is either null or ’F’, to a grade that indicates the course is successfully completed.
The update statement is normal SQL syntax except for the use of the variable nrow.
Figure 5.9 Using a trigger to maintain credits earned values.
A more realistic implementation of this example trigger would also handle grade corrections that change a successful completion grade to a fail grade, and handle insertions into the takes relation where the grade indicates successful completion.
As another example of the use of a trigger, the action on delete of a student tuple could be to check if the student has any entries in the takes relation, and if so, to delete them.
Many database systems support a variety of other triggering events, such as when a user (application) logs on to the database (that is, opens a connection), the system shuts down, or changes are made to system settings.
Triggers can be activatedbefore the event (insert, delete, or update) instead of after the event.
Triggers that execute before an event can serve as extra constraints that can prevent invalid updates, inserts, or deletes.
Instead of letting the invalid action proceed and cause an error, the trigger might take action to correct the problem so that the update, insert, or delete becomes valid.
For example, if we attempt to insert an instructor into a department whose name does not appear in the department relation, the trigger could insert a tuple into the department relation for that department name before the insertion generates a foreign-key violation.
As another example, suppose the value of an inserted grade is blank, presumably to indicate the absence of a grade.
We can define a trigger that replaces the value by the null value.
The set statement can be used to carry out such modifications.
An example of such a trigger appears in Figure 5.10
Instead of carrying out an action for each affected row, we can carry out a single action for the entire SQL statement that caused the insert, delete, or update.
To do so, we use the for each statement clause instead of the for each row clause.
The clauses referencing old table as or referencing new table as can then be used to refer to temporary tables (called transition tables) containing all the affected rows.
Transition tables cannot be used with before triggers, but can be.
Figure 5.10 Example of using set to change an inserted value.
A single SQL statement can then be used to carry out multiple actions on the basis of the transition tables.
Although the trigger syntax we describe here is part of the SQL standard, and is supported by IBM DB2, most other database systems have nonstandard syntax for specifying triggers, and may not implement all features in the SQL standard.
We outline a few of the differences below; see the respective systemmanuals for further details.
For example, in the Oracle syntax, unlike the SQL standard syntax, the keyword row does not appear in the referencing statement.
The reference to nrow in the select statement nested in the update statement must begin with a colon (:) to inform the system that the variable nrow is defined externally from the SQL statement.
Further, subqueries are not allowed in thewhen and if clauses.
It is possible to work around this problem by moving complex predicates from the when clause into a separate query that saves the result into a local variable, and then reference that variable in an if clause, and the body of the trigger then moves into the corresponding then clause.
Further, in Oracle, triggers are not allowed to execute a transaction rollback directly; however, they can instead use a function called raise application error to not only roll back the transaction, but also return an error message to the user/application that performed the update.
As another example, in Microsoft SQL Server the keyword on is used instead of after.
The referencing clause is omitted, and old and new rows are referenced by the tuple variables deleted and inserted.
Further, the for each row clause is omitted, and when is replaced by if.
The before specification is not supported, but an instead of specification is supported.
In PostgreSQL, triggers do not have a body, but instead invoke a procedure for each row, which can access variables new and old containing the old and new values of the row.
Instead of performing a rollback, the trigger can raise an exception, with an associated error message.
Triggers can be disabled or enabled; by default they are enabled when they are created, but can be disabled by using alter trigger trigger name disable (some databases use alternative syntax such as disable trigger trigger name)
A trigger that has been disabled can be enabled again.
A trigger can instead be dropped, which removes it permanently, by using the command drop trigger trigger name.
Returning to our warehouse inventory example, supposewe have the following relations:
Note that we have been careful to place an order only when the amount falls from above the minimum level to below the minimum level.
If we check only that the new value after an update is below the minimum level, we may place an order erroneously when the item has already been reordered.We can then use the trigger shown in Figure 5.11 for reordering the item.
SQL-based database systems use triggers widely, although before SQL:1999 they were not part of the SQL standard.
Unfortunately, each database system implemented its own syntax for triggers, leading to incompatibilities.
There are many good uses for triggers, such as those we have just seen in Section 5.3.2, but some uses are best handled by alternative techniques.
For example, we could implement the on delete cascade feature of a foreign-key constraint by using a trigger, instead of using the cascade feature.
Not only would this be more work to implement, but also, it would be much harder for a database user to understand the set of constraints implemented in the database.
As another example, triggers can be used to maintain materialized views.
For instance, if we wished to support very fast access to the total number of students registered for each course section, we could do this by creating a relation.
The value of total students for each course must be maintained up-to-date by triggers on insert, delete, or update of the takes relation.
Such maintenance may require insertion, update ordeletionof tuples from section registration, and triggers must be written accordingly.
However, many database systems now support materialized views, which are automatically maintained by the database system (see Section 4.2.3)
As a result, there is no need to write trigger code for maintaining such materialized views.
Triggers have been used for maintaining copies, or replicas, of databases.
A collection of triggers on insert, delete, or update can be created on each relation to record the changes in relations called change or delta relations.
A separate process copies over the changes to the replica of the database.
Modern database systems, however, provide built-in facilities for database replication, making triggers unnecessary for replication inmost cases.
Another problem with triggers lies in unintended execution of the triggered action when data are loaded from a backup copy,6 or when database updates at a site are replicated on a backup site.
In such cases, the triggered action has already been executed, and typically should not be executed again.
For backup replica systems that may have to take over from the primary system, triggers would have to be disabled initially, and enabledwhen the backup site takes over processing from the primary system.
As an alternative, some database systems allow triggers to be specified as not.
Other database systems provide a system variable that denotes that the database is a replica on which database actions are being replayed; the trigger body should check this variable and exit if it is true.
Both solutions remove the need for explicit disabling and enabling of triggers.
Triggers should be written with great care, since a trigger error detected at runtime causes the failure of the action statement that set off the trigger.
Furthermore, the action of one trigger can set off another trigger.
In the worst case, this could even lead to an infinite chain of triggering.
For example, suppose an insert trigger on a relation has an action that causes another (new) insert on the same relation.
The insert action then triggers yet another insert action, and so on ad infinitum.
Other systems flag as an error any trigger that attempts to reference the relation whose modification caused the trigger to execute in the first place.
Triggers can serve a very useful purpose, but they are best avoided when alternatives exist.
Many trigger applications can be substituted by appropriate use of stored procedures, which we discussed in Section 5.2
Suppose now that we want to find out which courses are a prerequisite whether directly or indirectly, for a specific course—say, CS-347
This instance of prereq differs from that used earlier for reasons that will become apparent as we use it to explain recursive queries.
The transitive closure of the relation prereq is a relation that contains all pairs (cid, pre) such that pre is a direct or indirect prerequisite of cid.
There are numerous applications that require computation of similar transitive closures on hierarchies.
For instance, organizations typically consist of several levels of organizational units.
Machines consist of parts that in turn have subparts, and so on; for example, a bicycle may have subparts such as wheels and pedals, which in turn have subparts such as tires, rims, and spokes.
Transitive closure can be used on such hierarchies to find, for example, all parts in a bicycle.
One way to write the above query is to use iteration: First find those courses that are a direct prerequisite of CS-347, then those courses that are a prerequisite of all the courses under the first set, and so on.
This iterative process continues until we reach an iteration where no courses are added.
Note that SQL allows the creation of temporary tables using the command create temporary table; such tables are available only within the transaction executing the query, and are dropped when the transaction finishes.
Moreover, if two instances of findAllPrereqs run concurrently, each gets its own copy of the temporary tables; if they shared a copy, their result could be incorrect.
The procedure inserts all direct prerequisites of course cid into new c prereq before the repeat loop.
The repeat loop first adds all courses in new c prereq to c prereq.
Next, it computes prerequisites of all those courses in new c prereq, except those that have already been found to be prerequisites of cid, and stores them in the temporary table temp.
Finally, it replaces the contents of new c prereq by the contents of temp.
The repeat loop terminates when it finds no new (indirect) prerequisites.
We note that the use of the except clause in the function ensures that the function works even in the (abnormal) case where there is a cycle of prerequisites.
For example, if a is aprerequisite for b, b is aprerequisite for c, and c is aprerequisite for a, there is a cycle.
While cycles may be unrealistic in course prerequisites, cycles are possible in other applications.
For instance, suppose we have a relation flights(to, from) that says which cities can be reached from which other cities by a direct flight.
All we have to do is to replace prereq by flight and replace attribute names correspondingly.
In this situation, there can be cycles of reachability, but the function would work correctly since it would eliminate cities that have already been seen.
It is rather inconvenient to specify transitive closure using iteration.
There is an alternative approach, using recursive view definitions, that is easier to use.
We can use recursion to define the set of courses that are prerequisites of a particular course, say CS-347, as follows.
The courses that are prerequisites (directly or indirectly) of CS-347 are:
Courses that are prerequisites for those courses that are prerequisites (directly or indirectly) for CS-347
Other examples of transitive closure, such as finding all subparts (direct or indirect) of a given part can also be defined in a similar manner, recursively.
Since the SQL:1999 version, the SQL standard supports a limited form of recursion, using the with recursive clause, where a view (or temporary view) is expressed in terms of itself.
Recursive queries can be used, for example, to express transitive closure concisely.
Recall that thewith clause is used to define a temporary view whose definition is available only to the query in which it is defined.
The additional keyword recursive specifies that the view is recursive.
For example, we can find every pair (cid,pre) such that pre is directly or indirectly a prerequisite for course cid, using the recursive SQL view shown in Figure 5.15
Any recursive view must be defined as the union of two subqueries: a base query that is nonrecursive and a recursive query that uses the recursive view.
In the example in Figure 5.15, the base query is the select on prereq while the recursive query computes the join of prereq and rec prereq.
The meaning of a recursive view is best understood as follows.
First compute the base query and add all the resultant tuples to the recursively defined view relation rec prereq (which is initially empty)
Next compute the recursive query using the current contents of the view relation, and add all the resulting tuples back to the view relation.
Keep repeating the above step until no new tuples are added to the view relation.
The resultant view relation instance is called a fixed point of the recursive view definition.
The term “fixed” refers to the fact that there is no further change.
The view relation is thus defined to contain exactly the tuples in the fixed-point instance.
Applying the above logic to our example, we first find all direct prerequisites of each course by executing the base query.
The recursive query adds one more level of courses in each iteration, until the maximum depth of the course-prereq relationship is reached.
At this point no new tuples are added to the view, and a fixed point is reached.
One way to evaluate the query with the selection is to compute the full contents of rec prereq using the iterative technique, and then select from this result only those tuples whose course id is CS-347
However, this would result in computing (course, prerequisite) pairs for all courses, all of which are irrelevant except for those for the course CS-347
In fact the database system is not required to use the above iterative technique to compute the full result of the recursive query and then perform the selection.
It may get the same result using other techniques that may be more efficient, such as that used in the function findAllPrereqs which we saw earlier.
See the bibliographic notes for references to more information on this topic.
Intuitively, if more tuples are added to the view relation, the recursive query should return at least the same set of tuples as before, and possibly return additional tuples.
In particular, recursive queries should not use any of the following constructs, since they would make the query nonmonotonic:
The meaning of recursive views can be defined by the iterative procedure as long as the recursive query is monotonic; if the recursive query is nonmonotonic, the meaning of the view is hard to define.
Recursive queries are discussed in more detail in the context of the Datalog query language, in Section B.3.6
Some implementations support recursive queries using a different syntax; see the respective system manuals for further details.
The aggregation support in SQL, which we have seen earlier, is quite powerful, and handlesmost common taskswith ease.However, there are some tasks that are hard to implement efficiently with the basic aggregation features.
In this section, we study features that were added to SQL to handle some such tasks.
Finding the position of a value in a larger set is a common operation.
A related type of query is to find the percentile in which a value in a (multi)set belongs, for example, the bottom third, middle third, or top third.
While such queries can be expressed using the SQL constructs we have seen so far, they are difficult to express and inefficient to evaluate.
Programmers may resort to writing the query partly in SQL and partly in a programming language.
We study SQL support for direct expression of these types of queries here.
In our university example, the takes relation shows the grade each student earned in each course taken.
To illustrate ranking, let us assume we have a view student grades (ID, GPA) giving the grade-point average of each student.8
The SQL statement to create the view student grades is somewhat complex since wemust convert the letter grades in the takes relation to numbers and weight the grades for each course by the number of credits for that course.
The definition of this view is the goal of Exercise 4.5
Note that the order of tuples in the output is not defined, so they may not be sorted by rank.
An extra order by clause is needed to get them in sorted order, as shown below.
A basic issue with ranking is how to deal with the case of multiple tuples that are the same on the ordering attribute(s)
In our example, this means deciding what to do if there are two students with the same GPA.
The rank function gives the same rank to all tuples that are equal on the order by attributes.
There is also a dense rank function that does not create gaps in the ordering.
It is possible to express the above query with the basic SQL aggregation functions, using the following query:
It should be clear that the rank of a student is merely 1 plus the number of students with a higher GPA, which is exactly what the above query specifies.
However, this computation of each student’s rank takes time linear in the size of the relation, leading to an overall time quadratic in the size of the relation.
On large relations, the above query could take a very long time to execute.
In contrast, the system’s implementation of the rank clause can sort the relation and compute the rank in much less time.
For instance, suppose we wish to rank students by department rather than across the entire university.
Assume that a view is defined like student grades but including the department name: dept grades(ID, dept name, GPA)
The following query then gives the rank of students within each section:
The outer order by clause orders the result tuples by department name, and within each department by the rank.
Multiple rank expressions can be used within a single select statement; thus we can obtain the overall rank and the rank within the department by using two rank expressions in the same select clause.
When ranking (possibly with partitioning) occurs along with a group by clause, the group by clause is applied first, and partitioning and ranking are done on the results of the group by.
Thus aggregate values can then be used for ranking.We could havewritten our ranking over the student grades view without using the view, using a single select clause.
The ranking functions can be used to find the top n tuples by embedding a ranking query within an outer-level query; we leave details as an exercise.
Note that the bottom n is simply the same as the top n with a reverse sorting order.
Several database systems provide nonstandard SQL extensions to specify directly that only the top n results are required; such extensions do not require the rank function and simplify the job of the optimizer.
For example, some databases allow a clause limit n to be added at the end of an SQL query to specify that only the first n tuples should be output; this clause is used in conjunction with an order by clause to fetch the top n tuples, as illustrated by the following query, which retrieves the IDs and GPAs of the top 10 students in order of GPA:
However, the limit clause does not support partitioning, so we cannot get the top nwithin each partition without performing ranking; further, if more than one student gets the same GPA, it is possible that one is included in the top 10, while another is excluded.
This function is particularly useful for constructing histograms based on percentiles.
We can show the quartile into which each student falls based on GPA by the following query:
The presence of null values can complicate the definition of rank, since it is not clear where they should occur first in the sort order.
Window queries compute an aggregate function over ranges of tuples.
This is useful, for example, to compute an aggregate of a fixed range of time; the time range is called a window.
Windows may overlap, in which case a tuple may contribute to more than one window.
This is unlike the partitions we saw earlier, where a tuple could contribute to only one partition.
An example of the use of windowing is trend analysis.
Sales may fluctuate widely from day to day based on factors like weather (for example a snowstorm, flood, hurricane, or earthquake might reduce sales for a period of time)
However, over a sufficiently long period of time, fluctuations might be less (continuing the example, sales may “make up” for weather-related downturns)
Stock market trend analysis is another example of the use of the windowing concept.
Various “moving averages” are found on business and investment Web sites.
It is relatively easy towrite an SQL query using those featureswe have already studied to compute an aggregate over onewindow, for example, sales over a fixed 3-day period.
However, if we want to do this for every 3-day period, the query becomes cumbersome.
Suppose we are given a view tot credits (year, num credits) giving the total number of credits taken.
This query computes averages over the 3 preceding tuples in the specified sort order.
The averages each year would be computed in a similar manner.
For the earliest year in the relation tot credits, the average would be over only that year itself, while for the next year, the average would be over two years.
Note that if the relation tot credits has more than one tuple for a specific year, there may be multiple possible orderings of tuples, that are sorted by year.
In this case, the definition of preceding tuples is based on the implementation dependent sort order, and is not uniquely defined.
Suppose that instead of going back a fixed number of tuples, we want the window to consist of all prior years.
That means the number of prior years considered is not fixed.
To get the average total credits over all prior years we write:
It is possible to use the keyword following in place of preceding.
If we did this in our example the year value specifies the beginning of the window instead of the end.
Similarly, we can specify a window beginning before the current tuple and ending after it:
Instead of a specific count of tuples, we can specify a range based on the value of the order by attribute.
To specify a range going back 4 years and including the current year, we write:
We leave the definition of this view in terms of our university example as an exercise.
Be sure to note the use of the keyword range in the above example.
In our example, all tuples pertain to the entire university.
Suppose instead, we have credit data for each department in a view tot credits dept (dept name, year, num credits) giving the total number of credits students took with the particular department in the specified year.
Again, we leave writing this view definition as an exercise.
We can write windowing queries that treat each department separately by partitioning by dept name:
An online analytical processing (OLAP) system is an interactive system that permits an analyst to view different summaries of multidimensional data.
The word online indicates that an analyst must be able to request new summaries and get responses online, within a few seconds, and should not be forced to wait for a long time to see the result of a query.
There are many OLAP products available, including some that ship with database products such as Microsoft SQL Server, and Oracle, and other standalone tools.
The initial versions of many OLAP tools assumed that data is memory resident.
Data analysis on small amounts of data can in fact be performed using spreadsheet applications, such as Excel.
However, OLAP on very large amounts of data requires that data be resident in a database, and requires support from the database for efficient preprocessing of data as well as for online query processing.
In this section, we study extensions of SQL to support such tasks.
Consider an application where a shop wants to find out what kinds of clothes are popular.
Let us suppose that clothes are characterized by their item name, color, and size, and that we have a relation sales with the schema.
Given a relation used for data analysis, we can identify some of its attributes as measure attributes, since they measure some value, and can be aggregated upon.
For instance, the attribute quantity of the sales relation is a measure attribute, since it measures the number of units sold.
Some (or all) of the other attributes of the relation are identified as dimension attributes, since they define the dimensions on which measure attributes, and summaries of measure attributes, are viewed.
In the sales relation, item name, color, and clothes size are dimension attributes.
A more realistic version of the sales relation would have additional dimensions, such as time and sales location, and additional measures such as monetary value of the sale.
Data that can be modeled as dimension attributes and measure attributes are called multidimensional data.
To analyze the multidimensional data, a manager may want to see data laid out as shown in the table in Figure 5.17
The table shows total quantities for different combinations of item name and color.
The table in Figure 5.17 is an example of a cross-tabulation (or cross-tab, for short), also referred to as a pivot-table.
In general, a cross-tab is a table derived from a relation (say R), where values for one attribute of relation R (say A) form the row headers and values for another attribute of relation R (say B) form the columnheader.
For example, in Figure 5.17, the attribute item name corresponds to A (with values “dark”, “pastel”, and “white”), and the attribute color corresponds to to B (with attributes “skirt”, “dress”, “shirt”, and “pants”)
Figure 5.17 Cross tabulation of sales by item name and color.
In our example, the aggregation used is the sum of the values for attribute quantity, across all values for clothes size, as indicated by “clothes size: all” above the cross-tab in Figure 5.17
Inour example, the cross-tab alsohas an extra columnandanextra rowstoring the totals of the cells in the row/column.
The generalization of a cross-tab, which is two-dimensional, to n dimensions can be visualized as an n-dimensional cube, called the data cube.
Figure 5.18 shows a data cube on the sales relation.
The data cube has three dimensions, item name, color, and clothes size, and the measure attribute is quantity.
Each cell is identified by values for these three dimensions.
Each cell in the data cube contains a value, just as in a cross-tab.
In Figure 5.18, the value contained in a cell is shown on one of the faces of the cell; other faces of the cell are shown blank if they are visible.
All cells contain values, even if they are not visible.
The value for a dimension may be all, in which case the cell contains a summary over all values of that dimension, as in the case of cross-tabs.
Grouping on the set of all n dimensions is useful only if the table may have duplicates.
With an OLAP system, a data analyst can look at different cross-tabs on the same data by interactively selecting the attributes in the cross-tab.
Each cross-tab is a two-dimensional view on a multidimensional data cube.
For instance, the analyst may select a cross-tab on item name and clothes size or a cross-tab on color and clothes size.
The operation of changing the dimensions used in a cross-tab is called pivoting.
Such an operation is referred to as slicing, since it can be thought of as viewing a slice of the data cube.
The operation is sometimes called dicing, particularly when values for multiple dimensions are fixed.
When a cross-tab is used to view a multidimensional cube, the values of dimension attributes that are not part of the cross-tab are shown above the crosstab.
The value of such an attribute can be all, as shown in Figure 5.17, indicating that data in the cross-tab are a summary over all values for the attribute.
Slicing/dicing simply consists of selecting specific values for these attributes, which are then displayed on top of the cross-tab.
The operation of moving from finer-granularity data to a coarser granularity (by means of aggregation) is called a rollup.
In our example, starting from the data cube on the sales table, we got our example cross-tab by rolling up on the attribute clothes size.
Clearly, finer-granularity data cannot be generated from coarse-granularity data; they must be generated either from the original data, or from even finer-granularity summary data.
Analysts may wish to view a dimension at different levels of detail.
For instance, an attribute of type datetime contains a date and a time of day.
Using time precise to a second (or less) may not be meaningful: An analyst who is interested in rough time of day may look at only the hour value.
An analyst who is interested in sales by day of the week may map the date to a day of the week and look only at that.
Another analyst may be interested in aggregates over a month, or a quarter, or for an entire year.
An analystmay be interested in viewing sales of clothes divided asmenswear and womenswear, and not interested in individual values.
After viewing the aggregates at the level of womenswear and menswear, an analyst may drill down the hierarchy to look at individual values.
An analyst looking at the detailed level may drill up the hierarchy and look at coarser-level aggregates.
Both levels can be displayed on the same cross-tab, as in Figure 5.20
A cross-tab is different from relational tables usually stored in databases, since the number of columns in the cross-tab depends on the actual data.
A change in the data values may result in adding more columns, which is not desirable for data storage.
However, a cross-tab view is desirable for display to users.
It is straightforward to represent a cross-tab without summary values in a relational form with a fixed number of columns.
A cross-tab with summary rows/columns can be represented by introducing a special value all to represent subtotals, as in Figure 5.21
The SQL standard actually uses the null value in place of all, but to avoid confusion with regular null values, we shall continue to use all.
Figure 5.20 Cross tabulation of sales with hierarchy on item name.
The value all can be thought of as representing the set of all values for an attribute.
Tuples with the value all for the color and clothes size dimensions can be obtained by an aggregation on the sales relation with a group by on the column item name.
Similarly, a group by on color, clothes size can be used to get the tuples with the value all for item name, and a group by with no attributes (which can simply be omitted in SQL) can be used to get the tuple with value all for item name, color, and clothes size.
For example, the fact that skirts and dresses fall under the womenswear category, and the pants and shirts under the menswear category can be represented by a relation itemcategory(item name, category)
As another example, a hierarchy on city can be represented by a single relation city hierarchy (ID, city, state, country, region), or by multiple relations, each mapping values in one level of the hierarchy to values at the next level.
We assume here that cities have unique identifiers, stored in the attribute ID, to avoid confusing between two cities with the same name, e.g., the Springfield in Missouri and the Springfield in Illinois.
The earliest OLAP systems used multidimensional arrays in memory to store data cubes, and are referred to as multidimensional OLAP (MOLAP) systems.
Later, OLAP facilities were integrated into relational systems, with data stored in a relational database.
Such systems are referred to as relational OLAP (ROLAP) systems.
Hybrid systems, which store some summaries in memory and store the base data and other summaries in a relational database, are called hybrid OLAP (HOLAP) systems.
The server contains the relational database aswell as anyMOLAPdata cubes.
Client systems obtain views of the data by communicating with the server.
Early OLAP implementations precomputed and stored entire data cubes, that is, groupings on all subsets of the dimension attributes.
Precomputation allows OLAP queries to be answered within a few seconds, even on datasets that may contain millions of tuples adding up to gigabytes of data.
However, there are 2n groupings with n dimension attributes; hierarchies on attributes increase the number further.
As a result, the entire data cube is often larger than the original relation that formed the data cube and in many cases it is not feasible to store the entire data cube.
Instead of precomputing and storing all possible groupings, it makes sense to precompute and store some of the groupings, and to compute others ondemand.
Instead of computing queries from the original relation, which may take a very long time, we can compute them from other precomputed queries.
For instance, suppose that a query requires grouping by (item name, color), and this has not been precomputed.
The query result can be computed from summaries by (item name, color, clothes size), if that has been precomputed.
See the bibliographical notes for references on how to select a good set of groupings for precomputation, given limits on the storage available for precomputed results.
Several SQL implementations, such as Microsoft SQL Server, and Oracle, support a pivot clause in SQL, which allows creation of cross-tabs.
Note that the for clause within the pivot clause specifies what values from the attribute color should appears as attribute names in the pivot result.
The attribute color itself is eliminated from the result, although all other attributes are retained, except that the values for the newly created attributes are specified to come from the attribute quantity.
In case more than one tuple contributes values to a given cell, the aggregate operation within the pivot clause specifies how the values should be combined.
In the above example, the quantity values are summed up.
Note that the pivot clause by itself does not compute the subtotals we saw in the pivot table from Figure 5.17
However, we can first generate the relational representation shown in Figure 5.21, as outlined shortly, and then apply the pivot clause on that representation to get an equivalent result.
In this case, the value all must also be listed in the for clause, and the order by clause needs to be modified to order all at the end.
The data in a data cube cannot be generated by a single SQL query, using the basic group by constructs, since aggregates are computed for several different groupings of the dimension attributes.
For this reason, SQL includes functions to form the grouping needed for OLAP.
The cube and rollup constructs in the group by clause allow multiple group by queries to be run in a single query with the result returned as a single relation in a style similar to that of the relation of Figure 5.21
We can find the number of items sold in each item name by writing a simple group by query:
The result of this query is shown in Figure 5.23
Similarly, we can find the number of items sold in each color, etc.
By using multiple attributes in the group by clause, we can find how many items were sold with a certain set of properties.
For example, we can find a breakdown of sales by item-name and color by writing:
The result of this query is shown in Figure 5.24
If, however, we want to generate the entire data cube using this approach, we would have to write a separate query for each of the following sets of attributes:
The cube construct allows us to accomplish this in one query:
So that the result of this query is indeed a relation, tuples in the result contain null as the value of those attributes not present in a particular grouping.
For example, tuples produced by grouping on clothes size have a schema (clothes size, sum(quantity))
They are converted to tuples on (item name, color, clothes size, sum(quantity)) by inserting null for item name and color.
The relation of Figure 5.21 is generated using grouping by item name and color.
It also uses all in place of null so as to be more readable to the average user.
To generate that relation in SQL, we arrange to substitute all for null.
The decode function allows substitution of values in an attribute of a tuple.
It compares value against thematch values and if a match is found, it replaces the attribute valuewith the corresponding replacement value.
If nomatch succeeds, then the attribute value is replaced with the default replacement value.
The decode function does not work as we might like for null values because, as we saw in Section 3.6, predicates on nulls evaluate to unknown, which ultimately becomes false.
Then the relation in Figure 5.21, with occurrences of all replaced by null, can be computed by the query:
The substitution of all is achieved using the SQL decode and grouping functions.
The decode function is conceptually simple but its syntax is somewhat hard to read.
The rollup construct is the same as the cube construct except that rollup generates fewer group by queries.
We saw that group by cube (item name, color, clothes size) generated all 8 ways of forming a group by query using some (or all or none) of the attributes.
Notice that the order of the attributes in the rollupmakes adifference; the final attribute (clothes size, in our example) appears in only one grouping, the penultimate (second last) attribute in 2 groupings, and so on, with the first attribute appearing in all groups but one (the empty grouping)
Why might we want the specific groupings that are used in rollup? These groups are of frequent practical interest for hierarchies (as in Figure 5.19, for example)
For the location hierarchy (Region, Country, State, City), we may want to group byRegion to get sales by region.
Thenwemaywant to “drill down” to the level of countries within each region, which means we would group by Region, Country.
The rollup construct allows us to specify this sequence of drilling down for further detail.
Multiple rollups and cubes can be used in a single group by clause.
The ODBC and JDBC standards define application program interfaces to access SQL databases from C and Java language programs.
Functions and procedures can be defined using SQLprocedural extensions that allow iteration and conditional (if-then-else) statements.
Triggers define actions to be executed automatically when certain events occur and corresponding conditions are satisfied.
Triggers have many uses, such as implementing business rules, audit logging, and even carrying out actions outside the database system.
Some queries, such as transitive closure, can be expressed either by using iteration or by using recursive SQL queries.
Recursion can be expressed using either recursive views or recursivewith clause definitions.
Online analytical processing (OLAP) tools help analysts view data summarized in different ways, so that they can gain insight into the functioning of an organization.
The data cube consists of multidimensional data summarized in different ways.
Precomputing the data cube helps speed up queries on summaries of data.
Cross-tab displays permit users to view two dimensions of multidimensional data at a time, along with summaries of the data.
Drill down, rollup, slicing, and dicing are among the operations that users perform with OLAP tools.
SQL, starting with the SQL:1999 standard, provides a variety of operators for data analysis, including cube and rollup operations.
Some systems support a pivot clause, which allows easy creation of cross-tabs.
Note that the ones we wrote in Figure 5.8 do not cover the update operation.
Modify the trigger on updates of takes, to handle all updates that can affect the value of tot cred.
Write a trigger to handle inserts to the takes relation.
Under what assumptions is it reasonable not to create triggers on the course relation?
Suppose that the view is materialized; that is, the view is computed and stored.
Write triggers to maintain the view, that is, to keep it up-to-date on insertions to and deletions from depositor or account.
Write an SQL trigger to carry out the following action: On delete of an account, for each owner of the account, check if the owner has any remaining accounts, and if she does not, delete her from the depositor relation.
Write an SQL query to compute the cube operation on the relation, giving the relation in Figure 5.21
That is, produce an English sentence like “It finds the manager of the toy department,” not a line-by-line description of what each Java statement does.
What do you need to know about relation r to be able to print the result in the specified tabular format.
Write a query to find companies whose employees earn a higher salary, on average, than the average salary at “First Bank Corporation”
Under what circumstances would you use each of these features?
Write a recursive SQL query that outputs the names of all subparts of the part with part-id “P-100”
Write a JDBC function using non-recursive SQL to find the total cost of part “P-100”, including the costs of all its subparts.
Be sure to take into account the fact that a part may have multiple occurrences of a subpart.
Describe how the trigger mechanism can be used to implement the on delete cascade option, when a tuple is deleted from s.
Most database systems place a limit on how deep the nesting can be.
Write an SQL query to compute a histogram of balance values, dividing the range 0 to the maximum account balance present, into three equal ranges.
Most database vendors provide OLAP tools as part of their database systems, or as add-on applications.
Tools may be integrated with a larger “business intelligence” product such as IBM Cognos.
Many companies also provide analysis tools for specific applications, such as customer relationship management (for example, Oracle Siebel CRM)
See the bibliographic notes of Chapter 3 for references to SQL standards and books on SQL.
References to books on Java (including JDBC) are also available at this URL.
Melton and Eisenberg [2000] provides a guide to SQLJ, JDBC, and related technologies.
In the context of functions and procedures in SQL, many database products support features beyond those specified in the standards, and do not support many of the features of the standard.
More information on these features may be found in the SQL user manuals of the respective products.
The original SQL proposals for assertions and triggers are discussed in Astrahan et al.
Recursive query processingwas first studied in detail in the context of a query language called Datalog, which was based on mathematical logic and followed the syntax of the logic programming language Prolog.
Ramakrishnan andUllman [1995] provides a survey of results in this area, including techniques to optimize queries that select a subset of tuples from a recursively defined view.
Efficient algorithms for computing data cubes are described by Agarwal et al.
There has been a substantial amount of research on the efficient processing of “top-k” queries that return only the top-k-ranked results.
A survey of that work appears in Ilyas et al.
In this chapter we present the formal model upon which SQL as well as other relational query languages are based.
We cover three formal languages.We start bypresenting the relational algebra, which forms the basis of the widely used SQL query language.
We then cover the tuple relational calculus and the domain relational calculus, which are declarative query languages based on mathematical logic.
It consists of a set of operations that take one or two relations as input and produce a new relation as their result.
The fundamental operations in the relational algebra are select, project, union, set difference, Cartesian product, and rename.
In addition to the fundamental operations, there are several other operations—namely, set intersection, natural join, and assignment.
We shall define these operations in terms of the fundamental operations.
The select, project, and rename operations are called unary operations, because they operate on one relation.
The other three operations operate on pairs of relations and are, therefore, called binary operations.
We can find all instructors with salary greater than $90,000 by writing:
To find all departments whose name is the same as their building name, we can write:
The term select in relational algebra has a different meaning than the one used in SQL, which is an unfortunate historical fact.
In relational algebra, the term select corresponds to what we refer to in SQL as where.
We emphasize the different interpretations here to minimize potential confusion.
Figure 6.3 shows the relation that results from this query.
The fact that the result of a relational operation is itself a relation is important.
Consider the more complicated query “Find the name of all instructors in the Physics department.”We write:
Notice that, instead of giving the name of a relation as the argument of the projection operation, we give an expression that evaluates to a relation.
The information is contained in the section relation (Figure 6.4)
To find the set of all courses taught in the Fall 2009 semester, we write:
To find the set of all courses taught in the Spring 2010 semester, we write:
To answer the query, we need the union of these two sets; that is, we need all section IDs that appear in either or both of the two relations.
The result relation for this query appears in Figure 6.5
Since relations are sets, duplicate values such as CS-101, which is offered in both semesters, are replaced by a single occurrence.
The relations r and s must be of the same arity.
That is, they must have the same number of attributes.
The domains of the ith attribute of r and the ith attribute of s must be the same, for all i.
Note that r and s can be either database relations or temporary relations that are the result of relational-algebra expressions.
The result relation for this query appears in Figure 6.6
As with the union operation, we must ensure that set differences are taken.
For those attributes that appear in only one of the two schemas, we shall usually drop the relation-name prefix.
We can then write the relation schema for r as:
This naming convention requires that the relations that are the arguments of the Cartesian-product operation have distinct names.
This requirement causes problems in some cases, such as when the Cartesian product of a relation with itself is desired.A similar problemarises ifwe use the result of a relational-algebra expression in a Cartesian product, since we shall need a name for the relation so.
In Section 6.1.1.7, we see how to avoid these problems by using the rename operation.
Suppose that we want to find the names of all instructors in the Physics department together with the course id of all courses they taught.
We need the information in both the instructor relation and the teaches relation to do so.
Wehave a relation that pertains only to instructors in the Physics department.
However, the course id column may contain information about courses that were not taught by the corresponding instructor.
If you do not see why that is true, recall that the Cartesian product takes all possible pairings of one tuple from instructor with one tuple of teaches.
Since the Cartesian-product operation associates every tuple of instructorwith every tuple of teaches, we know that if a an instructor is in the Physics department, and has taught a course (as recorded in the teaches relation), then there is some.
Finally, since we only want the names of all instructors in the Physics department together with the course id of all courses they taught, we do a projection:
The result of this expression, shown in Figure 6.10, is the correct answer to our query.
Observe that although instructor Gold is in the Physics department, he does not teach any course (as recorded in the teaches relation), and therefore does not appear in the result.
Note that there is often more than one way to write a query in relational algebra.
Note the subtle difference between the two queries: in the query above, the selection that restricts dept name to Physics is applied to instructor, and the Cartesian product is applied subsequently; in contrast, the Cartesian product was applied before the selection in the earlier query.
However, the two queries are equivalent; that is, they give the same result on any database.
A relation r by itself is considered a (trivial) relational-algebra expression.
Thus, we can also apply the rename operation to a relation r to get the same relation under a new name.
A second form of the rename operation is as follows: Assume that a relationalalgebra expression E has arity n.
We can now write the temporary relation that consists of the salaries that are not the largest:
This expression gives those salaries in the instructor relation for which a larger salary appears somewhere in the instructor relation (renamed as d)
Step 2: The query to find the largest salary in the university can be written as:
The positional notation also applies to results of relationalalgebra operations.
The operations in Section 6.1.1 allow us to give a complete definition of an expression in the relational algebra.
A basic expression in the relational algebra consists of either one of the following:
A general expression in the relational algebra is constructed out of smaller subexpressions.
The fundamental operations of the relational algebra are sufficient to express any relational-algebra query.However, if we restrict ourselves to just the fundamental operations, certain common queries are lengthy to express.
Therefore, we define additional operations that do not add any power to the algebra, but simplify.
For each new operation, we give an equivalent expression that uses only the fundamental operations.
The result relation for this query appears in Figure 6.13
Note that we can rewrite any relational-algebra expression that uses set intersection by replacing the intersection operation with a pair of set-difference operations as:
It is often desirable to simplify certain queries that require a Cartesian product.
Usually, a query that involves a Cartesian product includes a selection operation on the result of the Cartesian product.
The selection operation most often requires that all attributes that are common to the relations that are involved in the Cartesian product be equated.
In our example query from Section 6.1.1.6 that combined information from the instructor and teaches tables, the matching condition required instructor.ID to be equal to teaches.ID.
These are the only attributes in the two relations that have the same name.
Figure 6.14 The natural join of the instructor relation with the teaches relation.
Notice that we do not repeat those attributes that appear in the schemas of both relations; rather they appear only once.
Notice also the order in which the attributes are listed: first the attributes common to the schemas of both relations, second those attributes unique to the schema of the first relation, and finally, those attributes unique to the schema of the second relation.
Although the definition of natural join is complicated, the operation is easy to apply.
As an illustration, consider again the example “Find the names of all instructors together with the course id of all courses they taught.”We express this query by using the natural join as follows:
Since the schemas for instructor and teaches have the attribute ID in common, the natural-join operation considers only pairs of tuples that have the same value on ID.
It combines each such pair of tuples into a single tuple on the union of the two schemas; that is, (ID, name, dept name, salary, course id)
After performing the projection, we obtain the relation in Figure 6.15
Note that the union, intersection, and difference operations here are on sets of attributes, rather than on relations.
We did not specifywhich expressionwe intended, because the two are equivalent.
With the assignment operation, a query canbewritten as a sequential program consisting of a series of assignments followed by an expression whose value is displayed as the result of the query.
Note that the assignment operation does not provide any additional power to the algebra.
It is, however, a convenient way to express complex queries.
The outer-join operation is an extension of the join operation to deal withmissing information.
Suppose that there is some instructor who teaches no courses.
For example, instructors Califieri, Gold, and Singh do not appear in the result of the natural join, since they do not teach any course.
More generally, some tuples in either or both of the relations being joinedmay be “lost” in this way.
The outer join operation works in a manner similar to the natural join operation we have already studied, but preserves those tuples that would be lost in an join by creating tuples in the result containing null values.
Since outer-join operations may generate results containing null values, we need to specify how the different relational-algebra operations deal with null values.
Section 3.6 dealt with this issue in the context of SQL.
The same concepts apply for the case of relational algebra, and we omit details.
We now describe relational-algebra operations that provide the ability to write queries that cannot be expressed using the basic relational-algebra operations.
The second extended relational-algebra operation is the aggregate operation G, which permits the use of aggregate functions such as min or average, on sets of values.
Aggregate functions take a collection of values and return a single value as a result.
For example, the aggregate function sum takes a collection of values and returns the sum of the values.
The aggregate function avg returns the average of the values.
The aggregate function count returns the number of the elements in the collection, and returns 6 on the preceding collection.
The collections on which aggregate functions operate can have multiple occurrences of a value; the order in which the values appear is not relevant.
Sets are a special case of multisets where there is only one copy of each element.
To illustrate the concept of aggregation, we shall use the instructor relation.
Suppose that we want to find out the sum of salaries of all instructors; the relational-algebra expression for this query is:
The symbol G is the letter G in calligraphic font; read it as “calligraphic G.” The relational-algebra operation G signifies that aggregation is to be applied, and its subscript specifies the aggregate operation to be applied.
The result of the expression above is a relationwith a single attribute, containing a single rowwith a numerical value corresponding to the sum of the salaries of all instructors.
There are cases where we must eliminate multiple occurrences of a value before computing an aggregate function.
If we do want to eliminate duplicates, we use the same function names as before, with the addition of the hyphenated string “distinct” appended to the end of the function name (for example, countdistinct)
An example arises in the query “Find the total number of instructors who teach a course in the Spring 2010 semester.” In this case, an instructor counts only once, regardless of the number of course sections that the instructor teaches.
The required information is contained in the relation teaches, and we write this query as follows:
The aggregate function count-distinct ensures that even if an instructor teaches more than one course, she is counted only once in the result.
There are circumstances where wewould like to apply the aggregate function not to a single set of tuples, but instead to a group of sets of tuples.
As an illustration, consider the query “Find the average salary in each department.”We write this query as follows:
Figure 6.19 shows the tuples in the instructor relation grouped by the dept name attribute.
This is the first step in computing the query result.
The specified aggregate is computed for each group, and the result of the query is shown in Figure 6.20
Figure 6.19 Tuples of the instructor relation, grouped by the dept name attribute.
In contrast, consider the query “Find the average salary of all instructors.”We write this query as follows:
In this case the attribute dept name has been omitted from the left side of the G operator, so the entire relation is treated as a single group.
The general form of the aggregation operation G is as follows:
Gn constitute a list of attributes on which to group; each Fi is an aggregate function; and each Ai is an attribute name.
The meaning of the operation is as follows: The tuples in the result of expression E are partitioned into groups in such a way that:
Figure 6.20 The result relation for the query “Find the average salary in each department”
Unlike the relational algebra, SQL allows multiple copies of a tuple in an input relation as well as in a query result.
The SQL standard defines howmany copies of each tuple are there in the output of a query, which depends in turn on how many copies of tuples are present in the input relations.
To model this behavior of SQL, a version of relational algebra, called the multiset relational algebra, is defined to work on multisets, that is, sets that may contain duplicates.
The basic operations in the multiset relational algebra are defined as follows:
Multiset union, intersection and set difference can also be defined in a similar way, following the corresponding definitions in SQL, which we saw in Section 3.5
There is no change in the definition of the aggregation operation.
Gn can be empty, in which case there is a single group containing all tuples in the relation.
From a comparison of the relational algebra operations and the SQL operations, it should be clear that there is a close connection between the two.
Each Ai represents an attribute, and each ri a relation.
If the where clause is omitted, the predicate P is true.
More complex SQL queries can also be rewritten in relational algebra.
Join expressions in the from clause can be written using equivalent join expressions in relational algebra; we leave the details as an exercise for the reader.
However, subqueries in the where or select clause cannot be rewritten into relational algebra in such a straightforward manner, since there is no relationalalgebra operation equivalent to the subquery construct.
Extensions of relational algebra have been proposed for this task, but are beyond the scope of this book.
When we write a relational-algebra expression, we provide a sequence of procedures that generates the answer to our query.
The tuple relational calculus, by contrast, is a nonprocedural query language.
It describes the desired information without giving a specific procedure for obtaining that information.
A query in the tuple relational calculus is expressed as:
Before we give a formal definition of the tuple relational calculus, we return to some of the queries for which we wrote relational-algebra expressions in Section 6.1.1
Find the ID, name, dept name, salary for instructors whose salary is greater than $80,000:
Suppose that we want only the ID attribute, rather than all attributes of the instructor relation.
To write this query in the tuple relational calculus, we need to write an expression for a relation on the schema (ID)
To express this request, we need the construct “there exists” from mathematical logic.
In English, we read the preceding expression as “The set of all tuples t such that there exists a tuple s in relation instructor for which the values of t and s for the ID attribute are equal, and the value of s for the salary attribute is greater than $80,000.”
Tuple variable t is defined on only the ID attribute, since that is the only attribute having a condition specified for t.
Figure 6.21 Names of all instructors whose department is in the Watson building.
Tuple variable u is restricted to departments that are located in the Watson building, while tuple variable s is restricted to instructors whose dept name matches that of tuple variable u.
This expression gives us the set of all course id tuples for which at least one of the following holds:
The result of this query appeared earlier in Figure 6.5
In English, we interpret this expression as “The set of all students (that is, (ID) tuples t) such that, for all tuples u in the course relation, if the value of u on attribute dept name is ’Biology’, then there exists a tuple in the takes relation that includes the student ID and the course id.”
Note that there is a subtlety in the above query: If there is no course offered in the Biology department, all student IDs satisfy the condition.
The first line of the query expression is critical in this case—without the condition.
Tuple variable s is said to be a bound variable.
We build up formulae from atoms by using the following rules:
As we could for the relational algebra, we can write equivalent expressions that are not identical in appearance.
In the tuple relational calculus, these equivalences include the following three rules:
There are infinitely many tuples that are not in instructor.
Most of these tuples contain values that do not even appear in the database! Clearly, we do not wish to allow such expressions.
A second form of relational calculus, called domain relational calculus, uses domain variables that take on values from an attributes domain, rather than values for an entire tuple.
The domain relational calculus, however, is closely related to the tuple relational calculus.
Domain relational calculus serves as the theoretical basis of the widely used QBE language (see Appendix B.1), just as relational algebra serves as the basis for the SQL language.
An expression in the domain relational calculus is of the form.
An atom in the domain relational calculus has one of the following forms:
We build up formulae from atoms by using the following rules:
Find the instructor ID, name, dept name, and salary for instructors whose salary is greater than $80,000:
Find all instructor ID for instructors whose salary is greater than $80,000:
We now give several examples of queries in the domain relational calculus.
Find the names of all instructors in the Physics department together with the course id of all courses they teach:
Find all students who have taken all courses offered in the Biology department:
We noted that, in the tuple relational calculus (Section 6.2), it is possible to write expressions that may generate an infinite relation.
For the domain relational calculus, wemust be concerned also about the form of formulae within “there exists” and “for all” clauses.
In the tuple relational calculus, we restricted any existentially quantified variable to range over a specific relation.
Sincewedidnot do so in thedomain calculus, we add rules to the definition of safety to deal with cases like our example.
All values that appear in tuples of the expression are values from dom(P)
In general, there would be infinitely many values to test.
However, if the expression is safe, we know that we can restrict our attention to values from dom(P1)
This restriction reduces to a finite number the tuples we must consider.
When the domain relational calculus is restricted to safe expressions, it is equivalent in expressive power to the tuple relational calculus restricted to safe expressions.
Since we noted earlier that the restricted tuple relational calculus is equivalent to the relational algebra, all three of the following are equivalent:
We note that the domain relational calculus also does not have any equivalent of the aggregate operation, but it can be extended to support aggregation, and extending it to handle arithmetic expressions is straightforward.
The relational algebra defines a set of algebraic operations that operate on tables, and output tables as their results.
These operations can be combined to get expressions that express desired queries.
The algebra defines the basic operations used within relational query languages.
Extended operations, some of which add further expressive power to relational algebra.
The relational algebra is a terse, formal language that is inappropriate for casual users of a database system.
The tuple relational calculus and the domain relational calculus are nonprocedural languages that represent the basic power required in a relational query language.
The basic relational algebra is a procedural language that is equivalent in power to both forms of the relational calculus when they are restricted to safe expressions.
The relational calculi are terse, formal languages that are inappropriate for casual users of a database system.
These two formal languages form the basis for two more user-friendly languages, QBE and Datalog, that we cover in Appendix B.
Find the IDs of all students who were taught by an instructor named Einstein; make sure there are no duplicates in the result.
Find all instructors earning the highest salary (there may be more than one with the same salary)
Give an expression in the relational algebra to express each of the following queries:
Find the names of all employees who live in the same city and on the same street as do their managers.
Find the names of all employees in this database who do not work for “First Bank Corporation”
Find the names of all employeeswho earnmore than every employee of “Small Bank Corporation”
Describe how the theta-join operation can be extended so that tuples from the left, right, or both relations are not lost from the result of a theta join.
Write a relational algebra expression using the division operator to find the IDs of all students who have taken all Comp.
Hint: project takes to just ID and course id, and generate the set of.
Show how to write the above query in relational algebra, without using division.
By doing so, you would have shown how to define the division operation using the other relational algebra operations.
Give an expression in the tuple relational calculus that is equivalent to each of the following:
Give an expression in the domain relational calculus that is equivalent to each of the following:
Write expressions in relational algebra for each of the following queries:
Give an expression in tuple relational calculus for each of the following queries:
Find all cities of residence of all employees who work directly for “Jones.”
Find the name of the manager of the manager of “Jones.”
Find those employeeswho earnmore than all employees living in the city “Mumbai.”
Find the names of all students who have taken at least one Comp.
For each department, find the maximum salary of instructors in that department.
You may assume that every department has at least one instructor.
Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.
Give an expression in the relational algebra to express each of the following queries:
Find the names of all employees who work for “First Bank Corporation”
Find the names and cities of residence of all employees who work for “First Bank Corporation”
Find the names, street addresses, and cities of residence of all employees who work for “First Bank Corporation” and earn more than $10,000
Find the names of all employees in this database who live in the same city as the company for which they work.
Find all companies located in every city in which “Small Bank Corporation” is located.
Give a relational-algebra expression for each of the following queries:
Find those companies whose employees earn a higher salary, on average, than the average salary at First Bank Corporation.
Find the names of members who have borrowed any book published by “McGraw-Hill”
Find the name of members who have borrowed all books published by “McGraw-Hill”
Find the name and membership number of members who have borrowed more than five different books published by “McGraw-Hill”
For each publisher, find the name and membership number of members who have borrowed more than five books of that publisher.
Take into account that if an member does not borrow any books, then that member does not appear in the borrowed relation at all.
Give expressions in tuple relational calculus and domain relational calculus for each of the following queries:
Find the names of all employees who work for “First Bank Corporation”
Find the names and cities of residence of all employees who work for “First Bank Corporation”
Find the names, street addresses, and cities of residence of all employees who work for “First Bank Corporation” and earn more than $10,000
Find all employees who live in the same city as that in which the company for which they work is located.
Find all employees who live in the same city and on the same street as their managers.
Find all employees in the database who do not work for “First Bank Corporation”
Find all employees who earn more than every employee of “Small Bank Corporation”
Assume that the companies may be located in several cities.
Find all companies located in every city in which “Small Bank Corporation” is located.
The original definition of relational algebra is in Codd [1970]
Extensions to the relational model and discussions of incorporation of null values in the relational algebra (the RM/T model), as well as outer joins, are in Codd [1979]
The original definition of tuple relational calculus is in Codd [1972]
A formal proof of the equivalence of tuple relational calculus and relational algebra is in Codd [1972]
These large bodies of information do not exist in isolation.
They are part of the operation of some enterprise whose end product may be information from the database or may be some device or service for which the database plays only a supporting role.
The first two chapters of this part focus on the design of database schemas.
Instead of representing all data in tables, it distinguishes between basic objects, called entities, and relationships among these objects.
It is often used as a first step in database-schema design.
Relational database design—the design of the relational schema— was covered informally in earlier chapters.
There are, however, principles that can be used to distinguish good database designs from bad ones.
These are formalized by means of several “normal forms” that offer different trade-offs between the possibility of inconsistencies and the efficiency of certain queries.
This chapter first covers the design of Web-based interfaces to applications.
The chapter then describes how large applications are architected using multiple layers of abstraction.
Finally, the chapter provides a detailed discussion of security at the application and database levels.
Up to this point in the text, we have assumed a given database schema and studied how queries and updates are expressed.
We now consider how to design a database schema in the first place.
In this chapter, we focus on the entityrelationship data model (E-R), which provides a means of identifying entities to be represented in the database and how those entities are related.
Ultimately, the database design will be expressed in terms of a relational database design and an associated set of constraints.
We show in this chapter how an E-R design can be transformed into a set of relation schemas and how some of the constraints can be captured in that design.
Then, in Chapter 8, we consider in detail whether a set of relation schemas is a good or bad database design and study the process of creating good designs using a broader set of constraints.
These two chapters cover the fundamental concepts of database design.
The task of creating a database application is a complex one, involving design of the database schema, design of the programs that access and update the data, and design of a security scheme to control access to data.
The needs of the users play a central role in the design process.
In this chapter, we focus on the design of the database schema, although we briefly outline some of the other design tasks later in the chapter.
The design of a complete database application environment that meets the needs of the enterprise being modeled requires attention to a broad set of issues.
These additional aspects of the expected use of the database influence a variety of design choices at the physical, logical, and view levels.
However, such a direct design process is difficult for real-world applications, since they are oftenhighly complex.
Often no one person understands the complete data needs of an application.
The database designer must interact with users of the application to understand the needs of the application, represent them in a high-level fashion that can be understood by the users, and then translate the requirements into lower levels of the design.
A high-level data model serves the database designer by providing a conceptual framework in which to specify, in a systematic fashion, the data requirements of the database users, and a database structure that fulfills these requirements.
The initial phase of database design is to characterize fully the data needs of the prospective database users.
The database designer needs to interact extensively with domain experts and users to carry out this task.
The outcome of this phase is a specification of user requirements.
While there are techniques for diagrammatically representing user requirements, in this chapter we restrict ourselves to textual descriptions of user requirements.
Next, the designer chooses a data model and, by applying the concepts of the chosen data model, translates these requirements into a conceptual schema of the database.
The schema developed at this conceptual-design phase provides a detailed overview of the enterprise.
The designer reviews the schema to confirm that all data requirements are indeed satisfied and are not in conflict with one another.
She can also examine the design to remove any redundant features.
Her focus at this point is on describing the data and their relationships, rather than on specifying physical storage details.
A fully developed conceptual schema also indicates the functional requirements of the enterprise.
In a specification of functional requirements, users describe the kinds of operations (or transactions) that will be performed on the data.
Example operations include modifying or updating data, searching for and retrieving specific data, and deleting data.
At this stage of conceptual design, the designer can review the schema to ensure it meets functional requirements.
The process of moving from an abstract data model to the implementation of the database proceeds in two final design phases.
In the logical-design phase, the designer maps the high-level conceptual schema onto the implementation data model of the database system that.
Finally, the designer uses the resulting system-specific database schema in the subsequent physical-design phase, in which the physical features of the database are specified.
The physical schema of a database can be changed relatively easily after an application has been built.
However, changes to the logical schema are usually harder to carry out, since they may affect a number of queries and updates scattered across application code.
It is therefore important to carry out the database design phase with care, before building the rest of the database application.
A major part of the database design process is deciding how to represent in the design the various types of “things” such as people, places, products, and the like.
We use the term entity to refer to any such distinctly identifiable item.
In a university database, examples of entities would include instructors, students, departments, courses, and course offerings.1 The various entities are related to each other in a variety of ways, all of which need to be captured in the database design.
For example, a student takes a course offering, while an instructor teaches a course offering; teaches and takes are examples of relationships between entities.
In designing a database schema, we must ensure that we avoid two major pitfalls:
For example, if we store the course identifier and title of a course with each course offering, the title would be stored redundantly (that is, multiple times, unnecessarily) with each course offering.
It would suffice to store only the course identifier with each course offering, and to associate the title with the course identifier only once, in a course entity.
In the university example we have used so far, we have a relation with section information and a separate relation with course information.
Suppose that instead we have a single relation where we repeat all of the course information (course id, title, dept name, credits) once for each section (offering) of the course.
The biggest problem with such redundant representation of information is that the copies of a piece of information can become inconsistent if the.
A course may have run in multiple semesters, as well as multiple times in a semester.
We refer to each such offering of a course as a section.
For example, different offerings of a course may have the same course identifier, but may have different titles.
It would then become unclear what the correct title of the course is.
Incompleteness: A bad design may make certain aspects of the enterprise difficult or impossible to model.
For example, suppose that, as in case (1) above, we only had entities corresponding to course offering, without having an entity corresponding to courses.
Equivalently, in terms of relations, suppose we have a single relation where we repeat all of the course information once for each section that the course is offered.
It would then be impossible to represent information about a new course, unless that course is offered.
We might try to make do with the problematic design by storing null values for the section information.
Such a work-around is not only unattractive, but may be prevented by primary-key constraints.
There may be a large number of good designs from which we must choose.
Is the sale of this product a relationship between the customer and the product? Alternatively, is the sale itself an entity that is related both to the customer and to the product? This choice, though simple,maymake an important difference in what aspects of the enterprise can be modeledwell.
Considering the need tomake choices such as this for the large number of entities and relationships in a real-world enterprise, it is not hard to see that database design can be a challenging problem.
Indeed we shall see that it requires a combination of both science and “good taste.”
The E-R model is very useful in mapping the meanings and interactions of real-world enterprises onto a conceptual schema.
Because of this usefulness,many database-design tools draw on concepts from the E-R model.
The E-R data model employs three basic concepts: entity sets, relationship sets, and attributes, which we study first.
The E-Rmodel also has an associated diagrammatic representation, the E-R diagram, which we study later in this chapter.
An entity is a “thing” or “object” in the real world that is distinguishable from all other objects.
For example, each person in a university is an entity.
An entity has a set of properties, and the values for some set of properties may uniquely identify an entity.
For instance, a person may have a person id property whose.
Thus, the value 677-89-9011 for person id would uniquely identify one particular person in the university.
Similarly, courses can be thought of as entities, and course id uniquely identifies a course entity in the university.
An entity may be concrete, such as a person or a book, or it may be abstract, such as a course, a course offering, or a flight reservation.
An entity set is a set of entities of the same type that share the sameproperties, or attributes.
The set of all people who are instructors at a given university, for example, can be defined as the entity set instructor.
Similarly, the entity set student might represent the set of all students in the university.
In the process of modeling, we often use the term entity set in the abstract, without referring to a particular set of individual entities.
We use the term extension of the entity set to refer to the actual collection of entities belonging to the entity set.
Thus, the set of actual instructors in the university forms the extension of the entity set instructor.
For example, it is possible to define the entity set of all people in a university (person)
A person entity may be an instructor entity, a student entity, both, or neither.
Attributes are descriptive properties possessed by each member of an entity set.
The designation of an attribute for an entity set expresses that the database stores similar information concerning each entity in the entity set; however, each entity may have its own value for each attribute.
Possible attributes of the instructor entity set are ID, name, dept name, and salary.
In real life, there would be further attributes, such as street number, apartment number, state, postal code, and country, but we omit them to keep our examples simple.
Possible attributes of the course entity set are course id, title, dept name, and credits.
Each entity has a value for each of its attributes.
The ID attribute is used to identify instructors uniquely, since there may be more than one instructor with the same name.
In the United States, many enterprises find it convenient to use the social-security number of a person2 as an attribute whose value uniquely identifies the person.
In general the enterprise would have to create and assign a unique identifier for each instructor.
A database thus includes a collection of entity sets, each of which contains any number of entities of the same type.
Figure 7.1 shows part of a university database that consists of two entity sets: instructor and student.
To keep the figure simple, only some of the attributes of the two entity sets are shown.
A database for a university may include a number of other entity sets.
For example, in addition to keeping track of instructors and students, the university also has information about courses, which are represented by the entity set course.
In the United States, the government assigns to each person in the country a unique number, called a social-security number, to identify that person uniquely.
Each person is supposed to have only one social-security number, and no two people are supposed to have the same social-security number.
In a real setting, a university database may keep dozens of entity sets.
For example, we can define a relationship advisor that associates instructor Katz with student Shankar.
This relationship specifies that Katz is an advisor to student Shankar.
Consider the two entity sets instructor and student in Figure 7.1
As another example, consider the two entity sets student and section.
We can define the relationship set takes to denote the association between a student and the course sections in which that student is enrolled.
A relationship instance in an E-R schema represents an association between the named entities in the real-world enterprise that is being modeled.
This relationship instance represents that in the university, the instructorKatz is advising student Shankar.
The function that an entity plays in a relationship is called that entity’s role.
Since entity sets participating in a relationship set are generally distinct, roles.
However, they are useful when the meaning of a relationship needs clarification.
Such is the case when the entity sets of a relationship set are not distinct; that is, the same entity set participates in a relationship set more than once, in different roles.
For example, consider the entity set course that records information about all the courses offered in the university.
Consider a relationship set advisor with entity sets instructor and student.
We could associate the attribute date with that relationship to specify the date when an instructor became the advisor of a student.
Figure 7.3 shows the relationship set advisor with a descriptive attribute date.
Please note that Katz advises two students with two different advising dates.
As amore realistic example ofdescriptive attributes for relationships, consider the entity sets student and section, which participate in a relationship set takes.
We may wish to store a descriptive attribute grade with the relationship to record the grade that a student got in the class.
We may also store a descriptive attribute for credit to record whether a student has taken the course for credit, or is auditing (or sitting in on) the course.
A relationship instance in a given relationship set must be uniquely identifiable from its participating entities, without using the descriptive attributes.
To understand this point, supposewewant tomodel all the dateswhen an instructor.
Figure 7.3 date as attribute of the advisor relationship set.
The single-valued attribute date can store a single date only.
We cannot represent multiple dates by multiple relationship instances between the same instructor and a student, since the relationship instances would not be uniquely identifiable using only the participating entities.
The right way to handle this case is to create a multivalued attribute date, which can store all the dates.
It is possible to have more than one relationship set involving the same entity sets.
In our example, the instructor and student entity sets participate in the relationship set advisor.
Additionally, suppose each student must have another instructorwho serves as a department advisor (undergraduate or graduate)
Then the instructor and student entity sets may participate in another relationship set, dept advisor.
The relationship sets advisor and dept advisor provide examples of a binary relationship set—that is, one that involves two entity sets.Most of the relationship sets in a database system are binary.
Occasionally, however, relationship sets involve more than two entity sets.
As an example, suppose that we have an entity set project that represents all the research projects carried out in the university.
Each project can have multiple associated students and multiple associated instructors.
Furthermore, each student working on a project must have an associated instructor who guides the student on the project.
For now,we ignore the first two relationships, between project and instructor, and between project and student.
Instead, we focus on the information about which instructor is guiding which student on a particular project.
To represent this information, we relate the three entity sets through the relationship set proj guide, which indicates that a particular student is guided by a particular instructor on a particular project.
Note that a student could have different instructors as guides for different projects, which cannot be captured by a binary relationship between students and instructors.
The number of entity sets that participate in a relationship set is the degree of the relationship set.
An attribute, as used in the E-R model, can be characterized by the following attribute types.
In our examples thus far, the attributes have been simple; that is, they have not been divided into subparts.
Composite attributes, on the other hand, can be divided into subparts (that is, other attributes)
For example, an attribute name could be structured as a composite attribute consisting of first name,middle initial, and last name.
Using composite attributes in a design schema is a good choice if a user will wish to refer to an entire attribute on some occasions, and to only a component of the attribute on other occasions.
Suppose we were to to add an address to the student entity-set.
The address can be defined as the composite attribute addresswith the attributes street, city, state, and zip code.3 Composite attributes help us to group together related attributes, making the modeling cleaner.
Note also that a composite attribute may appear as a hierarchy.
In the composite attribute address, its component attribute street can be further divided into street number, street name, and apartment number.
Figure 7.4 depicts these examples of composite attributes for the instructor entity set.
The attributes in our examples all have a single value for a particular entity.
For instance, the student ID attribute for a specific student entity refers to only one student ID.
There may be instances where an attribute has a set of values for a specific entity.
We assume the address format used in the United States, which includes a numeric postal code called a zip code.
For example, a university may limit the number of phone numbers recorded for a single instructor to two.
Placing bounds in this case expresses that the phone number attribute of the instructor entity set may have between zero and two values.
The value for this type of attribute can be derived from the values of other related attributes or entities.
For instance, let us say that the instructor entity set has an attribute students advised, which represents how many students an instructor advises.We can derive the value for this attribute by counting the number of student entities associated with that instructor.
As another example, suppose that the instructor entity set has an attribute age that indicates the instructor’s age.
If the instructor entity set also has an attribute date of birth, we can calculate age from date of birth and the current date.
In this case, date of birthmay be referred to as a base attribute, or a stored attribute.
The value of a derived attribute is not stored but is computed when required.
An attribute takes a null value when an entity does not have a value for it.
The null value may indicate “not applicable”—that is, that the value does not exist for the entity.
Null can also designate that an attribute value is unknown.
An unknown value may be either missing (the value does exist, but we do not have that information) or not known (we do not know whether or not the value actually exists)
For instance, if the name value for a particular instructor is null, we assume that the value is missing, since every instructor must have a name.
A null value for the apartment number attribute could mean that the address does not include.
An E-R enterprise schema may define certain constraints to which the contents of a database must conform.
In this section, we examine mapping cardinalities and participation constraints.
Mapping cardinalities, or cardinality ratios, express the number of entities to which another entity can be associated via a relationship set.
Mapping cardinalities are most useful in describing binary relationship sets, although they can contribute to the description of relationship sets that involve more than two entity sets.
In this section, we shall concentrate on only binary relationship sets.
For a binary relationship set R between entity sets A and B, the mapping cardinality must be one of the following:
An entity in A is associated with at most one entity in B, and an entity in B is associated with at most one entity in A.
An entity in A is associated with any number (zero or more) of entities in B.
An entity in B, however, can be associated with at most one entity in A.
An entity in A is associated with at most one entity in B.
An entity in B, however, can be associated with any number (zero or more) of entities in A.
An entity in A is associated with any number (zero or more) of entities in B, and an entity in B is associated with any number (zero or more) of entities in A.
The appropriate mapping cardinality for a particular relationship set obviously depends on the real-world situation that the relationship set is modeling.
If, in a particular university, a student can be advised by only one instructor, and an instructor can advise several students, then the relationship set from instructor to student is one-to-many.
If a student can be advised by several instructors (as in the case of students advised jointly), the relationship set is many-to-many.
The participation of an entity set E in a relationship set R is said to be total if every entity in E participates in at least one relationship in R.
If only some entities in E participate in relationships in R, the participation of entity set E in relationship R is said to be partial.
In Figure 7.5a, the participation of B in the relationship set is total while the participation of A in the relationship set is partial.
In Figure 7.5b, the participation of both A and B in the relationship set are total.
For example, we expect every student entity to be related to at least one instructor through the advisor relationship.
Therefore the participation of student in the relationship set advisor is total.
Hence, it is possible that only some of the instructor entities are related to the student entity set through the advisor relationship, and the participation of instructor in the advisor relationship set is therefore partial.
We must have a way to specify how entities within a given entity set are distinguished.
Conceptually, individual entities are distinct; from a database perspective, however, the differences among them must be expressed in terms of their attributes.
Therefore, the values of the attribute values of an entity must be such that they can uniquely identify the entity.
In other words, no two entities in an entity set are allowed to have exactly the same value for all attributes.
The notion of a key for a relation schema, as defined in Section 2.3, applies directly to entity sets.
That is, a key for an entity is a set of attributes that suffice to distinguish entities from each other.
The concepts of superkey, candidate key, and primary key are applicable to entity sets just as they are applicable to relation schemas.
Keys also help to identify relationships uniquely, and thus distinguish relationships from each other.
Below, we define the corresponding notions of keys for relationships.
The primary key of an entity set allows us to distinguish among the various entities of the set.
We need a similar mechanism to distinguish among the various relationships of a relationship set.
Let primarykey(Ei ) denote the set of attributes that forms the primary key for entity set Ei.
Assume for now that the attribute names of all primary keys are unique.
The composition of the primary key for a relationship set depends on the set of attributes associated with the relationship set R.
If the relationship set R has no attributes associated with it, then the set of attributes.
In both of the above cases, the set of attributes.
If the attribute names of primary keys are not unique across entity sets, the.
The structure of the primary key for the relationship set depends on the mapping cardinality of the relationship set.
As an illustration, consider the entity sets instructor and student, and the relationship set advisor, with attribute date, in Section 7.2.2
Then the primary key of advisor consists of the union of the primary keys of instructor and student.
If the relationship is many-to-one from student to instructor—that is, each student can have have at most one advisor—then the primary key of advisor is simply the primary key of student.
However, if an instructor can advise only one studentthat is, if the advisor relationship is many-to-one from instructor to student—then the primary key of advisor is simply the primary key of instructor.
For one-to-one relationships either candidate key can be used as the primary key.
For nonbinary relationships, if no cardinality constraints are present then the superkey formed as described earlier in this section is the only candidate key, and it is chosen as the primary key.
The choice of the primary key is more complicated if cardinality constraints are present.
Since we have not discussed how to specify cardinality constraints on nonbinary relations,we donot discuss this issue further in this chapter.
When we design a database using the E-R model, we usually start by identifying those entity sets that should be included.
For example, in the university organization we have discussed thus far, we decided to include such entity sets as student, instructor, etc.
Once the entity sets are decided upon, we must choose the appropriate attributes.
These attributes are supposed to represent the various values we want to capture in the database.
In the university organization, we decided that for the instructor entity set, we will include the attributes ID, name, dept name, and salary.
We could have added the attributes: phone number, office number, home page, etc.
The choice of what attributes to include is up to the designer, who has a good understanding of the structure of the enterprise.
Once the entities and their corresponding attributes are chosen, the relationship sets among the various entities are formed.
These relationship setsmay result in a situation where attributes in the various entity sets are redundant and need to be removed from the original entity sets.
The entity set instructor includes the attributes ID, name, dept name, and salary, with ID forming the primary key.
The entity set department includes the attributes dept name, building, and budget, with dept name forming the primary key.
We model the fact that each instructor has an associated department using a relationship set inst dept relating instructor and department.
Since it is the primary key for the entity set department, it is redundant in the entity set instructor and needs to be removed.
Removing the attribute dept name from the instructor entity set may appear rather unintuitive, since the relation instructor that we used in the earlier chapters had an attribute dept name.
As we shall see later, when we create a relational schema from the E-R diagram, the attribute dept name in fact gets added to the relation instructor, but only if each instructor has at most one associated department.
If an instructor has more than one associated department, the relationship between instructors and departments is recorded in a separate relation inst dept.
Treating the connection between instructors and departments uniformly as a relationship, rather than as an attribute of instructor,makes the logical relationship explicit, andhelps avoid apremature assumption that each instructor is associated with only one department.
Similarly, the student entity set is related to the department entity set through the relationship set student dept and thus there is no need for a dept name attribute in student.
As another example, consider course offerings (sections) along with the time slots of the offerings.
Each time slot is identifiedbya time slot id, andhas associated with it a set of weekly meetings, each identified by a day of the week, start time, and end time.Wedecide tomodel the set ofweeklymeeting times as amultivalued composite attribute.
Suppose wemodel entity sets section and time slot as follows:
The entity set section includes the attributes course id, sec id, semester, year, building, room number, and time slot id, with (course id, sec id, year, semester) forming the primary key.
These entities are related through the relationship set sec time slot.
The attribute time slot id appears in both entity sets.
For our university example, we list the entity sets and their attributes below, with primary keys underlined:
We shall see later on that the primary key for the relation created from the entity set time slot includes day and start time; however, day and start time do not form part of the primary key of the entity set time slot.
We could optionally give a name, such as meeting, for the composite attribute containing day, start time, and end time.
You can verify that none of the entity sets has any attribute that is made redundant by one of the relationship sets.
As we saw briefly in Section 1.3.3, an E-R diagram can express the overall logical structure of a database graphically.
E-R diagrams are simple and clear—qualities that may well account in large part for the widespread use of the E-R model.
The first part, which in this textbook is shaded blue, contains the name of the entity set.
The second part contains the names of all the attributes of the entity set.
Dashed lines link attributes of a relationship set to the relationship set.
Double lines indicate total participation of an entity in a relationship set.
Consider the E-R diagram in Figure 7.7, which consists of two entity sets, instructor and student related throughabinary relationship set advisor.
The attributes associated with instructor are ID, name, and salary.
The attributes associated with student are ID, name, and tot cred.
In Figure 7.7, attributes of an entity set that are members of the primary key are underlined.
If a relationship set has some attributes associated with it, thenwe enclose the attributes in a rectangle and link the rectangle with a dashed line to the diamond representing that relationship set.
For example, in Figure 7.8, we have the date descriptive attribute attached to the relationship set advisor to specify the date on which an instructor became the advisor.
Figure 7.8 E-R diagram with an attribute attached to a relationship set.
One-to-one: We draw a directed line from the relationship set advisor to both entity sets instructor and student (see Figure 7.9a)
This indicates that an instructor may advise at most one student, and a student may have at most one advisor.
One-to-many: We draw a directed line from the relationship set advisor to the entity set instructor and an undirected line to the entity set student (see Figure 7.9b)
This indicates that an instructor may advise many students, but a student may have at most one advisor.
Many-to-one: We draw an undirected line from the relationship set advisor to the entity set instructor and a directed line to the entity set student.
This indicates that an instructor may advise at most one student, but a student may have many advisors.
Many-to-many:We draw an undirected line from the relationship set advisor to both entity sets instructor and student (see Figure 7.9c)
The E-R diagram in Figure 7.10 could alternatively have been drawn with a double line from student to advisor, and an arrow on the line from advisor to instructor, in place of the cardinality constraints shown.
This alternative diagram would enforce exactly the same constraints as the constraints shown in the figure.
Here, a composite attribute name, with component attributes first name, middle initial, and last name replaces the simple attribute name of instructor.
As another example, suppose we were to add an address to the instructor entity-set.
The address can be defined as the composite attribute address with the attributes.
Figure 7.11 E-R diagram with composite, multivalued, and derived attributes.
The attribute street is itself a composite attribute whose component attributes are street number, street name, and apartment number.
We indicate roles in E-R diagrams by labeling the lines that connect diamonds to rectangles.
Figure 7.12 shows the role indicators course id and prereq id between the course entity set and the prereq relationship set.
Nonbinary relationship sets can be specified easily in an E-R diagram.
Figure 7.13 consists of the three entity sets instructor, student, and project, related through the relationship set proj guide.
We can specify some types of many-to-one relationships in the case of nonbinary relationship sets.
Suppose a student can have at most one instructor as a guide on a project.
This constraint can be specified by an arrow pointing to instructor on the edge from proj guide.
We permit at most one arrow out of a relationship set, since an E-R diagram with two or more arrows out of a nonbinary relationship set can be interpreted in twoways.
Each of these interpretations has been used in different books and systems.
To avoid confusion, we permit only one arrow out of a relationship set, in which case the two interpretations are equivalent.
Consider a section entity, which is uniquely identified by a course identifier, semester, year, and section identifier.
Suppose we create a relationship set sec course between entity sets section and course.
Now, observe that the information in sec course is redundant, since section already has an attribute course id, which identifies the course with which the section is related.
One option to deal with this redundancy is to get rid of the.
An alternative way to deal with this redundancy is to not store the attribute course id in the section entity and to only store the remaining attributes sec id, year, and semester.6 However, the entity set section then does not have enough attributes to identify a particular section entity uniquely; although each section entity is distinct, sections for different courses may share the same sec id, year, and semester.
To deal with this problem, we treat the relationship sec course as a special relationship that provides extra information, in this case the course id, required to identify section entities uniquely.
The notion of weak entity set formalizes the above intuition.
An entity set that does not have sufficient attributes to form a primary key is termed aweak entity set.
An entity set that has a primary key is termed a strong entity set.
For a weak entity set to be meaningful, it must be associated with another entity set, called the identifying or owner entity set.
Every weak entity must be associated with an identifying entity; that is, the weak entity set is said to be existence dependenton the identifying entity set.
The identifying entity set is said to own the weak entity set that it identifies.
The relationship associating the weak entity set with the identifying entity set is called the identifying relationship.
The identifying relationship is many-to-one from the weak entity set to the identifying entity set, and the participation of the weak entity set in the relationship is total.
The identifying relationship set should not have any descriptive attributes, since any such attributes can instead be associatedwith theweak entity set.
In our example, the identifying entity set for section is course, and the relationship sec course, which associates section entities with their corresponding course entities, is the identifying relationship.
Although a weak entity set does not have a primary key, we nevertheless need a means of distinguishing among all those entities in the weak entity set that depend on one particular strong entity.
The discriminator of a weak entity set is a set of attributes that allows this distinction to be made.
For example, the discriminator of the weak entity set section consists of the attributes sec id, year, and semester, since, for each course, this set of attributes uniquely identifies one single section for that course.
The discriminator of a weak entity set is also called the partial key of the entity set.
Note that we could have chosen to make sec id globally unique across all courses offered in the university, in which case the section entity set would have.
Note that the relational schema we eventually create from the entity set section does have the attribute course id, for reasons that will become clear later, even though we have dropped the attribute course id from the entity set section.
However, conceptually, a section is still dependent on a course for its existence, which is made explicit by making it a weak entity set.
In E-R diagrams, a weak entity set is depicted via a rectangle, like a strong entity set, but there are two main differences:
The discriminator of a weak entity is underlined with a dashed, rather than a solid, line.
The relationship set connecting the weak entity set to the identifying strong entity set is depicted by a double diamond.
In Figure 7.14, the weak entity set section depends on the strong entity set course via the relationship set sec course.
The figure also illustrates the use of double lines to indicate total participation; the participation of the (weak) entity set section in the relationship sec course is total, meaning that every section must be related via sec course to some course.
Finally, the arrow from sec course to course indicates that each section is related to a single course.
A weak entity set can participate in relationships other than the identifying relationship.
For instance, the section entity could participate in a relationship with the time slot entity set, identifying the time when a particular class section meets.
A weak entity set may participate as owner in an identifying relationship with anotherweak entity set.
It is also possible to have aweak entity setwithmore than one identifying entity set.
A particular weak entity would then be identified by a combination of entities, one from each identifying entity set.
The primary key of the weak entity set would consist of the union of the primary keys of the identifying entity sets, plus the discriminator of the weak entity set.
In some cases, the database designer may choose to express a weak entity set as a multivalued composite attribute of the owner entity set.
In our example, this alternative would require that the entity set course have amultivalued, composite attribute section.
A weak entity set may be more appropriately modeled as an attribute if it participates in only the identifying relationship, and if it has few attributes.
Conversely, a weak entity set representation more aptly models a situation where the set participates in relationships other than the identifying relationship, and where the weak entity set has several attributes.
It is clear that section violates the requirements for being modeled as a multivalued composite attribute, and is modeled more aptly as a weak entity set.
In Figure 7.15, we show an E-R diagram that corresponds to the university enterprise that we have been using thus far in the text.
This E-R diagram is equivalent to the textual description of the university E-R model that we saw in Section 7.4, but with several additional constraints, and section now being a weak entity.
As a result, there is a double line in Figure 7.15 between instructor and inst dept, indicating total participation of instructor in inst dept; that is, each instructor must be associated with a department.
Further, there is an arrow from inst dept to department, indicating that each instructor can have at most one associated department.
Similarly, entity sets course and student have double lines to relationship sets course dept and stud dept respectively, as also entity set section to relationship set sec time slot.
The first two relationships, in turn, have an arrow pointing to the other relationship, department, while the third relationship has an arrow pointing to time slot.
Further, Figure 7.15 shows that the relationship set takes has a descriptive attribute grade, and that each student has at most one advisor.
The figure also shows that section is now a weak entity set, with attributes sec id, semester, and year forming thediscriminator; sec course is the identifying relationship set relating weak entity set section to the strong entity set course.
In Section 7.6, we shall show how this E-R diagram can be used to derive the various relation schemas we use.
We can represent a database that conforms to an E-R database schema by a collection of relation schemas.
For each entity set and for each relationship set in the database design, there is a unique relation schema to which we assign the name of the corresponding entity set or relationship set.
Both the E-R model and the relational database model are abstract, logical representations of real-world enterprises.
Because the twomodels employ similar design principles, we can convert an E-R design into a relational design.
In this section, we describe how an E-R schema can be represented by relation schemas and how constraints arising from the E-R design can be mapped to constraints on relation schemas.
We represent this entity by a schema called Ewith n distinct attributes.
Each tuple in a relation on this schema corresponds to one entity of the entity set E.
For schemas derived from strong entity sets, the primary key of the entity set serves as the primary key of the resulting schema.
This follows directly from the fact that each tuple corresponds to a specific entity in the entity set.
As an illustration, consider the entity set student of the E-R diagram in Figure 7.15
This entity set has three attributes: ID, name, tot cred.
We represent this entity set by a schema called student with three attributes:
Note that since student ID is the primary key of the entity set, it is also the primary key of the relation schema.
Continuingwith our example, for the E-R diagram in Figure 7.15, all the strong entity sets, except time slot, have only simple attributes.
As you can see, both the instructor and student schemas are different from the schemas we have used in the previous chapters (they do not contain the attribute dept name)
When a strong entity set has nonsimple attributes, things are a bit more complex.
We handle composite attributes by creating a separate attribute for each of the component attributes; we do not create a separate attribute for the composite attribute itself.
To illustrate, consider the version of the instructor entity set depicted in Figure 7.11
For the composite attribute name, the schema generated for instructor contains the attributes first name, middle name, and last name; there is no separate attribute or schema for name.
Similarly, for the composite attribute address, the schema generated contains the attributes street, city, state, and zip code.
Since street is a composite attribute it is replaced by street number, street name, and apt number.
Multivalued attributes, however, are an exception; new relation schemas are created for these attributes, as we shall see shortly.
Derived attributes are not explicitly represented in the relational data model.
The relational schema derived from the version of entity set instructor with complex attributes, without including the multivalued attribute, is thus:
For amultivalued attributeM, we create a relation schemaRwith an attribute A that corresponds to M and attributes corresponding to the primary key of the entity set or relationship set of which M is an attribute.
As an illustration, consider the E-R diagram in Figure 7.11 that depicts the entity set instructor, which includes the multivalued attribute phone number.
Eachphone number of an instructor is represented as a unique tuple in the relation on this schema.
We create a primary key of the relation schema consisting of all attributes of the schema.
In the above example, the primary key consists of both attributes of the relation instructor phone.
In addition, we create a foreign-key constraint on the relation schema created from the multivalued attribute, with the attribute generated from the primary key of the entity set referencing the relation generated from the entity set.
In the above example, the foreign-key constraint on the instructor phone relation would be that attribute ID references the instructor relation.
In the case that an entity set consists of only two attributes—a single primarykey attribute B and a singlemultivalued attributeM—the relation schema for the entity set would contain only one attribute, namely the primary-key attribute B.
We can drop this relation, while retaining the relation schema with the attribute B and attribute A that corresponds to M.
To illustrate, consider the entity set time slotdepicted in Figure 7.15.Here, time slot id is the primary key of the time slot entity set and there is a singlemultivalued attribute that happens also to be composite.
The entity set can be represented by just the following schema created from the multivalued composite attribute:
Although not represented as a constraint on the E-R diagram, we know that there cannot be two meetings of a class that start at the same time of the same day-ofthe-week but end at different times; based on this constraint, end time has been omitted from the primary key of the time slot schema.
The relation created from the entity setwould have only a single attribute time slot id; the optimization of dropping this relation has the benefit of simplifying the resultant database schema, although it has a drawback related to foreign keys, which we briefly discuss in Section 7.6.4
Let B be the strong entity set on which A depends.
We represent the entity set A by a relation schema called A with one attribute for each member of the set:
For schemas derived from a weak entity set, the combination of the primary key of the strong entity set and the discriminator of the weak entity set serves as the primary key of the schema.
In addition to creating a primary key, we also create a foreign-key constraint on the relation A, specifying that the.
The foreignkey constraint ensures that for each tuple representing a weak entity, there is a corresponding tuple representing the corresponding strong entity.
As an illustration, consider the weak entity set section in the E-R diagram of Figure 7.15
This entity set has the attributes: sec id, semester, and year.
The primary key of the course entity set, on which section depends, is course id.
Thus, we represent section by a schema with the following attributes:
The primary key consists of the primary key of the entity set course, along with the discriminator of section, which is sec id, semester, and year.
We also create a foreign-key constraint on the section schema, with the attribute course id referencing the primary key of the course schema, and the integrity constraint “on delete cascade”.7 Because of the “on delete cascade” specification on the foreign key constraint, if a course entity is deleted, then so are all the associated section entities.
We represent this relationship set by a relation schema called R with one attribute for each member of the set:
Wedescribed earlier, in Section 7.3.3, how to choose a primary key for a binary relationship set.
As we saw in that section, taking all the primary-key attributes from all the related entity sets serves to identify a particular tuple, but for one-toone, many-to-one, and one-to-many relationship sets, this turns out to be a larger set of attributes than we need in the primary key.
For a binary many-to-many relationship, the union of the primary-key attributes from the participating entity sets becomes the primary key.
For a binary one-to-one relationship set, the primary key of either entity set can be chosen as the primary key.
For a binarymany-to-one or one-to-many relationship set, the primary key of the entity set on the “many” side of the relationship set serves as the primary key.
The “on delete cascade” feature of foreign key constraints in SQL is described in Section 4.4.5
For an n-ary relationship set without any arrows on its edges, the union of the primary key-attributes from the participating entity sets becomes the primary key.
For an n-ary relationship set with an arrow on one of its edges, the primary keys of the entity sets not on the “arrow” side of the relationship set serve as the primary key for the schema.
Recall that we allowed only one arrow out of a relationship set.
We also create foreign-key constraints on the relation schema R as follows: For each entity set Ei related to relationship set R, we create a foreign-key constraint from relation schema R, with the attributes of R that were derived from primary-key attributes of Ei referencing the primary key of the relation schema representing Ei.
As an illustration, consider the relationship set advisor in the E-R diagram of Figure 7.15
Since the relationship set has no attributes, the advisor schema has two attributes, the primary keys of instructor and student.
Since both attributes have the same name, we rename them i ID and s ID.
Since the advisor relationship set is manyto-one from student to instructor the primary key for the advisor relation schema is s ID.
We also create two foreign-key constraints on the advisor relation, with attribute i ID referencing the primary key of instructor and attribute s ID referencing the primary key of student.
Observe that for the case of the relationship set prereq, the role indicators associated with the relationship are used as attribute names, since both roles refer to the same relation course.
Similar to the caseof advisor, theprimarykey for eachof the relations sec course, sec time slot, sec class, inst dept, stud dept and course dept consists of the primary key of only one of the two related entity sets, since each of the corresponding relationships is many-to-one.
Foreign keys are not shown in Figure 7.16, but for each of the relations in the figure there are two foreign-key constraints, referencing the two relations created from the two related entity sets.
Thus, for example, sec course has foreign keys referencing section and classroom, teaches has foreign keys referencing instructor and section, and takes has foreign keys referencing student and section.
The optimization that allowed us to create only a single relation schema from the entity set time slot, which had a multivalued attribute, prevents the creation of a foreign key from the relation schema sec time slot to the relation created from entity set time slot, since we dropped the relation created from the entity set time.
We retained the relation created from the multivalued attribute, and named it time slot, but this relation may potentially have no tuples corresponding to a time slot id, or may have multiple tuples corresponding to a time slot id; thus, time slot id in sec time slot cannot reference this relation.
The astute reader may wonder why we have not seen the schemas sec course, sec time slot, sec class, inst dept, stud dept, and course dept in the previous chapters.
The reason is that the algorithm we have presented thus far results in some schemas that can be either eliminated or combined with other schemas.
In general, the schema for the relationship set linking a weak entity set to its corresponding strong entity set is redundant and does not need to be present in a relational database design based upon an E-R diagram.
Consider a many-to-one relationship set AB from entity set A to entity set B.
Suppose further that the participation of A in the relationship is total; that is, every entity a in the entity set B must participate in the relationshipAB.
Then we can combine the schemas A and AB to form a single schema consisting of the union of attributes of both schemas.
The primary key of the combined schema is the primary key of the entity set into whose schema the relationship set schema was merged.
To illustrate, let’s examine the various relations in the E-R diagram of Figure 7.15 that satisfy the above criteria:
In the case of one-to-one relationships, the relation schema for the relationship set can be combined with the schemas for either of the entity sets.
We can combine schemas even if the participation is partial by using null values.
In the above example, if inst dept were partial, then we would store null values for the dept name attribute for those instructors who have no associated department.
Finally, we consider the foreign-key constraints that would have appeared in the schema representing the relationship set.
There would have been foreign-key constraints referencing each of the entity sets participating in the relationship set.
We drop the constraint referencing the entity set into whose schema the relationship set schema is merged, and add the other foreign-key constraints to the combined schema.
For example, inst dept has a foreign key constraint of the attribute dept name referencing the department relation.
The notions of an entity set and a relationship set are not precise, and it is possible to define a set of entities and the relationships among them in a number of different ways.
In this section, we examine basic issues in the design of an E-R database schema.
It can easily be argued that a phone is an entity in its own right with attributes phone number and location; the locationmay be the office or homewhere the phone is located, with mobile (cell) phones perhaps represented by the value “mobile.” If we take this point of view, we do not add the attribute phone number to the instructor.
A phone entity set with attributes phone number and location.
A relationship set inst phone, denoting the association between instructors.
What, then, is themain difference between these two definitions of an instructor? Treating a phone as an attribute phone number implies that instructors have precisely one phone number each.
Treating a phone as an entity phone permits instructors to have several phone numbers (including zero) associatedwith them.
However, we could instead easily define phone number as a multivalued attribute to allow multiple phones per instructor.
The main difference then is that treating a phone as an entity better models a situation where one may want to keep extra information about a phone, such as its location, or its type (mobile, IP phone, or plain old phone), or all who share.
Figure 7.17 Alternatives for adding phone to the instructor entity set.
Thus, treating phone as an entity is more general than treating it as an attribute and is appropriate when the generality may be useful.
In contrast, it would not be appropriate to treat the attribute name (of an instructor) as an entity; it is difficult to argue that name is an entity in its own right (in contrast to the phone)
Thus, it is appropriate to have name as an attribute of the instructor entity set.
Two natural questions thus arise: What constitutes an attribute, and what constitutes an entity set? Unfortunately, there are no simple answers.
The distinctions mainly depend on the structure of the real-world enterprise being modeled, and on the semantics associated with the attribute in question.
A common mistake is to use the primary key of an entity set as an attribute of another entity set, instead of using a relationship.
For example, it is incorrect to model the ID of a student as an attribute of an instructor even if each instructor advises only one student.
The relationship advisor is the correct way to represent the connection between students and instructors, since it makes their connection explicit, rather than implicit via an attribute.
Another related mistake that people sometimes make is to designate the primary-key attributes of the related entity sets as attributes of the relationship set.
For example, ID (the primary-key attributes of student) and ID (the primary key of instructor) should not appear as attributes of the relationship advisor.
This should not be done since the primary-key attributes are already implicit in the relationship set.8
It is not always clear whether an object is best expressed by an entity set or a relationship set.
In Figure 7.15, we used the takes relationship set to model the situationwhere a student takes a (section of a) course.
Note that we use double lines to indicate total participation by registration entities.
When we create a relation schema from the E-R schema, the attributes may appear in a schema created from the advisor relationship set, as we shall see later; however, they should not appear in the advisor relationship set.
Figure 7.18 Replacement of takes by registration and two relationship sets.
One possible guideline in determining whether to use an entity set or a relationship set is to designate a relationship set to describe an action that occurs between entities.
This approach can also be useful in deciding whether certain attributes may be more appropriately expressed as relationships.
Some relationships that appear to be nonbinary could actually be better represented by several binary relationships.
For instance, one could create a ternary relationship parent, relating a child to his/hermother and father.However, sucha relationship could alsobe represented by two binary relationships, mother and father, relating a child to his/her mother and father separately.
For simplicity, consider the abstract ternary (n = 3) relationship set R, relating entity sets A, B, and C.
We replace the relationship set R by an entity set E , and create three relationship sets as shown in Figure 7.19:
If the relationship set R had any attributes, these are assigned to entity set E ; further, a special identifying attribute is created for E (since it must be possible to distinguish different entities in an entity set on the basis of their attribute values)
For each relationship (ai , bi , ci ) in the relationship set R, we create a new entity ei in the entity set E.
Then, in each of the three new relationship sets, we insert a relationship as follows:
We can generalize this process in a straightforward manner to n-ary relationship sets.
Thus, conceptually, we can restrict the E-R model to include only binary relationship sets.
An identifying attribute may have to be created for the entity set created to represent the relationship set.
This attribute, alongwith the extra relationship sets required, increases the complexity of the design and (as we shall see in Section 7.6) overall storage requirements.
An n-ary relationship set shows more clearly that several entities participate in a single relationship.
There may not be a way to translate constraints on the ternary relationship into constraints on the binary relationships.
For example, consider a constraint that says that R is many-to-one from A, B to C ; that is, each pair of entities from Aand B is associated with at most one C entity.
This constraint cannot be expressed by using cardinality constraints on the relationship sets RA, RB , and RC.
Consider the relationship set proj guide in Section 7.2.2, relating instructor, student, and project.
We cannot directly split proj guide into binary relationships between instructor and project and between instructor and student.
Figure 7.20 date as an attribute of the student entity set.
The relationship set proj guide can be split into binary relationships by creating a new entity set as described above.However, doing sowould not be very natural.
The cardinality ratio of a relationship can affect the placement of relationship attributes.
Thus, attributes of one-to-one or one-to-many relationship sets can be associated with one of the participating entity sets, rather than with the relationship set.
For instance, let us specify that advisor is a one-to-many relationship set such that one instructor may advise several students, but each student can be advised by only a single instructor.
In this case, the attribute date, which specifies when the instructor became the advisor of a student, could be associated with the student entity set, as Figure 7.20 depicts.
To keep the figure simple, only some of the attributes of the two entity sets are shown.
Since each student entity participates in a relationship with at most one instance of instructor, making this attribute designation has the samemeaning as would placing datewith the advisor relationship set.
Attributes of a one-to-many relationship set can be repositioned to only the entity set on the “many” side of the relationship.
For one-to-one relationship sets, on the other hand, the relationship attribute can be associated with either one of the participating entities.
The design decision of where to place descriptive attributes in such cases —as a relationship or entity attribute—should reflect the characteristics of the enterprise being modeled.
The designer may choose to retain date as an attribute of advisor to express explicitly that the date refers to the advising relationship and not some other aspect of the student’s university status (for example, date of acceptance to the university)
The choice of attribute placement is more clear-cut for many-to-many relationship sets.
Returning to our example, let us specify the perhaps more realistic case that advisor is a many-to-many relationship set expressing that an instructor may advise one or more students, and that a student may be advised by one or more instructors.
If we are to express the date on which a specific instructor became the advisor of a specific student, date must be an attribute of the advisor relationship set, rather than either one of the participating entities.
If datewere an attribute of student, for instance, we could not determinewhich instructor became the advisor on that particular date.
When an attribute is determined by the combination of participating entity sets, rather than by either entity separately, that attribute must be associated with the many-to-many relationship set.
Figure 7.3 depicts the placement of date as a relationship attribute; again, to keep the figure simple, only some of the attributes of the two entity sets are shown.
Although the basic E-R concepts can model most database features, some aspects of a database may be more aptly expressed by certain extensions to the basic E-R model.
In this section, we discuss the extended E-R features of specialization, generalization, higher- and lower-level entity sets, attribute inheritance, and aggregation.
To help with the discussions, we shall use a slightly more elaborate database schema for the university.
In particular, we shall model the various people within a university by defining an entity set person, with attributes ID, name, and address.
An entity set may include subgroupings of entities that are distinct in some way from other entities in the set.
For instance, a subset of entities within an entity set may have attributes that are not shared by all the entities in the entity set.
The E-R model provides a means for representing these distinctive entity groupings.
As an example, the entity set person may be further classified as one of the following:
Each of these person types is described by a set of attributes that includes all the attributes of entity set person plus possibly additional attributes.
For example, employee entities may be described further by the attribute salary, whereas student entities may be described further by the attribute tot cred.
The process of designating subgroupings within an entity set is called specialization.
The specialization of person allows us to distinguish among person entities according to whether they correspond to employees or students: in general, a person could be an employee, a student, both, or neither.
As another example, suppose the university divides students into two categories: graduate andundergraduate.
Each of these student types is described by a set of attributes that includes all the attributes of the entity set student plus additional attributes.
As we saw earlier, student entities are described by the attributes ID, name, address, and tot cred.
The entity set graduatewould have all the attributes of student and an additional attribute office number.
The entity set undergraduate would have all the attributes of student, and an additional attribute residential college.
For instance, university employees may be further classified as one of the following:
Each of these employee types is described by a set of attributes that includes all the attributes of entity set employee plus additional attributes.
For example, instructor entities may be described further by the attribute rank while secretary entities are described by the attribute hours per week.
Further, secretary entitiesmay participate in a relationship secretary for between the secretary and employee entity sets, which identifies the employees who are assisted by a secretary.
An entity set may be specialized by more than one distinguishing feature.
In our example, the distinguishing feature among employee entities is the job the employee performs.
Another, coexistent, specialization could be based on whether the person is a temporary (limited term) employee or a permanent employee, resulting in the entity sets temporary employee and permanent employee.
When more than one specialization is formed on an entity set, a particular entity may belong to multiple specializations.
For instance, a given employee may be a temporary employee who is a secretary.
In terms of an E-R diagram, specialization is depicted by a hollow arrow-head pointing from the specialized entity to the other entity (see Figure 7.21)
We refer to this relationship as the ISA relationship, which stands for “is a” and represents, for example, that an instructor “is a” employee.
The way we depict specialization in an E-R diagram depends on whether an entity may belong to multiple specialized entity sets or if it must belong to at most one specialized entity set.
The former case (multiple sets permitted) is called overlapping specialization, while the latter case (at most one permitted) is called disjoint specialization.
For anoverlapping specialization (as is the case for student and employee as specializations of person), two separate arrows are used.
For a disjoint specialization (as is the case for instructor and secretary as specializations of employee), a single arrow is used.
The design process may also proceed in a bottom-upmanner, in which multiple entity sets are synthesized into a higher-level entity set on the basis of common features.
There are similarities between the instructor entity set and the secretary entity set in the sense that they have several attributes that are conceptually the same across the two entity sets: namely, the identifier, name, and salary attributes.
This commonality can be expressed by generalization, which is a containment relationship that exists between a higher-level entity set and one ormore lower-level entity sets.
In our example, employee is the higher-level entity set and instructor and secretary are lower-level entity sets.
In this case, attributes that are conceptually the same had different names in the two lower-level entity sets.
To create a generalization, the attributes must be given a common name and represented with the higher-level entity person.
We can use the attribute names ID, name, address, as we saw in the example in Section 7.8.1
Higher- and lower-level entity sets also may be designated by the terms superclass and subclass, respectively.
The person entity set is the superclass of the employee and student subclasses.
For all practical purposes, generalization is a simple inversion of specialization.
We apply both processes, in combination, in the course of designing the E-R schema for an enterprise.
In terms of the E-R diagram itself, we do not distinguish between specialization and generalization.
New levels of entity representation are distinguished (specialization) or synthesized (generalization) as the design schema comes to express fully the database application and the user requirements of the database.
Differences in the two approaches may be characterized by their starting point and overall goal.
Specialization stems from a single entity set; it emphasizes differences among entities within the set by creating distinct lower-level entity sets.
These lowerlevel entity sets may have attributes, or may participate in relationships, that do not apply to all the entities in the higher-level entity set.
Indeed, the reason a designer applies specialization is to represent such distinctive features.
If student and employee have exactly the same attributes as person entities, and participate in exactly the same relationships as person entities, there would be no need to specialize the person entity set.
Generalization proceeds from the recognition that a number of entity sets share some common features (namely, they are described by the same attributes and participate in the same relationship sets)
On the basis of their commonalities, generalization synthesizes these entity sets into a single, higher-level entity set.
Generalization is used to emphasize the similarities among lower-level entity sets and to hide the differences; it also permits an economy of representation in that shared attributes are not repeated.
A crucial property of the higher- and lower-level entities created by specialization and generalization is attribute inheritance.
The attributes of the higher-level entity sets are said to be inherited by the lower-level entity sets.
For example, student and employee inherit the attributes of person.
Thus, student is described by its ID, name, and address attributes, and additionally a tot cred attribute; employee is described by its ID, name, and address attributes, and additionally a salary attribute.
Attribute inheritance applies through all tiers of lower-level entity sets; thus, instructor and secretary, which are subclasses of employee, inherit the attributes ID, name, and address from person, in addition to inheriting salary from employee.
A lower-level entity set (or subclass) also inherits participation in the relationship sets inwhich its higher-level entity (or superclass) participates.
Like attribute inheritance, participation inheritance applies through all tiers of lower-level entity sets.
For example, suppose the person entity set participates in a relationship person deptwith department.
Then, the student, employee, instructor and secretary entity sets, which are subclasses of the person entity set, also implicitly participate in the person dept relationship with department.
The above entity sets can participate in any relationships in which the person entity set participates.
Whether a given portion of an E-R model was arrived at by specialization or generalization, the outcome is basically the same:
A higher-level entity set with attributes and relationships that apply to all of its lower-level entity sets.
Lower-level entity sets with distinctive features that apply only within a particular lower-level entity set.
Inwhat follows, althoughwe often refer to only generalization, the properties that we discuss belong fully to both processes.
In the figure, employee is a lower-level entity set of person and a higher-level entity set of the instructor and secretary entity sets.
In a hierarchy, a given entity set may be involved as a lowerlevel entity set in only one ISA relationship; that is, entity sets in this diagram have only single inheritance.
If an entity set is a lower-level entity set in more than one ISA relationship, then the entity set has multiple inheritance, and the resulting structure is said to be a lattice.
To model an enterprise more accurately, the database designer may choose to place certain constraints on a particular generalization.
One type of constraint involves determiningwhich entities can be members of a given lower-level entity set.
In condition-defined lower-level entity sets,membership is evaluated on the basis of whether or not an entity satisfies an explicit condition or predicate.
For example, assume that the higher-level entity set student has the attribute student type.
All student entities are evaluated on the defining student type attribute.
Only those entities that satisfy the condition student type = “graduate” are allowed to belong to the lower-level entity set graduate student.
All entities that satisfy the condition student type = “undergraduate” are included in undergraduate student.
Since all the lower-level entities are evaluated on the basis of the same attribute (in this case, on student type), this type of generalization is said to be attribute-defined.
User-defined lower-level entity sets are not constrained by a membership condition; rather, the database user assigns entities to a given entity set.
For instance, let us assume that, after 3 months of employment, university employees are assigned to one of four work teams.
We therefore represent the teams as four lower-level entity sets of the higher-level employee entity set.
A given employee is not assigned to a specific team entity automatically on the basis of an explicit defining condition.
Instead, the user in charge of this decision makes the team assignment on an individual basis.
The assignment is implemented by an operation that adds an entity to an entity set.
A second type of constraint relates to whether or not entities may belong to more than one lower-level entity set within a single generalization.
The lowerlevel entity sets may be one of the following:
A disjointness constraint requires that an entity belong to no more than one lower-level entity set.
In our example, student entity can satisfy only one condition for the student type attribute; an entity can be either a graduate student or an undergraduate student, but cannot be both.
In overlapping generalizations, the same entity may belong to more than one lower-level entity set within a single generalization.
For an illustration, consider the employee work-team example, and assume that certain employees participate inmore than onework team.Agiven employee may therefore appear in more than one of the team entity sets that are lowerlevel entity sets of employee.
In Figure 7.21, we assume a person may be both an employee and a student.
We show this overlapping generalization via separate arrows: one from employee to person and another from student to person.
Each higher-level entity must belong to a lower-level entity set.
Some higher-level entities may not belong to any lower-level entity set.
We can specify total generalization in an E-R diagramby adding the keyword “total” in the diagram and drawing a dashed line from the keyword to the corresponding hollow arrow-head to which it applies (for a total generalization), or to the set of hollow arrow-heads to which it applies (for an overlapping generalization)
The student generalization is total: All student entitiesmust be either graduate or undergraduate.
Because the higher-level entity set arrived at through generalization is generally composed of only those entities in the lower-level entity sets, the completeness constraint for a generalized higher-level entity set is usually total.
When the generalization is partial, a higher-level entity is not constrained to appear in a lower-level entity set.
Since employees are assigned to a team only after 3 months on the job, some employee entities may not be members of any of the lower-level team entity sets.
We may characterize the team entity sets more fully as a partial, overlapping specialization of employee.
The completeness and disjointness constraints, however, do not depend on each other.
We can see that certain insertion and deletion requirements follow from the constraints that apply to a given generalization or specialization.
For instance, when a total completeness constraint is in place, an entity inserted into a higherlevel entity set must also be inserted into at least one of the lower-level entity sets.
With a condition-defined constraint, all higher-level entities that satisfy the condition must be inserted into that lower-level entity set.
Finally, an entity that is deleted from a higher-level entity set also is deleted from all the associated lower-level entity sets to which it belongs.
One limitation of the E-R model is that it cannot express relationships among relationships.
To illustrate the need for such a construct, consider the ternary relationship proj guide, which we saw earlier, between an instructor, student and project (see Figure 7.13)
Now suppose that each instructor guiding a student on a project is required to file amonthly evaluation report.Wemodel the evaluation report as an entity evaluation, with a primary key evaluation id.
One alternative for recording the (student, project, instructor) combination to which an evaluation corresponds is to create a quaternary (4-way) relationship set eval for between instructor, student, project, and evaluation.
A quaternary relationship is required—a binary relationship between student and evaluation, for example, would not permit us to represent the (project, instructor) combination to which an evaluation corresponds.
Using the basic E-R modeling constructs, we obtain the E-R diagram of Figure 7.22
We have omitted the attributes of the entity sets, for simplicity.
It appears that the relationship sets proj guide and eval for can be combined into one single relationship set.
Nevertheless, we should not combine them into a single relationship, since some instructor, student, project combinations may not have an associated evaluation.
There is redundant information in the resultant figure, however, since every instructor, student, project combination in eval for must also be in proj guide.
If the evaluation were a value rather than a entity, we could instead make evaluation a multivalued composite attribute of the relationship set proj guide.
However, this alternative may not be be an option if an evaluation may also be related to other entities; for example, each evaluation report may be associated with a secretary who is responsible for further processing of the evaluation report to make scholarship payments.
The best way to model a situation such as the one just described is to use aggregation.
Aggregation is an abstraction through which relationships are treated as higher-level entities.
Thus, for our example, we regard the relationship set proj guide (relating the entity sets instructor, student, and project) as a higher-level entity set called proj guide.
Such an entity set is treated in the same manner as is any other entity set.
We can then create a binary relationship eval for between proj.
Figure 7.23 shows a notation for aggregation commonly used to represent this situation.
We are in a position now to describe how the extended E-R features can be translated into relation schemas.
We assume that ID is the primary key of person.
For each lower-level entity set, create a schema that includes an attribute for each of the attributes of that entity set plus one for each attribute of the primary key of the higher-level entity set.
Thus, for the E-R diagram of Figure 7.21 (ignoring the instructor and secretary entity sets) we have three schemas:
The primary-key attributes of the higher-level entity set become primarykey attributes of the higher-level entity set as well as all lower-level entity sets.
In addition, we create foreign-key constraints on the lower-level entity sets, with their primary-key attributes referencing the primary key of the relation created from the higher-level entity set.
In the above example, the ID attribute of employee would reference the primary key of person, and similarly for student.
An alternative representation is possible, if the generalization is disjoint and complete—that is, if no entity is a member of two lower-level entity sets directly below a higher-level entity set, and if every entity in the higher-level entity set is also a member of one of the lower-level entity sets.
Here, we do not create a schema for the higher-level entity set.
Instead, for each lowerlevel entity set, we create a schema that includes an attribute for each of the attributes of that entity set plus one for each attribute of the higher-level entity set.
Then, for the E-R diagram of Figure 7.21, we have two schemas:
Both these schemas have ID, which is the primary-key attribute of the higherlevel entity set person, as their primary key.
One drawback of the second method lies in defining foreign-key constraints.
To illustrate the problem, suppose we had a relationship set R involving entity set person.
With the first method, when we create a relation schema R from the relationship set, we would also define a foreign-key constraint on R, referencing the schema person.
To avoid this problem, we need to create a relation schema person containing at least the primary-key attributes of the person entity.
If the second method were used for an overlapping generalization, some values would be stored multiple times, unnecessarily.
For instance, if a person is both an employee and a student, values for street and city would be stored twice.
If the generalization were disjoint but not complete—that is, if some person is neither an employee nor a student—then an extra schema.
As an attempt to work around the problem, suppose employees and students are additionally represented in the person relation.
Unfortunately, name, street, and city information would then be stored redundantly in the person relation and the student relation for students, and similarly in the person relation and the employee relation for employees.
That suggests storing name, street, and city information only in the person relation and removing that information from student and employee.
If we do that, the result is exactly the first method we presented.
Designing schemas for an E-R diagram containing aggregation is straightforward.
The schema for the relationship set eval for between the aggregation of proj guide and the entity set evaluation includes an attribute for each attribute in the primary keys of the entity set evaluation, and the relationship set proj guide.
It also includes an attribute for any descriptive attributes, if they exist, of the relationship set eval for.
We then transform the relationship sets and entity sets within the aggregated entity set following the rules we have already defined.
The ruleswe saw earlier for creating primary-key and foreign-key constraints on relationship sets can be applied to relationship sets involving aggregations as well, with the aggregation treated like any other entity set.
The primary key of the aggregation is the primary key of its defining relationship set.
No separate relation is required to represent the aggregation; the relation created from the defining relationship is used instead.
A diagrammatic representation of the data model of an application is a very important part of designing a database schema.
An intuitive diagrammatic representation is particularly important since it eases communication of information between these groups of experts.
A number of alternative notations for modeling data have been proposed, of which E-R diagrams and UML class diagrams are the most widely used.
There is no universal standard for E-R diagram notation, and different books and E-R diagram software use different notations.
In the rest of this section, we study some of the alternative E-R diagram notations, as well as the UML class diagram notation.
To aid in comparison of our notation with these alternatives, Figure 7.24 summarizes the set of symbols we have used in our E-R diagram notation.
Figure 7.25 indicates some of the alternative E-R notations that are widely used.
One alternative representation of attributes of entities is to show them in ovals connected to the box representing the entity; primary key attributes are indicated by underlining them.
The above notation is shown at the top of the figure.
Relationship attributes can be similarly represented, by connecting the ovals to the diamond representing the relationship.
In another alternative notation shown on the right side of the figure, relationship sets are represented by lines between entity sets, without diamonds; only binary relationships can be modeled thus.
Cardinality constraints in such a notation are shown by “crow’s-foot” notation, as in the figure.
Total participation is specified in this notation by a vertical bar.
Similarly, partial participation is indicated by using a circle, again on the opposite side.
The bottom part of Figure 7.25 shows an alternative representation of generalization, using triangles instead of hollow arrow-heads.
In prior editions of this text up to the fifth edition, we used ovals to represent attributes, with triangles representing generalization, as shown in Figure 7.25
The notation using ovals for attributes and diamonds for relationships is close to the original form of E-R diagrams used by Chen in his paper that introduced the notion of E-R modeling.
IDEF1X uses the crow’s-foot notation, with vertical bars on the relationship edge to denote total participation and hollow circles to denote partial participation, and includes other notations that we have not shown.
In comparison with our previous notation, our new notation provides a more compact representation of attributes, and is also closer to the notation supported by many E-R modeling tools, in addition to being closer to the UML class diagram notation.
There are a variety of tools for constructing E-R diagrams, each of which has its own notational variants.
Some of the tools even provide a choice between several E-R notation variants.
See the references in the bibliographic notes for more information.
One key difference between entity sets in an E-R diagram and the relation schemas created from such entities is that attributes in the relational schema corresponding to E-R relationships, such as the dept name attribute of instructor, are not shown in the entity set in the E-R diagram.
Some data modeling tools allow users to choose between two views of the same entity, one an entity view without such attributes, and other a relational view with such attributes.
Data representation, however, forms only one part of an overall system design.
Other components include models of user interactions with the system, specification of functional modules of the system and their interaction, etc.
The Unified Modeling Language (UML) is a standard developed under the auspices of the Object Management Group (OMG) for creating specifications of various components of a software system.
Later in this section we illustrate a few features of class diagrams and how they relate to E-R diagrams.
Use case diagrams show the interaction between users and the system, in particular the steps of tasks that users perform (such as withdrawing money or registering for a course)
Activity diagrams depict the flow of tasks between various components of a system.
Implementation diagrams show the system components and their interconnections, both at the software component level and the hardware component level.
We do not attempt to provide detailed coverage of the different parts of UML here.
Insteadwe illustrate some features of that part of UML that relates to data modeling through examples.
Figure 7.26 shows several E-R diagram constructs and their equivalent UML class diagram constructs.
Objects are like entities, and have attributes, but additionally provide a set of functions (called methods) that can be invoked to compute values on the basis of attributes of the objects, or to update the object itself.
Since classes support encapsulation, UML allows attributes and methods to be prefixed with a “+”, “-”, or “#”, which denote respectively public, private and protected access.
Private attributes can only be used inmethods of the class, while protected attributes can be used only in methods of the class and its subclasses; these should be familiar to anyone who knows Java, C++ or C#
In UML terminology, relationship sets are referred to as associations; we shall refer to them as relationship sets for consistency with E-R terminology.
We represent binary relationship sets in UML by just drawing a line connecting the entity sets.
We write the relationship set name adjacent to the line.
We may also specify the role played by an entity set in a relationship set by writing the role name on the line, adjacent to the entity set.
Alternatively, we may write the relationship set name in a box, along with attributes of the relationship set, and connect the.
Figure 7.26 Symbols used in the UML class diagram notation.
This box can then be treated as an entity set, in the same way as an aggregation in E-R diagrams, and can participate in relationships with other entity sets.
Since UML version 1.3, UML supports nonbinary relationships, using the same diamond notation used in E-R diagrams.
Nonbinary relationships could not be directly represented in earlier versions of UML—they had to be converted to binary relationships by the technique we have seen earlier in Section 7.7.3
For example, a line between two entity sets with a small diamond at one end specifies that the entity on the diamond side contains the other entity (containment is called “aggregation” in UML terminology; do not confuse this use of aggregationwith the sense inwhich it is used in the E-Rmodel)
For example, a vehicle entity may contain an engine entity.
See the references in the bibliographic notes for more information on UML class diagrams.
Our extensive discussion of schema design in this chapter may create the false impression that schema design is the only component of a database design.
There are indeed several other considerations that we address more fully in subsequent chapters, and survey briefly here.
We have seen a variety of data constraints that can be expressed using SQL, including primary-key constraints, foreign-key constraints, check constraints, assertions, and triggers.
The most obvious one is the automation of consistency preservation.
By expressing constraints in the SQL data-definition language, the designer is able to ensure that the database system itself enforces the constraints.
This is more reliable than relying on each application program individually to enforce constraints.
It also provides a central location for the update of constraints and the addition of new ones.
A further advantage of stating constraints explicitly is that certain constraints are particularly useful in designing relational database schemas.
If we know, for example, that a social-security number uniquely identifies a person, then we can use a person’s social-security number to link data related to that person even if these data appear in multiple relations.
Contrast that with, for example, eye color, which is not a unique identifier.
Eye color could not be used to link data pertaining to a specific person across relations because that person’s data could not be distinguished fromdata pertaining to other peoplewith the same eye color.
In Section 7.6, we generated a set of relation schemas for a given E-R design using the constraints specified in the design.
In Chapter 8, we formalize this idea and related ones, and show how they can assist in the design of relational.
The formal approach to relational database design allows us to state in a precise manner when a given design is a good one and to transform poor designs into better ones.
Data constraints are useful as well in determining the physical structure of data.
Itmay be useful to store data that are closely related to each other in physical proximity on disk so as to gain efficiencies in disk access.
Certain index structures work better when the index is on a primary key.
Constraint enforcement comes at a potentially high price in performance each time the database is updated.
For each update, the system must check all of the constraints and either reject updates that fail the constraints or execute appropriate triggers.
The significance of the performance penalty depends not only on the frequency of update but also on how the database is designed.
Database system performance is a critical aspect of most enterprise information systems.
Performance pertains not only to the efficient use of the computing and storage hardware being used, but also to the efficiency of people who interact with the system and of processes that depend upon database data.
Throughput—the number of queries or updates (often referred to as transactions) that can be processed on average per unit of time.
Response time—the amount of time a single transaction takes from start to finish in either the average case or the worst case.
Systems that process largenumbers of transactions in abatch style focus onhaving high throughput.
Systems that interact with people or time-critical systems often focus on response time.
High throughput arises from obtaining high utilization of system components.
Doing somay result in certain transactions being delayed until such time that they can be run more efficiently.
Most commercial database systems historically have focused on throughput; however, a variety of applications includingWeb-based applications and telecommunication information systems require good response time on average and a reasonable bound on worst-case response time.
Anunderstanding of types of queries that are expected to be themost frequent helps in the design process.
Queries that involve joins require more resources to evaluate than those that do not.
In cases where a join is required, the database administratormay choose to create an index that facilitates evaluation of that join.
For queries—whether a join is involved or not—indices can be created to speed evaluation of selection predicates (SQL where clause) that are likely to appear.
Another aspect of queries that affects the choice of indices is the relative mix of update and read operations.
While an index may speed queries, it also slows updates, which are forced to do extra work to maintain the accuracy of the index.
Authorization constraints affect design of the database aswell because SQL allows access to be granted to users on the basis of components of the logical design of the database.
A relation schema may need to be decomposed into two or more schemas to facilitate the granting of access rights in SQL.
For example, an employee record may include data relating to payroll, job functions, and medical benefits.
Because different administrative units of the enterprise may manage each of these types of data, some users will need access to payroll data while being denied access to the job data, medical data, etc.
If these data are all in one relation, the desired division of access, though still feasible through the use of views, is more cumbersome.
Database applications are often part of a larger enterprise application that interacts not only with the database system but also with various specialized applications.
For example, in a manufacturing company, a computer-aided design (CAD) system may assist in the design of new products.
The CAD system may extract data from the database via an SQL statement, process the data internally, perhaps interacting with a product designer, and then update the database.
During this process, control of the data may pass among several product designers as well as other people.
It is created by an employee returning from a business trip (possibly by means of a special software package) and is subsequently routed to the employee’s manager, perhaps other higher-level managers, and eventually to the accounting department for payment (at which point it interacts with the enterprise’s accounting information systems)
In addition to the data on which workflows operate, the database may store data about the workflow itself, including the tasks making up a workflow and how they are to be routed among users.
Workflows thus specify a series of queries and updates to the database that may be taken into account as part of the databasedesign process.
Put in other terms, modeling the enterprise requires us not only to understand the semantics of the data but also the business processes that use those data.
The needs of an organization evolve continually, and the data that it needs to store also evolve correspondingly.
During the initial database-design phases, or during the development of an application, the database designer may realize that changes are required at the conceptual, logical, or physical schema levels.
Changes in the schema can affect all aspects of the database application.
A good database design anticipates future needs of an organization, and ensures that the schema requires minimal changes as the needs evolve.
It is important to distinguish between fundamental constraints that are expected to be permanent and constraints that are anticipated to change.
For example, the constraint that an instructor-id identify a unique instructor is fundamental.
On the other hand, a university may have a policy that an instructor can have only one department, which may change at a later date if joint appointments are allowed.
A database design that only allows one department per instructor might requiremajor changes if joint appointments are allowed.
Such joint appointments can be represented by adding an extra relationship, without modifying the instructor relation, as long as each instructor has only one primary department affiliation; a policy change that allows more than one primary affiliation may require a larger change in the database design.
A good design should account not only for current policies, but should also avoid or minimize changes due to changes that are anticipated, or have a reasonable chance of happening.
Furthermore, the enterprise that the database is serving likely interacts with other enterprises and, therefore, multiple databases may need to interact.
Conversion of data between different schemas is an important problem in real-world applications.
The XML data model, which we study in Chapter 23, is widely used for representing data when it is exchanged between different applications.
Database design mainly involves the design of the database schema.
It provides a convenient graphical representation to view data, relationships, and constraints.
The E-R model is intended primarily for the database-design process.
It was developed to facilitate database design by allowing the specification of an.
Such a schema represents the overall logical structure of the database.
This overall structure can be expressed graphically by an E-R diagram.
An entity is an object that exists in the real world and is distinguishable from other objects.
We express the distinction by associating with each entity a set of attributes that describes the object.
A relationship set is a collection of relationships of the same type, and an entity set is a collection of entities of the same type.
The terms superkey, candidate key, and primary key apply to entity and relationship sets as they do for relation schemas.
Identifying the primary key of a relationship set requires some care, since it is composed of attributes from one or more of the related entity sets.
Mapping cardinalities express the number of entities towhich another entity can be associated via a relationship set.
An entity set that does not have sufficient attributes to form a primary key is termed a weak entity set.
An entity set that has a primary key is termed a strong entity set.
The various features of the E-R model offer the database designer numerous choices in how to best represent the enterprise being modeled.
Concepts and objects may, in certain cases, be represented by entities, relationships, or attributes.
Aspects of the overall structure of the enterprise may be best described by using weak entity sets, generalization, specialization, or aggregation.
Often, the designermust weigh themerits of a simple, compactmodel versus those of a more precise, but more complex, one.
A database design specified by an E-R diagram can be represented by a collection of relation schemas.
For each entity set and for each relationship set in the database, there is a unique relation schema that is assigned the name of the corresponding entity set or relationship set.
This forms the basis for deriving a relational database design from an E-R diagram.
Specialization and generalization define a containment relationship between a higher-level entity set and one or more lower-level entity sets.
Specialization is the result of taking a subset of a higher-level entity set to form a lower-level entity set.
Generalization is the result of taking the union of two or more disjoint (lower-level) entity sets to produce a higher-level entity set.
The attributes of higher-level entity sets are inherited by lower-level entity sets.
Aggregation is an abstraction in which relationship sets (along with their associated entity sets) are treated as higher-level entity sets, and can participate in relationships.
Each car has associated with it zero to any number of recorded accidents.
Each insurance policy covers one or more cars, and has one or more premium payments associated with it.
Each payment is for a particular period of time, and has an associated due date, and the date when the payment was received.
Construct an E-R diagram that models exams as entities, and uses a ternary relationship, for the database.
Construct an alternative E-R diagram that uses only a binary relationship between student and section.
Make sure that only one relationship exists between a particular student and section pair, yet you can represent the marks that a student gets in different exams.
You should store thematches played, the scores in eachmatch, the players in each match, and individual player statistics for each match.
Why is allowing this redundancy a bad practice that one should avoid?
What do the following mean in terms of the structure of an enterprise schema?
Show a simple instance of E, A, B,C , RA, RB , and RC that cannot correspond to any instance of A, B,C , and R.
Modify the E-R diagram of Figure 7.27b to introduce constraints that will guarantee that any instance of E, A, B,C , RA, RB , and RC that satisfies the constraints will correspond to an instance of A, B,C , and R.
Modify the translation above to handle total participation constraints on the ternary relationship.
The above representation requires that we create a primary-key attribute for E.
Show how to treat E as a weak entity set so that a primary-key attribute is not required.
Outline what sort of redundancy will result if we do so.
Do the primary and foreign key constraints created on the relation enforce the many-to-one cardinality constraint? Explain why.
What extra constraints are required on the relation advisorto ensure that the one-to-one cardinality constraint is enforced?
In SQL, attributes participating in a foreign key constraint can be null.
Explain how a constraint on total participation of A in R can be enforced using not null constraints in SQL.
As a result, total participation constraints on a many-to-many relationship (or on the “one” side of a one-to-many relationship) cannot be enforced on the relations created from the relationship, using primary key, foreign key and not null constraints on the relations.
Explain how to enforce total participation constraints using complex check constraints or assertions (see Section 4.4.7)
Unfortunately, these features are not supported on any widely used database currently.
For entity sets A, B, and C , explain how attributes.
Discuss how to handle a case where an attribute of X has the same name as some attribute of Y.
Suppose we wish to track temporal changes, that is, changes to data over time.
Similarly, attribute values of an entity or relationship, such as title and credits of course, salary, or even name of instructor, and tot cred of student, can change over time.
We define a new data type called valid time, which is a time-interval, or a set of time-intervals.
We then associate a valid time attribute with each entity and relationship, recording the time periods during which the entity or relationship is valid.
Similarly, we model attributes that can change over time as a set of values, each with its own valid time.
Draw an E-R diagram with the student and instructor entities, and the advisor relationship, with the above extensions to track temporal changes.
Convert the above E-R diagram into a set of relations.
It should be clear that the set of relations generated above is rather complex, leading to difficulties in tasks such as writing queries in SQL.
An alternative approach, which is used more widely is to ignore temporal changes when designing the E-R model (in particular, temporal changes to attribute values), and to modify the relations generated from the E-R model to track temporal changes, as discussed later in Section 8.9
Associate with each patient a log of the various tests and examinations conducted.
Suppose the bookstore adds Blu-ray discs and downloadable video to its collection.
The same itemmay be present in one or both formats, with differing prices.
Extend the E-R diagram to model this addition, ignoring the effect on shopping baskets.
Now extend the E-R diagram, using generalization, to model the case where a shopping basket may contain any combination of books, Blu-ray discs, or downloadable video.
Each vehicle is identified by a vehicle identification number (VIN)
Each individual vehicle is a particular model of a particular brand offered by the company (e.g., the XF is a model of the car brand Jaguar of Tata Motors)
Each model can be offered with a variety of options, but an individual car may have only some (or none) of the available options.
The database needs to store information about models, brands, and options, as well as information about individual dealers, customers, and cars.
Each package must be identifiable and trackable, so the database must be able to store the location of the package and its history of locations.
The database must keep track of customers and their reservations, flights and their status, seat assignments on individual flights, and the schedule and routing of future flights.
Discuss the relative merits of these two alternative representations of a ternary relationship by binary relationships.
For each schema, specifywhat foreignkey constraints, if any, should be created.
Justify your placement of attributes at each level of the hierarchy.
Explain why they should not be placed at a higher or lower level.
Which of these constraints can the system check automatically? Explain your answer.
Many database systems provide tools for database design that support E-R diagrams.
These tools help adesigner create E-Rdiagrams, and they canautomatically create corresponding tables in a database.
See bibliographic notes of Chapter 1 for references to database-system vendors’ Web sites.
The drawing tool Dia, which is available as freeware, supports E-R diagrams and UML class diagrams.
A logical designmethodology for relational databases using the extended E-Rmodel is presented by Teorey et al.
However, a variety of E-R notations are in use today.
Thalheim [2000] provides a detailed textbook coverage of research in E-R modeling.
See www.uml.org for more information on UML standards and tools.
In this chapter, we consider the problem of designing a schema for a relational database.Manyof the issues in doing so are similar to design issueswe considered in Chapter 7 using the E-R model.
In general, the goal of relational database design is to generate a set of relation schemas that allows us to store informationwithout unnecessary redundancy, yet also allows us to retrieve information easily.
This is accomplished by designing schemas that are in an appropriate normal form.
To determine whether a relation schema is in one of the desirable normal forms, we need information about the real-world enterprise that we are modeling with the database.
Some of this information exists in a well-designed E-R diagram, but additional information about the enterprise may be needed as well.
In this chapter, we introduce a formal approach to relational database design based on the notion of functional dependencies.
We then define normal forms in terms of functional dependencies and other types of data dependencies.
We saw in Section 7.6 that it is possible to generate a set of relation schemas directly from the E-R design.
Obviously, the goodness (or badness) of the resulting set of schemas depends on how good the E-R design was in the first place.
Later in this chapter, we shall study preciseways of assessing the desirability of a collection of relation schemas.
However, we can go a long way toward a good design using concepts we have already studied.
For ease of reference, we repeat the schemas for the university database in Figure 8.1
Now, let us explore features of this relational database design as well as some alternatives.
Suppose that instead of having the schemas instructor and department, we have the schema:
This represents the result of a natural join on the relations corresponding to instructor and department.
This seems like a good idea because some queries can be expressed using fewer joins, until we think carefully about the facts about the university that led to our E-R design.
Let us consider the instance of the inst dept relation shown in Figure 8.2.Notice that we have to repeat the department information (“building” and “budget”) once for each instructor in the department.
It is important that all these tuples agree as to the budget amount since otherwise our database would be inconsistent.
In our original design using instructor and department, we stored the amount of each budget exactly once.
This suggests that using inst dept is a bad idea since it stores the budget amounts redundantly and runs the risk that some user might update the budget amount in one tuple but not all, and thus create inconsistency.
Even if we decided to live with the redundancy problem, there is still another problem with the inst dept schema.
Suppose we are creating a new department in the university.
In the alternative design above, we cannot represent directly the information concerning a department (dept name, building, budget) unless that department has at least one instructor at the university.
This is because tuples in the inst dept table require values for ID,name, and salary.
Thismeans thatwe cannot record information about the newly created department until the first instructor.
In the old design, the schema department can handle this, but under the revised design, we would have to create a tuple with a null value for building and budget.
In some cases null values are troublesome, as we saw in our study of SQL.
However, if we decide that this is not a problem to us in this case, then we can proceed to use the revised design.
Suppose again that, somehow, we had started out with the schema inst dept.
How would we recognize that it requires repetition of information and should be split into the two schemas instructor and department?
By observing the contents of actual relations on schema inst dept, we could note the repetition of information resulting from having to list the building and budget once for each instructor associatedwith a department.
A real-world database has a large number of schemas and an even larger number of attributes.
It does not allow us to determinewhether the lack of repetition is just a “lucky” special case or whether it is a manifestation of a general rule.
In our example, howwouldwe know that in ouruniversity organization, each department (identified by its department name) must reside in a single building and must have a single budget amount? Is the fact that the budget amount for the Comp.
In particular, we would need to discover that the university requires that every department (identified by its department name) must have only one building and one budget value.
In the case of inst dept, our process of creating an E-R design successfully avoided the creation of this schema.
Therefore, we need to allow the database designer to specify rules such as “each specific value for dept name corresponds to at most one budget” even in cases where dept name is not the primary key for the schema in question.
In other words, we need to write a rule that says “if there were a schema (dept name, budget), then dept name is able to serve as the primary key.” This rule is specified as a functional dependency.
Given such a rule, we now have sufficient information to recognize the problem of the inst dept schema.
Because dept name cannot be the primary key for inst dept (because a department may need several tuples in the relation on schema inst dept), the amount of a budget may have to be repeated.
Observations such as these and the rules (functional dependencies in particular) that result from them allow the database designer to recognize situations where a schema ought to be split, or decomposed, into two or more schemas.
It is not hard to see that the right way to decompose inst dept is into schemas instructor and department as in the original design.
Finding the right decomposition is much harder for schemaswith a large number of attributes and several functional dependencies.
To deal with this, we shall rely on a formal methodology that we develop later in this chapter.
Consider an extreme case where all we had were schemas consisting of one attribute.
No interesting relationships of any kind could be expressed.Now consider a less extreme casewhere we choose to decompose the employee schema (Section 7.8):
The flaw in this decomposition arises from the possibility that the enterprise has two employees with the same name.
This is not unlikely in practice, as many cultures have certain highly popular names.
Of course each person would have a unique employee-id, which is why ID can serve as the primary key.
As an example, let us assume two employees, both named Kim, work at the university and have the following tuples in the relation on schema employee in the original design:
Figure 8.3 shows these tuples, the resulting tuples using the schemas resulting from the decomposition, and the result if we attempted to regenerate the original tuples using a natural join.
As we see in the figure, the two original tuples appear in the result along with two new tuples that incorrectly mix data values pertaining to the two employees named Kim.
Although we have more tuples, we actually have less information in the following sense.
We can indicate that a certain street, city, and salary pertain to someone named Kim, but we are unable to distinguish which of the Kims.
Thus, our decomposition is unable to represent certain important facts about the university employees.
We shall refer to such decompositions as being lossy decompositions, and, conversely, to those that are not as lossless decompositions.
The E-R model allows entity sets and relationship sets to have attributes that have some degree of substructure.
When we create tables from E-R designs that contain these types of attributes, we eliminate this substructure.
For composite attributes, we let each component be an attribute in its own right.
For multivalued attributes, we create one tuple for each item in amultivalued set.
In the relational model, we formalize this idea that attributes do not have any substructure.
A domain is atomic if elements of the domain are considered to be indivisible units.
We say that a relation schema R is in first normal form (1NF) if the domains of all attributes of R are atomic.
A set of names is an example of a nonatomic value.
For example, if the schema of a relation employee included an attribute children whose domain elements are sets of names, the schema would not be in first normal form.
Composite attributes, such as an attribute address with component attributes street, city, state, and zip also have nonatomic domains.
Integers are assumed to be atomic, so the set of integers is an atomic domain; however, the set of all sets of integers is a nonatomic domain.
The distinction is that we do not normally consider integers to have subparts, but we consider sets of integers to have subparts—namely, the integers making up the set.
But the important issue is not what the domain itself is, but rather how we use domain elements in our database.
The domain of all integers would be nonatomic if we considered each integer to be an ordered list of digits.
As a practical illustration of the above point, consider an organization that assigns employees identification numbers of the following form: The first two letters specify the department and the remaining four digits are a unique number within the department for the employee.
Such identification numbers can be divided into smaller units, and are therefore nonatomic.
If a relation schema had an attribute whose domain consists of identification numbers encoded as above, the schema would not be in first normal form.
When such identification numbers are used, the department of an employee can be found by writing code that breaks up the structure of an identification number.
Doing so requires extra programming, and information gets encoded in the application program rather than in the database.
Further problems arise if such identification numbers are used as primary keys: When an employee changes departments, the employee’s identification number must be changed everywhere it occurs, which can be a difficult task, or the code that interprets the number would give a wrong result.
From the above discussion, it may appear that our use of course identifiers such as “CS-101”, where “CS” indicates the Computer Science department, means that the domain of course identifiers is not atomic.
Such a domain is not atomic as far as humans using the system are concerned.
However, the database application still treats the domain as atomic, as long as it does not attempt to split the identifier and interpret parts of the identifier as a department abbreviation.
The course schema stores the department name as a separate attribute, and the database application can use this attribute value to find the department of a course, instead.
Thus, our university schema can be considered to be in first normal form.
The use of set-valued attributes can lead to designswith redundant storage of data, which in turn can result in inconsistencies.
For instance, instead of having the relationship between instructors and sections being represented as a separate relation teaches, a database designermay be tempted to store a set of course section identifierswith each instructor and a set of instructor identifierswith each section.
The primary keys of section and instructor are used as identifiers.
Whenever data pertaining to which instructor teaches which section is changed, the update has to be performed at two places: in the set of instructors for the section, and the set of sections for the instructor.
Failure to perform both updates can leave the database in an inconsistent state.
Keeping only one of these sets, that either the set of instructors of a section, or the set of sections of an instructor, would avoid repeated information; however keeping only one of thesewould complicate some queries, and it is unclear which of the two to retain.
Some types of nonatomic values can be useful, although they should be used with care.
For example, composite-valued attributes are often useful, and setvalued attributes are also useful in many cases, which is why both are supported in the E-R model.
In many domains where entities have a complex structure, forcing a first normal form representation represents an unnecessary burden on the application programmer, who has to write code to convert data into atomic form.
There is also the runtime overhead of converting data back and forth from the atomic form.
Support for nonatomic values can thus be very useful in such domains.
In fact, modern database systems do support many types of nonatomic values, aswe shall see inChapter 22.However, in this chapterwe restrict ourselves to relations in first normal form and, thus, all domains are atomic.
In Section 8.1,we noted that there is a formalmethodology for evaluatingwhether a relational schema should be decomposed.
This methodology is based upon the concepts of keys and functional dependencies.
In discussing algorithms for relational database design, we shall need to talk about arbitrary relations and their schema, rather than talking only about examples.
Recalling our introduction to the relational model in Chapter 2, we summarize our notation here.
When we use a lowercase Greek letter, we are referring to a set.
A Roman letter is used when we wish to indicate that the set of attributes is definitely a schema.
When a set of attributes is a superkey, we denote it by K.
A superkey pertains to a specific relation schema, so we use the terminology “K is a superkey of r (R).”
In our examples, these names are intended to be realistic (for example, instructor), while in our definitions and algorithms, we use single letters, like r.
A relation, of course, has a particular value at any given time; we refer to that as an instance and use the term “instance of r”
When it is clear that we are talking about an instance, wemay use simply the relation name (for example, r )
A database models a set of entities and relationships in the real world.
There are usually a variety of constraints (rules) on the data in the real world.
For example, some of the constraints that are expected to hold in a university database are:
Each instructor and student is (primarily) associated with only one department.1
Each department has only one value for its budget, and only one associated building.
An instance of a relation that satisfies all such real-world constraints is called a legal instance of the relation; a legal instance of a database is one where all the relation instances are legal instances.
Some of the most commonly used types of real-world constraints can be represented formally as keys (superkeys, candidate keys and primary keys), or as functional dependencies, which we define below.
An instructor or a student can be associated with more than one department, for example as an adjunct faculty, or as a minor department.
Our simplified university schema models only the primary department associated with each instructor or student.
A real university schema would capture secondary associations in other relations.
Clearly, if no two tuples in r have the same value on K , then a K -value uniquely identifies a tuple in r.
Functional dependencies allow us to express constraints that we cannot express with superkeys.
We denote the fact that the pair of attributes (ID, dept name) forms a superkey for inst dept by writing:
To test instances of relations to see whether they satisfy a given set F of functional dependencies.
To specify constraints on the set of legal relations.We shall thus concern ourselves with only those relation instances that satisfy a given set of functional dependencies.
If we wish to constrain ourselves to relations on schema r (R) that satisfy a set F of functional dependencies, we say that F holds on r (R)
Given that a set of functional dependencies F holds on a relation r (R), it may be possible to infer that certain other functional dependencies must also hold on.
We will use the notation F+ to denote the closure of the set F , that is, the set of all functional dependencies that can be inferred given the set F.
Clearly F+ contains all of the functional dependencies in F.
A database design is in BCNF if each member of the set of relation schemas that constitutes the design is in BCNF.
We have already seen in Section 8.1 an example of a relational schema that is not in BCNF:
In other words, there is no nontrivial functional dependency with any combination of name, dept name, and salary, without ID, on the side.
Similarly, the department schema is in BCNF because all of the nontrivial functional dependencies that hold, such as:
When we decompose a schema that is not in BCNF, it may be that one or more of the resulting schemas are not in BCNF.
In such cases, further decomposition is required, the eventual result of which is a set of BCNF schemas.
We have seen several ways in which to express database consistency constraints: primary-key constraints, functional dependencies, check constraints, assertions, and triggers.
Testing these constraints each time the database is updated can be costly and, therefore, it is useful to design the database in a way that constraints can be tested efficiently.
In particular, if testing a functional dependency can be done by considering just one relation, then the cost of testing this constraint is low.
We shall see that, in some cases, decomposition into BCNF can prevent efficient testing of certain functional dependencies.
To illustrate this, suppose that we make a small change to our university organization.
In the design of Figure 7.15, a student may have only one advisor.
This follows from the relationship set advisor being many-to-one from student to advisor.
The “small” change we shall make is that an instructor can be associated with only a single department and a student may have more than one advisor, but at most one from a given department.3
One way to implement this change using the E-R design is by replacing the advisor relationship set with a ternary relationship set, dept advisor, involving entity sets instructor, student, and department that is many-to-one from the pair.
Such an arrangement makes sense for students with a double major.
With this new E-R diagram, the schemas for the instructor, department, and student are unchanged.
Although not specified in the E-R diagram, suppose we have the additional constraint that “an instructor can act as advisor for only a single department.”
Notice that with this design, we are forced to repeat the department name once for each time an instructor participates in a dept advisor relationship.
We see that dept advisor is not in BCNF because i ID is not a superkey.
The first two alternatives are the same as the two alternatives in the definition of BCNF.
The third alternative of the 3NF definition seems rather unintuitive, and it is not obviouswhy it is useful.
Its purpose will become more clear later, when we study decomposition into 3NF.
Observe that any schema that satisfies BCNF also satisfies 3NF, since each of its functional dependencies would satisfy one of the first two alternatives.
Now, let us again consider the dept advisor relationship set, which has the following functional dependencies:
Technically, it is possible that a dependency whose attributes do not all appear in any one schema is still implicitly enforced, because of the presence of other dependencies that imply it logically.We address that case later, in Section 8.4.5
It is of historical significance only and is not used in practice.
The definition we use is equivalent but easier to understand.
These trade-offs are described in more detail in Section 8.5.4
Using functional dependencies to decompose schemas may not be sufficient to avoid unnecessary repetition of information in certain cases.
Consider a slight variation in the instructor entity-set definition in which we record with each instructor a set of children’s names and a set of phone numbers.
Thus, phone number and child name would be multivalued attributes and, following our rules for generating schemas from an E-R design, we would have two schemas, one for each of the multivalued attributes, phone number and child name:
As a result we might think that such a combination is a good idea.
However, such a combination is a bad idea, as we can see by considering the example of an instructor with two children and two phone numbers.
In the combined schema, we must repeat the phone numbers once for each dependent:
If we did not repeat the phone numbers, and stored only the first and last tuple,wewould have recorded the dependent names and the phone numbers, but.
Because normal forms based on functional dependencies are not sufficient to deal with situations like this, other dependencies and normal forms have been defined.
We have seen in our examples that it is useful to be able to reason systematically about functional dependencies as part of a process of testing schemas for BCNF or 3NF.
We shall see that, given a set F of functional dependencies on a schema, we can prove that certain other functional dependencies also hold on the schema.
We say that such functional dependencies are “logically implied” by F.
When testing for normal forms, it is not sufficient to consider the given set of functional dependencies; rather, we need to consider all functional dependencies that hold on the schema.
More formally, given a relational schema r (R), a functional dependency f on R is logically implied by a set of functional dependencies F on r if every instance of r (R) that satisfies F also satisfies f.
Suppose we are given a relation schema r (A, B, C , G, H, I ) and the set of functional dependencies:
We can use the following three rules to find logically implied functional dependencies.
By applying these rules repeatedly, we can find all of F+, given F.
This collection of rules is called Armstrong’s axioms in honor of the person who first proposed it.
Armstrong’s axioms are sound, because they do not generate any incorrect functional dependencies.
They are complete, because, for a given set F of functional dependencies, they allow us to generate all F+
The bibliographical notes provide references for proofs of soundness and completeness.
Figure 8.7 shows a procedure that demonstrates formally how to use Armstrong’s axioms to compute F+
In this procedure, when a functional dependency is added to F+, it may be already present, and in that case there is no change to F+
We shall see an alternative way of computing F+ in Section 8.4.2
To illustrate how the algorithmworks, we shall use it to compute (AG)+ with the functional dependencies defined in Section 8.4.1
The first time that we execute the repeat loop to test each functional dependency, we find that:
The second time that we execute the repeat loop, no new attributes are added to result, and the algorithm terminates.
It turns out that, in the worst case, this algorithm may take an amount of time quadratic in the size of F.
There is a faster (although slightly more complex) algorithm that runs in time linear in the size of F; that algorithm is presented as part of Practice Exercise 8.8
Suppose that we have a set of functional dependencies F on a relation schema.
Whenever a user performs an update on the relation, the database system must ensure that the update does not violate any functional dependencies, that is, all the functional dependencies in F are satisfied in the new database state.
The system must roll back the update if it violates any functional dependencies in the set F.
We can reduce the effort spent in checking for violations by testing a simplified set of functional dependencies that has the same closure as the given set.
Any database that satisfies the simplified set of functional dependencies also satisfies the original set, and vice versa, since the two sets have the same closure.
We shall see how the simplified set can be constructed in a moment.
A canonical cover Fc for F is a set of dependencies such that F logically implies all dependencies in Fc , and Fc logically implies all dependencies in F.
Each left side of a functional dependency in Fc is unique.
A canonical cover for a set of functional dependencies F can be computed as depicted in Figure 8.9
It is important to note that when checking if an attribute is extraneous, the check uses the dependencies in the current value of Fc , and not the dependencies in F.
The canonical cover of F , Fc , can be shown to have the same closure as F ; hence, testing whether Fc is satisfied is equivalent to testing whether F is satisfied.
However, Fc is minimal in a certain sense—it does not contain extraneous attributes, and it combines functional dependencies with the same left side.
It is cheaper to test Fc than it is to test F itself.
Consider the following set F of functional dependencies on schema (A, B,C):
There are two functional dependencies with the same set of attributes on the left side of the arrow:
Given a set F of functional dependencies, it may be that an entire functional dependency in the set is extraneous, in the sense that dropping it does not change the closure of F.
We can show that a canonical cover Fc of F contains no such extraneous functional dependency.
Suppose that, to the contrary, there were such an extraneous functional dependency in Fc.
The right-side attributes of the dependency would then be extraneous, which is not possible by the definition of canonical covers.
As an exercise, can you find one more canonical cover for F ?
Let r (R) be a relation schema, and let F be a set of functional dependencies on r (R)
More precisely, we say the decomposition is lossless if, for all legal database instances (that is, database instances that satisfy the specified functional dependencies and other constraints), relation r contains the same set of tuples as the result of the following SQL query:
This is stated more succinctly in the relational algebra as:
A decomposition that is not a lossless decomposition is called a lossy decomposition.
The terms lossless-join decomposition and lossy-join decomposition are sometimes used in place of lossless decomposition and lossy decomposition.
As an example of a lossy decomposition, recall the decomposition of the employee schema into:
For the general case of decomposition of a schema into multiple schemas at once, the test for lossless decomposition ismore complicated.
While the test for binary decomposition is clearly a sufficient condition for lossless decomposition, it is a necessary condition only if all constraints are functional dependencies.
We shall see other types of constraints later (in particular, a type of constraint called multivalued dependencies discussed in Section 8.6.1), that can ensure that a decomposition is lossless even if no functional dependencies are present.
Using the theory of functional dependencies, it is easier to characterize dependency preservation than using the ad-hoc approach we took in Section 8.3.3
The restriction of F to Ri is the set Fi of all functional dependencies in F+ that include only attributes of Ri.
Since all functional dependencies in a restriction involve attributes of only one relation schema, it is possible to test such a dependency for satisfaction by checking only one relation.
First, note that if each member of F can be tested on one of the relations of the decomposition, then the decomposition is dependency preserving.
This is an easy way to show dependency preservation; however, it does not always work.
There are cases where, even though the decomposition is dependency preserving, there is a dependency inF that cannot be tested in anyone relation in thedecomposition.
Thus, this alternative test can be used only as a sufficient condition that is easy.
The two key ideas behind the above test are as follows:
This test takes polynomial time, instead of the exponential time required to compute F+
Real-world database schemas are much larger than the examples that fit in the pages of a book.
For this reason, we need algorithms for the generation of designs.
In this section, we present algorithms for BCNF and 3NF.
The definition of BCNF can be useddirectly to test if a relation is in BCNF.However, computation of F+ can be a tedious task.
We first describe below simplified tests for verifying if a relation is in BCNF.
If a relation is not in BCNF, it can be decomposed to create relations that are in BCNF.
Later in this section, we describe an algorithm to create a lossless decomposition of a relation, such that the decomposition is in BCNF.
Testing of a relation schema R to see if it satisfies BCNF can be simplified in some cases:
To check if a relation schema R is in BCNF, it suffices to check only the dependencies in the given set F for violation of BCNF, rather than check all dependencies in F+
We can show that if none of the dependencies in F causes a violation of BCNF, then none of the dependencies in F+ will cause a violation of BCNF, either.
An alternative BCNF test is sometimes easier than computing every dependency in F+
To check if a relation Ri in a decomposition of R is in BCNF, we apply this test:
We are now able to state a general method to decompose a relation schema so as to satisfy BCNF.
The algorithm uses dependencies that demonstrate violation of BCNF to perform the decomposition.
The BCNF decomposition algorithm takes time exponential in the size of the initial schema, since the algorithm for checking if a relation in the decomposition satisfies BCNF can take exponential time.
The bibliographical notes provide references to an algorithm that can compute a BCNF decomposition in polynomial time.
However, the algorithmmay “overnormalize,” that is, decompose a relation unnecessarily.
As a longer example of the use of the BCNF decomposition algorithm, suppose we have a database design using the class schema below:
The set of functional dependencies that we require to hold on class are:
The only nontrivial functional dependencies that hold on course include course id on the left side of the arrow.
Since course id is a key for course, the relation course is in BCNF.
Thus, the decomposition of class results in the three relation schemas course, classroom, and section, each of which is in BCNF.
These correspond to the schemas that we have used in this, and previous, chapters.
You can verify that the decomposition is lossless and dependency preserving.
The set of dependencies Fc used in the algorithm is a canonical cover for F.
Let us apply this algorithm to our example of Section 8.3.4, where we showed that:
The algorithm then finds that R2 contains a candidate key, so no further relation schema is created.
The resultant set of schemas can contain redundant schemas,with one schema Rk containing all the attributes of another schema Rj.
The algorithm deletes all such schemas that are contained in another schema.
Rj that is deleted can also be tested on the corresponding relation Rk , and the decomposition is lossless even if Rj is deleted.
The set of functional dependencies we listed there happen to be a canonical cover.
As a result, the algorithm gives us the same three schemas course, classroom, and section.
The above example illustrates an interesting property of the 3NF algorithm.
Sometimes, the result is not only in 3NF, but also in BCNF.
This suggests an alternative method of generating a BCNF design.
Then, for any schema in the 3NF design that is not in BCNF, decompose using the BCNF algorithm.
It ensures that the decomposition is a lossless decomposition by guaranteeing that at least one schema contains a candidate key for the schema being decomposed.
Practice Exercise 8.14 provides some insight into the proof that this suffices to guarantee a lossless decomposition.
This algorithm is also called the 3NF synthesis algorithm, since it takes a set of dependencies and adds one schema at a time, instead of decomposing the initial schema repeatedly.
The result is not uniquely defined, since a set of functional dependencies can have more than one canonical cover, and, further, in some cases, the result of the algorithm depends on the order in which it considers the dependencies in Fc.
Nevertheless, there are disadvantages to 3NF: We may have to use null values to represent some of the possible meaningful relationships among data items, and there is the problem of repetition of information.
Since it is not always possible to satisfy all three, we may be forced to choose between BCNF and dependency preservation with 3NF.
On the negative side, there is a space and time overhead due to the materialized view, but on the positive side, the application programmer need not worry about writing code to keep redundant data consistent on updates; it is the job of the database system to maintain the materialized view, that is, keep it up to date when the database is updated.
Later in the book, in Section 13.5, we outline how a database system can perform materialized view maintenance efficiently.
Unfortunately, most current database systems do not support constraints on materialized views.
Although the Oracle database does support constraints on materialized views, by default it performs view maintenance when the view is accessed, not when the underlying relation is updated;7 as a result a constraint violation may get detected well after the update has been performed, which makes the detection useless.
Some relation schemas, even though they are in BCNF, do not seem to be sufficiently normalized, in the sense that they still suffer from the problemof repetition of information.
Consider a variation of the university organization where an instructor may be associated with multiple departments.
The astute reader will recognize this schema as a non-BCNF schema because of the functional dependency.
Despite r2 being in BCNF, there is redundancy.We repeat the address information of each residence of an instructor once for each department with which the instructor is associated.
We could solve this problem by decomposing r2 further into:
To deal with this problem, we must define a new form of constraint, called.
As we did for functional dependencies, we shall use multivalued dependencies to define a normal form for relation schemas.
This normal form, called fourth normal form (4NF), is more restrictive than BCNF.
This repetition is unnecessary, since the relationship between an instructor and his address is independent of the relationship between that instructor and a department.
If an instructor with ID 22222 is associated with the Physics department, wewant that department to be associatedwith all of that instructor’s addresses.
Comparing the preceding examplewith our definition of multivalued dependency, we see that we want the multivalued dependency:
As with functional dependencies, we shall use multivalued dependencies in two ways:
To test relations to determine whether they are legal under a given set of functional and multivalued dependencies.
Figure 8.14 An example of redundancy in a relation on a BCNF schema.
To specify constraints on the set of legal relations; we shall thus concern ourselves with only those relations that satisfy a given set of functional and multivalued dependencies.
Let D denote a set of functional and multivalued dependencies.
The closure D+ of D is the set of all functional and multivalued dependencies logically implied by D.
As we did for functional dependencies, we can compute D+ from D, using the formal definitions of functional dependencies and multivalued dependencies.
We can manage with such reasoning for very simple multivalued dependencies.
Luckily, multivalued dependencies that occur in practice appear to be quite simple.
For complex dependencies, it is better to reason about sets of dependencies by using a system of inference rules.
A relation schema r (R) is in fourth normal form (4NF) with respect to a setD of functional and multivalued dependencies if, for all multivalued dependencies.
To check if each relation schema ri in the decomposition is in 4NF, we need to find what multivalued dependencies hold on each ri.
Recall that, for a set F of functional dependencies, the restriction Fi of F to Ri is all functional dependencies in F+ that include only attributes of Ri.
Now consider a set D of both functional and multivalued dependencies.
The restriction of D to Ri is the set Di consisting of:
All functional dependencies in D+ that include only attributes of Ri.
It is identical to the BCNF decomposition algorithm of Figure 8.11, except that it uses multivalued dependencies and uses the restriction of D+ to Ri.
This pair of schemas, which is in 4NF, eliminates the redundancy we encountered earlier.
The following fact about multivalued dependencies and losslessness shows that the algorithm of Figure 8.16 generates only lossless decompositions:
Let r (R) be a relation schema, and letD be a set of functional andmultivalued dependencies on R.
This decomposition is lossless of R if and only if at least one of the following multivalued dependencies is in D+:
The issue of dependency preservationwhenwe decompose a relation schema becomes more complicated in the presence of multivalued dependencies.
The fourth normal form is by no means the “ultimate” normal form.
As we saw earlier, multivalued dependencies help us understand and eliminate some forms of repetition of information that cannot be understood in terms of functional dependencies.
There are types of constraints called join dependencies that generalize multivalued dependencies, and lead to another normal form called project-join normal form (PJNF) (PJNF is called fifth normal form in some books)
There is a class of evenmore general constraints that leads to a normal form called domain-key normal form (DKNF)
A practical problem with the use of these generalized constraints is that they are not only hard to reason with, but there is also no set of sound and complete inference rules for reasoning about the constraints.
Conspicuous by its absence from our discussion of normal forms is second normal form (2NF)
We have not discussed it, because it is of historical interest only.
We simply define it, and let you experiment with it in Practice Exercise 8.17
So far we have looked at detailed issues about normal forms and normalization.
In this section, we study how normalization fits into the overall database-design process.
Earlier in the chapter, starting in Section 8.3, we assumed that a relation schema r (R) is given, and proceeded to normalize it.
There are several ways in which we could have come up with the schema r (R):
The normalization process then breaks up r (R) into smaller schemas.
Most examples of such dependencies arise out of poor E-R diagram design.
In the above example, if we had designed the E-R diagram correctly, we would have created a department entity set with attribute dept address and a relationship set between instructor and department.
Similarly, a relationship set involvingmore than two entity sets may result in a schema that may not be in a desirable normal form.
Since most relationship sets are binary, such cases are relatively rare.
If the generated relation schemas are not in desired normal form, the problem can be fixed in the ER diagram.
That is, normalization can be done formally as part of data modeling.
Alternatively, normalization can be left to the designer’s intuition during E-R modeling, and can be done formally on the relation schemas generated from the E-R model.
Indeed, the process of creating an E-R design tends to generate 4NF designs.
If a multivalued dependency holds and is not implied by the corresponding functional dependency, it usually arises from one of the following sources:
For amany-to-many relationship set each related entity set has its ownschemaand there is an additional schema for the relationship set.
For a multivalued attribute, a separate schema is created consisting of that attribute and the primary key of the entity set (as in the case of the phone number attribute of the entity set instructor)
The universal-relation approach to relational database design starts with an assumption that there is one single relation schema containing all attributes of interest.
This single schema defines how users and applications interact with the database.
A desirable feature of a database design is the unique-role assumption, which means that each attribute name has a unique meaning in the database.
This prevents us from using the same attribute to mean different things in different schemas.
For example, we might otherwise consider using the attribute number for phone number in the instructor schema and for room number in the classroom schema.
While it is a good idea to keep names for incompatible attributes distinct, if attributes of different relations have the same meaning, it may be a good idea to use the same attribute name.
For this reason we used the same attribute name “name” for both the instructor and the student entity sets.
If this was not the case (that is, we used different naming conventions for the instructor and student names), then if wewished to generalize these entity sets by creating a person entity set, we would have to rename the attribute.
Although technically, the order of attribute names in a schemadoes notmatter, it is convention to list primary-key attributes first.
This makes reading default output (as from select *) easier.
In large database schemas, relationship sets (and schemas derived therefrom) are often named via a concatenation of the names of related entity sets, perhaps with an intervening hyphen or underscore.
We have used a few such names, for example inst sec and student sec.
We used the names teaches and takes instead of using the longer concatenated names.
This was acceptable since it is not hard for you to remember the associated entity sets for a few relationship sets.
We cannot always create relationship-set names by simple concatenation; for example, a manager or works-for relationship between employees would not make much sense if itwere called employee employee! Similarly, if there aremultiple relationship sets possible between a pair of entity sets, the relationship-set namesmust include extra parts to identify the relationship set.
For example, wemay call an entity set of students student or students.
We have chosen to use the singular form in our database designs.
Using either singular or plural is acceptable, as long as the convention is used consistently across all entity sets.
As schemas grow larger, with increasing numbers of relationship sets, using consistent naming of attributes, relationships, and entities makes life much easier for the database designer and application programmers.
They use the redundancy to improve performance for specific applications.
The penalty paid for not using a normalized schema is the extra work (in terms of coding time and execution time) to keep redundant data consistent.
For instance, suppose all course prerequisites have to be displayed alongwith a course information, every time a course is accessed.
In our normalized schema, this requires a join of course with prereq.
One alternative to computing the join on the fly is to store a relation containing all the attributes of course and prereq.
However, the information for a course is repeated for every course prerequisite, and all copies must be updated by the application, whenever a course prerequisite is added or dropped.
The process of taking a normalized schema and making it nonnormalized is called denormalization, and designers use it to tune performance of systems to support time-critical operations.
A better alternative, supported by many database systems today, is to use the normalized schema, and additionally store the join of course and prereq as a materialized view.
Recall that amaterialized view is a viewwhose result is stored in the database and brought up to date when the relations used in the view are updated.
There are some aspects of database design that are not addressed by normalization, and can thus lead to bad database design.
Data pertaining to time or to ranges of time have several such issues.
We give examples here; obviously, such designs should be avoided.
However, this alternative design is clearly a bad idea—we would have to create a new relation every year, and we would also have to write new queries every year, to take each new relation into account.
Queries would also be more complicated since they may have to refer to many relations.
Here the only functional dependencies are from dept name to the other attributes, and again the relation is in BCNF.
This design is also a bad idea since it has problems similar to the previous design—namely we would have to modify the relation schema andwrite new queries every year.
Queries would also bemore complicated, since they may have to refer to many attributes.
Representations such as those in the dept year relation, with one column for each value of an attribute, are called crosstabs; they are widely used in spreadsheets and reports and in data analysis tools.
While such representations are useful for display to users, for the reasons just given, they are not desirable in a database design.
Suppose we retain data in our university organization showing not only the address of each instructor, but also all former addresses of which the university is aware.
We may then ask queries such as “Find all instructors who lived in Princeton in 1981.” In this case, we may have multiple addresses for instructors.
Each address has an associated start and end date, indicating when the instructor was resident at that address.
A special value for the end date, e.g., null, or a value.
In general, temporal data are data that have an associated time interval during which they are valid.8 We use the term snapshot of data to mean the value of the data at a particular point in time.
Thus a snapshot of course data gives the values of all attributes, such as title and department, of all courses at a particular point in time.
Modeling temporal data is a challenging problem for several reasons.
For example, suppose we have an instructor entity set with which wewish to associate a time-varying address.
To add temporal information to an address, wewould then have to create a multivalued attribute, each of whose values is a composite value containing an address and a time interval.
In addition to time-varying attribute values, entities may themselves have an associated valid time.
For example, a student entity may have a valid time from the date the student entered the university to the date the student graduated (or left the university)
For example, the prereq relationship may record when a course became a prerequisite for another course.
We would thus have to add valid time intervals to attribute values, entity sets, and relationship sets.
Adding such detail to an E-R diagram makes it very difficult to create and to comprehend.
There have been several proposals to extend the E-R notation to specify in a simple manner that an attribute value or relationship is time varying, but there are no accepted standards.
When we track data values across time, functional dependencies that we assumed to hold, such as:
The following constraint (expressed in English) would hold instead: “An instructor ID has only one street and city value for any given time t.”
Functional dependencies that hold at a particular point in time are called temporal functional dependencies.
We could extend the theory of relational database design to take temporal functional dependencies into account.
However, reasoning with regular functional dependencies is difficult enough already, and few designers are prepared to deal with temporal functional dependencies.
In practice, database designers fall back to simpler approaches to designing temporal databases.
One commonly used approach is to design the entire database (including E-R design and relational design) ignoring temporal changes (equivalently, taking only a snapshot into consideration)
There are other models of temporal data that distinguish between valid time and transaction time, the latter recording when a fact was recorded in the database.
The next step is to add valid time information to each such relation, by adding start and end time as attributes.
The title of the course may change over time, which can be handled by adding a valid time range; the resultant schema would be.
If the relation is updated by changing the course title to “Introduction to Java,” the time “9999-12-31” would be updated to the time until which the old value (“Introduction to C”) is valid, and a new tuple would be added containing the new title (“Introduction to Java”), with an appropriate start time.
If another relation had a foreign key referencing a temporal relation, the database designer has to decide if the reference is to the current version of the data or to the data as of a particular point in time.
For example, we may extend the department relation to track changes in the building or budget of a department across time, but a reference from the instructor or student relation may not care about the history of the building or budget, but may instead implicitly refer to the temporally current record for the corresponding dept name.
On the other hand, a record in a student’s transcript should refer to the course title at the time when the student took the course.
In this latter case, the referencing relation must also record time information, to identify a particular record from the course relation.
In our example, the year and semesterwhen the course was taken can bemapped to a representative time/date value such as midnight of the start date of the semester; the resulting time/date value is used to identify a particular record from the temporal version of the course relation, from which the title is retrieved.
The original primary key for a temporal relation would no longer uniquely identify a tuple.
To resolve this problem, we could add the start and end time attributes to the primary key.
It is possible to store data with overlapping intervals, which the primary-key constraint would not catch.
If the system supports a native valid time type, it can detect and prevent such overlapping time intervals.
To specify a foreign key referencing such a relation, the referencing tuples would have to include the start- and end-time attributes as part of their foreign key, and the values must match that in the referenced tuple.
Further, if the referenced tuple is updated (and the end time which was in the future is updated), the update must propagate to all the referencing tuples.
If the system supports temporal data in a better fashion, we can allow the referencing tuple to specify a point in time, rather than a range, and rely on the system to ensure that there is a tuple in the referenced relation whose valid time interval contains the point in time.
As a common special case, if all references to temporal data refer to only the current data, a simpler solution is to not add time information to the relation, but instead create a corresponding history relation that has temporal information, for past values.
For example, in our bank database, we could use the design we have created, ignoring temporal changes, to store only the current information.
Thus, the instructor relation may store only the current address, while a relation instructor history may contain all the attributes of instructor, with additional start time and end time attributes.
Although we have not provided any formal way to deal with temporal data, the issues we have discussed and the examples we have provided should help you in designing a database that records temporal data.
Further issues in handling temporal data, including temporal queries, are covered later, in Section 25.2
We showed pitfalls in database design, and how to systematically design a database schema that avoids the pitfalls.
The pitfalls included repeated information and inability to represent some information.
We showed the development of a relational database design from an E-R design, when schemas may be combined safely, and when a schema should be decomposed.
We described the assumptions of atomic domains and first normal form.
We introduced the concept of functional dependencies, and used it to present.
If the decomposition is dependency preserving, given a database update, all functional dependencies can be verifiable from individual relations, without computing a join of relations in the decomposition.
We placed special emphasis on what dependencies are logically implied by a set of dependencies.We also defined the notion of a canonical cover, which is aminimal set of functional dependencies equivalent to a given set of functional dependencies.
We used the canonical covers to decompose a relation into 3NF, which is a small relaxation of the BCNF condition.
We presented the notion of multivalued dependencies, which specify constraints that cannot be specified with functional dependencies alone.
Other normal forms, such as PJNF and DKNF, eliminate more subtle forms of redundancy.
However, these are hard to work with and are rarely used.
In reviewing the issues in this chapter, note that the reason we could define rigorous approaches to relational database design is that the relational data model rests on a firm mathematical foundation.
That is one of the primary advantages of the relationalmodel comparedwith the other datamodels that we have studied.
Show that this decomposition is a lossless decomposition if the following set F of functional dependencies holds:
A one-to-one relationship set exists between entity sets student and instructor.
A many-to-one relationship set exists between entity sets student and instructor.
What could go wrong on decomposition, if this property is violated?
What primary and foreign-key constraint do you expect to hold on the decomposed relations?
Give an example of an inconsistency that can arise due to an erroneous update, if the foreign-key constraint were not enforced on the decomposed relations above.
Hint: Show that the join of all the projections onto the schemas of the decomposition cannot have more tuples than the original relation.
Each integer i on the list indicates that A appears on the left side of the ith FD */
Hint: Show that every partial dependency is a transitive dependency.
Explain why each of these properties may indicate a bad relational database design.
Hint: Give an example of a relation r on schema R such that.
Compute a canonical cover for the above set of functional dependencies F ; give each step of your derivation with an explanation.
Give a 3NF decomposition of r based on the canonical cover.
Give a BCNF decomposition of r using the original set of functional dependencies.
Can you get the same BCNF decomposition of r as above, using the canonical cover?
The first discussion of relational database design theory appeared in an early paper by Codd [1970]
In that paper, Codd also introduced functional dependencies and first, second, and third normal forms.
Significant development of relational database theory occurred in the late 1970s.
Fundamental results on the lossless decomposition property appear in Aho et al.
See the bibliographical notes of Appendix C for further references to literature on normalization.
A survey of extensions to E-R modeling to handle temporal data is presented by Gregersen and Jensen [1999]
Practically all use of databases occurs from within application programs.
Correspondingly, almost all user interaction with databases is indirect, via application programs.
Not surprisingly, therefore, database systems have long supported tools such as form and GUI builders, which help in rapid development of applications that interface with users.
In recent years, the Web has become the most widely used user interface to databases.
In this chapter, we study tools and technologies that are used to build applications, focussing on interactive applications that use databases to store data.
After an introduction to application programs and user interfaces in Section 9.1, we focus on developing applications withWeb-based interfaces.
A short overview of Web application architectures in presented Section 9.4
We conclude the chapter with Section 9.8, which covers encryption and its use in applications.
Although many people interact with databases, very few people use a query language to interact with a database system directly.
The most common way in which users interact with databases is through an application program that provides a user interface at the front end, and interfaces with a database at the back end.
Such applications take input from users, typically through a formsbased interface, and either enter data into a database or extract information from a database based on the user input, and generate output, which is displayed to the user.
As an example of an application, consider a university registration system.
Like other such applications, the registration system first requires you to identify.
The application then uses your identity to extract information, such as your name and the courses for which you have registered, from the database and displays the information.
The application provides a number of interfaces that let you register for courses and query a variety of other information such as course and instructor information.
Organizations use such applications to automate a variety of tasks, such as sales, purchases, accounting and payroll, human-resources management, and inventory management, among many others.
Application programsmay be used evenwhen it is not apparent that they are being used.
For example, a news site may provide a page that is transparently customized to individual users, even if the user does not explicitly fill any forms when interacting with the site.
To do so, it actually runs an application program that generates a customized page for each user; customization can, for example, be based on the history of articles browsed by the user.
A typical application program includes a front-end component, which deals with the user interface, a back-end component, which communicates with a database, and a middle layer, which contains “business logic,” that is, code that executes specific requests for information or updates, enforcing rules of business such as what actions should be carried out to execute a given task, or who can carry out what task.
Application architectures have evolved over time, as illustrated in Figure 9.1
Applications such as airline reservations have been around since the 1960s.
In the early days of computer applications, applications ran on a large “mainframe” computer, and users interacted with the application through terminals, some of which even supported forms.
With the widespread use of personal computers, many organizations used a different architecture for internal applications, with applications running on the user’s computer, and accessing a central database.
This architecture, often called a “client–server” architecture, allowed the creation of powerful graphical user interfaces, which earlier terminal-based applications did not support.
However, software had to be installed on each user’s machine to run an application, making tasks such as upgrades harder.
Even in the personal computer era, when clientserver architectures became popular, mainframe architecture continued to be the.
In the past 15 years, Web browsers have become the universal front end to database applications, connecting to the back end through the Internet.
Browsers use a standardized syntax, the HyperText Markup Language (HTML) standard, which supports both formatted display of information, and creation of formsbased interfaces.
The HTML standard is independent of the operating system or browser, and pretty much every computer today has a Web browser installed.
Thus a Web-based application can be accessed from any computer that is connected to the Internet.
However, sophisticated user interfaces, supporting features well beyond what is possible using plain HTML, are now widely used, and are built with the scripting language JavaScript, which is supported by most Web browsers.
JavaScript programs, unlike programs written in C, can be run in a safe mode, guaranteeing they cannot cause security problems.
JavaScript programs are downloaded transparently to the browser and do not need any explicit software installation on the user’s computer.
While theWebbrowser provides the front end for user interaction, application programs constitute the back end.
Typically, requests from a browser are sent to a Web server, which in turn executes an application program to process the request.
A variety of technologies are available for creating application programs that run at the back end, including Java servlets, Java Server Pages (JSP), Active Server Page (ASP), or scripting languages such as PHP, Perl, or Python.
In the rest of this chapter, we describe how to build such applications, starting with Web technologies and tools for building Web interfaces, and technologies for building application programs, and then covering application architectures, and performance and security issues in building applications.
In this section, we review some of the fundamental technology behind the World Wide Web, for readers who are not familiar with the technology underlying the Web.
A uniform resource locator (URL) is a globally unique name for each document that can be accessed on the Web.
The first part of the URL indicates how the document is to be accessed: “http” indicates that the document is to be accessed by theHyperText Transfer Protocol.
The second part gives the name of a machine that has a Web server.
The rest of the URL is the path name of the file on the machine, or other unique identifier of a document within the machine.
A URL can contain the identifier of a program located on the Web server machine, as well as arguments to be given to the program.
On receiving a request for such aURL, the Web server executes the program, using the given arguments.
The program returns an HTML document to the Web server, which sends it back to the front end.
The HTML source shows a few of the HTML tags.
Every HTML page should be enclosed in an html tag, while the body of the page is enclosed in a body tag.
A table is specified by a table tag, which contains rows specified by a tr tag.
The header row of the table has table cells specified by a th tag, while regular.
We do not go into more details about the tags here; see the bibliographic notes for references containing more detailed descriptions of HTML.
Figure 9.4 shows how to specify an HTML form that allows users to select the person type (student or instructor) from a menu and to input a number in a text box.
Figure 9.5 shows how the above form is displayed in a Web browser.
Two methods of accepting input are illustrated in the form, but HTML also supports several other input methods.
The action attribute of the form tag specifies that when the form is submitted (by clicking on the submit button), the form data should be sent to the URL PersonQuery (the URL is relative to that of the page)
TheWeb server is configured such thatwhen this URL is accessed, a corresponding application program is invoked, with the user-provided values for the arguments persontype and name (specified in the select and input fields)
The application program generates an HTML document, which is then sent back and displayed to the user; we shall see how to construct such programs later in this chapter.
The getmethod encodes the values as part of the URL.
For example, if the Google search page used a form with an input parameter named qwith the getmethod, and the user typed in the string “silberschatz” and submitted the form, the browser would request the following URL from the Web server:
The postmethod would instead send a request for the URL http://www.google.com, and send the parameter values as part of the HTTP protocol exchange between the Web server and the browser.
The form in Figure 9.4 specifies that the form uses the getmethod.
Although HTML code can be created using a plain text editor, there are a number of editors that permit direct creation of HTML text by using a graphical interface.
Such editors allow constructs such as forms, menus, and tables to be inserted into the HTML document from a menu of choices, instead of manually typing in the code to generate the constructs.
The cascading stylesheet (CSS) standard allows the same stylesheet to be used for multiple HTML documents, giving a distinctive but uniform look to all the pages on a Web site.
Web servers provide powerful features, beyond the simple transfer of documents.
The most important feature is the ability to execute programs, with arguments supplied by the user, and to deliver the results back as an HTML document.
As a result, a Web server can act as an intermediary to provide access to a variety of information services.
A new service can be created by creating and installing an application program that provides the service.
The common gateway interface (CGI) standard defines how the Web server communicates with application programs.
The application program typically communicates with a database server, through ODBC, JDBC, or other protocols, in order to get or store data.
Figure 9.6 shows aWeb application built using a three-layer architecture, with a Web server, an application server, and a database server.
Using multiple levels of servers increases system overhead; the CGI interface starts a new process to service each request, which results in even greater overhead.
Most Web applications today therefore use a two-layer Web application architecture, where the application program runs within the Web server, as in Figure 9.7
We study systems based on the two-layer architecture in more detail in subsequent sections.
There is no continuous connection between the client and the Web server; when aWeb server receives a request, a connection is temporarily created to send the request and receive the response from the Web server.
But the connection may then be closed, and the next request could come over a new connection.
One important reason that HTTP is connectionless is that most computers have limits on the number of simultaneous connections they can accommodate, and if a large number of sites on the Web open connections to a single server, this limit would be exceeded, denying service to further users.
With a connectionless protocol, the connection can be broken as soon as a request is satisfied, leaving connections available for other requests.1
Most Web applications, however, need session information to allow meaningful user interaction.
For instance, applications typically restrict access to information, and therefore need to authenticate users.
For performance reasons, connections may be kept open for a short while, to allow subsequent requests to reuse the connection.
However, there is no guarantee that the connection will be kept open, and applications must be designed assuming the connection may be closed as soon as a request is serviced.
To implement sessions in spite of connections getting closed, extra information has to be stored at the client and returned with each request in a session; the server uses this information to identify that a request is part of a user session.
Extra information about the session also has to be maintained at the server.
This extra information is usually maintained in the form of a cookie at the client; a cookie is simply a small piece of text containing identifying information and with an associated name.
For example, google.com may set a cookie with the name prefs, which encodes preferences set by the user such as the preferred language and the number of answers displayed per page.
On each search request, google.com can retrieve the cookie named prefs from the user’s browser, and display results according to the specified preferences.
A domain (Web site) is permitted to retrieve only cookies that it has set, not cookies set by other domains, and cookie names can be reused across domains.
For the purpose of tracking a user session, an application may generate a session identifier (usually a random number not currently in use as a session identifier), and send a cookie named (for instance) sessionid containing the session identifier.
The session identifier is also stored locally at the server.
When a request comes in, the application server requests the cookie named sessionid from the client.
If the client does not have the cookie stored, or returns a value that is not currently recorded as a valid session identifier at the server, the application concludes that the request is not part of a current session.
If the cookie value matches a stored session identifier, the request is identified as part of an ongoing session.
If an application needs to identify users securely, it can set the cookie only after authenticating the user; for example a user may be authenticated only when a valid user name and password are submitted.2
For applications that do not require high security, such as publicly available news sites, cookies can be stored permanently at the browser and at the server; they identify the user on subsequent visits to the same site,without any identification information being typed in.
For applications that require higher security, the server may invalidate (drop) the session after a time-out period, or when the user logs out.
Typically a user logs out by clicking on a logout button, which submits a logout form, whose action is to invalidate the current session.
Invalidating a session merely consists of dropping the session identifier from the list of active sessions at the application server.
The user identifier could be stored at the client end, in a cookie named, for example, userid.
Such cookies can be used for low-security applications, such as free Web sites identifying their users.
However, for applications that require a higher level of security, such a mechanism creates a security risk: The value of a cookie can be changed at the browser by a malicious user, who can then masquerade as a different user.
Setting a cookie (named sessionid, for example) to a randomly generated session identifier (from a large space of numbers) makes it highly improbable that a user can masquerade as (that is, pretend to be) another user.
A sequentially generated session identifier, on the other hand, would be susceptible to masquerading.
In a two-layer Web architecture, an application runs as part of the Web server itself.
One way of implementing such an architecture is to load Java programs into the Web server.
The Java servlet specification defines an application programming interface for communication between the Web server and the application program.
The HttpServlet class in Java implements the servlet API specification; servlet classes used to implement specific functions are defined as subclasses of this class.3 Often theword servlet is used to refer to a Java program (and class) that implements the servlet interface.
Figure 9.8 shows a servlet example; we explain it in detail shortly.
The code for a servlet is loaded into theWeb server when the server is started, or when the server receives a remote HTTP request to execute a particular servlet.
The task of a servlet is to process such a request, which may involve accessing a database to retrieve necessary information, and dynamically generate an HTML page to be returned to the client browser.
Servlets are commonly used to generate dynamic responses to HTTP requests.
They can access inputs provided through HTML forms, apply “business logic” to decide what response to provide, and then generate HTML output to be sent back to the browser.
The form specifies that the HTTP get mechanism is used for transmitting parameters.
So the doGet() method of the servlet, as defined in the code, is invoked.
Each request results in a new thread within which the call is executed, so multiple requests can be handled in parallel.
The codeused to access thedatabase and to get attribute values from the query result is not shown; refer to Section 5.1.1.4 for details of how to use JDBC to access a database.
Outputting the results is to response is implemented by first getting a PrintWriter object out from response, and then printing the result in HTML format to out.
The servlet interface can also support non-HTTP requests, although our example uses only HTTP.
Recall that the interaction between a browser and aWeb server is stateless.
That is, each time the browser makes a request to the server, the browser needs to connect to the server, request some information, then disconnect from the server.
Cookies can be used to recognize that a request is from the same browser session as an.
The servlet API provides a method of tracking a session and storing information pertaining to it.
Invocation of the method getSession(false) of the class HttpServletRequest retrieves the HttpSession object corresponding to the browser that sent the request.
An argument value of true would have specified that a new session object must be created if the request is a new request.
When the getSession() method is invoked, the server first asks the client to return a cookie with a specified name.
If the client does not have a cookie of that name, or returns a value that does not match any ongoing session, then the request is not part of an ongoing session.
In this case, getSession() would return a null value, and the servlet could direct the user to a login page.
The servlet corresponding to the login page could verify that the password matches the user (for example, by looking up authentication information in the database)
If the user is properly authenticated, the login servlet would execute getSession(true), which would return a new session object.
To create a new session theWeb serverwould internally carry out the following tasks: set a cookie (called, for example, sessionId)with a session identifier as its associated value at the client browser, create a newsessionobject, and associate the session identifier valuewith the session object.
The servlet code can also store and look up (attribute-name, value) pairs in the HttpSession object, to maintain state across multiple requests within a session.
For example, after the user is authenticated and the session object has been created, the login servlet could store the user-id of the user as a session parameter by executing the method.
If the request was part of an ongoing session, the browser would have returned the cookie value, and the corresponding session object is returned by getSession()
The servlet can then retrieve session parameters such as user-id from the session object by executing the method.
If the attribute userid is not set, the function would return a null value, which would indicate that the client user has not been authenticated.
The life cycle of a servlet is controlled by the Web server in which the servlet has been deployed.
When there is a client request for a specific servlet, the server.
If not, the Web server loads the servlet class into the Java virtual machine (JVM), and creates an instance of the servlet class.
In addition, the server calls the init() method to initialize the servlet instance.
Notice that each servlet instance is initialized only once when it is loaded.
After making sure the servlet instance does exist, the server invokes the service method of the servlet, with a request object and a response object as parameters.
By default, the server creates a new thread to execute the service method; thus, multiple requests on a servlet can execute in parallel, without having to wait for earlier requests to complete execution.
Whenno longer required, a servlet can be shut downby calling the destroy() method.
The server can be set up to automatically shut down a servlet if no requests have been made on a servlet within a time-out period; the time-out period is a server parameter that can be set as appropriate for the application.
The best way to develop servlet applications is by using an IDE such as Eclipse or NetBeans, which come with Tomcat or Glassfish servers built in.
Application servers usually provide a variety of useful services, in addition to basic servlet support.
They allow applications to be deployed or stopped, and provide functionality to monitor the status of the application server, including performance statistics.
If a servlet file is modified, some application servers can detect this and recompile and reload the servlet transparently.
Writing even a simple Web application in a programming language such as Java or C is a time-consuming task that requires many lines of code and programmers who are familiar with the intricacies of the language.
An alternative approach, that of server-side scripting, provides a much easier method for creating many applications.
In server-side scripting, before delivering a Web page, the server executes the scripts embeddedwithin the HTML contents of the page.
Each piece of script, when executed, can generate text that is added to the page (or may even delete content from the page)
The executed script may contain SQL code that is executed against a database.
Many scripting languages also allow code written in languages such as Java, C#, VBScript, Perl, and Python to be embedded into or invoked from HTML pages.
Many of these languages come with libraries and tools, that together constitute a framework for Web application development.
The motivation is that, for many dynamicWeb pages, most of their content is still static (that is, the same content is present whenever the page is generated)
The dynamic content of the Web pages (which are generated, for example, on the basis of form parameters) is often a small part of the page.
Creating such pages by writing servlet code results in a large amount of HTML being coded as Java strings.
Figure 9.9 shows the source text of an JSP page that includes embedded Java code.
The Java code in the script is distinguished from the surrounding HTML code by being enclosed in <%
When a JSP page is requested by a browser, the application server generates HTML output from the page, which is sent back to the browser.
The following code performs the same actions as the JSP code in Figure 9.9
Note that the array is indexed by the parameter name; in PHP arrays can be indexed by arbitrary strings, not just numbers.
The function isset checks if the element of the array has been initialized.
The echo function prints its argument to the output HTML.
A suitably configured Web server would interpret any file whose name ends in “.php” to be a PHP file.
If the file is requested, the Web server process it in a manner similar to how JSP files are processed, and returns the generated HTML to the browser.
A number of libraries are available for the PHP language, including libraries for database access using ODBC (similar to JDBC in Java)
In the JSP code in the above figure, if no value was entered for the form parameter name, the script prints “Hello World”; if a value was entered, the script prints “Hello” followed by the name.
Amore realistic examplemay performmore complex actions, such as looking up values from a database using JDBC.
The set of tags is extensible, and a number of tag libraries have been implemented.
For example, there is a tag library that supports paginated display of large data sets, and a library that simplifies display and parsing of dates and times.
See the bibliographic notes for references to more information on JSP tag libraries.
Embedding of program code in documents allows Web pages to be active, carrying out activities such as animation by executing programs at the local site, instead of just presenting passive text and graphics.
The primary use of such programs is flexible interaction with the user, beyond the limited interaction power provided by HTML and HTML forms.
Further, executing programs at the client site speeds up interaction greatly, compared to every interaction being sent to a server site for processing.
A danger in supporting such programs is that, if the design of the system is done carelessly, program code embedded in a Web page (or equivalently, in an email message) can perform malicious actions on the user’s computer.
The malicious actions could range from reading private information, to deleting or modifying information on the computer, up to taking control of the computer and propagating the code to other computers (through email, for example)
A number of email viruses have spread widely in recent years in this way.
One of the reasons that the Java language became very popular is that it provides a safe mode for executing programs on users’ computers.
Unlike local programs, Java programs (applets) downloaded as part of a Web page have no authority to perform any actions that could be destructive.
They are permitted to display data on the screen, or to make a network connection to the server from which the Web page was downloaded, in order to fetch more information.
However, they are not permitted to access local files, to execute any system programs, or to make network connections to any other computers.
While Java is a full-fledged programming language, there are simpler languages, called scripting languages, that can enrich user interaction, while providing the same protection as Java.
These languages provide constructs that can be embedded with an HTML document.
Client-side scripting languages are languages designed to be executed on the client’s Web browser.
Of these, the JavaScript language is by far the most widely used.
The current generation of Web interfaces uses the JavaScript scripting language extensively to construct sophisticated user interfaces.
For example, functions written in JavaScript can be used to perform error checks (validation) on user input, such as a date string being properly formatted, or a value entered (such as age) being in an appropriate range.
These checks are carried out on the browser as data is entered, even before the data are sent to the Web server.
Figure 9.10 shows an example of a JavaScript function used to validate a form input.
The function is declared in the head section of the HTML document.
The form tag specifies that the validation function is to be invoked when the form is submitted.
If the validation fails, an alert box is shown to the user, and if it succeeds, the form is submitted to the server.
The browser parses HTML code into an in-memory tree structure defined by.
Figure 9.10 Example of JavaScript used to validate form input.
JavaScript code can modify the tree structure to carry out certain operations.
For example, suppose a user needs to enter a number of rows of data, for example multiple items in a single bill.
A table containing text boxes and other form input methods can be used to gather user input.
The table may have a default size, but if more rows are needed, the user may click on a button labeled (for example) “Add Item.” This button can be set up to invoke a JavaScript function that modifies the DOM tree by adding an extra row in the table.
Although the JavaScript language has been standardized, there are differences between browsers, particularly in the details of the DOMmodel.
As a result, JavaScript code that works on one browser may not work on another.
To avoid such problems, it is best to use a JavaScript library, such as Yahoo’s YUI library, which allows code to be written in a browser independent way.
Internally, the functions in the library can find out which browser is in use, and send appropriately generated JavaScript to the browser.
See the Tools section at the end of the chapter for more information on YUI and other libraries.
Today, JavaScript is widely used to create dynamic Web pages, using several technologies that are collectively called Ajax.
Programs written in JavaScript communicate with the Web server asynchronously (that is, in the background, without blocking user interaction with the Web browser), and can fetch data and display it.
As an example of the use of Ajax, consider a Web site with a form that allows you to select a country, and once a country has been selected, you are allowed to select a state from a list of states in that country.
Until the country is selected, the drop-down list of states is empty.
The Ajax framework allows the list of states to be downloaded from theWeb site in the backgroundwhen the country is selected, and as soon as the list has been fetched, it is added to the drop-down list, which allows you to select the state.
There are also special-purpose scripting languages for specialized tasks such as animation (for example, Flash and Shockwave) and three-dimensional modeling (Virtual Reality Markup Language (VRML))
Flash is very widely used today not only for animation, but also for handling streaming video content.
To handle their complexity, large applications are often broken into several layers:
The presentation or user interface layer, which deals with user interaction.
A single application may have several different versions of this layer, corresponding to distinct kinds of interfaces such as Web browsers, and user interfaces of mobile phones, which have much smaller screens.
The view defines the presentation of data; a single underlying model can have different views depending on the specific software/device used to access the application.
The controller receives events (user actions), executes actions on the model, and returns a view to the user.
The MVC architecture is used in a number of Web application frameworks, which are discussed later in Section 9.5.2
The business-logic layer, which provides a high-level view of data and actions on data.
We discuss the business-logic layer in more detail in Section 9.4.1
Many applications use an object-oriented language to code the business-logic layer, and use an object-oriented model of data, while the underlying database is a relational database.
In such cases, the data-access layer also provides themapping from the object-oriented data model used by the business logic to the relational model supported by the database.
We discuss such mappings in more detail in Section 9.4.2
Figure 9.11 shows the above layers, along with a sequence of steps taken to process a request from the Web browser.
The labels on the arrows in the figure indicate the order of the steps.
When the request is received by the application server, the controller sends a request to the model.
The model in turn uses the dataaccess layer to update or retrieve information from a database.
The result object created by the model is sent to the view module, which creates an HTML view of the result, to be displayed on the Web browser.
The view may be tailored based on the characteristics of the device used to view the result, for example whether it is a computer monitor with a large screen, or a small screen on a phone.
The business-logic layer of an application for managing a universitymay provide abstractions of entities such as students, instructors, courses, sections, etc., and actions such as admitting a student to the university, enrolling a student in a course, and so on.
The code implementing these actions ensures that business rules are satisfied; for example the code would ensure that a student can enroll for a course only if she has already completed course prerequisites, and has paid her tuition fees.
In addition, the business logic includes workflows, which describe how a particular task that involves multiple participants is handled.
For example, if a candidate applies to the university, there is a workflow that defines who should see and approve the application first, and if approved in the first step, who should see the application next, and so on until either an offer is made to the student, or a rejection note is sent out.
Workflow management also needs to deal with error situations; for example, if a deadline for approval/rejection is not met, a supervisor may need to be informed so she can intervene and ensure the application is processed.
In the simplest scenario, where the business-logic layer uses the same data model as the database, the data-access layer simply hides the details of interfacing with the database.
However, when the business-logic layer is written using an objectoriented programming language, it is natural to model data as objects, with methods invoked on objects.
In early implementations, programmers had towrite code for creating objects by fetching data from the database, and for storing updated objects back in the database.
However, such manual conversions between data models is cumbersome and error prone.
One approach to handling this problem was to develop a database system that natively stores objects, and relationships between objects.
However, object-oriented databases did not achieve commercial success for a variety of technical and commercial reasons.
An alternative approach is to use traditional relational databases to store data, but to automate the mapping of data in relations to in-memory objects, which are created on demand (since memory is usually not sufficient to store all data in the database), as well as the reverse mapping to store updated objects back as relations in the database.
Several systems have been developed to implement such object-relational mappings.
The Hibernate system is widely used for mapping from Java objects to relations.
In Hibernate, the mapping from each Java class to one or more relations is specified in a mapping file.
The mapping file can specify, for example, that a Java class called Student is mapped to the relation student, with the Java attribute ID mapped to the attribute student.ID, and so on.
Information about the database, such as the host on which it is running, and user name and password for connecting to the database, etc., are specified in a properties file.
The program has to open a session, which sets up the connection to the database.
TheHibernate code generates the SQL commands required to store corresponding data in the student relation.
A list of objects can be retrieved from the database by executing a query written in the Hibernate query language; this is similar to executing a query using JDBC, which returns a ResultSet containing a set of tuples.
Alternatively, a single object can be retrieved by providing its primary key.
The retrieved objects can be updated in memory; when the transaction on the ongoing Hibernate session is committed, Hibernate automatically saves the updated objects by making corresponding updates on relations in the database.
While entities in an E-R model naturally correspond to objects in an objectoriented language such as Java, relationships often do not.
Hibernate supports the ability to map such relationships as sets associated with objects.
For example, the takes relationship between student and section can be modeled by associating a set of sections with each student, and a set of students with each section.
Once the appropriate mapping is specified, Hibernate populates these sets automatically from the database relation takes, and updates to the sets are reflected back to the database relation on commit.
As an example of the use of Hibernate, we create a Java class corresponding to the student relation as follows.
The mapping of the class attributes of Student to attributes of the relation student is specified in a mapping file, in an XML format.
The following code snippet then creates a Student object and saves it to the database.
Hibernate automatically generates the required SQL insert statement to create a student tuple in the database.
To retrieve students, we can use the following code snippet.
The above code snippet uses a query in Hibernate’s HQL query language.
The HQL query is automatically translated to SQL by Hibernate and executed, and the results are converted into a list of Student objects.
The for loop iterates over the objects in this list and prints them out.
The above features help to provide the programmer a high-levelmodel of data without bothering about the details of the relational storage.
However, Hibernate, like other object-relational mapping systems, also allows programmers direct SQL access to the underlying relations.
Such direct access is particularly important for writing complex queries used for report generation.
The framework also provides an SQL-like query language called Entity SQL, which operates directly on the Entity Data Model.
In the past, most Web applications used only data available at the application server and its associated database.
In recent years, a wide variety of data is available on the Web that is intended to be processed by a program, rather than displayed directly to the user; the program may be running as part of a back-end application, or may be a script running in the browser.
Such data are typically accessed using what is in effect a Web application programming interface; that is, a function call request is sent using the HTTP protocol, executed at an application server, and the results sent back to the calling program.
A system that supports such an interface is called aWeb service.
In the simpler approach, called Representation State Transfer (or REST), Web service function calls are executed by a standard HTTP request to a URL at an application server, withparameters sent as standardHTTP request parameters.
The application server executes the request (which may involve updating the database at the server), generates and encodes the result, and returns the result as the result of the HTTP request.
The server can use any encoding for a particular requested URL; XML, and an encoding of JavaScript objects called JavaScript Object Notation(JSON), are widely used.
The requestor parses the returned page to access the returned data.
Inmany applications of such RESTfulWeb services (that is,Web services using REST), the requestor is JavaScript code running in aWebbrowser; the code updates the browser screen using the result of the function call.
For example, when you scroll the display on a map interface on theWeb, the part of the map that needs to be newly displayed may be fetched by JavaScript code using a RESTful interface, and then displayed on the screen.
Amore complex approach, sometimes referred to as “BigWeb Services,” uses XML encoding of parameters as well as results, has a formal definition of the Web API using a special language, and uses a protocol layer built on top of the HTTP protocol.
This approach is described in more detail in Section 23.7.3
Many applications wish to support some operations even when a client is disconnected from the application server.
As another example, if an email client is built as a Web application, a user may wish to compose an email even if her laptop is disconnected from the network, and have it sent when it is reconnected.
Building such applications requires local storage, preferably in the form of a database, in the client machine.
The Gears software (originally developed by Google) is a browser plug-in that provides a database, a local Web server, and support for parallel execution of JavaScript at the client.
The software works identically onmultiple operating system/browser platforms, allowing applications to support rich functionality without installation of any software (other than Gears itself)
Adobe’s AIR software also provides similar functionality for building Internet applications that can run outside the Web browser.
If Web applications are built without using tools or libraries for constructing the user interface, the programming effort required to construct the user interface can be significantly more than that required for business logic and database access.
Several approaches have been developed to reduce the effort required to build applications:
Provide a library of functions to generate user-interface elements with minimal programming.
Provide drag-and-drop features in an integrated development environment that allows user-interface elements to be dragged from a menu into a design viewof a page.
The integrated development environment generates code that creates the user-interface element by invoking library functions.
Automatically generate code for the user interface from a declarative specification.
All these approaches have been used for creating user interfaces, well before the Web was created, as part of Rapid Application Development (RAD) tools, and are now used extensively for creating Web applications as well.
Examples of tools designed for rapid development of interfaces for database applications include Oracle Forms, Sybase PowerBuilder, and Oracle Application Express (APEX)
Many HTML constructs are best generated by using appropriately defined functions, instead of being written as part of the code of eachWeb page.
Instead of writing lengthy HTML code to create the required menu each time it is used, it is preferable to define a function that outputs the menu, and to call the function wherever required.
Menus are often best generated from data in the database, such as a table containing country names or state names.
The function generating the menu executes a database query and populates the menu, using the query result.
Adding a country or state then requires only a change to the database, not to the application code.
This approach has the potential drawback of requiring increased database interaction, but such overhead can be minimized by caching query results at the application server.
Forms to input dates and times, or inputs that require validation, are similarly best generated by calling appropriately defined functions.
Such functions can output JavaScript code to perform validation at the browser.
Displaying a set of results from a query is a common task for many database applications.
It is possible to build a generic function that takes an SQL query (or ResultSet) as argument, and display the tuples in the query result (or ResultSet) in a tabular form.
To handle situations where the query result is very large, such a query result display function can provide for pagination of results.
The function can display a fixed number of records in a page and provide controls to step to the next or previous page or jump to a particular page of the results.
There is unfortunately no (widely used) standard Java API for functions to carry out the user-interface tasks described above.
Building such a library can be an interesting programming project.
However, there are other tools, such as the JavaServer Faces (JSF) framework, that support the features listed above.
The JSF framework includes a JSP tag library that implements these features.
The Netbeans IDE has a component called VisualWeb that builds on JSF, providing a visual development environmentwhere user interface components can be dragged and dropped into a page, and their properties customized.
For example, JSF provides components to create dropdown menus, or display a table, which can be configured to get their data from a database query.
ASP.NET is similar to JSP, in that code in a language such as Visual Basic or C# can be embedded within HTML code.
In addition, ASP.NET provides a variety of controls (scripting commands) that are interpreted at the server, and generate HTML that is then sent to the client.
These controls can significantly simplify the construction of Web interfaces.
We provide a brief overview of the benefits that these controls offer.
For example, controls such as drop-down menus and list boxes can be associated with a DataSet object.
The DataSet object is similar to a JDBC ResultSet object, and is typically created by executing a query on the database.
The HTML menu contents are then generated from the DataSet object’s contents; for example, a querymay retrieve the names of all departments in an organization into the DataSet, and the associated menu would contain these names.
Thus, menus that depend on database contents can be created in a convenient manner with very little programming.
Validator controls can be added to form input fields; these declaratively specify validity constraints such as value ranges, or whether the input is a required input for which a value must be provided by the user.
The server creates appropriate HTML code combined with JavaScript to perform the validation at the user’s browser.
Errormessages to be displayed on invalid input can be associated with each validator control.
User actions can be specified to have an associated action at the server.
For example, a menu control can specify that selecting a value from a menu has an associated server-side action (JavaScript code is generated to detect the selection event and initiate the server-side action)
Visual Basic/C# code that displays data pertaining to the selected value can be associated with the action at the server.
Thus, selecting a value from a menu can result in associated data on the page getting updated, without requiring the user to click on a submit button.
The DataGrid control provides a very convenient way of displaying query results.
A DataGrid is associated with a DataSet object, which is typically the result of a query.
The server generates HTML code that displays the query result as a table.
In addition, DataGrids provide several features, such as pagination, and allow the user to sort the result on chosen columns.
All the HTML code as well as server-side functionality to implement these features is generated automatically by the server.
The DataGrid even allows users to edit the data and submit changes back to the server.
The application developer can specify a function, to be executed when a row is edited, that can perform the update on the database.
Microsoft Visual Studio provides a graphical user interface for creating ASP pages using these features, further reducing the programming effort.
See the bibliographic notes for references to more information on ASP.NET.
There are a variety of Web application development frameworks that provide several commonly used features such as:
An object-oriented model with an object-relational mapping to store data in a relational database (as we saw in Section 9.4.2)
A (relatively) declarativeway of specifying a formwith validation constraints on user inputs, from which the system generates HTML and Javascript/Ajax code to implement the form.
A controller that maps user interaction events such as form submits to appropriate functions that handle the event.
Thus, these frameworks provide a variety of features that are required to build Web applications, in an integrated manner.
By generating forms from declarative specifications, andmanaging data access transparently, the frameworksminimize the amount of coding that a Web application programmer has to carry out.
There are a large number of such frameworks, based on different languages.
Some of the more widely used frameworks include Ruby on Rails, which is based on the Ruby programming language, JBoss Seam, Apache Struts, Swing, Tapestry, and WebObjects, all based on Java/JSP.
Some of these, such as Ruby on Rails and JBoss Seam provide a tool that can automatically create simple CRUD Web interfaces; that is, interfaces that support create, read, update and delete of objects/tuples, by generating code from an object model or a database.
Such tools are particularly useful to get simple applications running quickly, and the generated code can be edited to build more sophisticated Web interfaces.
Report generators are tools to generate human-readable summary reports from a database.
They integrate querying the database with the creation of formatted text and summary charts (such as bar or pie charts)
For example, a report may show the total sales in each of the past 2 months for each sales region.
The application developer can specify report formats by using the formatting facilities of the report generator.
Variables can be used to store parameters such as the month and the year and to define fields in the report.
Tables, graphs, bar charts, or other graphics can be defined via queries on the database.
The query definitions can make use of the parameter values stored in the variables.
Once we have defined a report structure on a report-generator facility, we can store it and can execute it at any time to generate a report.
Report-generator systems provide a variety of facilities for structuring tabular output, such as defining table and column headers, displaying subtotals for each group in a table, automatically splitting long tables into multiple pages, and displaying subtotals at the end of each page.
The data in the report are generated by aggregation on information about orders.
Report-generation tools are available from a variety of vendors, such as Crystal Reports and Microsoft (SQL Server Reporting Services)
Several application suites, such as Microsoft Office, provide a way of embedding formatted query results from a database directly into a document.
Chart-generation facilities provided by Crystal Reports, or by spreadsheets such as Excel can be used to access data from databases, and generate tabular depictions of data or graphical depictions using charts or graphs.
The charts are created initially from data generated by executing queries against the database; the queries can be re-executed and the charts regenerated when required, to generate a current version of the overall report.
In addition to generating static reports, report-generation tools support the creation of interactive reports.
For example, a user can “drill down” into areas of interest, for example move from an aggregate view showing the total sales across an entire year to the monthly sales figures for a particular year.
Web sites may be accessed by millions of people from across the globe, at rates of thousands of requests per second, or evenmore, for themost popular sites.
Ensuring that requests are served with low response times is a major challenge for Web site developers.
To do so, application developers try to speed up the processing of individual requests by using techniques such as caching, and exploit parallel processing by using multiple application servers.
Caching techniques of various types are used to exploit commonalities between transactions.
For instance, suppose the application code for servicing each user request needs to contact a database through JDBC.
Creating a new JDBC connection may take several milliseconds, so opening a new connection for each user request is not a good idea if very high transaction rates are to be supported.
The Connection poolingmethod is used to reduce this overhead; it works as follows.
The connection pool manager (a part of the application server) creates a pool (that is, a set) of open ODBC/JDBC connections.
Instead of opening a new connection to the database, the code servicing a user request (typically a servlet) asks for (requests) a connection from the connection pool and returns the connection to the pool when the code (servlet) completes its processing.
If the pool has no unused connections when a connection is requested, a new connection is opened to the database (taking care not to exceed the maximum number of connections that the database system can support concurrently)
If there are many open connections that have not been used for a while, the connection pool manager may close some of the open database connections.
Many application servers, and newer ODBC/JDBC drivers provide a built-in connection pool manager.
A common error that many programmers make when creating Web applications is to forget to close an opened JDBC connection (or equivalently, when connection pooling is used, to forget to return the connection to the connection pool)
Each request then opens a new connection to the database, and the database soon reaches the limit of how many open connections it can have at a time.
Such problems often do not show up on small-scale testing, since databases often allow hundreds of open connections, but show up only on intensive usage.
Some programmers assume that connections, like memory allocated by Java programs, are garbage collected automatically.
Unfortunately, this does not happen, and programmers are responsible for closing connections that they have opened.
Certain requestsmay result in exactly the samequery being resubmitted to the database.
The cost of communication with the database can be greatly reduced by caching the results of earlier queries and reusing them, so long as the query result has not changed at the database.
Some Web servers support such query-result caching; caching can otherwise be done explicitly in application code.
Costs can be further reduced by caching the final Web page that is sent in response to a request.
If a new request comes with exactly the same parameters as a previous request, the request does not perform any updates, and the resultant Web page is in the cache, then it can be reused to avoid the cost of recomputing the page.
Caching can be done at the level of fragments of Web pages, which are then assembled to create complete Web pages.
Cached query results and cachedWeb pages are forms of materialized views.
Some database systems (such as Microsoft SQL Server) provide a way for the application server to register a query with the database, and get a notification from the database when the result of the query changes.
Such a notification mechanism can be used to ensure that query results cached at the application server are up-to-date.
A commonly used approach to handling such very heavy loads is to use a large number of application servers running in parallel, each handling a fraction of the.
All requests from a particular client sessionmust go to the same application server, since the server maintains state for a client session.
This property can be ensured, for example, by routing all requests from a particular IP address to the same application server.
The underlying database is, however, shared by all the application servers, so that users see a consistent view of the database.
With the above architecture, the database could easily become the bottleneck, since it is shared.
Application designers pay particular attention to minimizing the number of requests to the database, by caching query results at the application server, as discussed earlier.
In addition, parallel database systems, described in Chapter 18, are used when required.
Application security has to deal with several security threats and issues beyond those handled by SQL authorization.
The first point where security has to be enforced is in the application.
To do so, applications must authenticate users, and ensure that users are only allowed to carry out authorized tasks.
There are manyways in which an application’s security can be compromised, even if the database system is itself secure, due to badly written application code.
In this section, we first describe several security loopholes that can permit hackers to carry out actions that bypass the authentication and authorization checks carried out by the application, and explain how to prevent such loopholes.
Later in the section,we describe techniques for secure authentication, and for finegrained authorization.
We then describe audit trails that can help in recovering from unauthorized access and from erroneous updates.
We conclude the section by describing issues in data privacy.
In SQL injection attacks, the attacker manages to get an application to execute an SQL query created by the attacker.
In Section 5.1.1.4, we saw an example of an SQL injection vulnerability if user inputs are concatenated directly with an SQL query and submitted to the database.
As another example of SQL injection vulnerability, consider the form source text shown in Figure 9.4
Suppose the corresponding servlet shown in Figure 9.8 creates an SQL query string using the following Java expression:
String query = “select * from student where name like ’%” + name + “%’”
The quote inserted by the attacker closes the string, the following semicolon terminates the query, and the following text inserted by the attacker gets interpreted as a second SQL query,while the closing quote has been commented out.
Thus, the malicious user has managed to insert an arbitrary SQL statement that is executed by the application.
The statement can cause significant damage, since it can perform any action on the database, bypassing all security measures implemented in the application code.
As discussed in Section 5.1.1.4, to avoid such attacks, it is best to use prepared statements to execute SQL queries.
When setting a parameter of a prepared query, JDBC automatically adds escape characters so that the user-supplied quote would no longer be able to terminate the string.
Equivalently, a function that adds such escape characters could be applied on input strings before they are concatenated with the SQL query, instead of using prepared statements.
Another source of SQL-injection risk comes from applications that create queries dynamically, based on selection conditions and ordering attributes specified in a form.
For example, an application may allow a user to specify what attribute should be used for sorting the results of a query.
An appropriate SQL query is constructed, based on the attribute specified.
Suppose the application takes the attribute name from a form, in the variable orderAttribute, and creates a query string such as the following:
String query = “select * from takes order by ” + orderAttribute;
A malicious user can send an arbitrary string in place of a meaningful orderAttribute value, even if the HTML form used to get the input tried to restrict the allowed values by providing a menu.
To avoid this kind of SQL injection, the application should ensure that the orderAttribute variable value is one of the allowed values (in our example, attribute names), before appending it.
AWeb site that allows users to enter text, such as a comment or a name, and then stores it and later displays it to other users, is potentially vulnerable to a kind of attack called a cross-site scripting (XSS) attack.
In such an attack, a malicious user enters code written in a client-side scripting language such as JavaScript or Flash instead of entering a valid name or comment.
When a different user views the entered text, the browser would execute the script, which can carry out actions such as sending private cookie information back to the malicious user, or even executing an action on a different Web server that the user may be logged into.
For example, suppose the user happens to be logged into her bank account at the time the script executes.
The script could send cookie information related to the bank account login back to the malicious user, who could use the information to connect to the bank’s Web server, fooling it into believing that the connection is from the original user.
Or, the script could access appropriate pages on the bank’s Web site, with appropriately set parameters, to execute a money transfer.
In fact this particular problem can occur even without scripting by simply using a line of code such as.
This latter kind of vulnerability is also called cross-site request forgery or XSRF (sometimes also called CSRF)
There are other more complex kinds of XSS and XSRF attacks, which we shall not get into here.
To protect against such attacks, two things need to be done:
Prevent your Web site from being used to launch XSS or XSRF attacks.
The simplest technique is to disallow any HTML tagswhatsoever in text input by users.
There are functions that detect, or strip all such tags.
These functions can be used to prevent HTML tags, and as a result, any scripts, from being displayed to other users.
In some cases HTML formatting is useful, and in that case functions that parse the text and allow limited HTML constructs, but disallow other dangerous constructs can be used instead; these must be designed carefully, since something as innocuous as an image include could potentially be dangerous in case there is a bug in the image display software that can be exploited.
Protect your Web site from XSS or XSRF attacks launched from other sites.
If the user has logged into your Web site, and visits a different Web site vulnerable to XSS, the malicious code executing on the user’s browser could execute actions on your Web site, or pass session information related to your Web site back to the malicious user who could try to exploit it.
This cannot be prevented altogether, but you can take a few steps to minimize the risk.
TheHTTPprotocol allows a server to check the referer of a page access, that is, the URL of the page that had the link that the user clicked on to initiate the page access.
By checking that the referer is valid, for example, that the referer URL is a page on the same Web site, XSS attacks that originated on a different Web page accessed by the user can be prevented.
Instead of using only the cookie to identify a session, the session could also be restricted to the IP address from which it was originally authenticated.
As a result, even if a malicious user gets a cookie, he may not be able to log in from a different computer.
In fact, the HTTP standard recommends that GETmethods should never perform any updates, for other reasons such as a page refresh repeating an action that should have happened only once.
Another problem that application developersmust dealwith is storingpasswords in clear text in the application code.
For example, programs such as JSP scripts often contain passwords in clear text.
If such scripts are stored in a directory accessible by a Web server, an external user may be able to access the source code of the script, and get access to the password for the database account used by the application.
To avoid such problems, many application servers provide mechanisms to store passwords in encrypted form, which the server decrypts before passing it on to the database.
Such a feature removes the need for storing passwords as clear text in application programs.
However, if the decryption key is also vulnerable to being exposed, this approach is not fully effective.
As another measure against compromised database passwords, many database systems allow access to the database to be restricted to a given set of Internet addresses, typically, the machines running the application servers.
Attempts to connect to the database from other Internet addresses are rejected.
Thus, unless the malicious user is able to log into the application server, she cannot do any damage even if she gains access to the database password.
Authentication refers to the task of verifying the identity of a person/software connecting to an application.
The simplest form of authentication consists of a secret password that must be presented when a user connects to the application.
Unfortunately, passwords are easily compromised, for example, by guessing, or by sniffing of packets on the network if the passwords are not sent encrypted.
More robust schemes are needed for critical applications, such as online bank accounts.
Many applications use two-factor authentication, where two independent factors (that is, pieces of information or processes) are used to identify a user.
The two factors should not share a common vulnerability; for example, if a system merely required two passwords, both could be vulnerable to leakage in the same manner (by network sniffing, or by a virus on the computer used by the user, for example)
While biometrics such as fingerprints or iris scanners can be used in situations where a user is physically present at the point of authentication, they are not very meaningful across a network.
Passwords are used as the first factor in most such two-factor authentication schemes.
Smart cards or other encryption devices connected through the USB interface, which can be used for authentication based on encryption techniques (see Section 9.8.3), are widely used as second factors.
One-time password devices, which generate a new pseudo-random number (say) every minute are also widely used as a second factor.
Each user is given one of these devices, and must enter the number displayed by the device at the time of authentication, along with the password, to authenticate himself.
The application server can generate the same sequence of pseudo-random numbers as the device given to the user, stopping at the number that would be displayed at the time of authentication, and verify that the numbers match.
This scheme requires that the clock in the device and at the server are synchronized reasonably closely.
Yet another second-factor approach is to send an SMSwith a (randomly generated) one-time password to the user’s phone (whose number is registered earlier) whenever the user wishes to log in to the application.
The user must possess a phonewith that number to receive the SMS, and then enter the one-time password, along with her regular password, to be authenticated.
It is worth noting that even with two-factor authentication, users may still be vulnerable to man-in-the-middle attacks.
In such attacks, a user attempting to connect to the application is diverted to a fake Web site, which accepts the password (including second factor passwords) from the user, and uses it immediately to authenticate to the original application.
The HTTPS protocol, described later in Section 9.8.3.2, is used to authenticate the Web site to the user (so the user does not connect to a fake site believing it to be the intended site)
The HTTPS protocol also encrypts data, and prevents man-in-the-middle attacks.
When users accessmultipleWeb sites, it is often annoying for a user to have to authenticate herself to each site separately, oftenwith different passwords on each site.
There are systems that allow the user to authenticate herself to one central authentication service, and other Web sites and applications can authenticate the user through the central authentication service; the same password can then be used to access multiple sites.
The LDAP protocol is widely used to implement such a central point of authentication; organizations implement an LDAP server containing user names and password information, and applications use the LDAP server to authenticate users.
In addition to authenticating users, a central authentication service can provide other services, for example, providing information about the user such as name, email, and address information, to the application.
This obviates the need to enter this information separately in each application.
Other directory systems such Microsoft’s Active Directories, also provide mechanisms for authenticating users as well as for providing user information.
Such single sign-on mechanisms have long been used in network authentication protocols such as Kerberos, and implementations are now available for Web applications.
The Security Assertion Markup Language (SAML) is a standard for exchanging authentication and authorization information between different security domains, to provide cross-organization single sign-on.
For example, suppose an application needs to provide access to all students from a particular university, say Yale.
The university can set up a Web-based service that carries out authentication.
Suppose a user connects to the application with a username such as “joe@yale.edu”
The application, instead of directly authenticating a user, diverts the user to Yale University’s authentication service, which authenticates the user, and then tells the application who the user is and may provide some additional information such as the category of the user (student or instructor) or other relevant information.
The user’s password and other authentication factors are never revealed to the application, and the user need not register explicitly with the application.
However, the application must trust the university’s authentication service when authenticating a user.
TheOpenID standard is an alternative for single sign-on across organizations, and has seen increasing acceptance in recent years.
A large number of popular Web sites, such as Google, Microsoft, Yahoo!, among many others, act as OpenID authentication providers.
Any application that acts as an OpenID client can then use any of these providers to authenticate a user; for example, a user who has a Yahoo! account can choose Yahoo! as the authentication provider.
The user is redirected to Yahoo! for authentication, and on successful authentication is transparently redirected back to the application, and can then continue using the application.
Although the SQL standard supports a fairly flexible system of authorization based on roles (described in Section 4.6), the SQL authorization model plays a very limited role in managing user authorizations in a typical application.
For instance, suppose you want all students to be able to see their own grades, but not the grades of anyone else.
Such authorization cannot be specified in SQL for at least two reasons:
With the growth in the Web, database accesses come primarily fromWeb application servers.
The end users typically do not have individual user identifiers on the database itself, and indeed there may only be a single user identifier in the database corresponding to all users of an application server.
Thus, authorization specification in SQL cannot be used in the above scenario.
It is possible for an application server to authenticate end users, and then.
Authorization must be at the level of individual tuples, if we are to authorize students to see only their own grades.
Such authorization is not possible in the current SQL standard,which permits authorization only on an entire relation or view, or on specified attributes of relations or views.
We could try to get around this limitation by creating for each student a view on the takes relation that shows only that student’s grades.
While this would work in principle, it would be extremely cumbersome since we would have to create one such view for every single student enrolled in the university, which is completely impractical.6
An alternative is to create a view of the form.
Users are then given authorization to this view, rather than to the underlying takes relation.
However, queries executed on behalf of studentsmust now be written on the view studentTakes, rather than on the original takes relation, whereas queries executed onbehalf of instructorsmayneed touse adifferent view.
The task of developing applications becomesmore complex as a result.
The task of authorization is today typically carried out entirely in the application, bypassing the authorization facilities of SQL.
At the application level, users are authorized to access specific interfaces, and may further be restricted to view or update certain data items only.
While carrying out authorization in the application gives a great deal of flexibility to application developers, there are problems, too.
The code for checking authorization becomes intermixed with the rest of the application code.
Implementing authorization through application code, rather than specifying it declaratively in SQL, makes it hard to ensure the absence of loopholes.
Because of an oversight, one of the application programs may not check for authorization, allowing unauthorized users access to confidential data.
Database systems are designed to manage large relations, but manage schema information such as views in a way that assumes smaller data volumes so as to enhance overall performance.
Verifying that all application programs make all required authorization checks involves reading through all the application-server code, a formidable task in a large system.
In other words, applications have a very large “surface area,” making the task of protecting the application significantly harder.
And in fact, security loopholes have been found in a variety of real-life applications.
In contrast, if a database directly supported fine-grained authorization, authorization policies could be specified and enforced at the SQL-level, which has a much smaller surface area.
Even if some of the application interfaces inadvertently omit required authorization checks, the SQL-level authorization could prevent unauthorized actions from being executed.
For example, the Oracle Virtual Private Database (VPD) allows a system administrator to associate a function with a relation; the function returns a predicate that must be added to any query that uses the relation (different functions can be defined for relations that are being updated)
For example, using our syntax for retrieving application user identifiers, the function for the takes relation can return a predicate such as:
This predicate is added to the where clause of every query that uses the takes relation.
As a result (assuming that the application program sets the user id value to the student’s ID), each student can see only the tuples corresponding to courses that she took.
Thus, VPD provides authorization at the level of specific tuples, or rows, of a relation, and is therefore said to be a row-level authorization mechanism.
A potential pitfall with adding a predicate as described above is that it may change the meaning of a query significantly.
For example, if a user wrote a query to find the average grade over all courses, she would end up getting the average of her grades, not all grades.
Although the system would give the “right” answer for the rewritten query, that answer would not correspond to the query the user may have thought she was submitting.
See the bibliographic notes for pointers to more information on Oracle VPD.
An audit trail is a log of all changes (inserts, deletes, and updates) to the application data, along with information such as which user performed the change and when the change was performed.
If application security is breached, or even if security was not breached, but some update was carried out erroneously, an audit trail can (a) help find out what happened, and who may have carried out the actions, and (b) aid in fixing the damage caused by the security breach or erroneous update.
For example, if a student’s grade is found to be incorrect, the audit log can be examined to locate when and how the grade was updated, as well as to find which user carried out the updates.
Audit trails can also be used to detect security breacheswhere a user’s account is compromised and accessed by an intruder.
For example, each time a user logs in, she may be informed about all updates in the audit trail that were done from that login in the recent past; if the user sees a update that she did not carry out, it is likely the account has been compromised.
It is possible to create a database-level audit trail by defining appropriate triggers on relation updates (using system-defined variables that identify the user name and time)
However, many database systems provide built-in mechanisms to create audit trails that are much more convenient to use.
Details of how to create audit trails vary across database systems, and you should refer to the database-system manuals for details.
Database-level audit trails are usually insufficient for applications, since they are usually unable to track who was the end user of the application.
Further, updates are recorded at a low level, in terms of updates to tuples of a relation, rather than at a higher level, in terms of the business logic.
Applications therefore usually create a higher-level audit trail, recording, for example, what action was carried out, by whom, when, and from which IP address the request originated.
A related issue is that of protecting the audit trail itself from being modified or deleted by users who breach application security.
One possible solution is to copy the audit trail to a different machine, to which the intruder would not have access, with each record in the trail copied as soon as it is generated.
In a world where an increasing amount of personal data are available online, people are increasingly worried about the privacy of their data.
For example, most people would want their personal medical data to be kept private and not revealed publicly.
However, the medical data must be made available to doctors and emergencymedical technicianswho treat the patient.Many countries have laws on privacy of such data that define when and to whom the data may be revealed.
Violation of privacy law can result in criminal penalties in some countries.
Applications that access such private data must be built carefully, keeping the privacy laws in mind.
On the other hand, aggregated private data can play an important role in many tasks such as detecting drug side effects, or in detecting the spread of epidemics.
How to make such data available to researchers carrying out such tasks, without compromising the privacy of individuals, is an important realworld problem.
As an example, suppose a hospital hides the name of the patient, but provides a researcher with the date of birth and the zip code (postal code) of the patient (both of which may be useful to the researcher)
Just these two pieces of information can be used to uniquely identify the patient in many cases (using information from an external database), compromising his privacy.
In this particular situation, one solution would be to give the year of birth but not the.
This would not provide enough information to uniquely identify most individuals.7
As another example, Web sites often collect personal data such as address, telephone, email, and credit-card information.
Such information may be required to carry out a transaction such as purchasing an item from a store.
However, the customer may not want the information to be made available to other organizations, or may want part of the information (such as credit-card numbers) to be erased after some period of time as a way to prevent it from falling into unauthorized hands in the event of a security breach.
Many Web sites allow customers to specify their privacy preferences, and must then ensure that these preferences are respected.
Encryption refers to the process of transforming data into a form that is unreadable, unless the reverse process of decryption is applied.
Encryption algorithms use an encryption key to performencryption, and require a decryption key (which could be the same as the encryption key depending on the encryption algorithm used) to perform decryption.
The oldest uses of encryption were for transmitting messages, encrypted using a secret key known only to the sender and the intended receiver.
Even if the message is intercepted by an enemy, the enemy, not knowing the key, will not be able to decrypt and understand the message.
Encryption is widely used today for protecting data in transit in a variety of applications such as data transfer on the Internet, and on cellular phone networks.
Encryption is also used to carry out other tasks, such as authentication, as we will see in Section 9.8.3
In the context of databases, encryption is used to store data in a secure way, so that even if the data is acquired by an unauthorized user (for example, a laptop computer containing the data is stolen), the data will not be accessible without a decryption key.
Many databases today store sensitive customer information, such as creditcard numbers, names, fingerprints, signatures, and identification numbers such as, in the United States, social-security numbers.
A criminal who gets access to such data can use it for a variety of illegal activities such as purchasing goods using a credit-card number, or even acquiring a credit card in someone else’s name.
Organizations such as credit-card companies use knowledge of personal information as a way of identifyingwho is requesting a service or goods.
Leakage of such personal information allows a criminal to impersonate someone else and get access to service or goods; such impersonation is referred to as identity theft.
Thus, applications that store such sensitive data must take great care to protect them from theft.
To reduce the chance of sensitive information being acquired by criminals, many countries and states today require by law that any database storing such sensitive informationmust store the information in an encrypted form.Abusiness that does not protect its data thus could be held criminally liable in case of data theft.
Thus, encryption is a critical component of any application that stores such sensitive information.
There are a vast number of techniques for the encryption of data.
Simple encryption techniques may not provide adequate security, since it may be easy for an unauthorized user to break the code.
As an example of a weak encryption technique, consider the substitution of each character with the next character in the alphabet.
If an unauthorized user sees only “Qfsszsjehf,” she probably has insufficient information to break the code.
However, if the intruder sees a large number of encrypted branch names, she could use statistical data regarding the relative frequency of characters to guess what substitution is being made (for example, E is the most common letter in English text, followed by T, A, O, N, I, and so on)
It is relatively simple for authorized users to encrypt and decrypt data.
It depends not on the secrecy of the algorithm, but rather on a parameter.
In a symmetric-key encryption technique, the encryption key is also used to decrypt data.
In contrast, in public-key (also known as asymmetric-key) encryption techniques, there are two different keys, the public key and the private key, used to encrypt and decrypt the data.
Its decryption key is extremely difficult for an intruder to determine, even if the intruder has access to encrypted data.
In the case of asymmetric-key encryption, it is extremely difficult to infer the private key even if the public key is available.
The Advanced Encryption Standard (AES) is a symmetric-key encryption algorithm that was adopted as an encryption standard by the U.S.
The algorithm runs a series of steps to jumble up the bits in a data block in a way that can be reversed during decryption, and performs an XOR operation with a 128-bit “round key” that is derived from the encryption key.
A new round key is generated from the encryption key for each block of data that is encrypted.
During decryption, the round keys are generated again from the encryption key and the encryption process is reversed to recover the original data.
An earlier standard called the Data Encryption Standard (DES), adopted in 1977, was very widely used earlier.
For any symmetric-key encryption scheme to work, authorized users must be provided with the encryption key via a secure mechanism.
This requirement is a major weakness, since the scheme is no more secure than the security of the mechanism by which the encryption key is transmitted.
Public-key encryption is an alternative scheme that avoids some of the problems faced by symmetric-key encryption techniques.
It is based on two keys: a public key and a private key.
Each userUi has a public key Ei and a private key Di.
All public keys are published: They can be seen by anyone.
Each private key is known to only the one user to whom the key belongs.
Because the encryption key for each user is public, it is possible to exchange information securely by this scheme.
Since only user U2 knows how to decrypt the data, information can be transferred securely.
For public-key encryption to work, there must be a scheme for encryption such that it is infeasible (that is, extremely hard) to deduce the private key, given the public key.
Such a scheme does exist and is based on these conditions:
There is an efficient algorithm for testing whether or not a number is prime.
No efficient algorithm is known for finding the prime factors of a number.
For purposes of this scheme, data are treated as a collection of integers.
The details of public-key encryption and themathematical justification of this technique’s properties are referenced in the bibliographic notes.
Although public-key encryption by this scheme is secure, it is also computationally very expensive.
Encryption of small values, such as identifiers or names, is made complicated by the possibility of dictionary attacks, particularly if the encryption key is publicly available.
For example, if date-of-birth fields are encrypted, an attacker trying to decrypt a particular encrypted value e could try encrypting every possible date of birth until he finds one whose encrypted value matches e.
Even if the encryption key is not publicly available, statistical information about data distributions can be used to figure out what an encrypted value represents in some cases, such as age or zip code.
Dictionary attacks can be deterred by adding extra random bits to the end of the value before encryption (and removing them after decryption)
Such extra bits, referred to as an initialization vector in AES, or as salt bits in other contexts, provide good protection against dictionary attack.
Many file systems and database systems today support encryption of data.
Such encryption protects the data from someone who is able to access the data, but is not able to access the decryption key.
In the case of file-system encryption, the data to be encrypted are usually large files and directories containing information about files.
In the context of databases, encryption can be done at several different levels.
At the lowest level, the disk blocks containing database data can be encrypted, using a key available to the database-system software.
When a block is retrieved fromdisk, it is first decrypted and then used in the usual fashion.
Such disk-blocklevel encryption protects against attackers who can access the disk contents but do not have access to the encryption key.
At the next higher level, specified (or all) attributes of a relation can be stored in encrypted form.
In this case, each attribute of a relation could have a different encryption key.Many databases today support encryption at the level of specified attributes as well as at the level of an entire relation, or all relations in a database.
Encryption of specified attributes minimizes the overhead of decryption, by allowing applications to encrypt only attributes that contain sensitive values such as credit-card numbers.
However, when individual attributes or relations are encrypted, databases typically do not allow primary and foreign key attributes to be encrypted, and do not support indexing on encrypted attributes.
Encryption also then needs to use extra random bits to prevent dictionary attacks, as described earlier.
A decryption key is obviously required to get access to encrypted data.
In this case, the decryption keys for different attributes can be stored in a file or relation (often referred to as “wallet”), which is itself encrypted using a master key.
A connection to the database that needs to access encrypted attributes must then provide the master key; unless this is provided, the connection will not be able to access encrypted data.
The master key would be stored in the application program (typically on a different computer), or memorized by the database user, and provided when the user connects to the database.
Encryption at the database level has the advantage of requiring relatively low time and space overhead, and does not require modification of applications.
For example, if data in a laptop computer database need to be protected from theft of the computer itself, such encryption can be used.
Similarly, someone who gets access to backup tapes of a database would not be able to access the data contained in the backups without knowing the decryption key.
An alternative to performing encryption in the database is to perform it before the data are sent to the database.
The application must then encrypt the data before sending it to the database, and decrypt the data when it is retrieved.
This approach to data encryption requires significant modifications to be done to the application, unlike encryption performed in a database system.
Password-based authentication is used widely by operating systems as well as databases.
However, the use of passwords has some drawbacks, especially over a network.
If an eavesdropper is able to “sniff” the data being sent over the network, she may be able to find the password as it is being sent across the network.
Once the eavesdropper has a user name and password, she can connect to the database, pretending to be the legitimate user.
The database system sends a challenge string to the user.
The user encrypts the challenge string using a secret password as encryption key and then returns the result.
The database system can verify the authenticity of the user by decrypting the string with the same secret password and checking the result with the original challenge string.
This scheme ensures that no passwords travel across the network.
Public-key systems can be used for encryption in challenge–response systems.
The database system encrypts a challenge string using the user’s public key and sends it to the user.
The user decrypts the string using her private key, and returns the result to the database system.
This scheme has the added benefit of not storing the secret password in the database, where it could potentially be seen by system administrators.
Storing the private key of a user on a computer (even a personal computer) has the risk that if the computer is compromised, the key may be revealed to an attacker who can then masquerade as the user.
In a smart card, the key can be stored on an embedded chip; the operating system of the smart card guarantees that the key can never be read, but.
Another interesting application of public-key encryption is in digital signatures to verify authenticity of data; digital signatures play the electronic role of physical signatures on documents.
The private key is used to “sign,” that is, encrypt, data, and the signed data can be made public.
Anyone can verify the signature by decrypting the data using the public key, but no one could have generated the signed data without having the private key.
Note the reversal of the roles of the public and private keys in this scheme.
Thus, we can authenticate the data; that is, we can verify that the data were indeed created by the personwho is supposed to have created them.
That is, in case the person who created the data later claims she did not create it (the electronic equivalent of claiming not to have signed the check), we can prove that that person must have created the data (unless her private key was leaked to others)
Authentication is, in general, a two-way process, where each of a pair of interacting entities authenticates itself to the other.
Such pairwise authentication is needed even when a client contacts a Web site, to prevent a malicious site from masquerading as a legal Web site.
Such masquerading could be done, for example, if the network routers were compromised, and data rerouted to the malicious site.
For a user to ensure that she is interacting with an authentic Web site, she must have the site’s public key.
This raises the problem of how the user can get the public key–if it is stored on theWeb site, themalicious site could supply a different key, and the user would have no way of verifying if the supplied public key is itself authentic.
Authentication can be handled by a system of digital certificates, whereby public keys are signed by a certification agency, whose public key is well known.
For example, the public keys of the root certification authorities are stored in standard Web browsers.
A certificate issued by them can be verified by using the stored public keys.
A two-level system would place an excessive burden of creating certificates on the root certification authorities, so a multilevel system is used instead, with one or more root certification authorities and a tree of certification authorities below each root.
Each authority (other than the root authorities) has a digital certificate issued by its parent.
A digital certificate issued by a certification authority A consists of a public key KA and an encrypted text E that can be decoded by using the public key.
Smart cards provide other functionality too, such as the ability to store cash digitally and make payments, which is not relevant in our context.
The encrypted text contains the name of the party to whom the certificate was issued and her public key Kc.
In case the certification authority A is not a root certification authority, the encrypted text also contains the digital certificate issued to A by its parent certification authority; this certificate authenticates the key KA itself.
That certificate may in turn contain a certificate from a further parent authority, and so on.
Digital certificates are widely used to authenticate Web sites to users, to prevent malicious sites from masquerading as other Web sites.
In the HTTPS protocol (the secure version of the HTTP protocol), the site provides its digital certificate to the browser, which then displays it to the user.
If the user accepts the certificate, the browser then uses the provided public key to encrypt data.
A malicious site will have access to the certificate, but not the private key, and will thus not be able to decrypt the data sent by the browser.
Only the authentic site, which has the corresponding private key, can decrypt the data sent by the browser.
To reduce encryption costs, HTTPS actually creates a one-time symmetric key after authentication, and uses it to encrypt data for the rest of the session.
The user must submit a digital certificate containing her public key to a site, which verifies that the certificate has been signed by a trusted authority.
The user’s public key can then be used in a challenge–response system to ensure that the user possesses the corresponding private key, thereby authenticating the user.
Application programs that use databases as back ends and interact with users have been around since the 1960s.
Today most applications use Web browsers as their front end, and a database as their back end, with an application server in between.
Web browsers communicate with Web servers by the HTTP protocol.
Web servers can pass on requests to application programs, and return the results to the browser.
Servlets are a widely used mechanism to write application programs.
Complex applications usually have a multilayer architecture, including a model implementing business logic, a controller, and a view mechanism to display results.
They may also include a data access layer that implements an object-relational mapping.
Many applications implement and use Web services, allowing functions to be invoked over HTTP.
A number of tools have been developed for rapid application development, and in particular to reduce the effort required to build user interfaces.
Techniques such as caching of various forms, including query result caching and connection pooling, and parallel processing are used to improve application performance.
Application developers must pay careful attention to security, to prevent attacks such as SQL injection attacks and cross-site scripting attacks.
Today application programs implement fine-grained, tuple-level authorization, dealingwith a large number of application users, completely outside the database system.
Database extensions to provide tuple-level access control and to deal with large numbers of application users have been developed, but are not standard as yet.
Protecting the privacy of data is an important task for database applications.
Many countries have legal requirements on protection of certain kinds of data, such as credit-card information or medical data.
Encryption plays a key role in protecting information and in authentication of users andWeb sites.
Symmetric-key encryption and public-key encryption are two contrasting but widely used approaches to encryption.
Encryption of certain sensitive data stored in databases is a legal requirement in many countries and states.
Encryption also plays a key role in authentication of users to applications, of Web sites to users, and for digital signatures.
Explain how this command can be used to find out if a particular Web page is not closing connections that it opened, or if connection pooling is used, not returning connections to the connection pool.
You should account for the fact that with connection pooling, the connection may not get closed immediately.
Suggest an approach for testing an application to find if it is vulnerable to SQL injection attacks on text input.
Can SQL injection occur with other forms of input? If so, how would you test for vulnerability?
Why do database systems not support indexing on encrypted attributes? Using your answer to this question, explain why database systems do not allow encryption of primary-key attributes.
Explain how the problems raised in Exercise 9.8 are avoided when the entire database is encrypted.
What is the effect on things (such as purchase orders or programs) certified by the impersonated company, and on things certified by other companies?
The number of times each value has been submitted previously should be stored in a database.
Your pseudocode must include a function to create a pool (providing a database connection string, database user name, and password as parameters), a function to request a connection from the pool, a connection to release a connection to the pool, and a function to close the connection pool.
List two features each of which reveals if a site uses Ajax, without having to look at the source code.
Using the above features, find three sites which use Ajax; you can view the HTML source of the page to check if the site is actually using Ajax.
How can the referer field be used to detect some XSS attacks?
Section 9.7.5, and an application based on our university schema.
What predicate (using a subquery) should be generated to allow each faculty member to see only takes tuples corresponding to course sections that they have taught?
Give an SQL query such that the querywith the predicate added gives a result that is a subset of the original query result without the added predicate.
Give an SQL query such that the querywith the predicate added gives a result containing a tuple that is not in the result of the original query without the added predicate.
Define triggers to create an audit trail, logging the information into a relation called, for example, takes trail.
The logged information should include the user-id (assume a function user id() provides this information) and a timestamp, in addition to old and new values.
You must also provide the schema of the takes trail relation.
Can the above implementation guarantee that updates made by a malicious database administrator (or someone who manages to get the administrator’s password) will be in the audit trail? Explain your answer.
This may be done by misleading email, or even by breaking into the network infrastructure and rerouting network traffic destined for, saymybank.com, to the hacker’s site.
Explain how the protocol might use digital certificates to verify authenticity of the site.
Why is it more secure than a traditional password-based system?
Each of the following is a large project, which can be a semester-long project done by a group of students.
The difficulty of the project can be adjusted easily by adding or deleting features.
Project 9.1 Pick your favorite interactive Web site, such as Bebo, Blogger, Facebook, Flickr, Last.FM, Twitter, Wikipedia; these are just a few examples, there are many more.
Most of these sites manage a large amount of data, and use databases to store and process the data.
Implement a subset of the functionality of the Web site you picked.
Most of today’s popular Web sites make extensive use of Javascript to create rich interfaces.
You may wish to go easy on this for your project, at least initially, since it takes time to build such intefaces, and then add more features to your interfaces, as time permits.
Make use of Web application development frameworks, or Javascript libraries available on theWeb, such as the Yahoo User Interface library, to speed up your development.
Project 9.2 Create a “mashup” which uses Web services such as Google or Yahoo maps APIs to create an interactiveWeb sites.
For example, the map APIs provide a way to display a map on the Web page, with other information overlayed on the maps.
You could implement a restaurant recommendation system, with users contributing information about restaurants such as location, cuisine, price range, and ratings.
Results of user searches could be displayed on the map.
You could allow Wikipedia-like features, such as allowing users to add information and edit information added by other users, along with moderators who can weed out malicious updates.
You could also implement social features, such as giving more importance to ratings provided by your friends.
Project 9.3 Your university probably uses a course-management systems such as Moodle, Blackboard, or WebCT.
Implement a subset of the functionality of such a course-management system.
You could also provide polls and other mechanisms for getting feedback.
Design and implement a Web-based system to enter, update, and view the data.
Project 9.5 Design and implement a shopping cart system that lets shoppers collect items into a shopping cart (you can decide what information is to be supplied for each item) and purchased together.
You should check for availability of the item and deal with nonavailable items as you feel appropriate.
Project 9.6 Design and implement a Web-based system to record student registration and grade information for courses at a university.
The number of assignments/exams should not be predefined; that is, more assignments/exams can be added at any time.
The system should also support grading, permitting cutoffs to be specified for various grades.
You may also wish to integrate it with the student registration system of Project 9.6 (perhaps being implemented by another project team)
Project 9.8 Design and implement a Web-based system for booking classrooms at your university.
Periodic booking (fixed days/times each week for a whole semester) must be supported.
Cancellation of specific lectures in a periodic booking should also be supported.
You may also wish to integrate it with the student registration system of Project 9.6 (perhaps being implemented by another project team) so that classrooms can be booked for courses, and cancellations of a lecture or addition of extra lectures can be noted at a single interface, and will be reflected in the classroombooking and communicated to students via email.
You should support distributed contribution of questions (by teaching assistants, for example), editing of questions by whoever is in charge of the course, and creation of tests from the available set of questions.
You should also be able to administer tests online, either at a fixed time for all students, or at any time but with a time limit from start to finish (support one or both), and give students feedback on their scores at the end of the allotted time.
Project 9.10 Design and implement a system for managing email customer service.
There is a set of customer service agents who reply to email.
If the email is part of an ongoing series of replies (tracked using the in-reply-to field of email) the mail should preferably be replied to by the same agent who replied earlier.
The system should track all incoming mail and replies, so an agent can see the history of questions from a customer before replying to an email.
You may also wish to support alerting services, whereby a user can register interest in items in a particular category, perhaps with other constraints as well, without publicly advertising her interest, and is notified when such an item is listed for sale.
The system tracks which articles were read by a user, so they are not displayed again.
You may also wish to provide a rating service for articles, so that articles with high rating are highlighted, permitting the busy reader to skip low-rated articles.
Project 9.13 Design and implement a Web-based system for managing a sports “ladder.” Many people register, and may be given some initial rankings (perhaps based on past performance)
Anyone can challenge anyone else to a match, and the rankings are adjusted according to the result.
One simple system for adjusting rankings just moves the winner ahead of the loser in.
The service should permit entering of information about publications, such as title, authors, year, where the publication appeared, and pages.
Authors should be a separate entity with attributes such as name, institution, department, email, address, and home page.
Your application should support multiple views on the same data.
For instance, you should provide all publications by a given author (sorted by year, for example), or all publications by authors from a given institution or department.
You should also support search by keywords, on the overall database as well as within each of the views.
Project 9.15 A common task in any organization is to collect structured information from a group of people.
For example, a manager may need to ask employees to enter their vacation plans, a professor may wish to collect feedback on a particular topic from students, or a student organizing an event may wish to allow other students to register for the event, or someone may wish to conduct an online vote on some topic.
Create a system thatwill allowusers to easily create information collection.
The event creator should be able to specify a set of inputs (with types, default values, and validation checks) that the users will have to provide.
The event should have an associated deadline, and the system should have the ability to send reminders to users who have not yet submitted their information.
The event creator may be given the option of automatic enforcement of the deadline based on a specified date/time, or choosing to login and declare the deadline is over.
Statistics about the submissions should be generated—to do so, the event creator may be allowed to create simple summaries on the entered information.
The event creator may choose to make some of the summaries public, viewable by all users, either continually (e.g., how many people have responded) or after the deadline (e.g., what was the average feedback score)
Project 9.16 Create a library of functions to simplify creation of Web interfaces.
You must implement at least the following functions: a function to display a JDBC result set (with tabular formatting), functions to create different types of text and numeric inputs (with validation criteria such as input type and optional range, enforced at the client by appropriate JavaScript code), functions to input date and time values (with default values), and functions to create menu items based on a result set.
For extra credit, allow the user to set style parameters such as colors and fonts, and provide pagination support in the tables (hidden form parameters can be used to specify which page is to be displayed)
Build a sample database application to illustrate the use of these functions.
Provide interfaces to schedule multiuser events, where an event creator can add a number of users who are invited to the event.
For extra credits implement a Web service that can be used by a reminder program running on the client machine.
Development of a Web application requires several software tools such as an application server, a compiler, and an editor for a programming language such as Java or C#, and other optional tools such as a Web server.
There are several integrated development environments that provide support for Web application development.
The two most popular open-source IDEs are Eclipse, developed by IBM, and Netbeans, developed by SunMicrosystems.
Microsoft’s Visual Studio is the most widely used IDE in the Windows world.
The ApacheWeb server (apache.org) is the most widely used Web server today.
Some of the above tools are open-source software that can be used free of cost, some are free for noncommercial use or for personal use, while others need to be paid for.
Information on JSP tag libraries can also be found at this URL.
Although a database system provides a high-level view of data, ultimately data have to be stored as bits on one or more storage devices.
A vast majority of databases today store data on magnetic disk (and, increasingly, on flash storage) and fetch data into main memory for processing, or copy data onto tapes and other backup devices for archival storage.
Chapter 10 begins with an overview of physical storage media, including mechanisms to minimize the chance of data loss due to device failures.
The chapter then describes how records aremapped to files,which in turn aremapped to bits on the disk.
Many queries reference only a small proportion of the records in a file.
An index is a structure that helps locate desired records of a relation quickly, without examining all records.
The index in this textbook is an example, although, unlike database indices, it is meant for human use.
Chapter 11 describes several types of indices used in database systems.
User queries have to be executed on the database contents, which reside on storage devices.
It is usually convenient to break up queries into smaller operations, roughly corresponding to the relational-algebra operations.
Chapter 12 describes how queries are processed, presenting algorithms for implementing individual operations, and then outlining how the operations are executed in synchrony, to process a query.
There aremany alternativeways of processing a query,which canhavewidely varying costs.
Query optimization refers to the process of finding the lowest-cost method of evaluating a given query.
In preceding chapters,we have emphasized the higher-levelmodels of a database.
For example, at the conceptual or logical level, we viewed the database, in the relational model, as a collection of tables.
Indeed, the logical model of the database is the correct level for database users to focus on.
This is because the goal of a database system is to simplify and facilitate access to data; users of the system should not be burdened unnecessarily with the physical details of the implementation of the system.
We start with characteristics of the underlying storage media, such as disk and tape systems.
We then define various data structures that allow fast access to data.
We consider several alternative structures, each best suited to a different kind of access to data.
The final choice of data structure needs to be made on the basis of the expected use of the system and of the physical characteristics of the specific machine.
These storagemedia are classified by the speed with which data can be accessed, by the cost per unit of data to buy the medium, and by the medium’s reliability.
Cachememory is relatively small; its use is managed by the computer system hardware.
We shall not be concerned about managing cache storage in the database system.
It is, however, worth noting that database implementors do pay attention to cache effects when designing query processing data structures and algorithms.
The storagemedium used for data that are available to be operated on ismainmemory.
Although main memory may contain several gigabytes of data on a personal computer, or even hundreds of gigabytes of data in large server systems, it is generally too small (or too expensive) for storing the entire database.
The contents of main memory are usually lost if a power failure or system crash occurs.
Flash memory differs from main memory in that stored data are retained even if power is turned off (or fails)
There are two types of flash memory, called NAND and NOR flash.
Of these, NAND flash has a much higher storage capacity for a given cost, and is widely used for data storage in devices such as cameras, music players, and cell phones, and increasingly, in laptop computers as well.
Flash memory has a lower cost per byte than main memory, in addition to being nonvolatile; that is, it retains stored data even if power is switched off.
Flashmemory is also widely used for storing data in “USB keys,”which can.
Such USB keys have become a popular means of transporting data between computer systems (“floppy disks” played the same role in earlier days, but their limited capacity has made them obsolete now)
Flash memory is also increasingly used as a replacement for magnetic disks for storing moderate amounts of data.
Further, flash memory is increasingly being used in server systems to improve performance by caching frequently used data, since it provides faster access than disk, with larger storage capacity than main memory (for a given cost)
The primary medium for the long-term online storage of data is the magnetic disk.
The system must move the data from disk to main memory so that they can be accessed.
After the system has performed the designated operations, the data that have been modified must be written to disk.
Disk capacities have been growing at about 50 percent per year, and we can expect disks of much larger capacity every year.
Disk-storage devices themselves may sometimes fail and thus destroy data, but such failures usually occur much less frequently than do system crashes.
The expression digital versatile disk is also used in place of digital video disk, since DVDs can hold any digital data, not just video data.
Data are stored optically on a disk, and are read by a laser.
The optical disks used in read-only compact disks (CD-ROM) or read-only digital video disks (DVD-ROM) cannot be written, but are supplied with data prerecorded.
There are also “record-once” versions of compact disk (called CD-R) and digital video disk (called DVD-R and DVD+R), which can be written only once; such disks are also called write-once, read-many (WORM) disks.
There are also “multiple-write” versions of compact disk (called CD-RW) and digital video disk (DVD-RW, DVD+RW, and DVD-RAM), which can be written multiple times.
Optical disk jukebox systems contain a few drives and numerous disks that can be loaded into one of the drives automatically (by a robot arm) on demand.
Tape storage is used primarily for backup and archival data.
Although magnetic tape is cheaper than disks, access to data is much slower, because the tape must be accessed sequentially from the beginning.
For this reason, tape storage is referred to as sequential-access storage.
In contrast, disk storage is referred to as direct-access storage because it is possible to read data from any location on disk.
The various storage media can be organized in a hierarchy (Figure 10.1) according to their speed and their cost.
As we move down the hierarchy, the cost per bit decreases, whereas the access time increases.
This trade-off is reasonable; if a given storage system were both faster and less expensive than another—other properties being the same —then there would be no reason to use the slower, more expensive memory.
In fact, many early storage devices, including paper tape and core memories, are relegated to museums now that magnetic tape and semiconductor memory have become faster and cheaper.
Magnetic tapes themselves were used to store active data backwhen diskswere expensive and had low storage capacity.
Today, almost all active data are stored on disks, except in very rare cases where they are stored on tape or in optical jukeboxes.
The fastest storage media—for example, cache and main memory—are referred to as primary storage.
The media in the next level in the hierarchy—for example, magnetic disks—are referred to as secondary storage, or online storage.
The media in the lowest level in the hierarchy—for example, magnetic tape and optical-disk jukeboxes—are referred to as tertiary storage, or offline storage.
In addition to the speed and cost of the various storage systems, there is also the issue of storage volatility.
Volatile storage loses its contents when the power to the device is removed.
In the hierarchy shown in Figure 10.1, the storage systems from main memory up are volatile, whereas the storage systems below.
Magnetic disks provide the bulk of secondary storage for modern computer systems.
Although disk capacities have been growing year after year, the storage requirements of large applications have also been growing very fast, in some cases even faster than the growth rate of disk capacities.
In recent years, flash-memory storage sizes have grown rapidly, and flash storage is increasingly becoming a competitor to magnetic disk storage for several applications.
Its two surfaces are covered with a magnetic material, and information is recorded on the surfaces.
There is a read–write head positioned just above the surface of the platter.
The disk surface is logically divided into tracks, which are subdivided into sectors.
A sector is the smallest unit of information that can be read from or written to the disk.
The numbers vary among different models; higher-capacity models usually have more sectors per track and more tracks on each platter.
The read–write head stores information on a sector magnetically as reversals of the direction of magnetization of the magnetic material.
Each side of a platter of a disk has a read–write head that moves across the platter to access different tracks.
A disk typically contains many platters, and the read–write heads of all the tracks are mounted on a single assembly called a disk arm, and move together.
The disk platters mounted on a spindle and the heads mounted on a disk arm are together known as head–disk assemblies.
Since the heads on all the platters move together, when the head on one platter is on the ith track, the heads on all other platters are also on the ith track of their respective platters.Hence, the ith tracks of all the platters together are called the ith cylinder.
Today, disks with a platter diameter of 312 inches dominate the market.
They have a lower cost and faster seek times (due to smaller seek distances) than do the larger-diameter disks (up to 14 inches) that were common earlier, yet they provide high storage capacity.
Disks with even smaller diameters are used in portable devices such as laptop computers, and some handheld computers and portable music players.
The read–write heads are kept as close as possible to the disk surface to increase the recording density.
Because the head floats so close to the surface, platters must be machined carefully to be flat.
If the head contacts the disk surface, the head can scrape the recording medium off the disk, destroying the data that had been there.
In older-generation disks, the head touching the surface caused the removed medium to become airborne and to come between the other heads and their platters, causing more crashes; a head crash could thus result in failure of the entire disk.
Current-generation disk drives use a thin film of magnetic metal as recording medium.
They are much less susceptible to failure by head crashes than the older oxide-coated disks.
A disk controller interfaces between the computer system and the actual hardware of the disk drive; in modern disk systems, the disk controller is implemented within the disk drive unit.
A disk controller accepts high-level commands to read or write a sector, and initiates actions, such as moving the disk arm to the right track and actually reading or writing the data.
Disk controllers also attach checksums to each sector that is written; the checksum is computed from the data written to the sector.
When the sector is read back, the controller computes the checksum again from the retrieved data and compares it with the stored checksum; if the data are corrupted, with a high probability the newly computed checksumwill not match the stored checksum.
If such an error occurs, the controller will retry the read several times; if the error continues to occur, the controller will signal a read failure.
Another interesting task that disk controllers perform is remapping of bad sectors.
If the controller detects that a sector is damaged when the disk is initially formatted, or when an attempt is made to write the sector, it can logicallymap the sector to a different physical location (allocated from a pool of extra sectors set aside for this purpose)
The remapping is noted on disk or in nonvolatile memory, and the write is carried out on the new location.
Disks are connected to a computer system through a high-speed interconnection.
Portable external disk systems often use the USB interface or the IEEE 1394 FireWire interface.
While disks are usually connected directly by cables to the disk interface of the computer system, they can be situated remotely and connected by a high-speed network to the disk controller.
In the storage area network (SAN) architecture, large numbers of disks are connected by a high-speed network to a number.
The disks are usually organized locally using a storage organization technique called redundant arrays of independent disks (RAID) (described later, in Section 10.3), to give the servers a logical view of a very large and very reliable disk.
The computer and the disk subsystem continue to use the SCSI, SAS, or Fiber Channel interface protocols to talk with each other, although they may be separated by a network.
Remote access to disks across a storage area network means that disks can be shared by multiple computers that could run different parts of an application in parallel.
Remote access also means that disks containing important data can be kept in a central server roomwhere they can be monitored and maintained by system administrators, instead of being scattered in different parts of an organization.
Themainmeasures of the qualities of a disk are capacity, access time, data-transfer rate, and reliability.
Access time is the time from when a read or write request is issued to when data transfer begins.
To access (that is, to read or write) data on a given sector of a disk, the arm first must move so that it is positioned over the correct track, and then must wait for the sector to appear under it as the disk rotates.
The time for repositioning the arm is called the seek time, and it increases with the distance that the arm must move.
Smaller disks tend to have lower seek times since the head has to travel a smaller distance.
The average seek time is the average of the seek times, measured over a sequence of (uniformly distributed) random requests.
If all tracks have the same number of sectors, and we disregard the time required for the head to start moving and to stop moving, we can show that the average seek time is one-third the worst-case seek time.
Taking these factors into account, the average seek time is around one-half of themaximum seek time.
Once the head has reached the desired track, the time spent waiting for the sector to be accessed to appear under the head is called the rotational latency time.
On an average, one-half of a rotation of the disk is required for the beginning of the desired sector to appear under the head.
Thus, the average latency time of the disk is one-half the time for a full rotation of the disk.
Once the first sector of the data to be accessed has come under the head, data transfer begins.
The mean time to failure of a disk (or of any other system) is the amount of time that, on average, we can expect the system to run continuously without any failure.
The transfer rate of an interface is shared between all disks attached to the interface, except for the serial interfaces which allow only one disk to be connected to each interface.
Requests for disk I/O are generated both by the file system and by the virtual memory manager found in most operating systems.
Each request specifies the address on the disk to be referenced; that address is in the form of a block number.
A block is a logical unit consisting of a fixed number of contiguous sectors.
Data are transferred between disk and main memory in units of blocks.
The term page is often used to refer to blocks, although in a few contexts (such as flash memory) they refer to different things.
A sequence of requests for blocks from disk may be classified as a sequential access pattern or a random access pattern.
In a sequential access pattern, successive requests are for successive block numbers, which are on the same track, or on adjacent tracks.
To read blocks in sequential access, a disk seek may be required for the first block, but successive requests would either not require a seek, or.
In contrast, in a random access pattern, successive requests are for blocks that are randomly located on disk.
Since only a small amount (one block) of data is read per seek, the transfer rate is significantly lower with a random access pattern than with a sequential access pattern.
A number of techniques have been developed for improving the speed of access to blocks.
Blocks that are read from disk are stored temporarily in an inmemory buffer, to satisfy future requests.
Buffering is done by both the operating system and the database system.
Database buffering is discussed in more detail in Section 10.8
Read-ahead.When a disk block is accessed, consecutive blocks from the same track are read into an in-memory buffer even if there is no pending request for the blocks.
In the case of sequential access, such read-ahead ensures that many blocks are already inmemory when they are requested, andminimizes the time wasted in disk seeks and rotational latency per block read.
Operating systems also routinely perform read-ahead for consecutive blocks of an operating system file.
Read-ahead is, however, not very useful for random block accesses.
If several blocks from a cylinder need to be transferred from disk to main memory, we may be able to save access time by requesting the blocks in the order in which they will pass under the heads.
A commonly used algorithm is the elevator algorithm, which works in the same way many elevators do.
Suppose that, initially, the arm is moving from the innermost track toward the outside of the disk.
Under the elevator algorithm’s control, for each track for which there is an access request, the arm stops at that track, services requests for the track, and then continues moving outward until there are no waiting requests for tracks farther out.
At this point, the arm changes direction, and moves toward the inside, again stopping at each track for which there is a request, until it reaches a track where there is no request for tracks farther toward the center.
Disk controllers usually perform the task of reordering read requests to improve performance, since they are intimately aware of the organization of blocks on disk, of the rotational position of the disk platters, and of the position of the disk arm.
To reduce block-access time, we can organize blocks on disk in a way that corresponds closely to the way we expect data to be accessed.
For example, if we expect a file to be accessed sequentially, then we should ideally keep all the blocks of the file sequentially on adjacent cylinders.
Older operating systems, such as the IBM mainframe operating systems, provided programmers fine control on placement of files, allowing a programmer to reserve a set of cylinders for storing a file.
Subsequent operating systems, such as Unix and Microsoft Windows, hide the disk organization from users, and manage the allocation internally.
Although they do not guarantee that all blocks of a file are laid out sequentially, they allocate multiple consecutive blocks (an extent) at a time to a file.
Sequential access to the file then only needs one seek per extent, instead of one seek per block.
Over time, a sequential file that has multiple small appends may become fragmented; that is, its blocks become scattered all over the disk.
To reduce fragmentation, the system can make a backup copy of the data on disk and restore the entire disk.
The restore operation writes back the blocks of each file contiguously (or nearly so)
Some systems (such as different versions of the Windows operating system) have utilities that scan the disk and then move blocks to decrease the fragmentation.
The performance increases realized from these techniques can be large.
Since the contents of main memory are lost in a power failure, information about database updates has to be recorded on disk to survive possible system crashes.
We can use nonvolatile random-access memory (NVRAM) to speed up disk writes drastically.
The contents of NVRAM are not lost in power failure.
A common way to implement NVRAM is to use battery–backed-up RAM, although flash memory is also increasingly being used for nonvolatile write buffering.
The idea is that, when the database system (or the operating system) requests that a block be written to disk, the disk controller writes the block to an NVRAM buffer, and immediately notifies the operating system that the write completed successfully.
The controller writes the data to their destination on disk whenever the disk does not have any other requests, or when the NVRAM buffer becomes full.
When the database system requests a block write, it notices a delay only if the NVRAM buffer is full.
On recovery from a system crash, any pending buffered writes in the NVRAM are written back to the disk.
Another approach to reducing write latencies is to use a log diskthat is, a disk devoted to writing a sequential log—in much the same way as a nonvolatile RAM buffer.
As before, the data have to be written to their actual location on disk as well, but the log disk can do the write later, without the database system having to wait for the write to complete.
Furthermore, the log disk can reorder the writes to minimize disk-arm movement.
If the system crashes before some writes to the actual disk location have completed, when the system comes back up it reads the log disk to find those writes that had not been completed, and carries them out then.
File systems that support log disks as above are called journaling file systems.
Journaling file systems can be implemented evenwithout a separate log disk, keeping data and the log on the same disk.
Doing so reduces the monetary cost, at the expense of lower performance.
Mostmodern file systems implement journaling, and use the log diskwhen.
Earlier-generation file systems allowed write reordering without using a log disk, and ran the risk that the file system data structures on disk would be corrupted if the system crashed.
Suppose, for example, that a file systemused a linked list, and inserted a new node at the end by first writing the data for the new node, then updating the pointer from the previous node.
Suppose also that the writes were reordered, so the pointer was updated first, and the system crashes before the new node is written.
The contents of the node would then be whatever junk was on disk earlier, resulting in a corrupted data structure.
To deal with the possibility of such data structure corruption, earliergeneration file systems had to perform a file system consistency check on system restart, to ensure that the data structures were consistent.
And if they were not, extra steps had to be taken to restore them to consistency.
These checks resulted in long delays in system restart after a crash, and the delays becameworse as disk systemsgrew tohigher capacities.
Journalingfile systems allow quick restart without the need for such file system consistency checks.
However, writes performed by applications are usually not written to the log disk.
Asmentioned in Section 10.1, there are two types of flash memory, NOR flash and NAND flash.
Pages in a NAND flash are thus similar to sectors in a magnetic disk.
But NAND flash is significantly cheaper than NOR flash, and has much higher storage capacity, and is by far the more widely used.
Storage systems built using NAND flash provide the same block-oriented interface as disk storage.
Flash memory has a lower transfer rate than magnetic disks, with 20 megabytes per second being common.
However, solid state drives use multiple flash memory chips in parallel, to increase transfer rates to over 200 megabytes per second, which is faster than transfer rates of most disks.
A write to a page of flash memory typically takes a fewmicroseconds.
However, once written, a page of flash memory cannot be directly overwritten.
The size of an erase block (often referred to as just “block” in flash literature) is usually significantly larger than the block size of the storage system.
Once this limit is reached, errors in storing bits are likely to occur.
Flash memory systems limit the impact of both the slow erase speed and the update limits bymapping logical page numbers to physical page numbers.When a logical page is updated, it can be remapped to any already erased physical page, and the original location can be erased later.
Each physical page has a small area of memory where its logical address is stored; if the logical address is remapped to a different physical page, the original physical page is marked as deleted.
Thus by scanning the physical pages, we can find where each logical page resides.
Blocks containing multiple deleted pages are periodically erased, taking care to first copy nondeleted pages in those blocks to a different block (the translation table is updated for these nondeleted pages)
Since each physical page can be updated only a fixed number of times, physical pages that have been erased many times are assigned “cold data,” that is, data that are rarely updated, while pages that have not been erased many times are used to store “hot data,” that is, data that are updated frequently.
This principle of evenly distributing erase operations across physical blocks is calledwear leveling, and is usually performed transparently by flash-memory controllers.
If a physical page is damaged due to an excessive number of updates, it can be removed from usage, without affecting the flash memory as a whole.
File systems and database storage structures can thus see an identical logical view of the underlying storage structure, regardless of whether it is flash or magnetic storage.
Hybrid disk drives are hard-disk systems that combine magnetic storage with a smaller amount of flash memory, which is used as a cache for frequently.
Frequently accessed data that are rarely updated are ideal for caching in flash memory.
The data-storage requirements of some applications (in particularWeb, database, and multimedia applications) have been growing so fast that a large number of disks are needed to store their data, even though disk-drive capacities have been growing very fast.
Having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written, if the disks are operated in parallel.
Several independent reads or writes can also be performed in parallel.
Furthermore, this setup offers the potential for improving the reliability of data storage, because redundant information can be stored on multiple disks.
Thus, failure of one disk does not lead to loss of data.
A variety of disk-organization techniques, collectively called redundant arrays of independent disks (RAID), have been proposed to achieve improved performance and reliability.
In the past, system designers viewed storage systems composed of several small, cheap disks as a cost-effective alternative to using large, expensive disks; the cost permegabyte of the smaller diskswas less than that of larger disks.
In fact, the I in RAID, which now stands for independent, originally stood for inexpensive.
Today, however, all disks are physically small, and larger-capacity disks actually have a lower cost per megabyte.
Another key justification for RAID use is easier management and operations.
The chance that at least one disk out of a set of N disks will fail is much higher than the chance that a specific single disk will fail.
The solution to the problem of reliability is to introduce redundancy; that is, we store extra information that is not needed normally, but that can be used in the event of failure of a disk to rebuild the lost information.
Thus, even if a disk fails, data are not lost, so the effective mean time to failure is increased, provided that we count only failures that lead to loss of data or to nonavailability of data.
The simplest (but most expensive) approach to introducing redundancy is to duplicate every disk.
A logical disk then consists of two physical disks, and every write is carried.
If one of the disks fails, the data can be read from the other.
Data will be lost only if the second disk fails before the first failed disk is repaired.
You should be aware that the assumption of independence of disk failures is not valid.
Power failures, and natural disasters such as earthquakes, fires, and floods, may result in damage to both disks at the same time.
As disks age, the probability of failure increases, increasing the chance that a second disk will fail while the first is being repaired.
In spite of all these considerations, however, mirrored-disk systems offer much higher reliability than do single-disk systems.
Power failures are a particular source of concern, since they occur far more frequently than do natural disasters.
Power failures are not a concern if there is no data transfer to disk in progress when they occur.
However, even with mirroring of disks, if writes are in progress to the same block in both disks, and power fails before both blocks are fully written, the two blocks can be in an inconsistent state.
The solution to this problem is to write one copy first, then the next, so that one of the two copies is always consistent.
Some extra actions are required when we restart after a power failure, to recover from incomplete writes.
Now let us consider the benefit of parallel access to multiple disks.
With disk mirroring, the rate at which read requests can be handled is doubled, since read requests can be sent to either disk (as long as both disks in a pair are functional, as is almost always the case)
The transfer rate of each read is the same as in a single-disk system, but the number of reads per unit time has doubled.
With multiple disks, we can improve the transfer rate as well (or instead) by striping data across multiple disks.
In its simplest form, data striping consists of splitting the bits of each byte across multiple disks; such striping is called bitlevel striping.
For example, if we have an array of eight disks, we write bit i of each byte to disk i.
The array of eight disks can be treated as a single disk with sectors that are eight times the normal size, and, more important, that has eight times the transfer rate.
In such an organization, every disk participates in every access (read or write), so the number of accesses that can be processed per second is about the same as on a single disk, but each access can read eight times as many.
For example, if we use an array of four disks, bits i and 4 + i of each byte go to disk i.
Block-level striping is the most commonly used form of data striping.
Other levels of striping, such as bytes of a sector or sectors of a block, also are possible.
In summary, there are two main goals of parallelism in a disk system:
Load-balance multiple small accesses (block accesses), so that the throughput of such accesses increases.
Parallelize large accesses so that the response time of large accesses is reduced.
Striping provides high data-transfer rates, but does not improve reliability.
Various alternative schemes aim to provide redundancy at lower cost by combining disk stripingwith “parity” bits (which we describe next)
The schemes are classified into RAID levels, as in Figure 10.3
In the figure, P indicates error-correcting bits, and C indicates a second copy of the data.
For all levels, the figure depicts four disks’ worth of data, and the extra disks depicted are used to store redundant information for failure recovery.
The mirrored array can still be used, so there is no loss of data.
Similarly, if the stored parity bit gets damaged, it will not match the computed parity.
Thus, all 1-bit errors will be detected by thememory system.
Error-correcting schemes store 2 or more extra bits, and can reconstruct the data if a single bit gets damaged.
The idea of error-correcting codes can be used directly in disk arrays by striping bytes across disks.
If one of the disks fails, the remaining bits of the byte and the associated error-correction bits can be read from other disks, and can be used to reconstruct the damaged data.
Since reads and writes of a byte are spread out over multiple disks, with N-way striping of data, the transfer rate for reading or writing a single block is N times faster than a RAID level 1 organization using N-way striping.
On the other hand, RAID level 3 supports a lower number of I/O operations per second, since every disk has to participate in every I/O request.
If one of the disks fails, the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk.
Ablock read accesses only one disk, allowing other requests to be processed by the other disks.
Thus, the data-transfer rate for each access is slower, but multiple read accesses can proceed in parallel, leading to a higher overall I/O rate.
The transfer rates for large reads is high, since all the disks can be read in parallel; large writes also have high transfer rates, since the data and parity can be written in parallel.
Small independent writes, on the other hand, cannot be performed in parallel.
A write of a block has to access the disk on which the block is stored, as well as the parity disk, since the parity block has to be updated.
Moreover, both the old value of the parity block and the old value of the block being written have to be read for the new parity to be computed.
Thus, a single write requires four disk accesses: two to read the two old blocks, and two to write the two blocks.
For each set of N logical blocks, one of the disks stores the parity, and the other N disks store the blocks.
Note that a parity block cannot store parity for blocks in the same disk, since then a disk failure would result in loss of data as well as of parity, and hence would not be recoverable.
Instead of using parity, level 6 uses error-correcting codes such as the ReedSolomon codes (see the bibliographical notes)
Finally, we note that several variations have been proposed to the basic RAID schemes described here, and different vendors use different terminologies for the variants.
The factors to be taken into account in choosing a RAID level are:
The time to rebuild the data of a failed disk can be significant, and it varies with the RAID level that is used.
Rebuilding is easiest for RAID level 1, since data can be copied from another disk; for the other levels, we need to access all the other disks in the array to rebuild data of a failed disk.
The rebuild performance of a RAID system may be an important factor if continuous availability of data is required, as it is in high-performance database systems.
Furthermore, since rebuild time can form a significant part of the repair time, rebuild performance also influences the mean time to data loss.
For small transfers, the disk access time dominates anyway, so the benefit of parallel reads diminishes.
For applications where data are read frequently, and written rarely, level 5 is the preferred choice.
Disk-storage capacities have been growing at a rate of over 50 percent per year for many years, and the cost per byte has been falling at the same rate.
For example, how many disks should there be in an array? How many bits should be protected by each parity bit? If there are more disks in an array, data-transfer rates are higher, but the system will be more expensive.
If there are more bits protected by a parity bit, the space overhead due to parity bits is lower, but there is an increased chance that a second disk will fail before the first failed disk is repaired, and that will result in data loss.
Another issue in the choice of RAID implementations is at the level of hardware.
However, there are significant benefits to be had by building special-purpose hardware to support RAID, which we outline below; systemswith special hardware support are called hardware RAID systems.
Hardware RAID implementations can use nonvolatile RAM to record writes before they are performed.
Even if all writes are completed properly, there is a small chance of a sector in a disk becoming unreadable at some point, even though it was successfully written earlier.
Reasons for loss of data on individual sectors could range from manufacturing defects, to data corruption on a track when an adjacent track is written repeatedly.
Such loss of data that were successfully written earlier is sometimes referred to as a latent failure, or as bit rot.
When such a failure happens, if it is detected early the data can be recovered from the remaining disks in the RAID organization.
However, if such a failure remains undetected, a single disk failure could lead to data loss if a sector in one of the other disks has a latent failure.
To minimize the chance of such data loss, good RAID controllers perform scrubbing; that is, during periods when disks are idle, every sector of every disk is read, and if any sector is found to be unreadable, the data are recovered from the remaining disks in the RAID organization, and the sector is written back.
If the physical sector is damaged, the disk controller would remap the logical sector address to a different physical sector on disk.
The power supply, or the disk controller, or even the system interconnection in a RAID system could become a single point of failure that could stop functioning of the RAID system.
To avoid this possibility, good RAID implementations have multiple redundant power supplies (with battery backups so they continue to function even if power fails)
Such RAID systems have multiple disk interfaces, andmultiple interconnections to connect the RAID system to the computer system (or to a network of computer systems)
Thus, failure of any single component will not stop the functioning of the RAID system.
The concepts of RAID have been generalized to other storage devices, including arrays of tapes, and even to the broadcast of data over wireless systems.
When applied to arrays of tapes, the RAID structures are able to recover data even if one of the tapes in an array of tapes is damaged.
When applied to broadcast of data, a block of data is split into short units and is broadcast along with a parity unit; if one of the units is not received for any reason, it can be reconstructed from the other units.
In a large database system, someof the datamayhave to reside on tertiary storage.
The two most common tertiary storage media are optical disks and magnetic tapes.
Compact disks have been a popular medium for distributing software, multimedia data such as audio and images, and other electronically published information.
Digital video disks (DVDs) have now replaced compact disks in applications that require larger amounts of data.
CDandDVDdrives havemuch longer seek times (100milliseconds is common) than do magnetic-disk drives, since the head assembly is heavier.
Rotational speeds are typically lower than those of magnetic disks, although the faster CD and DVD drives have rotation speeds of about 3000 rotations per minute, which is comparable to speeds of lower-end magnetic-disk drives.
Rotational speeds of CD drives originally corresponded to the audio CD standards, and the speeds of DVD drives originally corresponded to the DVD video standards, but currentgeneration drives rotate at many times the standard rate.
The record-once versions of optical disks (CD-R, DVD-R, and DVD+R) are popular for distribution of data and particularly for archival storage of data because they have a high capacity, have a longer lifetime than magnetic disks, and can be removed and stored at a remote location.
Since they cannot be overwritten, they can be used to store information that should not be modified, such as audit trails.
The multiple-write versions (CD-RW, DVD-RW, DVD+RW, and DVD-RAM) are also used for archival purposes.
The aggregate storage capacity of such a system can be many terabytes.
When a disk is accessed, it is loaded by a mechanical arm from a rack onto a drive (any disk that was already in the drive must first be placed back on the rack)
The disk load/unload time is usually of the order of a few seconds —very much longer than disk access times.
Although magnetic tapes are relatively permanent, and can hold large volumes of data, they are slow in comparison tomagnetic and optical disks.
Thus, they cannot provide random access for secondary-storage requirements, although historically, prior to the use of magnetic disks, tapes were used as a secondary-storage medium.
Tapes are used mainly for backup, for storage of infrequently used information, and as an off-line medium for transferring information from one system to another.
Tapes are also used for storing large volumes of data, such as video or image data, that either do not need to be accessible quickly or are so voluminous that magnetic-disk storage would be too expensive.
A tape is kept in a spool, and is wound or rewound past a read–write head.
Moving to the correct spot on a tape can take seconds or evenminutes, rather than.
Capacities vary, depending on the length and width of the tape and on the density at which the head can read and write.
The market is currently fragmented among a wide variety of tape formats.
Data-transfer rates are of the order of a few to tens of megabytes per second.
Tape devices are quite reliable, and good tape drive systems perform a read of the just-written data to ensure that it has been recorded correctly.
Tapes, however, have limits on the number of times that they can be read or written reliably.
Tape jukeboxes, like optical disk jukeboxes, hold large numbers of tapes,with a few drives onto which the tapes can be mounted; they are used for storing large volumes of data, ranging up to many petabytes (1015 bytes), with access times on the order of seconds to a fewminutes.
Applications that need such enormous data storage include imaging systems that gather data from remote-sensing satellites, and large video libraries for television broadcasters.
Some tape formats (such as the Accelis format) support faster seek times (of the order of tens of seconds), and are intended for applications that retrieve information from jukeboxes.
Most other tape formats provide larger capacities, at the cost of slower access; such formats are ideal for data backup, where fast seeks are not important.
Tape drives have been unable to keep up with the enormous improvements in disk drive capacity and corresponding reduction in storage cost.
While the cost of tapes is low, the cost of tape drives and tape libraries is significantly higher than the cost of a disk drive: a tape library capable of storing a few terabytes can costs tens of thousands of dollars.
Backing up data to disk drives has become a cost-effective alternative to tape backup for a number of applications.
A database is mapped into a number of different files that are maintained by the underlying operating system.
A file is organized logically as a sequence of records.
Files are provided as a basic construct in operating systems, so we shall assume the existence of an underlying file system.
We need to consider ways of representing logical data models in terms of files.
Each file is also logically partitioned into fixed-length storage units called blocks, which are the units of both storage allocation and data transfer.
Larger block sizes can be useful in some database applications.
A block may contain several records; the exact set of records that a block contains is determined by the form of physical data organization being used.
This assumption is realistic formost data-processing applications, such as our university example.
There are certainly several kinds of large data items, such as images, that can be significantly larger than a block.
We briefly discuss how to handle such large data items later, in Section 10.5.2, by storing large data items separately, and storing a pointer to the data item in the record.
In addition, we shall require that each record is entirely contained in a single block; that is, no record is contained partly in one block, and partly in another.
This restriction simplifies and speeds up access to data items.
In a relational database, tuples of distinct relations are generally of different sizes.
One approach to mapping the database to files is to use several files, and to store records of only one fixed length in any given file.
An alternative is to structure our files so that we can accommodate multiple lengths for records; however, files of fixed-length records are easier to implement than are files of variable-length records.Manyof the techniquesused for the former canbe applied to the variable-length case.
Thus, we begin by considering a file of fixed-length records, and consider storage of variable-length records later.
As an example, let us consider a file of instructor records for our university database.
Each record of this file is defined (in pseudocode) as:
Suppose that instead of allocating a variable amount of bytes for the attributes ID, name, and dept name, we allocate the maximum number of bytes that each attribute can hold.
Unless the block size happens to be a multiple of 53 (which is unlikely), some records will cross block boundaries.
That is, part of the record will be stored in one block and part in another.
It would thus require two block accesses to read or write such a record.
It is difficult to delete a record from this structure.
The space occupied by the record to be deleted must be filled with some other record of the file, or wemust have a way of marking deleted records so that they can be ignored.
To avoid the first problem, we allocate only as many records to a block as wouldfit entirely in the block (this number canbe computed easily bydividing the block size by the record size, and discarding the fractional part)
When a record is deleted, we couldmove the record that came after it into the space formerly occupied by the deleted record, and so on, until every record following the deleted record has beenmoved ahead (Figure 10.5)
Such an approach requires moving a large number of records.
It might be easier simply to move the final record of the file into the space occupied by the deleted record (Figure 10.6)
It is undesirable tomove records to occupy the space freedby adeleted record, since doing so requires additional block accesses.
Since insertions tend to bemore frequent than deletions, it is acceptable to leave open the space occupied by the.
A simple marker on a deleted record is not sufficient, since it is hard to find this available space when an insertion is being done.
At the beginning of the file, we allocate a certain number of bytes as a file header.
The headerwill contain a variety of information about the file.
For now, all we need to store there is the address of the first recordwhose contents are deleted.
We use this first record to store the address of the second available record, and so on.
Intuitively, we can think of these stored addresses as pointers, since they point to the location of a record.
The deleted records thus form a linked list, which is often referred to as a free list.
On insertion of a new record, we use the record pointed to by the header.
We change the header pointer to point to the next available record.
If no space is available, we add the new record to the end of the file.
Insertion and deletion for files of fixed-length records are simple to implement, because the space made available by a deleted record is exactly the space needed to insert a record.
If we allow records of variable length in a file, this match no longer holds.
An inserted record may not fit in the space left free by a deleted record, or it may fill only part of that space.
Record types that allow variable lengths for one or more fields.
Record types that allow repeating fields, such as arrays or multisets.
How to represent a single record in such a way that individual attributes can be extracted easily.
How to store variable-length records within a block, such that records in a block can be extracted easily.
The representation of a record with variable-length attributes typically has twoparts: an initial partwithfixed length attributes, followedbydata for variablelength attributes.
Fixed-length attributes, such as numeric values, dates, or fixedlength character strings are allocated as many bytes as required to store their value.
Variable-length attributes, such as varchar types, are represented in the initial part of the record by a pair (offset, length), where offset denotes where the data for that attribute begins within the record, and length is the length in bytes of the variable-sized attribute.
The values for these attributes are stored consecutively, after the initial fixed-length part of the record.
Thus, the initial part of the record stores a fixed size of information about each attribute, whether it is fixed-length or variable-length.
An example of such a record representation is shown in Figure 10.8
The figure shows an instructor record, whose first three attributes ID, name, and dept name are variable-length strings, and whose fourth attribute salary is a fixed-sized number.
We assume that the offset and length values are stored in two bytes each, for a total of 4 bytes per attribute.
The salary attribute is assumed to be stored in 8 bytes, and each string takes as many bytes as it has characters.
The figure also illustrates the use of a null bitmap, which indicates which attributes of the record have a null value.
Since the record has four attributes, the null bitmap for this recordfits in 1 byte, althoughmore bytesmaybe requiredwith more attributes.
In some representations, the null bitmap is stored at the beginning of the record, and for attributes that are null, no data (value, or offset/length) are stored at all.
Such a representation would save some storage space, at the cost of extra work to extract attributes of the record.
This representation is particularly useful for certain applications where records have a large number of fields, most of which are null.
We next address the problem of storing variable-length records in a block.
The slotted-page structure is commonly used for organizing records within a block, and is shown in Figure 10.9.3 There is a header at the beginning of each block, containing the following information:
An array whose entries contain the location and size of each record.
The actual records are allocated contiguously in the block, starting from the end of the block.
The free space in the block is contiguous, between the final entry in the header array, and the first record.
If a record is inserted, space is allocated for it at the end of free space, and an entry containing its size and location is added to the header.
The slotted-page structure requires that there be nopointers that point directly to records.
Instead, pointersmust point to the entry in the header that contains the.
This level of indirection allows records tobemoved to prevent fragmentation of space inside a block, while supporting indirect pointers to the record.
Databases often store data that can be much larger than a disk block.
For instance, an image or an audio recording may be multiple megabytes in size, while a video object may be multiple gigabytes in size.
Recall that SQL supports the types blob and clob, which store binary and character large objects.
Most relational databases restrict the size of a record to be no larger than the size of a block, to simplify buffer management and free-space management.
Large objects are often stored in a special file (or collection of files) instead of being stored with the other (short) attributes of records in which they occur.
A (logical) pointer to the object is then stored in the record containing the large object.
Large objects are often represented using B+-tree file organizations, which we study in Section 11.4.1
B+-tree file organizations permit us to read an entire object, or specified byte ranges in the object, as well as to insert and delete parts of the object.
So far, we have studied how records are represented in a file structure.
Given a set of records, the next question is how to organize them in a file.
Several of the possible ways of organizing records in files are:
Any record can be placed anywhere in the file where there is space for the record.
Records are stored in sequential order, according to the value of a “search key” of each record.
A hash function is computed on some attribute of each record.
The result of the hash function specifies in which block of the.
Chapter 11 describes this organization; it is closely related to the indexing structures described in that chapter.
For example, records of the two relations can be considered to be related if they would match in a join of the two relations.
A sequential file is designed for efficient processing of records in sorted order based on some search key.
A search key is any attribute or set of attributes; it need not be the primary key, or even a superkey.
To permit fast retrieval of records in search-key order, we chain together records by pointers.
The pointer in each record points to the next record in search-key order.
Furthermore, tominimize the number of block accesses in sequential file processing,we store records physically in search-key order, or as close to search-key order as possible.
Figure 10.10 shows a sequential file of instructor records taken from our university example.
In that example, the records are stored in search-key order, using ID as the search key.
It is difficult, however, to maintain physical sequential order as records are inserted and deleted, since it is costly to move many records as a result of a single.
We can manage deletion by using pointer chains, as we saw previously.
Locate the record in the file that comes before the record to be inserted in search-key order.
If there is a free record (that is, space left after a deletion) within the same block as this record, insert the new record there.
In either case, adjust the pointers so as to chain together the records in search-key order.
The structure in Figure 10.11 allows fast insertion of new records, but forces sequential file-processing applications to process records in an order that does not match the physical order of the records.
If relatively few records need to be stored in overflow blocks, this approach works well.
Eventually, however, the correspondence between search-key order and physical order may be totally lost over a period of time, in which case sequential processing will become much less efficient.
At this point, the file should be reorganized so that it is once again physically in sequential order.
Such reorganizations are costly, and must be done during times when the system load is low.
The frequency with which reorganizations are needed depends on the frequency of insertion of new records.
In the extreme case in which insertions rarely occur, it is possible always to keep the file in physically sorted order.
In such a case, the pointer field in Figure 10.10 is not needed.
Many relational database systems store each relation in a separate file, so that they can take full advantage of the file system that the operating system provides.
Usually, tuples of a relation can be represented as fixed-length records.
Thus, relations can be mapped to a simple file structure.
This simple implementation of a relational database system is well suited to low-cost database implementations as in, for example, embedded systems or portable devices.
In such systems, the size of the database is small, so little is gained from a sophisticated file structure.
Furthermore, in such environments, it is essential that the overall size of the object code for the database system be small.
A simple file structure reduces the amount of code needed to implement the system.
This simple approach to relational database implementation becomes less satisfactory as the size of the database increases.
We have seen that there are performance advantages to be gained from careful assignment of records to blocks, and from careful organization of the blocks themselves.
Clearly, a more complicated file structuremay be beneficial, even if we retain the strategy of storing each relation in a separate file.
However, many large-scale database systems do not rely directly on the underlying operating system for file management.
Instead, one large operatingsystem file is allocated to the database system.
The database system stores all relations in this one file, and manages the file itself.
Even if multiple relations are stored in a single file, by default most databases store records of only one relation in a given block.
However, in some cases it can be useful to store records of more than one relation in a single block.
To see the advantage of storing records of multiple relations in one block, consider the following SQL query for the university database:
This query computes a join of the department and instructor relations.
Thus, for each tuple of department, the system must locate the instructor tuples with the same value for dept name.
Regardless of how these records are located, however, they need to be transferred from disk into main memory.
In the worst case, each record will reside on a different block, forcing us to do one block read for each record required by the query.
In Figure 10.14, we show a file structure designed for efficient execution of queries involving the natural join of department and instructor.
The instructor tuples for each ID are stored near the department tuple for the corresponding dept name.
This structure mixes together tuples of two relations, but allows for efficient processing of the join.
When a tuple of the department relation is read, the entire block containing that tuple is copied from disk into mainmemory.
Since the corresponding instructor tuples are stored on the disk near the department tuple, the block containing the department tuple contains tuples of the instructor relation needed to process the query.
If a department has so many instructors that the instructor records do not fit in one block, the remaining records appear on nearby blocks.
A multitable clustering file organization is a file organization, such as that illustrated in Figure 10.14, that stores related records of two or more relations in each block.
Such a file organization allows us to read records that would satisfy the join condition by using one block read.
Thus, we are able to process this particular query more efficiently.
In the representation shown in Figure 10.14, the dept name attribute is omitted from instructor records since it can be inferred from the associated department record; the attribute may be retained in some implementations, to simplify access to the attributes.We assume that each record contains the identifier of the relation to which it belongs, although this is not shown in Figure 10.14
Our use of clustering of multiple tables into a single file has enhanced processing of a particular join (that of department and instructor), but it results in slowing processing of other types of queries.
When multitable clustering is to be used depends on the types of queries that the database designer believes to be most frequent.
Careful use of multitable clustering can produce significant performance gains in query processing.
So far, we have considered only the representation of the relations themselves.
A relational database system needs to maintain data about the relations, such as the schema of the relations.
In general, such “data about data” is referred to as metadata.
Relational schemas and other metadata about relations are stored in a structure called thedata dictionary or system catalog.
Among the types of information that the system must store are these:
Names of views defined on the database, and definitions of those views.
In addition, many systems keep the following data on users of the system:
Further, the databasemay store statistical anddescriptivedata about the relations, such as:
Method of storage for each relation (for example, clustered or nonclustered)
The data dictionary may also note the storage organization (sequential, hash, or heap) of relations, and the location where each relation is stored:
If relations are stored in operating system files, the dictionary would note the names of the file (or files) containing each relation.
If the database stores all relations in a single file, the dictionary may note the blocks containing records of each relation in a data structure such as a linked list.
In Chapter 11, in which we study indices, we shall see a need to store information about each index on each of the relations:
All this metadata information constitutes, in effect, a miniature database.
Some database systems store such metadata by using special-purpose data structures and code.
It is generally preferable to store the data about the database as relations in the database itself.
By using database relations to store system metadata, we simplify the overall structure of the system and harness the full power of the database for fast access to system data.
The exact choice of how to represent system metadata by relations must be made by the system designers.
One possible representation, with primary keys underlined, is shown in Figure 10.16
In this representation, the attribute index attributes of the relation Index metadata is assumed to contain a list of one or more attributes, which can be represented by a character string such as “dept name, building”
The Index metadata relation is thus not in first normal form; it can be normalized, but the above representation is likely to be more efficient to access.
The data dictionary is often stored in a nonnormalized form to achieve fast access.
Whenever the database system needs to retrieve records from a relation, it must first consult the Relation metadata relation to find the location and storage organization of the relation, and then fetch records using this information.
However, the storage organization and location of the Relation metadata relation itself.
Amajor goal of the database system is to minimize the number of block transfers between the disk and memory.
One way to reduce the number of disk accesses is to keep as many blocks as possible in main memory.
The goal is to maximize the chance that, when a block is accessed, it is already in main memory, and, thus, no disk access is required.
Since it is not possible to keep all blocks in main memory, we need to manage the allocation of the space available in main memory for the storage of blocks.
The buffer is that part of main memory available for storage of copies of disk blocks.
There is always a copy kept on disk of every block, but the copy on disk may be a version of the block older than the version in the buffer.
The subsystem responsible for the allocation of buffer space is called the buffer manager.
Programs in a database systemmake requests (that is, calls) on the buffermanager when they need a block from disk.
If the block is already in the buffer, the buffer manager passes the address of the block in main memory to the requester.
The thrown-out block is written back to disk only if it has been modified since themost recent time that it waswritten to the disk.
Then, the buffermanager reads in the requested block from the disk to the buffer, and passes the address of the block in main memory to the requester.
The internal actions of the buffer manager are transparent to the programs that issue disk-block requests.
If you are familiar with operating-system concepts, you will note that the buffer manager appears to be nothing more than a virtual-memory manager, like those found in most operating systems.
One difference is that the size of the database might be larger than the hardware address space of a machine, so memory addresses are not sufficient to address all disk blocks.
Further, to serve the database system well, the buffer manager must use techniques more sophisticated than typical virtual-memory management schemes:
When there is no room left in the buffer, a block must be removed from the buffer before a new one can be read in.
Most operating systems use a least recently used (LRU) scheme, in which the block that was referenced least recently is written back to disk and is removed from the buffer.
This simple approach can be improved on for database applications.
For the database system to be able to recover from crashes (Chapter 16), it is necessary to restrict those times when a block may be written back to disk.
For instance, most recovery systems require that a block should not be written to disk while an update on the block is in progress.
A block that is not allowed to be written back to disk is said to be pinned.
Although many operating systems do not support pinned blocks, such a feature is essential for a database system that is resilient to crashes.
There are situations in which it is necessary to write back the block to disk, even though the buffer space that it occupies is not needed.
This write is called the forced output of a block.
We shall see the reason for forced output in Chapter 16; briefly, main-memory contents and thus buffer contents are lost in a crash, whereas data on disk usually survive a crash.
The goal of a replacement strategy for blocks in the buffer is to minimize accesses to the disk.
For general-purpose programs, it is not possible to predict accurately which blockswill be referenced.
Therefore, operating systems use the past pattern of block references as a predictor of future references.
The assumption generally made is that blocks that have been referenced recently are likely to be referenced again.
Therefore, if a block must be replaced, the least recently referenced block is replaced.
This approach is called the least recently used (LRU) block-replacement scheme.
However, a database system is able to predict the pattern of future referencesmore accurately than an operating system.
A user request to the database system involves several steps.
The database system is often able to determine in advance which blocks will be needed by looking at each of the steps required to perform the userrequested operation.
Thus, unlike operating systems, whichmust rely on the past to predict the future, database systems may have information regarding at least the short-term future.
To illustrate how information about future block access allows us to improve the LRU strategy, consider the processing of the SQL query:
Assume that the strategy chosen to process this request is given by the pseudocode program shown in Figure 10.17
Assume that the two relations of this example are stored in separate files.
In this example, we can see that, once a tuple of instructor has been processed, that tuple is not needed again.
Therefore, once processing of an entire block of instructor tuples is completed, that block is no longer needed in main memory, even though it has been used recently.
The buffer manager should be instructed to free the space occupied by an instructor block as soon as the final tuple has been processed.
Now consider blocks containing department tuples.We need to examine every block of department tuples once for each tuple of the instructor relation.
Thus, the most recently used department block will be the final block to be re-referenced, and the least recently used department block is the block that will be referenced next.
This assumption set is the exact opposite of the one that forms the basis for the LRU strategy.
Indeed, the optimal strategy for block replacement for the above procedure is themost recently used (MRU) strategy.
If a department block must be removed from the buffer, the MRU strategy chooses the most recently used block (blocks are not eligible for replacement while they are being used)
For the MRU strategy to work correctly for our example, the system must pin the department block currently being processed.
After the final department tuple has been processed, the block is unpinned, and it becomes the most recently used block.
In addition to using knowledge that the system may have about the request being processed, the buffer manager can use statistical information about the probability that a requestwill reference a particular relation.
For example, the data dictionary that (as we will see in detail in Section 10.7) keeps track of the logical schema of the relations as well as their physical storage information is one of the most frequently accessed parts of the database.
Thus, the buffer manager should try not to remove data-dictionary blocks frommain memory, unless other factors dictate that it do so.
Since an index for a file may be accessed more frequently than the file itself, the buffer manager should, in general, not remove index blocks from main memory if alternatives are available.
The ideal database block-replacement strategy needs knowledge of the database operations—both those being performed and those that will be performed in the future.
No single strategy is known that handles all the possible scenarios well.
Indeed, a surprisingly large number of database systems use LRU, despite that strategy’s faults.
The strategy that the buffer manager uses for block replacement is influenced by factors other than the time at which the block will be referenced again.
If the system is processing requests by several users concurrently, the concurrencycontrol subsystem (Chapter 15) may need to delay certain requests, to ensure preservation of database consistency.
Specifically, blocks needed by active (nondelayed) requests can be retained in the buffer at the expense of blocks needed by the delayed requests.
The crash-recovery subsystem (Chapter 16) imposes stringent constraints on block replacement.
If a block has been modified, the buffer manager is not allowed to write back the new version of the block in the buffer to disk, since that would destroy the old version.
Instead, the block manager must seek permission from the crash-recovery subsystem beforewriting out a block.
The crash-recovery subsystemmay demand that certain other blocks be force-output before it grants permission to the buffer manager to output the block requested.
Several types of data storage exist in most computer systems.
They are classified by the speed with which they can access data, by their cost per unit of data to buy the memory, and by their reliability.
Among the media available are cache, main memory, flash memory, magnetic disks, optical disks, and magnetic tapes.
Two factors determine the reliability of storage media: whether a power failure or system crash causes data to be lost, and what the likelihood is of physical failure of the storage device.
We can reduce the likelihood of physical failure by retaining multiple copies of data.
Or we can use more sophisticated methods based on redundant arrays of independent disks (RAID)
By striping data acrossdisks, thesemethodsoffer high throughput rates on large accesses; by introducing redundancy across disks, they improve reliability greatly.
Several different RAID organizations are possible, each with different cost, performance, and reliability characteristics.
We can organize a file logically as a sequence of records mapped onto disk blocks.
One approach to mapping the database to files is to use several files, and to store records of only one fixed length in any given file.
An alternative is to structure files so that they can accommodate multiple lengths for records.
The slotted-page method is widely used to handle varying length records within a disk block.
Since data are transferred between disk storage and main memory in units of a block, it is worthwhile to assign file records to blocks in such a way that a single block contains related records.
If we can access several of the records wewantwith only one block access,we savedisk accesses.
Since disk accesses are usually the bottleneck in the performance of a database system, careful assignment of records to blocks can pay significant performance dividends.
The data dictionary, also referred to as the system catalog, keeps track of metadata, that is data about data, such as relation names, attribute names and types, storage information, integrity constraints, and user information.
One way to reduce the number of disk accesses is to keep as many blocks as possible in main memory.
Since it is not possible to keep all blocks in main memory, we need to manage the allocation of the space available in main memory for the storage of blocks.
The buffer is that part of main memory available for storage of copies of disk blocks.
The subsystem responsible for the allocation of buffer space is called the buffer manager.
How is the flash translation table, which is used to map logical page numbers to physical page numbers, created in memory?
How big would the flash translation table be, assuming each page has a 32 bit address, and the table is stored as an array.
Suggest how to reduce the size of the translation table if very often long ranges of consecutive logical page numbers are mapped to consecutive physical page numbers.
An atomic block write is one where either the disk block is fully written or nothing is written (i.e., there are no partial writes)
Suggest schemes for getting the effect of atomic block writes with the following RAID schemes.
Compare the relative merits of the following techniques for implementing the deletion:
Give an example instance of these two relations, with three sections, each of which has five students.
Give a file structure of these relations that uses multitable clustering.
For each block in the file, two bits are maintained in the bitmap.
Such bitmaps can be kept in memory even for quite large files.
Describe how to keep the bitmap up to date on record insertions and deletions.
Outline the benefit of the bitmap technique over free lists in searching for free space and in updating free space information.
Given that database buffer sizes are very large, what (in-memory) data structure would you use for the above task?
Give the speed with which data can be accessed on each medium.
Thus, the data in the failed disk must be rebuilt and written to the replacement diskwhile the system is in operation.Which of the RAID levels yields the least amount of interference between the rebuild and ongoing disk accesses? Explain your answer.
For variable length fields, if the value is null, what would be stored in the offset and length fields?
In some applications, tuples have a very large number of attributes, most of which are null.
Can you modify the record representation such that the only overhead for a null attribute is the single bit in the null bitmap.
Discuss how the control on replacement that it provides would be useful for the implementation of database systems.
Store multiple relations (perhaps even the entire database) in one file.
Current generation disks have more sectors per track on outer tracks, and fewer sectors per track on inner tracks (since they are shorter in length)
What is the effect of such a change on each of the three main indicators of disk speed?
Consider a buffer manager that, instead of LRU, uses the rate of reference to objects, that is, how often an object has been accessed in the last n seconds.
Suppose we want to store in the buffer objects of varying sizes, and varying read costs (such asWeb pages, whose read cost depends on the site from which they are fetched)
Suggest how a buffer manager may choose which block to evict from the buffer.
Rosch [2003] presents an excellent overview of computer hardware, including extensive coverage of all types of storage technology such as magnetic disks, optical disks, tapes, and storage interfaces.
Patterson [2004] provides a good discussion on how latency improvements have lagged behind bandwidth (transfer rate) improvements.
With the rapid increase in CPU speeds, cache memory located along with the CPU has become much faster than main memory.
Although database systems do not control what data is kept in cache, there is an increasing motivation to organize data inmemory andwrite programs in such a way that cache utilization is maximized.
Work in this area includes Rao and Ross [2000], Ailamaki et al.
Discussions of redundant arrays of inexpensive disks (RAID) are presented by Patterson et al.
The storage structure of specific database systems, such as IBM DB2, Oracle, Microsoft SQL Server, and PostgreSQL are documented in their respective system manuals.
Buffer management is discussed in most operating-system texts, including in Silberschatz et al.
Chou and Dewitt [1985] presents algorithms for buffer management in database systems, and describes a performance evaluation.
Many queries reference only a small proportion of the records in a file.
For example, a query like “Find all instructors in the Physics department” or “Find the total number of credits earned by the student with ID 22201” references only a fraction of the student records.
It is inefficient for the system to read every tuple in the instructor relation to check if the dept name value is “Physics”
Likewise, it is inefficient to read the entire student relation just to find the one tuple for the ID “32556,”
Ideally, the system should be able to locate these records directly.
To allow these forms of access, we design additional structures that we associate with files.
An index for a file in a database systemworks in much the same way as the index in this textbook.
If we want to learn about a particular topic (specified by a word or a phrase) in this textbook, we can search for the topic in the index at the back of the book, find the pages where it occurs, and then read the pages to find the information for which we are looking.
The words in the index are in sorted order, making it easy to find the word we want.
Moreover, the index is much smaller than the book, further reducing the effort needed.
Database-system indices play the same role as book indices in libraries.
For example, to retrieve a student record given an ID, the database systemwould look up an index to find on which disk block the corresponding record resides, and then fetch the disk block, to get the appropriate student record.
Keeping a sorted list of students’ ID would not work well on very large databases with thousands of students, since the index would itself be very big; further, even though keeping the index sorted reduces the search time, finding a student can still be rather time-consuming.
We shall discuss several of these techniques in this chapter.
Based on a uniform distribution of values across a range of buckets.
The bucket to which a value is assigned is determined by a function, called a hash function.
We shall consider several techniques for both ordered indexing and hashing.
Rather, each technique is best suited to particular database applications.
Each technique must be evaluated on the basis of these factors:
Access types: The types of access that are supported efficiently.
Access types can include finding records with a specified attribute value and finding records whose attribute values fall in a specified range.
Access time: The time it takes to find a particular data item, or set of items, using the technique in question.
Insertion time: The time it takes to insert a newdata item.
This value includes the time it takes to find the correct place to insert the new data item, as well as the time it takes to update the index structure.
Deletion time: The time it takes to delete a data item.
This value includes the time it takes to find the item to be deleted, as well as the time it takes to update the index structure.
Space overhead: The additional space occupied by an index structure.
Provided that the amount of additional space is moderate, it is usually worthwhile to sacrifice the space to achieve improved performance.
We often want to have more than one index for a file.
For example, we may wish to search for a book by author, by subject, or by title.
An attribute or set of attributes used to look up records in a file is called a search key.
Note that this definition of key differs from that used in primary key, candidate key, and superkey.
This duplicate meaning for key is (unfortunately) well established in practice.
Using our notion of a search key, we see that if there are several indices on a file, there are several search keys.
To gain fast random access to records in a file, we can use an index structure.
Each index structure is associated with a particular search key.
Just like the index of a book or a library catalog, an ordered index stores the values of the search keys in sorted order, and associates with each search key the records that contain it.
The records in the indexedfilemay themselves be stored in some sorted order, just as books in a library are stored according to some attribute such as the Dewey decimal number.
A file may have several indices, on different search keys.
If the file containing the records is sequentially ordered, a clustering index is an index whose search key also defines the sequential order of the file.
The search key of a clustering index is often the primary key, although that is not necessarily so.
Indices whose search key specifies an order different from the sequential order of the file are called nonclustering indices, or secondary indices.
The terms “clustered” and “nonclustered” are often used in place of “clustering” and “nonclustering.”
Such files, with a clustering index on the search key, are called index-sequential files.
They represent one of the oldest index schemes used in database systems.
They are designed for applications that require both sequential processing of the entire file and random access to individual records.
Figure 11.1 shows a sequential file of instructor records taken from our university example.
In the example of Figure 11.1, the records are stored in sorted order of instructor ID, which is used as the search key.
An index entry, or index record, consists of a search-key value and pointers to one or more records with that value as their search-key value.
The pointer to a record consists of the identifier of a disk block and an offset within the disk block to identify the record within the block.
There are two types of ordered indices that we can use:
Dense index: In a dense index, an index entry appears for every search-key value in the file.
In a dense clustering index, the index record contains the search-key value and a pointer to the first data record with that search-key value.
The rest of the recordswith the same search-key valuewould be stored sequentially after the first record, since, because the index is a clustering one, records are sorted on the same search key.
In a dense nonclustering index, the index must store a list of pointers to all records with the same search-key value.
Sparse index: In a sparse index, an index entry appears for only some of the search-key values.
Sparse indices can be used only if the relation is stored in sorted order of the search key, that is, if the index is a clustering index.
As is true in dense indices, each index entry contains a search-key value and a pointer to the first data record with that search-key value.
To locate a record, we find the index entry with the largest search-key value that is less than or equal to the search-key value for which we are looking.
We start at the record pointed to by that index entry, and follow the pointers in the file until we find the desired record.
Suppose that we are looking up the record of instructor with ID “22222”
Using the dense index of Figure 11.2, we follow the pointer directly to the desired record.
Since ID is a primary key, there exists only one such record and the search is complete.
We then read the instructor file in sequential order until we find the desired record.
The header of each page lists the first word alphabetically on that page.
The words at the top of each page of the book index together form a sparse index on the contents of the dictionary pages.
As another example, suppose that the search-key value is not not a primary key.
Figure 11.4 shows a dense clustering index for the instructor file with the search key being dept name.
Observe that in this case the instructor file is sorted on the search key dept name, instead of ID, otherwise the index on dept name would be a nonclustering index.
Suppose that we are looking up records for the History department.
Using the dense index of Figure 11.4, we follow the pointer directly to the first History record.
We process this record, and follow the pointer in that record to locate the next record in search-key (dept name) order.
We continue processing records until we encounter a record for a department other than History.
As we have seen, it is generally faster to locate a record if we have a dense index rather than a sparse index.
However, sparse indices have advantages over dense indices in that they require less space and they impose less maintenance overhead for insertions and deletions.
There is a trade-off that the system designer must make between access time and space overhead.
Although the decision regarding this trade-off depends on the specific application, a good compromise is to have a sparse index with one index entry per block.
The reason this design is a good trade-off is that the dominant cost in processing a database request is the time that it takes to bring a block from disk into main memory.
Once we have brought in the block, the time to scan the entire block is negligible.
Using this sparse index, we locate the block containing the record that we are seeking.
Thus, unless the record is on an overflow block (see Section 10.6.1), we minimize block accesses while keeping the size of the index (and thus our space overhead) as small as possible.
For the preceding technique to be fully general, we must consider the case where records for one search-key value occupy several blocks.
It is easy tomodify our scheme to handle this situation.
Suppose we build a dense index on a relation with 1,000,000 tuples.
Such large indices are stored as sequential files on disk.
If an index is small enough to be kept entirely in main memory, the search time to find an entry is low.
However, if the index is so large that not all of it can be kept in memory, index blocks must be fetched from disk when required.
Even if an index is smaller than the main memory of a computer, main memory is also required for a number of other tasks, so it may not be possible to keep the entire index in memory.
The search for an entry in the index then requires several disk-block reads.
To deal with this problem, we treat the index just as we would treat any other sequential file, and construct a sparse outer index on the original index, which we now call the inner index, as shown in Figure 11.5
Note that the index entries are always in sorted order, allowing the outer index to be sparse.
To locate a record, we first use binary search on the outer index to find the record for the largest search-key value less than or equal to the one that we desire.
The pointer points to a block of the inner index.
We scan this block until we find the record that has the largest search-key value less than or equal to the one that we desire.
The pointer in this record points to the block of the file that contains the record for which we are looking.
If we assume that the outer index is already in main memory, we would read only one index block for a search using a multilevel index, rather than the 14 blocks we read with binary search.
As a result, we can perform 14 times as many index searches per second.
If our file is extremely large, even the outer index may grow too large to fit in main memory.
With a 100,000,000 tuple relation, the inner index would occupy.
Since there are many demands onmainmemory, it may not be possible to reserve that much main memory just for this particular outer index.
In such a case, we can create yet another level of index.
Indeed, we can repeat this process as many times as necessary.
Searching for records with a multilevel index requires significantly fewer I/O operations than does searching for records by binary search.1
Multilevel indices are closely related to tree structures, such as the binary trees used for in-memory indexing.
Regardless of what form of index is used, every indexmust be updatedwhenever a record is either inserted into or deleted from the file.
Further, in case a record in the file is updated, any indexwhose search-key attribute is affected by the update must also be updated; for example, if the department of an instructor is changed, an index on the dept name attribute of instructormust be updated correspondingly.
Such a record update can be modeled as a deletion of the old record, followed by an insertion of the new value of the record, which results in an index deletion followed by an index insertion.
As a result we only need to consider insertion and deletion on an index, and do not need to consider updates explicitly.
First, the system performs a lookup using the search-key value that appears in the record to be inserted.
The actions the system takes next depend on whether the index is dense or sparse:
If the index entry stores pointers to all recordswith the same searchkey value, the system adds a pointer to the new record in the index entry.
Otherwise, the index entry stores a pointer to only the first record with the search-key value.
The system then places the record being inserted after the other records with the same search-key values.
Sparse indices: We assume that the index stores an entry for each block.
In the early days of disk-based indices, each level of the index corresponded to a unit of physical storage.
Thus, we may have indices at the track, cylinder, and disk levels.
Such a hierarchy does not make sense today since disk subsystems hide the physical details of disk storage, and the number of disks and platters per disk is very small compared to the number of cylinders or bytes per track.
On the other hand, if the new record has the least search-key value in its block, the system updates the index entry pointing to the block; if not, the system makes no change to the index.
To delete a record, the systemfirst looks up the record to be deleted.
The actions the system takes next depend on whether the index is dense or sparse:
If the index entry stores pointers to all recordswith the same searchkey value, the system deletes the pointer to the deleted record from the index entry.
Otherwise, the index entry stores a pointer to only the first record with the search-key value.
In this case, if the deleted record was the first recordwith the search-key value, the system updates the index entry to point to the next record.
If the deleted record was the only record with its search key, the system replaces the corresponding index record with an index record for the next search-key value (in search-key order)
If the next search-key value already has an index entry, the entry is deleted instead of being replaced.
Otherwise, if the index entry for the search-key value points to the record being deleted, the system updates the index entry to point to the next record with the same search-key value.
Insertion anddeletion algorithms formultilevel indices are a simple extension of the scheme just described.
On deletion or insertion, the system updates the lowest-level index as described.
As far as the second level is concerned, the lowest-level index is merely a file containing records—thus, if there is any change in the lowest-level index, the system updates the second-level index as described.
The same technique applies to further levels of the index, if there are any.
Secondary indices must be dense, with an index entry for every search-key value, and a pointer to every record in the file.
If a secondary index stores only some of the search-key values, records with intermediate search-key values may be anywhere in the file and, in general, we cannot find them without searching the entire file.
A secondary index on a candidate key looks just like a dense clustering index, except that the records pointed to by successive values in the index are not stored sequentially.
In general, however, secondary indices may have a different structure from clustering indices.
If the search key of a clustering index is not a candidate key, it suffices if the index points to the first record with a particular value for the search key, since the other records can be fetched by a sequential scan of the file.
In contrast, if the search key of a secondary index is not a candidate key, it is not enough to point to just the first record with each search-key value.
The remaining records with the same search-key value could be anywhere in the file, since the records are ordered by the search key of the clustering index, rather than by the search key of the secondary index.
Therefore, a secondary indexmust contain pointers to all the records.
We can use an extra level of indirection to implement secondary indices on search keys that are not candidate keys.
The pointers in such a secondary index do not point directly to the file.
Instead, each points to a bucket that contains pointers to the file.
Figure 11.6 shows the structure of a secondary index that uses an extra level of indirection on the instructor file, on the search key salary.
A sequential scan in clustering index order is efficient because records in the file are stored physically in the same order as the index order.
However, we cannot (except in rare special cases) store a file physically ordered by both the search key of the clustering index and the search key of a secondary index.
Figure 11.6 Secondary index on instructor file, on noncandidate key salary.
If a relation is declared to have a primary key, most database implementations automatically create an index on the primary key.
Whenever a tuple is inserted into the relation, the index can be used to check that the primary key constraint is not violated (that is, there are no duplicates on the primary key value)
Without the index on the primary key, whenever a tuple is inserted, the entire relation would have to be read to ensure that the primary-key constraint is satisfied.
Because secondary-key order and physical-key order differ, if we attempt to scan the file sequentially in secondary-key order, the reading of each record is likely to require the reading of a new block from disk, which is very slow.
The procedure described earlier for deletion and insertion can also be applied to secondary indices; the actions taken are those described for dense indices storing a pointer to every record in the file.
If a file hasmultiple indices, whenever the file is modified, every index must be updated.
Secondary indices improve the performance of queries that use keys other than the search key of the clustering index.
However, they impose a significant overhead on modification of the database.
The designer of a database decides which secondary indices are desirable on the basis of an estimate of the relative frequency of queries and modifications.
Although the examples we have seen so far have had a single attribute in a search key, in general a search key can havemore than one attribute.
A search key containing more than one attribute is referred to as a composite search key.
The structure of the index is the same as that of any other index, the only difference being that the search key is not a single attribute, but rather is a list of attributes.
Lexicographic ordering is basically the same as alphabetic ordering of words.
As an example, consider an index on the takes relation, on the composite search key (course id, semester, year)
Such an index would be useful to find all students who have registered for a particular course in a particular semester/year.
An ordered index on a composite key can also be used to answer several other kinds of queries efficiently, as we shall see later in Section 11.5.2
The main disadvantage of the index-sequential file organization is that performance degrades as the file grows, both for index lookups and for sequential scans.
Although this degradation can be remedied by reorganization of the file, frequent reorganizations are undesirable.
We shall see that the B+-tree structure imposes performance overhead on insertion and deletion, and adds space overhead.
The overhead is acceptable even for frequently modified files, since the cost of file reorganization is avoided.
Furthermore, since nodes may be as much as half empty (if they have the minimum number of children), there is some wasted space.
This space overhead, too, is acceptable given the performance benefits of the B+-tree structure.
The ranges of values in each leaf do not overlap, except if there are duplicate search-key values, in which case a value may be present in more than one leaf.
Specifically, if Li and L j are leaf nodes and i < j , then every search-key value in Li is less than or equal to every search-key value in L j.
If the B+-tree index is used as a dense index (as is usually the case) every search-key value must appear in some leaf node.
Now we can explain the use of the pointer Pn.
Since there is a linear order on the leaves based on the search-key values that they contain, we use Pn to chain.
This ordering allows for efficient sequential processing of the file.
We have shown instructor names abbreviated to 3 characters in order to depict the tree clearly; in reality, the tree nodes would contain the full names.
We have also omitted null pointers for simplicity; any pointer field in the figure that does not have an arrow is understood to have a null value.
As before, we have abbreviated instructor names only for clarity of presentation.
That is, the length of every path from the root to a leaf node is the same.
Indeed, the “B” in B+-tree stands for “balanced.” It is the balance property of B+-trees that ensures good performance for lookup, insertion, and deletion.
Let us consider how we process queries on a B+-tree.
Suppose that we wish to find records with a search-key value of V.
Figure 11.11 presents pseudocode for a function find() to carry out this task.
Intuitively, the function starts at the root of the tree, and traverses the tree down until it reaches a leaf node that would contain the specified value if it exists in the tree.
Specifically, starting with the root as the current node, the function repeats the following steps until a leaf node is reached.
First, the current node is examined, looking for the smallest i such that search-key value Ki is greater.
Set C = root node while (C is not a leaf node) begin.
Let Pm = last non-null pointer in the node Set C = C.Pm.
At the leaf node, if there is a search-key value equal toV, let Ki be the first such value; pointer Pi directs us to a record with search-key value Ki.
If no search-key with value V is found in the leaf node, no record with key value V exists in the relation, and function find returns null, to indicate failure.
If there is at most one record with a search key value V (for example, if the index is on a primary key) the procedure that calls the find function simply uses the pointer L .Pi to retrieve the record and is done.
However, in case there may be more than one matching record, the remaining records also need to be fetched.
Procedure printAll shown in Figure 11.11 shows how to fetch all records with a specified search key V.
The procedure first steps through the remaining keys in the node L, to find other records with search-key value V.
If node L contains at least one search-key value greater than V, then there are no more records matching V.
Otherwise, the next leaf, pointed to by Pn may contain further entries for V.
The node pointed to by Pn must then be searched to find further records with search-key value V.
If the highest search-key value in the node pointed to by Pn is also V, further leaves may have to be traversed, in order to find all matching records.
The repeat loop in printAll carries out the task of traversing leaf nodes until all matching records have been found.
Such an iterator interface would provide a method next(), which can be called repeatedly to fetch successive records with the specified search-key.
The next() method would step through the entries at the leaf level, in a manner similar to printAll, but each call takes only one step, and records where it left off, so that successive calls next step through successive records.
We omit details for simplicity, and leave the pseudocode for the iterator interface as an exercise for the interested reader.
B+-trees can also be used to find all records with search key values in a specified range (L ,U)
When a record is inserted into, or deleted from a relation, indices on the relation must be updated correspondingly.
Recall that updates to a record can bemodeled as a deletion of the old record followed by insertion of the updated record.
Hence we only consider the case of insertion and deletion.
Using the same technique as for lookup from the find() function (Figure 11.11), we first find the leaf node inwhich the search-key valuewould appear.We then insert an entry (that is, a search-key value and record pointer pair) in the leaf node, positioning it such that the search keys are still in order.
Using the same technique as for lookup, we find the leaf node containing the entry to be deleted, by performing a lookup on the search-key value of the deleted record; if there are multiple entries with the same searchkey value, we search across all entries with the same search-key value until we find the entry that points to the record being deleted.We then remove the entry from the leaf node.
All entries in the leaf node that are to the right of the deleted entry are shifted left by one position, so that there are no gaps in the entries after the entry is deleted.
We now consider the general case of insertion and deletion, dealingwith node splitting and node coalescing.
Figure 11.12 Split of leaf node on insertion of “Adams”
Having split a leaf node, we must insert the new leaf node into the B+-tree structure.
In our example, the new node has “Califieri” as its smallest search-key value.
We need to insert an entry with this search-key value, and a pointer to the new node, into the parent of the leaf node that was split.
The B+-tree of Figure 11.13 shows the result of the insertion.
It was possible to perform this insertion with no further node split, because there was room in the parent node for the new entry.
If there were no room, the parent would have had to be split, requiring an entry to be added to its parent.
In the worst case, all nodes along the path to the root must be split.
If the root itself is split, the entire tree becomes deeper.
Splitting of a nonleaf node is a little different from splitting of a leaf node.
The leaf node in which “Lamport” is to be inserted already has entries “Gold”, “Katz”, and “Kim”, and as a result the leaf node has to be split.
The new right-hand-side node resulting from the split contains the search-key values “Kim” and “Lamport”
To do so, the parent node is conceptually expanded temporarily, the entry added, and the overfull node is then immediately split.
When an overfull nonleaf node is split, the child pointers are divided among the original and the newly created nodes; in our example, the original node is left with the first three pointers, and the newly created node to the right gets the remaining two pointers.
The search key values are, however, handled a little differently.
The search key values that lie between the pointers moved to the right node (in our example, the value “Kim”) are moved along with the pointers, while those that lie between the pointers that stay on the left (in our example, “Califieri” and “Einstein”) remain undisturbed.
However, the search key value that lies between the pointers that stay on the left, and the pointers that move to the right node is treated differently.
In our example, the search key value “Gold” lies between the three pointers that went to the left node, and the two pointers that went to the right node.
The value “Gold” is not added to either of the split nodes.
In this case, the parent node is the root, and it has enough space for the new entry.
The general technique for insertion into a B+-tree is to determine the leaf node l into which insertion must occur.
If a split results, insert the new node into the parent of node l.
If this insertion causes a split, proceed recursively up the tree until either an insertion does not cause a split or a new root is created.
The procedure insert inserts a key-value pointer pair into the index, using two subsidiary procedures insert in leaf and insert in parent.
In the pseudocode, L , N, P and T denote pointers to nodes, with L being used to denote a leaf node.
The pseudocode alsomakes use of the function parent(N) to find the parent of a node N.
We can compute a list of nodes in the path from the root to the leaf while initially finding the leaf node, and can use it later to find the parent of any node in the path efficiently.
The procedures insert into index and insert in parent use a temporary area of memory T to store the contents of a node being split.
The procedures can be modified to copy data from the node being split directly to the newly created node, reducing the time required for copying data.
However, the use of the temporary space T simplifies the procedures.
In this case, we look at a sibling node; in our example, the only sibling is the nonleaf node containing the search keys “Califieri”, “Einstein”, and “Gold”
If possible, we try to coalesce the node with its sibling.
In this case, coalescing is not possible, since the node and its sibling together have five pointers, against a maximumof four.
The solution in this case is to redistribute the pointers between.
As a result, as can be seen in the B+-tree in Figure 11.16, after redistribution of pointers between siblings, the value “Gold” has moved up into the parent, while the value that was there earlier, “Mozart”, has moved down into the right sibling.
We next delete the search-key values “Singh” and “Wu” from the B+-tree of Figure 11.16
The deletion of the first of these values does not make the leaf node underfull, but the deletion of the second value does.
It is not possible to merge the underfull node with its sibling, so a redistribution of values is carried out, moving the search-key value “Kim” into the node containing “Mozart”, resulting in the tree shown in Figure 11.17
The value separating the two siblings has been updated in the parent, from “Mozart” to “Kim”
Nowwe delete “Gold” from the above tree; the result is shown in Figure 11.18
This results in an underfull leaf, which can now be merged with its sibling.
The resultant deletion of an entry from the parent node (the nonleaf node containing “Kim”) makes the parent underfull (it is left with just one pointer)
This time around, the parent node can be merged with its sibling.
This merge results in the search-key value “Gold” moving down from the parent into the merged node.
As a result of this merge, an entry is deleted from its parent, which happens to be the root of the tree.
And as a result of that deletion, the root is left with only one child pointer and no search-key value, violating the condition that the root.
It is worth noting that, as a result of deletion, a key value that is present in a nonleaf node of the B+-treemay not be present at any leaf of the tree.
For example, in Figure 11.18, the value “Gold” has been deleted from the leaf level, but is still present in a nonleaf node.
In general, to delete a value in a B+-tree, we perform a lookup on the value and delete it.
If the node is too small, we delete it from its parent.
This deletion results in recursive application of the deletion algorithm until the root is reached, a parent remains adequately full after deletion, or redistribution is applied.
If a relation can have more than one record containing the same search key value (that is, two or more records can have the same values for the indexed attributes), the search key is said to be a nonunique search key.
One problem with nonunique search keys is in the efficiency of record deletion.
Suppose a particular search-key value occurs a large number of times, and one of the records with that search key is to be deleted.
The deletion may have to search through a number of entries, potentially across multiple leaf nodes, to find the entry corresponding to the particular record being deleted.
A simple solution to this problem, used by most database systems, is to make search keys unique by creating a composite search key containing the original search key and another attribute, which together are unique across all records.
When a record is to be deleted, the composite search-key value is computed from the record, and then used to look up the index.
A search with the original search-key attribute simply ignores the value of the uniquifier attribute when comparing search-key values.
With nonunique search keys, our B+-tree structure stores each key value as many times as there are records containing that value.
An alternative is to store each key value only once in the tree, and to keep a bucket (or list) of record pointerswith a search-key value, to handle nonunique search keys.
This approach is more space efficient since it stores the key value only once; however, it creates several complications when B+-trees are implemented.
If the buckets are kept in the leaf node, extra code is needed to deal with variable-size buckets, and to deal with buckets that grow larger than the size of the leaf node.
If the buckets are stored in separate blocks, an extra I/O operation may be required to fetch records.
In addition to these problems, the bucket approach also has the problem of inefficiency for record deletion if a search-key value occurs a large number of times.
In other words, the cost of insertion and deletion operations in terms of I/O operations is proportional to the height of the B+-tree, and is therefore low.
It is the speed of operation on B+-trees that makes them a frequently used index structure in database implementations.
In practice, operations on B+-trees result in fewer I/O operations than the worst-case bounds.
Conversely, with the same fanout, the total number of nonleaf nodes in a B+-tree would be just a little more than 1/100th of the number of leaf nodes.
As a result, with memory sizes of several gigabytes being common today, for B+-trees that are used frequently, even if the relation is very large it is quite likely that most of the nonleaf nodes are already in the database buffer when they are accessed.
Thus, typically only one or two I/O operations are required to perform a lookup.
As a result, on an average an insert will require just a little more than one I/O operation to write updated blocks.
Although B+-trees only guarantee that nodeswill be at least half full, if entries are inserted in random order, nodes can be expected to be more than two-thirds full on average.
If entries are inserted in sorted order, on the other hand, nodes will be only half full.
We leave it as an exercise to the reader to figure out why nodes would be only half full in the latter case.
In this section, we discuss several extensions and variations of the B+-tree index structure.
As mentioned in Section 11.3, the main drawback of index-sequential file organization is the degradation of performance as the file grows: With growth, an increasing percentage of index entries and actual records become out of order, and are stored in overflow blocks.
We solve the degradation of index lookups by using B+-tree indices on the file.
We solve the degradation problem for storing the actual records by using the leaf level of the B+-tree to organize the blocks containing the actual records.
We use the B+-tree structure not only as an index, but also as an organizer for records in a file.
In aB+-tree file organization, the leaf nodes of the tree store records, instead of storing pointers to records.
Figure 11.20 shows an example of a B+-tree file organization.
Since records are usually larger than pointers, the maximum number of records that can be stored in a leaf node is less than the number of pointers in a nonleaf node.
However, the leaf nodes are still required to be at least half full.
When we use a B+-tree for file organization, space utilization is particularly important, since the space occupied by the records is likely to be much more than the space occupied by keys and pointers.
We can improve the utilization of space in a B+-tree by involving more sibling nodes in redistribution during splits and merges.
The technique is applicable to both leaf nodes and nonleaf nodes, and works as follows:
Note that in a B+-tree index or file organization, leaf nodes that are adjacent to each other in the tree may be located at different places on disk.
When a file organization is newly created on a set of records, it is possible to allocate blocks that are mostly contiguous on disk to leaf nodes that are contiguous in the tree.
Thus a sequential scan of leaf nodeswould correspond to amostly sequential scan on disk.
As insertions and deletions occur on the tree, sequentiality is increasingly.
B+-tree file organizations can also be used to store large objects, such as SQL clobs and blobs, which may be larger than a disk block, and as large as multiple gigabytes.
Such large objects can be stored by splitting them into sequences of smaller records that are organized in a B+-tree file organization.
The records can be sequentially numbered, or numbered by the byte offset of the record within the large object, and the record number can be used as the search key.
Some file organizations, such as the B+-tree file organization, may change the location of records evenwhen the records have not been updated.
As an example, when a leaf node is split in a B+-tree file organization, a number of records are moved to a new node.
In such cases, all secondary indices that store pointers to the relocated records would have to be updated, even though the values in the records may not have changed.
Each leaf node may contain a fairly large number of records, and each of them may be in different locations on each secondary index.
Thus a leaf-node split may require tens or even hundreds of I/O operations to update all affected secondary indices, making it a very expensive operation.
A widely used solution for this problem is as follows: In secondary indices, in place of pointers to the indexed records, we store the values of the primaryindex search-key attributes.
For example, supposewe have aprimary index on the attribute ID of relation instructor; then a secondary index on dept namewould store with each department name a list of instructor’s ID values of the corresponding records, instead of storing pointers to the records.
Relocation of records because of leaf-node splits then does not require any update on any such secondary index.
However, locating a record using the secondary index now requires two steps: First we use the secondary index to find the primary-index search-key values, and then we use the primary index to find the corresponding records.
The above approach thus greatly reduces the cost of index update due to file reorganization, although it increases the cost of accessing data using a secondary index.
The first problem is that strings can be of variable length.
The second problem is that strings can be long, leading to a low fanout and a correspondingly increased tree height.
With variable-length search keys, different nodes can have different fanouts even if they are full.
A node must then be split if it is full, that is, there is no space to add a new entry, regardless of howmany search entries it has.
Similarly, nodes can be merged or entries redistributed depending on what fraction of the space in the nodes is used, instead of being based on the maximum number of entries that the node can hold.
The fanout of nodes can be increased by using a technique called prefix compression.
With prefix compression, we do not store the entire search key value at nonleaf nodes.
We only store a prefix of each search key value that is sufficient to distinguish between the key values in the subtrees that it separates.
For example, if we had an index on names, the key value at a nonleaf node could be a prefix of a name; it may suffice to store “Silb” at a nonleaf node, instead of the full “Silberschatz” if the closest values in the two subtrees that it separates are, say, “Silas” and “Silver” respectively.
As we saw earlier, insertion of a record in a B+-tree requires a number of I/O operations that in the worst case is proportional to the height of the tree, which is usually fairly small (typically five or less, even for large relations)
Now consider the case where a B+-tree is being built on a large relation.
Suppose the relation is significantly larger than main memory, and we are constructing a nonclustering index on the relation such that the index is also larger than main memory.
In this case, as we scan the relation and add entries to the B+-tree, it is quite likely that each leaf node accessed is not in the database buffer when it is accessed, since there is no particular ordering of the entries.
With such randomly ordered accesses to blocks, each time an entry is added to the leaf, a disk seek will be required to fetch the block containing the leaf node.
The block will probably be evicted from the disk buffer before another entry is added to the block, leading to another disk seek to write the block back to disk.
Thus a random read and a random write operation may be required for each entry inserted.
Insertion of a large number of entries at a time into an index is referred to as bulk loading of the index.
An efficient way to perform bulk loading of an index is as follows.
First, create a temporary file containing index entries for the relation, then sort the file on the search key of the index being constructed, and finally scan the sorted file and insert the entries into the index.
There are efficient algorithms for sorting large relations, which are described later in Section 12.4, which can sort even a large file with an I/O cost comparable to that of reading the file a few times, assuming a reasonable amount of main memory is available.
There is a significant benefit to sorting the entries before inserting them into the B+-tree.
When the entries are inserted in sorted order, all entries that go to a particular leaf node will appear consecutively, and the leaf needs to be written out only once; nodes will never have to be read from disk during bulk load, if the B+-tree was empty to start with.
Each leaf node will thus incur only one I/O operation even though many entries may be inserted into the node.
Even these I/O operations can be expected to be sequential, if successive leaf nodes are allocated on successive disk blocks, and few disk seeks would be required.
If the B+-tree is initially empty, it can be constructed faster by building it bottom-up, from the leaf level, instead of using the usual insert procedure.
In bottom-up B+-tree construction, after sorting the entries as we just described, we break up the sorted entries into blocks, keeping as many entries in a block as can fit in the block; the resulting blocks form the leaf level of the B+-tree.
The minimumvalue in each block, alongwith the pointer to the block, is used to create entries in the next level of the B+-tree, pointing to the leaf blocks.
Each further level of the tree is similarly constructed using the minimum values associated with each node one level below, until the root is created.
Most database systems implement efficient techniques based on sorting of entries, and bottom-up construction, when creating an index on a relation, although they use the normal insertion procedure when tuples are added one at a time to a relation with an existing index.
Some database systems recommend that if a very large number of tuples are added at once to an already existing relation, indices on the relation (other than any index on the primary key) should be dropped, and then re-created after the tuples are inserted, to take advantage of efficient bulk-loading techniques.
The primary distinction between the two approaches is that a B-tree eliminates the redundant storage of search-key values.
In the B+-tree of Figure 11.13, the search keys “Califieri”, “Einstein”, “Gold”, “Mozart”, and “Srinivasan” appear in nonleaf nodes, in addition to appearing in the leaf nodes.
Every search-key value appears in some leaf node; several are repeated in nonleaf nodes.
A B-tree allows search-key values to appear only once (if they are unique), unlike a B+-tree, where a value may appear in a nonleaf node, in addition to appearing in a leaf node.
Since search keys are not repeated in the B-tree, we may be able to store the index in fewer tree nodes than in the corresponding B+-tree index.
However, since search keys that appear in nonleaf nodes appear nowhere else in the B-tree, we are forced to include an additional.
These additional pointers point to either file records or buckets for the associated search key.
It is worth noting that many database system manuals, articles in industry literature, and industry professionals use the term B-tree to refer to the data structure that we call the B+-tree.
In fact, it would be fair to say that in current usage, the term B-tree is assumed to be synonymous with B+-tree.
However, in this book we use the terms B-tree and B+-tree as they were originally defined, to avoid confusion between the two data structures.
The number of nodes accessed in a lookup in a B-tree depends on where the search key is located.
A lookup on a B+-tree requires traversal of a path from the root of the tree to some leaf node.
In contrast, it is sometimes possible to find the desired value in a B-tree before reaching a leaf node.
However, roughly n times as many keys are stored in the leaf level of a B-tree as in the nonleaf levels, and, since n is typically large, the benefit of finding certain values early is relatively.
Moreover, the fact that fewer search keys appear in a nonleaf B-tree node, compared to B+-trees, implies that a B-tree has a smaller fanout and therefore may have depth greater than that of the corresponding B+-tree.
Thus, lookup in a B-tree is faster for some search keys but slower for others, although, in general, lookup time is still proportional to the logarithm of the number of search keys.
In a B+-tree, the deleted entry always appears in a leaf.
In a B-tree, the deleted entry may appear in a nonleaf node.
The proper value must be selected as a replacement from the subtree of the node containing the deleted entry.
Specifically, if search key Ki is deleted, the smallest search key appearing in the subtree of pointer Pi + 1 must be moved to the field formerly occupied by Ki.
Further actions need to be taken if the leaf node now has too few entries.
In contrast, insertion in a B-tree is only slightly more complicated than is insertion in a B+-tree.
The space advantages of B-trees are marginal for large indices, and usually do not outweigh the disadvantages that we have noted.
Thus, pretty much all database-system implementations use the B+-tree data structure, even if (as we discussed earlier) they refer to the data structure as a B-tree.
In our description of indexing so far, we have assumed that data are resident on magnetic disks.
Although this assumption continues to be true for the most part, flash memory capacities have grown significantly, and the cost of flash memory per gigabyte has dropped equally significantly, making flash memory storage a serious contender for replacing magnetic-disk storage for many applications.
A natural question is, how would this change affect the index structure.
Flash-memory storage is structured as blocks, and the B+-tree index structure can be used for flash-memory storage.
The benefit of themuch faster access speeds is clear for index lookups.
Instead of requiring an average of 10 milliseconds to seek to and read a block, a random block can be read in about amicrosecond from flash-memory.
The optimum B+-tree node size for flash-memory is typically smaller than that with disk.
The only real drawback with flash memory is that it does not permit inplace updates to data at the physical level, although it appears to do so logically.
Every update turns into a copy+write of an entire flash-memory block, requiring the old copy of the block to be erased subsequently; a block erase takes about 1 millisecond.
There is ongoing research aimed at developing index structures that can reduce the number of block erases.
Meanwhile, standard B+-tree indices can continue to be used even on flash-memory storage, with acceptable update performance, and significantly improved lookup performance compared to disk storage.
Until now, we have assumed implicitly that only one index on one attribute is used to process a query on a relation.
Assume that the instructorfile has two indices: one for dept name and one for salary.
Use the index on dept name to find all records pertaining to the Finance department.
Use the index on salary to find all records pertaining to instructors with salary of $80,000
Examine each such record to see whether the department name is “Finance”
Also, use the index on salary to find pointers to all records pertaining to instructors with a salary of $80,000
Those pointers that are in the intersection point to records pertaining to instructors of the Finance department and with salary of $80,000
The third strategy is the only one of the three that takes advantage of the existence of multiple indices.
However, even this strategy may be a poor choice if all of the following hold:
There are many records pertaining to instructors with a salary of $80,000
There are only a few records pertaining to both the Finance department and.
If these conditions hold, we must scan a large number of pointers to produce a small result.
An index structure called a “bitmap index” can in some cases greatly speed up the intersection operation used in the third strategy.
An alternative strategy for this case is to create and use an index on a composite search key (dept name, salary)—that is, the search key consisting of the department name concatenated with the instructor salary.
We can use an ordered (B+-tree) index on the above composite search key to answer efficiently queries of the form.
Queries such as the following query, which specifies an equality condition on the first attribute of the search key (dept name) and a range on the second attribute of the search key (salary), can also be handled efficiently since they correspond to a range query on the search attribute.
We can even use an ordered index on the search key (dept name, salary) to answer the following query on only one attribute efficiently:
The use of an ordered-index structure on a composite search key, however, has a few shortcomings.
However, each record is likely to be in a different disk block, because of the ordering of records in the file, leading to many I/O operations.
The difference between this query and the previous two queries is that the condition on the first attribute (dept name) is a comparison condition, rather than.
The condition does not correspond to a range query on the search key.
To speed the processing of general composite search-key queries (which can involve one or more comparison operations), we can use several special structures.
There is another structure, called the R-tree, that can be used for this purpose.
The R-tree is an extension of the B+-tree to handle indexing on multiple dimensions.
Covering indices are indices that store the values of some attributes (other than the search-key attributes) along with the pointers to the record.
Storing extra attribute values is useful with secondary indices, since they allow us to answer some queries using just the index, without even looking up the actual records.
For example, suppose that we have a nonclustering index on the ID attribute of the instructor relation.
If we store the value of the salary attribute along with the record pointer, we can answer queries that require the salary (but not the other attribute, dept name) without accessing the instructor record.
The same effect could be obtained by creating an index on the search key (ID, salary), but a covering index reduces the size of the search key, allowing a larger fanout in the nonleaf nodes, and potentially reducing the height of the index.
One disadvantage of sequential file organization is that we must access an index structure to locate data, or must use binary search, and that results in more I/O operations.
File organizations based on the technique of hashing allow us to avoid accessing an index structure.
Hashing also provides a way of constructing indices.We study file organizations and indices based on hashing in the following sections.
In our description of hashing, we shall use the term bucket to denote a unit of storage that can store one or more records.
A bucket is typically a disk block, but could be chosen to be smaller or larger than a disk block.
Formally, let K denote the set of all search-key values, and let B denote the set of all bucket addresses.
A hash function h is a function from K to B.
To insert a record with search key Ki , we compute h(Ki ), which gives the address of the bucket for that record.
Assume for now that there is space in the bucket to store the record.
To perform a lookup on a search-key value Ki , we simply compute h(Ki ), then search the bucket with that address.
Thus, we have to check the search-key value of every record in the bucket to verify that the record is one that we want.
If the search-key value of the record to be deleted is Ki , we compute h(Ki ), then search the corresponding bucket for that record, and delete the record from the bucket.
In a hash file organization, we obtain the address of the disk block containing a desired record directly by computing a function on the search-key value of the record.
In a hash index organization we organize the search keys, with their associated pointers, into a hash file structure.
The worst possible hash function maps all search-key values to the same bucket.
Such a function is undesirable because all the records have to be kept in the same bucket.
A lookup has to examine every such record to find the one desired.
An ideal hash function distributes the stored keys uniformly across all the buckets, so that every bucket has the same number of records.
Since we do not know at design time precisely which search-key values will be stored in the file, we want to choose a hash function that assigns search-key values to buckets in such a way that the distribution has these qualities:
That is, the hash function assigns each bucket the same number of search-key values from the set of all possible search-key values.
That is, in the average case, each bucket will have nearly the same number of values assigned to it, regardless of the actual distribution of search-key values.
More precisely, the hash value will not be correlated to any externally visible ordering on the search-key values, such as alphabetic ordering or ordering by the length of the search keys; the hash function will appear to be random.
As an illustration of these principles, let us choose a hash function for the instructor file using the search key dept name.
The hash function that we choose must have the desirable properties not only on the example instructor file that we have been using, but also on an instructor file of realistic size for a large university with many departments.
Assume that we decide to have 26 buckets, and we define a hash function that maps names beginning with the ith letter of the alphabet to the ith bucket.
This hash function has the virtue of simplicity, but it fails to provide a uniform distribution, since we expect more names to begin with such letters as B and R than Q and X, for example.
Now suppose that we want a hash function on the search key salary.
Figure 11.23 Hash organization of instructor file, with dept name as the key.
As a result, the distribution of records is not uniform—some buckets receive more records than others do.
If the function has a random distribution, even if there are such correlations in the search keys, the randomness of the distribution will make it very likely that all buckets will have roughly the same number of records, as long as each search key occurs in only a small fraction of the records.
If a single search key occurs in a large fraction of the records, the bucket containing it is likely to have more records than other buckets, regardless of the hash function used.
Typical hash functions perform computation on the internal binary machine representation of characters in the search key.
A simple hash function of this type first computes the sum of the binary representations of the characters of a key, then returns the sum modulo the number of buckets.
Figure 11.23 shows the application of such a scheme, with eight buckets, to the instructor file, under the assumption that the ith letter in the alphabet is represented by the integer i.
The following hash function is a better alternative for hashing strings.
Let s be a string of length n, and let s[i] denote the ith byte of the string.
The result of the above functionmodulo the number of buckets can then be used for indexing.
A bad hash function may result in lookup taking time proportional to the number of search keys in the file.
A welldesigned function gives an average-case lookup time that is a (small) constant, independent of the number of search keys in the file.
So far, we have assumed that, when a record is inserted, the bucket to which it is mapped has space to store the record.
If the bucket does not have enough space, a bucket overflow is said to occur.
The number of buckets, which we denote nB , must be chosen such that nB > nr/ fr , where nr denotes the total number of records that will be stored and fr denotes the number of records that will fit in a bucket.
This designation, of course, assumes that the total number of records is known when the hash function is chosen.
Some buckets are assigned more records than are others, so a bucket may overflow even when other buckets still have space.
The chosen hash function may result in nonuniform distribution of search keys.
Despite allocation of a few more buckets than required, bucket overflow can still occur.
If a record must be inserted into a bucket b, and b is already full, the system provides an overflow bucket for b, and inserts the record into the overflow bucket.
If the overflow bucket is also full, the system provides another overflow bucket, and so on.
All the overflow buckets of a given bucket are chained together in a linked list, as in Figure 11.24
Overflow handling using such a linked list is called overflow chaining.
We must change the lookup algorithm slightly to handle overflow chaining.
As before, the systemuses the hash function on the search key to identify a bucket.
The systemmust examine all the records in bucket b to see whether they match the search key, as before.
In addition, if bucket b has overflow buckets, the system must examine the records in all the overflow buckets also.
The form of hash structure that we have just described is sometimes referred to as closed hashing.
Under an alternative approach, called open hashing, the set of buckets is fixed, and there are no overflow chains.
Instead, if a bucket is full, the system inserts records in some other bucket in the initial set of buckets B.
One policy is to use the next bucket (in cyclic order) that has space; this policy is called linear probing.
Other policies, such as computing further hash functions, are also used.
Open hashing has been used to construct symbol tables for compilers and assemblers, but closed hashing is preferable for database systems.
The reason is that deletion under open hashing is troublesome.
Usually, compilers and assemblers perform only lookup and insertion operations on their symbol tables.
However, in a database system, it is important to be able to handle deletion as well as insertion.
Thus, open hashing is of only minor importance in database implementation.
An important drawback to the form of hashing that we have described is that we must choose the hash function when we implement the system, and it cannot be changed easily thereafter if the file being indexed grows or shrinks.
Since the function h maps search-key values to a fixed set B of bucket addresses, we waste space if B is made large to handle future growth of the file.
If B is too small, the buckets contain records of many different search-key values, and bucket overflows can occur.
We study later, in Section 11.7, how the number of buckets and the hash function can be changed dynamically.
Figure 11.25 Hash index on search key ID of instructor file.
Hashing can be used not only for file organization, but also for index-structure creation.
A hash index organizes the search keys, with their associated pointers, into a hash file structure.
We apply a hash function on a search key to identify a bucket, and store the key and its associated pointers in the bucket (or in overflow buckets)
Figure 11.25 shows a secondary hash index on the instructor file, for the search key ID.
The hash index has eight buckets, each of size 2 (realistic indices would, of course, have much larger bucket sizes)
One of the buckets has three keysmapped to it, so it has an overflow bucket.
In this example, ID is a primary key for instructor, so each search key has.
In general, multiple pointers can be associated with each key.
We use the term hash index to denote hash file structures as well as secondary hash indices.
A hash index is never needed as a clustering index structure, since, if a file itself is organized by hashing, there is no need for a separate hash index structure on it.
However, since hash file organization provides the same direct access to records that indexing provides, we pretend that a file organized by hashing also has a clustering hash index on it.
As we have seen, the need to fix the set B of bucket addresses presents a serious problemwith the static hashing technique of the previous section.Most databases grow larger over time.
If we are to use static hashing for such a database, we have three classes of options:
Choose a hash function based on the current file size.
This option will result in performance degradation as the database grows.
Choose a hash function based on the anticipated size of the file at some point in the future.
Although performance degradation is avoided, a significant amount of space may be wasted initially.
Periodically reorganize the hash structure in response to file growth.
Such a reorganization involves choosing a new hash function, recomputing the hash function on every record in the file, and generating new bucket assignments.
Furthermore, it is necessary to forbid access to the file during reorganization.
Several dynamic hashing techniques allow the hash function to be modified dynamically to accommodate the growth or shrinkage of the database.
In this section we describe one form of dynamic hashing, called extendable hashing.
The bibliographical notes provide references to other forms of dynamic hashing.
Extendable hashing copes with changes in database size by splitting and coalescing buckets as the database grows and shrinks.
Moreover, since the reorganization is performed on only one bucket at a time, the resulting performance overhead is acceptably low.
With extendable hashing, we choose a hash function h with the desirable properties of uniformity and randomness.
However, this hash function generates values over a relatively large range—namely, b-bit binary integers.
The i appearing above the bucket address table in the figure indicates that i bits of the hash value h(K ) are required to determine the correct bucket for K.
This number will, of course, change as the file grows.
Although i bits are required to find the correct entry in the bucket address table, several consecutive table entries may point to the same bucket.
All such entries will have a common hash prefix, but the length of this prefix may be less than i.
Therefore, we associate with each bucket an integer giving the length of the commonhash prefix.
In Figure 11.26 the integer associated with bucket j is shown as i j.
We now see how to perform lookup, insertion, and deletion on an extendable hash structure.
To locate the bucket containing search-key value Kl , the system takes the first i high-order bits of h(Kl), looks at the corresponding table entry for this bit string, and follows the bucket pointer in the table entry.
To insert a record with search-key value Kl , the system follows the same procedure for lookup as before, ending up in some bucket—say, j.
If there is room in the bucket, the system inserts the record in the bucket.
If, on the other hand, the bucket is full, it must split the bucket and redistribute the current records, plus the new one.
To split the bucket, the system must first determine from the hash value whether it needs to increase the number of bits that it uses.
If i = i j , only one entry in the bucket address table points to bucket j.
Therefore, the system needs to increase the size of the bucket address table so that it can include pointers to the two buckets that result from splitting bucket j.
It does so by considering an additional bit of the hash value.
It increments the value of i by 1, thus doubling the size of the bucket address table.
It replaces each entry by two entries, both of which contain the same pointer as the original entry.
Now two entries in the bucket address table point to bucket j.
The system allocates a new bucket (bucket z), and sets the second entry to point to the new bucket.
Next, it rehashes each record in bucket j and, depending on the first i bits (remember the system has added 1 to i), either keeps it in bucket j or allocates it to the newly created bucket.
The system now reattempts the insertion of the new record.
Usually, the attemptwill succeed.However, if all the records in bucket j, aswell as the new record, have the same hash-value prefix, it will be necessary to split a bucket again, since all the records in bucket j and the new record are assigned to the same bucket.
If the hash function has been chosen carefully, it is unlikely that a single insertion will require that a bucket be split more than once, unless there are a large number of records with the same search key.
If all the records in bucket j have the same search-key value, no amount of splitting will help.
In such cases, overflow buckets are used to store the records, as in static hashing.
If i > i j , then more than one entry in the bucket address table points to bucket j.
Thus, the system can split bucket j without increasing the size of the bucket address table.
Observe that all the entries that point to bucket j correspond to hash prefixes that have the same value on the leftmost i j bits.
The system allocates a new bucket (bucket z), and sets i j and iz to the value that results from adding 1 to the original i j value.
Next, the system needs to adjust the entries in the bucket address table that previously pointed to bucket j.
Note that with the new value for i j , not all the entries correspond to hash prefixes that have the same value on the leftmost i j bits.
The system leaves the first half of the entries as they were (pointing to bucket j), and sets all the remaining entries to point to the newly created bucket (bucket z)
Next, as in the previous case, the system rehashes each record in bucket j, and allocates it either to bucket j or to the newly created bucket z.
In the unlikely case that it again fails, it applies one of the two cases, i = i j or i > i j , as appropriate.
Note that, in both cases, the system needs to recompute the hash function on only the records in bucket j.
To delete a record with search-key value Kl , the system follows the same procedure for lookup as before, ending up in some bucket—say, j.
It removes both the search key from the bucket and the record from the file.
Note that, at this point, several buckets can be coalesced, and the size of the bucket address table can be cut in half.
The procedure for deciding on which buckets can be coalesced and how to coalesce buckets is left to you to do as an exercise.
The conditions under which the bucket address table can be reduced in size are also left to you as an exercise.
Unlike coalescing of buckets, changing the size of the bucket address table is a rather expensive operation if the table is large.
Assume that, initially, the file is empty, as in Figure 11.28
To illustrate all the features of extendable hashing in a small structure, we shall make the unrealistic assumption that a bucket can hold only two records.
Thebucket address table contains a pointer to the one bucket, and the system inserts the record.
The system also places this record in the one bucket of our structure.
Since i = i0, we need to increase the number of bits that we use from the hash value.
This increase in the number of bits necessitates doubling the size of the bucket address table to two entries.
The system splits the bucket, placing in the new bucket those records whose search key has a hash value beginning with 1, and leaving in the original bucket the other records.
Figure 11.29 shows the state of our structure after the split.
Once again, we find the bucket full and i = i1
This increase in the number of bits necessitates doubling the size of the bucket address table to four entries, as in Figure 11.30
Next, we insert the records of “Califieri”, “Singh”, and “Crick” without any bucket overflow.
This overflow cannot be handled by increasing the number of bits, since there are three records with exactly the same hash value.
We continue in this manner until we have inserted all the instructor records of Figure 11.1
We now examine the advantages and disadvantages of extendable hashing, compared with static hashing.
The main advantage of extendable hashing is that performance does not degrade as the file grows.
Although the bucket address table incurs additional overhead, it contains one pointer for each hash value for the current prefix length.
The main space saving of extendable hashing over other forms of hashing is that no buckets need to be reserved for future growth; rather, buckets can be allocated dynamically.
A disadvantage of extendable hashing is that lookup involves an additional level of indirection, since the system must access the bucket address table before accessing the bucket itself.
This extra reference has only a minor effect on performance.
Although the hash structures that we discussed in Section 11.6 do not.
Thus, extendable hashing appears to be a highly attractive technique, provided that we are willing to accept the added complexity involved in its implementation.
The bibliographical notes reference more detailed descriptions of extendable hashing implementation.
The bibliographical notes also provide references to another form of dynamic hashing called linear hashing, which avoids the extra level of indirection associated with extendable hashing, at the possible cost of more overflow buckets.
We have seen several ordered-indexing schemes and several hashing schemes.
We can organize files of records as ordered files by using index-sequential organization or B+-tree organizations.
Finally, we can organize them as heap files, where the records are not ordered in any particular way.
A database-system implementor could providemany schemes, leaving the final decision of which schemes to use to the database designer.
However, such an approach requires the implementor to write more code, adding both to the cost of the system and to the space that the system occupies.
Most database systems support B+-trees andmay additionally support some form of hash file organization or hash indices.
To make a choice of file organization and indexing techniques for a relation, the implementor or the database designer must consider the following issues:
Is the cost of periodic reorganization of the index or hash organization acceptable?
Is it desirable to optimize average access time at the expense of increasing.
We have already examined the first three of these issues, first in our review of the relative merits of specific indexing techniques, and again in our discussion of hashing techniques.
The fourth issue, the expected type of query, is critical to the choice of ordered indexing or hashing.
For queries of this form, a hashing scheme is preferable.
An ordered-index lookup requires time proportional to the log of the number of values in r for Ai.
In a hash structure, however, the average lookup time is a constant independent of the size of the database.
The only advantage to an index over a hash structure for this form of query is that the worst-case lookup time is proportional to the log of the number of values in r for Ai.
By contrast, for hashing, the worst-case lookup time is proportional to the number of values in r for Ai.
However, the worst-case lookup time is unlikely to occur with hashing, and hashing is preferable in this case.
Ordered-index techniques are preferable to hashing in cases where the query specifies a range of values.
Let us consider how we process this query using an ordered index.
If, instead of an ordered index, we have a hash structure, we can perform a lookup on c1 and can locate the corresponding bucket—but it is not easy, in general, to determine the next bucket that must be examined.
The difficulty arises because a good hash function assigns values randomly to buckets.
Thus, there is no simple notion of “next bucket in sorted order.” The reason we cannot chain buckets together in sorted order on Ai is that each bucket is assignedmany searchkey values.
Since values are scattered randomly by the hash function, the values in the specified range are likely to be scattered across many or all of the buckets.
Therefore, we have to read all the buckets to find the required search keys.
Usually the designer will choose ordered indexing unless it is known in advance that range queries will be infrequent, in which case hashing would be chosen.
Hash organizations are particularly useful for temporary files created during query processing, if lookups based on a key value are required, but no range queries will be performed.
Bitmap indices are a specialized type of index designed for easy querying on multiple keys, although each bitmap index is built on a single key.
Given a number n, it must be easy to retrieve.
This is particularly easy to achieve if records are fixed in size, and allocated on consecutive blocks of a file.
The record number can then be translated easily into a block number and a number that identifies the record within the block.
In its simplest form, a bitmap index on the attribute A of relation r consists of one bitmap for each value that A can take.
Each bitmap has as many bits as the number of records in the relation.
The ith bit of the bitmap for value v j is set to 1 if the record numbered i has the value v j for attribute A.
In our example, there is one bitmap for the value m and one for f.
The ith bit of the bitmap for m is set to 1 if the gender value of the record numbered i is m.
Figure 11.35 shows an example of bitmap indices on a relation instructor info.
The simplest way of retrieving all records with value m (or value f) would be to simply read all records of the relation and select those records with value m (or f, respectively)
The bitmap index doesn’t really help to speed up such a selection.
In fact, bitmap indices are useful for selections mainly when there are selections on multiple keys.
Suppose we create a bitmap index on attribute income level, which we described earlier, in addition to the bitmap index on gender.
To evaluate this selection, we fetch the bitmaps for gender value f and the bitmap for income level value L2, and perform an intersection (logical-and) of the two bitmaps.
If there are further conditions, the fraction of records satisfying all the conditions is likely to be quite small.
The system can then compute the query result by finding all bits with value 1 in the intersection bitmap and retrieving the corresponding records.
If the fraction is large, scanning the entire relation would remain the cheaper alternative.
Another important use of bitmaps is to count the number of tuples satisfying a given selection.
Bitmap indices are generally quite small compared to the actual relation size.
Records are typically at least tens of bytes to hundreds of bytes long, whereas a single bit represents the record in a bitmap.
Thus the space occupied by a single bitmap is usually less than 1 percent of the space occupied by the relation.
If an attribute A of the relation can take on only one of eight values, a bitmap index on attribute Awould consist of eight bitmaps, which together occupy only 1 percent of the size of the relation.
Deletion of records creates gaps in the sequence of records, since shifting records (or record numbers) to fill gaps would be extremely expensive.
We shall see the need for existence bitmaps in Section 11.9.2
Insertion of records should not affect the sequence numbering of.
Therefore, we can do insertion either by appending records to the end of the file or by replacing deleted records.
We can compute the intersection of two bitmaps easily by using a for loop: the ith iteration of the loop computes the and of the ith bits of the two bitmaps.
We can speed up computation of the intersection greatly by using bit-wise and instructions supported by most computer instruction sets.
A bit-wise and instruction takes two words as input and outputs a word where each bit is the logical and of the bits in corresponding positions of the input words.
Just as bitmap intersection is useful for computing the and of two conditions, bitmap union is useful for computing the or of two conditions.
The procedure for bitmap union is exactly the same as for intersection, except we use bit-wise or instructions instead of bit-wise and instructions.
The complement operation can be used to compute a predicate involving the negation of a condition, such as not (income-level = L1)
If some records have been deleted, however, just computing the complement of a bitmap is not sufficient.
A similar problem also arises when the value of an attribute is null.
To make sure that the bits corresponding to deleted records are set to 0 in the result, the complement bitmap must be intersected with the existence bitmap to turn off the bits for deleted records.
Similarly, to handle null values, the complement bitmap must also be intersected with the complement of the bitmap for the value null.2
Counting the number of bits that are 1 in a bitmap can be done quickly by a clever technique.
We take each byte of the bitmap, use it to index into this array, and add the stored count to the total count.
Handling predicates such as is unknownwould cause further complications, which would in general require use of an extra bitmap to track which operation results are unknown.
Bitmaps can be combined with regular B+-tree indices for relations where a few attribute values are extremely common, and other values also occur, but much less frequently.
In a B+-tree index leaf, for each valuewewould normallymaintain a list of all records with that value for the indexed attribute.
Each element of the list would be a record identifier, consisting of at least 32 bits, and usually more.
For a value that occurs in many records, we store a bitmap instead of a list of records.
Thus, bitmaps can be used as a compressed storage mechanism at the leaf nodes of B+-trees for those values that occur very frequently.
The SQL standard does not provide any way for the database user or administrator to control what indices are created and maintained in the database system.
Indices are not required for correctness, since they are redundant data structures.
However, indices are important for efficient processing of transactions, including both update transactions and queries.
Indices are also important for efficient enforcement of integrity constraints.
In principle, a database system can decide automatically what indices to create.
However, because of the space cost of indices, as well as the effect of indices onupdate processing, it is not easy to automaticallymake the right choices about what indices to maintain.
Although the syntax that we show is widely used and supported by many database systems, it is not part of the SQL standard.
The SQL standard does not support control of the physical database schema; it restricts itself to the logical database schema.
We create an index with the create index command, which takes the form:
The attribute-list is the list of attributes of the relations that form the search key for the index.
To define an index named dept index on the instructor relation with dept name as the search key, we write:
If we wish to declare that the search key is a candidate key, we add the attribute unique to the index definition.
If, at the time we enter the create unique index command, dept name is not a candidate key, the system will display an error message, and the attempt to create the index will fail.
If the index-creation attempt succeeds, any subsequent attempt to insert a tuple that violates the key declaration will fail.
Note that the unique feature is redundant if the database system supports the unique declaration of the SQL standard.
Many database systems also provide a way to specify the type of index to be used (such as B+-tree or hashing)
Some database systems also permit one of the indices on a relation to be declared to be clustered; the system then stores the relation sorted by the search-key of the clustered index.
The index name we specified for an index is required to drop an index.
Many queries reference only a small proportion of the records in a file.
To reduce the overhead in searching for these records, we can construct indices for the files that store the database.
Index-sequential files are one of the oldest index schemes used in database systems.
To permit fast retrieval of records in search-key order, records are stored sequentially, and out-of-order records are chained together.
To allow fast random access, we use an index structure.
There are two types of indices that we can use: dense indices and sparse indices.
Dense indices contain entries for every search-key value, whereas sparse indices contain entries only for some search-key values.
If the sort order of a search key matches the sort order of a relation, an index on the search key is called a clustering index.
Secondary indices improve the performance of queries that use search keys other than the search key of the clustering index.
However, they impose an overhead on modification of the database.
The primary disadvantage of the index-sequential file organization is that performance degrades as the file grows.
To overcome this deficiency, we can use a B+-tree index.
A B+-tree index takes the form of a balanced tree, in which every path from the root of the tree to a leaf of the tree is of the same length.
B+-trees are much shorter than other balanced binary-tree structures such as AVL trees, and therefore require fewer disk accesses to locate records.
Insertion and deletion, however, are somewhat more complicated, but still efficient.
The number of operations required for lookup, insertion, and deletion on B+-trees is proportional to the logarithm to the base N of the number of records in the relation, where each nonleaf node stores N pointers.
We can use B+-trees for indexing a file containing records, as well as to organize records into a file.
The primary advantage of a B-tree is that the B-tree eliminates the redundant storage of search-key values.
The major disadvantages are overall complexity and reduced fanout for a given node size.
System designers almost universally prefer B+-tree indices over B-tree indices in practice.
Sequential file organizations require an index structure to locate data.
File organizations based on hashing, by contrast, allow us to find the address of a data item directly by computing a function on the search-key value of the desired record.
Since we do not know at design time precisely which searchkey values will be stored in the file, a good hash function to choose is one that assigns search-key values to buckets such that the distribution is both uniform and random.
Static hashing uses hash functions in which the set of bucket addresses is fixed.
Such hash functions cannot easily accommodate databases that grow significantly larger over time.
There are several dynamic hashing techniques that allow the hash function to be modified.
One example is extendable hashing, which copeswith changes in database size by splitting and coalescing buckets as the database grows and shrinks.
We can also use hashing to create secondary indices; such indices are called hash indices.
For notational convenience, we assume hash file organizations have an implicit hash index on the search key used for hashing.
Ordered indices such as B+-trees and hash indices can be used for selections based on equality conditions involving single attributes.
When multiple attributes are involved in a selection condition, we can intersect record identifiers retrieved from multiple indices.
Bitmap indices provide a very compact representation for indexing attributes with very few distinct values.
Intersection operations are extremely fast on bitmaps, making them ideal for supporting queries on multiple attributes.
Assume that the tree is initially empty and values are added in ascending order.
Construct B+-trees for the cases where the number of pointers that will fit in one node is as follows:
What is the expected height of the tree as a function of n?
Show the extendable hash structure for this file if the hash function is h(x) = x mod 8 and buckets can hold three records.
Also give pseudocode for the iterator class, including the variables in the iterator object, and the next() method.
Do not bother about reducing the size of the bucket address table.
Give details of how the count should be maintained when buckets are split, coalesced, or deleted.
Note: Reducing the size of the bucket address table is an expensive operation, and subsequent inserts may cause the table to grow again.
Consider a query that requests all instructors in the Finance department with salary of 80000 or more.
Outline the steps in answering the query, and show the final and intermediate bitmaps constructed to answer the query.
Give a formula for the cost of building the B+-tree index by inserting one record at a time.
Assume each block will hold an average of f entries, and that all levels of the tree above the leaf are in memory.
Write pseudocode for bottom-up construction of a B+-tree, which was outlined in Section 11.4.4
You can assume that a function to efficiently sort a large file is available.
Suggest how the file organization may be reorganized to restore sequentiality.
An alternative to reorganization is to allocate leaf pages in units of n blocks, for some reasonably large n.
When the first leaf of a B+tree is allocated, only one block of an n-block unit is used, and the remaining pages are free.
If a page splits, and its n-block unit has a free page, that space is used for the new page.
If the n-block unit is full, another n-block unit is allocated, and the first n/2 leaf pages are placed in one n-block unit, and the remaining in the second n-block unit.
What is the worst case occupancy of allocated space, assuming.
Is it possible that leaf nodes allocated to an n-node block unit.
Under the reasonable assumption that buffer space is sufficient to store a n-page block, how many seeks would be required for a leaf-level scan of the B+-tree, in the worst case? Compare this number with the worst case if leaf pages are allocated a block at a time.
The technique of redistributing values to siblings to improve space utilization is likely to be more efficient when usedwith the above allocation scheme for leaf blocks.
What effect could this change have on the height of the B+-tree?
Discuss the relative merits of each technique in database applications.
Suppose secondary indices stored record identifiers that are pointers to records on disk.
What would be the effect on the secondary indices if a node split happens in the file organization?
What would be the cost of updating all affected records in a secondary index?
How does using the search key of the file organization as a logical record identifier solve this problem?
What is the extra cost due to the use of such logical record identifiers?
Make sure that your technique works even in the presence of null values, by using a bitmap for the value null.
Suggest how to implement the hash table, if it can be much larger than C blocks.
Discussions of the basic data structures in indexing and hashing can be found in Cormen et al.
The bibliographical notes in Chapter 15 provide references to research on allowing concurrent accesses and updates on B+-trees.
Gray and Reuter [1993] provide a good description of issues in the implementation of B+-trees.
Several alternative tree and treelike search structures have been proposed.
Tries are trees whose structure is based on the “digits” of keys (for example, a dictionary thumb index, which has one entry for each letter)
Such trees may not be balanced in the sense that B+-trees are.
Knuth [1973] analyzes a large number of different hashing techniques.
A performance comparison with extendable hashing is given by Rathi et al.
Bitmap indices, and variants called bit-sliced indices and projection indices, are described in O’Neil and Quass [1997]
They providevery large speedups on certain types of queries, and are today implemented on most database systems.
Query processing refers to the range of activities involved in extracting data from a database.
The activities include translation of queries in high-level database languages into expressions that can be used at the physical level of the file system, a variety of query-optimizing transformations, and actual evaluation of queries.
The steps involved in processing a query appear in Figure 12.1
Before query processing can begin, the system must translate the query into a usable form.
A language such as SQL is suitable for human use, but is ill suited to be the system’s internal representation of a query.
A more useful internal representation is one based on the extended relational algebra.
Thus, the first action the system must take in query processing is to translate a given query into its internal form.
This translation process is similar to the work performed by the parser of a compiler.
In generating the internal form of the query, the parser checks the syntax of the user’s query, verifies that the relation names appearing in the query are names of the relations in the database, and so on.
The system constructs a parse-tree representation of the query, which it then translates into a relational-algebra expression.
If the query was expressed in terms of a view, the translation phase also replaces all uses of the view by the relational-algebra expression that defines the view.1 Most compiler texts cover parsing in detail.
For materialized views, the expression defining the view has already been evaluated and stored.
Therefore, the stored relation can be used, instead of uses of the view being replaced by the expression defining the view.
Given a query, there are generally a variety of methods for computing the answer.
For example, we have seen that, in SQL, a query could be expressed in several different ways.
Each SQL query can itself be translated into a relationalalgebra expression in one of several ways.
Furthermore, the relational-algebra representationof a query specifies onlypartially how to evaluate a query; there are usually severalways to evaluate relational-algebra expressions.As an illustration, consider the query:
This query can be translated into either of the following relational-algebra expressions:
Further, we can execute each relational-algebra operation by one of several different algorithms.
If a B+-tree index is available on the attribute salary, we can use the index instead to locate the tuples.
To specify fully how to evaluate a query, we need not only to provide the relational-algebra expression, but also to annotate it with instructions specifying how to evaluate each operation.
A relationalalgebra operation annotated with instructions on how to evaluate it is called an evaluation primitive.
A sequence of primitive operations that can be used to evaluate a query is a query-execution plan or query-evaluation plan.
The query-execution engine takes a query-evaluation plan, executes that plan, and returns the answers to the query.
The different evaluation plans for a given query can have different costs.
We do not expect users to write their queries in a way that suggests the most efficient evaluation plan.
Rather, it is the responsibility of the system to construct a queryevaluation plan that minimizes the cost of query evaluation; this task is called query optimization.
Once the query plan is chosen, the query is evaluated with that plan, and the result of the query is output.
The sequence of steps already described for processing a query is representative; not all databases exactly follow those steps.
For instance, instead of using the relational-algebra representation, several databases use an annotated parsetree representation based on the structure of the given SQL query.
However, the concepts that we describe here form the basis of query processing in databases.
In order to optimize a query, a query optimizer must know the cost of each operation.
Although the exact cost is hard to compute, since it depends on many parameters such as actual memory available to the operation, it is possible to get a rough estimate of execution cost for each operation.
Section 12.2 outlines how we measure the cost of a query.
Several operations may be grouped together into a pipeline, in which each of the operations starts working on its input tuples even as they are being generated by another operation.
In Section 12.7, we examine how to coordinate the execution of multiple operations in a query evaluation plan, in particular, how to use pipelined operations to avoid writing intermediate results to disk.
There are multiple possible evaluation plans for a query, and it is important to be able to compare the alternatives in terms of their (estimated) cost, and choose the best plan.
To do so, we must estimate the cost of individual operations, and combine them to get the cost of a query evaluation plan.
Thus, as we study evaluation algorithms for each operation later in this chapter, we also outline how to estimate the cost of the operation.
In large database systems, the cost to access data from disk is usually themost important cost, since disk accesses are slow compared to in-memory operations.
Moreover, CPU speeds have been improving much faster than have disk speeds.
Thus, it is likely that the time spent in disk activity will continue to dominate the total time to execute a query.
The CPU time taken for a task is harder to estimate since it depends on low-level details of the execution code.
Although real-life query optimizers do take CPU costs into account, for simplicity in this book we ignore CPU costs and use only disk-access costs to measure the cost of a query-evaluation plan.
We can refine our cost estimates further by distinguishing block reads from block writes, since block writes are typically about twice as expensive as reads (this is because disk systems read sectors back after they are written to verify that the write was successful)
For simplicity, we ignore this detail, and leave it to you to work out more precise cost estimates for various operations.
The cost estimateswe give do not include the cost of writing the final result of an operation back to disk.
The costs of all the algorithms that we consider depend on the size of the buffer in main memory.
In the best case, all data can be read into the buffers, and the disk does not need to be accessed again.
In the worst case, we assume that the buffer can hold only a few blocks of data—approximately one block per relation.
When presenting cost estimates, we generally assume the worst case.
Some database systems perform test seeks and block transfers to estimate average seek and block transfer costs, as part of the software installation process.
In addition, although we assume that data must be read from disk initially, it is possible that a block that is accessed is already present in the in-memory buffer.
Again, for simplicity, we ignore this effect; as a result, the actual disk-access cost during the execution of a plan may be less than the estimated cost.
The response time for a query-evaluation plan (that is, the wall-clock time required to execute the plan), assuming no other activity is going on in the computer, would account for all these costs, and could be used as a measure of the cost of the plan.
Unfortunately, the response time of a plan is very hard to estimate without actually executing the plan, for the following reasons:
The response time depends on the contents of the buffer when the query begins execution; this information is not available when the query is optimized, and is hard to account for even if it were available.
In a systemwithmultiple disks, the response time depends on how accesses are distributed among disks, which is hard to estimate without detailed knowledge of data layout on disk.
Interestingly, a plan may get a better response time at the cost of extra resource consumption.
For example, if a system has multiple disks, a plan A that requires extra disk reads, but performs the reads in parallel across multiple disks may finish faster than another plan B that has fewer disk reads, but from only one disk.
However, if many instances of a query using plan A run concurrently, the overall response timemayactually bemore than if the same instances are executed using plan B, since plan Agenerates more load on the disks.
As a result, instead of trying to minimize the response time, optimizers generally try tominimize the total resource consumption of a query plan.
Ourmodel of estimating the total disk access time (including seek and data transfer) is an example of such a resource consumption–based model of query cost.
In query processing, the file scan is the lowest-level operator to access data.
File scans are search algorithms that locate and retrieve records that fulfill a selection condition.
In relational systems, a file scan allows an entire relation to be read in those cases where the relation is stored in a single, dedicated file.
Consider a selection operation on a relation whose tuples are stored together in one file.
The most straightforward way of performing a selection is as follows:
In a linear search, the system scans each file block and tests all records to see whether they satisfy the selection condition.
An initial seek is required to access the first block of the file.
Although it may be slower than other algorithms for implementing selection, the linear-search algorithm can be applied to any file, regardless of the ordering of the file, or the availability of indices, or the nature of the selection operation.
The other algorithms that we shall study are not applicable in all cases, but when applicable they are generally faster than linear search.
Cost estimates for linear scan, as well as for other selection algorithms, are shown in Figure 12.3
In the figure, we use hi to represent the height of the B+tree.
Real-life optimizers usually assume that the root of the tree is present in the in-memory buffer since it is frequently accessed.
Some optimizers even assume that all but the leaf level of the tree is present in memory, since they are accessed relatively frequently, and usually less than 1 percent of the nodes of a B+-tree are nonleaf nodes.
Index structures are referred to as access paths, since they provide a path through which data can be located and accessed.
In Chapter 11, we pointed out that it is efficient to read the records of a file in an order corresponding closely to physical order.
Recall that a primary index (also referred to as a clustering index) is an index that allows the records of a file to be read in an order that corresponds to the physical order in the file.
An index that is not a primary index is called a secondary index.
Search algorithms that use an index are referred to as index scans.
We use the selection predicate to guide us in the choice of the index to use in processing the query.
For an equality comparison on a key attributewith a primary index,we can use the index to retrieve a single record that satisfies the corresponding equality condition.
We can retrieve multiple records by using a primary index when the selection condition specifies an equality comparison on a nonkey attribute, A.
The only difference from the previous case is that multiple records may need to be fetched.
However, the records must be stored consecutively in the file since the file is sorted on the search key.
Selections specifying an equality condition can use a secondary index.
This strategy can retrieve a single record if the equality condition is on a key; multiple recordsmay be retrieved if the indexing field is not a key.
The time cost in this case is the same as that for a primary index (case A2)
In the second case, each record may be resident on a different block, which.
Since at most one record satisfies condition, scan can be terminated as soon as the required record is found.
In the worst case, br blocks transfers are still required.
Index lookup traverses the height of the tree plus one I/O to fetch the record; each of these I/O operations requires a seek and a block transfer.
One seek for each level of the tree, one seek for the first block.
Here b is the number of blocks containing records with the specified search key, all of which are read.
These blocks are leaf blocks assumed to be stored sequentially (since it is a primary index) and don’t require additional seeks.
Here, cost of index traversal is the same as for A3, but each record may be on a different block, requiring a seek per record.
If the in-memory buffer is large, the block containing the record may already be in the buffer.
It is possible to construct an estimate of the average or expected cost of the selection by taking into account the probability of the block containing the record already being in the buffer.
For large buffers, that estimate will be much less than the worst-case estimate.
In certain algorithms, including A2, the use of a B+-tree file organization can save one access since records are stored at the leaf-level of the tree.
Accessing a record through such a secondary index is then more expensive: First the secondary index is searched to find the primary index search-key values, then the primary index is looked up to find the records.
The cost formulae described for secondary indices have to be modified appropriately if such indices are used.
Recall that if B+-tree file organizations are used to store relations, records may be moved between blocks when leaf nodes are split or merged, and when records are redistributed.
The secondary index provides pointers to the records, but to get the actual records we have to fetch the records by using the pointers.
This step may require an I/O operation for each record fetched, since consecutive records may be on different disk blocks; as before, each I/O operation requires a disk seek and a block transfer.
If the number of retrieved records is large, using the secondary index may be even more expensive than using linear search.
Therefore the secondary index should be used only if very few records are selected.
So far, we have considered only simple selection conditions of the form A op B, where op is an equality or comparison operation.We now considermore complex selection predicates.
Conjunction: A conjunctive selection is a selection of the form:
Disjunction: A disjunctive selection is a selection of the form:
We can implement a selection operation involving either a conjunction or a disjunction of simple conditions by using one of the following algorithms:
We first determine whether an access path is available for an attribute in one of the simple conditions.
An appropriate composite index (that is, an index on multiple attributes) may be available for some conjunctive selections.
If the selection specifies an equality condition on two or more attributes, and a composite index exists on these combined attribute.
Another alternative for implementing conjunctive selection operations involves the use of record pointers or record identifiers.
This algorithm requires indices with record pointers, on the fields involved in the individual conditions.
The algorithm scans each index for pointers to tuples that satisfy an individual condition.
The intersection of all the retrieved pointers is the set of pointers to tuples that satisfy the conjunctive condition.
The algorithm then uses the pointers to retrieve the actual records.
If indices are not available on all the individual conditions, then the algorithm tests the retrieved records against the remaining conditions.
This cost can be reduced by sorting the list of pointers and retrieving records in the sorted order.
If access paths are available on all the conditions of a disjunctive selection, each index is scanned for pointers to tuples that satisfy the individual condition.
The union of all the retrieved pointers yields the set of pointers to all tuples that satisfy the disjunctive condition.
We then use the pointers to retrieve the actual records.
However, if even one of the conditions does not have an access path, we have to perform a linear scan of the relation to find tuples that satisfy the condition.
Therefore, if there is even one such condition in the disjunct, the most efficient access method is a linear scan, with the disjunctive condition tested on each tuple during the scan.
The implementation of selections with negation conditions is left to you as an exercise (Practice Exercise 12.6)
Sorting of data plays an important role in database systems for two reasons.
First, SQL queries can specify that the output be sorted.
Second, and equally important for query processing, several of the relational operations, such as joins, can be implemented efficiently if the input relations are first sorted.
Thus, we discuss sorting here before discussing the join operation in Section 12.5
We can sort a relation by building an index on the sort key, and then using that index to read the relation in sorted order.
However, such a process orders the relation only logically, through an index, rather than physically.
For this reason, it may be desirable to order the records physically.
The problem of sorting has been studied extensively, both for relations that fit entirely in main memory and for relations that are bigger than memory.
In the first case, standard sorting techniques such as quick-sort can be used.
Sorting of relations that do not fit in memory is called external sorting.
The most commonly used technique for external sorting is the external sort–merge algorithm.We describe the external sort–merge algorithm next.
Let Mdenote the number of blocks in the main-memory buffer available for sorting, that is, the number of disk blocks whose contents can be buffered in available mainmemory.
In the first stage, a number of sorted runs are created; each run is sorted, but contains only some of the records of the relation.
Suppose, for now, that the total number of runs, N, is less than M, so that we can allocate one block to each run and have space left to hold one block of output.
The output file is buffered to reduce the number of disk write operations.
The preceding merge operation is a generalization of the two-way merge used by the standard in-memory sortmerge algorithm; it merges N runs, so it is called an N-way merge.
Figure 12.4 illustrates the steps of the external sort–merge for an example relation.
For illustration purposes, we assume that only one tuple fits in a block ( fr = 1), and we assume that memory holds at most three blocks.
During the merge stage, two blocks are used for input and one for output.
We compute the disk-access cost for the external sort–merge in this way: Let br denote the number of blocks containing records of relation r.
In this section, we study several algorithms for computing the join of relations, and we analyze their respective costs.
To be more precise, since we read each run separately and may get fewer than bb blocks when reading the end of a run, we may require an extra seek for each run.
If one of the relations fits entirely in main memory, it is beneficial to use that relation as the inner relation, since the inner relation would then be read only once.
Therefore, if s is small enough to fit in main memory, our strategy requires only a total br + bs block transfers and 2 seeks—the same cost as that for the case where both relations fit in memory.
If the buffer is too small to hold either relation entirely in memory, we can still obtain a major saving in block accesses if we process the relations on a per-block basis, rather than on a per-tuple basis.
Figure 12.6 shows block nested-loop join, which is a variant of the nested-loop join where every block of the inner relation is paired with every block of the outer relation.
Within each pair of blocks, every tuple in one block is paired with every tuple in the other block, to generate all pairs of tuples.
As before, all pairs of tuples that satisfy the join condition are added to the result.
The primary difference in cost between the block nested-loop join and the basic nested-loop join is that, in the worst case, each block in the inner relation s is read only once for each block in the outer relation, instead of once for each tuple.
The performance of the nested-loop and block nested-loop procedures can be further improved:
If the join attributes in a natural join or an equi-join form a key on the inner relation, then for each outer relation tuple the inner loop can terminate as soon as the first match is found.
We can scan the inner loop alternately forward and backward.
This scanning method orders the requests for disk blocks so that the data remaining in the buffer from the previous scan can be reused, thus reducing the number of disk accesses needed.
If an index is available on the inner loop’s join attribute, we can replace file scans with more efficient index lookups.
In a nested-loop join (Figure 12.5), if an index is available on the inner loop’s join attribute, index lookups can replace file scans.
For each tuple tr in the outer relation r, the index is used to look up tuples in s thatwill satisfy the join condition with tuple tr.
This join method is called an indexed nested-loop join; it can be used with existing indices, as well as with temporary indices created for the sole purpose of evaluating the join.
The cost formula indicates that, if indices are available on both relations r and s, it is generally most efficient to use the one with fewer tuples as the outer relation.
These pointers point initially to the first tuple of the respective relations.
As the algorithm proceeds, the pointers move through the relation.
A group of tuples of one relation with the same value on the join attributes is read into Ss.
The algorithm in Figure 12.7 requires that every set of tuples Ss fit in mainmemory; we discuss extensions of the algorithm to avoid this requirement shortly.
Then, the corresponding tuples (if any) of the other relation are read in, and are processed as they are read.
It is instructive to go through the steps of the merge-join algorithm on the relations shown in the figure.
The merge-join algorithm of Figure 12.7 requires that each set Ss of all tuples with the same value for the join attributes must fit in main memory.
This requirement can usually be met, even if the relation s is large.
If there are some join attribute values for which Ss is larger than available memory, a block nested-loop join can be performed for such sets Ss , matching themwith corresponding blocks of tuples in r with the same values for the join attributes.
If either of the input relations r and s is not sorted on the join attributes, they can be sorted first, and then themerge-join algorithm can be used.
Themerge-join algorithm can also be easily extended from natural joins to the more general case of equi-joins.
Once the relations are in sorted order, tuples with the same value on the join attributes are in consecutive order.
Thereby, each tuple in the sorted order needs to be read only once, and, as a result, each block is also read only once.
Since it makes only a single pass through both files (assuming all sets Ss fit in memory) the merge-join method is efficient; the number of block transfers is equal to the sum of the number of blocks in both files, br + bs.
If either of the input relations r and s is not sorted on the join attributes, they must be sorted first; the cost of sorting must then be added to the above costs.
If some some sets Ss do not fit in memory, the cost would increase slightly.
Suppose the relations are not sorted, and the memory size is the worst case, only three blocks.
With a memory size of 25 blocks, and the relations not sorted, the cost of sorting followed by merge join would be as follows:
It is possible to perform a variation of the merge-join operation on unsorted tuples, if secondary indices exist on both join attributes.
The algorithm scans the records through the indices, resulting in their being retrieved in sorted order.
Hence, each tuple access could involve accessing a disk block, and that is costly.
To avoid this cost, we can use a hybrid merge-join technique that combines indices with merge join.
Suppose that one of the relations is sorted; the other is unsorted, but has a secondary B+-tree index on the join attributes.
The hybrid merge-join algorithm merges the sorted relation with the leaf entries of the secondary B+-tree index.
The result file contains tuples from the sorted relation and addresses for tuples of the unsorted relation.
The result file is then sorted on the addresses of tuples of the unsorted relation, allowing efficient retrieval of the corresponding tuples, in physical storage order, to complete the join.
Extensions of the technique to handle two unsorted relations are left as an exercise for you.
Like the merge-join algorithm, the hash-join algorithm can be used to implement natural joins and equi-joins.
In the hash-join algorithm, a hash function h is used to partition tuples of both relations.
The basic idea is to partition the tuples of each of the relations into sets that have the same hash value on the join attributes.
The idea behind the hash-join algorithm is this: Suppose that an r tuple and an s tuple satisfy the join condition; then, they have the same value for the join attributes.
If that value is hashed to some value i , the r tuple has to be in ri and the s tuple in si.
Therefore, r tuples in ri need only to be compared with s tuples in si ; they do not need to be compared with s tuples in any other partition.
For example, if d is a tuple in student, c a tuple in takes, and h a hash function on the ID attributes of the tuples, then d and cmust be tested only if h(c) = h(d)
If h(c) = h(d), then c and dmust have different values for ID.
However, if h(c) = h(d), wemust test c and d to see whether the values in their join attributes are the same, since it is possible that c and d have different iids that have the same hash value.
The hash index on si is built in memory, so there is no need to access the disk to retrieve the tuples.
The hash function used to build this hash index must be different from the hash function h used earlier, but is still applied to only the join.
Partition s */ for each tuple ts in s do begin.
In the course of the indexed nested-loop join, the system uses this hash index to retrieve records that match records in the probe input.
The build and probe phases require only a single pass through both the build and probe inputs.
It is straightforward to extend the hash-join algorithm to compute general equi-joins.
If the value of nh is greater than or equal to the number of blocks of memory, the relations cannot be partitioned in one pass, since there will not be enough buffer blocks.
In one pass, the input can be split into at most as many partitions as there are blocks available for use as output buffers.
Each bucket generated by one pass is separately read in and.
The hash function used in a pass is, of course, different from the one used in the previous pass.
The system repeats this splitting of the input until each partition of the build input fits in memory.
Hash-table overflow occurs in partition i of the build relation s if the hash index on si is larger than main memory.
Hash-table overflow can occur if there are many tuples in the build relation with the same values for the join attributes, or if the hash function does not have the properties of randomness and uniformity.
In either case, some of the partitions will have more tuples than the average, whereas others will have fewer; partitioning is then said to be skewed.
We can handle a small amount of skew by increasing the number of partitions so that the expected size of each partition (including the hash index on the partition) is somewhat less than the size of memory.
Even if, by using a fudge factor, we are conservative on the sizes of the partitions, overflows can still occur.
Overflow resolution proceeds in this way: If si , for any i , is found to be too large, it is further partitioned into smaller partitions by using a different hash function.
Similarly, ri is also partitioned using the new hash function, and only tuples in the matching partitions need to be joined.
In contrast, overflow avoidance performs the partitioning carefully, so that overflows never occur during the build phase.
In overflow avoidance, the build relation s is initially partitioned into many small partitions, and then some partitions are combined in such a way that each combined partition fits in memory.
The probe relation r is partitioned in the same way as the combined partitions on s, but the sizes of ri do not matter.
If a large number of tuples in s have the same value for the join attributes, the resolution and avoidance techniques may fail on some partitions.
In that case, instead of creating an in-memory hash index and using a nested-loop join to join the partitions, we can use other join techniques, such as block nested-loop join, on those partitions.
First, consider the case where recursive partitioning is not required.
The partitioning of the two relations r and s calls for a complete reading of both relations, and a subsequent writing back of them.
This operation requires 2(br + bs) block transfers, where br and bs denote the number of blocks containing records of relations r and s, respectively.
The build and probe phases read each of the partitions once, calling for further br + bs block transfers.
The number of blocks occupied by partitions could be slightlymore than br +bs , as a result of partially filled blocks.
Accessing such partially filled blocks can add an overhead of at most 2nh for each of the relations, since each of the nh partitions could have a partially filled block that has to be written and read back.
The overhead 4nh is usually quite small compared to br + bs , and can be ignored.
Again assuming bb blocks are allocated for buffering each partition, and ignoring the relatively small number of seeks during the build and probe phase, hash join with recursive partitioning requires:
The hash join can be improved if the main-memory size is large.
When the entire build input can be kept in main memory, nh can be set to 0; then, the hashjoin algorithm executes quickly, without partitioning the relations into temporary files, regardless of the probe input’s size.
The cost estimate goes down to br + bs block transfers and two seeks.
After they are used for probing, the tuples can be discarded, so the partition r0 does not occupy any memory space.
The system writes out tuples in the other partitions as usual, and joins them later.
The savings of hybrid hash join can be significant if the build input is only slightly bigger than memory.
Then, the hybrid hash-join algorithm is useful if the size of memory is significantly more than 20 megabytes; memory sizes of gigabytes or more are common on computers today.
Nested-loop and block nested-loop joins can be used regardless of the join conditions.
The other join techniques are more efficient than the nested-loop join and its variants, but can handle only simple join conditions, such as natural joins or equi-joins.
We can implement joins with complex join conditions, such as conjunctions and disjunctions, by using the efficient join techniques, if we apply the techniques developed in Section 12.3.3 for handling complex selections.
Section 12.6 describes algorithms for computing the union of relations.
Identical tuples will appear adjacent to each other as a result of sorting, and all but one copy can be removed.With external sort–merge, duplicates foundwhile a run is being created can be removed before the run is written to disk, thereby reducing the number of block transfers.
The worst-case cost estimate for duplicate elimination is the same as the worst-case cost estimate for sorting of the relation.
We can also implement duplicate elimination by hashing, as in the hash-join algorithm.
First, the relation is partitioned on the basis of a hash function on the whole tuple.
Then, each partition is read in, and an in-memory hash index is constructed.
While constructing the hash index, a tuple is inserted only if it is not already present.
After all tuples in the partition have been processed, the tuples in the hash index arewritten to the result.
The cost estimate is the same as that for the cost of processing (partitioning and reading each partition) of the build relation in a hash join.
Because of the relatively high cost of duplicate elimination, SQL requires an explicit request by the user to remove duplicates; otherwise, the duplicates are retained.
Duplicates can be eliminated by the methods described in Section 12.6.1
If the attributes in the projection list include a key of the relation, no duplicates will exist; hence, duplicate elimination is not required.
Generalized projection can be implemented in the same way as projection.
For all these operations, only one scan of the two sorted input relations is required, so the cost is br + bs block transfers if the relations are sorted in the same order.
Assuming a worst case of one block buffer for each relation, a total of br + bs disk seeks would be required in addition to br + bs block transfers.
The number of seeks can be reduced by allocating extra buffer blocks.
If the relations are not sorted initially, the cost of sorting has to be included.
Any sort order can be used in evaluation of set operations, provided that both inputs have that same sort order.
Add the tuples in si to the hash index only if they are not alreadypresent.
Add the tuples in the hash index to the result.
For each tuple in si , probe the hash index and output the tuple to the result only if it is already present in the hash index.
For each tuple in si , probe the hash index, and, if the tuple is present in the hash index, delete it from the hash index.
Add the tuples remaining in the hash index to the result.
We can implement the outer-join operations by using one of two strategies:
It is easy to extend the nested-loop join algorithms to compute the left outer join: Tuples in the outer relation that do not match any tuple in the inner relation are written to the output after being padded with null values.
However, it is hard to extend the nested-loop join to compute the full outer join.
Natural outer joins and outer joins with an equi-join condition can be computed by extensions of the merge-join and hash-join algorithms.
Merge join can be extended to compute the full outer join as follows: When the merge of the two relations is being done, tuples in either relation that do not match any tuple in the other relation can be padded with nulls and written to the output.
Similarly, we can extend merge join to compute the left and right outer joins by writing out nonmatching tuples (padded with nulls) from only one of the relations.
Since the relations are sorted, it is easy to detect whether or not a tuple matches any tuples from the other relation.
For example, when a merge join of takes and student is done, the tuples are read in sorted order of ID, and it is easy to check, for each tuple, whether there is a matching tuple in the other.
The cost estimates for implementing outer joins using the merge-join algorithm are the same as are those for the corresponding join.
The only difference lies in size of the result, and therefore in the block transfers for writing it out, which we did not count in our earlier cost estimates.
The extension of the hash-join algorithm to compute outer joins is left for.
The aggregation operation can be implemented in the same way as duplicate.
However, instead of eliminating tuples with the same value for the grouping attribute, we gather them into groups, and apply the aggregation operations on each group to get the result.
The cost estimate for implementing the aggregation operation is the same as the cost of duplicate elimination, for aggregate functions such asmin,max, sum, count, and avg.
Instead of gathering all the tuples in a group and then applying the aggregation operations, we can implement the aggregation operations sum,min,max, count, and avg on the fly as the groups are being constructed.
For the case of sum, min, and max, when two tuples in the same group are found, the system replaces them by a single tuple containing the sum, min, or max, respectively, of the columns being aggregated.
For the count operation, it maintains a running count for each group for which a tuple has been found.
If all tuples of the result fit inmemory, both the sort-based and the hash-based implementations do not need to write any tuples to disk.
As the tuples are read in, they can be inserted in a sorted tree structure or in a hash index.
When we use on-the-fly aggregation techniques, only one tuple needs to be stored for each of the groups.
So far, we have studied how individual relational operations are carried out.
Now we consider how to evaluate an expression containing multiple operations.
The obvious way to evaluate an expression is simply to evaluate one operation at a time, in an appropriate order.
The result of each evaluation is materialized in a temporary relation for subsequent use.
A disadvantage to this approach is the need to construct the temporary relations, which (unless they are small) must be written to disk.
An alternative approach is to evaluate several operations simultaneously in a pipeline, with the results of one operation passed on to the next, without the need to store a temporary relation.
We shall see that the costs of these approaches can differ substantially, but also that there are cases where only the materialization approach is feasible.
It is easiest to understand intuitively how to evaluate an expression by looking at a pictorial representation of the expression in an operator tree.
If we apply the materialization approach, we start from the lowest-level operations in the expression (at the bottom of the tree)
In our example, there is only one such operation: the selection operation on department.
The inputs to the lowest-level operations are relations in the database.
We execute these operations by the algorithms that we studied earlier, and we store the results in temporary relations.
We can use these temporary relations to execute the operations at the next level up in the tree, where the inputs now are either temporary relations or relations stored in the database.
In our example, the inputs to the join are the instructor relation and the temporary relation created by the selection on department.
The join can now be evaluated, creating another temporary relation.
By repeating the process, wewill eventually evaluate the operation at the root of the tree, giving the final result of the expression.
In our example, we get the final result by executing the projection operation at the root of the tree, using as input the temporary relation created by the join.
Evaluation as just described is called materialized evaluation, since the results of each intermediate operation are created (materialized) and then are used for evaluation of the next-level operations.
Double buffering (using two buffers, with one continuing execution of the algorithm while the other is being written out) allows the algorithm to execute more quickly by performing CPU activity in parallel with I/O activity.
The number of seeks can be reduced by allocating extra blocks to the output buffer, andwriting out multiple blocks at once.
We can improve query-evaluation efficiency by reducing the number of temporary files that are produced.
It eliminates the cost of reading and writing temporary relations, reducing the cost of query evaluation.
It can start generating query results quickly, if the root operator of a queryevaluation plan is combined in a pipeline with its inputs.
This can be quite useful if the results are displayed to a user as they are generated, since otherwise there may be a long delay before the user sees any query results.
In the example of Figure 12.11, all three operations can be placed in a pipeline, which passes the results of the selection to the join as they are generated.
In turn, it passes the results of the join to the projection as they are generated.
The memory requirements are low, since results of an operation are not stored for long.
However, as a result of pipelining, the inputs to the operations are not available all at once for processing.
Each time that an operation receives a request for tuples, it computes the next tuple (or tuples) to be returned, and then returns that tuple.
If the inputs of the operation are not pipelined, the next tuple(s) to be returned can be computed from the input relations, while the system keeps track of what has been returned so far.
If it has somepipelined inputs, the operation alsomakes requests for tuples from its pipelined inputs.
Using the tuples received from its pipelined inputs, the operation computes tuples for its output, and passes them up to its parent.
In a producer-driven pipeline, operations do not wait for requests to produce tuples, but instead generate the tuples eagerly.
Each operation in a producer-driven pipeline is modeled as a separate process or thread within.
We describe below how demand-driven and producer-driven pipelines can be implemented.
Each operation in a demand-driven pipeline can be implemented as an iterator that provides the following functions: open(), next(), and close()
After a call to open(), each call to next() returns the next output tuple of the operation.
The implementation of the operation in turn calls open() and next() on its inputs, to get its input tuples when required.
The function close() tells an iterator that no more tuples are required.
The iterator maintains the state of its execution in between calls, so that successive next() requests receive successive result tuples.
For example, for an iterator implementing the select operation using linear search, the open() operation starts a file scan, and the iterator’s state records the point towhich the file has been scanned.When the next() function is called, the file scan continues from after the previous point; when the next tuple satisfying the selection is found by scanning the file, the tuple is returned after storing the point where it was found in the iterator state.
A merge-join iterator’s open() operation would open its inputs, and if they are not already sorted, it would also sort the inputs.
On calls to next(), it would return the next pair of matching tuples.
The state information would consist of up to where each input had been scanned.
Details of the implementation of iterators are left for you to complete in Practice Exercise 12.7
Producer-driven pipelines, on the other hand, are implemented in a different manner.
For each pair of adjacent operations in a producer-driven pipeline, the system creates a buffer to hold tuples being passed from one operation to the next.
The processes or threads corresponding to different operations execute concurrently.
Each operation at the bottom of a pipeline continually generates output tuples, and puts them in its output buffer, until the buffer is full.
An operation at any other level of a pipeline generates output tuples when it gets input tuples from lower down in the pipeline, until its output buffer is full.
Once the operation uses a tuple from a pipelined input, it removes the tuple from its input buffer.
In either case, once the output buffer is full, the operation waits until its parent operation removes tuples from the buffer, so that the buffer has space formore tuples.
At this point, the operation generatesmore tuples, until the buffer is full again.
The operation repeats this process until all the output tuples have been generated.
It is necessary for the system to switch between operations only when an output buffer is full, or an input buffer is empty andmore input tuples are needed to generate any more output tuples.
Using producer-driven pipelining can be thought of as pushing data up an operation tree from below, whereas using demand-driven pipelining can be thought of as pulling data up an operation tree from the top.
Whereas tuples are generated eagerly in producer-driven pipelining, they are generated lazily, on demand, in demand-driven pipelining.
However, producer-driven pipelining is very useful in parallel processing systems.
Some operations, such as sorting, are inherently blocking operations, that is, they may not be able to output any results until all tuples from their inputs have been examined.5
Other operations, such as join, are not inherently blocking, but specific evaluation algorithms may be blocking.
For example, the hash-join algorithm is a blocking operation, since it requires both its inputs to be fully retrieved and partitioned, before it outputs any tuples.
On the other hand, the indexed nested loops join algorithm can output result tuples as it gets tuples for the outer relation.
It is therefore said to be pipelined on its outer (left-hand side) relation, although it is blocking on its indexed (right-hand side) input, since the index must be fully constructed before the indexed nested-loop join algorithm can execute.
Hybrid hash join can be viewed as partially pipelined on the probe relation, since it can output tuples from the first partition as tuples are received for the probe relation.
However, tuples that are not in the first partition will be output only after the entire pipelined input relation is received.
Hybrid hash join thus provides pipelined evaluation on its probe input if the build input fits entirely in memory, or nearly pipelined evaluation if most of the build input fits in memory.
If both inputs are sorted on the join attribute, and the join condition is an equi-join, merge join can be used, with both its inputs pipelined.
However, in the more common case that the two inputs that we desire to pipeline into the join are not already sorted, another alternative is the doublepipelined join technique, shown in Figure 12.12
The algorithm assumes that the input tuples for both input relations, r and s, are pipelined.
Tuplesmade available for both relations are queued for processing in a single queue.
Special queue entries, called Endr and Ends , which serve as end-of-file markers, are inserted in the queue after all tuples from r and s (respectively) have been generated.
For efficient evaluation, appropriate indices should be built on the relations r and s.
As tuples are added to r and s, the indices must be kept up to date.
When hash indices are used on r and s, the resultant algorithm is called the double-pipelined hash-join technique.
The double-pipelined join algorithm in Figure 12.12 assumes that both inputs fit in memory.
In case the two inputs are larger than memory, it is still possible to use the double-pipelined join technique as usual until available memory is full.
Blocking operations such as sorting may be able to output tuples early if the input is known to satisfy some special properties such as being sorted, or partially sorted, already.
However, in the absence of such information, blocking operations cannot output tuples early.
The first action that the system must perform on a query is to translate the query into its internal form,which (for relational database systems) is usually based on the relational algebra.
In the process of generating the internal form of the query, the parser checks the syntax of the user’s query, verifies that the relation names appearing in the query are names of relations in the database, and so on.
If the query was expressed in terms of a view, the parser replaces all references to the view name with the relational-algebra expression to compute the view.
Given a query, there are generally a variety of methods for computing the answer.
It is the responsibility of the query optimizer to transform the query.
We can process simple selection operations by performing a linear scan, or by making use of indices.
We can handle complex selections by computing unions and intersections of the results of simple selections.
We can sort relations larger than memory by the external sort–merge algorithm.
Queries involving anatural joinmaybeprocessed in severalways, depending on the availability of indices and the formof physical storage for the relations.
If the join result is almost as large as the Cartesian product of the two relations, a block nested-loop join strategy may be advantageous.
The hash-join algorithm partitions the relations into several pieces, such that each piece of one of the relations fits in memory.
The partitioning is carried out with a hash function on the join attributes, so that corresponding pairs of partitions can be joined independently.
Duplicate elimination, projection, set operations (union, intersection, and difference), and aggregation can be done by sorting or by hashing.
Outer-join operations can be implemented by simple extensions of join algorithms.
Hashing and sorting are dual, in the sense that any operation such as duplicate elimination, projection, aggregation, join, and outer join that can be implemented by hashing can also be implemented by sorting, and vice versa; that is, any operation that can be implemented by sorting can also be implemented by hashing.
An expression can be evaluated by means of materialization, where the system computes the result of each subexpression and stores it on disk, and then uses it to compute the result of the parent expression.
Pipelining helps to avoid writing the results of many subexpressions to disk, by using the results in the parent expression even as they are being generated.
Write an efficient relational-algebra expression that is equivalent to this query.
Why is it inefficient? Describe a way, using sorting, to reduce the cost of retrieving tuples of the inner relation.
Under what conditions would this algorithm be more efficient than hybrid merge join?
Suppose that a B+-tree index on branch city is available on relation branch, and that no other index is available.
List different ways to handle the following selections that involve negation:
Show what state information the iterator must maintain between calls.
Describe how to implement each operation using sorting, and using hashing.
Describe how to extend the hash-join algorithm to compute the natural left outer join, the natural right outer join and the natural full outer join.
Hint: Keep extra informationwith each tuple in the hash index, to detect whether any tuple in the probe relation matches the tuple in the hash index.
Try out your algorithm on the takes and student relations.
Suppose you need to sort relation r using sort–merge and merge-join the result with an already sorted relation s.
Describe how the output of the sort of r can be pipelined to the merge join without being written back to disk.
The same idea is applicable even if both inputs to the merge join are the outputs of sort–merge operations.
However, the available memory has to be shared between the two merge operations (the merge-join algorithm itself needs very little memory)
What is the effect of having to share memory on the cost of each sort–merge operation?
Your pseudocode must define the standard iterator functions open(), next(), and close()
Show what state information the iterator must maintain between calls.
Describe how to compute these together using a single sorting of r.
A query processor must parse statements in the query language, and must translate them into an internal form.
Parsing of query languages differs little fromparsing of traditional programming languages.
Graefe andMcKenna [1993b] presents an excellent survey of query-evaluation techniques.
Knuth [1973] presents an excellent description of external sorting algorithms, including an optimization called replacement selection, which can create initial runs that are (on the average) twice the size of memory.
According to performance studies conducted in the mid-1970s, database systems of that period used only nested-loop join and merge join.
These studies, including Blasgen and Eswaran [1976], which was related to the development of System R, determined that either the nested-loop join or merge join nearly always provided the optimal join method.
Hence, these two were the only join algorithms implemented in System R.
However, Blasgen and Eswaran [1976] did not include an analysis of hash-join algorithms.
Today, hash joins are considered to be highly efficient and widely used.
Query optimization is the process of selecting themost efficient query-evaluation plan from among the many strategies usually possible for processing a given query, especially if the query is complex.
We do not expect users to write their queries so that they can be processed efficiently.
Rather, we expect the system to construct a query-evaluation plan that minimizes the cost of query evaluation.
One aspect of optimization occurs at the relational-algebra level, where the system attempts to find an expression that is equivalent to the given expression, but more efficient to execute.
Another aspect is selecting a detailed strategy for processing the query, such as choosing the algorithm to use for executing an operation, choosing the specific indices to use, and so on.
The difference in cost (in terms of evaluation time) between a good strategy and a bad strategy is often substantial, and may be several orders of magnitude.
Hence, it is worthwhile for the system to spend a substantial amount of time on the selection of a good strategy for processing a query, even if the query is executed only once.
Consider the following relational-algebra expression, for the query “Find the names of all instructors in the Music department together with the course title of all the courses that the instructors teach.”
Note that the projection of course on (course id,title) is required since course shares an attribute dept namewith instructor; if we did not remove this attribute using the projection, the above expression using natural joins would return only courses from the Music department, even if some Music department instructors taught courses in other departments.
Since we are concerned with only those tuples in the instructor relation that pertain to the Music department, we do not need to consider those tuples that do not have dept name = “Music”
By reducing the number of tuples of the instructor relation that we need to access, we reduce the size of the intermediate result.
An evaluation plan defines exactly what algorithm should be used for each operation, and how the execution of the operations should be coordinated.
As we have seen, several different algorithms can be used for each relational operation, giving rise to alternative evaluation plans.
In the figure, hash join has been chosen for one of the join operations, while the other uses merge join, after sorting the relations on the join attribute, which is ID.
Where edges are marked as pipelined, the output of the producer is pipelined directly to the consumer, without being written out to disk.
Given a relational-algebra expression, it is the job of the query optimizer to come upwith a query-evaluation plan that computes the same result as the given expression, and is the least-costly way of generating the result (or, at least, is not much costlier than the least-costly way)
To find the least-costly query-evaluation plan, the optimizer needs to generate alternative plans that produce the same result as the given expression, and to choose the least-costly one.
As evaluation plans are generated, their costs are estimated by using statistical information about the relations, such as relation sizes and index depths.
To implement the first step, the query optimizer must generate expressions equivalent to a given expression.
It does so bymeans of equivalence rules that specify how to transform an expression into a logically equivalent one.
In Section 13.3 we describe how to estimate statistics of the results of each operation in a query plan.
Using these statistics with the cost formulae in Chapter 12 allows us to estimate the costs of individual operations.
The individual costs are combined to determine the estimated cost of evaluating a given relational-algebra expression, as outlined earlier in Section 12.7
In Section 13.4, we describe how to choose a query-evaluation plan.
We can choose one based on the estimated cost of the plans.
Since the cost is an estimate, the selected plan is not necessarily the least-costly plan; however, as long as the estimates are good, the plan is likely to be the least-costly one, or not much more costly than it.
Finally, materialized views help to speed up processing of certain queries.
In Section 13.5, we study how to “maintain” materialized views—that is, to keep them up-to-date—and how to perform query optimization with materialized views.
Most database systems provide a way to view the evaluation plan chosen to execute a given query.
It is usually best to use the GUIprovidedwith the database system to view evaluation plans.
The estimated costs for the plan are also displayed along with the plan.
It is worth noting that the costs are usually not in any externally meaningful unit, such as seconds or I/O operations, but rather in units ofwhatever cost model the optimizer uses.
Some optimizers such as PostgreSQL display two cost-estimate numbers; the first indicates the estimated cost for outputting the first result, and the second indicates the estimated cost for outputting all results.
A query can be expressed in several different ways, with different costs of evaluation.
In this section, rather than take the relational expression as given, we consider alternative, equivalent expressions.
Two relational-algebra expressions are said to be equivalent if, on every legal database instance, the two expressions generate the same set of tuples.
Recall that a legal database instance is one that satisfies all the integrity constraints specified in the database schema.
Note that the order of the tuples is irrelevant; the two expressions may generate the tuples in different orders, but would be considered equivalent as long as the set of tuples is the same.
In SQL, the inputs and outputs aremultisets of tuples, and themultiset version of the relational algebra (described in the box in page 238) is used for evaluating SQL queries.
Two expressions in the multiset version of the relational algebra are said to be equivalent if on every legal database the two expressions generate the same multiset of tuples.
The discussion in this chapter is based on the relational.
We leave extensions to the multiset version of the relational algebra to you as exercises.
An equivalence rule says that expressions of two forms are equivalent.
We can replace an expression of the first form by an expression of the second form, or vice versa—that is, we can replace an expression of the second form by an expression of the first form—since the two expressions generate the same result on any valid database.
The optimizer uses equivalence rules to transform expressions into other logically equivalent expressions.
Recall that the natural-join operator is simply a special case of the theta-join operator; hence, natural joins are also commutative.
The selection operation distributes over the theta-join operation under the following two conditions:
More equivalences involving extended relational operators, such as the outer join and aggregation, are discussed in the exercises.
We now illustrate the use of the equivalence rules.We use our university example with the relation schemas:
We can carry out this transformation by using rule 7.a.
Remember that the rule merely says that the two expressions are equivalent; it does not say that one is better than the other.
Multiple equivalence rules can be used, one after the other, on a query or on parts of the query.
Then, using rule 7.a, we can rewrite our query as:
Using rule 1, we can break the selection into two selections, to get the following subexpression:
However, the latter form of the expression provides a new opportunity to apply Rule 7.a (“perform selections early”), resulting in the subexpression:
Figure 13.4 depicts the initial expression and the final expression after all these transformations.
A set of equivalence rules is said to beminimal if no rule can be derived from any combination of the others.
The preceding example illustrates that the set of equivalence rules in Section 13.2.1 is notminimal.
An expression equivalent to the original expression may be generated in different ways; the number of different ways of generating an expression increases when we use a nonminimal set of equivalence rules.
The only attributes that wemust retain are those that either appear in the result of the query or are needed to process subsequent operations.
In our example, the only attributes we need from the join of instructor and teaches are name and course id.
Agood ordering of join operations is important for reducing the size of temporary results; hence, most query optimizers pay a lot of attention to the join order.
Although these expressions are equivalent, the costs of computing them may differ.
There are other options to consider for evaluating our query.
We do not care about the order in which attributes appear in a join, since it is easy to change the order before displaying the result.
Query optimizers use equivalence rules to systematically generate expressions equivalent to the given query expression.
Conceptually, this can be done as outlined in Figure 13.5
Given a query expression E , the set of equivalent expressions EQ initially contains only E.
Now, each expression in EQ is matched with each equivalence rule.
If an expression, say Ei , of any subexpression ei of Ei (which could, as a special case, be Ei itself) matches one side of an equivalence rule, the optimizer generates a new expression where ei is transformed tomatch the other side of the rule.
This process continues until no more new expressions can be generated.
The preceding process is extremely costly both in space and in time, but optimizers can greatly reduce both the space and time cost, using two key ideas.
Match each expression Ei in EQwith each equivalence rule Rj if any subexpression ei of Ei matches one side of Rj.
Even ei and its transformed version usually share many identical subexpressions.
It is not always necessary to generate every expression that can be generated with the equivalence rules.
If an optimizer takes cost estimates of evaluation into account, it may be able to avoid examining some of the expressions, as we shall see in Section 13.4.We can reduce the time required for optimization by using techniques such as these.
In this section, we first list some statistics about database relations that are stored in database-system catalogs, and then show how to use the statistics to estimate statistics on the results of various relational operations.
One thing that will become clear later in this section is that the estimates are not very accurate, since they are based on assumptions that may not hold exactly.
A query-evaluation plan that has the lowest estimated execution cost may therefore not actually have the lowest actual execution cost.
However, realworld experience has shown that even if estimates are not precise, the plans with the lowest estimated costs usually have actual execution costs that are either the lowest actual execution costs, or are close to the lowest actual execution costs.
The database-system catalog stores the following statistical information about database relations:
If we assume that the tuples of relation r are stored together physically in a file, the following equation holds:
Statistics about indices, such as the heights of B+-tree indices and number of leaf pages in the indices, are also maintained in the catalog.
If we wish to maintain accurate statistics, then, every time a relation is modified, we must also update the statistics.
Therefore, most systems do not update the statistics on every modification.
Instead, they update the statistics during periods of light system load.
As a result, the statistics used for choosing a query-processing strategy may not be completely accurate.
However, if not too many updates occur in the intervals between the updates of the statistics, the statistics will be sufficiently accurate to provide a good estimation of the relative costs of the different plans.
Real-world optimizers oftenmaintain further statistical information to improve the accuracy of their cost estimates of evaluation plans.
For instance, most databases store the distribution of values for each attribute as a histogram: in a histogram the values for the attribute are divided into a number of ranges, and with each range the histogram associates thenumber of tupleswhose attribute value lies in that range.
Histograms used in database systems usually record the number of distinct values in each range, in addition to the number of tuples with attribute values in that range.
With each range we store a count of the number of person tuples whose age values lie in that range, and the number of distinct age values that lie in that.
Without such histogram information, an optimizer would have to assume that the distribution of values is uniform; that is, each range has the same count.
A histogram takes up only a little space, so histograms on several different attributes can be stored in the system catalog.
There are several types of histograms used in database systems.
For example, an equi-width histogram divides the range of values into equal-sized ranges, whereas an equi-depth histogram adjusts the boundaries of the ranges such that each range has the same number of values.
The size estimate of the result of a selection operation depends on the selection predicate.
We first consider a single equality predicate, then a single comparison predicate, and finally combinations of predicates.
If a histogram is available on attribute A, we can locate the range that contains the value a , and modify the above-mentioned estimate nr/V(A, r )
Conceptually, statistics on relations can be thought of as materialized views, which should be automatically maintained when relations are modified.
Unfortunately, keeping statistics up-to-date on every insert, delete or update to the database can be very expensive.
On the other hand, optimizers generally do not need exact statistics: an error of a few percent may result in a plan that is not quite optimal being chosen, but the alternative plan chosen is likely to have a cost which is within a few percent of the optimal cost.
Thus, it is acceptable to have statistics that are approximate.
Database systems reduce the cost of generating and maintaining statistics, as outlined below, by exploiting the fact that statistics can be approximate.
Statistics are often computed from a sample of the underlying data, instead of examining the entire collection of data.
For example, a fairly accurate histogram can be computed from a sample of a few thousand tuples, even on a relation that hasmillions, or hundreds of millions of records.
However, the sample used must be a random sample; a sample that is not random may have an excessive representation of one part of the relation, and can give misleading results.
For example, if we used a sample of instructors to compute a histogram on salaries, if the sample has an overrepresentation of lower-paid instructors the histogram would result in wrong estimates.
Database systems today routinely use random sampling to create statistics.
Statistics are not maintained on every update to the database.
They rely on database administrators periodically running a command to update statistics.
Oracle and PostgreSQL provide an SQL command called analyze that generates statistics on specified relations, or on all relations.
You should be aware that optimizers sometimes choose very bad plans due to incorrect statistics.
Many database systems, such as IBM DB2, Oracle, and SQL Server, update statistics automatically at certain points of time.
For example, the system can keep approximate track of how many tuples there are in a relation and recompute statistics if this number changes significantly.
Another approach is to compare estimated cardinalities of a relation scan with actual cardinalities when a query is executed, and if they differ significantly, initiate an update of statistics for that relation.
If a histogram is available on attribute A, we can get a more accurate estimate; we leave the details as an exercise for you.
In some cases, such as when the query is part of a stored procedure, the value v may not be available when the query is optimized.
In such cases, we assume that approximately one-half the recordswill satisfy the comparison condition.
That is, we assume the result has nr/2 tuples; the estimate may be very inaccurate, but is the best we can do without any further information.
Multiplying this value by nr gives us the estimated number of tuples that satisfy the selection.
Estimating the size of a natural join is somewhat more complicated than.
The preceding estimate of join size may be too high if the V(A, r ) values for attribute A in r have few values in common with the V(A, s) values for attribute A in s.
However, this situation is unlikely to happen in the real world, since dangling tuples either do not exist or constitute only a small fraction of the tuples, in most real-world relations.
More important, the preceding estimate depends on the assumption that each value appears with equal probability.
More sophisticated techniques for size estimation have to be used if this assumption does not hold.
For example, if we have histograms on the join attributes of both relations, and both histograms have the same ranges, then we can use the above estimation technique within each range, using the number of rows with values in the range instead of nr or ns , and the number of distinct values in that range, instead of V(A, r ) or V(A, s)
We then add up the size estimates obtained for each range to get the overall size estimate.
We leave the case where both relations have histograms on the join attribute, but the histograms have different ranges, as an exercise for you.
To illustrate all these ways of estimating join sizes, consider the expression:
V(ID, takes) = 2500, which implies that only half the students have taken.
Weoutline belowhowto estimate the sizes of the results of other relational-algebra operations.
Aggregation: The size of AGF (r ) is simply V(A, r ), since there is one tuple in AGF (r ) for each distinct value of A.
Generation of expressions is only part of the query-optimization process, since each operation in the expression can be implemented with different algorithms.
An evaluation plan defines exactly what algorithm should be used for each operation, and how the execution of the operations should be coordinated.
A cost-based optimizer explores the space of all query-evaluation plans that are equivalent to the given query, and chooses the one with the least estimated cost.
We have seen how equivalence rules can be used to generate equivalent plans.
We first cover a simpler version of cost-based optimization, which involves only join-order and join algorithm selection, in Section 13.4.1
Later in Section 13.4.2 we briefly sketch how a general-purpose optimizer based on equivalence rules can be built, without going into details.
Exploring the space of all possible plans may be too expensive for complex queries.
Most optimizers include heuristics to reduce the cost of query optimization, at the potential risk of not finding the optimal plan.
The most common type of query in SQL consists of a join of a few relations, with join predicates and selections specified in the where clause.
In this section we consider the problem of choosing the optimal join order for such a query.
For a complex join query, the number of different query plans that are equivalent to the query can be large.
Luckily, it is not necessary to generate all the expressions equivalent to a given expression.
For example, suppose we want to find the best join order of the form:
The procedure applies selections on individual relations at the earliest possible point, that is, when the relations are accessed.
It is easiest to understand the procedure assuming that all joins are natural joins, although the procedure works unchanged with any join condition.
With arbitrary join conditions, the join of two subexpressions is understood to include all join conditions that relate attributes from the two subexpressions.
If S contains only one relation, the best way of accessing S (taking selections on S, if any, into account) is recorded in bestplan.
Thismay involve using an index to identify tuples, and then fetching the tuples (often referred to as an index scan), or scanning the entire relation (often referred to as a relation scan).1 If there is any selection condition on S, other than those ensured by an index scan, a selection operation is added to the plan, to ensure all selections on S are satisfied.
Otherwise, if S contains more than one relation, the procedure tries every way of dividing S into two disjoint subsets.
For each division, the procedure recursively finds the best plans for each of the two subsets, and then computes the cost of the overall plan by using that division.2 The procedure picks the cheapest plan from among all the alternatives for dividing S into two sets.
The cheapest plan and its cost are stored in the array bestplan, and returned by the.
If an index contains all the attributes of a relation that are used in a query, it is possible to perform an index-only scan, which retrieves the required attribute values from the index, without fetching actual tuples.
Plan P2 may contain an indexed access to r , based on selection conditions on r.
To allow indexed nested loops join to be used, the index lookup using the selection condition on r would be dropped from P2; instead, the selection condition would be checked on tuples returned from the index on the join attributes of r.
Hence, it is not sufficient to find the best join order for each subset of the set of n given relations.
Instead, we have to find the best join order for each subset, for each interesting sort order of the join result for that subset.
The number of interesting sort orders is generally not large.
The cost of the extended algorithm depends on the number of interesting orders for each subset of relations; since this number has been found to be small in practice, the cost remains at O(3n)
Although both numbers still increase rapidly with n, commonly occurring joins usually have less than 10 relations, and can be handled easily.
Many optimizers follow an approach based on using heuristic transformations to handle constructs other than joins, and applying the cost-based join order selection algorithm to subexpressions involving only joins and selections.
Details of such heuristics are for the most part specific to individual optimizers, and we do not cover them.
However, heuristic transformations to handle nested queries are widely used, and are considered in more detail in Section 13.4.4
In this section, however, we outline how to create a general-purpose costbased optimizer based on equivalence rules, which can handle a wide variety of query constructs.
The benefit of using equivalence rules is that it is easy to extend the optimizer with new rules to handle different query constructs.
For example, nested queries can be represented using extended relational-algebra constructs, and transformations of nested queries can be expressed as equivalence rules.
In Section 13.2.4, we saw how an optimizer could systematically generate all expressions equivalent to the given query.
The procedure for generating equivalent expressions can be modified to generate all possible evaluation plans as follows: A new class of equivalence rules, called physical equivalence rules, is added that allows a logical operation, such as a join, to be transformed to a physical operation, such as a hash join, or a nested-loops join.
By adding such rules to the original set of equivalence rules, the procedure can generate all possible evaluation plans.
The cost estimation techniques we have seen earlier can then be used to choose the optimal (that is, the least-cost) plan.
However, the procedure shown in Section 13.2.4 is very expensive, even if we do not consider generation of evaluation plans.
A space-efficient representation of expressions that avoids making multiple copies of the same subexpressions when equivalence rules are applied.
Efficient techniques for detecting duplicate derivations of the same expression.
A form of dynamic programming based onmemoization, which stores the optimal query evaluation plan for a subexpression when it is optimized for the first time; subsequent requests to optimize the same subexpression are handled by returning the already memoized plan.
Techniques that avoid generating all possible equivalent plans, by keeping track of the cheapest plan generated for any subexpression up to any point of time, and pruning away any plan that ismore expensive than the cheapest plan found so far for that subexpression.
The details are more complex than we wish to deal with here.
This approach was pioneered by the Volcano research project, and the query optimizer of SQL Server is based on this approach.
Adrawback of cost-based optimization is the cost of optimization itself.
Although the cost of query optimization can be reduced by clever algorithms, the number of different evaluation plans for a query can be very large, and finding the optimal plan from this set requires a lot of computational effort.
Hence, optimizers use heuristics to reduce the cost of optimization.
Anexample of a heuristic rule is the following rule for transforming relationalalgebra queries:
A heuristic optimizer would use this rule without finding out whether the cost is reducedby this transformation.
In thefirst transformation example in Section13.2, the selection operation was pushed into a join.
The projection operation, like the selection operation, reduces the size of relations.
Thus, whenever we need to generate a temporary relation, it is advantageous to apply immediately any projections that are possible.
This advantage suggests a companion to the “perform selections early” heuristic:
It is usually better to perform selections earlier than projections, since selections have the potential to reduce the sizes of relations greatly, and selections enable the use of indices to access tuples.
An example similar to the one used for the selection heuristic should convince you that this heuristic does not always reduce the cost.
Most practical query optimizers have further heuristics to reduce the cost of optimization.
For example, many query optimizers, such as the System R optimizer,3 do not consider all join orders, but rather restrict the search to particular kinds of join orders.
Left-deep join orders are particularly convenient for pipelined evaluation, since the right operand is a stored relation, and thus only one input to each join is pipelined.
Figure 13.8 illustrates the difference between left-deep join trees and non-leftdeep join trees.
With the use of dynamicprogramming optimizations, the System R optimizer can find the best join order in time O(n2n)
Contrast this cost with the O(3n) time required to find the best overall join order.
The System R optimizer uses heuristics to push selections and projections down the query tree.
A heuristic approach to reduce the cost of join-order selection, which was originally used in some versions of Oracle, works roughly this way: For an n-way join, it considers n evaluation plans.
System R was one of the first implementations of SQL, and its optimizer pioneered the idea of cost-based join-order optimization.
The heuristic constructs the join order for each of the n evaluation plans by repeatedly selecting the “best” relation to join next, on the basis of a ranking of the available access paths.
Either nested-loop or sort-merge join is chosen for each of the joins, depending on the available access paths.
Finally, the heuristic chooses one of the n evaluation plans in a heuristic manner, on the basis of minimizing the number of nested-loop joins that do not have an index available on the inner relation and on the number of sort-merge joins.
Most optimizers allow a cost budget to be specified for query optimization.
The search for the optimal plan is terminated when the optimization cost budget is exceeded, and the best plan found up to that point is returned.
The budget itself may be set dynamically; for example, if a cheap plan is found for a query, the budget may be reduced, on the premise that there is no point spending a lot of time optimizing the query if the best plan found so far is already quite cheap.
On the other hand, if the best plan found so far is expensive, it makes sense to invest more time in optimization, which could result in a significant reduction in execution time.
To best exploit this idea, optimizers usually first apply cheap heuristics to find a plan, and then start full cost-based optimizationwith a budget based on the heuristically chosen plan.
Many applications execute the same query repeatedly, but with different values for the constants.
For example, a university application may repeatedly execute a query to find the courses for which a student has registered, but each time for a different studentwith a different value for the student ID.
As a heuristic, many optimizers optimize a query once, with whatever values were provided for the constants when the query was first submitted, and cache the query plan.
Whenever the query is executed again, perhaps with new values for constants, the cached query plan is reused (using new values for the constants, of course)
The optimal plan for the new constants may differ from the optimal plan for the initial values, but as a heuristic the cached plan is reused.4 Caching and reuse of query plans is referred to as plan caching.
Even with the use of heuristics, cost-based query optimization imposes a substantial overhead on query processing.
However, the added cost of cost-based query optimization is usually more than offset by the saving at query-execution time, which is dominated by slow disk accesses.
The difference in execution time between a good plan and a bad one may be huge, making query optimization essential.
The achieved saving is magnified in those applications that run on a regular basis, where a query can be optimized once, and the selected query plan can be used each time the query is executed.
The bibliographical notes give references to descriptions of the query optimizers of actual database systems.
The parameters are the variables from an outer level query that are used in the nested subquery (these variables are called correlation variables)
For instance, supposewe have the following query, to find the names of all instructors who taught a course in 2007:
Conceptually, the subquery can be viewed as a function that takes a parameter (here, instructor.ID) and returns the set of all courses taught in 2007 by instructors (with the same ID)
For the student registration query, the plan would almost certainly be the same for any student ID.
But a query that took a range of student IDs, and returned registration information for all student IDs in that range, would probably have a different optimal plan if the range is very small than if the range is large.
In the preceding example, the predicate tests if the result of the subquery evaluation is empty.
This technique for evaluating a query with a nested subquery is called correlated evaluation.
Correlated evaluation is not very efficient, since the subquery is separately evaluated for each tuple in the outer level query.
A large number of random disk I/O operations may result.
Efficient join algorithmshelp avoid expensive random I/O.Where the transformation is not possible, the optimizer keeps the subqueries as separate expressions, optimizes them separately, and then evaluates them by correlated evaluation.
As an example of transforming a nested subquery into a join, the query in the preceding example can be rewritten as:
To properly reflect SQL semantics, the number of duplicate derivations should not change because of the rewriting; the rewritten query can bemodified to ensure this property, as we shall see shortly.
In general, it may not be possible to directly move the nested subquery relations into the from clause of the outer query.
Instead, we create a temporary relation that contains the results of the nested query without the selections using correlation variables from the outer query, and join the temporary table with the outer level query.
Here, V contains all attributes that are used in selections with correlation variables in the nested subquery.
In our example, the original query would have been transformed to:
The querywe rewrote to illustrate creation of a temporary relation can be obtained by simplifying the above transformed query, assuming the number of duplicates of each tuple does not matter.
The process of replacing a nested query by a query with a join (possibly with a temporary relation) is called decorrelation.
Decorrelation is more complicated when the nested subquery uses aggregation, or when the result of the nested subquery is used to test for equality, or when the condition linking the nested subquery to the outer query is not exists, and so on.
We do not attempt to give algorithms for the general case, and instead refer you to relevant items in the bibliographical notes.
Optimization of complex nested subqueries is a difficult task, as you can infer from the above discussion, and many optimizers do only a limited amount of decorrelation.
It is best to avoid using complex nested subqueries,where possible, since we cannot be sure that the query optimizer will succeed in converting them to a form that can be evaluated efficiently.
When a view is defined, normally the database stores only the query defining the view.
In contrast, a materialized view is a view whose contents are computed and stored.
Materialized views constitute redundant data, in that their contents can be inferred from the view definition and the rest of the database contents.
However, it is much cheaper in many cases to read the contents of a materialized view than to compute the contents of the view by executing the query defining the view.
Materialized views are important for improving performance in some applications.
Consider this view, which gives the total salary in each department:
Suppose the total salary amount at a department is required frequently.
Computing the view requires reading every instructor tuple pertaining to a department, and summing up the salary amounts, which can be time-consuming.
In contrast, if the viewdefinition of the total salary amount werematerialized, the total salary amount could be found by looking up a single tuple in the materialized view.5
A problem with materialized views is that they must be kept up-to-date when the data used in the view definition changes.
For instance, if the salary value of an instructor is updated, the materialized viewwill become inconsistent with the underlying data, and it must be updated.
The task of keeping amaterialized view up-to-date with the underlying data is known as view maintenance.
Views can be maintained by manually written code: That is, every piece of code that updates the salary value can be modified to also update the total salary amount for the corresponding department.
However, this approach is error prone, since it is easy to miss some places where the salary is updated, and the materialized view will then no longer match the underlying data.
Another option for maintaining materialized views is to define triggers on insert, delete, and update of each relation in the viewdefinition.
The triggersmust modify the contents of the materialized view, to take into account the change that caused the trigger to fire.
A simplistic way of doing so is to completely recompute the materialized view on every update.
A better option is to modify only the affected parts of the materialized view, which is known as incremental view maintenance.
We describe how to perform incremental view maintenance in Section 13.5.2
Modern database systems provide more direct support for incremental view maintenance.
Database-system programmers no longer need to define triggers for view maintenance.
Instead, once a view is declared to be materialized, the database system computes the contents of the view and incrementally updates the contents when the underlying data change.
Most database systems perform immediate viewmaintenance; that is, incremental viewmaintenance is performed as soon as an update occurs, as part of the updating transaction.
Some database systems also support deferred view maintenance, where viewmaintenance is deferred to a later time; for example, updates may be collected throughout a day, and materialized views may be updated at night.
However, materialized views with deferred view maintenance may not be consistent with the underlying relations on which they are defined.
The difference may not be all that large for a medium-sized university, but in other settings the difference can be very large.
For example, if the materialized view computed total sales of each product, from a sales relation with tens of millions of tuples, the difference between computing the aggregate from the underlying data, and looking up the materialized view can be many orders of magnitude.
To understand how to maintain materialized views incrementally, we start off by considering individual operations, and then we see how to handle a complete expression.
The changes to a relation that can cause a materialized view to become outof-date are inserts, deletes, and updates.
To simplify our description, we replace updates to a tuple by deletion of the tuple followed by insertion of the updated tuple.
The changes (inserts and deletes) to a relation or expression are referred to as its differential.
Now suppose r is modified by deleting a set of tuples denoted by dr.
Deletes on s are handled in an exactly symmetric fashion.
Similarly, if r is modified by deleting a set of tuples dr , the new value of v can be computed as:
When a set of tuples dr is deleted from r , for each tuple t in dr we do the following: Let t.A denote the projection of t on the attribute A.
If the count becomes 0, (t.A) is deleted from the materialized view.
The aggregate operations in SQL are count, sum, avg, min, andmax:
When a set of tuples ir is inserted into r , for each tuple t in ir we do the following: We look for the group t.A in the materialized view.
If it is not present, we add (t.A, 1) to the materialized view.
If the group t.A is present, we add 1 to the count of the group.
When a set of tuples dr is deleted from r , for each tuple t in dr we do the following: We look for the group t.A in the materialized view, and subtract 1 from the count for the group.
If the count becomes 0, we delete the tuple for the group t.A from the materialized view.
When a set of tuples ir is inserted into r , for each tuple t in ir we do.
If it is not present, we add (t.A, t.B) to the materialized view; in addition, we store a count of 1 associated with (t.A, t.B), just as we did for projection.
If the group t.A is present, we add the value of t.B to the aggregate value for the group, and add 1 to the count of the group.
When a set of tuples dr is deleted from r , for each tuple t in dr we do the following: We look for the group t.A in the materialized view, and subtract t.B from the aggregate value for the group.
Without keeping the extra count value,wewould not be able to distinguish a case where the sum for a group is 0 from the case where the last tuple in a group is deleted.
Directly updating the average on an insert or delete is not possible, since.
Instead, to handle the case of avg, we maintain the sum and count aggregate values as described earlier, and compute the average as the sum divided by the count.
Maintaining the aggregate values min and max on deletions may be more expensive.
For example, if the tuple corresponding to the minimum value for a group is deleted from r , we have to look at the other tuples of r that are in the same group to find the new minimum value.
Outer joins are handled in much the same way as joins, but with some extra work.
In the case of deletion from r we have to handle tuples in s that no longer match any tuple in r.
In the case of insertion to r , we have to handle tuples in s that did not match any tuple in r.
So far we have seen how to update incrementally the result of a single operation.
To handle an entire expression, we can derive expressions for computing the incremental change to the result of each subexpression, starting from the smallest subexpressions.
See the bibliographical notes for further details on incremental view maintenance with expressions.
Query optimization can be performed by treating materialized views just like regular relations.
The bibliographical notes give pointers to research showing how to efficiently perform query optimization with materialized views.
Another related optimization problem is that of materialized view selection, namely, “What is the best set of views to materialize?” This decision must be made on the basis of the system workload, which is a sequence of queries and updates that reflects the typical load on the system.
One simple criterion would be to select a set of materialized views that minimizes the overall execution time of the workload of queries and updates, including the time taken to maintain the materialized views.
Database administrators usually modify this criterion to take into account the importance of different queries and updates: Fast response may be required for some queries and updates, but a slow responsemay be acceptable for others.
Indices are just like materialized views, in that they too are derived data, can speed up queries, and may slow down updates.
Thus, the problem of index selection is closely related to that of materialized view selection, although it is simpler.
Most database systems provide tools to help the database administrator with index and materialized view selection.
These tools examine the history of queries and updates, and suggest indices and views to bematerialized.
There are a number of opportunities for optimizing queries, beyond those we have seen so far.
Many queries fetch results sorted on some attributes, and require only the top K results for some K.
For example, some databases support a limit K clause which results in only the top K results being returned by the query.
In other cases, the query may not specify such a limit, but the optimizer may allow a hint to be specified, indicating that only the top K results of the query are likely to be retrieved, even if the query generates more results.
When K is small, a query optimization plan that generates the entire set of results, then sorts and generates the top K , is very inefficient since it discards most of the intermediate results that it computes.
Several techniques have been proposed to optimize such top-K queries.
One approach is to use pipelined plans that can generate the results in sorted order.
Another approach is to estimatewhat is the highest value on the sorted attributes that will appear in the top-K output, and introduce selection predicates that eliminate larger values.
If extra tuples beyond the top-K are generated they are discarded, and if too few tuples are generated then the selection condition is changed and the query is re-executed.
See the bibliographical notes for references to work on top-K optimization.
When queries are generated through views, sometimes more relations are joined than are needed for computation of the query.
For example, a view v may include the join of instructor and department, but a use of the view v may use only attributes from instructor.
The join attribute dept name of instructor is a foreign key referencing department.
Assuming that instructor.dept name has been declared not null, the join with department can be dropped, with no impact on the query.
For, under the above assumption, the join with department does not eliminate any tuples from instructor, nor does it result in extra copies of any instructor tuple.
Dropping a relation from a join as above is an example of join minimization.
In fact, join minimization can be performed in other situations as well.
The problem can be avoided by executing the queries defining theupdate first, creating a list of affected tuples, andupdating the tuples and indices as the last step.
However, breaking up the execution plan in such a fashion increases the execution cost.
Update plans can be optimized by checking if the Halloween problem can occur, and if it cannot occur, updates can be performedwhile the query is being processed, reducing the update overheads.
For example, the Halloween problem cannot occur if the update does not affect index attributes.
Even if it does, if the updates decrease the value, while the index is scanned in increasing order, updated tuples will not be encountered again during the scan.
In such cases, the index can be updated even while the query is being executed, reducing the overall cost.
Update queries that result in a large number of updates can also be optimized by collecting the updates as a batch, and then applying the batch of updates separately to each affected index.
When applying the batch of updates to an index, the batch is first sorted in the index order for that index; such sorting can greatly reduce the amount of random I/O required for updating indices.
Such optimizations of updates are implemented in most database systems.
When a batch of queries are submitted together, a query optimizer can potentially exploit common subexpressions between the different queries, evaluating them once and reusing them where required.
Complex queries may in fact have subexpressions repeated in different parts of the query, which can be similarly exploited, to reduce query evaluation cost.
Common subexpression elimination optimizes subexpressions shared by different expressions in a program, by computing and storing the result, and reusing it wherever the subexpression occurs.
Exploiting common subexpressions among evaluation plans chosen for each of a batch of queries is just as useful in database query evaluation, and is implemented by some databases.
However, multiquery optimization can do even better in some cases: A query typically has more than one evaluation plan, and a judiciously chosen set of query evaluation plans for the queries may provide for a greater sharing and lesser cost than that afforded by choosing the lowest cost evaluation plan for each query.
More details on multiquery optimization may be found in references cited in the bibliographical notes.
Sharing of relation scans between queries is another limited form of multiquery optimization that is implemented in some databases.
The shared-scan optimization works as follows: Instead of reading the relation repeatedly from disk, once for each query that needs to scan a relation, data are read once from disk, and pipelined to each of the queries.
The shared-scan optimization is particularly useful when multiple queries perform a scan on a single large relation (typically a “fact table”)
Plan caching, which we saw earlier in Section 13.4.3, is used as a heuristic in many databases.
Recall that with plan caching, if a query is invoked with some constants, the plan chosen by the optimizer is cached, and reused if the query is submitted again, even if the constants in the query are different.
For example, suppose a query takes a department name as a parameter, and retrieves all courses of the department.
With plan caching, a plan chosen when the query is executed for the first time, say for the Music department, is reused if the query is executed for any other department.
Such reuse of plans by plan caching is reasonable if the optimal query plan is not significantly affected by the exact value of the constants in the query.However, if the plan is affected by the value of the constants, parametric query optimization is an alternative.
In parametric query optimization, a query is optimized without being provided specific values for its parameters, for example, dept name in the preceding example.
The optimizer then outputs several plans, each optimal for a different parameter value.
A plan would be output by the optimizer only if it is optimal for some possible value of the parameters.
The set of alternative plans output by the optimizer are stored.
When a query is submitted with specific values for its parameters, instead of performing a full optimization, the cheapest plan from the set of alternative plans computed earlier is used.
Finding the cheapest such plan usually takes much less time than reoptimization.
See the bibliographical notes for references on parametric query optimization.
Given a query, there are generally a variety of methods for computing the answer.
It is the responsibility of the system to transform the query as entered by the user into an equivalent query that can be computed more efficiently.
The process of finding a good strategy for processing a query is called query optimization.
The evaluation of complex queries involves many accesses to disk.
Since the transfer of data from disk is slow relative to the speed of main memory and the CPU of the computer system, it is worthwhile to allocate a considerable amount of processing to choose a method that minimizes disk accesses.
There are a number of equivalence rules that we can use to transform an expression into an equivalent one.We use these rules to generate systematically all expressions equivalent to the given query.
The first step in selecting a query-processing strategy is to find a relational-algebra expression that is equivalent to the given expression and is estimated to cost less to execute.
The strategy that the database system chooses for evaluating an operation depends on the size of each relation and on the distribution of values within columns.
So that they can base the strategy choice on reliable information, database systems may store statistics for each relation r.
Most database systems use histograms to store the number of values for an attribute within each of several ranges of values.
These statistics allow us to estimate the sizes of the results of various operations, as well as the cost of executing the operations.
Statistical information about relations is particularly useful when several indices are available to assist in the processing of a query.
The presence of these structures has a significant influence on the choice of a query-processing strategy.
Alternative evaluation plans for each expression can be generated by equivalence rules, and the cheapest plan across all expressions can be chosen.
Several optimization techniques are available to reduce the number of alternative expressions and plans that need to be generated.
We use heuristics to reduce the number of plans considered, and thereby to reduce the cost of optimization.
Heuristic rules for transforming relationalalgebra queries include “Perform selection operations as early as possible,” “Perform projections early,” and “Avoid Cartesian products.”
Materialized views can be used to speed up query processing.
Incremental view maintenance is needed to efficiently update materialized views when the underlying relations are modified.
The differential of an operation can be computed by means of algebraic expressions involving differentials of the inputs of the operation.
Other issues related to materialized views include how to optimize queries by making use of available materialized views, and how to select views to be materialized.
A number of advanced optimization techniques have been proposed such as top-K optimization, join minimization, optimization of updates, multiquery optimization, and parametric query optimization.
Explain how you can apply them to improve the efficiency of certain queries:
In the preceding expressions, if both occurrences of max were replaced by minwould the expressions be equivalent?
Findoutwhatplan is generated for this queryon thedatabase system you use.
Some database systems would use a (block) nested-loop join for this query, which can be very inefficient.
Briefly explain how hash-join or merge-join can be used for this query.
How can the above conditions be relaxed if agg is one ofmin ormax?
Suppose you are given a query that computes the natural join of a set of relations S.
If youwish, you can derive the formula for the number of complete binary trees with n nodes from the formula for the number of binary trees with n nodes.
This number is known as the Catalan number, and its derivation can be found in any standard textbook on data structures or algorithms.
Assume that you can store and look up information about a set of relations (such as the optimal join order for the set, and the cost of that join order) in constant time.
If you find this exercise difficult, at least show the looser time bound of O(22n)
Write a nested query on the relation account to find, for each branch with name starting with B, all accounts with the maximum balance at the branch.
Rewrite the preceding query, without using a nested subquery; in other words, decorrelate the query.
Give a procedure (similar to that described in Section 13.4.4) for decorrelating such queries.
The multiset version of the semijoin operation returns the same set of tuples, but each tuple has exactly as many copies as it had in r.
The semijoin operation is widely used for decorrelation of nested queries.
What would be the best way to handle the following selection?
Show using an example that the two expressions are not equivalent in general.
Show how to decorrelate the above query using the multiset version of the semijoin operation, defined in Exercise 13.14
When the join is on a foreign key of r referencing s, where the foreign key attribute is declared to be not null.
Give an example of a query that can be answered by using the index only, without looking at the tuples in the relation.
Query plans that use only the index, without accessing the actual relation, are called index-only plans.
Give a simple sufficient condition onU thatwill ensure that theHalloween problem cannot occur, regardless of the execution plan chosen, or the indices that exist.
Estimation of statistics of query results, such as result size, is addressed by Ioannidis and Poosala [1995], Poosala et al.
Nonuniform distributions of values cause problems for estimation of query size and cost.
Cost-estimation techniques that use histograms of value distributions have been proposed to tackle the problem.
The use of random sampling for constructing histograms is well known in statistics, but issues in histogram construction in the context of databases is discussed in Chaudhuri et al.
Klug [1982] was an early work on optimization of relational-algebra expressions with aggregate functions.
Optimization of top-K queries is addressed in Carey and Kossmann [1998] and Bruno et al.
Query optimization in the presence ofmaterialized views is addressed by Chaudhuri et al.
Index selection and materialized view selection are addressed by Ross et al.
Optimization of top-K queries is addressed in Carey and Kossmann [1998] and Bruno et al.
The notion of a tableau was introduced by Aho et al.
Parametric query-optimization algorithms have been proposed by Ioannidis et al.
Sellis [1988] was an early work on multiquery optimization, while Roy et al.
The term transaction refers to a collection of operations that form a single logical unit of work.
For instance, transfer of money from one account to another is a transaction consisting of two updates, one to each account.
It is important that either all actions of a transaction be executed completely, or, in case of some failure, partial effects of each incomplete transaction be undone.
Further, once a transaction is successfully executed, its effects must persist in the database—a system failure should not result in the database forgetting about a transaction that successfully completed.
In a database system where multiple transactions are executing concurrently, if updates to shared data are not controlled there is potential for transactions to see inconsistent intermediate states created by updates of other transactions.
Such a situation can result in erroneous updates to data stored in the database.
Thus, database systems must provide mechanisms to isolate transactions from the effects of other concurrently executing transactions.
Chapter 14 describes the concept of a transaction in detail, including the properties of atomicity, durability, isolation, and other properties provided by the transaction abstraction.
In particular, the chapter makes precise the notion of isolation by means of a concept called serializability.
Chapter 16 describes the recovery management component of a database, which implements the atomicity and durability properties.
Often, a collection of several operations on the database appears to be a single unit from the point of view of the database user.
For example, a transfer of funds from a checking account to a savings account is a single operation from the customer’s standpoint;within the database system, however, it consists of several operations.
Clearly, it is essential that all these operations occur, or that, in case of a failure, none occur.
It would be unacceptable if the checking account were debited but the savings account not credited.
Collections of operations that form a single logical unit of work are called transactions.
A database system must ensure proper execution of transactions despite failures—either the entire transaction executes, or none of it does.
Furthermore, it must manage concurrent execution of transactions in a way that avoids the introduction of inconsistency.
In our funds-transfer example, a transaction computing the customer’s total balance might see the checking-account balance before it is debited by the funds-transfer transaction, but see the savings balance after it is credited.
A transaction is a unit of program execution that accesses and possibly updates various data items.
Usually, a transaction is initiated by a user program written in a high-level data-manipulation language (typically SQL), or programming language (for example, C++, or Java), with embedded database accesses in JDBC or ODBC.
A transaction is delimited by statements (or function calls) of the form begin transaction and end transaction.
The transaction consists of all operations executed between the begin transaction and end transaction.
This collection of steps must appear to the user as a single, indivisible unit.
Since a transaction is indivisible, it either executes in its entirety or not at all.
Thus, if a transaction begins to execute but fails for whatever reason, any changes to the.
This requirement holds regardless of whether the transaction itself failed (for example, if it divided by zero), the operating system crashed, or the computer itself stopped operating.
As we shall see, ensuring that this requirement is met is difficult since some changes to the database may still be stored only in the main-memory variables of the transaction, while others may have been written to the database and stored on disk.
Furthermore, since a transaction is a single unit, its actions cannot appear to be separated by other database operations not part of the transaction.
While we wish to present this user-level impression of transactions, we know that reality is quite different.
Even a single SQL statement involves many separate accesses to the database, and a transaction may consist of several SQL statements.
Therefore, the database systemmust take special actions to ensure that transactions operate properly without interference from concurrently executing database statements.
Even if the system ensures correct execution of a transaction, this serves little purpose if the system subsequently crashes and, as a result, the system “forgets” about the transaction.
Because of the above three properties, transactions are an ideal way of structuring interaction with a database.
This leads us to impose a requirement on transactions themselves.
A transaction must preserve database consistency—if a transaction is run atomically in isolation starting from a consistent database, the database must again be consistent at the end of the transaction.
This consistency requirement goes beyond the data integrity constraints we have seen earlier (such as primary-key constraints, referential integrity, check constraints, and the like)
How this is done is the responsibility of the programmer who codes a transaction.
To restate the above more concisely, we require that the database system maintain the following properties of the transactions:
Either all operations of the transaction are reflected properly in the database, or none are.
Execution of a transaction in isolation (that is, with no other transaction executing concurrently) preserves the consistency of the database.
Even though multiple transactions may execute concurrently, the systemguarantees that, for every pair of transactions Ti and Tj , it appears to Ti that either Tj finished execution before Ti started or Tj started execution after Ti finished.
Thus, each transaction is unaware of other transactions executing concurrently in the system.
After a transaction completes successfully, the changes it has made to the database persist, even if there are system failures.
These properties are often called the ACID properties; the acronym is derived from the first letter of each of the four properties.
As we shall see later, ensuring the isolation property may have a significant adverse effect on system performance.
For this reason, some applications compromise on the isolation property.
We shall study these compromises after first studying the strict enforcement of the ACID properties.
Because SQL is a powerful and complex language, we begin our study of transactions with a simple database language that focuses on when data are moved from disk to main memory and from main memory to disk.
In doing this, we ignore SQL insert and delete operations, and defer considering them until Section 15.8
The only actual operations on the data are restricted in our simple language to arithmetic operations.
Later we shall discuss transactions in a realistic, SQL-based contextwith a richer set of operations.
The data items in our simplifiedmodel contain a single data value (a number in our examples)
We shall illustrate the transaction concept using a simple bank application consisting of several accounts and a set of transactions that access and update those accounts.
It is important to know if a change to adata itemappears only inmainmemory or if it has been written to the database on disk.
In a real database system, the write operation does not necessarily result in the immediate update of the data on the disk; the write operation may be temporarily stored elsewhere and executed on the disk later.
For now, however, we shall assume that the write operation updates the database immediately.
Let Ti be a transaction that transfers $50 from account A to account B.
For ease of presentation, we consider them in an order different from the order A-C-I-D.
Consistency: The consistency requirement here is that the sum of A and B be unchanged by the execution of the transaction.
Without the consistency requirement, money could be created or destroyed by the transaction! It can be verified easily that, if the database is consistent before an execution of the transaction, the database remains consistent after the execution of the transaction.
Ensuring consistency for an individual transaction is the responsibility of the application programmer who codes the transaction.
This task may be facilitated by automatic testing of integrity constraints, as we discussed in Section 4.4
Now suppose that, during the execution of transaction Ti , a failure occurs that prevents Ti from completing its execution successfully.
Further, suppose that the failure happened after the write(A) operation but before the write(B) operation.
The system destroyed $50 as a result of this failure.
In particular, we note that the sum A + B is no longer preserved.
Thus, because of the failure, the state of the system no longer reflects a real state of the world that the database is supposed to capture.
We must ensure that such inconsistencies are not visible in a database system.
Note, however, that the system must at some point be in an inconsistent state.
Thus, if the transaction never started orwas guaranteed to complete, such an inconsistent statewould not be visible except during the execution of the transaction.
That is the reason for the atomicity requirement: If the atomicity property is present, all actions of the transaction are reflected in the database, or none are.
The basic idea behind ensuring atomicity is this: The database system keeps track (on disk) of the old values of any data on which a transaction performs a write.
This information is written to a file called the log.
If the transaction does not complete its execution, the database system restores the old values from the log to make it appear as though the transaction never executed.
Durability:Once the execution of the transaction completes successfully, and the user who initiated the transaction has been notified that the transfer of.
The durability property guarantees that, once a transaction completes successfully, all the updates that it carried out on the database persist, even if there is a system failure after the transaction completes execution.
We assume for now that a failure of the computer systemmay result in loss of data in main memory, but data written to disk are never lost.
The updates carried out by the transaction have been written to disk before the transaction completes.
Information about the updates carried out by the transaction and written to disk is sufficient to enable the database to reconstruct the updates when the database system is restarted after the failure.
The recovery system of the database, described in Chapter 16, is responsible for ensuring durability, in addition to ensuring atomicity.
Isolation: Even if the consistency and atomicity properties are ensured for each transaction, if several transactions are executed concurrently, their operations may interleave in some undesirable way, resulting in an inconsistent state.
For example, as we saw earlier, the database is temporarily inconsistent while the transaction to transfer funds from A to B is executing, with the deducted total written to Aand the increased total yet to be written to B.
If a second concurrently running transaction reads A and B at this intermediate point and computes A+B, it will observe an inconsistent value.
Furthermore, if this second transaction then performs updates on A and B based on the inconsistent values that it read, the database may be left in an inconsistent state even after both transactions have completed.
A way to avoid the problem of concurrently executing transactions is to execute transactions serially—that is, one after the other.
However, concurrent execution of transactions provides significant performance benefits, as we shall see in Section 14.5
Other solutions have therefore been developed; they allow multiple transactions to execute concurrently.
We discuss the problems caused by concurrently executing transactions in Section 14.5
The isolation property of a transaction ensures that the concurrent execution of transactions results in a system state that is equivalent to a state that could have been obtained had these transactions executed one at a time in some order.
We shall discuss the principles of isolation further in Section 14.6
To understand how to ensure the atomicity and durability properties of a transaction, we must gain a better understanding of how the various data items in the database may be stored and accessed.
InChapter 10we saw that storagemedia can be distinguished by their relative speed, capacity, and resilience to failure, and classified as volatile storage or nonvolatile storage.We review these terms, and introduce another class of storage, called stable storage.
Information residing in volatile storage does not usually survive system crashes.
Examples of such storage are main memory and cache memory.
Access to volatile storage is extremely fast, both because of the speed of the memory access itself, and because it is possible to access any data item in volatile storage directly.
Examples of nonvolatile storage include secondary storage devices such as magnetic disk and flash storage, used for online storage, and tertiary storage devices such as optical media, and magnetic tapes, used for archival storage.
At the current state of technology, nonvolatile storage is slower than volatile storage, particularly for random access.
Both secondary and tertiary storage devices, however, are susceptible to failure which may result in loss of information.
Although stable storage is theoretically impossible to obtain, it can be closely approximated by techniques that make data loss extremely unlikely.
To implement stable storage, we replicate the information in several nonvolatile storage media (usually disk) with independent failure modes.
Updates must be done with care to ensure that a failure during an update to stable storage does not cause a loss of information.
The distinctions among the various storage types can be less clear in practice than in our presentation.
For example, certain systems, for example some RAID controllers, provide battery backup, so that some main memory can survive system crashes and power failures.
For a transaction to be durable, its changes need to bewritten to stable storage.
Similarly, for a transaction to be atomic, log records need to be written to stable storage before any changes are made to the database on disk.
Clearly, the degree to which a system ensures durability and atomicity depends on how stable its implementation of stable storage really is.
In some cases, a single copy on disk is considered sufficient, but applications whose data are highly valuable andwhose.
As we noted earlier, a transaction may not always complete its execution successfully.
If we are to ensure the atomicity property, an aborted transaction must have no effect on the state of the database.
Thus, any changes that the aborted transaction made to the database must be undone.
Once the changes caused by an aborted transaction have been undone, we say that the transaction has been rolled back.
It is part of the responsibility of the recovery scheme tomanage transaction aborts.
Each database modification made by a transaction is first recorded in the log.We record the identifier of the transaction performing themodification, the identifier of the data item being modified, and both the old value (prior to modification) and the new value (after modification) of the data item.
Maintaining a log provides the possibility of redoing a modification to ensure atomicity and durability as well as the possibility of undoing a modification to ensure atomicity in case of a failure during transaction execution.
A transaction that completes its execution successfully is said to be committed.
A committed transaction that has performedupdates transforms the database into a new consistent state, which must persist even if there is a system failure.
Once a transaction has committed, we cannot undo its effects by aborting it.
The only way to undo the effects of a committed transaction is to execute a compensating transaction.
However, it is not always possible to create such a compensating transaction.
Therefore, the responsibility of writing and executing a compensating transaction is left to the user, and is not handled by the database system.
We need to be more precise about what we mean by successful completion of a transaction.
A transaction must be in one of the following states:
Active, the initial state; the transaction stays in this state while it is executing.
Failed, after the discovery that normal execution can no longer proceed.
Aborted, after the transaction has been rolled back and the database has been.
The state diagram corresponding to a transaction appears in Figure 14.1
We say that a transaction has committed only if it has entered the committed state.
Similarly, we say that a transaction has aborted only if it has entered the aborted state.A transaction is said tohave terminated if it has either committedor aborted.
When it finishes its final statement, it enters the partially committed state.
At this point, the transaction has completed its execution, but it is still possible that it may have to be aborted, since the actual output may still be temporarily residing in main memory, and thus a hardware failure may preclude its successful completion.
The database system then writes out enough information to disk that, even in the event of a failure, the updates performed by the transaction can be re-created when the system restarts after the failure.
When the last of this information is written out, the transaction enters the committed state.
As mentioned earlier, we assume for now that failures do not result in loss of data on disk.
Chapter 16 discusses techniques to deal with loss of data on disk.
A transaction enters the failed state after the system determines that the transaction can no longer proceedwith its normal execution (for example, because of hardware or logical errors)
It can restart the transaction, but only if the transaction was aborted as a result of some hardware or software error that was not created through the internal logic of the transaction.
A restarted transaction is considered to be a new transaction.
It usually does so because of some internal logical error that can be corrected only by rewriting the application program, or because the input was bad, or because the desired data were not found in the database.
We must be cautious when dealing with observable external writes, such as writes to a user’s screen, or sending email.
Once such a write has occurred, it cannot be erased, since it may have been seen external to the database system.
Most systems allow suchwrites to take place only after the transaction has entered the committed state.
One way to implement such a scheme is for the database system to store any value associated with such external writes temporarily in a special relation in the database, and to perform the actual writes only after the transaction enters the committed state.
If the system should fail after the transaction has entered the committed state, but before it could complete the external writes, the database system will carry out the external writes (using the data in nonvolatile storage) when the system is restarted.
Handling external writes can be more complicated in some situations.
For example, suppose the external action is that of dispensing cash at an automated teller machine, and the system fails just before the cash is actually dispensed (we assume that cash can be dispensed atomically)
It makes no sense to dispense cash when the system is restarted, since the user may have left the machine.
In such a case a compensating transaction, such as depositing the cash back in the user’s account, needs to be executed when the system is restarted.
As another example, consider a user making a booking over the Web.
It is possible that the database system or the application server crashes just after the booking transaction commits.
It is also possible that the network connection to the user is lost just after the booking transaction commits.
In either case, even though the transaction has committed, the external write has not taken place.
To handle such situations, the application must be designed such that when the user connects to the Web application again, she will be able to see whether her transaction had succeeded or not.
For certain applications, it may be desirable to allow active transactions to display data to users, particularly for long-duration transactions that run for minutes or hours.
Unfortunately, we cannot allow such output of observable data unless we are willing to compromise transaction atomicity.
In Chapter 26, we discuss alternative transaction models that support long-duration, interactive transactions.
Allowing multiple transactions to update data concurrently causes several complications with consistency of the data, as we saw earlier.
Ensuring consistency in spite of concurrent execution of transactions requires extra work; it is far easier to insist that transactions run serially—that is, one at a time, each starting only after the previous one has completed.
The CPU and the disks in a computer system can operate in parallel.
Therefore, I/O activity can be done in parallel with processing at the CPU.
While a read or write on behalf of one transaction is in progress on one disk, another transaction can be running in the CPU, while another disk may be executing a read or write on behalf of a third transaction.
All of this increases the throughput of the system—that is, the number of transactions executed in a given amount of time.
Correspondingly, the processor and disk utilization also increase; in other words, the processor and disk spend less time idle, or not performing any useful work.
There may be a mix of transactions running on a system, some short and some long.
If transactions run serially, a short transaction may have to wait for a preceding long transaction to complete, which can lead to unpredictable delays in running a transaction.
If the transactions are operating on different parts of the database, it is better to let them run concurrently, sharing the CPU cycles and disk accesses among them.
Moreover, it also reduces the average response time: the average time for a transaction to be completed after it has been submitted.
When several transactions run concurrently, the isolation propertymay be violated, resulting in database consistency being destroyed despite the correctness of each individual transaction.
In this section, we present the concept of schedules to help identify those executions that are guaranteed to ensure the isolation property and thus database consistency.
The database systemmust control the interaction among the concurrent transactions to prevent them fromdestroying the consistency of the database.
Consider again the simplified banking system of Section 14.1, which has several accounts, and a set of transactions that access and update those accounts.
Several current trends in the field of computing are giving rise to an increase in the amount of concurrency possible.
As database systems exploit this concurrency to increase overall system performance, there will necessarily be an increasing number of transactions run concurrently.
Therefore, there was never any real concurrency in the computer.
The only concurrency was apparent concurrency created by the operating system as it shared the processor among several distinct tasks or processes.Modern computers are likely to havemany processors.
These may be truly distinct processors all part of the one computer.
However even a single processor may be able to run more than one process at a time by having multiple cores.
The Intel Core Duo processor is a well-known example of such a multicore processor.
For database systems to take advantage of multiple processors and multiple cores, two approaches are being taken.
One is to find parallelism within a single transaction or query.
Another is to support a very large number of concurrent transactions.
Many service providers now use large collections of computers rather than large mainframe computers to provide their services.
They are making this choice based on the lower cost of this approach.
A result of this is yet a further increase in the degree of concurrency that can be supported.
The bibliographic notes refer to texts that describe these advances in computer architecture and parallel computing.
Chapter 18 describes algorithms for building parallel database systems, which exploit multiple processors and multiple cores.
Thus, the total amount ofmoney in accountsA and B—that is, the sumA + B—is preserved after the execution of both transactions.
They represent the chronological order in which instructions are executed in the system.
Clearly, a schedule for a set of transactions must consist of all instructions of those transactions, and must preserve the order in which the instructions appear in each individual transaction.
For example, in transaction T1, the instruction write(A) must appear before the instruction read(B), in any valid schedule.
Note that we include in our schedules the commit operation to indicate that the transaction has entered the committed state.
These schedules are serial: Each serial schedule consists of a sequence of instructions from various transactions, where the instructions belonging to one single transaction appear together in that schedule.
When the database system executes several transactions concurrently, the corresponding schedule no longer needs to be serial.
If two transactions are running concurrently, the operating system may execute one transaction for a little while, then perform a context switch, execute the second transaction for some time, and then switch back to the first transaction for some time, and so on.
With multiple transactions, the CPU time is shared among all the transactions.
Several execution sequences are possible, since the various instructions from both transactions may now be interleaved.
In general, it is not possible to predict exactly how many instructions of a transaction will be executed before the CPU switches to another transaction.1
Returning to our previous example, suppose that the two transactions are executed concurrently.
This final state is an inconsistent state, since we have gained $50 in the process of the concurrent execution.
Indeed, the sumA + B is not preserved by the execution of the two transactions.
If control of concurrent execution is left entirely to the operating system,many possible schedules, including ones that leave the database in an inconsistent state, such as the one just described, are possible.
It is the job of the database system to ensure that any schedule that is executed will leave the database in a consistent state.
The number of possible schedules for a set of n transactions is very large.
Considering all the possible ways that steps of transactions might be interleaved, the total number of possible schedules is much larger than n!
We can ensure consistency of the database under concurrent execution by making sure that any schedule that is executed has the same effect as a schedule that could have occurred without any concurrent execution.
That is, the schedule should, in some sense, be equivalent to a serial schedule.
Certainly, serial schedules are serializable, but if steps of multiple transactions are interleaved, it is harder to determine whether a schedule is serializable.
Since transactions are programs, it is difficult to determine exactly what operations a transaction performs and how operations of various transactions interact.
For this reason, we shall not consider the various types of operations that a transaction can perform on a data item, but instead consider only two operations: read and write.
We assume that, between a read(Q) instruction and a write(Q) instruction on a data itemQ, a transaction may perform an arbitrary sequence of operations on the copy of Q that is residing in the local buffer of the transaction.
In this model, the only significant operations of a transaction, from a scheduling point of view, are its read and write instructions.
Commit operations, though relevant, are not considered until Section 14.7
In this section, we discuss different forms of schedule equivalence, but focus on a particular form called conflict serializability.
The order of I and J does not matter, since the same value of Q is read by Ti and Tj , regardless of the order.
If I comes before J , then Ti does not read the value of Q that is written by Tj in instruction J.
If J comes before I , then Ti reads the value of Q that is written by Tj.
The order of I and J matters for reasons similar to those of the previous case.
Since both instructions are write operations, the order of these instructions does not affect either Ti or Tj.
However, the value obtained by the next read(Q) instruction of S is affected, since the result of only the latter of the two write instructions is preserved in the database.
If there is no other write(Q) instruction after I and J in S, then the order of I and J directly affects the final value of Q in the database state that results from schedule S.
Thus, only in the case where both I and J are read instructions does the relative order of their execution not matter.
We say that I and J conflict if they are operations by different transactions on the same data item, and at least one of these instructions is a write operation.
Thus, we have shown that schedule 3 is equivalent to a serial schedule.
This equivalence implies that, regardless of the initial system state, schedule 3 will produce the same final state as will some serial schedule.
Not all serial schedules are conflict equivalent to each other.
The concept of conflict equivalence leads to the concept of conflict serializability.
We say that a schedule S is conflict serializable if it is conflict equivalent to a serial schedule.
We use the term conflict equivalent to distinguish the way we have just defined equivalence from other definitions that we shall discuss later on in this section.
If the precedence graph for S has a cycle, then schedule S is not conflict serializable.
If the graph contains no cycles, then the schedule S is conflict serializable.
A serializability order of the transactions can be obtained by finding a linear order consistent with the partial order of the precedence graph.
Thus, to test for conflict serializability, we need to construct the precedence graph and to invoke a cycle-detection algorithm.
Cycle-detection algorithms can be found in standard textbooks on algorithms.
Cycle-detection algorithms, such as those based on depth-first search, require on the order of n2 operations, where n is the number of vertices in the graph (that is, the number of transactions)
We can see from this example that there are less-stringent definitions of schedule equivalence than conflict equivalence.
In general, such analysis is hard to implement and is computationally expensive.
In our example, the final result is the same as that of a serial schedule because of the mathematical fact that addition and subtraction are commutative.
While this may be easy to see in our simple example, the general case is not so easy since a transaction may be expressed as a complex SQL statement, a Java program with JDBC calls, etc.
However, there are other definitions of schedule equivalence based purely on the read and write operations.
One such definition is view equivalence, a definition that leads to the concept of view serializability.
So far, we have studied schedules while assuming implicitly that there are no transaction failures.
We now address the effect of transaction failures during concurrent execution.
Testing for view serializability has been proven to be NP-complete, which means that it is virtually certain that no efficient test for view serializability exists.
If a transaction Ti fails, for whatever reason, we need to undo the effect of this transaction to ensure the atomicity property of the transaction.
In a system that allows concurrent execution, the atomicity property requires that any transaction Tj that is dependent on Ti (that is, Tj has read data written by Ti ) is also aborted.
To achieve this, we need to place restrictions on the type of schedules permitted in the system.
In the following two subsections, we address the issue of what schedules are acceptable from the viewpoint of recovery from transaction failure.
We describe in Chapter 15 how to ensure that only such acceptable schedules are generated.
We call this a partial schedule because we have not included a commit or abort operation for T6
Notice that T7 commits immediately after executing the read(A) instruction.
Because of this, we must abort T7 to ensure atomicity.
Thus, we have a situation where it is impossible to recover correctly from the failure of T6
A recoverable schedule is one where, for each pair of transactions Ti and Tj such that Tj reads a data item previously written by Ti , the commit operation of Ti appears before the commit operation of Tj.
Even if a schedule is recoverable, to recover correctly from the failure of a transaction Ti , we may have to roll back several transactions.
Such situations occur if transactions have read data written by Ti.
As an illustration, consider the partial schedule of Figure 14.15
Cascading rollback is undesirable, since it leads to the undoing of a significant amount of work.
It is desirable to restrict the schedules to those where cascading rollbacks cannot occur.
Formally, a cascadeless schedule is one where, for each pair of transactions Ti and Tj such that Tj reads a data item previously written by Ti , the commit operation of Ti appears before the read operation of Tj.
It is easy to verify that every cascadeless schedule is also recoverable.
Serializability is a useful concept because it allows programmers to ignore issues related to concurrency when they code transactions.
If every transaction has the property that it maintains database consistency if executed alone, then serializability ensures that concurrent executions maintain consistency.
However, the protocols required to ensure serializability may allow too little concurrency for certain applications.
The use of weaker levels of consistency places additional burdens on programmers for ensuring database correctness.
The SQL standard also allows a transaction to specify that itmaybe executed in such a way that it becomes nonserializable with respect to other transactions.
For instance, a transaction may operate at the isolation level of read uncommitted, which permits the transaction to read a data item even if it was written by a transaction that has not been committed.
If these transactions were to execute in a serializable fashion, they could interfere with other transactions, causing the others’ execution to be delayed.
The isolation levels specified by the SQL standard are as follows:
However, as we shall explain shortly, some database systems implement this isolation level in a manner that may, in certain cases, allow nonserializable executions.
Repeatable read allows only committed data to be read and further requires that, between two reads of a data item by a transaction, no other transaction is allowed to update it.
However, the transaction may not be serializable with respect to other transactions.
For instance, when it is searching for data satisfying some conditions, a transaction may find some of the data inserted by a committed transaction, but may not find other data inserted by the same transaction.
Read committed allows only committed data to be read, but does not require repeatable reads.
For instance, between two reads of a data item by the transaction, another transaction may have updated the data item and committed.
All the isolation levels above additionally disallow dirty writes, that is, they disallowwrites to a data item that has already beenwritten by another transaction that has not yet committed or aborted.
Many database systems run, by default, at the read-committed isolation level.
In SQL, it is possible to set the isolation level explicitly, rather than accepting the system’s default setting.
For example, the statement “set transaction isolation level serializable;” sets the isolation level to serializable; any of the other isolation levels may be specified instead.
The above syntax is supported by Oracle, PostgreSQL and SQL Server; DB2 uses the syntax “change isolation level,”with its own abbreviations for isolation levels.
Changing of the isolation level must be done as the first statement of a transaction.
An application designermay decide to accept aweaker isolation level in order to improve system performance.
While it may seem shortsighted to risk database consistency for performance, this trade-off makes sense if we can be sure that the inconsistency that may occur is not relevant to the application.
As long as the implementation ensures serializability, the designer of a database application or a user of an application does not need to know the details of such implementations, except perhaps for dealing with performance issues.
Unfortunately, even if the isolation level is set to serializable, some database systems actually implement a weaker level of isolation, which does not rule out every possible nonserializable execution; we revisit this issue in Section 14.9
If weaker levels of isolation are used, either explicitly or implicitly, the application designer has to be aware of some details of the implementation, to avoid or minimize the chance of inconsistency due to lack of serializability.
Serializable schedules are the ideal way to ensure consistency, but in our dayto-day lives, we don’t impose such stringent requirements.
A Web site offering goods for sale may list an item as being in stock, yet by the time a user selects the item and goes through the checkout process, that item might no longer be available.
Viewed from a database perspective, this would be a nonrepeatable read.
Assume that a traveler has already booked an itinerary and now is selecting seats for each flight.
Many airline Web sites allow the user to step through the various flights and choose a seat, after which the user is asked to confirm the selection.
It could be that other travelers are selecting seats or changing their seat selections for the same flights at the same time.
The seat availability that the traveler was shown is thus actually changing, but the traveler is shown a snapshot of the seat availability as of when the traveler started the seat selection process.
Even if two travelers are selecting seats at the same time, most likely they will select different seats, and if so there would be no real conflict.
However, the transactions are not serializable, since each traveler has read data that was subsequently updated by the other traveler, leading to a cycle in the precedence graph.
If two travelers performing seat selection concurrently actually selected the same seat, one of them would not be able to get the seat they selected; however, the situation could be easily resolved by asking the traveler to perform the selection again, with updated seat availability information.
It is possible to enforce serializability by allowing only one traveler to do seat selection for a particular flight at a time.
However, doing so could cause significant delays as travelers would have to wait for their flight to become available for seat selection; in particular a traveler who takes a long time to make a choice could cause seriousproblems for other travelers.
Instead, any such transaction is typically broken up into a part that requires user interaction, and a part that runs exclusively on the database.
In the example above, the database transaction would check if the seats chosen by the user are still available, and if so update the seat selection in the database.
Serializability is ensured only for the transactions that run on the database, without user interaction.
So far, we have seen what properties a schedule must have if it is to leave the database in a consistent state and allow transaction failures to be handled in a safe manner.
While a transaction holds a lock, no other transaction is allowed to acquire the lock, and all must therefore wait for the lock to be released.
As a result of the locking policy, only one transaction can execute at a time.
These are trivially serializable, and it is easy to verify that they are recoverable and cascadeless as well.
In otherwords, it provides a poor degree of concurrency (indeed, no concurrency at all)
As we saw in Section 14.5, concurrent execution has substantial performance benefits.
Instead of locking the entire database, a transaction could, instead, lock only those data items that it accesses.
Under such a policy, the transaction must hold locks long enough to ensure serializability, but for a period short enough not to harm performance excessively.
Complicating matters are SQL statements like those we saw in Section 14.10, where the data items accessed depend on a where clause.
In Chapter 15, we present the two-phase locking protocol, a simple, widely used technique that ensures serializability.
Stated simply, two-phase locking requires a transaction to have two phases, one where it acquires locks but does not release any, and a second phase where the transaction releases locks but does not acquire any.
In practice, locks are usually released only when the transaction completes its execution and has been either committed or aborted.
Further improvements to locking result if we have two kinds of locks: shared and exclusive.
Shared locks are used for data that the transaction reads and exclusive locks are used for those it writes.
Many transactions can hold shared locks on the same data item at the same time, but a transaction is allowed an exclusive lockonadata itemonly if noother transactionholds any lock (regardless of whether shared or exclusive) on the data item.
This use of two modes of locks along with two-phase locking allows concurrent reading of data while still ensuring serializability.
Another category of techniques for the implementation of isolation assigns each transaction a timestamp, typically when it begins.
The read timestamp of a data item holds the largest (that is, the most recent) timestamp of those transactions that read the data item.
The write timestamp of a data item holds the timestamp of the transaction that.
Timestamps are used to ensure that transactions access each data item in order of the transactions’ timestamps if their accesses conflict.
When this is not possible, offending transactions are aborted and restarted with a new timestamp.
By maintaining more than one version of a data item, it is possible to allow a transaction to read an old version of a data item rather than a newer version written by an uncommitted transaction or by a transaction that should come later in the serialization order.
One in particular, called snapshot isolation, is widely used in practice.
In snapshot isolation, we can imagine that each transaction is given its own version, or snapshot, of the database when it begins.4 It reads data from this private version and is thus isolated from the updates made by other transactions.
If the transaction updates the database, that update appears only in its own version, not in the actual database itself.
Information about these updates is saved so that the updates can be applied to the “real” database if the transaction commits.
When a transaction T enters the partially committed state, it then proceeds to the committed state only if no other concurrent transaction hasmodified data that T intends to update.
Snapshot isolation ensures that attempts to read data never need to wait (unlike locking)
Read-only transactions cannot be aborted; only those thatmodify data run a slight risk of aborting.
Since each transaction reads its own version or snapshot of the database, reading data does not cause subsequent update attempts by other transactions to wait (unlike locking)
Since most transactions are read-only (and most others read more data than they update), this is often a major source of performance improvement as compared to locking.
Of course, in reality, the entire database is not copied.
Multiple versions are kept only of those data items that are changed.
Oracle, PostgreSQL, and SQL Server offer the option of snapshot isolation.
Oracle and PostgreSQL implement the serializable isolation level using snapshot isolation.
As a result, their implementation of serializability can, in exceptional circumstances, result in a nonserializable execution being allowed.
In Section 4.3, we presented the SQL syntax for specifying the beginning and end of transactions.
Now that we have seen some of the issues in ensuring the ACID properties for transactions, we are ready to consider how those properties are ensured when transactions are specified as a sequence of SQL statements rather than the restricted model of simple reads and writes that we considered up to this point.
In our simple model, we assumed a set of data items exists.
While our simple model allowed data-item values to be changed, it did not allow data items to be created or deleted.
In SQL, however, insert statements create new data and delete statements delete data.
These two statements are, in effect, write operations, since they change the database, but their interactions with the actions of other transactions are different from what we saw in our simple model.
As an example, consider the following SQL query on our university database that finds all instructors who earn more than $90,000
Using our sample instructor relation (Appendix A.3), we find that only Einstein and Brandt satisfy the condition.
Now assume that around the same time we are running our query, another user inserts a new instructor named “James” whose salary is $100,000
The result of our query will be different depending on whether this insert comes before or after our query is run.
In a concurrent execution of these transactions, it is intuitively clear that they conflict, but this is a conflict not captured by our simple model.
This situation is referred to as the phantomphenomenon, because a conflict may exist on “phantom” data.
Our simple model of transactions required that operations operate on a specific data item given as an argument to the operation.
In our simplemodel, we can look at the read andwrite steps to see which data items are referenced.
But in an SQL statement, the specific data items (tuples) referenced may be determined by a where clause predicate.
So the same transaction, if run more than once, might.
One way of dealing with the above problem is to recognize that it is not sufficient for concurrency control to consider only the tuples that are accessed by a transaction; the information used to find the tuples that are accessed by the transaction must also be considered for the purpose of concurrency control.
The information used to find tuples could be updated by an insertion or deletion, or in the case of an index, even by an update to a search-key attribute.
For example, if locking is used for concurrency control, the data structures that track the tuples in a relation, as well as index structures, must be appropriately locked.
However, such locking can lead to poor concurrency in some situations; index-locking protocols which maximize concurrency, while ensuring serializability in spite of inserts, deletes, and predicates in queries, are discussed in Section 15.8.3
We now face an interesting situation in determining whether our query conflicts with the update statement.
If our query reads the entire instructor relation, then it reads the tuple with Wu’s data and conflicts with the update.
However, using the above approach, it would appear that the existence of a conflict depends on a low-level query processing decision by the system that is unrelated to a user-level view of the meaning of the two SQL statements! An alternative approach to concurrency control treats an insert, delete or update as conflicting with a predicate on a relation, if it could affect the set of tuples selected by a predicate.
Locking based on this idea is called predicate locking; however predicate locking is expensive, and not used in practice.
A transaction is aunit of programexecution that accesses andpossibly updates various data items.
Understanding the concept of a transaction is critical for understanding and implementing updates of data in a database in such a way that concurrent executions and failures of various forms do not result in the database becoming inconsistent.
Transactions are required to have the ACID properties: atomicity, consistency, isolation, and durability.
Atomicity ensures that either all the effects of a transaction are reflected in the database, or none are; a failure cannot leave the database in a state where a transaction is partially executed.
Consistency ensures that, if the database is initially consistent, the execution of the transaction (by itself) leaves the database in a consistent state.
Isolation ensures that concurrently executing transactions are isolated from one another, so that each has the impression that no other transaction is executing concurrently with it.
Durability ensures that, once a transaction has been committed, that transaction’s updates do not get lost, even if there is a system failure.
Concurrent execution of transactions improves throughput of transactions and system utilization, and also reduces waiting time of transactions.
The various types of storage in a computer are volatile storage, nonvolatile storage, and stable storage.
Data in volatile storage, such as in RAM, are lost when the computer crashes.
Data in nonvolatile storage, such as disk, are not lost when the computer crashes, but may occasionally be lost because of failures such as disk crashes.
Stable storage that must be accessible online is approximated with mirrored disks, or other forms of RAID, which provide redundant data storage.
Offline, or archival, stable storage may consist of multiple tape copies of data stored in physically secure locations.
When several transactions execute concurrently on the database, the consistency of data may no longer be preserved.
It is therefore necessary for the system to control the interaction among the concurrent transactions.
Since a transaction is a unit that preserves consistency, a serial execution of transactions guarantees that consistency is preserved.
A schedule captures the key actions of transactions that affect concurrent execution, such as read and write operations, while abstracting away internal details of the execution of the transaction.
We require that any schedule produced by concurrent processing of a set of transactionswill have an effect equivalent to a schedule producedwhen these transactions are run serially in some order.
Serializability of schedules generated by concurrently executing transactions can be ensured through one of a variety of mechanisms called concurrencycontrol policies.
We can test a given schedule for conflict serializability by constructing a precedence graph for the schedule, and by searching for absence of cycles in the graph.
Schedules must be recoverable, to make sure that if transaction a sees the effects of transaction b, and b then aborts, then a also gets aborted.
Schedules should preferably be cascadeless, so that the abort of a transaction does not result in cascading aborts of other transactions.
Cascadelessness is ensured by allowing transactions to only read committed data.
What are the steps involved in creation and deletion of files, and in writing data to a file?
Explain how the issues of atomicity and durability are relevant to the creation and deletion of files and to writing data to files.
The update performed by Tk has been lost, since the update done by Tj ignored the value written by Tk.
Give an example of a schedule showing the lost update anomaly.
Give an example schedule to show that the lost update anomaly is possible with the read committed isolation level.
Explain why the lost update anomaly is not possible with the repeatable read isolation level.
Describe a particular scenario in which a nonserializable execution occurs that would present a problem for the bank.
Describe a particular scenario in which a nonserializable execution occurs, but the airline may be willing to accept it in order to gain better overall performance.
Consider a database system that runs on a system with multiple processors, where it is not always possible to establish an exact ordering between operations that executed on different processors.
However, operations on a data item can be totally ordered.
Does the above situation cause any problem for the definition of conflict.
Show that every serial execution involving these two transactions preserves the consistency of the database.
Give an example of a schedule where one transaction uses the pred read operation on relation r and another concurrent transactions deletes a tuple from r , but the schedule does not exhibit a phantom conflict.
To do so, you have to give the schema of relation r , and show the attribute values of the deleted tuple.
Bernstein and Newcomer [1997] provides textbook coverage of various aspects of transaction processing.
The concept of serializability was formalized by Eswaran et al.
We saw in Chapter 14 that one of the fundamental properties of a transaction is isolation.
When several transactions execute concurrently in the database, however, the isolation property may no longer be preserved.
To ensure that it is, the system must control the interaction among the concurrent transactions; this control is achieved through one of a variety of mechanisms called concurrencycontrol schemes.
In this chapter, we consider the management of concurrently executing transactions, and we ignore failures.
In Chapter 16, we shall see how the system can recover from failures.
No one scheme is clearly the best; each one has advantages.
In practice, the most frequently used schemes are two-phase locking and snapshot isolation.
Oneway to ensure isolation is to require that data items be accessed in amutually exclusive manner; that is, while one transaction is accessing a data item, no other transaction can modify that data item.
The most common method used to implement this requirement is to allow a transaction to access a data item only if it is currently holding a lock on that item.
There are various modes in which a data item may be locked.
In this section, we restrict our attention to two modes:
If a transaction Ti has obtained a shared-mode lock (denoted by S) on item Q, then Ti can read, but cannot write, Q.
If a transaction Ti has obtained an exclusive-mode lock (denoted by X) on item Q, then Ti can both read and write Q.
We require that every transaction request a lock in an appropriate mode on data item Q, depending on the types of operations that it will perform on Q.
The use of these two lock modes allows multiple transactions to read a data item but limits write access to just one transaction at a time.
Note that sharedmode is compatiblewith sharedmode, but notwith exclusive mode.
At any time, several shared-mode locks can be held simultaneously (by different transactions) on a particular data item.
A subsequent exclusive-mode lock request has to wait until the currently held shared-mode locks are released.
A transaction requests a shared lock on data item Q by executing the lockS(Q) instruction.
Similarly, a transaction requests an exclusive lock through the lock-X(Q) instruction.
A transaction can unlock a data item Q by the unlock(Q) instruction.
To access a data item, transaction Ti must first lock that item.
If the data item is already locked by another transaction in an incompatible mode, the concurrencycontrol manager will not grant the lock until all incompatible locks held by other transactions have been released.
Thus, Ti is made to wait until all incompatible locks held by other transactions have been released.
Transaction Ti may unlock a data item that it had locked at some earlier point.
Note that a transaction must hold a lock on a data item as long as it accesses that item.
Moreover, it is not necessarily desirable for a transaction to unlock a data item immediately after its final access of that data item, since serializability may not be ensured.
Let A and B be two accounts that are accessed by transactions T1
The transaction making a lock request cannot execute its next action until the concurrencycontrol manager grants the lock.
Hence, the lock must be granted in the interval of time between the lock-request operation and the following action of the transaction.
Exactly when within this interval the lock is granted is not important; we can safely assume that the lock is granted just before the following action of the transaction.
Suppose now that unlocking is delayed to the end of the transaction.
T4 will not print out an inconsistent result in any of them; we shall see why later.
Thus, we have arrived at a state where neither of these transactions can ever proceed with its normal execution.
Once a transaction has been rolled back, the data items that were locked by that transaction are unlocked.
These data items are then available to the other transaction, which can continue with its execution.
We shall return to the issue of deadlock handling in Section 15.2
If we do not use locking, or if we unlock data items too soon after reading or writing them, we may get inconsistent states.
On the other hand, if we do not unlock a data item before requesting a lock on another data item, deadlocks may occur.
There are ways to avoid deadlock in some situations, as we shall see in Section 15.1.5
However, in general, deadlocks are a necessary evil associated with locking, if we want to avoid inconsistent states.
We shall require that each transaction in the system follow a set of rules, called a lockingprotocol, indicatingwhen a transactionmay lock and unlock each of the data items.
When a transaction requests a lock on a data item in a particular mode, and no other transaction has a lock on the same data item in a conflicting mode, the lock can be granted.
However, care must be taken to avoid the following scenario.
Meanwhile, a transaction T3 may request a shared-mode lock on the same data item.
In fact, it is possible that there is a sequence of transactions that each requests a shared-mode lock on the data item, and each transaction releases the lock a short while after it is granted, but T1 never gets the exclusive-mode lock on the data item.
The transaction T1 may never make progress, and is said to be starved.
There is no other transaction holding a lock on Q in a mode that conflicts with M.
There is no other transaction that is waiting for a lock on Q and that made its lock request before Ti.
Thus, a lock request will never get blocked by a lock request that is made later.
One protocol that ensures serializability is the two-phase locking protocol.
This protocol requires that each transaction issue lock and unlock requests in two phases:
A transaction may obtain locks, but may not release any lock.
A transaction may release locks, but may not obtain any new locks.
Once the transaction releases a lock, it enters the shrinking phase, and it can issue no more lock requests.
Note that the unlock instructions do not need to appear at the end of the transaction.
For example, in the case of transaction T3, we could move the unlock(B) instruction to just after the lock-X(A) instruction, and still retain the two-phase locking property.
We can show that the two-phase locking protocol ensures conflict serializability.
The point in the schedule where the transaction has obtained its final lock (the end of its growing phase) is called the lock point of the transaction.
Now, transactions can be ordered according to their lock pointsthis ordering is, in fact, a serializability ordering for the transactions.
We leave the proof as an exercise for you to do (see Practice Exercise 15.1)
Recall from Section 14.7.2 that, in addition to being serializable, schedules should be cascadeless.
As an illustration, consider the partial schedule of Figure 15.8
Cascading rollbacks can be avoided by a modification of two-phase locking called the strict two-phase locking protocol.
This protocol requires not only that locking be two phase, but also that all exclusive-mode locks taken by a transaction be held until that transaction commits.
This requirement ensures that any data written by an uncommitted transaction are locked in exclusive mode until the transaction commits, preventing any other transaction from reading the data.
Another variant of two-phase locking is the rigorous two-phase locking protocol, which requires that all locks be held until the transaction commits.
We can easily verify that, with rigorous two-phase locking, transactions can be serialized in the order in which they commit.
Consider the following two transactions, forwhichwe have shown only some of the significant read and write operations:
Therefore, any concurrent execution of both transactions amounts to a serial execution.
This observation leads us to a refinement of the basic two-phase locking protocol, in which lock conversions are allowed.
We shall provide a mechanism for upgrading a shared lock to an exclusive lock, and downgrading an exclusive lock to a shared lock.
We denote conversion from shared to exclusive modes by upgrade, and from exclusive to shared by downgrade.
Rather, upgrading can take place in only the growing phase, whereas downgrading can take place in only the shrinking phase.
Note that a transaction attempting to upgrade a lock on an item Q may be forced to wait.
This enforced wait occurs if Q is currently locked by another transaction in shared mode.
Further, if exclusive locks are held until the end of the transaction, the schedules are cascadeless.
Strict two-phase locking and rigorous two-phase locking (with lock conversions) are used extensively in commercial database systems.
A simple but widely used scheme automatically generates the appropriate lock and unlock instructions for a transaction, on the basis of read and write requests from the transaction:
When a transaction Ti issues a read(Q) operation, the system issues a lockS(Q) instruction followed by the read(Q) instruction.
When Ti issues a write(Q) operation, the system checks to see whether Ti already holds a shared lock on Q.
If it does, then the system issues an upgrade(Q) instruction, followed by the write(Q) instruction.
Otherwise, the system issues a lock-X(Q) instruction, followed by the write(Q) instruction.
All locks obtained by a transaction are unlocked after that transaction commits or aborts.
A lock manager can be implemented as a process that receives messages from transactions and sends messages in reply.
The lock-manager process replies to lock-request messages with lock-grant messages, or with messages requesting rollback of the transaction (in case of deadlocks)
Unlock messages require only an acknowledgment in response, but may result in a grant message to another waiting transaction.
The lockmanager uses this data structure: For each data item that is currently locked, it maintains a linked list of records, one for each request, in the order in which the requests arrived.
It uses a hash table, indexed on the name of a data item, to find the linked list (if any) for a data item; this table is called the lock table.
Each record of the linked list for a data item notes which transaction made the request, and what lock mode it requested.
The record also notes if the request has currently been granted.
The lock table uses overflow chaining, so there is a linked list of data items for each entry in the lock table.
There is also a list of transactions that have been granted locks, or are waiting for locks, for each of the data items.
Granted locks are the rectangles filled in a darker shade, while waiting requests are the rectangles filled in a lighter shade.
We have omitted the lock mode to keep the figure simple.
Although the figure does not show it, the lock table should also maintain an index on transaction identifiers, so that it is possible to determine efficiently the set of locks held by a given transaction.
When a lock request message arrives, it adds a record to the end of the linked list for the data item, if the linked list is present.
Otherwise it creates a new linked list, containing only the record for the request.
It always grants a lock request on a data item that is not currently locked.
But if the transaction requests a lock on an item on which a lock is currently held, the lockmanager grants the request only if it is compatiblewith the locks that are currently held, and all earlier requests have been granted already.
When the lock manager receives an unlock message from a transaction, it deletes the record for that data item in the linked list corresponding to that transaction.
It tests the record that follows, if any, as described in the previous paragraph, to see if that request cannowbegranted.
If it can, the lockmanager grants that request, and processes the record following it, if any, similarly, and so on.
If a transaction aborts, the lock manager deletes any waiting request made by the transaction.
Once the database system has taken appropriate actions to undo the transaction (see Section 16.3), it releases all locks held by the aborted transaction.
This algorithm guarantees freedom from starvation for lock requests, since a request can never be granted while a request received earlier is waiting to be granted.
We study how to detect and handle deadlocks later, in Section 15.2.2
Section 17.2.1 describes an alternative implementation—one that uses shared memory instead of message passing for lock request/grant.
As noted in Section 15.1.3, if we wish to develop protocols that are not two phase, we need additional information on how each transactionwill access the database.
There are various models that can give us the additional information, each differing in the amount of information provided.
The simplest model requires that we have prior knowledge about the order in which the database items will be accessed.
Given such information, it is possible to construct locking protocols that are not two phase, but that, nevertheless, ensure conflict serializability.
This partial ordering may be the result of either the logical or the physical organization of the data, or it may be imposed solely for the purpose of concurrency control.
The partial ordering implies that the set Dmay now be viewed as a directed acyclic graph, called a database graph.
In this section, for the sake of simplicity, we will restrict our attention to only those graphs that are rooted trees.
We shall present a simpleprotocol, called the tree protocol,which is restricted to employonly exclusive locks.
References to other, more complex, graph-based locking protocols are in the bibliographical notes.
In the tree protocol, the only lock instruction allowed is lock-X.
Each transaction Ti can lock a data item at most once, and must observe the following rules:
The first lock by Ti may be on any data item.
Subsequently, a data item Q can be locked by Ti only if the parent of Q is currently locked by Ti.
A data item that has been locked and unlocked by Ti cannot subsequently be relocked by Ti.
All schedules that are legal under the tree protocol are conflict serializable.
To illustrate this protocol, consider the database graph of Figure 15.11
One possible schedule in which these four transactions participated appears in Figure 15.12
Note that, during its execution, transaction T10 holds locks on two disjoint subtrees.
Observe that the schedule of Figure 15.12 is conflict serializable.
It can be shown not only that the tree protocol ensures conflict serializability, but also that this protocol ensures freedom from deadlock.
The tree protocol in Figure 15.12 does not ensure recoverability and cascadelessness.
To ensure recoverability and cascadelessness, the protocol can be modified to not permit release of exclusive locks until the end of the transaction.
Holding exclusive locks until the end of the transaction reduces concurrency.
Here is an alternative that improves concurrency, but ensures only recoverability: For each data item with an uncommitted write, we record which transaction performed the last write to the data item.
Whenever a transaction Ti performs a read of an uncommitted data item, we record a commit dependency of Ti on the.
Transaction Ti is then not permitted to commit until the commit of all transactions on which it has a commit dependency.
If any of these transactions aborts, Ti must also be aborted.
The tree-locking protocol has an advantage over the two-phase locking protocol in that, unlike two-phase locking, it is deadlock-free, so no rollbacks are required.
The tree-locking protocol has another advantage over the two-phase locking protocol in that unlocking may occur earlier.
Earlier unlocking may lead to shorter waiting times, and to an increase in concurrency.
However, the protocol has the disadvantage that, in some cases, a transaction may have to lock data items that it does not access.
For example, a transaction that needs to access data itemsA and J in the database graph of Figure 15.11 must lock not only A and J, but also data items B, D, and H.
This additional locking results in increased locking overhead, the possibility of additional waiting time, and a potential decrease in concurrency.
Further, without prior knowledge of what data items will need to be locked, transactions will have to lock the root of the tree, and that can reduce concurrency greatly.
Indeed, there are schedules possible under the two-phase locking protocol that are not possible under the treeprotocol, and vice versa.
The only remedy to this undesirable situation is for the system to invoke some drastic action, such as rolling back some of the transactions involved in the deadlock.
Rollback of a transaction may be partial: That is, a transaction may be rolled back to the point where it obtained a lock whose release resolves the deadlock.
There are two principal methods for dealing with the deadlock problem.
We can use a deadlock prevention protocol to ensure that the systemwill never enter a deadlock state.
Alternatively, we can allow the system to enter a deadlock state, and then try to recover by using a deadlock detection and deadlock recovery scheme.
Prevention is commonly used if the probability that the system would enter a deadlock state is relatively high; otherwise, detection and recovery are more efficient.
Note that a detection and recovery scheme requires overhead that includes not only the run-time cost of maintaining the necessary information and of executing the detection algorithm, but also the potential losses inherent in recovery from a deadlock.
One approach ensures that no cyclic waits can occur by ordering the requests for locks, or requiring all locks to be acquired together.
The other approach is closer to deadlock recovery, and performs transaction rollback instead of waiting for a lock, whenever the wait could potentially result in a deadlock.
The simplest scheme under the first approach requires that each transaction locks all its data items before it begins execution.
Moreover, either all are locked in one step or none are locked.
Another approach for preventing deadlocks is to impose an ordering of all data items, and to require that a transaction lock data items only in a sequence consistent with the ordering.
We have seen one such scheme in the tree protocol, which uses a partial ordering of data items.
A variation of this approach is to use a total order of data items, in conjunction with two-phase locking.
Once a transaction has locked a particular item, it cannot request locks on items that precede that item in the ordering.
This scheme is easy to implement, as long as the set of data items accessed by a transaction is known when the transaction starts execution.
The second approach for preventing deadlocks is to use preemption and transaction rollbacks.
In preemption, when a transaction Tj requests a lock that transaction Ti holds, the lock granted to Ti may be preempted by rolling back of Ti , and granting of the lock to Tj.
To control the preemption, we assign a unique timestamp, based on a counter or on the system clock, to each transaction when it begins.
The system uses these timestamps only to decide whether a transaction should wait or roll back.
If a transaction is rolled back, it retains its old timestamp when restarted.
When transaction Ti requests a data item currently held by Tj , Ti is allowed to wait only if it has a timestamp smaller than that of Tj (that is, Ti is older than Tj )
When transaction Ti requests a data item currently held by Tj , Ti is allowed to wait only if it has a timestamp larger than that of Tj (that is, Ti is younger than Tj )
Otherwise, Tj is rolled back (Tj is wounded by Ti )
The major problem with both of these schemes is that unnecessary rollbacks may occur.
Another simple approach to deadlock prevention is based on lock timeouts.
In this approach, a transaction that has requested a lock waits for at most a specified amount of time.
If the lock has not been granted within that time, the transaction is said to time out, and it rolls itself back and restarts.
If there was in fact a deadlock, one or more transactions involved in the deadlock will time out and roll back, allowing the others to proceed.
This scheme falls somewhere between deadlock prevention, where a deadlock will never occur, and deadlock detection and recovery, which Section 15.2.2 discusses.
Too long a wait results in unnecessary delays once a deadlock has occurred.
Too short a wait results in transaction rollback even when there is no deadlock, leading to wasted resources.
If a system does not employ some protocol that ensures deadlock freedom, then a detection and recovery scheme must be used.
An algorithm that examines the state of the system is invoked periodically to determine whether a deadlock has occurred.
If one has, then the system must attempt to recover from the deadlock.
Maintain information about the current allocation of data items to transactions, as well as any outstanding data item requests.
Provide an algorithm that uses this information to determine whether the system has entered a deadlock state.
Recover from the deadlock when the detection algorithm determines that a deadlock exists.
A deadlock exists in the system if and only if the wait-for graph contains a cycle.
Each transaction involved in the cycle is said to be deadlocked.
To detect deadlocks, the system needs to maintain the wait-for graph, and periodically to invoke an algorithm that searches for a cycle in the graph.
To illustrate these concepts, consider thewait-for graph in Figure 15.13, which depicts the following situation:
Since the graph has no cycle, the system is not in a deadlock state.
Consequently, the question arises: When should we invoke the detection.
If deadlocks occur frequently, then the detection algorithm should be invoked more frequently.
In addition, the number of cycles in the graph may also grow.
In the worst case, we would invoke the detection algorithm every time a request for allocation could not be granted immediately.
When a detection algorithm determines that a deadlock exists, the system must recover from the deadlock.
The most common solution is to roll back one or more transactions to break the deadlock.
Given a set of deadlocked transactions, wemust determine which transaction (or transactions) to roll back to break the deadlock.
We should roll back those transactions that will incur the minimum cost.
Many factors may determine the cost of a rollback, including:
How long the transaction has computed, and how much longer the transaction will compute before it completes its designated task.
How many more data items the transaction needs for it to complete.
Oncewe have decided that a particular transactionmust be rolled back, we must determine how far this transaction should be rolled back.
The simplest solution is a total rollback: Abort the transaction and then restart it.
However, it is more effective to roll back the transaction only as far as necessary to break the deadlock.
Such partial rollback requires the system to maintain additional information about the state of all the running transactions.
Specifically, the sequence of lock requests/grants and updates performed by the transaction needs to be recorded.
The deadlock detection mechanism should decide which locks the selected transaction needs to release in order to break the deadlock.
The selected transaction must be rolled back to the point where it obtained the first of these locks, undoing all actions it took after that point.
Furthermore, the transactions must be capable of resuming executionafter apartial rollback.
In a system where the selection of victims is based primarily on cost factors, it may happen that the same transaction is always picked as a victim.
As a result, this transaction never completes its designated task, thus there is starvation.
We must ensure that a transaction can be picked as a victim only a (small) finite number of times.
The most common solution is to include the number of rollbacks in the cost factor.
There are circumstances, however, where it would be advantageous to group several data items, and to treat them as one individual synchronization unit.
For example, if a transaction Ti needs to access the entire database, and a locking protocol is used, then Ti must lock each item in the database.
It would be better if Ti could issue a single lock request to lock the entire database.
On the other hand, if transaction Tj needs to access only a few data items, it should not be required to lock the entire database, since otherwise concurrency is lost.
What is needed is amechanism to allow the system to definemultiple levels of granularity.
This is done by allowingdata items to be of various sizes anddefining a hierarchy of data granularities, where the small granularities are nested within larger ones.
Such a hierarchy can be represented graphically as a tree.
Note that the tree that we describe here is significantly different from that used by the tree protocol (Section 15.1.5)
In the tree protocol, each node is an independent data item.
As an illustration, consider the tree of Figure 15.15, which consists of four levels of nodes.
Below it are nodes of type area; the database consists of exactly these areas.
Each area in turn has nodes of type file as its children.
Each area contains exactly those files that are its child nodes.
As before, the file consists of exactly those records that are its child nodes, and no record can be present in more than one file.
As we did in the twophase locking protocol, we shall use shared and exclusive lock modes.
When a transaction locks a node, in either shared or exclusive mode, the transaction also has implicitly locked all the descendants of that node in the same lock mode.
For example, if transaction Ti gets an explicit lock on file Fc of Figure 15.15, in exclusive mode, then it has an implicit lock in exclusive mode on all the records belonging to that file.
It does not need to lock the individual records of Fc explicitly.
Suppose that transaction Tj wishes to lock record rb6 of file Fb.
Since Ti has locked Fb explicitly, it follows that rb6 is also locked (implicitly)
If any node in that path is locked in an incompatible mode, then Tj must be delayed.
Suppose now that transaction Tk wishes to lock the entire database.
To do so, it simply must lock the root of the hierarchy.
Note, however, that Tk should not succeed in locking the root node, since Ti is currently holding a lock on part of the tree (specifically, on file Fb)
But how does the system determine if the root node can be locked?One possibility is for it to search the entire tree.
A more efficient way to gain this knowledge is to introduce a new class of lock modes, called intention lock modes.
If a node is locked in an intention mode, explicit locking is done at a lower level of the tree (that is, at a finer granularity)
Intention locks are put on all the ancestors of a node before that node is locked explicitly.
Thus, a transaction does not need to search the entire tree to determine whether it can lock a node successfully.
A transactionwishing to lock a node—say, Q—must traverse a path in the tree from the root to Q.
While traversing the tree, the transaction locks the various nodes in an intention mode.
There is an intention mode associated with shared mode, and there is one with exclusive mode.
If a node is locked in intention-shared (IS) mode, explicit locking is being done at a lower level of the tree, but with only shared-mode locks.
The compatibility function for these lock modes is in Figure 15.16
It requires that a transaction Ti that attempts to lock a nodeQmust follow these rules:
Transaction Ti must observe the lock-compatibility function of Figure 15.16
Transaction Ti must lock the root of the tree first, and can lock it in anymode.
Transaction Ti can lock a node Q in S or IS mode only if Ti currently has the parent of Q locked in either IX or IS mode.
Transaction Ti can lock a node Q in X, SIX, or IX mode only if Ti currently has the parent of Q locked in either IX or SIX mode.
Transaction Ti can lock a node only if Ti has not previously unlocked any node (that is, Ti is two phase)
Transaction Ti can unlock a node Q only if Ti currently has none of the children of Q locked.
As an illustration of the protocol, consider the tree of Figure 15.15 and these transactions:
Suppose that transaction T23 reads all the records in file Fa.
It can do so after locking the database in S mode.
It is particularly useful in applications that include a mix of:
Long transactions that produce reports from an entire file or set of files.
There is a similar locking protocol that is applicable to database systems in which data granularities are organized in the form of a directed acyclic graph.
The locking protocols that we have described thus far determine the order between every pair of conflicting transactions at execution time by the first lock that both members of the pair request that involves incompatible modes.
Another method for determining the serializability order is to select an ordering among transactions in advance.
The most common method for doing so is to use a timestamp-ordering scheme.
With each transaction Ti in the system, we associate a unique fixed timestamp, denoted by TS(Ti )
This timestamp is assigned by the database system before the transaction Ti starts execution.
If a transaction Ti has been assigned timestamp TS(Ti ), and a new transaction Tj enters the system, then TS(Ti ) < TS(Tj )
Use a logical counter that is incremented after a new timestamp has been assigned; that is, a transaction’s timestamp is equal to the value of the counter when the transaction enters the system.
Thus, if TS(Ti ) < TS(Tj ), then the system must ensure that the produced schedule is equivalent to a serial schedule in which transaction Ti appears before transaction Tj.
To implement this scheme, we associatewith each data itemQ two timestamp values:
W-timestamp(Q) denotes the largest timestamp of any transaction that executed write(Q) successfully.
R-timestamp(Q) denotes the largest timestamp of any transaction that executed read(Q) successfully.
These timestamps are updated whenever a new read(Q) or write(Q) instruction is executed.
The timestamp-ordering protocol ensures that any conflicting read and write operations are executed in timestamp order.
If TS(Ti ) < W-timestamp(Q), then Ti needs to read a value of Q that was already overwritten.
Hence, the read operation is rejected, and Ti is rolled back.
If TS(Ti ) < R-timestamp(Q), then the value of Q that Ti is producing was needed previously, and the system assumed that that valuewould never be produced.
Hence, the system rejects the write operation and rolls Ti back.
If TS(Ti )<W-timestamp(Q), then Ti is attempting to write an obsolete value of Q.
Hence, the system rejects this write operation and rolls Ti back.
Otherwise, the system executes the write operation and sets W-timestamp(Q) to TS(Ti )
Transaction T25 displays the contents of accounts A and B:
In presenting schedules under the timestamp protocol, we shall assume that a transaction is assigned a timestamp immediately before its first instruction.
We note that the preceding execution can also be produced by the two-phase locking protocol.
There are, however, schedules that are possible under the twophase locking protocol, but are not possible under the timestamp protocol, and vice versa (see Exercise 15.29)
This is because conflicting operations are processed in timestamp order.
The protocol ensures freedom from deadlock, since no transaction ever waits.
However, there is a possibility of starvation of long transactions if a sequence of conflicting short transactions causes repeated restarting of the long transaction.
If a transaction is suffering from repeated restarts, conflicting transactions need to be temporarily blocked to enable the transaction to finish.
However, it can be extended to make the schedules recoverable, in one of several ways:
Recoverability and cascadelessness can be ensured by performing all writes together at the end of the transaction.
The writes must be atomic in the following sense: While the writes are in progress, no transaction is permitted to access any of the data items that have been written.
Recoverability and cascadelessness can also be guaranteed by using a limited form of locking, whereby reads of uncommitted items are postponed until the transaction that updated the item commits (see Exercise 15.30)
Recoverability alone can be ensured by tracking uncommitted writes, and allowinga transactionTi to commit only after the commit of any transaction that wrote a value that Ti read.
Commit dependencies, outlined in Section 15.1.5, can be used for this purpose.
We now present a modification to the timestamp-ordering protocol that allows greater potential concurrency than does the protocol of Section 15.4.2
Although the rollback of T27 is required by the timestamp-ordering protocol, it is unnecessary.
The protocol rules for write operations, however, are slightly different from the timestamp-ordering protocol of Section 15.4.2
The modification to the timestamp-ordering protocol, called Thomas’ write rule, is this: Suppose that transaction Ti issues write(Q)
If TS(Ti ) < R-timestamp(Q), then the value of Q that Ti is producing was previously needed, and it had been assumed that the value would never be produced.
Hence, the system rejects the write operation and rolls Ti back.
If TS(Ti )<W-timestamp(Q), then Ti is attempting to write an obsolete value of Q.
By ignoring thewrite, Thomas’write rule allows schedules that are not conflict serializable but are nevertheless correct.
Thomas’ write rule makes use of view serializability by, in effect, deleting obsolete write operations from the transactions that issue them.
This modification of transactions makes it possible to generate serializable schedules that would not be possible under the other protocols presented in this chapter.
Under Thomas’ write rule, the write(Q) operation of T27 would be ignored.
In cases where a majority of transactions are read-only transactions, the rate of conflicts among transactions may be low.
It may be better to use an alternative scheme that imposes less overhead.
A difficulty in reducing the overhead is that we do not know in advance which transactions will be involved in a conflict.
To gain that knowledge, we need a scheme for monitoring the system.
The validation protocol requires that each transaction Ti executes in two or three different phases in its lifetime, depending on whether it is a read-only or an update transaction.
There is another form of equivalence that is less stringent than conflict equivalence, but that, like conflict equivalence, is based on only the read and write operations of transactions.
The concept of view equivalence leads to the concept of view serializability.
We say that a schedule S is view serializable if it is view equivalent to a serial schedule.
Indeed, schedule 5 is not conflict serializable, since every pair of consecutive instructions conflicts, and, thus, no swapping of instructions is possible.
Blind writes appear in any view-serializable schedule that is not conflict serializable.
It reads the values of the various data items and stores them in variables local to Ti.
It performs all write operations on temporary local variables, without updates of the actual database.
The validation test (described below) is applied to transaction Ti.
This determines whether Ti is allowed to proceed to the write phase without causing a violation of serializability.
If a transaction fails the validation test, the system aborts the transaction.
If the validation test succeeds for transaction Ti , the temporary local variables that hold the results of any write operations performed by Ti are copied to the database.
Each transactionmust go through thephases in the order shown.However, phases of concurrently executing transactions can be interleaved.
To perform the validation test, we need to know when the various phases of transactions took place.
We shall, therefore, associate three different timestamps with each transaction Ti :
Validation(Ti ), the time when Ti finished its read phase and started its validation phase.
Finish(Ti ), the time when Ti finished its write phase.
We determine the serializability order by the timestamp-ordering technique, using the value of the timestamp Validation(Ti )
Thus, the value TS(Ti ) = Validation(Ti ) and, if TS(Tj ) < TS(Tk), then any produced schedule must be equivalent to a serial schedule in which transaction Tj appears before transaction Tk.
The reason we have chosen Validation(Ti ), rather than Start(Ti ), as the timestamp of transaction Ti is that we can expect faster response time provided that conflict rates among transactions are indeed low.
The validation test for transaction Ti requires that, for all transactions Tk with TS(Tk) < TS(Ti ), one of the following two conditions must hold:
Since Tk completes its execution before Ti started, the serializability order is indeed maintained.
This condition ensures that the writes of Tk and Ti do not overlap.
Since the writes of Tk do not affect the read of Ti , and since Ti cannot affect the read of Tk , the serializability order is indeed maintained.
Note that thewrites to the actual variables are performed only after the validation phase of T26
Thus, T25 reads the old values of B and A, and this schedule is serializable.
The validation scheme automatically guards against cascading rollbacks, since the actual writes take place only after the transaction issuing the write has committed.
However, there is a possibility of starvation of long transactions, due to a sequence of conflicting short transactions that cause repeated restarts of the long transaction.
To avoid starvation, conflicting transactions must be temporarily blocked, to enable the long transaction to finish.
In contrast, locking and timestamp ordering are pessimistic in that they force a wait or a rollback whenever a conflict is detected, even though there is a chance that the schedule may be conflict serializable.
For example, a read operation may be delayed because the appropriate value has not been written yet; or it may be rejected (that is, the issuing transaction must be aborted) because the value that it was supposed to read has already been overwritten.
These difficulties could be avoided if old copies of each data item were kept in a system.
It is also crucial, for performance reasons, that a transaction be able to determine easily and quickly which version of the data item should be read.
The timestamp-ordering protocol can be extended to a multiversion protocol.
With each transaction Ti in the system, we associate a unique static timestamp, denoted by TS(Ti )
The database system assigns this timestamp before the transaction starts execution, as described in Section 15.4
W-timestamp(Qk) is the timestamp of the transaction that created version.
R-timestamp(Qk) is the largest timestampof any transaction that successfully read version Qk.
A transaction—say, Ti—creates a new version Qk of data item Q by issuing a write(Q) operation.
The content field of the version holds the value written by Ti.
The system initializes the W-timestamp and R-timestamp to TS(Ti )
It updates the R-timestamp value of Qk whenever a transaction Tj reads the content of Qk , and R-timestamp(Qk) < TS(Tj )
The scheme operates as follows: Suppose that transaction Ti issues a read(Q) or write(Q) operation.
Let Qk denote the version of Q whose write timestamp is the largest write timestamp less than or equal to TS(Ti )
If transaction Ti issues a read(Q), then the value returned is the content of version Qk.
If transaction Ti issues write(Q), and if TS(Ti ) < R-timestamp(Qk), then the system rolls back transaction Ti.
On the other hand, if TS(Ti ) = Wtimestamp(Qk), the systemoverwrites the contents ofQk ; otherwise (if TS(Ti ) > R-timestamp(Qk)), it creates a new version of Q.
A transaction reads the most recent version that comes before it in time.
The second rule forces a transaction to abort if it is “too late” in doing a write.
More precisely, if Ti attempts to write a version that some other transaction would have read, then we cannot allow that write to succeed.
Versions that are no longer needed are removed according to the following rule: Suppose that there are two versions, Qk and Qj , of a data item, and that both.
Then, the older of the two versions Qk and Qj will not be used again, and can be deleted.
The multiversion timestamp-ordering scheme has the desirable property that a read request never fails and is never made to wait.
In typical database systems, where reading is a more frequent operation than is writing, this advantage may be of major practical significance.
First, the reading of a data item also requires the updating of the R-timestamp field, resulting in two potential disk accesses, rather than one.
Second, the conflicts between transactions are resolved through rollbacks, rather than throughwaits.
This multiversion timestamp-ordering scheme does not ensure recoverability and cascadelessness.
It can be extended in the same manner as the basic timestamp-ordering scheme, to make it recoverable and cascadeless.
The multiversion two-phase locking protocol attempts to combine the advantages of multiversion concurrency control with the advantages of two-phase locking.
Update transactions perform rigorous two-phase locking; that is, they hold all locks up to the end of the transaction.
Thus, they can be serialized according to their commit order.
Each version of a data item has a single timestamp.
The timestamp in this case is not a real clock-based timestamp, but rather is a counter, which we will call the ts-counter, that is incremented during commit processing.
The database system assigns read-only transactions a timestamp by reading the current value of ts-counter before they start execution; they follow the multiversion timestamp-ordering protocol for performing reads.
Thus, when a read-only transaction Ti issues a read(Q), the value returned is the contents of the version whose timestamp is the largest timestamp less than or equal to TS(Ti )
Only one update transaction is allowed to perform commit processing at a time.
As a result, read-only transactions that start after Ti increments ts-counter will see the values updated by Ti , whereas those that start before Ti increments ts-counter will see the value before the updates by Ti.
Multiversion two-phase locking also ensures that schedules are recoverable and cascadeless.
Suppose there are two versions, Qk and Qj , of a data item, and that both versions have a timestamp less than or equal to the timestamp of the oldest read-only transaction in the system.
Then, the older of the two versions Qk and Qj will not be used again and can be deleted.
Here, we take a more detailed look into how it works.
Conceptually, snapshot isolation involves giving a transaction a “snapshot” of the database at the timewhen it begins its execution.
It then operates on that snapshot in complete isolation from concurrent transactions.
The data values in the snapshot consist only of values written by committed transactions.
This isolation is ideal for read-only transactions since they never wait and are never aborted by the concurrency manager.
Transactions that update the database must, of course, interact with potentially conflicting concurrent update transactions before updates are actually placed in the database.
Updates are kept in the transaction’s private workspace until the transaction successfully commits, at which point the updates are written to the database.
When a transaction T is allowed to commit, the transition of T to the committed state and the writing of all of the updates made by T to the database must be done as an atomic action so that any snapshot created for another transaction either includes all updates by transaction T or none of them.
Deciding whether or not to allow an update transaction to commit requires some care.
Potentially, two transactions running concurrently might both update the same data item.
Since these two transactions operate in isolation using their own private snapshots, neither transaction sees the update made by the other.
If both transactions are allowed to write to the database, the first update written will be overwritten by the second.
There are twovariants of snapshot isolation, both ofwhichprevent lost updates.
They are called first committer wins and first updater wins.
Both approaches are based on testing the transaction against concurrent transactions.
A transaction is said to be concurrent with T if it was active or partially committed at any point from the start of T up to and including the timewhen this test is being performed.
Under first committer wins, when a transaction T enters the partially committed state, the following actions are taken in an atomic action:
A test ismade to see if any transaction that was concurrent with T has already written an update to the database for some data item that T intends to write.
If no such transaction is found, then T commits and its updates are written.
This approach is called “first committer wins” because if transactions conflict, the first one to be tested using the above rule succeeds in writing its updates, while the subsequent ones are forced to abort.
Details of how to implement the above tests are addressed in Exercise 15.19
Under first updater wins the system uses a locking mechanism that applies only to updates (reads are unaffected by this, since they do not obtain locks)
When a transaction Ti attempts to update a data item, it requests a write lock on that data item.
If the lock is not held by a concurrent transaction, the following steps are taken after the lock is acquired:
If the item has been updated by any concurrent transaction, then Ti aborts.
Otherwise Ti may proceed with its execution including possibly committing.
If, however, some other concurrent transaction Tj already holds a write lock on that data item, then Ti cannot proceed and the following rules are followed:
This approach is called “first updater wins” because if transactions conflict, the first one to obtain the lock is the one that is permitted to commit and perform its update.
Those that attempt the update later abort unless the first updater subsequently aborts for some other reason.
As an alternative to waiting to see if the first updater Tj aborts, a subsequent updater Ti can be aborted as soon as it finds that the write lock it wishes to obtain is held by Tj.
Snapshot isolation is attractive in practice because the overhead is low and no aborts occur unless two concurrent transactions update the same data item.
There is, however, one serious problemwith the snapshot isolation scheme as we have presented it, and as it is implemented in practice: snapshot isolation does not ensure serializability.
This is true even in Oracle, which uses snapshot isolation.
Suppose that we have two concurrent transactions Ti and Tj and two data items A and B.
For simplicity, we assume there are no other concurrent transactions.
Since Ti and Tj are concurrent, neither transaction sees the update by the other in its snapshot.
There is an edge in the precedence graph from Ti to Tj because Ti reads the value of A that existed before Tj writes A.
There is also an edge in the precedence graph from Tj to Ti because Tj reads the value of B that existed before Ti writes B.
Since there is a cycle in the precedence graph, the result is a nonserializable schedule.
This situation, where each of a pair of transactions has read data that is written by the other, but there is no data written by both transactions, is referred to as write skew.
As a concrete example of write skew, consider a banking scenario.
Suppose that the bank enforces the integrity constraint that the sum of the balances in the checking and the savings account of a customer must not be negative.
Since each of the transactions checks the integrity constraint on its own snapshot, if they run concurrently each will believe that the sum of the balances after the withdrawal is $100, and therefore its withdrawal does not violate the constraint.
Since the two transactions update different data items, they do not have any update conflict, and under snapshot isolation both of them can commit.
It is worth noting that integrity constraints that are enforced by the database, such as primary-key and foreign-key constraints, cannot be checked on a snapshot; otherwise it would be possible for two concurrent transactions to insert two tuples with the same primary key value, or for a transaction to insert a foreign key value that is concurrently deleted from the referenced table.
Instead, the database system must check these constraints on the current state of the database, as part of validation at the time of commit.
For the next example, we shall consider two concurrent update transactions that do not themselves present any problem as regards serializability unless.
Suppose that we have two concurrent transactions Ti and Tj and two data items Aand B.
Since Ti accesses only data item B, there are no conflicts on data item A and therefore there is no cycle in the precedence graph.
The only edge in the precedence graph is the edge from Tj to Ti because Tj reads the value of B that existed before Ti writes B.
However, let us suppose that Ti commits while Tj is still active.
Suppose that, after Ti commits but before Tj commits, a new read-only transaction Tk enters the system and Tk reads both A and B.
Its snapshot includes the update by Ti because Ti has already committed.
However, since Tj has not committed, its update has not yet been written to the database and is not included in the snapshot seen by Tk.
Consider the edges that are added to the precedence graph on account of Tk.
There is an edge in the precedence graph from Ti to Tk because Ti writes the value of B that existed before Tk reads B.
There is an edge in the precedence graph from Tk to Tj because Tk reads the value of A that existed before Tj writes A.
That leads to a cycle in the precedence graph, showing that the resulting schedule is nonserializable.
The above anomalies may not be as troublesome as they first appear.
Recall that the reason for serializability is to ensure that, despite concurrent execution of transactions, database consistency is preserved.
Since consistency is the goal, we can accept the potential for nonserializable executions if we are sure that those nonserializable executions that might occur will not lead to inconsistency.
The second example above is a problem only if the application that submits the read-only transaction (Tk) cares about seeing updates to A and B out of order.
In that example, we did not specify the database consistency constraints that each transaction expects to hold.
If we are dealing with a financial database, it might be a very serious matter for Tk to read updates out of proper serial order.
On the other hand, if Aand B are enrollments in two sections of the same course, then Tk may not demand perfect serialization and we may know from our applications that update rates are low enough that any inaccuracy in what Tk reads is not significant.
The fact that the database must check integrity constraints at the time of commit, and not on a snapshot, also helps avoid inconsistencies in some situations.
Some financial applications create consecutive sequence numbers, for example to number bills, by taking the maximum current bill number and adding 1 to the value to get a new bill number.
If two such transactions run concurrently, each would see the same set of bills in its snapshot, and each would create a new bill with the same number.
Both transactions pass the validation tests for snapshot isolation, since they do not update any tuple in common.
However, the execution is not serializable; the resultant database state cannot be obtained by any serial.
Creating two bills with the same number could have serious legal implications.
The above problem is an example of the phantom phenomenon, since the insert performed by each transaction conflicts with the read performed by the other transaction to find themaximumbill number, but the conflict is not detected by snapshot isolation.1
Luckily, in most such applications the bill number would have been declared as a primary key, and the database systemwould detect the primary key violation outside the snapshot, and roll back one of the two transactions.2
An application developer can guard against certain snapshot anomalies by appending a for update clause to the SQL select query as illustrated below:
Adding the for update clause causes the system to treat data that are read as if they had been updated for purposes of concurrency control.
In our first example of write skew, if the for update clause is appended to the select queries that read the account balances, only one of the two concurrent transactions would be allowed to commit since it appears that both transactions have updated both the checking and savings balances.
In our second example of nonserializable execution, if the author of transaction Tk wished to avoid this anomaly, the for update clause could be appended to the select query, even though there is in fact no update.
In our example, if Tk used select for update, it would be treated as if it had updated A and B when it read them.
The result would be that either Tk or Tj would be aborted, and retried later as a new transaction.
In this example, the queries in the other two transactions do not need the for update clause to be added; unnecessary use of the for update clause can cause significant reduction in concurrency.
Formal methods exist (see the bibliographical notes) to determine whether a given mix of transactions runs the risk of nonserializable execution under snapshot isolation, and to decide on what conflicts to introduce (using the for update clause, for example), to ensure serializability.
Of course, such methods can work only if we know in advance what transactions are being executed.
In some applications, all transactions are from a predetermined set of transactions making this analysis possible.
However, if the application allows unrestricted, ad-hoc transactions, then no such analysis is possible.
The SQL standard uses the term phantom problem to refer to non-repeatable predicate reads, leading some to claim that snapshot isolation avoids the phantom problem; however, such a claim is not valid under our definition of phantom conflict.
The problem of duplicate bill numbers actually occurred several times in a financial application in I.I.T.
Bombay, where (for reasons too complex to discuss here) the bill number was not a primary key, and was detected by financial auditors.
Of the three widely used systems that support snapshot isolation, SQL Server offers the option of a serializable isolation level that truly ensures serializability along with a snapshot isolation level that provides the performance advantages of snapshot isolation (along with the potential for the anomalies discussed above)
In Oracle and PostgreSQL, the serializable isolation level offers only snapshot isolation.
Until now, we have restricted our attention to read and write operations.
This restriction limits transactions to data items already in the database.
Some transactions require not only access to existing data items, but also the ability to create new data items.
To examine how such transactions affect concurrency control, we introduce these additional operations:
An attempt by a transaction Ti to perform a read(Q) operation after Q has been deleted results in a logical error in Ti.
Likewise, an attempt by a transaction Ti to perform a read(Q) operation before Q has been inserted results in a logical error in Ti.
It is also a logical error to attempt to delete a nonexistent data item.
Let Ii and I j be instructions of Ti and Tj , respectively, that appear in schedule S in consecutive order.
If Ii comes before I j , Tj will have a logical error.
If I j comes before Ii , Tj can execute the read operation successfully.
If Ii comes before I j , Tj will have a logical error.
If I j comes before Ii , Tj can execute the write operation successfully.
If Ii comes before I j , Ti will have a logical error.
If I j comes before Ii , Ti will have a logical error.
Suppose that data itemQ did not exist prior to the execution of Ii and I j.
Then, if Ii comes before I j , a logical error results for Ti.
If I j comes before Ii , then no logical error results.
Likewise, ifQ existed prior to the execution of Ii and I j , then a logical error results if I j comes before Ii , but not otherwise.
Under the two-phase locking protocol, an exclusive lock is required on a data item before that item can be deleted.
Under the timestamp-ordering protocol, a test similar to that for a writemust be performed.
Hence, the delete operation is rejected, and Ti is rolled back.
Hence, this delete operation is rejected, and Ti is rolled back.
Wehave already seen that an insert(Q) operation conflicts with adelete(Q) operation.
Similarly, insert(Q) conflictswith a read(Q) operationor awrite(Q) operation; no read or write can be performed on a data item before it exists.
Under the two-phase locking protocol, if Ti performs an insert(Q) operation, Ti is given an exclusive lock on the newly created data item Q.
Consider transaction T30 that executes the following SQL query on the university database:
Transaction T30 requires access to all tuples of the instructor relation pertaining to the Physics department.
Let T31 be a transaction that executes the following SQL insertion:
We expect there to be potential for a conflict for the following reasons:
If concurrency control is performed at the tuple granularity, this conflict would go undetected.
As a result, the system could fail to prevent a nonserializable schedule.
In addition to the phantom problem, we also need to deal with the situation we saw in Section 14.10, where a transaction used an index to find only tuples with dept name = “Physics”, and as a result did not read any tuples with other department names.
If another transaction updates one of these tuples, changing its department name to Physics, a problem equivalent to the phantom problem occurs.
Both problems are rooted in predicate reads, and have a common solution.
To prevent the above problems, we allow transaction T30 to prevent other transactions from creating new tuples in the instructor relation with dept name = “Physics”, and from updating the department name of an existing instructor tuple to Physics.
To find all instructor tuples with dept name = “Physics”, T30 must search either the whole instructor relation, or at least an index on the relation.
Up to now, we have assumed implicitly that the only data items accessed by a transaction are tuples.
Clearly, it is not sufficient merely to lock the tuples that are accessed; the information used to find the tuples that are accessed by the transaction must also be locked.
Locking of information used to find tuples can be implemented by associating a data item with the relation; the data item represents the information used to find the tuples in the relation.
Transactions, such as T30, that read the information about what tuples are in a relation would then have to lock the data item corresponding to the relation in sharedmode.
Transactions, such as T31, that update the information about what tuples are in a relation would have to lock the data item in exclusive mode.
Similarly, transactions that use an index to retrieve tuples must lock the index itself.
By locking the data item, a transaction only prevents other transactions from updating information about what tuples are in the relation.
A transaction that directly accesses a tuple can be granted a lock on the tuples even.
The major disadvantage of locking a data item corresponding to the relation, or locking anentire index, is the lowdegree of concurrency—two transactions that insert different tuples into a relation are prevented from executing concurrently.
A better solution is an index-locking technique that avoids locking the whole index.
Any transaction that inserts a tuple into a relation must insert information into every index maintained on the relation.
We eliminate the phantom phenomenon by imposing a locking protocol for indices.
As we saw in Chapter 11, every search-key value is associated with an index leaf node.
A query will usually use one or more indices to access a relation.
An insert must insert the new tuple in all indices on the relation.
In our example, we assume that there is an index on instructor for dept name.
Then, T31 must modify the leaf containing the key “Physics”
The index-locking protocol takes advantage of the availability of indices on a relation, by turning instances of the phantom phenomenon into conflicts on locks on index leaf nodes.
A transaction Ti can access tuples of a relation only after first finding them.
For the purpose of the index-locking protocol, a relation scan is treated as a scan through all the leaves of one of the indices.
A transaction Ti that performs a lookup (whether a range lookup or a point lookup)must acquire a shared lock on all the index leaf nodes that it accesses.
A transaction Ti may not insert, delete, or update a tuple ti in a relation r without updating all indices on r.
The transaction must obtain exclusive locks on all index leaf nodes that are affected by the insertion, deletion, or update.
For insertion and deletion, the leaf nodes affected are those that contain (after insertion) or contained (before deletion) the search-key value of the tuple.
For updates, the leaf nodes affected are those that (before the modification) contained the old value of the search key, and nodes that (after the modification) contain the new value of the search key.
The rules of the two-phase locking protocol must be observed.
Note that the index-locking protocol does not address concurrency control on internal nodes of an index; techniques for concurrency control on indices, which minimize lock conflicts, are presented in Section 15.10
Locking an index leaf node prevents any update to the node, even if the update did not actually conflict with the predicate.
As noted in Section 14.10, it would appear that the existence of a conflict between transactions depends on a low-level query-processing decision by the system that is unrelated to a user-level view of the meaning of the two transactions.
Inserts and deletes of the relation must then be checked to see if they satisfy the predicate; if they do, there is a lock conflict, forcing the insert or delete to wait till the predicate lock is released.
For updates, both the initial value and the final value of the tuple must be checked against the predicate.
Such conflicting inserts, deletes and updates affect the set of tuples selected by the predicate, and cannot be allowed to execute concurrently with the query that acquired the (shared) predicate lock.
We call the above protocol predicate locking;3 predicate locking is not used in practice since it is more expensive to implement than the index-locking protocol, and does not give significant additional benefits.
In Section 14.5, we discussed the isolation levels specified by the SQL standard: serializable, repeatable read, read committed, and read uncommitted.
In this section, we first briefly outline some older terminology relating to consistency levels weaker than serializability and relate it to the SQL standard levels.
We then discuss the issue of concurrency control for transactions that involve user interaction, an issue that we briefly discussed earlier in Section 14.8
The purpose of degree-two consistency is to avoid cascading aborts without necessarily ensuring serializability.
The locking protocol for degree-two consistency uses the same two lock modes that we used for the two-phase locking protocol: shared (S) and exclusive (X)
A transaction must hold the appropriate lock mode when it accesses a data item, but two-phase behavior is not required.
In contrast to the situation in two-phase locking, S-locks may be released at any time, and locks may be acquired at any time.
The term predicate locking was used for a version of the protocol that used shared and exclusive locks on predicates, and was thus more complicated.
The version we present here, with only shared locks on predicates, is also referred to as precision locking.
Indeed, a transaction may read the same data item twice and obtain different results.
Clearly, reads are not repeatable, but since exclusive locks are held until transaction commit, no transaction can read an uncommitted value.
Thus, degreetwo consistency is one particular implementation of the read-committed isolation level.
Cursor stability is a form of degree-two consistency designed for programs that iterate over tuples of a relation by using cursors.
Instead of locking the entire relation, cursor stability ensures that:
The tuple that is currently being processed by the iteration is locked in shared mode.
Any modified tuples are locked in exclusive mode until the transaction commits.
Cursor stability is used in practice on heavily accessed relations as a means of increasing concurrency and improving system performance.
Applications that use cursor stability must be coded in a way that ensures database consistency despite the possibility of nonserializable schedules.
Thus, the use of cursor stability is limited to specialized situations with simple consistency constraints.
Suppose we treat all the steps from when the seat availability is initially shown to the user, till the seat selection is confirmed, as a single transaction.
If two-phase locking is used, the entire set of seats on a flight would be locked in shared mode till the user has completed the seat selection, and no other transaction would be able to update the seat allocation information in this period.
Clearly such locking would be a very bad idea since a user may take a long time to make a selection, or even just abandon the transaction without explicitly cancelling it.
Timestamp protocols or validation could be used instead, which avoid the problem of locking, but both these protocols would abort the transaction for a user A if any other user B has updated the seat allocation information, even if the seat selected by B does not conflict with the seat selected by user A.
Snapshot isolation is a good option in this situation, since it would not abort the transaction of user A as long as B did not select the same seat as A.
However, snapshot isolation requires the database to remember information about updates performed by a transaction even after it has committed, as long as any other concurrent transaction is still active, which can be problematic for long duration transactions.
Another option is to split a transaction that involves user interaction into two or more transactions, such that no transaction spans a user interaction.
If our seat selection transaction is split thus, the first transaction would read the seat availability, while the second transaction would complete the allocation of the selected seat.
If the second transaction is written carelessly, it could assign the selected seat to the user, without checking if the seat was meanwhile assigned to some other user, resulting in a lost-update problem.
To avoid the problem, as we outlined in Section 14.8, the second transaction should perform the seat allocation only if the seat was not meanwhile assigned to some other user.
The above idea has been generalized in an alternative concurrency control scheme, which uses version numbers stored in tuples to avoid lost updates.
The schema of each relation is altered by adding an extra version number attribute, which is initialized to 0 when the tuple is created.
When a transaction reads (for the first time) a tuple that it intends to update, it remembers the version number of that tuple.
The read is performed as a stand-alone transaction on the database, and hence any locks that may be obtained are released immediately.
Updates are done locally, and copied to the database as part of commit processing, using the following stepswhich are executed atomically (that is, as part of a single database transaction):
For each updated tuple, the transaction checks if the current version number is the same as the version number of the tuple when it was first read by the transaction.
If the version numbers do not match, the transaction is aborted, rolling back all the updates it performed.
If the version number check succeeds for all updated tuples, the transaction commits.
It is worth noting that a timestamp could be used instead of the version number, without impacting the scheme in any way.
Observe the close similarity between the above scheme and snapshot isolation.
However, unlike snapshot isolation, the reads performed by transaction may not correspond to a snapshot of the database; and unlike the validation-based protocol, reads performed by the transaction are not validated.
We refer to the above scheme as optimistic concurrency control without read validation.
Optimistic concurrency control without read validation provides a weak level of serializability, and does not ensure serializability.
The above scheme has been widely used by application developers to handle transactions that involve user interaction.
An attractive feature of the scheme is that it can be implemented easily on top of a database system.
Transactions that involve user interaction are called conversations in Hibernate to differentiate them from regular transactions validation using version numbers is very useful for such transactions.
Object-relational mapping systems also cache database tuples in the formof objects inmemory, and execute transactions on the cachedobjects; updates on the objects are converted into updates on the database when the transaction commits.Datamay remain in cache for a long time, and if transactionsupdate such cached data, there is a risk of lost updates.
Hibernate and other object-relational mapping systems therefore perform the version number checks transparently as part of commit processing.
Hibernate allows programmers to bypass the cache and execute transactions directly on the database, if serializability is desired.
Luckily, indices do not have to be treated like other database structures.
It is perfectly acceptable for a transaction to perform a lookup on an index twice, and to find that the structure of the index has changed in between, as long as the index lookup returns the correct set.
Thus, it is acceptable to have nonserializable concurrent access to an index, as long as the accuracy of the index is maintained.
We outline two techniques for managing concurrent access to B+-trees.
The bibliographical notes reference other techniques for B+-trees, aswell as techniques for other index structures.
The techniques that we present for concurrency control on B+-trees are based on locking, but neither two-phase locking nor the tree protocol is employed.
The algorithms for lookup, insertion, and deletion are those used in Chapter 11, with only minor modifications.
When searching for a key value, the crabbing protocol first locks the root node in shared mode.
When traversing down the tree, it acquires a shared lock on the child node to be traversed further.
After acquiring the lock on the child node, it releases the lock on the parent node.
It repeats this process until it reaches a leaf node.
When inserting or deleting a key value, the crabbing protocol takes these actions:
It follows the same protocol as for searching until it reaches the desired leaf node.
Up to this point, it obtains (and releases) only shared locks.
It locks the leaf node in exclusive mode and inserts or deletes the key value.
If it needs to split a node or coalesce it with its siblings, or redistribute key values between siblings, the crabbing protocol locks the parent of the node in exclusive mode.
After performing these actions, it releases the locks on the node and siblings.
If the parent requires splitting, coalescing, or redistribution of key values, theprotocol retains the lockon theparent, and splitting, coalescing, or redistribution propagates further in the same manner.
The protocol gets its name from the way in which crabs advance by moving sideways, moving the legs on one side, then the legs on the other, and so on alternately.
The progress of locking while the protocol both goes down the tree and goes back up (in case of splits, coalescing, or redistribution) proceeds in a similar crab-like manner.
Once a particular operation releases a lock on a node, other operations can access that node.
There is a possibility of deadlocks between search operations coming down the tree, and splits, coalescing, or redistribution propagating up the tree.
The system can easily handle such deadlocks by restarting the search operation from the root, after releasing the locks held by the operation.
This pointer is required because a lookup that occurswhile a node is being split may have to search not only that node but also that node’s right sibling (if one exists)
We shall illustrate this technique with an example later, but we first present the modified procedures of the B-link-tree locking protocol.
Each node of the B+-tree must be locked in shared mode before it is accessed.
A lock on a nonleaf node is released before any lock on any other node in the B+-tree is requested.
If a split occurs concurrently with a lookup, the desired search-key valuemay no longer appearwithin the range of values represented by a node accessed during lookup.
In such a case, the search-key value is in the range represented by a sibling node, which the system locates by following the pointer to the right sibling.
However, the system locks leaf nodes following the two-phase locking protocol, as Section 15.8.3 describes, to avoid the phantom phenomenon.
The system follows the rules for lookup to locate the leaf node into which it will make the insertion or deletion.
It upgrades the shared-mode lock on this node to exclusivemode, and performs the insertion or deletion.
It locks leaf nodes affected by insertion or deletion following the two-phase locking protocol, as Section 15.8.3 describes, to avoid the phantom phenomenon.
If the transaction splits a node, it creates a new node according to the algorithm of Section 11.3 and makes it the right sibling of the original node.
The right-sibling pointers of both the original node and the new node are set.
Following this, the transaction releases the exclusive lock on the original node (provided it is an internal node; leaf nodes are locked in two-phase manner), and then requests an exclusive lock on the parent, so that it can insert a pointer to the new node.
There is no need to lock or unlock the new node.
If a node has too few search-key values after a deletion, the node with which it will be coalesced must be locked in exclusive mode.
Once the transaction has coalesced these two nodes, it requests an exclusive lock on the parent so that the deleted node can be removed.
At this point, the transaction releases the locks on the coalesced nodes.
Unless the parent node must be coalesced also, its lock is released.
Observe this important fact: An insertion or deletion may lock a node, unlock it, and subsequently relock it.
Furthermore, a lookup that runs concurrently with a split or coalescence operation may find that the desired search key has been moved to the right-sibling node by the split or coalescence operation.
As an illustration, consider the B-link tree in Figure 15.21
Assume that there are two concurrent operations on this B-link tree:
It does a lookup on “Chemistry”, andfinds that the node intowhich “Chemistry” should be inserted is full.
It therefore converts its shared lock on the node to exclusivemode, and creates a new node.
The original node now contains the search-key values “Biology” and “Chemistry”
Now assume that a context switch occurs that results in control passing to the lookup operation.
This lookup operation accesses the root, and follows the pointer to the left child of the root.
It then accesses that node, and obtains a pointer to the left child.
Since this node is currently locked by the insertion operation in exclusive mode, the lookup operation must wait.
Note that, at this point, the lookup operation holds no locks at all!
The insertion operation now unlocks the leaf node and relocks its parent, this time in exclusive mode.
It completes the insertion, leaving the B-link tree as in Figure 15.22
However, it is holding a pointer to an incorrect leaf node.
It therefore follows the right-sibling pointer to locate the next node.
If this node, too, turns out to be incorrect, the lookup follows that node’s right-sibling pointer.
It can be shown that, if a lookup holds a pointer to an incorrect node, then, by following right-sibling pointers, the lookup must eventually reach the correct node.
The lookup would then have to restart from the root.
This solution results in nodes that contain too few search-key values and that violate some properties of B+-trees.
In most databases, however, insertions are more frequent than deletions, so it is likely that nodes that have too few search-key values will gain additional values relatively quickly.
When several transactions execute concurrently in the database, the consistency of data may no longer be preserved.
The most common ones are locking protocols, timestampordering schemes, validation techniques, and multiversion schemes.
A locking protocol is a set of rules that state when a transaction may lock and unlock each of the data items in the database.
The two-phase locking protocol allows a transaction to lock a new data item only if that transaction has not yet unlocked any data item.
In the absence of information concerning the manner in which data items are accessed, the two-phase locking protocol is both necessary and sufficient for ensuring serializability.
The strict two-phase locking protocol permits release of exclusive locks only at the end of transaction, in order to ensure recoverability and cascadelessness.
The rigorous two-phase locking protocol releases all locks only at the end of the transaction.
Graph-based locking protocols impose restrictions on the order in which items are accessed, and can thereby ensure serializability without requiring the use of two-phase locking, and can additionally ensure deadlock freedom.
One way to prevent deadlock is to use an ordering of data items, and to request locks in a sequence consistent with the ordering.
Another way to prevent deadlock is to use preemption and transaction rollbacks.
To control the preemption, we assign a unique timestamp to each transaction.
The system uses these timestamps to decide whether a transaction should wait or roll back.
If a transaction is rolled back, it retains its old timestampwhen restarted.
If deadlocks are not prevented, the system must deal with them by using a deadlock detection and recovery scheme.
A system is in a deadlock state if and only if the wait-for graph contains a cycle.
When the deadlock detection algorithm determines that a deadlock exists, the system must recover from the deadlock.
It does so by rolling back one or more transactions to break the deadlock.
There are circumstances where it would be advantageous to group several data items, and to treat them as one aggregate data item for purposes of working, resulting in multiple levels of granularity.
We allow data items of various sizes, and define a hierarchy of data items, where the small items are nested within larger ones.
Such a hierarchy can be represented graphically as a tree.
Locks are acquired in root-to-leaf order; they are released in leaf-to-root order.
A timestamp-ordering scheme ensures serializability by selecting an ordering in advance between every pair of transactions.
A unique fixed timestamp is associated with each transaction in the system.
Thus, if the timestamp of transaction Ti is smaller than the timestampof transaction Tj , then the scheme ensures that the produced schedule is equivalent to a serial schedule inwhich transaction Ti appears before transaction Tj.
It does so by rolling back a transaction whenever such an order is violated.
A unique fixed timestamp is associated with each transaction in the system.
The serializability order is determined by the timestamp of the transaction.
If it does not pass the validation test, the system rolls it back to its initial state.
When a read operation is issued, the system selects one of the versions to be read.
In multiversion timestamp ordering, a write operation may result in the rollback of the transaction.
In multiversion two-phase locking, write operations may result in a lock wait or, possibly, in deadlock.
Snapshot isolation does not guarantee serializability, but is nevertheless supported bymany database systems.
A delete operation may be performed only if the transaction deleting the tuple has an exclusive lock on the tuple to be deleted.
A transaction that inserts a new tuple into the database is given an exclusive lock on the tuple.
Insertions can lead to the phantom phenomenon, in which an insertion logically conflicts with a query even though the two transactions may access no tuple in common.
Such conflict cannot be detected if locking is done only on tuples accessed by the transactions.
Locking is required on the data used to find the tuples in the relation.
The index-locking technique solves this problem by requiring locks on certain index nodes.
These locks ensure that all conflicting transactions conflict on a real data item, rather than on a phantom.
Weak levels of consistency are used in some applicationswhere consistency of query results is not critical, and using serializability would result in queries adversely affecting transaction processing.
Degree-two consistency is one such weaker level of consistency; cursor stability is a special case of degreetwo consistency, and is widely used.
Concurrency control is a challenging task for transactions that span user interactions.
Applications often implement a scheme based on validation of writes using version numbers stored in tuples; this scheme provides a weak level of serializability, and can be implemented at the application level without modifications to the database.
Often, special techniques are applied in B+-trees to allow greater concurrency.
These techniques allow nonserializable access to the B+-tree, but they ensure that the B+-tree structure is correct, and ensure that accesses to the database itself are serializable.
Can the execution of these transactions result in a deadlock?
Suppose that we insert a dummy vertex between each pair of vertices.
Show that, if we follow the tree protocol on the new tree, we get better concurrency than if we follow the tree protocol on the original tree.
A transaction can be either a read-only transaction, in which case it can request only shared locks, or an update transaction, in which case it can request only exclusive locks.
Each transaction must follow the rules of the tree protocol.
Read-only transactionsmay lock anydata itemfirst,whereas update transactions must lock the root first.
To lock any other vertex, the transaction must be holding a lock on.
To lock any other vertex, the transaction must have visited all the.
Rather, objects (or the corresponding pages) must be locked when the objects are accessed.
Most modern operating systems allow the user to set access protections (no access, read, write) on pages, and memory access that violate the access protections result in a protection violation (see the Unix mprotect command, for example)
Describe how the accessprotection mechanism can be used for page-level locking in a persistent programming language.
The value ofX is not available to the transaction unless the latter executes a read(X)
Figure 15.23 shows a lock-compatibility matrix for three lock modes: share mode, exclusive mode, and incrementation mode.
Show that, if all transactions lock the data that they access in the correspondingmode, then two-phase locking ensures serializability.
Show that the inclusion of increment mode locks allows for increased concurrency.
Provide examples of both situations, and compare the relative amount of concurrency allowed.
Show that by choosing Validation(Ti ), rather than Start(Ti ), as the timestamp of transaction Ti , we can expect better response time, provided that conflict rates among transactions are indeed low.
Give a schedule whereby the timestamp test for a write operation fails and causes the first transaction to be restarted, in turn causing a cascading abort of the other transaction.
Show how this could result in starvation of both transactions.
Such a situation, where two or more processes carry out actions, but are unable to complete their task because of interaction with the other processes, is called a livelock.
Since a split may occur on an insert that affects the root, it appears that an insert operation cannot release any locks until it has completed the entire operation.
Under what circumstances is it possible to release a lock earlier?
A straightforward implementation uses a start timestamp and a commit timestamp for each transaction, in addition to an update set, that is the set of data items updated by the transaction.
The first-updater-wins scheme can be implemented using timestamps as described above, except that validation is done immediately after acquiring an exclusive lock, instead of being done at commit time.
Explain how to assign write timestamps to data items to implement the first-updater-wins scheme.
Show that as a result of locking, if the validation is repeated at.
Explain why there is no need to perform validation and other.
The database is organized as a forest of rooted trees.
The first lock in each tree may be on any data item.
The second, and all subsequent, locks in a tree may be requested only.
A data item may not be relocked by Ti after it has been unlocked by.
Given that SIX and S locks are stronger than IX or IS locks, why does the protocol not allow locking a node in S or IS mode if the parent is locked in either SIX or S mode?
However, unlike the multiversion timestamp-ordering scheme, which guarantees serializability, snapshot isolation does not guarantee serializability.
Explain what is the key difference between the protocols that results in this difference.
Why may this phenomenon lead to an incorrect concurrent execution despite the use of the two-phase locking protocol?
Explain how you can increase concurrency (and throughput) by ordering the operations of the transaction.
Show by an example that this protocol does not guarantee serializability.
Bernstein and Newcomer [1997] provides textbook coverage of various aspects of transaction processing including concurrency control.
The two-phase locking protocol was introduced by Eswaran et al.
Other nontwo-phase locking protocols that operate on more general graphs are described in Yannakakis et al.
Korth [1983] explores various lockmodes that can be obtained from the basic shared and exclusive lock modes.
This approach includes a class of lockmodes called updatemodes to dealwith lock conversion.
An extension of the protocol to ensure deadlock freedom is presented by Korth [1982]
A timestamp algorithm that does not require any rollback to ensure serializability is presented by Buckley and Silberschatz [1983]
The levels of consistency—or isolation—offered in SQL are explained and critiqued in Berenson et al.
Many commercial database systems use version-based approaches in combination with locking.
PostgreSQL, Oracle, and SQL Server all support forms of the snapshot isolation protocol mentioned in Section 15.6.2
A computer system, like any other device, is subject to failure from a variety of causes: disk crash, power outage, software error, a fire in the machine room, even sabotage.
Therefore, the database system must take actions in advance to ensure that the atomicity and durability properties of transactions, introduced in Chapter 14, are preserved.
An integral part of a database system is a recovery scheme that can restore the database to the consistent state that existed before the failure.
The recovery scheme must also provide high availability; that is, it must minimize the time for which the database is not usable after a failure.
There are various types of failure that may occur in a system, each of which needs to be dealt with in a different manner.
In this chapter, we shall consider only the following types of failure:
There are two types of errors thatmay cause a transaction to fail:
The transaction can no longer continue with its normal execution because of some internal condition, such as bad input, data not found, overflow, or resource limit exceeded.
The system has entered an undesirable state (for example, deadlock), as a result of which a transaction cannot continue with its normal execution.
The transaction, however, can be reexecuted at a later time.
The content of nonvolatile storage remains intact, and is not corrupted.
The assumption that hardware errors and bugs in the software bring the system to a halt, but do not corrupt the nonvolatile storage contents, is known as the fail-stop assumption.
Well-designed systems have numerous internal checks, at the hardware and the software level, that bring the system to a halt when there is an error.
A disk block loses its content as a result of either a head crash or failure during a data-transfer operation.
Copies of the data on other disks, or archival backups on tertiary media, such as DVD or tapes, are used to recover from the failure.
To determine how the system should recover from failures, we need to identify the failure modes of those devices used for storing data.
Next, we must consider how these failure modes affect the contents of the database.
We can then propose algorithms to ensure database consistency and transaction atomicity despite failures.
Actions taken during normal transaction processing to ensure that enough information exists to allow recovery from failures.
Actions taken after a failure to recover the database contents to a state that ensures database consistency, transaction atomicity, and durability.
As we saw in Chapter 10, the various data items in the database may be stored and accessed in a number of different storage media.
In Section 14.3, we saw that storage media can be distinguished by their relative speed, capacity, and resilience to failure.
Stable storage or, more accurately, an approximation thereof, plays a critical role in recovery algorithms.
To implement stable storage, we need to replicate the needed information in several nonvolatile storagemedia (usually disk) with independent failuremodes, and to update the information in a controlledmanner to ensure that failure during data transfer does not damage the needed information.
Recall (from Chapter 10) that RAID systems guarantee that the failure of a single disk (even during data transfer) will not result in loss of data.
The simplest and fastest form of RAID is the mirrored disk, which keeps two copies of each block, on separate disks.
Other forms of RAID offer lower costs, but at the expense of lower performance.
However, since tapes cannot be carried off site continually, updates since the most recent time that tapes were carried off site could be lost in such a disaster.
More secure systems keep a copy of each block of stable storage at a remote site, writing it out over a computer network, in addition to storing the block on a local disk system.
Since the blocks are output to a remote system as and when they are output to local storage, once an output operation is complete, the output is not lost, even in the event of a disaster such as a fire or flood.
In the remainder of this section, we discuss how storage media can be protected from failure during data transfer.
A failure occurred in themidst of transfer, and the destination block has incorrect information.
The failure occurred sufficiently early during the transfer that the destination block remains intact.
We require that, if a data-transfer failure occurs, the system detects it and invokes a recovery procedure to restore the block to a consistent state.
To do so, the system must maintain two physical blocks for each logical database block; in the case of mirrored disks, both blocks are at the same location; in the case of remote backup, one of the blocks is local, whereas the other is at a remote site.
When the firstwrite completes successfully,write the same information onto the second physical block.
The output is completed only after the second write completes successfully.
If the system fails while blocks are being written, it is possible that the two copies of a block are inconsistent with each other.
During recovery, for each block, the system would need to examine two copies of the blocks.
If both are the same and no detectable error exists, then no further actions are necessary.
Recall that errors in a disk block, such as a partial write to the block, are detected by storing a checksum with each block.
If the system detects an error in one block, then it.
If both blocks contain no detectable error, but they differ in content, then the system replaces the content of the first block with the value of the second.
This recovery procedure ensures that a write to stable storage either succeeds completely (that is, updates all copies) or results in no change.
The requirement of comparing every corresponding pair of blocks during recovery is expensive to meet.
We can reduce the cost greatly by keeping track of block writes that are in progress, using a small amount of nonvolatile RAM.
On recovery, only blocks for which writes were in progress need to be compared.
We can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage.
Although a large number of copies reduces the probability of a failure to even lower than two copies do, it is usually reasonable to simulate stable storage with only two copies.
Blocks are the units of data transfer to and fromdisk, andmay contain several data items.
We shall assume that no data item spans two or more blocks.
This assumption is realistic for most data-processing applications, such as a bank or a university.
Transactions input information from the disk to main memory, and then output the information back onto the disk.
The input and output operations are done in block units.
The blocks residing on the disk are referred to as physical blocks; the blocks residing temporarily in main memory are referred to as buffer blocks.
The area of memory where blocks reside temporarily is called the disk buffer.
Block movements between disk and main memory are initiated through the following two operations:
Conceptually, each transaction Ti has a private work area in which copies of.
The system creates this work area when the transaction is initiated; the system removes it when the transaction.
There is a special category of database system, called main-memory database systems, where the entire database can be loaded into memory at once.
Each data item X kept in the work area of transaction Ti is denoted by xi.
Transaction Ti interacts with the database system by transferring data to and from its work area to the system buffer.
If block BX on which X resides is not in main memory, it issues input(BX)
It assigns to xi the value of X from the buffer block.
If block BX on which X resides is not in main memory, it issues input(BX)
It assigns the value of xi to X in buffer BX.
Note that both operations may require the transfer of a block from disk to main memory.
They do not, however, specifically require the transfer of a block from main memory to disk.
A buffer block is eventually written out to the disk either because the buffer manager needs the memory space for other purposes or because the database systemwishes to reflect the change to B on the disk.We shall say that the database system performs a force-output of buffer B if it issues an output(B)
When a transaction needs to access a data item X for the first time, it must execute read(X)
The system then performs all updates to X on xi.
At any point during its execution a transaction may execute write(X) to reflect the change to X in the database itself; write(X) must certainly be done after the final write to X.
The output(BX) operation for the buffer block BX onwhich X resides does not need to take effect immediately afterwrite(X) is executed, since the block BX may contain other data items that are still being accessed.
Notice that, if the system crashes after thewrite(X) operation was executed but before output(BX) was executed, the new value ofX is never written to disk and, thus, is lost.
As we shall see shortly, the database system executes extra actions to ensure that updates performed by committed transactions are not lost even if there is a system crash.
Suppose that a systemcrashhas occurredduring the execution of Ti , after output(BA) has taken place, but before output(BB) was executed,where BA and BB denote the buffer blocks on which A and B reside.
Since the memory contents were lost, we do not know the fate of the transaction.
Unfortunately, there is no way to find out by examining the database state what blocks had been output, and what had not, before the crash.
Our goal is to perform either all or no database modifications made by Ti.
However, if Ti performedmultiple database modifications, several output operations may be required, and a failure may occur after some of these modifications have been made, but before all of them are made.
Aswe shall see, this information can help us ensure that all modifications performed by committed transactions are reflected in the database (perhaps during the course of recovery actions after a crash)
This information can also help us ensure that no modifications made by an aborted transaction persist in the database.
The most widely used structure for recording database modifications is the log.
The log is a sequence of log records, recording all the update activities in the database.
In the shadow-copy scheme, a transaction that wants to update the database first creates a complete copy of the database.
All updates are done on the new database copy, leaving the original copy, the shadow copy, untouched.
If at any point the transaction has to be aborted, the systemmerely deletes the new copy.
The old copy of the database has not been affected.
The current copy of the database is identified by a pointer, called db-pointer, which is stored on disk.
If the transaction partially commits (that is, executes its final statement) it is committed as follows: First, the operating system is asked to make sure that all pages of the new copy of the database have been written out to disk.
After the operating system has written all the pages to disk, the database system updates the pointer dbpointer to point to the new copy of the database; the new copy then becomes the current copy of the database.
The transaction is said to have been committed at the point where the updated db-pointer is written to disk.
The implementation actuallydependson thewrite todb-pointer beingatomic; that is, either all its bytes are written or none of its bytes are written.
Disk systems provide atomic updates to entire blocks, or at least to a disk sector.
In other words, the disk system guarantees that it will update db-pointer atomically, as long as we make sure that db-pointer lies entirely in a single sector, which we can ensure by storing db-pointer at the beginning of a block.
Shadow copy schemes are commonly used by text editors (saving the file is equivalent to transaction commit,while quittingwithout saving the file is equivalent to transaction abort)
Shadow copying can be used for small databases, but copying a large database would be extremely expensive.
A variant of shadowcopying, called shadow-paging, reduces copying as follows: the scheme uses a page table containing pointers to all pages; the page table itself and all updated pages are copied to a new location.
Any page which is not updated by a transaction is not copied, but instead the new page table just stores a pointer to the original page.
When a transaction commits, it atomically updates the pointer to the page table, which acts as db-pointer, to point to the new copy.
Shadow paging unfortunately does not work well with concurrent transactions and is not widely used in databases.
Transaction identifier, which is the unique identifier of the transaction that performed the write operation.
Data-item identifier, which is the unique identifier of the data item written.
Typically, it is the location on disk of the data item, consisting of the block identifier of the block on which the data item resides, and an offset within the block.
Old value, which is the value of the data item prior to the write.
New value, which is the value that the data item will have after the write.
Other special log records exist to record significant events during transaction processing, such as the start of a transaction and the commit or abort of a transaction.
We shall introduce several other types of log records later.
Whenever a transaction performs a write, it is essential that the log record.
Once a log record exists, we can output the modification to the database if that is desirable.
Also, we have the ability to undo a modification that has already been output to the database.
We undo it by using the old-value field in log records.
For log records to be useful for recovery from system and disk failures, the log must reside in stable storage.
For now, we assume that every log record is written to the end of the log on stable storage as soon as it is created.
In Section 16.5, we shall see when it is safe to relax this requirement so as to reduce the overhead imposed by logging.
Observe that the log contains a complete record of all database activity.
As a result, the volume of data stored in the log may become unreasonably large.
In Section 16.3.6, we shall show when it is safe to erase log information.
As we noted earlier, a transaction creates a log record prior to modifying the database.
The log records allow the system to undo changesmade by a transaction in the event that the transaction must be aborted; they allow the system also to redo changes made by a transaction if the transaction has committed but the system crashed before those changes could be stored in the database on disk.
In order for us to understand the role of these log records in recovery, we need to consider the steps a transaction takes in modifying a data item:
The transaction performs some computations in its own private part ofmain memory.
The transaction modifies the data block in the disk buffer in main memory holding the data item.
The database systemexecutes theoutput operation thatwrites the data block to disk.
We say a transaction modifies the database if it performs an update on a disk buffer, or on the disk itself; updates to the private part of main memory do not count as database modifications.
As described, they work correctly even with deferred modification, but can be optimized to reduce overhead when used with deferred modification; we leave details as an exercise.
A recovery algorithm must take into account a variety of factors, including:
The possibility that a transaction may have committed although some of its database modifications exist only in the disk buffer in main memory and not in the database on disk.
The possibility that a transaction may have modified the database while in the active state and, as a result of a subsequent failure, may need to abort.
Because all database modifications must be preceded by the creation of a log record, the system has available both the old value prior to the modification of the data item and the new value that is to be written for the data item.
This allows the system to perform undo and redo operations as appropriate.
Undo using a log record sets the data item specified in the log record to the old value.
Redo using a log record sets the data item specified in the log record to the new value.
To avoid such situations, recovery algorithms usually require that if a data item has been modified by a transaction, no other transaction can modify the data item until the first transaction commits or aborts.
This requirement can be ensured by acquiring an exclusive lock on any updated data item and holding the lock until the transaction commits; in other words, by using strict two-phase locking.
We discuss later, in Section 16.7, how the above requirement can be relaxed in certain cases.
However, it is worth noting that some implementations of snapshot isolation use immediate modification, but provide a logical snapshot on demand: when a transaction needs to read an item that a concurrent transaction has updated, a copy of the (already updated) item is made, and updates made by concurrent transactions are rolled back on the copy of the item.
Similarly, immediate modification of the database is a natural fit with two-phase locking, but deferred modification can also be used with two-phase locking.
We say that a transaction has committedwhen its commit log record, which is the last log record of the transaction, has been output to stable storage; at that point all earlier log records have already been output to stable storage.
Thus, there is enough information in the log to ensure that even if there is a system crash, the updates of the transaction can be redone.
Thus, the output of the block containing the commit log record is the single atomic action that results in a transaction getting committed.2
With most log-based recovery techniques, including the ones we describe in this chapter, blocks containing the data items modified by a transaction do not have to be output to stable storage when the transaction commits, but can be output some time later.
The portion of the log containing the relevant information concerning these two transactions appears in Figure 16.2
Using the log, the system can handle any failure that does not result in the loss of information in nonvolatile storage.
Both these proceduresmake use of the log to find the set of data items updated by each transaction Ti , and their respective old and new values.
The order in which updates are carried out by redo is important; when recovering from a system crash, if updates to a particular data item are applied in an order different from the order in which they were applied originally, the final state of that data item will have a wrong value.
Most recovery algorithms, including the one we describe in Section 16.4, do not perform redo of each transaction separately; instead they perform a single scan of the log, during which redo actions are performed for each log record as it is encountered.
In the recovery scheme that we describe in Section 16.4:
The undo operation not only restores the data items to their old value, but also writes log records to record the updates performed as part of the undo process.
These log records are special redo-only log records, since they do not need to contain the old-value of the updated data item.
As with the redo procedure, the order in which undo operations are performed is important; again we postpone details to Section 16.4
As we shall see in Section 16.4, the undo(Ti ) procedure is executed only once for a transaction, if the transaction is rolled back during normal processing or if on recovering from a system crash, neither a commit nor an abort record is found for transaction Ti.
As a result, every transaction will eventually have either a commit or an abort record in the log.
After a system crash has occurred, the system consults the log to determine which transactions need to be redone, and which need to be undone so as to ensure atomicity.
Figure 16.4 The same log, shown at three different times.
Thus, the end result will be to undo Ti ’s modifications in this case.
This slight redundancy simplifies the recovery algorithm and enables faster overall recovery time.
Suppose that the system crashes before the completion of the transactions.
The state of the logs for each of these cases appears in Figure 16.4
First, let us assume that the crash occurs just after the log record for the step:
Next, let us assume that the crash comes just after the log record for the step:
When the system comes back up, two recovery actions need to be taken.
Finally, let us assume that the crash occurs just after the log record:
When a system crash occurs, we must consult the log to determine those transactions that need to be redone and those that need to be undone.
In principle, we need to search the entire log to determine this information.
Most of the transactions that, according to our algorithm, need to be redone have already written their updates into the database.
Although redoing them will cause no harm, it will nevertheless cause recovery to take longer.
We describe below a simple checkpoint scheme that (a) does not permit any.
We discuss later how to modify the checkpointing and recovery procedures to provide more flexibility by relaxing both these requirements.
Output onto stable storage all log records currently residing in main memory.
Transactions are not allowed to perform any update actions, such as writing to a buffer block or writing a log record, while a checkpoint is in progress.
We discuss how this requirement can be enforced, later, in Section 16.5.2
Consider a transaction Ti that completed prior to the checkpoint.
Any database modifications made by Ti must have been written to the database either prior to the checkpoint or as part of the checkpoint itself.
Thus, at recovery time, there is no need to perform a redo operation on Ti.
Note that we need only examine the part of the log starting with the last checkpoint log record to find the set of transactions T , and to find out whether a commit or abort record occurs in the log for each transaction in T.
Consider the set of transactions L in a checkpoint log record.
For each transaction Ti in L, log records of the transaction that occur prior to the checkpoint log record may be needed to undo the transaction, in case it does not commit.
These log records can be erased whenever the database system needs to reclaim the space occupied by these records.
The requirement that transactions must not perform any updates to buffer blocks or to the log during checkpointing can be bothersome, since transaction processing has to halt while a checkpoint is in progress.
A fuzzy checkpoint is a checkpoint where transactions are allowed to perform updates even while buffer blocks are being written out.
Later in Section 16.8 we describe a checkpoint scheme that is not only fuzzy, but does not even require all modified buffer blocks to be output to disk at the time of the checkpoint.
Until now, in discussing recovery, we have identified transactions that need to be redone and those that need to be undone, but we have not given a precise algorithm for performing these actions.
We are now ready to present the full recovery algorithm using log records for recovery from transaction failure and a combination of the most recent checkpoint and log records to recover from a system crash.
The recovery algorithm described in this section requires that a data item that has been updated by an uncommitted transaction cannot be modified by any other transaction, until the first transaction has either committed or aborted.
Recall that this restriction was discussed earlier, in Section 16.3.3
First consider transaction rollback during normal operation (that is, not during recovery froma systemcrash)
The value V1 is written to data item Xj , and.
Such records do not need undo information, since we never need to undo such an undo operation.
Observe that every update action performed by the transaction or on behalf of the transaction, including actions taken to restore data items to their old value, have now been recorded in the log.
In Section 16.4.2 we shall see why this is a good idea.
Recovery actions, when the database system is restarted after a crash, take place in two phases:
In the redo phase, the system replays updates of all transactions by scanning the log forward from the last checkpoint.
The log records that are replayed include log records for transactions that were rolled back before system crash, and those that had not committed when the system crash occurred.
This phase also determines all transactions that were incomplete at the time of the crash, andmust therefore be rolled back.
The specific steps taken while scanning the log are as follows:
At the end of the redo phase, undo-list contains the list of all transactions that are incomplete, that is, they neither committed nor completed rollback before the crash.
In the undo phase, the system rolls back all transactions in the undo-list.
It performs rollback by scanning the log backward from the end.
Whenever it finds a log record belonging to a transaction in the undolist, it performs undo actions just as if the log record had been found during the rollback of a failed transaction.
After the undo phase of recovery terminates, normal transaction processing can resume.
Observe that the redo phase replays every log record since the most recent checkpoint record.
In other words, this phase of restart recovery repeats all the update actions that were executed after the checkpoint, and whose log records reached the stable log.
The actions include actions of incomplete transactions and the actions carried out to rollback failed transactions.
The actions are repeated in the same order in which they were originally carried out; hence, this process is called repeating history.
Although it may appear wasteful, repeating history even for failed transactions simplifies recovery schemes.
Figure 16.5 shows an example of actions logged during normal operation, and actions performed during failure recovery.
Observe how the value of data item B is restored during the rollback of T0
When recovering from a crash, in the redo phase, the system performs a redo of all operations after the last checkpoint record.
Figure 16.5 Example of logged actions, and actions during recovery.
Since undo-list contains no more transactions, the undo phase terminates, completing recovery.
In this section, we consider several subtle details that are essential to the implementation of a crash-recovery scheme that ensures data consistency and imposes a minimal amount of overhead on interactions with the database.
So far, we have assumed that every log record is output to stable storage at the time it is created.
This assumption imposes a high overhead on system execution for several reasons: Typically, output to stable storage is in units of blocks.
In most cases, a log record is much smaller than a block.
Thus, the output of each log record translates to a much larger output at the physical level.
Furthermore, as we saw in Section 16.2.1, the output of a block to stable storage may involve several output operations at the physical level.
The cost of outputting a block to stable storage is sufficiently high that it is desirable to output multiple log records at once.
To do so, we write log records to a log buffer in mainmemory, where they stay temporarily until they are output to stable storage.
Multiple log records can be gathered in the log buffer and output.
The order of log records in the stable storage must be exactly the same as the order in which they were written to the log buffer.
As a result of log buffering, a log record may reside in only main memory (volatile storage) for a considerable time before it is output to stable storage.
Since such log records are lost if the system crashes, we must impose additional requirements on the recovery techniques to ensure transaction atomicity:
Before a block of data in main memory can be output to the database (in nonvolatile storage), all log records pertaining to data in that block must have been output to stable storage.
Strictly speaking, the WAL rule requires only that the undo information in the log has been output to stable storage, and it permits the redo information to be written later.
The difference is relevant in systems where undo information and redo information are stored in separate log records.
The three rules state situations inwhich certain log recordsmust have been output to stable storage.
There is no problem resulting from the output of log records earlier than necessary.
Thus, when the system finds it necessary to output a log record to stable storage, it outputs an entire block of log records, if there are enough log records in main memory to fill a block.
If there are insufficient log records to fill the block, all log records in main memory are combined into a partially full block and are output to stable storage.
Writing the buffered log to disk is sometimes referred to as a log force.
In Section 16.2.2,we described the use of a two-level storage hierarchy.
The system stores the database in nonvolatile storage (disk), and brings blocks of data into main memory as needed.
One might expect that transactions would force-output all modified blocks to disk when they commit.
The alternative, the no-force policy, allows a transaction to commit even if it has modified some blocks that have not yet been written back to disk.
All the recovery algorithms described in this chapter work correctly even with the no-force policy.
As a result, the standard approach taken by most systems is the no-force policy.
Similarly, one might expect that blocks modified by a transaction that is still active should not be written to disk.
The alternative, the steal policy, allows the system to write modified blocks to disk even if the transactions that made those modifications have not all committed.
As long as the write-ahead logging rule is followed, all the recovery algorithms we study in the chapterwork correctly evenwith the steal policy.
Further, the no-steal policy does not work with transactions that perform a large number of updates, since the buffer may get filled with updated pages that cannot be evicted to disk, and the transaction cannot then proceed.
As a result, the standard approach taken by most systems is the steal policy.
Assume that the block onwhich B resides is not in main memory, and that main memory is full.
Suppose that the block on which A resides is chosen to be output to disk.
The system can use the log record during recovery to bring the database back to a consistent state.
It is important that no writes to the block B1 be in progress while the block is being output, since such a write could violate the write-ahead logging rule.
We can ensure that there are no writes in progress by using a special means of locking:
Before a transaction performs a write on a data item, it acquires an exclusive lock on the block in which the data item resides.
The lock is released immediately after the update has been performed.
Output log records to stable storage until all log records pertaining to block B1 have been output.
These locks, and other similar locks that are held for a short duration, are often referred to as latches.
Locks on buffer blocks can also be used to ensure that buffer blocks are not updated, and log records are not generated,while a checkpoint is in progress.
This restriction may be enforced by acquiring exclusive locks on all buffer blocks, as well as an exclusive lock on the log, before the checkpoint operation is performed.
These locks can be released as soon as the checkpoint operation has completed.
Database systems usually have a process that continually cycles through the buffer blocks, outputting modified buffer blocks back to disk.
The above locking protocol must of course be followed when the blocks are output.
As a result of continuous output of modified blocks, the number of dirty blocks in the buffer, that is, blocks that have been modified in the buffer but have not been subsequently output, is minimized.
Thus, the number of blocks that have to be output during a checkpoint is minimized; further, when a block needs to be evicted from the buffer it is likely that there will be a non-dirty block available for eviction, allowing the input to proceed immediately instead of waiting for an output to complete.
We can manage the database buffer by using one of two approaches:
The database system reserves part of main memory to serve as a buffer that it, rather than the operating system,manages.
The database systemmanages data-block transfer in accordance with the requirements in Section 16.5.2
This approach has the drawback of limiting flexibility in the use of main memory.
The buffer must be kept small enough that other applications have sufficient main memory available for their needs.
However, even when the other applications are not running, the database will not be able to make use of all the available memory.
Likewise, non-database applications may not use that part of main memory reserved for the database buffer, even if some of the pages in the database buffer are not being used.
The database system implements its buffer within the virtual memory provided by the operating system.
Since the operating system knows about the memory requirements of all processes in the system, ideally it should be in charge of deciding what buffer blocks must be force-output to disk, and when.
The database system in turn would force-output the buffer blocks to the database, after writing relevant log records to stable storage.
Unfortunately, almost all current-generation operating systems retain complete control of virtual memory.
The operating system reserves space on disk for storing virtual-memory pages that are not currently in main memory; this space is called swap space.
If the operating system decides to output a block Bx, that block is output to the swap space on disk, and there is no way for the database system to get control of the output of buffer blocks.
Therefore, if the database buffer is in virtual memory, transfers between database files and the buffer in virtual memory must be managed by the database system, which enforces the write-ahead logging requirements that we discussed.
This approach may result in extra output of data to disk.
If a block Bx is output by the operating system, that block is not output to the database.
Instead, it is output to the swap space for the operating system’s virtual memory.
When the database system needs to output Bx, the operating systemmay need first to input Bx from its swap space.
Thus, instead of a single output of Bx, there may be two outputs of Bx (one by the operating system, and one by the database system) and one extra input of Bx.
Althoughboth approaches suffer from somedrawbacks, one or the othermust be chosen unless the operating system is designed to support the requirements of database logging.
The checkpointing technique described in Section 16.3.6 requires that all updates to the database be temporarily suspended while the checkpoint is in progress.
If the number of pages in the buffer is large, a checkpoint may take a long time to finish, which can result in an unacceptable interruption in processing of transactions.
To avoid such interruptions, the checkpointing technique can be modified to permit updates to start once the checkpoint record has been written, but before the modified buffer blocks are written to disk.
Since pages are output to disk only after the checkpoint record has been written, it is possible that the system could crash before all pages are written.
One way to deal with incomplete checkpoints is this: The location in the log of the checkpoint record of the last completed checkpoint is stored in a fixed position, last-checkpoint, on disk.
The system does not update this information when it writes the checkpoint record.
Instead, before it writes the checkpoint record, it creates a list of all modified buffer blocks.
The last-checkpoint information is updated only after all buffer blocks in the list of modified buffer blocks have been output to disk.
Even with fuzzy checkpointing, a buffer block must not be updated while it is being output to disk, although other buffer blocks may be updated concurrently.
The write-ahead log protocol must be followed so that (undo) log records pertaining to a block are on stable storage before the block is output.
Until now, we have considered only the case where a failure results in the loss of information residing in volatile storage while the content of the nonvolatile storage remains intact.
Although failures in which the content of nonvolatile storage is lost are rare, we nevertheless need to be prepared to deal with this type of failure.
Our discussions apply as well to other nonvolatile storage types.
The basic scheme is to dump the entire contents of the database to stable storage periodically—say, once per day.
If a failure occurs that results in the loss of physical database blocks, the system uses the most recent dump in restoring the database to a previous consistent state.
Once this restoration has been accomplished, the system uses the log to bring the database system to the most recent consistent state.
One approach to database dumping requires that no transactionmay be active during the dump procedure, and uses a procedure similar to checkpointing:
Output all log records currently residing in main memory onto stable storage.
To recover from the loss of nonvolatile storage, the system restores the database to disk by using the most recent dump.
Then, it consults the log and redoes all the actions since the most recent dump occurred.
In case of a partial failure of nonvolatile storage, such as the failure of a single block or a few blocks, only those blocks need to be restored, and redo actions performed only for those blocks.
A dump of the database contents is also referred to as an archival dump, since we can archive the dumps and use them later to examine old states of the database.
Dumps of a database and checkpointing of buffers are similar.
Most database systems also support an SQL dump, which writes out SQL DDL statements and SQL insert statements to a file, which can then be reexecuted to.
Such dumps are useful when migrating data to a different instance of the database, or to a different version of the database software, since the physical locations and layout may be different in the other database instance or database software version.
The simple dump procedure described here is costly for the following two reasons.
First, the entire database must be copied to stable storage, resulting in considerable data transfer.
Second, since transaction processing is halted during the dump procedure, CPU cycles are wasted.
Fuzzy dump schemes have been developed that allow transactions to be active while the dump is in progress.
Any index used in processing a transaction, such as a B+-tree, can be treated as normal data, but to increase concurrency, we can use the B+-tree concurrencycontrol algorithm described in Section 15.10 to allow locks to be released early, in a non-two-phase manner.
In the rest of this section, we see how to extend the recovery algorithm of Section 16.4 to support early lock release.
The insertion and deletion operations are examples of a class of operations that require logical undo operations since they release locks early; we call such operations logical operations.
Such early lock release is important not only for indices, but also for operations on other system data structures that are accessed and updated very frequently; examples include data structures that track the blocks containing records of a relation, the free space in a block, and the free blocks in a database.
If locks were not released early after performing operations on such data structures, transactions would tend to run serially, affecting system performance.
The theory of conflict serializability has been extended to operations, based on what operations conflict with what other operations.
Recall that an entry consists of a key value and a record identifier, or a key value and a record in the case of the leaf level of a B+-tree file organization.
However, insert and delete operations conflict with other insert and delete operations, as well as with read operations, if they use the same key value.
See the bibliographical notes for references to more information on this topic.
Operations acquire lower-level lockswhile they execute, but release themwhen they complete; the corresponding transaction must however retain a higher-level lock in a two-phase manner to prevent concurrent transactions from executing conflicting actions.
For example, while an insert operation is being performed on a B+-tree page, a short-term lock is obtained on the page, allowing entries in the page to be shifted during the insert; the short-term lock is released as soon as the page has been updated.
Such early lock release allows a second insert to execute on the same page.
However, each transaction must obtain a lock on the key values being inserted or deleted, and retain it in a two-phase manner, to prevent a concurrent transaction from executing a conflicting read, insert or delete operation on the same key value.
Once the lower-level lock is released, the operation cannot be undone byusing the old values of updated data items, and must instead be undone by executing a compensating operation; such an operation is called a logical undo operation.
It is important that the lower-level locks acquired during an operation are sufficient to perform a subsequent logical undo of the operation, for reasons explained later in Section 16.7.4
Thus, theusual old-value andnew-value information is written out as usual for each update performed by the operation; the old-value information is required in case the transaction needs to be rolled back before the operation completes.
For example, if the operation inserted an entry in a B+-tree, the undo informationU would indicate that a deletion operation is to be performed, and would identify the B+-tree and what entry to delete from the tree.
Such logging of information about operations is called logical logging.
In contrast, logging of old-value and new-value information is called physical logging, and the corresponding log records are called physical log records.
Note that in the above scheme, logical logging is used only for undo, not for redo; redo operations are performed exclusively using physical log record.
This is because the state of the database after a system failure may reflect some updates.
The position in the log of the operation-begin log record can be used as the unique identifier.
Data structures such as B+-trees would not be in a consistent state, and neither logical redo nor logical undo operations can be performed on an inconsistent data structure.
To perform logical redo or undo, the database state on disk must be operation consistent, that is, it should not have partial effects of any operation.
However, as we shall see, the physical redo processing in the redo phase of the recovery scheme, along with undo processing using physical log records ensures that the parts of the database accessed by a logical undo operation are in an operation consistent state, before the logical undo operation is performed.
An operation is said to be idempotent if executing it several times in a row gives the same result as executing it once.
Operations such as inserting an entry into a B+-tree may not be idempotent, and the recovery algorithmmust therefore make sure that an operation that has already been performed is not performed again.
On the other hand, a physical log record is idempotent, since the corresponding data item would have the same value regardless of whether the logged update is executed one or multiple times.
When rolling back a transaction Ti , the log is scanned backwards, and log records corresponding to Ti are processed as follows:
Physical log records encountered during the scan are handled as described earlier, except those that are skipped as described shortly.
Incomplete logical operations are undone using the physical log records generated by the operation.
It rolls back the operation by using the undo information U in the log record.
It logs the updates performed during the rollback of the operation just like updates performed when the operation was first executed.
After it finds the operation-begin log record, it processes log records of transaction Ti in the normal manner again.
Observe that the system logs physical undo information for the updates performed during rollback, instead of using a redo-only compensation log records.
Observe also that skipping over physical log records when the operationend log record is found during rollback ensures that the old values in the physical log record are not used for rollback, once the operation completes.
An operation-abort log record would be found only if a transaction that is being rolled back had been partially rolled back earlier.
Recall that logical operations may not be idempotent, and hence a logical undo operation must not be performed multiple times.
These preceding log records must be skipped to prevent multiple rollback of the same operation, in case there had been a crash during an earlier rollback, and the transaction had already been partly rolled back.
If a failure occurs while a logical operation is in progress, the operation-end log record for the operation will not be found when the transaction is rolled back.
However, for every update performed by the operation, undo information—in the form of the old value in the physical log records—is available in the log.
The physical log records will be used to roll back the incomplete operation.
Now suppose an operation undo was in progress when the system crash occurred, which could happen if a transaction was being rolled back when the crash occurred.
Then the physical log records written during operation undo would be found, and the partial operation undo would itself be undone using these physical log records.
Continuing in the backward scan of the log, the original operation’s operation-end record would then be found, and the operation undo would be executed again.
Rolling back the partial effects of the earlier undo operation using the physical log records brings the database to a consistent state, allowing the logical undo operation to be executed again.
Figure 16.6 shows an example of a log generated by two transactions, which add or subtract a value from a data item.
The logical undo operation needs to add or subtract a value from the data item, instead of restoring an old value to the data item.
The annotations on the figure indicate that before an operation completes, rollback can perform physical undo; after the operation completes and releases lower-level locks, the undo must be performed by subtracting or adding a value, instead of restoring the old value.
Figure 16.7 shows an example of recovery from a crash, with logical undo logging.
In the redo pass, the actions of O4 that are after the checkpoint log record are redone.
In this example, there are no other intervening log records, but in general log records from other transactions may be found before we reach the operation-begin log record; such log records should of course not be skipped (unless they are part of a completed operation for the corresponding transaction and the algorithm skips those records)
At this point undo-list is empty, and the undo phase is complete.
As mentioned earlier, it is important that the lower-level locks acquired during an operation are sufficient to perform a subsequent logical undo of the operation; otherwise concurrent operations that execute during normal processing may cause problems in the undo-phase.
Assume also that neither transaction had committed when the system crashed.
If both the original operation and its logical undo operation access a single page (such operations are called physiological operations, and are discussed in Section 16.8), the locking requirement above ismet easily.
Otherwise the details of the specific operation need to be considered when deciding on what lower-level locks need to be obtained.
For example, update operations on a B+-tree could obtain a short-term lock on the root, to ensure that operations execute serially.
See the bibliographical notes for references on B+-tree concurrency control and recovery exploiting logical undo logging.
See the bibliographical notes also for references to an alternative approach, called multi-level recovery, which relaxes this locking requirement.
The state of the art in recovery methods is best illustrated by the ARIES recovery method.
In contrast, ARIES uses a number of techniques to reduce the time taken for recovery, and to reduce the overhead of checkpointing.
In particular, ARIES is able to avoid redoing many logged operations that have already been applied and to reduce the amount of information logged.
The price paid is greater complexity; the benefits are worth the price.
The major differences between ARIES and the recovery algorithm presented earlier are that ARIES:
Uses a log sequence number (LSN) to identify log records, and stores LSNs in database pages to identifywhich operations have been applied to a database page.
Supports physiological redo operations, which are physical in that the affected page is physically identified, but can be logical within the page.
For instance, the deletion of a record from a pagemay result inmany other.
With physical redo logging, all bytes of the page affected by the shifting of records must be logged.
With physiological logging, the deletion operation can be logged, resulting in a much smaller log record.
Redo of the deletion operation would delete the record and shift other records as required.
Uses a dirty page table to minimize unnecessary redos during recovery.
As mentioned earlier, dirty pages are those that have been updated in memory, and the disk version is not up-to-date.
It flushes dirty pages in the background, continuously, instead of writing them during checkpoints.
In the rest of this section, we provide an overview of ARIES.
The bibliographical notes list references that provide a complete description of ARIES.
Each log record in ARIES has a log sequence number (LSN) that uniquely identifies the record.
The number is conceptually just a logical identifier whose value is greater for log records that occur later in the log.
In practice, the LSN is generated in such a way that it can also be used to locate the log record on disk.
Typically, ARIES splits a log into multiple log files, each of which has a file number.
When a log file grows to some limit, ARIES appends further log records to a new log file; the new log file has a file number that is higher by 1 than the previous log file.
The LSN then consists of a file number and an offset within the file.
Whenever an update operation (whether physical or physiological) occurs on apage, the operation stores the LSN of its log record in the PageLSN field of the page.
During the redo phase of recovery, any log records with LSN less than or equal to the PageLSN of a page should not be executed on the page, since their actions are already reflected on the page.
In combination with a scheme for recording PageLSNs as part of checkpointing, which we present later, ARIES can avoid even readingmany pages for which logged operations are already reflected on disk.
The PageLSN is essential for ensuring idempotence in the presence of physiological redo operations, since reapplying a physiological redo that has already been applied to a page could cause incorrect changes to a page.
Pages should not be flushed to disk while an update is in progress, since physiological operations cannot be redone on the partially updated state of the page on disk.
Therefore, ARIES uses latches on buffer pages to prevent them from being written to disk while they are being updated.
It releases the buffer page latch only after the update is completed, and the log record for the update has been written to the log.
Each log record also contains the LSN of the previous log record of the same transaction.
This value, stored in the PrevLSN field, permits log records of a transaction to be fetched backward, without reading the whole log.
There are special redo-only log records generated during transaction rollback, called compensation log records (CLRs) in ARIES.
These serve the same purpose as the redo-only log records in our earlier recovery scheme.
In addition CLRs serve the role of the operation-abort log records in our scheme.
The CLRs have an extra field, called the UndoNextLSN, that records the LSN of the log that needs to be undone next, when the transaction is being rolled back.
This field serves the same purpose as the operation identifier in the operation-abort log record in our earlier recovery scheme, which helps to skip over log records that have already been rolled back.
The DirtyPageTable contains a list of pages that have been updated in the database buffer.
For each page, it stores the PageLSN and a field called the RecLSN, which helps identify log records that have been applied already to the version of the page on disk.
When a page is inserted into the DirtyPageTable (when it is first modified in the buffer pool), the value of RecLSN is set to the current end of log.
Whenever the page is flushed to disk, the page is removed from the DirtyPageTable.
A checkpoint log record contains the DirtyPageTable and a list of active transactions.
For each transaction, the checkpoint log record also notes LastLSN, the LSN of the last log record written by the transaction.
A fixed position on disk also notes the LSN of the last (complete) checkpoint log record.
Figure 16.8 illustrates some of the data structures used in ARIES.
The log records shown in the figure are prefixed by their LSN; these may not be explicitly stored, but inferred from the position in the log, in an actual implementation.
The data item identifier in a log record is shown in two parts, for example 4894.1; the first identifies the page, and the second part identifies a record within the page (we assume a slotted page record organization within a page)
Note that the log is shown with newest records on top, since older log records, which are on disk, are shown lower in the figure.
Each page (whether in the buffer or on disk) has an associated PageLSN field.
By comparing PageLSNs for the pages in the buffer with the PageLSNs for the corresponding pages in stable storage, you can observe that the DirtyPageTable contains entries for all pages in the buffer that have been modified since they were fetched from stable storage.
The RecLSN entry in the DirtyPageTable reflects the LSN at the end of the log when the page was added to DirtyPageTable, and would be greater than or equal to the PageLSN for that page on stable storage.
Analysis pass: This pass determineswhich transactions to undo,which pages were dirty at the time of the crash, and the LSN from which the redo pass should start.
Redo pass: This pass starts from a position determined during analysis, and performs a redo, repeating history, to bring the database to a state it was in before the crash.
Undo pass: This pass rolls back all transactions that were incomplete at the time of crash.
The analysis pass finds the last complete checkpoint log record, and reads in the DirtyPageTable from this record.
It then sets RedoLSN to the minimum of the RecLSNs of the pages in the DirtyPageTable.
If there are no dirty pages, it sets RedoLSN to the LSN of the checkpoint log record.
The redo pass starts its scan of the log from RedoLSN.
All the log records earlier than this point have already been applied to the database pages on disk.
The analysis pass initially sets the list of transactions to be undone, undo-list, to the list of transactions in the checkpoint log record.
The analysis pass also reads from the checkpoint log record the LSNs of the last log record for each transaction in undo-list.
All transactions left in undo-list at the end of analysis have to be rolled back later, in the undo pass.
The analysis pass also keeps track of the last record of each transaction in undo-list, which is used in the undo pass.
The analysis pass also updates DirtyPageTable whenever it finds a log record for an update on a page.
If the page is not in DirtyPageTable, the analysis pass adds it to DirtyPageTable, and sets the RecLSN of the page to the LSN of the log record.
If the page is not in DirtyPageTable or the LSN of the update log record is less than the RecLSN of the page in DirtyPageTable, then the redo pass skips the log record.
Otherwise the redo pass fetches the page from disk, and if the PageLSN is less than the LSN of the log record, it redoes the log record.
Note that if either of the tests is negative, then the effects of the log record have already appeared on the page; otherwise the effects of the log record are not reflected on the page.
Since ARIES allows non-idempotent physiological log records, a log record should not be redone if its effect is already reflected on the page.
If the first test is negative, it is not even necessary to fetch the page from disk to check its PageLSN.
It performs a single backward scan of the log, undoing all transactions in undo-list.
The undo pass examines only log records of transactions in undo-list; the last LSN recorded during the analysis pass is used to find the last log record for each transaction in undo-list.
Whenever an update log record is found, it is used to perform an undo (whether for transaction rollback during normal processing, or during the restart undo pass)
The undo pass generates a CLR containing the undo action performed (which must be physiological)
It sets the UndoNextLSN of the CLR to the PrevLSN value of the update log record.
If a CLR is found, its UndoNextLSN value indicates the LSN of the next log record to be undone for that transaction; later log records for that transaction have already been rolled back.
For log records other than CLRs, the PrevLSN field of the log record indicates the LSN of the next log record to be undone for that transaction.
The next log record to be processed at each stop in the undo pass is the maximum, across all transactions in undo-list, of next log record LSN.
Figure 16.9 illustrates the recovery actions performedbyARIES, on an example log.
The PrevLSN values in the log records are shown using arrows in the figure, while the UndoNextLSN value is shown using a dashed arrow for the one compensation log record, with LSN 7565, in the figure.
Note that this LSN is less than the LSN of the checkpoint log record, since the ARIES checkpointing algorithm does not flush modified pages to stable storage.
At the end of the analysis pass, the list of transactions to be undone consists of only T145 in this example.
The redo pass for the above example starts from LSN 7564 and performs redo of log records whose pages appear in DirtyPageTable.
Nested top actions: ARIES allows the logging of operations that should not be undone even if a transaction gets rolled back; for example, if a transaction allocates a page to a relation, even if the transaction is rolled back the page allocation should not be undone since other transactions may have stored records in the page.
Such operations that should not be undone are called nested topactions.
Recovery independence: Some pages can be recovered independently from others, so that they can be used evenwhile other pages are being recovered.
If some pages of a disk fail, they can be recoveredwithout stopping transaction processing on other pages.
Savepoints: Transactions can record savepoints, and can be rolled back partially, up to a savepoint.
This can be quite useful for deadlock handling, since transactions can be rolled back up to a point that permits release of required locks, and then restarted from that point.
Programmers can also use savepoints to undo a transaction partially, and then continue execution; this approach can be useful to handle certain kinds of errors detected during the transaction execution.
Recovery optimizations: The DirtyPageTable can be used to prefetch pages during redo, instead of fetching a page only when the system finds a log record to be applied to the page.
Out-of-order redo is also possible: Redo can be postponed on a page being fetched from disk, and performed when the page is fetched.
In summary, the ARIES algorithm is a state-of-the-art recovery algorithm, incorporating a variety of optimizations designed to improve concurrency, reduce logging overhead, and reduce recovery time.
Such systems are vulnerable to environmental disasters such as fire, flooding, or earthquakes.
Such systems must provide high availability; that is, the time for which the system is unusable must be extremely small.
We can achieve high availability by performing transaction processing at one site, called the primary site, and having a remote backup site where all the data from the primary site are replicated.
The remote backup site is sometimes also called the secondary site.
The remote backup site must be physically separated from the primary—for example, we can locate it in a different state—so that a disaster at the primary does not damage.
Figure 16.10 shows the architecture of a remote backup system.
When the primary site fails, the remote backup site takes over processing.
First, however, it performs recovery, using its (perhaps outdated) copy of the data from the primary, and the log records received from the primary.
In effect, the remote backup site is performing recovery actions that would have been performed at the primary site when the latter recovered.
Standard recovery algorithms, with minor modifications, can be used for recovery at the remote backup site.
Once recovery has been performed, the remote backup site starts processing transactions.
Availability is greatly increased over a single-site system, since the system can recover even if all data at the primary site are lost.
Several issues must be addressed in designing a remote backup system:
It is important for the remote backup system to detect when the primary has failed.
Failure of communication lines can fool the remote backup into believing that the primary has failed.
To avoid this problem, we maintain several communication links with independent modes of failure between the primary and the remote backup.
For example, several independent network connections, including perhaps a modem connection over a telephone line, may be used.
These connections may be backed up via manual intervention by operators, who can communicate over the telephone system.
When the primary fails, the backup site takes over processing and becomes the new primary.
When the original primary site recovers, it can either play the role of remote backup, or take over the role of primary site again.
In either case, the old primary must receive a log of updates carried out by the backup site while the old primary was down.
The simplest way of transferring control is for the old primary to receive redo logs from the old backup site, and to catch up with the updates by applying them locally.
The old primary can then act as a remote backup site.
If control must be transferred back, the old backup site can pretend to have failed, resulting in the old primary taking over.
If the log at the remote backup grows large, recovery will take a long time.
The remote backup site can periodically process the redo log records that it has received and can perform a checkpoint, so that earlier parts of the log can be deleted.
The delay before the remote backup takes over can be significantly reduced as a result.
A hot-spare configuration can make takeover by the backup site almost instantaneous.
In this configuration, the remote backup site continually processes redo log records as they arrive, applying the updates locally.
As soon as the failure of the primary is detected, the backup site completes recovery by rolling back incomplete transactions; it is then ready to process new transactions.
To ensure that the updates of a committed transaction are durable, a transaction must not be declared committed until its log records have reached the backup site.
This delay can result in a longer wait to commit a transaction, and some systems therefore permit lower degrees of durability.
A transaction commits as soon as its commit log record is written to stable storage at the primary site.
The problem with this scheme is that the updates of a committed transaction may not have made it to the backup site, when the backup site takes over processing.
When the primary site recovers, the lost updates cannot be merged in directly, since the updatesmay conflict with later updates performed at the backup site.
Thus, human intervention may be required to bring the database to a consistent state.
A transaction commits as soon as its commit log record is written to stable storage at the primary and the backup site.
The problem with this scheme is that transaction processing cannot.
Thus, availability is actually less than in the single-site case, although the probability of data loss is much less.
This scheme is the same as two-very-safe if both primary and backup sites are active.
If only the primary is active, the transaction is allowed to commit as soon as its commit log record is written to stable storage at the primary site.
This scheme provides better availability than does two-very-safe, while avoiding the problem of lost transactions faced by the one-safe scheme.
It results in a slower commit than the one-safe scheme, but the benefits generally outweigh the cost.
Several commercial shared-disk systems provide a level of fault tolerance that is intermediate between centralized and remote backup systems.
In these commercial systems, the failure of a CPU does not result in system failure.
Instead, other CPUs take over, and they carry out recovery.
Since data are on a shared disk, there is no need for transfer of log records.
However, we should safeguard the data from disk failure by using, for example, a RAID disk organization.
An alternative way of achieving high availability is to use a distributed database, with data replicated at more than one site.
Transactions are then required to update all replicas of any data item that they update.
A computer system, like any other mechanical or electrical device, is subject to failure.
There are a variety of causes of such failure, including disk crash, power failure, and software errors.
In each of these cases, information concerning the database system is lost.
In addition to system failures, transactions may also fail for various reasons, such as violation of integrity constraints or deadlocks.
An integral part of a database system is a recovery scheme that is responsible for the detection of failures and for the restoration of the database to a state that existed before the occurrence of the failure.
The various types of storage in a computer are volatile storage, nonvolatile storage, and stable storage.
Data in volatile storage, such as in RAM, are lost when the computer crashes.
Data in nonvolatile storage, such as disk, are not lost when the computer crashes, but may occasionally be lost because of failures such as disk crashes.
Stable storage that must be accessible online is approximated with mirrored disks, or other forms of RAID, which provide redundant data storage.
Offline, or archival, stable storage may consist of multiple tape copies of data stored in a physically secure location.
In case of failure, the state of the database systemmay no longer be consistent; that is, it may not reflect a state of the world that the database is supposed to capture.
To preserve consistency, we require that each transaction be atomic.
It is the responsibility of the recovery scheme to ensure the atomicity and durability property.
In log-based schemes, all updates are recorded on a log, which must be kept in stable storage.
A transaction is considered to have committed when its last log record, which is the commit log record for the transaction, has been output to stable storage.
Log records contain old values and new values for all updated data items.
The new values are used in case the updates need to be redone after a system crash.
The old values are used to roll back the updates of the transaction if.
With deferred modification, log records do not need to contain old values of updated data items.
To reduce the overhead of searching the log and redoing transactions, we can use checkpointing techniques.
Modern recovery algorithms are based on the concept of repeating history, whereby all actions taken during normal operation (since the last completed checkpoint) are replayed during the redo pass of recovery.
Repeating history restores the system state to what it was at the time the last log record was output to stable storage before the system crashed.
Undo is then performed from this state, by executing an undo pass that processes log records of incomplete transactions in reverse order.
Undo of an incomplete transaction writes out special redo-only log records, and an abort log record.
After that, the transaction can be considered to have completed, and it will not be undone again.
Transaction processing is based on a storage model in which main memory holds a log buffer, a database buffer, and a system buffer.
The system buffer holds pages of system object code and local work areas of transactions.
Efficient implementation of a recovery scheme requires that the number of writes to the database and to stable storage be minimized.
Log records may be kept in volatile log buffer initially, but must be written to stable storage when one of the following conditions occurs:
Before a block of data in main memory is output to the database (in nonvolatile storage), all log records pertaining to data in that block must have been output to stable storage.
Modern recovery techniques support high-concurrency locking techniques, such as those used for B+-tree concurrency control.
These techniques allow early release of lower-level locks obtained by operations such as inserts or deletes, which allows other such operations to be performed by other transactions.
After lower-level locks are released, physical undo is not possible, and instead logical undo, such as a deletion to undo an insertion, is required.
To recover from failures that result in the loss of nonvolatile storage, wemust dump the entire contents of the database onto stable storage periodicallysay, once per day.
If a failure occurs that results in the loss of physical database blocks, we use the most recent dump in restoring the database to a previous consistent state.
Once this restoration has been accomplished, we use the log to bring the database system to the most recent consistent state.
The ARIES recovery scheme is a state-of-the-art scheme that supports a number of features to provide greater concurrency, reduce logging overheads, and minimize recovery time.
It is also based on repeating history, and allows logical undo operations.
The scheme flushes pages on a continuous basis and does not need to flush all pages at the time of a checkpoint.
It uses log sequence numbers (LSNs) to implement a variety of optimizations that reduce the time taken for recovery.
Remote backup systems provide a high degree of availability, allowing transaction processing to continue even if the primary site is destroyed by a fire, flood, or earthquake.
Data and log records from a primary site are continually backed up to a remote backup site.
How often should checkpoints be performed?Howdoes the frequency of checkpoints affect:
The time it takes to recover from a system crash?
The time it takes to recover from a media (disk) failure?
When can a log record be deleted, in each of these cases, using the recovery algorithm of Section 16.4?
Is the old-value part of an update log record required any more? Why or why not?
If old values are not stored in update log records, transaction undo is clearly not feasible.
How would the redo-phase of recovery have to be modified as a result?
Suggest how to efficiently implement a data item read, ensuring that a transaction sees its own updates.
What problemwould arise with the above technique, if transactions perform a large number of updates?
Suggest how to share as many nodes as possible between the new copy and the shadow-copy of the B+-tree, assuming that updates are made only to leaf entries, with no insertions and deletions.
Even with the above optimization, logging is much cheaper than a shadow-copy scheme, for transactions that perform small updates.
When recovering from a system crash, transactions that were rolled back earlier would then be included in undo-list, and rolled back again.
Give an example to show how actions taken during the undo phase of recovery could result in an incorrect database state.
Hint: Consider a data item updated by an aborted transaction, and then updated by a transaction that commits.
Explain why, and explain how ARIES ensures that such actions are not rolled back.
What problem can occur if the first transaction needs to be rolled back?
Would this problem be an issue if page-level locking is used instead of tuple-level locking?
Suggest how to solve this problem while supporting tuple-level locking, by logging post-commit actions in special log records, and executing them after commit.
Make sure your scheme ensures that such actions are performed exactly once.
Give an example to show that using the normal transaction undo mechanism to undo such a transaction could lead to an inconsistent state.
One way to handle this situation is to bring the whole database to a state prior to the commit of the erroneous transaction (called point-in-time recovery)
Transactions that committed later have their effects rolled back with this scheme.
Suggest a modification to the recovery algorithm of Section 16.4 to implement point-in-time recovery using database dumps.
Later nonerroneous transactions can be re-executed logically, if the updates are available in the form of SQL but cannot be re-executed using their log records.
What effect would the transaction have on recovery time with the.
Suppose there is a crash during recovery, just after before the operation abort log record is written for operation O1
If at the beginning of the analysis pass, a page is not in the checkpoint.
What is RecLSN, and how is it used to minimize unnecessary redos?
Data loss must be avoided but some loss of availability may be.
Transaction commit must be accomplished quickly, even at the cost of loss of some committed transactions in a disaster.
A high degree of availability and durability is required, but a longer running time for the transaction commit protocol is acceptable.
The snapshot view seen by transaction Ti reflects updates of all transactions that had committed when Ti started, and the updates of Ti ; updates of all other transactions are not visible to Ti.
Describe a scheme for buffer handling whereby transactions are given a snapshot view of pages in the buffer.
Include details of how to use the log to generate the snapshot view.
You can assume that operations as well as their undo actions affect only one page.
Gray and Reuter [1993] is an excellent textbook source of information about recovery, including interesting implementation and historical details.
Bernstein and Goodman [1981] is an early textbook source of information on concurrency control and recovery.
An overview of the recovery scheme of System R is presented by Gray et al.
Tutorial and survey papers on various recovery techniques for database systems include Gray [1978], Lindsay et al.
The concepts of fuzzy checkpointing and fuzzy dumps are described in Lindsay et al.
A comprehensive presentation of the principles of recovery is offered by Haerder and Reuter [1983]
The state-of-the-art in recovery methods is best illustrated by the ARIES recovery method, described in Mohan et al.
Mohan and Levine [1992] presents ARIES IM, an extension of ARIES to optimize B+-tree concurrency control and recovery using logical undo logging.
A generalized version of the theory of serializability, with short duration lower-level locks during operations, combined with longer duration higher-level locks, is described by Weikum [1991]
In Section 16.7.3, we saw the requirement that an operation should acquire all lower-level locks that may be needed for the logical undo of the operation.
This requirement can be relaxed by performing all physical undo operations first, before perfoming any logical undo operations.
A generalized version of this idea, calledmulti-level recovery, presented inWeikum et al.
Remote backup algorithms for disaster recovery are presented in King et al.
Database systems can be centralized, where one server machine executes operations on the database.
Database systems can also be designed to exploit parallel computer architectures.
Chapter 17 first outlines the architectures of database systems running on server systems, which are used in centralized and client–server architectures.
The various processes that together implement the functionality of a database are outlined here.
The chapter then outlines parallel computer architectures, and parallel database architectures designed for different types of parallel computers.
Chapter 18 describes how various actions of a database, in particular query processing, can be implemented to exploit parallel processing.
Chapter 19 presents a number of issues that arise in a distributed database, and describes how to deal with each issue.
The issues include how to store data, how to ensure atomicity of transactions that execute at multiple sites, how to perform concurrency control, and how to provide high availability in the presence of failures.a Cloud-based data storage systems, distributed query processing and directory systems are also described in this chapter.
The architecture of a database system is greatly influenced by the underlying computer system on which it runs, in particular by such aspects of computer architecture as networking, parallelism, and distribution:
Networkingof computers allows some tasks tobe executedona server system and some tasks to be executed on client systems.
This division of work has led to client–server database systems.
Parallel processing within a computer system allows database-system activities to be speededup, allowing faster response to transactions, aswell asmore transactions per second.
Queries can be processed in a way that exploits the parallelism offered by the underlying computer system.
The need for parallel query processing has led to parallel database systems.
Distributing data across sites in an organization allows those data to reside where they are generated or most needed, but still to be accessible from other sites and from other departments.
Keeping multiple copies of the database acrossdifferent sites also allows large organizations to continue their database operations even when one site is affected by a natural disaster, such as flood, fire, or earthquake.
Distributed database systems handle geographically or administratively distributed data spread across multiple database systems.
We study the architecture of database systems in this chapter, starting with the traditional centralized systems, and covering client–server, parallel, and distributed database systems.
Centralized database systems are those that run on a single computer system and do not interact with other computer systems.
Such database systems span a range from single-user database systems running on personal computers to high-performance database systems running on high-end server systems.
Amodern, general-purpose computer system consists of one to a few processors and a number of device controllers that are connected through a common bus that provides access to shared memory (Figure 17.1)
The processors have local cache memories that store local copies of parts of thememory, to speedup access to data.
Each processor may have several independent cores, each of which can execute a separate instruction stream.
Each device controller is in charge of a specific type of device (for example, a disk drive, an audio device, or a video display)
The processors and the device controllers can execute concurrently, competing for memory access.
Cache memory reduces the contention for memory access, since it reduces the number of times that the processor needs to access the shared memory.
We distinguish twoways inwhich computers are used: as single-user systems and as multiuser systems.
A typical single-user system is a desktop unit used by a single person, usually with only one processor and one or two hard disks, and usually only one person using the machine at a time.
A typical multiuser system, on the other hand, has more disks and more memory and may have multiple processors.
It serves a large number of users who are connected to the system remotely.
Database systems designed for use by single users usually do not provide many of the facilities that a multiuser database provides.
In particular, they may not support concurrency control, which is not required when only a single user can generate updates.
Provisions for crash recovery in such systems are either absent or primitive—for example, they may consist of simply making a backup of the database before any update.
Some such systems do not support SQL, and they provide a simpler query language, such as a variant of QBE.
Althoughmost general-purpose computer systems inuse todayhavemultiple processors, they have coarse-granularity parallelism, with only a few processors (about two to four, typically), all sharing the main memory.
Databases running on such machines usually do not attempt to partition a single query among the processors; instead, they run each query on a single processor, allowing multiple queries to run concurrently.
Thus, such systems support a higher throughput; that is, they allow a greater number of transactions to run per second, although individual transactions do not run any faster.
Thus, coarse-granularity parallel machines logically appear to be identical to singleprocessor machines, and database systems designed for time-shared machines can be easily adapted to run on them.
In contrast, machines with fine-granularity parallelism have a large number of processors, and database systems running on such machines attempt to parallelize single tasks (queries, for example) submitted by users.
We study the architecture of parallel database systems in Section 17.3
Parallelism is emerging as a critical issue in the future design of database systems.
Whereas today those computer systems with multicore processors have only a few cores, future processors will have large numbers of cores.1 As a result, parallel database systems, which once were specialized systems running on specially designed hardware, will become the norm.
As personal computers became faster, more powerful, and cheaper, there was a shift away from the centralized system architecture.
Correspondingly, personal computers assumed the user-interface functionality that used to be handled directly by the centralized systems.
As a result, centralized systems today act as server systems that satisfy requests generated by client systems.
Figure 17.2 shows the general structure of a client–server system.
Functionality provided by database systems can be broadly divided into two parts—the front end and the back end.
The back end manages access structures, query evaluation and optimization, concurrency control, and recovery.
The front end of a database system consists of tools such as the SQL user interface, forms interfaces, report generation tools, and data mining and analysis tools (see Figure 17.3)
The interface between the front end and the back end is through SQL, or through an application program.
The reasons for this pertain to issues in computer architecture related to heat generation and power consumption.
Rather than make processors significantly faster, computer architects are using advances in chip design to put more cores on a single chip, a trend likely to continue for some time.
Standards suchasODBC and JDBC,whichwe saw inChapter 3,weredeveloped to interface clients with servers.
Any client that uses the ODBC or JDBC interface can connect to any server that provides the interface.
In effect, they provide front ends specialized for particular tasks.
The application server, in effect, acts as a client to the database server.
These calls appear like ordinary procedure calls to the programmer, but all the remote procedure calls from a client are enclosed in a single transaction at the server end.
Thus, if the transaction aborts, the server can undo the effects of the individual remote procedure calls.
Server systems can be broadly categorized as transaction servers anddata servers.
Transaction-server systems, also called query-server systems, provide an interface to which clients can send requests to perform an action, in response to which they execute the action and send back results to the client.
Usually, client machines ship transactions to the server systems, where those transactions are executed, and results are shipped back to clients that are in charge of displaying the data.
Requests may be specified by using SQL, or through a specialized application program interface.
Data-server systems allow clients to interact with the servers by making requests to read or update data, in units such as files or pages.
For example, file servers provide a file-system interface where clients can create, update, read, and delete files.
Data servers for database systems offer much more functionality; they support units of data—such as pages, tuples, or objects —that are smaller than a file.
They provide indexing facilities for data, and provide transaction facilities so that the data are never left in an inconsistent state if a client machine or process fails.
A typical transaction-server system today consists ofmultiple processes accessing data in shared memory, as in Figure 17.4
The processes that form part of the database system include:
Server processes: These are processes that receive user queries (transactions), execute them, and send the results back.
The queries may be submitted to the server processes from a user interface, or from a user process running embedded SQL, or via JDBC, ODBC, or similar protocols.
Some database systems use a separate process for each user session, and a few use a single database process for all user sessions, but with multiple threads so that multiple queries can execute concurrently.
A thread is like a process, but multiple threads execute as part of the same process, and all threads within a process run in the same virtual-memory space.
Many database systems use a hybrid architecture, with multiple processes, each one running multiple threads.
Lock manager process: This process implements lock manager functionality, which includes lock grant, lock release, and deadlock detection.
Database writer process: There are one or more processes that output modified buffer blocks back to disk on a continuous basis.
Log writer process: This process outputs log records from the log record buffer to stable storage.
Process monitor process: This process monitors other processes, and if any.
Log buffer, containing log records waiting to be output to the log on stable.
Cached query plans, which can be reused if the same query is submitted again.
All database processes can access the data in shared memory.
Since multiple processes may read or perform updates on data structures in shared memory, there must be a mechanism to ensure that a data structure is modified by at most one process at a time, and no process is reading a data structure while it is being written by others.
Such mutual exclusion can be implemented by means of operating system functions called semaphores.
Alternative implementations, with less overhead, use special atomic instructions supported by the computer hardware; one type of atomic instruction tests a memory location and sets it to 1 atomically.
Further implementation details of mutual exclusion can be found in any standard operating system textbook.
The mutual exclusion mechanisms are also used to implement latches.
To avoid the overhead of message passing, in many database systems, server processes implement locking by directly updating the lock table (which is in shared memory), instead of sending lock request messages to a lock manager process.
The lock request procedure executes the actions that the lock manager process would take on getting a lock request.
The actions on lock request and release are like those in Section 15.1.4, but with two significant differences:
If a lock cannot be obtained immediately because of a lock conflict, the lock request code may monitor the lock table to check when the lock has been granted.
The lock release code updates the lock table to note which process has been granted the lock.
To avoid repeated checks on the lock table, operating system semaphores can be used by the lock request code to wait for a lock grant notification.
The lock release code must then use the semaphore mechanism to notify waiting transactions that their locks have been granted.
Even if the system handles lock requests through shared memory, it still uses the lock manager process for deadlock detection.
Data-server systems are used in local-area networks, where there is a high-speed connection between the clients and the server, the clientmachines are comparable in processing power to the server machine, and the tasks to be executed are computation intensive.
In such an environment, it makes sense to ship data to client machines, to perform all processing at the client machine (which may take a while), and then to ship the data back to the server machine.
Note that this architecture requires full back-end functionality at the clients.
Data-server architectures have been particularly popular in object-oriented database systems (Chapter 22)
Interesting issues arise in such an architecture, since the time cost of communication between the client and the server is high compared to that of a local memory reference (milliseconds, versus less than 100 nanoseconds):
The unit of communication for data can be of coarse granularity, such as a page, or fine granularity, such as a tuple (or an object, in the context of object-oriented database systems)
We use the term item to refer to both tuples and objects.
If the unit of communication is a single item, the overhead of message passing is high compared to the amount of data transmitted.
Instead,when an item is requested, it makes sense also to send back other items that are likely to be used in the near future.
Fetching items even before they are requested is called prefetching.
Page shipping can be considered a form of prefetching if multiple items reside on a page, since all the items in the page are shipped when a process desires to access a single item in the page.
Locks are usually granted by the server for the data items that it ships to the client machines.
A disadvantage of page shipping is that client machines may be granted locks of too coarse a granularity —a lock on a page implicitly locks all items contained in the page.
Even if the client is not accessing some items in the page, it has implicitly acquired locks on all prefetched items.
Other client machines that require locks on those items may be blocked unnecessarily.
Techniques for lock de-escalation have been proposed where the server can request its clients to transfer back locks on prefetched items.
If the client machine does not need a prefetched item, it can transfer locks on the item back to the server, and the locks can then be allocated to other clients.
Data that are shipped to a client on behalf of a transaction can be cached at the client, even after the transaction completes, if sufficient storage space is available.
Successive transactions at the same client may be able to make use of the cached data.
However, cache coherency is an issue: Even if a transaction finds cached data, it must make sure that those data are up to date, since they may have been updated by a different client after they were cached.
Thus, a message must still be exchanged with the server to check validity of the data, and to acquire a lock on the data.
If the use of data is mostly partitioned among the clients, with clients rarely requestingdata that are also requestedby other clients, locks can also be cached at the client machine.
Suppose that a client finds a data item in the cache, and that it also finds the lock required for an access to the data item in the cache.
Then, the access can proceed without any communication with the server.
However, the server must keep track of cached locks; if a client requests a lock from the server, the server must call back all conflicting locks on the data item from any other client machines that have cached the locks.
The task becomes more complicated when machine failures are taken into account.
This technique differs from lock de-escalation in that lock caching takes place across transactions; otherwise, the two techniques are similar.
Servers are usually owned by the enterprise providing the service, but there is an increasing trend for service providers to rely at least in part upon servers that are owned by a “third party” that is neither the client nor the service provider.
One model for using third-party servers is to outsource the entire service to another company that hosts the service on its own computers using its own software.
This allows the service provider to ignore most details of technology and focus on the marketing of the service.
Another model for using third-party servers is cloud computing, in which the service provider runs its own software, but runs it on computers provided by another company.
Under this model, the third party does not provide any of the application software; it provides only a collection of machines.
These machines are not “real”machines, but rather simulated by software that allows a single real computer to simulate several independent computers.
The service provider runs its software (possibly including a database system) on these virtual machines.
A major advantage of cloud computing is that the service provider can add machines as needed to meet demand and release them at times of light load.
This can prove to be highly cost-effective in terms of both money and energy.
A thirdmodel uses a cloud computing service as adata server; such cloud-based data storage systems are covered in detail in Section 19.9
Database applications using cloud-based storage may run on the same cloud (that is, the same set of machines), or on another cloud.
Parallel systems improve processing and I/O speeds by usingmultiple processors and disks in parallel.
Parallel machines are becoming increasingly common, making the study of parallel database systems correspondingly more important.
The driving force behind parallel database systems is the demands of applications that have to query extremely large databases (of the order of terabytes—that is, 1012 bytes) or that have to process an extremely large number of transactions per second (of the order of thousands of transactions per second)
Centralized and client –server database systems are not powerful enough to handle such applications.
Virtually all high-end machines today offer some degree of coarse-grain parallelism: at least two or four processors.
Parallel computers with hundreds of processors and disks are available commercially.
A system that processes a large number of small transactions can improve throughput by processing many transactions in parallel.
A system that processes large transactions can improve response time as well as throughput by performing subtasks of each transaction in parallel.
Two important issues in studying parallelism are speedup and scaleup.
Running a given task in less time by increasing the degree of parallelism is called speedup.
Handling larger tasks by increasing the degree of parallelism is called scaleup.
Consider a database application running on a parallel system with a certain number of processors and disks.
Now suppose that we increase the size of the system by increasing the number of processors, disks, and other components of the system.
The goal is to process the task in time inversely proportional to the number of processors and disks allocated.
Suppose that the execution time of a task on the larger machine is TL , and that the execution time of the same task on the smallermachine is TS.
The parallel system is said to demonstrate linear speedup if the speedup is N when the larger system has N times the resources (processors, disk, and so on) of the smaller system.
If the speedup is less than N, the system is said to demonstrate sublinear speedup.
Scaleup relates to the ability to process larger tasks in the same amount of time by providing more resources.
Let Q be a task, and let QN be a task that is N times bigger than Q.
Suppose that the execution time of task Q on a given machine.
The parallel system ML is said to demonstrate linear scaleup on task Q if TL = TS.
If TL > TS, the system is said to demonstrate sublinear scaleup.
Figure 17.6 illustrates linear and sublinear scaleups (where the resources increase in proportion to problem size)
There are two kinds of scaleup that are relevant in parallel database systems, depending on how the size of the task is measured:
Inbatch scaleup, the size of the database increases, and the tasks are large jobs whose runtimedepends on the size of the database.An example of such a task is a scan of a relation whose size is proportional to the size of the database.
Thus, the size of the database is the measure of the size of the problem.
Batch scaleup also applies in scientific applications, such as executing a query at an N-times finer resolution or performing an N-times longer simulation.
In transaction scaleup, the rate at which transactions are submitted to the database increases and the size of the database increases proportionally to the transaction rate.
Such transaction processing is especially well adapted for parallel execution, since transactions can run concurrently and independently on separate processors, and each transaction takes roughly the same amount of time, even if the database grows.
Scaleup is usually the more important metric for measuring efficiency of parallel database systems.
The goal of parallelism in database systems is usually to make sure that the database system can continue to perform at an acceptable speed, even as the size of the database and the number of transactions increases.
Increasing the capacity of the system by increasing the parallelism provides a smoother path for growth for an enterprise than does replacing a centralized.
However, we must also look at absolute performance numbers when using scaleup measures; a machine that scales up linearly may perform worse than a machine that scales less than linearly, simply because the latter machine is much faster to start off with.
There is a start-up cost associated with initiating a single process.
In a parallel operation consisting of thousands of processes, the start-up time may overshadow the actual processing time, affecting speedup adversely.
Since processes executing in a parallel system often access shared resources, a slowdown may result from the interference of each new process as it competes with existing processes for commonly held resources, such as a systembus, or shareddisks, or even locks.
By breaking down a single task into a number of parallel steps, we reduce the size of the average step.
Nonetheless, the service time for the single slowest step will determine the service time for the task as a whole.
It is often difficult to divide a task into exactly equal-sized parts, and the way that the sizes are distributed is therefore skewed.
Parallel systems consist of a set of components (processors, memory, and disks) that can communicate with each other via an interconnection network.
Figure 17.7 shows three commonly used types of interconnection networks:
All the system components can send data on and receive data from a single communication bus.
The bus could be an Ethernet or a parallel interconnect.
However, they do not scale well with increasing parallelism, since the bus can handle communication from only one component at a time.
The components are nodes in a grid, and each component connects to all its adjacent components in the grid.
Nodes that are not directly connected can communicate with one another by routing messages via a sequence of intermediate nodes that are directly connected to one another.
The number of communication links grows as the number of components grows, and the communication capacity of a mesh therefore scales better with increasing parallelism.
The components are numbered in binary, and a component is connected to another if the binary representations of their numbers differ in exactly one bit.
Thus, each of the n components is connected to log(n) other components.
In a hypercube interconnection, a message from a component can reach any other component by going through at most log(n) links.
Thus communication delays in a hypercube are significantly lower than in a mesh.
Among the most prominent ones are those in Figure 17.8 (in the figure, M denotes memory, P denotes a processor, and disks are shown as cylinders):
All the processors share a common set of disks (Figure 17.8b)
The processors share neither a common memory nor common disk (Figure 17.8c)
This model is a hybrid of the preceding three architectures (Figure 17.8d)
Techniques used to speed up transaction processing on data-server systems, such as data and lock caching and lock de-escalation, outlined in Section 17.2.2, can also be used in shared-disk parallel databases as well as in shared-nothing parallel databases.
In fact, they are very important for efficient transaction processing in such systems.
In a shared-memory architecture, the processors and disks have access to a common memory, typically via a bus or through an interconnection network.
The benefit of shared memory is extremely efficient communication between processors—data in shared memory can be accessed by any processor without being moved with software.
A processor can send messages to other processors much faster by using memory writes (which usually take less than a microsecond) than by sending a message through a communication mechanism.
Adding more processors does not help after a point, since the processors will spend most of their time waiting for their turn on the bus to access memory.
Shared-memory architectures usually have large memory caches at each processor, so that referencing of the shared memory is avoided whenever possible.
However, at least some of the data will not be in the cache, and accesses will have to go to the shared memory.
Moreover, the caches need to be kept coherent; that is, if a processor performs a write to a memory location, the data in that memory location should be either updated at or removed from any processor where the data are cached.
Maintaining cache coherency becomes an increasing overhead with increasing numbers of processors.
Consequently, shared-memory machines are not capable of scaling up beyond a point; current shared-memory machines cannot support more than 64 processors.
In the shared-disk model, all processors can access all disks directly via an interconnection network, but the processors have private memories.
There are two advantages of this architecture over a shared-memory architecture.
First, since each processor has its own memory, the memory bus is not a bottleneck.
Second, it offers a cheap way to provide a degree of fault tolerance: If a processor (or its memory) fails, the other processors can take over its tasks, since the database is resident on disks that are accessible from all processors.
The main problem with a shared-disk system is again scalability.
Although the memory bus is no longer a bottleneck, the interconnection to the disk subsystem is now a bottleneck; it is particularly so in a situation where the database makes a large number of accesses to disks.
Compared to shared-memory systems, shared-disk systems can scale to a somewhat larger number of processors, but communication across processors is slower (up to a few milliseconds in the absence of special-purpose hardware for communication), since it has to go through a communication network.
In a shared-nothing system, each node of the machine consists of a processor, memory, and one or more disks.
The processors at one node may communicate with another processor at another node by a high-speed interconnection network.
A node functions as the server for the data on the disk or disks that the node owns.
Moreover, the interconnection networks for shared-nothing systems are usually designed to be scalable, so that their transmission capacity increases as more nodes are added.
Consequently, shared-nothing architectures are more scalable and can easily support a large number of processors.
The main drawbacks of shared-nothing systems are the costs of communication and of nonlocal disk access, which are higher than in a shared-memory or shared-disk architecture since sending data involves software interaction at both ends.
The hierarchical architecture combines the characteristics of shared-memory, shared-disk, and shared-nothing architectures.
At the top level, the system consists of nodes that are connected by an interconnection network and do not share disks or memory with one another.
Each node of the system could actually be a shared-memory system with a few processors.
Alternatively, each node could be a shared-disk system, and each of the systems sharing a set of disks could be a shared-memory system.
Thus, a system could be built as a hierarchy, with shared-memory architecture with a few processors at the base, and a shared-nothing architecture at the top, with possibly a shared-disk architecture in the middle.
Figure 17.8d illustrates a hierarchical architecture with shared-memory nodes connected together in a shared-nothing architecture.
Commercial parallel database systems today run on several of these architectures.
Since access speeds differ, depending on whether the page is available locally or not, such an architecture is also referred to as a nonuniform memory architecture (NUMA)
In a distributed database system, the database is stored on several computers.
The computers in a distributed system communicate with one another through various communication media, such as high-speed private networks or the Internet.
The computers in a distributed systemmayvary in size and function, ranging fromworkstationsup tomainframe systems.
The computers in a distributed system are referred to by a number of different names, such as sites or nodes, depending on the context in which they are mentioned.
We mainly use the term site, to emphasize the physical distribution of these systems.
The general structure of a distributed system appears in Figure 17.9
The main differences between shared-nothing parallel databases and distributed databases are that distributed databases are typically geographically separated, are separately administered, and have a slower interconnection.
Another major difference is that, in a distributed database system, we differentiate between local and global transactions.
A local transaction is one that accesses data only from sites where the transaction was initiated.
A global transaction, on the other hand, is one that either accesses data in a site different from the one at which the transaction was initiated, or accesses data in several different sites.
There are several reasons for building distributed database systems, including sharing of data, autonomy, and availability.
Themajor advantage in building a distributed database system is the provision of an environment where users at one site may be able to access the data residing at other sites.
For instance, in a distributed university system, where each campus stores data related to that campus, it is possible for a user in one campus to access data in another campus.
Without this capability, the transfer of student records from one campus to another campus would have to resort to some external mechanism that would couple existing systems.
The primary advantage of sharing data by means of data distribution is that each site is able to retain a degree of control over data that are stored locally.
In a centralized system, the database administrator of the central site controls the database.
In a distributed system, there is a global database administrator responsible for the entire system.
A part of these responsibilities is delegated to the local database administrator for each site.
Depending on the design of the distributed database system, each administrator may have a different degree of local autonomy.
The possibility of local autonomy is often a major advantage of distributed databases.
If one site fails in a distributed system, the remaining sites may be able to continue operating.
In particular, if data items are replicated in several sites, a transaction needing a particular data item may find that item in any of several sites.
Thus, the failure of a site does not necessarily imply the shutdown of the system.
The failure of one site must be detected by the system, and appropriate action may be needed to recover from the failure.
The systemmust no longer use the services of the failed site.
Finally, when the failed site recovers or is repaired, mechanisms must be available to integrate it smoothly back into the system.
Availability is crucial for database systems used for real-time applications.
Loss of access to data by, for example, an airline may result in the loss of potential ticket buyers to competitors.
Consider a banking system consisting of four branches in four different cities.
Each branch has its own computer, with a database of all the accounts maintained at that branch.
There also exists one single site that maintains information about all the branches of the bank.
If the transactionwas initiated at the Valleyview branch, then it is considered local; otherwise, it is considered global.
If a distributed database is built from scratch, it would indeed be possible to achieve the above goals.
Such systems are sometimes called multidatabase systems or heterogeneous distributed database systems.
We discuss these systems in Section 19.8, where we show how to achieve a degree of global control despite the heterogeneity of the component systems.
Atomicity of transactions is an important issue in building a distributed database system.
If a transaction runs across two sites, unless the system designers are careful, it may commit at one site and abort at another, leading to an inconsistent state.
The two-phase commit protocol (2PC) is the most widely used of these protocols.
The basic idea behind 2PC is for each site to execute the transaction until it enters the partially committed state, and then leave the commit decision to a single coordinator site; the transaction is said to be in the ready state at a site at this point.
The coordinator decides to commit the transaction only if the transaction reaches the ready state at every site where it executed; otherwise (for example, if the transaction aborts at any site), the coordinator decides to abort the transaction.
Every site where the transaction executed must follow the decision of the coordinator.
If a site fails when a transaction is in ready state, when the site recovers from failure it should be in a position to either commit or abort the transaction, depending on the decision of the coordinator.
Since a transactionmay access data items at several sites, transaction managers at several sites mayneed to coordinate to implement concurrency control.
If locking is used, locking can be performed locally at the sites containing accessed data items, but there is also a possibility of deadlock involving transactions originating at multiple sites.
Therefore deadlock detection needs to be carried out across multiple sites.
Failures are more common in distributed systems since not only may computers fail, but communication links may also fail.
Replication of data items, which is the key to the continued functioning of distributed databaseswhen failures occur, further complicates concurrency control.
Section 19.5 provides detailed coverage of concurrency control in distributed databases.
The standard transaction models, based on multiple actions carried out by a single program unit, are often inappropriate for carrying out tasks that cross the boundaries of databases that cannot or will not cooperate to implement protocols such as 2PC.
Alternative approaches, based on persistent messaging for communication, are generally used for such tasks; persistent messaging is discussed in Section 19.4.3
When the tasks to be carried out are complex, involving multiple databases and/or multiple interactions with humans, coordination of the tasks and ensuring transaction properties for the tasks become more complicated.
Workflow management systems are systems designed to help with carrying out such tasks, and are described in Section 26.2
In case an organization has to choose between a distributed architecture and a centralized architecture for implementing an application, the system architect must balance the advantages against the disadvantages of distribution of data.
We have already seen the advantages of using distributed databases.
The primary disadvantage of distributed database systems is the added complexity required to ensure proper coordination among the sites.
It is more difficult to implement a distributed database system; thus, it is more costly.
The exchange of messages and the additional computation required to achieve intersite coordination are a form of overhead that does not arise in centralized systems.
There are several approaches to distributed database design, ranging from fully distributed designs to ones that include a large degree of centralization.
Distributed databases and client–server systems are built around communication networks.
There are basically two types of networks: local-area networks and wide-area networks.
The main difference between the two is the way in which they are distributed geographically.
In local-area networks, processors are distributed over small geographical areas, such as a single building or a number of adjacent buildings.
In wide-area networks, on the other hand, a number of autonomous processors are distributed over a large geographical area (such as the United States or the entire world)
These differences imply major variations in the speed and reliability of the communication network, and are reflected in the distributed operating-system design.
People recognized that, for many enterprises, numerous small computers, each with its own selfcontained applications, are more economical than a single large system.
Because each small computer is likely to need access to a full complement of peripheral devices (such as disks and printers), and because some form of data sharing is likely to occur in a single enterprise, it was a natural step to connect these small systems into a network.
All the sites in such systems are close to one another, so the communication links tend to have a higher speed and lower error rate than do their counterparts in wide-area networks.
The most common links in a local-area network are twisted pair, coaxial cable, fiber optics, and wireless connections.
Communication speeds range from tens of megabits per second (for wireless local-area networks), to 1 gigabit per second for Gigabit Ethernet.
A storage-area network (SAN) is a special type of high-speed local-area network designed to connect large banks of storage devices (disks) to computers that use the data (see Figure 17.11)
The motivation for using storage-area networks to connect multiple computers to large banks of storage devices is essentially the same as that for shared-disk databases, namely:
High availability, since data are still accessible even if a computer fails.
Storagearea networks are usually built with redundancy, such as multiple paths between nodes, so if a component such as a link or a connection to the network fails, the network continues to function.
Wide-area networks (WANs) emerged in the late 1960s, mainly as an academic researchproject to provide efficient communication among sites, allowinghardware and software to be shared conveniently and economically by a wide community of users.
Systems that allowed remote terminals to be connected to a central computer via telephone lines were developed in the early 1960s, but they were not trueWANs.
The first WAN to be designed and developedwas the Arpanet.
The Arpanet has grown from a four-site experimental network to aworldwide network of networks, the Internet, comprising hundreds of millions of computer systems.
Typical links on the Internet are fiber-optic lines and, sometimes, satellite channels.
Data rates for wide-area links typically range from a fewmegabits per second to hundreds of gigabits per second.
The last link, to end user sites, has traditionally been the slowest link, using such technologies as digital subscriber line (DSL) technology (supporting a fewmegabits per second) or dial-up modem connections over land-based telephone lines (supporting up to 56 kilobits per second)
Today, the last link is typically a cable modem or fiber optic connection (each supporting tens of megabits per second), or a wireless connection supporting several megabits per second.
Applications whose data and computing resources are distributed geographically have to be carefully designed to ensure latency does not affect system performance excessively.
In discontinuous connection WANs, such as those based on mobile wireless connections, hosts are connected to the network only part of the time.
In continuous connection WANs, such as the wired Internet, hosts are connected to the network at all times.
Networks that are not continuously connected typically do not allow transactions across sites, but may keep local copies of remote data, and refresh the copies periodically (every night, for instance)
For applications where consistency is not critical, such as sharing of documents, groupware systems such as Lotus Notes allow updates of remote data to be made locally, and the updates are then propagated back to the remote site periodically.
There is a potential for conflicting updates at different sites, conflicts that have to be detected and resolved.
With the growth of personal computers and local-area networking, the database frontend functionality has moved increasingly to clients, with server systems providing the back-end functionality.
Client–server interface protocols have helped the growth of client–server database systems.
Servers can be either transaction servers or data servers, although the use of transaction servers greatly exceeds the use of data servers for providing database services.
So that these processes have access to common data, such as the database buffer, systems store such data in sharedmemory.
In addition to processes that handle queries, there are system processes that carry out tasks such as lock and log management and checkpointing.
Such systems strive to minimize communication between clients and servers by cachingdata and locks at the clients.
Parallel database systems consist of multiple processors and multiple disks connected by a fast interconnection network.
Speedup measures how much we can increase processing speed by increasing parallelism for a single transaction.
Scaleup measures how well we can handle an increased number of transactions by increasing parallelism.
Interference, skew, and start-up costs act as barriers to getting ideal speedup and scaleup.
Parallel database architectures include the shared-memory, shared-disk, shared-nothing, and hierarchical architectures.
These architectures have different trade-offs of scalability versus communication speed.
The systems communicate with one another through a communication network.
Local-area networks connect nodes that are distributed over small geographical areas, such as a single building or a few adjacent buildings.
Wide-area networks connect nodes spread over a large geographical area.
The Internet is the most extensively used wide-area network today.
Storage-area networks are a special type of local-area network designed to provide fast interconnection between large banks of storage devices and multiple computers.
Consider instead a scenario where client and server machines have exactly the same power.
Would it make sense to build a client–server system in such a scenario? Why?Which scenariowould be better suited to adata-server architecture?
What is the effect of the speed of the interconnection between the client and the server on the choice between tuple and page shipping?
If page shipping is used, the cache of data at the client can be organized either as a tuple cache or a page cache.
The page cache stores data in units of a page, while the tuple cache stores data in units of tuples.
Describe one benefit of a tuple cache over a page cache.
Howmuch speedup can one hope to attain if parallelism is used only for the SQL code? Explain.
Show how this fact can explain the phenomenon of superlinear speedup, where an application sees a speedup greater than the amount of resources allocated to it.
Does this architecture correspond to a bus, mesh or hypercube architecture? If not, how would you describe this interconnection architecture?
On the other hand, data-server architectures are popular for client–server object-oriented database systems, where transactions are expected to be relatively long.
Give two reasons why data servers may be popular for object-oriented databases but not for relational databases.
Suppose the company is growing rapidly each year, and has outgrown its current computer system.
When you are choosing a new parallel computer, what measure is most relevant—speedup, batch scaleup, or transaction scaleup? Why?
Is two-phase locking appropriate for serializing access to the data structures in shared memory? Explain your answer.
What impact would such a memory architecture have on the number of processors that can be supported in a shared-memory system?
Suppose the only way the databases interact is by electronic transfer of money between themselves, using persistent messaging.
Would such a system qualify as a distributed database? Why?
Abadi [2009] provides an excellent introduction to cloud computing and the challenges of running database transactions in such an environment.
Gray and Reuter [1993] provides a textbook description of transaction processing, including the architecture of client–server and distributed systems.
DeWitt and Gray [1992] surveys parallel database systems, including their architecture and performance measures.
A survey of parallel computer architectures is presented by Duncan [1990]
Dubois and Thakkar [1992] is a collection of papers on scalable shared-memory architectures.
Rdb is now owned by Oracle, and is called Oracle Rdb.
The Teradata database machine was among the earliest commercial systems to use the shared-nothing database architecture.
The Grace and the Gamma research prototypes also used shared-nothing architectures.
In this chapter, we discuss fundamental algorithms for parallel database systems that are based on the relational data model.
In particular, we focus on the placement of data onmultiple disks and the parallel evaluation of relational operations, both of which have been instrumental in the success of parallel databases.
At one point over two decades ago, parallel database systems had been nearly written off, even by some of their staunchest advocates.
Today, they are successfully marketed by practically every database-system vendor.
The transaction requirements of organizations have grown with increasing use of computers.
Moreover, the growth of the World Wide Web has created many sites with millions of viewers, and the increasing amounts of data collected from these viewers has produced extremely large databases atmany companies.
Organizations are using these increasingly large volumes of data—such as data about what items people buy, what Web links users click on, and when people make telephone calls—to plan their activities and pricing.
Queries used for such purposes are called decision-support queries, and the data requirements for such queries may run into terabytes.
Single-processor systems are not capable of handling such large volumes of data at the required rates.
The set-oriented nature of database queries naturally lends itself to parallelization.
A number of commercial and research systems have demonstrated the power and scalability of parallel query processing.
Asmicroprocessors have become cheap, parallelmachines have become common and relatively inexpensive.
Individual processors have themselves become parallel machines using multicore architectures.
Aswe discussed in Chapter 17, parallelism is used to provide speedup,where queries are executed faster because more resources, such as processors and disks, are provided.
Parallelism is also used to provide scaleup, where increasing workloads are handled without increased response time, via an increase in the degree of parallelism.
We outlined in Chapter 17 the different architectures for parallel database systems: shared-memory, shared-disk, shared-nothing, and hierarchical architectures.
Briefly, in shared-memory architectures, all processors share a common memory and disks; in shared-disk architectures, processors have independent memories, but share disks; in shared-nothing architectures, processors share neither memory nor disks; and hierarchical architectures have nodes that share neither memory nor disks with each other, but internally each node has a sharedmemory or a shared-disk architecture.
In its simplest form, I/O parallelism refers to reducing the time required to retrieve relations from disk by partitioning the relations over multiple disks.
The most common form of data partitioning in a parallel database environment is horizontal partitioning.
In horizontal partitioning, the tuples of a relation are divided (or declustered) among many disks, so that each tuple resides on one disk.
This strategy scans the relation in any order and sends the ith tuple to disk number Di mod n.
The round-robin scheme ensures an even distribution of tuples across disks; that is, each disk has approximately the same number of tuples as the others.
Once a relation has been partitioned among several disks, we can retrieve it in parallel, using all the disks.
Similarly, when a relation is being partitioned, it can be written to multiple disks in parallel.
Thus, the transfer rates for reading or writing an entire relation are much faster with I/O parallelism than without it.
However, reading an entire relation, or scanning a relation, is only one kind of access to data.
The different partitioning techniques support these types of access at different levels of efficiency:
The scheme is ideally suited for applications that wish to read the entire relation sequentially for each query.
With this scheme, both point queries and range queries are complicated to process, since each of the ndisks must be used for the search.
This scheme is best suited for point queries based on the partitioning attribute.
Directing a query to a single disk saves the start-up cost of initiating a query on multiple disks, and leaves the other disks free to process other queries.
Hash partitioning is also useful for sequential scans of the entire relation.
If the hash function is a good randomizing function, and the partitioning attributes form a key of the relation, then the number of tuples in each of the disks is approximately the same, without much variance.
Hence, the time taken to scan the relation is approximately 1/n of the time required to scan the relation in a single disk system.
Therefore, all the disks need to be scanned for range queries to be answered.
For point queries, we can consult the partitioning vector to locate the diskwhere the tuple resides.
For range queries,we consult the partitioning vector to find the range of disks on which the tuples may reside.
In both cases, the search narrows to exactly those disks that might have any tuples of interest.
An advantage of this feature is that, if there are only a few tuples in the queried range, then the query is typically sent to one disk, as opposed to all the disks.
Since other disks can be used to answer other queries, range partitioning results in higher throughput while maintaining good response time.
On the other hand, if there are many tuples in the queried range (as there are when the queried range is a larger fraction of the domain of the relation), many tuples have to be retrieved from a few disks, resulting in an I/O bottleneck (hot spot) at those disks.
In this example of execution skew, all processing occurs in one—or only a few—partitions.
In contrast, hash partitioning and round-robin partitioning would engage all the disks for such queries, giving a faster response time for approximately the same throughput.
The type of partitioning also affects other relational operations, such as joins, as we shall see in Section 18.5
Thus, the choice of partitioning technique also depends on the operations that need to be executed.
In general, hash partitioning or range partitioning are preferred to round-robin partitioning.
In a system with many disks, the number of disks across which to partition a relation can be chosen in this way: If a relation contains only a few tuples that will fit into a single disk block, then it is better to assign the relation to a single disk.
Large relations are preferably partitioned across all the available disks.
If a relation consists of m disk blocks and there are n disks available in the system, then the relation should be allocatedmin(m, n) disks.
When a relation is partitioned (by a technique other than round-robin), there may be a skew in the distribution of tuples, with a high percentage of tuples placed in some partitions and fewer tuples in other partitions.
Attribute-value skew refers to the fact that some values appear in the partitioning attributes of many tuples.
Partition skew refers to the fact that there may be load imbalance in the partitioning, even when there is no attribute skew.
Attribute-value skew can result in skewed partitioning regardless of whether range partitioning or hash partitioning is used.
If the partition vector is not chosen carefully, range partitioning may result in partition skew.
Partition skew is less likely with hash partitioning, if a good hash function is chosen.
As Section 17.3.1 noted, even a small skew can result in a significant decrease in performance.
Skew becomes an increasing problem with a higher degree of parallelism.
Thus, we see that the loss of speedup due to skew increases with parallelism.
A balanced range-partitioning vector can be constructed by sorting: The relation is first sorted on the partitioning attributes.
After every 1/n of the relation has been read, the value of the partitioning attribute of the next tuple is added to the partition vector.
Here, n denotes the number of partitions to be constructed.
In case there are many tuples with the same value for the partitioning attribute, the technique can still result in some skew.
The main disadvantage of this method is the extra I/O overhead incurred in doing the initial sort.
The I/O overhead for constructing balanced range-partition vectors can be reduced by constructing and storing a frequency table, or histogram, of the attribute values for each attribute of each relation.
A histogram takes up only a little space, so histograms on several different attributes can be stored in the system catalog.
It is straightforward to construct a balanced range-partitioning function given a histogram on the partitioning attributes.
If the histogram is not stored, it can be computed approximately by sampling the relation, using only tuples from a randomly chosen subset of the disk blocks of the relation.
Another approach to minimizing the effect of skew, particularly with range partitioning, is to use virtual processors.
In the virtual processor approach, we pretend there are several times as many virtual processors as the number of real processors.
Any of the partitioning techniques and query-evaluation techniques that we study later in this chapter can be used, but they map tuples and work to virtual processors instead of to real processors.
Virtual processors, in turn, are mapped to real processors, usually by round-robin partitioning.
The idea is that even if one range had many more tuples than the others because of skew, these tuples would get split across multiple virtual processor.
Round-robin allocation of virtual processors to real processors would distribute the extra work among multiple real processors, so that one processor does not have to bear all the burden.
In interquery parallelism, different queries or transactions execute in parallel with one another.
Transaction throughput can be increased by this form of parallelism.
However, the response times of individual transactions are no faster than they would be if the transactions were run in isolation.
Database systemsdesigned for single-processor systems canbeusedwith feworno changes on a shared-memory parallel architecture, since even sequential database systems support concurrent processing.
Transactions that would have operated in a timeshared concurrent manner on a sequential machine operate in parallel in the shared-memory parallel architecture.
Supporting interquery parallelism is more complicated in a shared-disk or shared-nothing architecture.
Processors have to perform some tasks, such as locking and logging, in a coordinated fashion, and that requires that they pass messages to each other.
A parallel database system must also ensure that two processors do not update the same data independently at the same time.
Further, when a processor accesses or updates data, the database systemmust ensure that the processor has the latest version of the data in its buffer pool.
The problem of ensuring that the version is the latest is known as the cache-coherency problem.
Before any read or write access to a page, a transaction locks the page in shared or exclusive mode, as appropriate.
Immediately after the transaction obtains either a shared or exclusive lock on a page, it also reads the most recent copy of the page from the shared disk.
Before a transaction releases an exclusive lock on a page, it flushes the page to the shared disk; then, it releases the lock.
This protocol ensures that, when a transaction sets a shared or exclusive lock on a page, it gets the correct copy of the page.
More complex protocols avoid the repeated reading and writing to disk required by the preceding protocol.
Such protocols do not write pages to disk when exclusive locks are released.
When a shared or exclusive lock is obtained, if the most recent version of a page is in the buffer pool of some processor, the page is obtained from there.
The protocols have to be designed to handle concurrent requests.
The shared-disk protocols can be extended to shared-nothing architectures by this scheme: Each page has a home processor Pi , and is stored on disk Di.
When other processors want to read or write the page, they send requests to the home processor Pi of the page, since they cannot directly communicate with the disk.
The other actions are the same as in the shared-disk protocols.
The Oracle and Oracle Rdb systems are examples of shared-disk parallel database systems that support interquery parallelism.
Intraquery parallelism refers to the execution of a single query in parallel on multiple processors and disks.
Using intraquery parallelism is important for speeding up long-running queries.
Interquery parallelism does not help in this task, since each query is run sequentially.
To illustrate the parallel evaluation of a query, consider a query that requires a relation to be sorted.
Suppose that the relation has been partitioned across multiple disks by range partitioning on some attribute, and the sort is requested on the partitioning attribute.
The sort operation can be implemented by sorting each partition in parallel, then concatenating the sorted partitions to get the final sorted relation.
Thus, we can parallelize a query by parallelizing individual operations.
There is another source of parallelism in evaluating a query: The operator tree for a query can contain multiple operations.We can parallelize the evaluation of the operator tree by evaluating in parallel some of the operations that do not depend on one another.
Further, as Chapter 12 mentions, we may be able to pipeline the output of one operation to another operation.
In summary, the executionof a single query canbeparallelized in twodifferent ways:
We can speed up processing of a query by parallelizing the execution of each individual operation, such as sort, select, project, and join.
We can speed up processing of a query by executing in parallel the different operations in a query expression.
The two forms of parallelism are complementary, and can be used simultaneously on a query.
Since the number of operations in a typical query is small, compared to the number of tuples processed by each operation, the first form of parallelism can scale better with increasing parallelism.
However, with the relatively small number of processors in typical parallel systems today, both forms of parallelism are important.
In the following discussion of parallelization of queries, we assume that the queries are read only.
The choice of algorithms for parallelizing query evaluation depends on the machine architecture.
Rather than present algorithms for each architecture separately, we use a shared-nothing architecture model in our description.
Thus, we explicitly describe when data have to be transferred from one processor to another.
We can simulate this model easily by using the other architectures, since transfer of data can be done via shared memory in a sharedmemory architecture, and via shared disks in a shared-disk architecture.
Hence, algorithms for shared-nothing architectures can be used on the other architectures, too.
We mention occasionally how the algorithms can be further optimized for shared-memory or shared-disk systems.
Since relational operations work on relations containing large sets of tuples, we can parallelize the operations by executing them in parallel on different subsets of the relations.
Since the number of tuples in a relation can be large, the degree of parallelism is potentially enormous.
If the relation has been partitioned in any other way, we can sort it in one of two ways:
We can range-partition it on the sort attributes, and then sort each partition separately.
We can use a parallel version of the external sort–merge algorithm.
Range-partitioning sort works in two steps: first range partitioning the relation, then sorting each partition separately.
When we sort by range partitioning the relation, it is not necessary to range-partition the relation on the same set of processors or disks as those on which that relation is stored.
Redistribute the tuples in the relation, using a range-partition strategy, so that all tuples that lie within the ith range are sent to processor Pi , which stores the relation temporarily on disk Di.
To implement range partitioning, in parallel every processor reads the tuples from its disk and sends the tuples to their destination processors.
Pm also receives tuples belonging to its partition, and stores them locally.
Each of the processors sorts its partition of the relation locally, without interaction with the other processors.
Each processor executes the same operation—namely, sorting—on a different data set.
Execution of the same operation in parallel on different sets of data is called data parallelism.
Wemust do range partitioningwith a good range-partition vector, so that each partition will have approximately the same number of tuples.
Virtual processor partitioning can also be used to reduce skew.
Each processor Pi locally sorts the data on disk Di.
The system then merges the sorted runs on each processor to get the final sorted output.
The merging of the sorted runs in step 2 can be parallelized by this sequence of actions:
Each processor Pi performs a merge on the streams as they are received, to get a single sorted run.
To avoid this problem, each processor repeatedly sends a block of data to each partition.
In other words, each processor sends the first block of every partition, then sends the second block of every partition, and so on.
Some machines, such as the Teradata Purpose-Built Platform Family machines, use specialized hardware to performmerging.
The BYNET interconnection network in the Teradata machines can merge output from multiple processors to give a single sorted output.
The join operation requires that the system test pairs of tuples to see whether they satisfy the join condition; if they do, the system adds the pair to the join output.
Parallel join algorithms attempt to split the pairs to be tested over several processors.
Then, the system collects the results from each processor to produce the final result.
In either case, the same partitioning function must be used for both relations.
For range partitioning, the same partition vector must be used for both relations.
For hash partitioning, the same hash function must be used on both relations.
Figure 18.2 depicts the partitioning in a partitioned parallel join.
Once the relations are partitioned, we can use any join technique locally at each processor Pi to compute the join of ri and si.
For example, hash join, merge join, or nested-loop join could be used.
If one or both of the relations r and s are already partitioned on the join attributes (by either hash partitioning or range partitioning), the work needed for partitioning is reduced greatly.
If the relations are not partitioned, or are partitioned on attributes other than the join attributes, then the tuples need to be repartitioned.
Each processor Pi reads in the tuples on disk Di , computes for each tuple t the partition j to which t belongs, and sends tuple t to processor Pj.
We can optimize the join algorithm used locally at each processor to reduce I/O by buffering some of the tuples to memory, instead of writing them to disk.
We can parallelize such joins by using a technique called fragment and replicate.
Any partitioning technique can be used on r , including round-robin partitioning.
The system replicates the other relation, s, across all the processors.
Processor Pi then locally computes the join of ri with all of s, using any join technique.
All that is required is to replicate s across all processors.
Fragment and replicate works with any join condition, since every tuple in r can be tested with every tuple in s.
Fragment and replicate usually has a higher cost than partitioning when both relations are of roughly the same size, since at least one of the relations has to be replicated.
However, if one of the relations—say, s—is small, it may be cheaper to replicate s across all processors, rather than to repartition r and s on the join attributes.
In such a case, asymmetric fragment and replicate is preferable, even though partitioning could be used.
If the size of s is less than that of r , the parallel hash-join algorithm proceeds this way:
Choose a hash function—say, h1—that takes the join attribute value of each tuple in r and s and maps the tuple to one of the n processors.
Let ri denote the tuples of relation r that are mapped to processor Pi ; similarly, let si denote the tuples of relation s that are mapped to processor Pi.
Each processor Pi reads the tuples of s that are on its disk Di and sends each tuple to the appropriate processor on the basis of hash function h1
As the destination processor Pi receives the tuples of si , it further partitions them by another hash function, h2, which the processor uses to compute the hash join locally.
The partitioning at this stage is exactly the same as in the partitioning phase of the sequential hash-join algorithm.
Each processor Pi executes this step independently from the other processors.
As it receives each tuple, the destination processor repartitions it by the function h2, just as the probe relation is partitioned in the sequential hash-join algorithm.
Each processor Pi executes the build and probe phases of the hash-join algorithm on the local partitions ri and si of r and s to produce a partition of the final result of the hash join.
The hash join at each processor is independent of that at other processors, and receiving the tuples of ri and si is similar to reading them from disk.
Therefore, any of the optimizations of the hash join described in Chapter 12 can be applied as well to the parallel case.
In particular, we can use the hybrid hash-join algorithm to cache some of the incoming tuples in memory, and thus avoid the costs of writing them and of reading them back in.
Suppose that relation r is stored by partitioning; the attribute on which it is partitioned does not matter.
Suppose too that there is an index on a join attribute of relation r at each of the partitions of relation r.
Eachprocessor Pj where apartition of relation s is stored reads the tuples of relation s stored in Dj , and replicates the tuples to every other processor Pi.
At the end of this phase, relation s is replicated at all sites that store tuples of relation r.
Now, each processor Pi performs an indexed nested-loop join of relation s with the ith partition of relation r.
However, the replication of relation s must be synchronized with the join so that there is enough space in the in-memory buffers at each processor Pi to hold the tuples of relation s that have been received but that have not yet been used in the join.
The evaluation of other relational operations also can be parallelized:
Duplicates can be eliminated by sorting; either of the parallel sort techniques can be used, optimized to eliminate duplicates as soon as they appear during sorting.
We can also parallelize duplicate elimination by partitioning the tuples (by either range or hash partitioning) and eliminating duplicates locally at each processor.
Projection without duplicate elimination can be performed as tuples are read in from disk in parallel.
If duplicates are to be eliminated, either of the techniques just described can be used.
We can parallelize the operation by partitioning the relation on the grouping attributes, and then computing the aggregate values locally at each processor.
If the relation is already partitioned on the grouping attributes, the first step can be skipped.
We can reduce the cost of transferring tuples during partitioning by partly computing aggregate values before partitioning, at least for the commonly used aggregate functions.
Consider an aggregation operation on a relation r , using the sum aggregate function on attribute B, with grouping on attribute A.
The system canperform the operation at eachprocessor Pi on those r tuples stored ondisk Di.
This computation results in tupleswith partial sums at each processor; there is one tuple at Pi for each value for attribute A present in r tuples stored on Di.
The system partitions the result of the local aggregation on the grouping attribute A, and performs the aggregation again (on tuples with the partial sums) at each processor Pi to get the final result.
As a result of this optimization, fewer tuples need to be sent to other processors during partitioning.
This idea can be extended easily to the min and max aggregate functions.
Extensions to the count and avg aggregate functions are left for you to do in Exercise 18.12
The parallelization of other operations is covered in several of the exercises.
We achieve parallelism by partitioning the I/O among multiple disks, and partitioning the CPU work among multiple processors.
If such a split is achieved without any overhead, and if there is no skew in the splitting of work, a parallel operation using n processors will take 1/n times as long as the same operation on a single processor.
We already know how to estimate the cost of an operation such as a join or a selection.
The time cost of parallel processing would then be 1/n of the time cost of sequential processing of the operation.
Contention for resources—such as memory, disk, and the communication network—resulting in delays.
Cost of assembling the final result by transmitting partial results from each processor.
The time taken by a parallel operation can be estimated as:
Assuming that the tuples are distributed without any skew, the number of tuples sent to each processor can be estimated as 1/n of the total number of tuples.
The preceding estimate will be an optimistic estimate, since skew is common.
Even though breakingdowna single query into a number of parallel steps reduces the size of the average step, it is the time for processing the single slowest step that determines the time taken for processing the query as a whole.
A partitioned parallel evaluation, for instance, is only as fast as the slowest of the parallel executions.
Thus, any skew in the distribution of the work across processors greatly affects performance.
The problem of skew in partitioning is closely related to the problem of partition overflow in sequential hash joins (Chapter 12)
We can use overflow resolution and avoidance techniques developed for hash joins to handle skew when hash partitioning is used.
We can use balanced range partitioning and virtual processor partitioning to minimize skew due to range partitioning, as in Section 18.2.3
There are two forms of interoperation parallelism: pipelined parallelism and independent parallelism.
As discussed in Chapter 12, pipelining forms an important source of economy of computation for database query processing.
Recall that, in pipelining, the output tuples of one operation, A, are consumed by a second operation, B, even before the first operation has produced the entire set of tuples in its output.
The major advantage of pipelined execution in a sequential evaluation is that we can carry out a sequence of such operations without writing any of the intermediate results to disk.
Parallel systems use pipelining primarily for the same reason that sequential systems do.
However, pipelines are a source of parallelism as well, in the same way that instruction pipelines are a source of parallelism in hardware design.
It is possible to run operations Aand B simultaneously on different processors, so that B consumes tuples in parallel with A producing them.
Pipelined parallelism is useful with a small number of processors, but does not scale up well.
First, pipeline chains generally do not attain sufficient length to provide a high degree of parallelism.
Second, it is not possible to pipeline relational operators that do not produce output until all inputs have been accessed, such as the set-difference operation.
Third, only marginal speedup is obtained for the frequent cases in which one operator’s execution cost is much higher than are those of the others.
All things considered, when the degree of parallelism is high, pipelining is a less important source of parallelism than partitioning.
The real reason for using pipelining is that pipelined executions can avoid writing intermediate results to disk.
Operations in a query expression that do not depend on one another can be executed in parallel.
Like pipelined parallelism, independent parallelism does not provide a high degree of parallelism and is less useful in a highly parallel system, although it is useful with a lower degree of parallelism.
Query optimizers account in large measure for the success of relational technology.
Recall that a query optimizer takes a query and finds the cheapest execution plan among the many possible execution plans that give the same answer.
Query optimizers for parallel query evaluation are more complicated than query optimizers for sequential query evaluation.
First, the cost models are more complicated, since partitioning costs have to be accounted for, and issues such as skew and resource contention must be taken into account.
More important is the issue of how to parallelize a query.
Suppose that we have somehow chosen an expression (from among those equivalent to the query) to be used for evaluating the query.
The expression canbe represented by anoperator tree, as in Section 12.1
To evaluate an operator tree in a parallel system, wemust make the following decisions:
How to parallelize each operation, and how many processors to use for it.
What operations to pipeline across different processors, what operations to.
These decisions constitute the task of scheduling the execution tree.
Determining the resources of each kind—such as processors, disks, andmemory—that should be allocated to each operation in the tree is another aspect of the optimization problem.
For instance, it may appear wise to use the maximum amount of parallelism available, but it is a good idea not to execute certain operations in parallel.
Operations whose computational requirements are significantly smaller than the communication overhead should be clustered with one of their.
Otherwise, the advantage of parallelism is negated by the overhead of communication.
One concern is that long pipelines do not lend themselves to good resource utilization.
Unless the operations are coarse grained, the final operation of the pipeline may wait for a long time to get inputs, while holding precious resources, such as memory.
The number of parallel evaluation plans fromwhich to choose is much larger than the number of sequential evaluation plans.
Optimizing parallel queries by considering all alternatives is therefore much more expensive than optimizing sequential queries.
Hence, we usually adopt heuristic approaches to reduce the number of parallel execution plans that we have to consider.
The first heuristic is to consider only evaluation plans that parallelize every operation across all processors, and that do not use any pipelining.
Finding the best such execution plan is like doing query optimization in a sequential system.
The main differences lie in how the partitioning is performed and what cost-estimation formula is used.
The second heuristic is to choose themost efficient sequential evaluation plan, and then to parallelize the operations in that evaluation plan.
The Volcano parallel database popularized a model of parallelization called the exchange-operator model.
This model uses existing implementations of operations, operating on local copies of data, coupled with an exchange operation that moves data around between different processors.
Exchange operators can be introduced into an evaluation plan to transform it into a parallel evaluation plan.
Yet another dimension of optimization is the design of physical-storage organization to speed up queries.
The database administrator must choose a physical organization that appears to be good for the expected mix of database queries.
Thus, the area of parallel query optimization is complex, and it is still an area of active research.
So far this chapter has concentrated on parallelization of data storage and of query processing.
Since large-scale parallel database systems are used primarily for storing large volumes of data, and for processing decision-support queries on those data, these topics are the most important in a parallel database system.
Parallel loading of data from external sources is an important requirement, if we are to handle large volumes of incoming data.
A large parallel database system must also address these availability issues:
With a large number of processors and disks, the probability that at least one processor or disk will malfunction is significantly greater than in a singleprocessor system with one disk.
A poorly designed parallel system will stop functioning if any component (processor or disk) fails.
Assuming that the probability of failure of a single processor or disk is small, the probability of failure of the system goes up linearly with the number of processors and disks.
Therefore, large-scale parallel database systems, such as Teradata, and IBM Informix XPS, are designed to operate even if a processor or disk fails.
If a processor fails, the data that it stored can still be accessed from the other processors.
The system keeps track of failed processors and distributes the work among functioning processors.
Requests for data stored at the failed site are automatically routed to the backup sites that store a replica of the data.
If all the data of a processor Aare replicated at a single processor B, B will have to handle all the requests to A as well as those to itself, and that will result in B becoming a bottleneck.
Therefore, the replicas of the data of a processor are partitioned across multiple other processors.
When we are dealing with large volumes of data (ranging in the terabytes), simple operations, such as creating indices, and changes to schema, such as adding a column to a relation, can take a long time—perhaps hours or even days.
Therefore, it is unacceptable for the database system to be unavailable while such operations are in progress.
Most database systems allow such operations to be performed online, that is, while the system is executing other transactions.
A system that supports this feature allows insertions, deletions, and updates on a relation even as an index is being built on the relation.
The index-building operation therefore cannot lock the entire relation in shared mode, as it would have done otherwise.
Instead, the process keeps track of updates that occur while it is active and incorporates the changes into the index being constructed.
Most database systems today support online index construction, since this feature is very important even for non-parallel database systems.
Each of these products runs on systems containing tens to thousands of nodes, with each node running an instance of an underlying database; Each product manages the partitioning of data, as well as parallel processing of queries, across the database instances.
Netezza, Greenplum and Aster Data use PostgreSQL as the underlying database; DATAllegro originally used Ingres as the underlying database system, but moved to SQL Server subsequent to its acquisition by Microsoft.
By building on top of an existingdatabase system, these systems are able to leverage the data storage, query processing, and transaction management features of the underlying database, leaving them free to focus on data partitioning (including replication for fault tolerance), fast interprocessor communication, parallel query processing, and parallel-query optimization.
It is also worth mentioning that Netezza and DATAllegro actually sell data warehouse “appliances”, which include hardware and software, allowing customers to build parallel databases with minimal effort.
Parallelism has become commonplace on most computers today, even some of the smallest, due to current trends in computer architecture.
As a result, virtually all database systems today run on a parallel platform.
In this section, we shall explore briefly the reasons for this architectural trend and the effects this has on database system design and implementation.
This increase results from an exponential growth in the number of transistors that could be fit within a unit area of a silicon chip, and is known popularly as Moore’s law, named after Intel co-founder Gordon Moore.
Technically, Moore’s law is not a law, but rather an observation and a prediction regarding technology trends.
Until recently, the increase in the number of transistors and the decrease in their size led to ever-faster processors.
This is problematic in terms of energy consumption and cost, battery life for mobile computers, and heat dissipation (all the power used eventually turns into heat)
As a result, modern processors typically are not one single processor but rather consist of several processors on one chip.
Tomaintain a distinction between on-chip multiprocessors and traditional processors, the term core is used for an on-chip processor.
Thus we say that a machine has a multicore processor.2
Each core is capable of processing an independent streamofmachine instructions.
However, because processors are able to process data faster than it can be accessed from main memory, main memory can become a bottleneck that limits overall performance.
For this reason, computer designers include one or more levels of cache memory in a computer system.
Cache memory is more costly than main memory on a per-byte basis, but offers a faster access time.
The use of the term core here is different from the use of that term in the early days of computing to refer to a main-memory technology based on magnetic cores.
The result is an extension of the storage hierarchy that we discussed in Chapter 10 to include the various levels of cache below main memory.
Although the database system can control the transfer of data between disk and main memory, the computer hardware maintains control over the transfer of data among the various levels of cache and between cache and main memory.
Despite this lack of direct control, the database system’s performance can be affected by how cache is utilized.
If a core needs to access a data item that is not in cache, it must be fetched from main memory.
Because main memory is so much slower than processors, a significant amount of potential processing speed may be lost while a core waits for data frommain memory.
One way in which computer designers attempt to limit the impact of cache misses is viamultithreading.
A thread is an execution stream that shares memory3 with other threads running on the same core.
If the thread currently executing on a core suffers a cache miss (or other type of wait), the core proceeds to execute another thread, thereby not wasting computing speed while waiting.
Threads introduce yet another source of parallelismbeyond themultiplicity of cores.
Each new generation of processors supports more cores and more threads.
The architecture trend of slower increase in raw speed accompanied by the growth in the number of cores has significant implications for database system design, as we shall see shortly.
It would appear that database systems are an ideal application to take advantage of large numbers of cores and threads, since database systems support large numbers of concurrent transactions.
However, there are a variety of factors that make optimal use of modern processors challenging.
As we allow a higher degree of concurrency to take advantage of the parallelism of modern processors, we increase the amount of data that needs to be in cache.
This can result in more cache misses, perhaps so many that even a multithreaded core has to wait for data from memory.
When concurrent transactions access data in common, some sort of restrictionsmust be imposed on that concurrent access.
Those restrictions, whether based on locks, timestamps, or validation, result in waiting or the loss of work due to transaction aborts.
To avoid excessive amounts of waiting or lost work, it is ideal that concurrent transactions conflict rarely, but attempting to ensure that can increase the amount of data needed in cache, resulting in more cache misses.
Finally, there are components of a database system shared by all transactions.
In a system using locking, the lock table is shared by all transactions and access to.
Similarly, the buffermanager, the logmanager, and the recoverymanager serve all transactions and are potential bottlenecks.
Because having a large number of concurrent transactions may not take optimal advantage ofmodern processors, it is desirable to findways to allowmultiple cores to work on a single transaction.
This requires the database query processor to find effective ways to parallelize queries without creating excessive demands on cache.
This can be done by creating pipelines of database operations from queries and by finding ways to parallelize individual database operations.
The adaptation of database system design and database query processing to multicore and multithreaded systems remains an area of active research.
Parallel databases have gained significant commercial acceptance in the past 20 years.
In I/O parallelism, relations are partitioned among available disks so that they can be retrieved faster.
Three commonly used partitioning techniques are round-robin partitioning, hash partitioning, and range partitioning.
Skew is a major problem, especially with increasing degrees of parallelism.
Balanced partitioning vectors, using histograms, and virtual processor partitioning are among the techniques used to reduce skew.
In interquery parallelism, we run different queries concurrently to increase throughput.
Intraquery parallelism attempts to reduce the cost of running a query.
There are two types of intraquery parallelism: intraoperation parallelism and interoperation parallelism.
We use intraoperation parallelism to execute relational operations, such as sorts and joins, in parallel.
Intraoperation parallelism is natural for relational operations, since they are set oriented.
There are two basic approaches to parallelizing a binary operation such as a join.
In partitioned parallelism, the relations are split into several parts, and tuples in ri are joined only with tuples from si.
Partitioned parallelism can be used only for natural and equi-joins.
In fragment and replicate, both relations are partitioned and each partition is replicated.
In asymmetric fragment and replicate, one of the relations is replicated while the other is partitioned.
Unlike partitioned parallelism, fragment and replicate and asymmetric fragment and replicate can be used with any join condition.
Both parallelization techniques can work in conjunction with any join technique.
In independent parallelism, different operations that do not depend on one another are executed in parallel.
In pipelined parallelism, processors send the results of one operation to another operation as those results are computed, without waiting for the entire operation to finish.
Query optimization in parallel databases is significantly more complex than query optimization in sequential databases.
Modern multicore processors are introducing new research problems in parallel databases.
Increasing the throughput of a system with many small queries.
Increasing the throughput of a systemwith a few large queries,when the number of disks and processors is large.
Would the arguments you advanced in part a hold if the machine has a shared-memory architecture? Explain why or why not.
Would the arguments in part a hold with independent parallelism? (That is, are there caseswhere, even if theoperations arenotpipelined and there are many processors available, it is still a good idea to perform several operations on the same processor?)
Give a load-balanced range partitioning function to divide the values into 5 partitions.
Write an algorithm for computing a balanced range partition with p partitions, given a histogram of frequency distributions containing n ranges.
Instead of keeping the extra copy of data items from a processor at a single backup processor, it is a good idea to partition the copies of the data items of a processor across multiple processors.
Explain how virtual-processor partitioning can be used to efficiently implement the partitioning of the copies as described above.
What are the benefits and drawbacks of using RAID storage instead of storing an extra copy of each data item?
Can the idea of partitioning (including virtual processor partitioning) be applied to indices? Explain your answer, considering the following two cases (assuming for simplicity that partitioning aswell as indexing are on single attributes):
Where the index is on the partitioning attribute of the relation.
Where the index is on an attribute other than the partitioning attribute of the relation.
Even if virtual-processor partitioning is used, a particular virtual processor may end up with a very large number of tuples after the update, and repartitioning would then be required.
Suppose a virtual processor has a significant excess of tuples (say, twice the average)
Explain how repartitioning can be done by splitting the partition, thereby increasing the number of virtual processors.
If, instead of round-robin allocation of virtual processors, virtual partitions can be allocated to processors in an arbitrary fashion, with a mapping table tracking the allocation.
If a particular node has excess load (compared to the others), explain how load can be balanced.
Assuming there are no updates, does query processing have to be stopped while repartitioning, or reallocation of virtual processors, is carried out? Explain your answer.
In each case, what can be done to reduce the skew?
Left outer join, if the join condition involves only equality.
Left outer join, if the join condition involves comparisons other than equality.
Full outer join, if the join condition involves comparisons other than equality.
Is intraquery parallelism required in such a situation? If not, why, and what form of parallelism is appropriate?
What form of skew would be of significance with such a workload?
Suppose most transactions accessed one account record, which includes an account type attribute, andanassociated account type master record, which provides information about the account type.
How would you partition and/or replicate data to speed up transactions? You may assume that the account type master relation is rarely updated.
Given aworkload of SQL queries on a single relation, what attributes would be candidates for partitioning?
How would you choose between the alternative partitioning techniques, based on the workload?
Is it possible to partition a relation on more than one attribute? Explain your answer.
Teradata was one of the first commercial parallel database systems, and continues to have a large market share.
The Red Brick Warehouse was another early parallel database system; Red Brick was was acquired by Informix, which was itself acquired by IBM.
Cache-coherency protocols for parallel database systems are discussed by Dias et al.
Graefe and McKenna [1993b] presents an excellent survey of query processing, including parallel processing of queries.
Parallel sorting on multicore and multithreaded processors is discussed in Garcia and Korth [2005] and Chen et al.
Skew handling in parallel joins is described by Walton et al.
Unlike parallel systems, in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.
Furthermore, the database systems that run on each sitemay have a substantial degree ofmutual independence.
Each site may participate in the execution of transactions that access data at one site, or several sites.
The main difference between centralized and distributed database systems is that, in the former, the data reside in one single location, whereas in the latter, the data reside in several locations.
This distribution of data is the cause of many difficulties in transaction processing and query processing.
We start by classifying distributed databases as homogeneous or heterogeneous, in Section 19.1
We then address the question of how to store data in a distributed database in Section 19.2
In Section 19.3, we outline a model for transaction processing in a distributed database.
In Section 19.4, we describe how to implement atomic transactions in a distributed database by using special commit protocols.
In Section 19.5, we describe concurrency control in distributed databases.
In Section 19.6, we outline how to provide high availability in a distributed database by exploiting replication, so the system can continue processing transactions even when there is a failure.
We address query processing in distributed databases in Section 19.7
In Section 19.8, we outline issues in handling heterogeneous databases.
In Section 19.10, we describe directory systems, which can be viewed as a specialized form of distributed databases.
In this chapter, we illustrate all our examples using the bank database of Figure 19.1
In a homogeneous distributed database system, all sites have identical databasemanagement system software, are aware of one another, and agree to cooperate in processing users’ requests.
That softwaremust also cooperatewith other sites in exchanging information about transactions, to make transaction processing possible across multiple sites.
The sites may not be aware of one another, and they may provide only limited facilities for cooperation in transaction processing.
The differences in schemas are often a major problem for query processing, while the divergence in software becomes a hindrance for processing transactions that access multiple sites.
Consider a relation r that is to be stored in the database.
There are two approaches to storing this relation in the distributed database:
The system maintains several identical replicas (copies) of the relation, and stores each replica at a different site.
The alternative to replication is to store only one copy of relation r.
The system partitions the relation into several fragments, and stores each fragment at a different site.
Fragmentation and replication can be combined: A relation can be partitioned into several fragments and there may be several replicas of each fragment.
In the following subsections, we elaborate on each of these techniques.
If relation r is replicated, a copy of relation r is stored in two or more sites.
In the most extreme case, we have full replication, in which a copy is stored in every site in the system.
There are a number of advantages and disadvantages to replication.
If one of the sites containing relation r fails, then the relation r can be found in another site.
Thus, the system can continue to process queries involving r, despite the failure of one site.
In the case where the majority of accesses to the relation r result in only the reading of the relation, then several sites can process queries involving r in parallel.
Themore replicas of r there are, the greater the chance that the needed data will be found in the site where the transaction is executing.
The systemmust ensure that all replicas of a relation r are consistent; otherwise, erroneous computationsmay result.
Thus, whenever r is updated, the update must be propagated to all sites containing replicas.
For example, in a banking system, where account information is replicated in various sites, it is necessary to ensure that the balance in a particular account agrees in all sites.
In general, replication enhances the performance of read operations and increases the availability of data to read-only transactions.
We can simplify the management of replicas of relation r by choosing one of them as the primary copy of r.
For example, in a banking system, an account can be associated with the site in which the account has been opened.
We shall examine the primary copy scheme and other options for distributed concurrency control in Section 19.5
These fragments contain sufficient information to allow reconstruction of the original relation r.
There are two different schemes for fragmenting a relation: horizontal fragmentation and vertical fragmentation.
Horizontal fragmentation splits the relation by assigning each tuple of r to one or more fragments.
Vertical fragmentation splits the relation by decomposing the scheme R of relation r.
Each tuple of relation r must belong to at least one of the fragments, so that the original relation can be reconstructed, if needed.
As an illustration, the account relation can be divided into several different fragments, each of which consists of tuples of accounts belonging to a particular branch.
If the banking system has only two branches—Hillside and Valleyview —then there are two different fragments:
Horizontal fragmentation is usually used to keep tuples at the sites where they are used the most, to minimize data transfer.
In general, a horizontal fragment can be defined as a selection on the global relation r.
That is, we use a predicate Pi to construct fragment ri :
We reconstruct the relation r by taking the union of all fragments; that is:
By changing the selection predicates used to construct the fragments, we can have a particular tuple of r appear in more than one of the ri.
In its simplest form, vertical fragmentation is the same as decomposition (see Chapter 8)
The fragmentation should be done in such a way that we can reconstruct relation r from the fragments by taking the natural join:
One way of ensuring that the relation r can be reconstructed is to include the primary-key attributes of R in each Ri.
It is often convenient to add a special attribute, called a tuple-id, to the schema R.
The tuple-id value of a tuple is a unique value that distinguishes the tuple from all other tuples.
The tuple-id attribute thus serves as a candidate key for the augmented schema, and is included in each Ri.
The physical or logical address for a tuple can be used as a tuple-id, since each tuple has a unique address.
To illustrate vertical fragmentation, consider a university database with a relation employee info that stores, for each employee, employee id, name, designation, and salary.
For privacy reasons, this relationmay be fragmented into a relation employee private info containing employee id and salary, and another relation employee public info containing attributes employee id, name, and designation.
These may be stored at different sites, again, possibly for security reasons.
The two types of fragmentation can be applied to a single schema; for instance, the fragments obtained by horizontally fragmenting a relation can be further partitioned vertically.
In general, a fragment can be replicated, replicas of fragments can be fragmented further, and so on.
The user of a distributed database system should not be required to know where the data are physically located nor how the data can be accessed at the specific local site.
Users are not required to know how a relation has been fragmented.
The distributed system may replicate an object to increase either system performance or data availability.
Users do not have to be concerned with what data objects have been replicated, or where replicas have been placed.
Users are not required to know the physical location of the data.
The distributed database system should be able to find any data as long as the data identifier is supplied by the user transaction.
Data items—such as relations, fragments, and replicas—must have unique names.
This property is easy to ensure in a centralized database.
In a distributed database, however, wemust take care to ensure that two sites do not use the same name for distinct data items.
One solution to this problem is to require all names to be registered in a central name server.
The name server helps to ensure that the same name does not get used for different data items.
We can also use the name server to locate a data item, given the name of the item.
First, the name server may become a performance bottleneck when data items are located by their names, resulting in poor performance.
Second, if the name server crashes, it may not be possible for any site in the distributed system to continue to run.
A more widely used alternative approach requires that each site prefix its own site identifier to any name that it generates.
This approach ensures that no two sites generate the same name (since each site has a unique identifier)
This solution, however, fails to achieve location transparency, since site identifiers are attached tonames.
Thus, the account relation might be referred to as site17
Many database systems use the Internet address (IP address) of a site to identify it.
To overcome this problem, the database system can create a set of alternative names, or aliases, for data items.
A user may thus refer to data items by simple names that are translated by the system to complete names.
The mapping of aliases to the real names can be stored at each site.
With aliases, the user can be unaware of the physical location of a data item.
Furthermore, the user will be unaffected if the database administrator decides to move a data item from one site to another.
Users should not have to refer to a specific replica of a data item.
Instead, the system should determine which replica to reference on a read request, and.
We can ensure that it does so by maintaining a catalog table, which the system uses to determine all replicas for the data item.
Access to the various data items in a distributed system is usually accomplished through transactions, which must preserve the ACID properties (Section 14.1)
There are two types of transaction thatwe need to consider.
The local transactions are those that access and update data in only one local database; the global transactions are those that access and update data in several local databases.
However, for global transactions, this task is much more complicated, since several sites may be participating in execution.
The failure of one of these sites, or the failure of a communication link connecting these sites, may result in erroneous computations.
In this section, we study the system structure of a distributed database and its possible failure modes.
In Section 19.6, we study how a distributed database can continue functioning even in the presence of various types of failure.
Each site has its own local transaction manager, whose function is to ensure the ACID properties of those transactions that execute at that site.
To understand how such a manager can be implemented, consider an abstract model of a transaction system, in which each site contains two subsystems:
The transaction manager manages the execution of those transactions (or subtransactions) that access data stored in a local site.
Note that each such transaction may be either a local transaction (that is, a transaction that executes at only that site) or part of a global transaction (that is, a transaction that executes at several sites)
The transaction coordinator coordinates the execution of the various transactions (both local and global) initiated at that site.
The structure of a transaction manager is similar in many respects to the.
As we shall see, we need to modify both the recovery and concurrency schemes to accommodate the distribution of transactions.
The transaction coordinator subsystem is not needed in the centralized environment, since a transaction accesses data at only a single site.
A transaction coordinator, as its name implies, is responsible for coordinating the execution of all the transactions initiated at that site.
Breaking the transaction into a number of subtransactions and distributing.
Coordinating the termination of the transaction, which may result in the transaction being committed at all sites or aborted at all sites.
A distributed system may suffer from the same types of failure that a centralized system does (for example, software errors, hardware errors, or disk crashes)
There are, however, additional types of failure with which we need to deal in a distributed environment.
The loss or corruption of messages is always a possibility in a distributed system.
Information about suchprotocolsmaybe found in standard textbooks on networking (see the bibliographical notes)
However, if two sites A and B are not directly connected, messages from one to the other must be routed through a sequence of communication links.
If a communication link fails, messages that would have been transmitted across the link must be rerouted.
In some cases, it is possible to find another route through the network, so that the messages are able to reach their destination.
In other cases, a failure may result in there being no connection between some pairs of sites.
A system is partitioned if it has been split into two (or more) subsystems, called partitions, that lack any connection between them.
Note that, under this definition, a partition may consist of a single node.
If we are to ensure atomicity, all the sites in which a transaction T executed must agree on the final outcome of the execution.
To ensure this property, the transaction coordinator of T must execute a commit protocol.
We first describe how the two-phase commit protocol (2PC) operates during normal operation, then describe how it handles failures and finally how it carries out recovery and concurrency control.
Consider a transaction T initiated at site Si , where the transaction coordinator is Ci.
When T completes its execution—that is, when all the sites at which T has executed inform Ci that T has completed—Ci starts the 2PC protocol.
It then sends a prepare T message to all sites at which T executed.
On receiving such a message, the transaction manager at that site.
The transaction manager then replies with a ready T message to Ci.
When Ci receives responses to the prepare T message from all the sites, or when a prespecified interval of time has elapsed since the prepare T message was sent out, Ci can determine whether the transaction T can be committed or aborted.
Transaction T can be committed if Ci received a ready T message from all the participating sites.
At this point, the fate of the transaction has been sealed.
Following this point, the coordinator sends either a commit T or an abort Tmessage to all participating sites.
When a site receives that message, it records the message in the log.
A site at which T executed can unconditionally abort T at any time before it sends the message ready T to the coordinator.
Once the message is sent, the transaction is said to be in the ready state at the site.
The ready T message is, in effect, a promise by a site to follow the coordinator’s order to commit T or to abort T.
To make such a promise, the needed information must first be stored in stable storage.
Otherwise, if the site crashes after sending ready T, it may be unable to make good on its promise.
Further, locks acquired by the transaction must continue to be held until the transaction completes.
Since unanimity is required to commit a transaction, the fate of T is sealed as soon as at least one site responds abort T.
Since the coordinator site Si is one of the sites at which T executed, the coordinator can decide unilaterally to abort T.
The final verdict regarding T is determined at the time that the coordinator writes that verdict (commit or abort) to the log and forces that verdict to stable storage.
In some implementations of the 2PC protocol, a site sends an acknowledge T message to the coordinator at the end of the second phase of the protocol.
The 2PC protocol responds in different ways to various types of failures:
If the coordinator Ci detects that a site has failed, it takes these actions: If the site fails before responding with a ready T message to Ci , the coordinator assumes that it responded with an abort T message.
If the site fails after the coordinator has received the readyTmessage from the site, the coordinator executes the rest of the commit protocol in the normal fashion, ignoring the failure of the site.
When a participating site Sk recovers from a failure, it must examine its log to determine the fate of those transactions that were in the midst of execution.
The log contains no control records (abort, commit, ready) concerning T.
Thus, we know that Sk failed before responding to the prepare Tmessage from Ci.
Since the failure of Sk precludes the sending of such a response, by our algorithm Ci must abort T.
If the coordinator fails in themidst of the execution of the commit protocol for transaction T, then the participating sites must decide the fate of T.
We shall see that, in certain cases, the participating sites cannot decide whether to commit or abort T, and therefore these sites must wait for the recovery of the failed coordinator.
However, the coordinator may have decided to abort T, but not to commit T.
Rather than wait for Ci to recover, it is preferable to abort T.
Since the coordinator has failed, it is impossible to determine whether a decision has been made, and if one has, what that decision is, until the coordinator recovers.
Since the fate ofT remains in doubt,Tmay continue to hold system resources.
For example, if locking is used, T may hold locks on data at active sites.
Such a situation is undesirable, because it may be hours or days before Ci is again active.
During this time, other transactions may be forced to wait for T.
As a result, data items may be unavailable not only on the failed site (Ci ), but on active sites as well.
This situation is called the blocking problem, because T is blocked pending the recovery of site Ci.
From the viewpoint of the sites in one of the partitions, it appears that the sites in other partitions have failed.
Sites that are not in the partition containing the coordinator simply execute the protocol to deal with failure of the coordinator.
The coordinator and the sites that are in the same partition as the coordinator follow the usual commit protocol, assuming that the sites in the other partitions have failed.
Thus, the major disadvantage of the 2PC protocol is that coordinator failure may result in blocking, where a decision either to commit or to abort Tmay have to be postponed until Ci recovers.
When a failed site restarts, we can perform recovery by using, for example, the recovery algorithm described in Section 16.4
The recovering site must determine the commit–abort status of such transactions by contacting other sites, as described in Section 19.4.1.2
If recovery is done as just described, however, normal transaction processing at the site cannot begin until all in-doubt transactions have been committed or rolled back.
Finding the status of in-doubt transactions can be slow, sincemultiple sites may have to be contacted.
Further, if the coordinator has failed, and no other site has information about the commit–abort status of an incomplete transaction, recovery potentially could become blocked if 2PC is used.
As a result, the site performing restart recovery may remain unusable for a long period.
To circumvent this problem, recovery algorithms typically provide support for noting lock information in the log.
We are assuming here that locking is used for concurrency control.
After lock reacquisition is complete for all in-doubt transactions, transaction processing can start at the site, even before the commit–abort status of the indoubt transactions is determined.
The commit or rollback of in-doubt transactions proceeds concurrently with the execution of new transactions.
Note that new transactions that have a lock conflict with anywrite locks held by in-doubt transactions will be unable tomake progress until the conflicting in-doubt transactions have been committed or rolled back.
The three-phase commit (3PC) protocol is an extension of the two-phase commit protocol that avoids the blocking problem under certain assumptions.
In particular, it is assumed that no network partition occurs, and not more than k sites fail, where k is some predetermined number.
Under these assumptions, the protocol avoids blocking by introducing an extra third phase where multiple sites are involved in the decision to commit.
Instead of directly noting the commit decision in its persistent storage, the coordinator first ensures that at least k other sites know that it intended to commit the transaction.
If the coordinator fails, the remaining sites first select a new coordinator.
This new coordinator checks the status of the protocol from the remaining sites; if the coordinator had decided to commit, at least one of the other k sites that it informed will be up and will ensure that the commit decision is respected.
The new coordinator restarts the third phase of the protocol if some site knew that the old coordinator intended to commit the transaction.
While the 3PC protocol has the desirable property of not blocking unless k sites fail, it has the drawback that a partitioning of the network may appear to be the same as more than k sites failing, which would lead to blocking.
The protocol also has to be implemented carefully to ensure that network partitioning (or more than k sites failing) does not result in inconsistencies, where a transaction is committed in one partition and aborted in another.
Because of its overhead, the 3PC protocol is not widely used.
See the bibliographical notes for references giving more details of the 3PC protocol.
For many applications, the blocking problem of two-phase commit is not acceptable.
The problem here is the notion of a single transaction that works across multiple sites.
In this section, we describe how to use persistent messaging to avoid the problem of distributed commit, and then briefly outline the larger issue of workflows; workflows are considered in more detail in Section 26.2
To understand persistent messaging, consider how one might transfer funds between twodifferent banks, eachwith its own computer.One approach is to have.
However, the transactionmayhave to update the total bank balance, and blocking could have a serious impact on all other transactions at each bank, since almost all transactions at the bank would update the total bank balance.
In contrast, consider how funds transfer by a bank check occurs.
The bank first deducts the amount of the check from the available balance and prints out a check.
The check is then physically transferred to the other bank where it is deposited.
After verifying the check, the bank increases the local balance by the amount of the check.
So that funds are not lost or incorrectly increased, the check must not be lost, and must not be duplicated and depositedmore than once.When the bank computers are connected by a network, persistent messages provide the same service as the check (but much faster, of course)
Persistent messages are messages that are guaranteed to be delivered to the recipient exactly once (neither less nor more), regardless of failures, if the transaction sending themessage commits, and are guaranteed to not be delivered if the transaction aborts.
Database recovery techniques are used to implement persistent messaging on top of the normal network channels, as we shall see shortly.
In contrast, regular messages may be lost or may even be delivered multiple times in some situations.
For instance, if the account where the check is to be deposited has been closed, the check must be sent back to the originating account and credited back there.
Both sitesmust therefore be providedwith error-handling code, along with code to handle the persistent messages.
In contrast, with two-phase commit, the error would be detected by the transaction, which would then never deduct the amount in the first place.
The types of exception conditions that may arise depend on the application, so it is not possible for the database system to handle exceptions automatically.
For instance, it is not acceptable to just lose the money being transferred if the receiving account has been closed; the money must be credited back to the originating account, and if that is not possible for some reason, humans must be alerted to resolve the situation manually.
There are many applications where the benefit of eliminating blocking is well worth the extra effort to implement systems that use persistent messages.
In fact, few organizations would agree to support two-phase commit for transactions originating outside the organization, since failures could result in blocking of access to local data.
Persistent messaging therefore plays an important role in carrying out transactions that cross organizational boundaries.
Workflows provide a general model of transaction processing involving multiple sites and possibly human processing of certain steps.
For instance, when a bank receives a loan application, there are many steps it must take, including contacting external credit-checking agencies, before approving or rejecting a loan application.
Persistent messaging can be implemented on top of an unreliable messaging infrastructure, which may lose messages or deliver them multiple times, by these protocols:
When a transaction wishes to send a persistent message, it writes a record containing themessage in a special relationmessages to send, instead of directly sending out the message.
A message delivery process monitors the relation, and when a new message is found, it sends the message to its destination.
The message delivery process deletes a message from the relation only after it receives an acknowledgment from the destination site.
If it receives no acknowledgement from the destination site, after some time it sends the message again.
In case of permanent failures, the system will decide, after some period of time, that the message is undeliverable.
Exception handling code provided by the application is then invoked to deal with the failure.
Writing the message to a relation and processing it only after the transaction commits ensures that the message will be delivered if and only if the transaction commits.
Repeatedly sending it guarantees it will be delivered even if there are (temporary) system or network failures.
When a site receives a persistent message, it runs a transaction that adds the message to a special received messages relation, provided it is not already present in the relation (the unique message identifier allows duplicates to be detected)
After the transaction commits, or if the message was already present in the relation, the receiving site sends an acknowledgment back to the sending site.
Note that sending the acknowledgment before the transaction commits is not safe, since a system failure may then result in loss of the message.
Checking whether the message has been received earlier is essential to avoid multiple deliveries of the message.
In many messaging systems, it is possible for messages to get delayed arbitrarily, although such delays are very unlikely.
Therefore, to be safe, the message must never be deleted from the received messages relation.
Deleting it could result in a duplicate delivery not being detected.
But as a result, the received messages relation may grow indefinitely.
To deal with this problem, each message is given a timestamp, and if the timestamp of a received message is older than some cutoff, the message is discarded.
All messages recorded in the received messages relation that are older than the cutoff can be deleted.
We assume that each site participates in the execution of a commit protocol to ensure global transaction atomicity.
The protocols we describe in this section require updates to be done on all replicas of a data item.
If any site containing a replica of a data item has failed, updates to the data item cannot be processed.
In Section 19.6, we describe protocols that can continue transaction processing even if some sites or links have failed, thereby providing high availability.
The various locking protocols described in Chapter 15 can be used in a distributed environment.
The only change that needs to be incorporated is in the way the lock manager deals with replicated data.
We present several possible schemes that are applicable to an environment where data can be replicated in several sites.
As in Chapter 15, we shall assume the existence of the shared and exclusive lock modes.
In the single lock-manager approach, the systemmaintains a single lock manager that resides in a single chosen site—say Si.
All lock and unlock requests are made at site Si.
When a transaction needs to lock a data item, it sends a lock request to Si.
The lock manager determines whether the lock can be granted immediately.
If the lock can be granted, the lockmanager sends a message to that effect to the site at which the lock request was initiated.
Otherwise, the request is delayed until it can be granted, at which time a message is sent to the site at which the lock request was initiated.
The transaction can read the data item from any one of the sites at which a replica of the data item resides.
In the case of a write, all the sites where a replica of the data item resides must be involved in the writing.
This scheme requires two messages for handling lock requests and one message for handling unlock requests.
Since all lock and unlock requests are made at one site, the deadlock-handling algorithms discussed in Chapter 15 can be applied directly.
The site Si becomes a bottleneck, since all requests must be processed there.
If the site Si fails, the concurrency controller is lost.
Either processing must stop, or a recovery scheme must be used so that a backup site can take over lock management from Si , as described in Section 19.6.5
Each site maintains a local lock manager whose function is to administer the lock and unlock requests for those data items that are stored in that site.
When a transaction wishes to lock a data item Q that is not replicated and resides at site Si , a message is sent to the lockmanager at site Si requesting a lock (in a particular lock mode)
If data item Q is locked in an incompatible mode, then the request is delayed until it can be granted.
Once it has determined that the lock request can be granted, the lock manager sends a message back to the initiator indicating that it has granted the lock request.
It has a reasonably low overhead, requiring two message transfers for handling lock requests, and one message transfer for handling unlock requests.
However, deadlock handling is more complex, since the lock and unlock requests are no longer made at a single site: There may be intersite deadlocks even when there is no deadlock within a single site.
When a system uses data replication, we can choose one of the replicas as the primary copy.
For eachdata itemQ, the primary copyofQmust reside inprecisely one site, which we call the primary site of Q.
When a transaction needs to lock a data item Q, it requests a lock at the primary site of Q.
As before, the response to the request is delayed until it can be granted.
The primary copy enables concurrency control for replicated data to be handled like that for unreplicated data.
However, if the primary site of Q fails, Q is inaccessible, even though other sites containing a replica may be accessible.
The majority protocol works this way: If data item Q is replicated in n different sites, then a lock-request message must be sent to more than one-half of the n sites in whichQ is stored.
Each lock manager determines whether the lock can be granted immediately (as far as it is concerned)
As before, the response is delayed until the request can be granted.
The transaction does not operate on Q until it has successfully obtained a lock on a majority of the replicas of Q.
We assume for now thatwrites are performedon all replicas, requiring all sites containing replicas to be available.
The protocol also deals with replicated data in a decentralized manner, thus avoiding the drawbacks of central control.
The majority protocol is more complicated to implement than are the previous schemes.
Each then must wait to acquire the third lock; hence, a deadlock has occurred.
Luckily, we can avoid such deadlocks with relative ease, by requiring all sites to request locks on the replicas of a data item in the same predetermined order.
The difference from the majority protocol is that requests for shared locks are given more favorable treatment than requests for exclusive locks.
Shared locks.When a transactionneeds to lockdata itemQ, it simply requests a lock on Q from the lock manager at one site that contains a replica of Q.
When a transaction needs to lock data item Q, it requests a lock on Q from the lock manager at all sites that contain a replica of Q.
As before, the response to the request is delayed until it can be granted.
The biased scheme has the advantage of imposing less overhead on read.
This savings is especially significant in common cases in which the frequency of read is much greater than the frequency of write.
Furthermore, the biased protocol shares the majority protocol’s disadvantage of complexity in handling deadlock.
The quorum consensus protocol is a generalization of the majority protocol.
The quorum consensus protocol assigns each site a nonnegative weight.
It assigns read and write operations on an item x two integers, called read quorum Qr and write quorum Qw, that must satisfy the following condition, where S is the total weight of all sites at which x resides:
To execute a read operation, enough replicas must be locked that their total weight is at least r.
To execute a write operation, enough replicas must be locked so that their total weight is at least w.
A benefit of the quorum consensus approach is that it can permit the cost of either read or write locking to be selectively reduced by appropriately defining the read and write quorums.
For instance, with a small read quorum, reads need to obtain fewer locks, but the write quorum will be higher, hence writes need to obtain more locks.
Also, if higher weights are given to some sites (for example, those less likely to fail), fewer sites need to be accessed for acquiring locks.
In fact, by setting weights and quorums appropriately, the quorum consensus protocol can simulate the majority protocol and the biased protocols.
Like the majority protocol, quorum consensus can be extended to work even in the presence of site failures, as we shall see in Section 19.6.1
The principal idea behind the timestamping scheme in Section 15.4 is that each transaction is given a unique timestamp that the system uses in deciding the serialization order.
Our first task, then, in generalizing the centralized scheme to a distributed scheme is to develop a scheme for generating unique timestamps.
There are two primary methods for generating unique timestamps, one centralized and one distributed.
In the centralized scheme, a single site distributes the timestamps.
The site can use a logical counter or its own local clock for this purpose.
In the distributed scheme, each site generates a unique local timestamp by using either a logical counter or the local clock.
We obtain the unique global timestamp by concatenating the unique local timestamp with the site identifier, which alsomust be unique (Figure 19.3)
The order of concatenation is important! We use the site identifier in the least significant position to ensure that the global timestamps generated in one site are not always greater than those generated in another site.
Compare this technique for generating unique timestamps with the one that we presented in Section 19.2.3 for generating unique names.
We may still have a problem if one site generates local timestamps at a rate faster than that of the other sites.
In such a case, the fast site’s logical counter will be larger than that of other sites.
Therefore, all timestamps generated by the fast site will be larger than those generated by other sites.
What we need is a mechanism to ensure that local timestamps are generated fairly across the system.
We define within each site Si a logical clock (LCi ), which generates the unique local timestamp.
The logical clock can be implemented as a counter that is incremented after a new local timestamp is generated.
To ensure that the various logical clocks are synchronized, we require that a site Si advance its logical clock whenever a transaction Ti with timestamp <x,y> visits that site and x is greater than the current value of LCi.
If the system clock is used to generate timestamps, then timestamps will be assigned fairly, provided that no site has a system clock that runs fast or slow.
Since clocks may not be perfectly accurate, a technique similar to that for logical clocks must be used to ensure that no clock gets far ahead of or behind another clock.
Many commercial databases today support replication, which can take one of several forms.
With master–slave replication, the database allows updates at a primary site, and automatically propagates updates to replicas at other sites.
Transactions may read the replicas at other sites, but are not permitted to update them.
An important feature of such replication is that transactions do not obtain locks at remote sites.
The databasemay be configured to propagate updates immediately after they occur at the primary, or to propagate updates only periodically.
Master–slave replication is particularly useful for distributing information, for instance from a central office to branch offices of an organization.
Another use for this formof replication is in creating a copy of the database to run large queries, so that queries do not interfere with transactions.
Updates should be propagated periodically—every night, for example—so that update propagation does not interfere with query processing.
It also supports snapshot refresh, which can be done either by recomputing the snapshot or by incrementally updating it.
Oracle supports automatic refresh, either continuously or at periodic intervals.
Withmultimaster replication (also called update-anywhere replication) updates are permitted at any replica of a data item, and are automatically propagated to all replicas.
This model is the basic model used to manage replicas in distributed databases.
Transactions update the local copy and the system updates other replicas transparently.
Many database systems use the biased protocol, where writes have to lock and update all replicas and reads lock and read any one replica, as their currencycontrol technique.
Many database systemsprovide an alternative formof updating: They update at one site, with lazy propagation of updates to other sites, instead of immediately applying updates to all replicas as part of the transaction performing the update.
Schemes based on lazy propagation allow transaction processing (including updates) to proceed even if a site is disconnected from the network, thus improving availability, but, unfortunately, do so at the cost of consistency.
One of two approaches is usually followed when lazy propagation is used:
Updates at replicas are translated into updates at a primary site, which are then propagated lazily to all replicas.
This approach ensures that updates to an item are ordered serially, although serializability problems can occur, since transactions may read an old value of some other data item and use it to perform an update.
Updates are performed at any replica and propagated to all other replicas.
This approach can cause even more problems, since the same data item may be updated concurrently at multiple sites.
Some conflicts due to the lack of distributed concurrency control can be detected when updates are propagated to other sites (we shall see how in Section 25.5.4), but resolving the conflict involves rolling back committed transactions, and durability of committed transactions is therefore not guaranteed.
Further, human intervention may be required to deal with conflicts.
The above schemes should therefore be avoided or used with care.
For example, we can use the tree protocol by defining a global tree among the system data items.
Similarly, the timestamp-ordering approach could be directly applied to a distributed environment, as we saw in Section 19.5.2
If we allow deadlocks to occur and rely on deadlock detection, the main problem in a distributed system is deciding how to maintain the wait-for graph.
Common techniques for dealing with this issue require that each site keep a local wait-for graph.
The nodes of the graph correspond to all the transactions (local as well as nonlocal) that are currently either holding or requesting any of the items local to that site.
For example, Figure 19.4 depicts a system consisting of two sites, each maintaining its local wait-for graph.
Clearly, if any local wait-for graph has a cycle, deadlock has occurred.
On the other hand, the fact that there are no cycles in any of the local wait-for graphs does not mean that there are no deadlocks.
To illustrate this problem, we consider the local wait-for graphs of Figure 19.4
Each wait-for graph is acyclic; nevertheless, a deadlock exists in the system because the union of the local wait-for graphs contains a cycle.
In the centralized deadlock detection approach, the system constructs and maintains a global wait-for graph (the union of all the local graphs) in a single site: the deadlock-detection coordinator.
Since there is communication delay in the system, we must distinguish between two types of wait-for graphs.
The real graph describes the real but unknown state of the system at any instance in time, as would be seen by an omniscient observer.
The global wait-for graph can be reconstructed or updated under these conditions:
Whenever a new edge is inserted in or removed from one of the local wait-for graphs.
Periodically, when a number of changes have occurred in a local wait-for graph.
When the coordinator invokes the deadlock-detection algorithm, it searches its global graph.
If it finds a cycle, it selects a victim to be rolled back.
The coordinator must notify all the sites that a particular transaction has been selected as victim.
The likelihood of false cycles is usually sufficiently low that they do not cause a serious performance problem.
A deadlock has indeed occurred and a victim has been picked, while one of the transactions was aborted for reasons unrelated to the deadlock.
At the same time, the coordinator has discovered a cycle, and has picked T3 as a victim.
Deadlock detection can be done in a distributed manner, with several sites taking on parts of the task, instead of it being done at a single site.
One of the goals in using distributed databases is high availability; that is, the database must function almost all the time.
In particular, since failures are more likely in large distributed systems, a distributed database must continue functioning even when there are various types of failures.
The ability to continue functioning even during failures is referred to as robustness.
For a distributed system to be robust, it must detect failures, reconfigure the system so that computation may continue, and recoverwhen a processor or a link is repaired.
The different types of failures are handled in different ways.
Repeated retransmission of amessage across a link, without receipt of an acknowledgment, is usually a symptom of a link failure.
The network usually attempts to find an alternative route for the message.
Failure to find such a route is usually a symptom of network partition.
It is generally not possible, however, to differentiate clearly between site failure and network partition.
The system can usually detect that a failure has occurred, but it may not be able to identify the type of failure.
The problem is partly addressed by usingmultiple links between sites, so that even if one link fails the sites will remain connected.
However, multiple link failure can still occur, so there are situations where we cannot be sure whether a site failure or network partition has occurred.
Suppose that site S1 has discovered that a failure has occurred.
It must then initiate a procedure that will allow the system to reconfigure, and to continue with the normal mode of operation.
However, in some cases, when data objects are replicated it may be possible to proceed with reads and updates even though some replicas are inaccessible.
In this case, when a failed site recovers, if it had replicas of any data object, it must obtain the current values of these data objects, andmust ensure that it receives all future updates.
When a site rejoins, care must be taken to ensure that data at the site are consistent, as we shall see in Section 19.6.3
If a failed site is a central server for some subsystem, an election must be held to determine the new server (see Section 19.6.5)
Examples of central servers include aname server, a concurrency coordinator, or a global deadlock detector.
Since it is, in general, not possible to distinguish between network link failures and site failures, any reconfiguration scheme must be designed to work correctly in case of a partitioning of the network.
In particular, these situations must be avoided to ensure consistency:
Two or more central servers are elected in distinct partitions.
Although traditional database systems place a premium on consistency, there are many applications today that value availability more than consistency.
The design of replication protocols is different for such systems, and is discussed in Section 19.6.6
In this approach, each data object stores with it a version number to detect when it was last written.
Whenever a transaction writes an object it also updates the version number in this way:
If data object a is replicated in n different sites, then a lock-request message must be sent to more than one-half of the n sites at which a is stored.
Read operations look at all replicas on which a lock has been obtained, and read the value from the replica that has the highest version number.
Optionally, they may also write this value back to replicas with lower version numbers.
Writes read all the replicas just like reads to find the highest version number (this step would normally have been performed earlier in the transaction by a read, and the result can be reused)
The new version number is one more than the highest version number.
The write operation writes all the replicas on which it has obtained locks, and sets the version number at all the replicas to the new version number.
If these requirements are violated, the transaction must be aborted.
As long as the requirements are satisfied, the two-phase commit protocol can be used, as usual, on the sites that are available.
In this scheme, reintegration is trivial; nothing needs to be done.
This is because writes would have updated a majority of the replicas, while reads will read a majority of the replicas and find at least one replica that has the latest version.
The version numbering technique usedwith themajority protocol can also be used tomake the quorum consensus protocol work in the presence of failures.We leave the (straightforward) details to the reader.
However, the danger of failures preventing the system from processing transactions increases if some sites are given higher weights.
As a special case of quorum consensus, we can employ the biased protocol by giving unit weights to all sites, setting the read quorum to 1, and setting the write quorum to n (all sites)
In this special case, there is no need to use version numbers; however, if even a single site containing a data item fails, no write to the item can proceed, since the write quorum will not be available.
This protocol is called the read one, write all protocol since all replicas must be written.
To allow work to proceed in the event of failures, we would like to be able to use a read one, write all available protocol.
In this approach, a read operation proceeds as in the read one, write all scheme; any available replica can be read, and a read lock is obtained at that replica.
A write operation is shipped to all replicas; and write locks are acquired on all the replicas.
If a site is down, the transaction manager proceeds without waiting for the site to recover.
While this approach appears very attractive, there are several complications.
In particular, temporary communication failure may cause a site to appear to be unavailable, resulting in a write not being performed, but when the link is restored, the site is not aware that it has to perform some reintegration actions to.
Further, if the network partitions, each partitionmay proceed to update the same data item, believing that sites in the other partitions are all dead.
The read one, write all available scheme can be used if there is never any network partitioning, but it can result in inconsistencies in the event of network partitions.
Reintegration of a repaired site or link into the system requires care.
When a failed site recovers, it must initiate a procedure to update its system tables to reflect changes made while it was down.
If the site had replicas of any data items, it must obtain the current values of these data items and ensure that it receives all future updates.
Reintegration of a site is more complicated than it may seem to be at first glance, since there may be updates to the data items processed during the time that the site is recovering.
An easy solution is to halt the entire system temporarily while the failed site rejoins it.
In most applications, however, such a temporary halt is unacceptably disruptive.
Techniques have been developed to allow failed sites to reintegrate while concurrent updates to data items proceed concurrently.
Before a read or write lock is granted on any data item, the site must ensure that it has caught up on all updates to the data item.
If a failed link recovers, two ormore partitions can be rejoined.
Since a partitioning of the network limits the allowable operations by some or all sites, all sites should be informed promptly of the recovery of the link.
See the bibliographical notes formore information on recovery in distributed systems.
Remote backup systems, which we studied in Section 16.9, and replication in distributed databases are two alternative approaches to providing high availability.
The main difference between the two schemes is that with remote backup systems, actions such as concurrency control and recovery are performed at a single site, and only data and log records are replicated at the other site.
In particular, remote backup systems help avoid two-phase commit, and its resultant overheads.
Also, transactions need to contact only one site (the primary site), and thus avoid the overhead of running transaction code at multiple sites.
Thus remote backup systems offer a lower-cost approach to high availability than replication.
On the other hand, replication can provide greater availability by having multiple replicas available and using the majority protocol.
Several of the algorithms that we have presented require the use of a coordinator.
If the coordinator fails because of a failure of the site atwhich it resides, the system can continue execution only by restarting a new coordinator on another site.
A backup coordinator is a site that, in addition to other tasks, maintains enough information locally to allow it to assume the role of coordinator with minimal disruption to the distributed system.
All messages directed to the coordinator are received by both the coordinator and its backup.
The backup coordinator executes the same algorithms and maintains the same internal state information (such as, for a concurrency coordinator, the lock table) as does the actual coordinator.
The only difference in function between the coordinator and its backup is that the backup does not take any action that affects other sites.
In the event that the backup coordinator detects the failure of the actual coordinator, it assumes the role of coordinator.
Since the backuphas all the information available to it that the failed coordinator had, processing can continue without interruption.
The prime advantage to the backup approach is the ability to continue processing immediately.
If a backup were not ready to assume the coordinator’s responsibility, a newly appointed coordinator would have to seek information from all sites in the system so that it could execute the coordination tasks.
Frequently, the only source of some of the requisite information is the failed coordinator.
In this case, it may be necessary to abort several (or all) active transactions, and to restart them under the control of the new coordinator.
Thus, the backup-coordinator approach avoids a substantial amount of delay while the distributed system recovers from a coordinator failure.
The disadvantage is the overhead of duplicate execution of the coordinator’s tasks.
Furthermore, a coordinator and its backup need to communicate regularly to ensure that their activities are synchronized.
In short, the backup-coordinator approach incurs overhead during normal processing to allow fast recovery from a coordinator failure.
Election algorithms require that a unique identification number be associated with each active site in the system.
The bully algorithm for election works as follows: To keep the notation and the discussion simple, assume that the identification number of site Si is i and that the chosen coordinator will always be the active site with the largest identification number.
Hence, when a coordinator fails, the algorithm must elect the active site that has the largest identification number.
The algorithmmust send this number to each active site in the system.
In addition, the algorithm must provide a mechanism by which a site recovering from a crash can identify the current coordinator.
Suppose that site Si sends a request that is not answered by the coordinator within a prespecified time interval T.
In this situation, it is assumed that the coordinator has failed, and Si tries to elect itself as the site for the new coordinator.
Site Si sends an election message to every site that has a higher identification number.
Site Si then waits, for a time interval T, for an answer from any one of these sites.
If it receives no response within time T, it assumes that all sites with numbers greater than i have failed, and it elects itself as the site for the new coordinator and sends amessage to inform all active sites with identification numbers lower than i that it is the site at which the new coordinator resides.
After a failed site recovers, it immediately begins execution of the same algorithm.
If there are no active sites with higher numbers, the recovered site forces all sites with lower numbers to let it become the coordinator site, even if there is a currently active coordinator with a lower number.
It is for this reason that the algorithm is termed the bully algorithm.
If the network partitions, the bully algorithm elects a separate coordinator in each partition; to ensure that at most one coordinator is elected, winning sites should additionally verify that a majority of the sites are in their partition.
The protocols we have seen so far require a (weighted) majority of sites be in a partition for updates to proceed.
Sites that are in a minority partition cannot process updates; if a network failure results in more than two partitions, no partition may have a majority of sites.
Under such a situation, the system would be completely unavailable for updates, and depending on the read-quorum, may even become unavailable for reads.
Ideally, we would like to have consistency and availability, even in the face of partitions.
Unfortunately, this is not possible, a fact that is crystallized in the so-called CAP theorem, which states that any distributed database can have at most two of the following three properties:
The proof of the CAP theorem uses the following definition of consistency, with replicateddata: an execution of a set of operations (reads andwrites) on replicated data is said to be consistent if its result is the same as if the operations were executed on a single site, in some sequential order, and the sequential order is consistent with the ordering of operations issued by each process (transaction)
The notion of consistency is similar to atomicity of transactions, but with each operation treated as a transaction, and is weaker than the atomicity property of transactions.
In any large-scale distributed system, partitions cannot be prevented, and as a result either of availability or consistency has to be sacrificed.
The schemes we have seen earlier sacrifice availability for consistency in the face of partitions.
Consider a Web-based social-networking system that replicates its data on three servers, and a network partition occurs that prevents the servers from communicating with each other.
Since none of the partitions has a majority, it would not be possible to execute updates on any of the partitions.
If one of these servers is in the same partition as a user, the user actually has access to data, but would be unable to update the data, since another user may be concurrently updating the same object in another partition, which could potentially lead to inconsistency.
Inconsistency is not as great a risk in a social-networking system as in a banking database.
A designer of such a system may decide that a user who can access the system should be allowed to perform updates on whatever replicas are accessible, even at the risk of inconsistency.
In contrast to systems such as banking databases that require the ACID properties, systems such as the social-networking systemmentioned above are said to require the BASE properties:
The primary requirement is availability, even at the cost of consistency.
Soft state refers to the property that the state of the databasemay not be precisely defined, with each replica possibly having a somewhat different state due to partitioning of the network.
Eventually consistent is the requirement that once a partitioning is resolved, eventually all replicas will become consistent with each other.
This last step requires that inconsistent copies of data items be identified; if one is an earlier version of the other, the earlier version can be replaced by the later version.
Restoring consistency in the face of inconsistent updates requires that the updates be merged in some way that is meaningful to the application.
This step cannot be handled by the database; instead the database detects and informs the application about the inconsistency, and the application then decides how to resolve the inconsistency.
In Chapter 13, we saw that there are a variety of methods for computing the answer to a query.
We examined several techniques for choosing a strategy for processing a query that minimize the amount of time that it takes to compute the answer.
For centralized systems, the primary criterion for measuring the cost of a particular strategy is the number of disk accesses.
In a distributed system, we must take into account several other matters, including:
The potential gain in performance from having several sites process parts of.
The relative cost of data transfer over the network and data transfer to and from disk varies widely depending on the type of network and on the speed of the disks.
Thus, in general, we cannot focus solely on disk costs or on network costs.
Rather, we must find a good trade-off between the two.
Consider an extremely simple query: “Find all the tuples in the account relation.” Although the query is simple—indeed, trivial—processing it is not trivial, since the account relation may be fragmented, replicated, or both, as we saw in Section 19.2
If the account relation is replicated, we have a choice of replica to make.
If no replicas are fragmented, we choose the replica for which the transmission cost is lowest.
However, if a replica is fragmented, the choice is not so easy to make, since we need to compute several joins or unions to reconstruct the account relation.
In this case, the number of strategies for our simple example may be large.
Query optimization by exhaustive enumeration of all alternative strategies may not be practical in such situations.
Fragmentation transparency implies that a user may write a query such as:
Using the query-optimization techniques of Chapter 13, we can simplify the preceding expression automatically.
The first involves only account1, and thus can be evaluated at the Hillside site.
The second involves only account2, and thus can be evaluated at the Valleyview site.
There is a further optimization that can be made in evaluating:
Since account1 has only tuples pertaining to the Hillside branch, we can eliminate the selection operation.
This expression is the empty set, regardless of the contents of the account relation.
Thus, our final strategy is for the Hillside site to return account1 as the result.
As we saw in Chapter 13, a major decision in the selection of a query-processing strategy is choosing a join strategy.
Let SI denote the site at which the query was issued.
The system needs to produce the result at site SI.
Among the possible strategies for processing this query are these:
Using the techniques of Chapter 13, choose a strategy for processing the entire query locally at site SI.
Among the factors that must be considered are the volume of data being shipped, the cost of transmitting a block.
Since join is associative and commutative, we can rewrite this expression as:
This strategy is particularly advantageous when relatively few tuples of r2 contribute to the join.
This situation is likely to occur if r1 is the result of a relational-algebra expression involving selection.
If a sufficiently small fraction of tuples in r2 contribute.
For joins of several relations, this strategy can be extended to a series of semijoin steps.
A substantial body of theory has been developed regarding the use of semijoins for query optimization.
Some of this theory is referenced in the bibliographical notes.
Many new database applications require data from a variety of preexisting databases located in a heterogeneous collection of hardware and software environments.
Manipulation of information located in a heterogeneous distributed database requires an additional software layer on top of existing database systems.
A multidatabase system creates the illusion of logical database integration without requiring physical database integration.
Full integration of heterogeneous systems into a homogeneous distributed database is often difficult or impossible:
The investment in application programs based on existing database systems may be huge, and the cost of converting these applications may be prohibitive.
Even if integration is technically possible, it may not be politically possible, because the existing database systems belong to different corporations or organizations.
In such cases, it is important for a multidatabase system to allow the local database systems to retain a high degree of autonomy over the local database and transactions running against that data.
For these reasons, multidatabase systems offer significant advantages that outweigh their overhead.
In this section,weprovide anoverviewof the challenges faced in constructing a multidatabase environment from the standpoint of data definition and query processing.
Each local database management system may use a different data model.
For instance, some may employ the relational model, whereas others may employ olderdatamodels, suchas thenetworkmodel (seeAppendixD) or thehierarchical model (see Appendix E)
Since themultidatabase system is supposed to provide the illusion of a single, integrated database system, a common data model must be used.
A commonly used choice is the relational model, with SQL as the common query language.
Another difficulty is the provision of a common conceptual schema.
Themultidatabase systemmust integrate these separate schemas into one common schema.
Schema integration is a complicated task, mainly because of the semantic heterogeneity.
All these seeminglyminor distinctionsmust be properly recorded in the common global conceptual schema.
As we noted earlier, the alternative of converting each database to a common format may not be feasible without obsoleting existing application programs.
Given a query on a global schema, the query may have to be translated into queries on local schemas at each of the sites where the query has to be executed.
The query results have to be translated back into the global schema.
Wrappers can even be used to provide a relational view of nonrelational data sources, such as Web pages (possibly with forms interfaces), flat files, hierarchical and network databases, and directory systems.
Some data sources may provide only limited query capabilities; for instance, theymay support selections, but not joins.
Theymay even restrict the form of selections, allowing selections only on certain fields; Web data sources with form interfaces are an example of such data sources.
Queries may therefore have to be broken up, to be partly performed at the data source and partly at the site issuing the query.
In general, more than one site may need to be accessed to answer a given query.
Answers retrieved from the sites may have to be processed to remove duplicates.
Global query optimization in a heterogeneous database is difficult, since the query execution system may not know what the costs are of alternative query plans at different sites.
The usual solution is to rely on only local-level optimization, and just use heuristics at the global level.
Mediator systems are systems that integrate multiple heterogeneous data sources, providing an integrated global view of the data and providing query facilities on the global view.
The terms mediator and multidatabase are often used in an interchangeable fashion, and systems that are called mediators may support limited forms of transactions.
These transactions are executed by each local database system outside of the multidatabase system’s control.
The multidatabase system is aware of the fact that local transactions may run at the local sites, but it is not aware of what specific transactions are being executed, or of what data they may access.
Ensuring the local autonomyof eachdatabase system requires that no changes be made to its software.
A database system at one site thus is not able to communicate directly with one at any other site to synchronize the execution of a global transaction active at several sites.
In addition, in case of locking, the local system must be able to guard against the possibility of local deadlocks.
The guarantee of local serializability is not sufficient to ensure global serializability.
Indeed, even if there is no concurrency among global transactions (that is, a global transaction is submitted only after the previous one commits or aborts), local serializability is not sufficient to ensure global serializability (see Practice Exercise 19.14)
Depending on the implementation of the local database systems, a global transaction may not be able to control the precise locking behavior of its local subtransactions.
Thus, even if all local database systems follow two-phase locking, it may be possible only to ensure that each local transaction follows the rules of the protocol.
For example, one local database systemmay commit its subtransaction and release locks, while the subtransaction at another local system is still executing.
If the local systems permit control of locking behavior and all systems follow two-phase locking, then the multidatabase system can ensure that global transactions lock in a two-phasemanner and the lock points of conflicting transactions would then define their global serialization order.
There are many protocols for ensuring consistency despite concurrent execution of global and local transactions in multidatabase systems.
Some are based on imposing sufficient conditions to ensure global serializability.
Others ensure only a form of consistency weaker than serializability, but achieve this consistency by less restrictive means.
Section 26.6 describes approaches to consistency without serializability; other approaches are cited in the bibliographical notes.
Early multidatabase systems restricted global transactions to be read only.
They thus avoided the possibility of global transactions introducing inconsistency to the data, but were not sufficiently restrictive to ensure global serializability.
It is indeed possible to get such global schedules and to develop a scheme to ensure global serializability, and we ask you to do both in Practice Exercise 19.15
There are a number of general schemes to ensure global serializability in an environment where update as well as read-only transactions can execute.
Several of these schemes are based on the idea of a ticket.
A special data item called a ticket is created in each local database system.
Every global transaction that accesses data at a site must write the ticket at that site.
This requirement ensures that global transactions conflict directly at every site they visit.
Furthermore, the global transaction manager can control the order in which global transactions are serialized, by controlling the order in which the tickets are accessed.
If we want to ensure global serializability in an environment where no direct local conflicts are generated in each site, some assumptions must be made about the schedules allowed by the local database system.
For example, if the local schedules are such that the commit order and serialization order are always identical, we can ensure serializability by controlling only the order in which transactions commit.
A related problem in multidatabase systems is that of global atomic commit.
If all local systems follow the two-phase commit protocol, that protocol can be used to achieve global atomicity.
However, local systems not designed to be part of a distributed systemmay not be able to participate in such a protocol.
Even if a local system is capable of supporting two-phase commit, the organization owning the systemmay be unwilling to permit waiting in cases where blocking occurs.
In such cases, compromises may be made that allow for lack of atomicity in certain failure modes.
Further discussion of these matters appears in the literature (see the bibliographical notes)
Initial vendors of software services provided specific customizable applications that they hosted on their own machines.
The concept of cloud computing developed as vendors began to offer generic computers as a service on which clients could run software applications of their choosing.
A client can make arrangements with a cloud-computing vendor to obtain a certain number of machines of a.
Both the number of machines and the amount of storage can grow and shrink as needed.
In addition to providing computing services, many vendors also provide other services such as data storage services, map services, and other services that can be accessed using a Web-service application programming interface.
Many enterprises are finding the model of cloud computing and services beneficial.
It saves client enterprises the need to maintain a large system-support staff and allows new enterprises to begin operation without having to make a large, up-front capital investment in computing systems.
Further, as the needs of the enterprise grow, more resources (computing and storage) can be added as required; the cloud-computing vendor generally has very large clusters of computers, making it easy for the vendor to allocate resources on demand.
They include traditional computing vendors as well as companies, such as Amazon and Google, that are seeking to leverage the large infrastructure they have in place for their core businesses.
Web applications that need to store and retrieve data for very large numbers of users (ranging frommillions to hundreds of millions) have been amajor driver of cloud-based databases.
The needs of these applications differ from those of traditional database applications, since they value availability and scalability over consistency.
Several cloud-based data-storage systems have been developed in recent years to serve the needs of such applications.
We discuss issues in building such data-storage systems on the cloud in Section 19.9.1
In Section 19.9.2, we consider issues in running traditional database systems on a cloud.
Cloud-based databases have features of both homogeneous and heterogeneous systems.Although the data are ownedby one organization (the client) and are part of one unified distributed database, the underlying computers are owned and operated by another organization (the service vendor)
The computers are remote from the client’s location(s) and are accessed over the Internet.
As a result, some of the challenges of heterogeneous distributed systems remain, particularly as regards transaction processing.
However, many of the organizational and political challenges of heterogeneous systems are avoided.
Finally, in Section 19.9.3, we discuss several technical as well as nontechnical challenges that cloud databases face today.
To handle the datamanagement needs of such applications, datamust be partitioned across thousands of processors.
A number of systems for data storage on the cloud have been developed and deployed over the past few years to address data management requirements of such applications; these include Bigtable from Google, Simple Storage Service (S3) from Amazon, which provides a Web interface to Dynamo, which is a keyvalue storage system, Cassandra, from FaceBook, which is similar to Bigtable, and.
Sherpa/PNUTS fromYahoo!, the data storage component of theAzure environment from Microsoft, and several other systems.
In this section, we provide an overview of the architecture of such datastorage systems.
Although some people refer to these systems as distributed database systems, they do not provide many of the features which are viewed as standard on database systems today, such as support for SQL, or for transactions with the ACID properties.
As an example of data management needs of Web applications, consider the profile of a user,whichneeds to be accessible to anumber of different applications that are run by an organization.
The profile contains a variety of attributes, and there are frequent additions to the attributes stored in the profile.
A simple relational representation is often not sufficient for such complex data.
Some cloud-based data-storage systems support XML (described in Chapter 23) for representing such complex data.
The XML and JSON representations provide flexibility in the set of attributes that a record contains, as well as the types of these attributes.
Yet others, such as Bigtable, define their own data model for complex data including support for records with a very large number of optional columns.
We revisit the Bigtable data model later in this section.
Further, many such Web applications either do not need extensive query language support, or at least, can manage without such support.
The primary mode of data access is to store data with an associated key, and to retrieve data with that key.
In the above user profile example, the key for user-profile data would be the user’s identifier.
There are applications that conceptually require joins, but implement the joins by a form of view materialization.
For example, in a social-networking application, each user should be shown new posts from all her friends.
Unfortunately, finding the set of friends and then querying each one to find their posts may lead to a significant amount of delay when the data are distributed across a large number of machines.
An alternative is as follows: whenever a user makes a post, a message is sent to all friends of that user, and the data associated with each of the friends is updated with a summary of the new post.
When that user checks for updates, all required data are available in one place and can be retrieved quickly.
Thus, cloud data-storage systems are, at their core, based on two primitive functions,put(key, value), used to store valueswith anassociatedkey, andget(key), which retrieves the stored value associated with the specified key.
Some systems such as Bigtable additionally provide range queries on key values.
In Bigtable, a record is not stored as a single value, but is instead split into component attributes that are stored separately.
Each attribute value is just a string as far as Bigtable is concerned.
JavaScript Object Notation, or JSON, is a textual representation of complex data types which is widely used for transmitting data between applications, as well as to store complex data.
The above example illustrates objects, which contain (attribute-name, value) pairs, as well as arrays, delimited by square brackets.
Libraries have been developed to transform data between the JSON representation and the object representation used in the JavaScript and PHP scripting languages, as well as other programming languages.
The get() function returns the attribute names alongwith the values.
For efficient retrieval of all attributes of a record, the storage system stores entries sorted by the key, so all attribute values of a particular record are clustered together.
In fact, the record identifier can itself be structured hierarchically, although to Bigtable itself the record identifier is just a string.
For example, an application that stores pages retrieved from a web crawl could map a URL of the form:
Further, a single instance of Bigtable can store data for multiple applications, with multiple tables per application, by simply prefixing the application name and table name to the record identifier.
Data-storage systems typically allow multiple versions of data items to be stored.
Versions are often identified by timestamp, but may be alternatively identified by an integer value that is incremented whenever a new version of a data item is created.
Lookups can specify the required version of a data item, or can pick the version with the highest version number.
Partitioning of data is, of course, the key to handling extremely large scale in data-storage systems.
Unlike regular parallel databases, it is usually not possible to decide on a partitioning function ahead of time.
Further, if load increases, more servers need to be added and each server should be able to take on parts of the load incrementally.
To solve both these problems, data-storage systems typically partition data into relatively small units (small on such systems may mean of the order of hundreds of megabytes)
These partitions are often called tablets, reflecting the fact that each tablet is a fragment of a table.
The partitioning of data should be done on the search key, so that a request for a specific key value is directed to a single tablet; otherwise each request would require processing at multiple sites, increasing the load on the system greatly.
Two approaches are used: either range partitioning is used directly on the key, or a hash function is applied on the key, and range partitioning is applied on the result of the hash function.
The site to which a tablet is assigned acts as the master site for that tablet.
All updates are routed through this site, and updates are then propagated to replicas of the tablet.
Lookups are also sent to the same site, so that reads are consistent with writes.
The partitioning of data into tablets is not fixed up front, but happens dynamically.
As data are inserted, if a tablet grows too big, it is broken into smaller parts.
Further, even if a tablet is not large enough to merit being broken up, if the load (get/put operations) on that tablet are excessive, the tablet may be broken into smaller tablets, which can be distributed across two or more sites to share the load.
Usually the number of tablets is much larger than the number of sites, for the same reason that virtual partitioning is used in parallel databases.
It is important to know which site in the overall system is responsible for a particular tablet.
This can be done by having a tablet controller site which tracks the partitioning function, to map a get() request to one or more tablets, and a mapping function from tablets to sites, to find which site were responsible for which tablet.
Each request coming into the system must be routed to the correct site; if a single tablet controller site is responsible for this task, it would soon.
Instead, the mapping information can be replicated on a set of router sites, which route requests to the site with the appropriate tablet.
Protocols to update mapping information when a tablet is split or moved are designed in such a way that no locking is used; a request may as a result end up at a wrong site.
The problem is handled by detecting that the site is no longer responsible for the key specified by the request, and rerouting the request based on up-to-date mapping information.
Figure 19.7 depicts the architecture of a cloud data-storage system, based loosely on the PNUTS architecture.
Other systems provide similar functionality, although their architecture may vary.
For example, Bigtable does not have separate routers; the partitioning and tablet-server mapping information is stored in the Google file system, and clients read the information from the file system, and decide where to send their requests.
Data-storage systems on the cloud typically do not fully support ACID transactions.
The cost of two-phase commit is too high, and two-phase commit can lead to blocking in the event of failures, which is not acceptable to typical Web applications.
This means that such systems typically do not even support a transactionally consistent secondary index: the secondary index would be partitioned on a different attribute from the key used for storing the data, and an insert or update would then need to update two sites, which would require two-phase commit.
At best, such systems support transactions on data within a single tablet, which is controlled by a a single master site.
If the current version number of the data item is more recent than the specified version number, the update is not performed.
The test-and-set function can be used by applications to implement a limited form of validation-based concurrency control, with validation restricted to data items in a single tablet.
In a system with thousands of sites, at any time it is almost guaranteed that several of the sites will be down.
A data-storage system on the cloud must be able to continue normal processing even with many sites down.
Such systems replicate data (such as tablets) to multiple machines in a cluster, so that a copy of the data is likely to be available even if some machines of a cluster are down.
A cluster is a collection of machines in a data center.
For example, the Google File System (GFS), which is a distributed fault-tolerant file system, replicates all file system blocks at three or more nodes in a cluster.
Normal operation can continue as long as at least one copy of the data is available (key system data, such as the mapping of files to nodes, is replicated at more nodes, a majority of which need to be available)
In addition, replication is also used across geographically distributed clusters, for reasons that we shall see shortly.
Since each tablet is controlled by a single master site, if the site fails the tablet should be reassigned to a different site that has a copy of the tablet,which becomes the new master site for the tablet.
Updates to a tablet are logged, and the log is itself replicated.When a site fails, the tablets at the site are assigned to other sites; the new master site of each tablet is responsible for performing recovery actions using the log to bring its copy of the tablet to an up-to-date consistent state, after which updates and lookups can be performed on the tablet.
In Bigtable, as an example, mapping information is stored in an index structure, and the index as well as the actual tablet data are stored in the file system.
Tablet data updates are not flushed immediately, but log data are.
The file system ensures that the file system data are replicated and will be available even in the face of failure of a few nodes in the cluster.
Thus, when a tablet is reassigned, the new master site for the tablet has access to up-to-date log data.
Replication at a remote site is therefore essential for high availability.
For many Web applications, round-trip delays across a long-distance network can affect performance significantly, a problem that is increasing with the use of Ajax applications that require multiple rounds of communication between the browser and the application.
To deal with this problem, users are connectedwith application servers that are closest to them geographically, and data are replicated at multiple data centers so that one of the replicas is likely to be close to the application server.
However, the danger of partitioning of the network is increased as a result.
Given thatmostWeb applications place a greater premiumon availability than on consistency, data-storage systems on the cloud usually allow updates to proceed.
Multimaster replication with lazy propagation of updates, which we saw in Section 19.5.3, is typically used for processing updates.
Lazy propagation implies that updates are not propagated to replicas as part of the updating transaction, although they are typically propagated as soon as possible, typically using a messaging infrastructure.
In addition to propagating updates to replicas of a data item, updates to secondary indices, or to certain kinds of materialized views (such as the updates from friends, in a social-networking applicationwe sawearlier in Section 19.9.1.1), can be sent using the messaging infrastructure.
Secondary indices are basically tables, partitioned just like regular tables, based on the index search key; an update of a record in a table can be mapped to updates of one or more tablets in a secondary index on the table.
There is no transactional guarantee on the updates of such secondary indices or materialized views, and only a best-effort guarantee in terms of when the updates reach their destination.
We now consider the issue of implementing a traditional distributed database system, supporting ACID properties and queries, on a cloud.
The concept of computing utilities is an old one, envisioned back in the 1960s.
The firstmanifestation of the conceptwas in timesharing systems inwhich several users shared access to a single mainframe computer.
Cloud computing makes extensive use of the virtual-machine concept to provide computing services.
Virtual machines provide great flexibility since clients may choose their own software environment including not only the application software but also the operating system.
Virtual machines of several clients can run on a single physical computer, if the computing needs of the clients are low.
On the other hand, an entire computer can be allocated to each virtual machine of a client whose virtual machines have a high load.
A client may request several virtual machines over which to run an application.
This makes it easy to add or subtract computing power as workloads grow and shrink simply by adding or releasing virtual machines.
Having a set of virtual machines works well for applications that are easily parallelized.
Database systems, as we have seen, fall into this category.
Each virtual machine can run database system code locally and behave in a manner similar to a site in a homogeneous distributed database system.
Cloud-based databases certainly have several important advantages compared to building a computing infrastructure from scratch, and are in fact essential for certain applications.
However, cloud-based database systems also have several disadvantages that we shall now explore.
Unlike purely computational applications inwhich parallel computations run largely independently, distributed database systems require frequent communication and coordination among sites for:
In our earlier study of distributed databases, we assumed (implicitly) that the database administrator had control over the physical location of data.
In a cloud system, the physical location of data is under the control of the vendor, not the client.
As a result, the physical placement of data may be suboptimal in terms of communication cost, and this may result in a large number of remote lock requests and large transfers of data across virtual machines.
Effective query optimization requires that the optimizer have accurate cost measures for operations.
Lacking knowledge of the physical placement of data, the optimizer has to rely on estimates that may be highly inaccurate, resulting in poor execution strategies.
Because remote accesses are relatively slow compared to local access, these issues can have a significant impact on performance.
The above issues are a particular challenge for implementing traditional database applications on the cloud, although less challenging for simple datastorage systems.
The next few challenges we discuss apply equally to both application scenarios.
Indeed many contracts have clauses imposing penalties on the vendor if a certain level of availability is not maintained.
This replication is done by the vendor without specific knowledge of the application.
Since replication is under control of the cloud and not under the control of the database system, care must be used when the database system accesses data so as to ensure that the latest versions of the data are read.
Failure to take these issues properly into account can result in a loss of the atomicity or isolation properties.
In many current cloud database applications, the application itself may need to take some responsibility for consistency.
Users of cloud computing must be willing to accept that their data are held by another organization.
This may present a variety of risks in terms of security and legal liability.
If the cloud vendor suffers a security breach, client data may be divulged, causing the client to face legal challenges from its customers.
Yet the client has no direct control over cloud-vendor security.
These issues become more complex if the cloud vendor chooses to store data (or replicas of data) in a foreign country.
So, for example, if a German company’s data are replicated on a server in New York, then the privacy laws of the United States rather than those of Germany or the.
The cloud vendor might be required to release client data to the U.S.
Specific cloud vendors offer their clients varying degrees of control over how their data are distributed and replicated.
Some vendors offer database services directly to their clients rather than require clients to contract for raw storage and virtual machines over which to run their own database systems.
The market for cloud services continues to evolve rapidly, but it is clear that a database administrator who is contracting for cloud services has to consider a wide variety of technical, economic, and legal issues in order to ensure the privacy and security of data, guarantees of the ACID properties (or an acceptable approximation thereof), and adequate performance despite the likelihood of data being distributed over a wide geographic area.
The bibliographical notes provide some of the current thinking on these topics.
Much new literature is likely to appear in the next few years, and many of the current issues in cloud databases are being addressed by the research community.
Consider an organization that wishes to make data about its employees available to a variety of people in the organization; examples of the kinds of data include name, designation, employee-id, address, email address, phone number, fax number, and so on.
In the precomputerization days, organizations would create physical directories of employees and distribute them across the organization.
In the world of physical telephone directories, directories that satisfy lookups in the forward direction are called white pages, while directories that satisfy lookups in the reverse direction are called yellow pages.
In today’s networked world, the need for directories is still present and, if anything, even more important.
However, directories today need to be available over a computer network, rather than in a physical (paper) form.
Directory information can be made available through Web interfaces, as many organizations, and phone companies in particular, do.
Directories can be used for storing other types of information, much like file system directories.
For instance, Web browsers can store personal bookmarks and other browser settings in a directory system.
A user can thus access the same settings from multiple locations, such as at home and at work, without having to share a file system.
Several directory access protocols have been developed to provide a standardizedway of accessing data in a directory.
The most widely used among them today is the Lightweight Directory Access Protocol (LDAP)
Obviously all the types of data in our examples can be stored without much trouble in a database system, and accessed through protocols such as JDBC or ODBC.
The question then is,why comeupwith a specialized protocol for accessing directory information? There are at least two answers to the question.
First, directory access protocols are simplified protocols that cater to a limited type of access to data.
Second, and more important, directory systems provide a simple mechanism to name objects in a hierarchical fashion, similar to file system directory names, which can be used in a distributed directory system to specify what information is stored in each of the directory servers.
For example, a particular directory server may store information for Bell Laboratories employees in Murray Hill, while another may store information for Bell Laboratories employees in Bangalore, giving both sites autonomy in controlling their local data.
The directory access protocol can be used to obtain data from both directories across a network.
More important, the directory system can be set up to automatically forward queries made at one site to the other site, without user intervention.
For these reasons, several organizations have directory systems to make organizational information available online through a directory access protocol.
Information in an organizational directory can be used for a variety of purposes, such as to find addresses, phone numbers, or email addresses of people, to find which departments people are in, and to track department hierarchies.
Directories are also used to authenticate users: applications can collect authentication information such as passwords from users and authenticate them using the directory.
As may be expected, several directory implementations find it beneficial to use relational databases to store data, instead of creating special-purpose storage systems.
In general adirectory system is implemented as oneormore servers,which service multiple clients.
Clients use the application programmer interface defined by the directory system to communicate with the directory servers.
Directory access protocols also define a data model and access control.
The X.500 directory access protocol, defined by the International Organization for Standardization (ISO), is a standard for accessing directory information.
However, the protocol is rather complex, and is notwidely used.
TheLightweight Directory Access Protocol (LDAP) providesmany of the X.500 features, but with less complexity, and is widely used.
In the rest of this section, we shall outline the data model and access protocol details of LDAP.
In LDAP, directories store entries, which are similar to objects.
Each entry must have a distinguished name (DN), which uniquely identifies the entry.
A DN is in turn made up of a sequence of relative distinguished names (RDNs)
For example, an entry may have the following distinguished name:
As you can see, the distinguished name in this example is a combination of a name and (organizational) address, starting with a person’s name, then giving the organizational unit (ou), the organization (o), and country (c)
The order of the components of a distinguished name reflects the normal postal address order, rather than the reverse order used in specifying path names for files.
The set of RDNs for a DN is defined by the schema of the directory system.
Moreover, entries can be specified to be of one or more object classes.
It is not necessary that there be a single most-specific object class to which an entry belongs.
Entries are organized into a directory information tree (DIT), according to their distinguished names.
Entries at the leaf level of the tree usually represent specific objects.
Entries that are internal nodes represent objects such as organizational units, organizations, or countries.
The children of a node have a DN containing all the RDNs of the parent, and one or more additional RDNs.
For instance, an internal node may have a DN c=USA, and all entries below it have the value USA for the RDN c.
The entire distinguished name need not be stored in an entry.
The system can generate the distinguished name of an entry by traversing up the DIT from the entry, collecting the RDN=value components to create the full distinguished name.
Entries may have more than one distinguished name—for example, an entry for a person in more than one organization.
To deal with such cases, the leaf level of a DIT can be an alias, which points to an entry in another branch of the tree.
Unlike SQL, LDAP does not define either a data-definition language or a datamanipulation language.
However, LDAP defines a network protocol for carrying out data definition andmanipulation.
Users of LDAP can either use an application programming interface or use tools provided by various vendors to perform data definition and manipulation.
The querying mechanism in LDAP is very simple, consisting of just selections and projections, without any join.
A base—that is, a node within a DIT—by giving its distinguished name (the path from the root to the node)
A search condition, which can be a Boolean combination of conditions on individual attributes.
Equality, matching by wild-card characters, and approximate equality (the exact definition of approximate equality is system dependent) are supported.
A scope, which can be just the base, the base and its children, or the entire subtree beneath the base.
The query can also specify whether to automatically dereference aliases; if alias dereferences are turned off, alias entries can be returned as answers.
One way of querying an LDAP data source is by using LDAP URLs.
The first URL returns all attributes of all entries at the server with organization being Yale University, and country being USA.
The second URL executes a search query (selection) cn=Silberschatz on the subtree of the node with distinguished name o=Yale University, c=USA.
The first field is the distinguished name, here o=Yale University,c=USA.
The second field, the list of attributes to return, is left empty, meaning return all attributes.
The third attribute, sub, indicates that the entire subtree is to be searched.
A second way of querying an LDAP directory is by using an application programming interface.
Figure 19.8 shows a piece of C code used to connect to an LDAP server and run a query against the server.
The code first opens a connection to an LDAP server by ldap open and ldap bind.
The arguments to ldap search s are the LDAP connection handle, the DN of the base from which the search should be done, the scope of the search, the search condition, the list of attributes to be returned, and an attribute called attrsonly, which, if set to 1, would result in only the schema of the result being returned, without any actual tuples.
The last argument is an output argument that returns the result of the search as an LDAPMessage structure.
The first for loop iterates over and prints each entry in the result.
Note that an entry may have multiple attributes, and the second for loop prints each attribute.
Since attributes in LDAP may be multivalued, the third for loop prints each value of an attribute.
The calls ldap msgfree and ldap value free free memory that is.
Figure 19.8 does not show code for handling error conditions.
The LDAP API also contains functions to create, update, and delete entries, as well as other operations on the DIT.
Each function call behaves like a separate transaction; LDAP does not support atomicity of multiple updates.
Information about an organization may be split into multiple DITs, each of which stores information about some entries.
RDN=value pairs that identify what information the DIT stores; the pairs are concatenated to the rest of the distinguished name generated by traversing from the entry to the root.
For instance, the suffix of a DIT may be o=Lucent, c=USA, while another may have the suffix o=Lucent, c=India.
A node in a DIT may contain a referral to another node in another DIT; for instance, the organizational unit Bell Labs under o=Lucent, c=USA may have its own DIT, in which case the DIT for o=Lucent, c=USA would have a node ou=Bell Labs representing a referral to the DIT for Bell Labs.
Referrals are the key component that help organize a distributed collection of directories into an integrated system.
When a server gets a query on a DIT, it may return a referral to the client, which then issues a query on the referenced DIT.
Access to the referenced DIT is transparent, proceeding without the user’s knowledge.
Alternatively, the server itself may issue the query to the referred DIT and return the results along with locally computed results.
The hierarchical naming mechanism used by LDAP helps break up control of information across parts of an organization.
The referral facility then helps integrate all the directories in an organization into a single virtual directory.
Although it is not an LDAP requirement, organizations often choose to break up information either by geography (for instance, an organization may maintain a directory for each site where the organization has a large presence) or by organizational structure (for instance, each organizational unit, such as department, maintains its own directory)
Many LDAP implementations support master–slave and multimaster replication of DITs, although replication is not part of the current LDAP version 3 standard.
A distributed database system consists of a collection of sites, each of which maintains a local database system.
Each site is able to process local transactions: those transactions that access data in only that single site.
In addition, a sitemay participate in the execution of global transactions: those transactions that access data in several sites.
The execution of global transactions requires communication among the sites.
Distributed databases may be homogeneous, where all sites have a common schema and database system code, or heterogeneous, where the schemas and system codes may differ.
There are several issues involved in storing a relation in the distributed database, including replication and fragmentation.
It is essential that the system minimize the degree to which a user needs to be aware of how a relation is stored.
A distributed systemmay suffer from the same types of failure that can afflict a centralized system.
Each of these problems needs to be considered in the design of a distributed recovery scheme.
To ensure atomicity, all the sites in which a transaction T executedmust agree on the final outcome of the execution.
To ensure this property, the transaction coordinator ofTmust execute a commit protocol.
The most widely used commit protocol is the two-phase commit protocol.
The two-phase commit protocol may lead to blocking, the situation in which the fate of a transaction cannot be determined until a failed site (the coordinator) recovers.
We can use the three-phase commit protocol to reduce the probability of blocking.
Persistent messaging provides an alternative model for handling distributed transactions.
The model breaks a single transaction into parts that are executed at different databases.
Persistent messages (which are guaranteed to be delivered exactly once, regardless of failures), are sent to remote sites to request actions to be taken there.
While persistent messaging avoids the blocking problem, application developers have to write code to handle various types of failures.
In the case of locking protocols, the only change that needs to be incorporated is in the way that the lock manager is implemented.
Protocols for handling replicated data include the primary copy, majority, biased, and quorum consensus protocols.
These have different trade-offs in terms of cost and ability to work in the presence of failures.
In the case of timestamping and validation schemes, the only needed change is to develop a mechanism for generating unique global timestamps.
Many database systems support lazy replication, where updates are propagated to replicas outside the scope of the transaction that performed the update.
Such facilities must be used with great care, since they may result in nonserializable executions.
To provide high availability, a distributed database must detect failures, reconfigure itself so that computation may continue, and recover when a processor or a link is repaired.
The task is greatly complicated by the fact that it is hard to distinguish between network partitions and site failures.
The majority protocol can be extended by using version numbers to permit.
While the protocol has a significant overhead, it works regardless of the type of failure.
Less-expensive protocols are available to deal with site failures, but they assume network partitioning does not occur.
Some of the distributed algorithms require the use of a coordinator.
To provide high availability, the systemmustmaintain a backup copy that is ready to assume responsibility if the coordinator fails.
Another approach is to choose the new coordinator after the coordinator has failed.
The algorithms that determinewhich site should act as a coordinator are called election algorithms.
Queries on a distributed database may need to access multiple sites.
Several optimization techniques are available to identify the best set of sites to access.
Queries can be rewritten automatically in terms of fragments of relations and then choices can be made among the replicas of each fragment.
Semijoin techniques may be employed to reduce data transfer involved in joining relations (or fragments or relicas thereof) across distinct sites.
Heterogeneous distributed databases allow sites to have their own schemas and database system code.
Amultidatabase system provides an environment in which new database applications can access data from a variety of preexisting databases located in various heterogeneous hardware and software environments.
Amultidatabase system creates the illusion of logical database integration, without requiring physical database integration.
A large number of data-storage systems on the cloud have been built in recent years, in response to data storage needs of extremely large-scale Web applications.
These data-storage systems allow scalability to thousands of nodes, with geographic distribution, and high availability.
However, they do not support the usual ACID properties, and they achieve availability during partitions at the cost of consistency of replicas.
Current data-storage systems also do not support SQL, and provide only a simple put()/get() interface.
While cloud computing is attractive even for traditional databases, there are several challenges due to lack of control on data placement and geographic replication.
Directory systems can be viewed as a specialized form of database, where information is organized in a hierarchical fashion similar to the way files are organized in a file system.
Directories are accessed by standardized directory access protocols such as LDAP.
Directories can contain referrals to other directories, which help build an integrated view whereby a query is sent to a single directory, and it is transparently executed at all relevant directories.
Which items in your list from part a are also applicable to a centralized system?
What implications does your answer have for recovery in distributed systems?
Suggest an alternative scheme based on sequence numbers instead of timestamps.
The edge (Ti , Tj , n) is inserted in the local wait-for graph of S1
The edge (Ti , Tj , n) is inserted in the local wait-for graph of S3 only if Tj has received the request message and cannot immediately grant the requested resource.
A request from Ti to Tj in the same site is handled in the usual manner; no timestamps are associated with the edge (Ti , Tj )
A central coordinator invokes the detection algorithm by sending an initiating message to each site in the system.
On receiving this message, a site sends its local wait-for graph to the coordinator.
Note that such a graph contains all the local information that the site has about the state of the real graph.
The wait-for graph reflects an instantaneous state of the site, but it is not synchronized with respect to any other site.
When the controller has received a reply from each site, it constructs a graph as follows:
The graph contains a vertex for every transaction in the system.
The graph has an edge (Ti , Tj ) if and only if:
Show that, if there is a cycle in the constructed graph, then the system is in a deadlock state, and that, if there is no cycle in the constructed graph, then the system was not in a deadlock state when the execution of the algorithm began.
Assume that each fragment has two replicas: one stored at the New York site and one stored locally at the plant site.
Describe a good processing strategy for the following queries entered at the San Jose site.
Suggest ways in which the multidatabase system can ensure that there is at most one active global transaction at any time.
Show by example that it is possible for a nonserializable global schedule to result despite the assumptions.
Show by example that nonserializable executionsmay result in such a system.
Show how you could use a ticket scheme to ensure global serializability.
Chapter 15 to a distributed database, the site responsible for the root of the DAG may become a bottleneck.
All transactions are given all possible intention-mode locks on the.
Show that these modifications alleviate this problem without allowing any nonserializable schedules.
Assume that the employee relation is fragmented horizontally by plant number, and that each fragment is stored locally at its corresponding plant site.
Assume that the machine relation is stored in its entirety at the Armonk site.
Describe a good strategy for processing each of the following queries.
Find all employees at plants that contain machines whose type is “milling machine.”
Textbook discussions of distributed databases are offered by Ozsu and Valduriez [1999]
The implementation of the transaction concept in a distributed database is presented byGray [1981] andTraiger et al.
Distributed concurrency control is covered by Bernstein and Goodman [1981]
The transaction manager of R* is described in Mohan et al.
The problem of concurrent updates to replicated data was revisited in the context of data warehouses by Gray et al.
The user manuals of various database systems provide details of how they handle replication and consistency.
Huang and Garcia-Molina [2001] addresses exactly-once semantics in a replicated messaging system.
Dynamic query optimization in multidatabases is addressed by Ozcan et al.
Transaction processing in multidatabase systems is discussed in Mehrotra et al.
A collection of papers on data management on cloud systems is in Ooi and S.
The implementation of Google’s Bigtable is described in Chang et al.
Experience in building a database using Amazon’s S3 cloud-based storage is described in Brantner et al.
An approach to making transactions work correctly in cloud systems is discussed in Lomet et al.
Database queries are often designed to extract specific information, such as the balance of an account or the sum of a customer’s account balances.
However, queries designed to help formulate a corporate strategy usually requires access to large amounts of data originating at multiple sources.
A data warehouse is a repository of data gathered frommultiple sources and stored under a common, unified database schema.
For example, a retailer may discover that certain products tend to be purchased together, and may use that information to develop marketing strategies.
This process of knowledge discovery from data is called data mining.
Chapter 20 addresses the issues of data warehousing and data mining.
However, there is an enormous amount of unstructured textual data on the Internet, on intranets within organizations, and on the computers of individual users.
Users wish to find relevant information from this vast body of mostly textual information, using simple query mechanisms such as keyword queries.
The field of information retrieval deals with querying of such unstructured data, and pays particular attention to the ranking of query results.
Although this area of research is several decades old, it has undergone tremendous growth with the development of the World WideWeb.
Chapter 21 provides an introduction to the field of information retrieval.
Businesses have begun to exploit the burgeoning data online to make better decisions about their activities, such as what items to stock and how best to target customers to increase sales.
The first aspect is to gather data frommultiple sources into a central repository, called a datawarehouse.
Issues involved inwarehousing include techniques for dealing with dirty data, that is, data with some errors, and with techniques for efficient storage and indexing of large volumes of data.
The second aspect is to analyze the gathered data to find information or knowledge that can be the basis for business decisions.
Another approach to getting knowledge from data is to use data mining, which aims at detecting various types of patterns in large volumes of data.
Data mining supplements various types of statistical techniques with similar goals.
Decision-support systems help managers to decide what products to stock in a shop, what products to manufacture in a factory, or which of the applicants should be admitted to a university.
For example, company databases often contain enormous quantities of information about customers and transactions.
The size of the information storage required may range up to hundreds of gigabytes, or even terabytes, for large.
Transaction information for a retailer may include the name or identifier (such as credit-card number) of the customer, the items purchased, the price paid, and the dates on which the purchases were made.
Information about the items purchased may include the name of the item, the manufacturer, the model number, the color, and the size.
Customer informationmay include credit history, annual income, residence, age, and even educational background.
Such large databases can be treasure troves of information for making business decisions, such as what items to stock and what discounts to offer.
For instance, a retail company may notice a sudden spurt in purchases of flannel shirts in the Pacific Northwest, may realize that there is a trend, and may start stocking a larger number of such shirts in shops in that area.
As another example, a car company may find, on querying its database, that most of its small sports cars are bought by young women whose annual incomes are above $50,000
The company may then target its marketing to attract more such women to buy its small sports cars, andmay avoid wastingmoney trying to attract other categories of people to buy those cars.
In both cases, the company has identified patterns in customer behavior and has used the patterns to make business decisions.
The storage and retrieval of data for decision support raises several issues:
Although many decision-support queries can be written in SQL, others either cannot be expressed in SQL or cannot be expressed easily in SQL.
Several SQL extensions have therefore been proposed to make data analysis easier.
In Section 5.6, we covered SQL extensions for data analysis and techniques for online analytical processing (OLAP)
Database query languages are not suited to the performance of detailed statistical analyses of data.
There are several packages, such as SAS and S++, that help in statistical analysis.
Such packages have been interfaced with databases, to allow large volumes of data to be stored in the database and retrieved efficiently for analysis.
The field of statistical analysis is a large discipline on its own; see the references in the bibliographical notes for more information.
For performance reasons (as well as for reasons of organization control), the data sources usually will not permit other parts of the company to retrieve data on demand.
To execute queries efficiently on such diverse data, companies have built data warehouses.
Data warehouses gather data from multiple sources under a unified schema, at a single site.
Thus, they provide the user a single uniform interface to data.We study issues in building andmaintaining a data warehouse in Section 20.2
The area of decision support can be broadly viewed as covering all the above areas, although some people use the term in a narrower sense that excludes statistical analysis and data mining.
Large companies have presences in many places, each of which may generate a large volume of data.
For instance, large retail chains have hundreds or thousands of stores, whereas insurance companies may have data from thousands of local branches.
Further, large organizations have a complex internal organization structure, and therefore different data may be present in different locations, or on different operational systems, or under different schemas.
Corporate decision makers require access to information from multiple such sources.
A data warehouse is a repository (or archive) of information gathered from multiple sources, stored under a unified schema, at a single site.
Once gathered, the data are stored for a long time, permitting access to historical data.
Thus, data warehouses provide the user a single consolidated interface to data, making decision-support queries easier to write.
Figure 20.1 shows the architecture of a typical data warehouse, and illustrates the gathering of data, the storage of data, and the querying and data analysis support.
Among the issues to be addressed in building a warehouse are the following:
Credit bureaus are companies that gather information about consumers from multiple sources and compute a creditworthiness score for each consumer.
In a source-driven architecture for gathering data, the data sources transmit new information, either continually (as transaction processing takes place), or periodically (nightly, for example)
In a destination-driven architecture, the data warehouse periodically sends requests for new data to the sources.
Unless updates at the sources are replicated at the warehouse via twophase commit, thewarehousewill never be quite up-to-datewith the sources.
Two-phase commit is usually far too expensive to be an option, so data warehouses typically have slightly out-of-date data.
That, however, is usually not a problem for decision-support systems.
What schema to use.Data sources that have been constructed independently are likely to have different schemas.
Part of the task of a warehouse is to perform schema integration, and to convert data to the integrated schema before they are stored.
As a result, the data stored in the warehouse are not just a copy of the data at the sources.
Instead, they can be thought of as a materialized view of the data at the sources.
Data transformation and cleansing.The task of correcting and preprocessing data is called data cleansing.
Data sources often deliver data with numerous minor inconsistencies, which can be corrected.
For example, names are often misspelled, and addressesmay have street, area, or city names misspelled, or postal codes entered incorrectly.
These can be corrected to a reasonable extent by consulting a database of street names and postal codes in each city.
The approximate matching of data required for this task is referred to as fuzzy lookup.
Address lists collected from multiple sources may have duplicates that need to be eliminated in a merge–purge operation (this operation is also referred to as deduplication)
Data may be transformed in ways other than cleansing, such as changing the units of measure, or converting the data to a different schema by joining data from multiple source relations.
Data warehouses typically have graphical tools to support data transformation.
Such tools allow transformation to be specified as boxes, and edges can be created between boxes to indicate the flow of data.
Conditional boxes can route data to an appropriate next step in transformation.
See Figure 30.7 for an example of a transformation specified using the graphical tool provided by Microsoft SQL Server.
Updates on relations at the data sources must be propagated to the data warehouse.
If the relations at the data warehouse are exactly the same as those at the data source, the propagation is straightforward.
If they are not, the problem of propagating updates is basically the view-maintenance problem, which was discussed in Section 13.5
However, we can answer many queries by maintaining just summary data obtained by aggregation on a relation, rather than maintaining the entire relation.
For example, instead of storing data about every sale of clothing, we can store total sales of clothing by item name and category.
Suppose that a relation r has been replaced by a summary relation s.
Users may still be permitted to pose queries as though the relation r were available online.
If the query requires only summary data, it may be possible to transform it into an equivalent one using s instead; see Section 13.5
The different steps involved in getting data into a data warehouse are called extract, transform, and load or ETL tasks; extraction refers to getting data from the sources, while load refers to loading the data into the data warehouse.
Datawarehouses typically have schemas that are designed for data analysis, using tools such as OLAP tools.
Thus, the data are usually multidimensional data, with dimension attributes andmeasure attributes.
A table recording sales information for a retail store, with one tuple for each item that is sold, is a typical example of a fact table.
The dimensions of the sales table would include what the item is (usually an item identifier such as that used in bar codes), the date when the item is sold, which location (store) the item was sold from, which customer bought the item, and so on.
The measure attributes may include the number of items sold and the price of the items.
To minimize storage requirements, dimension attributes are usually short identifiers that are foreign keys into other tables called dimension tables.
For instance, a fact table sales would have attributes item id, store id, customer id, and.
The attribute store id is a foreign key into a dimension table store, which has other attributes such as store location (city, state, country)
The item id attribute of the sales table would be a foreign key into a dimension table item info, which would contain information such as the name of the item, the category to which the item belongs, and other item details such as color and size.
The customer id attribute would be a foreign key into a customer table containing attributes such as name and address of the customer.
We can also view the date attribute as a foreign key into a date info table giving the month, quarter, and year of each date.
Such a schema, with a fact table, multiple dimension tables, and foreign keys from the fact table to the dimension tables, is called a star schema.
More complex data-warehouse designs may have multiple levels of dimension tables; for instance, the item info table may have an attribute manufacturer id that is a foreign key into another table giving details of the manufacturer.
Complex datawarehouse designs may also have more than one fact table.
Column-oriented storage has at least two major benefits over row-oriented storage:
When a query needs to access only a few attributes of a relation with a large number of attributes, the remaining attributes need not be fetched from disk into memory.
In contrast, in row-oriented storage, not only are irrelevant attributes fetched into memory, but they may also get prefetched into processor cache, wasting cache space and memory bandwidth, if they are stored adjacent to attributes used in the query.
Storing values of the same type together increases the effectiveness of compression; compression can greatly reduce both the disk storage cost and the time to retrieve data from disk.
On the other hand, column-oriented storage has the drawback that storing or fetching a single tuple requires multiple I/O operations.
However, column-oriented storage is gaining increasing acceptance for data-warehousing applications, where accesses are rarely to individual tuples, but rather require scanning and aggregating multiple tuples.
Sybase IQ was one of the early products to use column-oriented storage, but there are now several research projects and companies that have developed databases based on column-oriented storage systems.
These systems have been able to demonstrate significant performance gains for many data-warehousing applications.
See the bibliographical notes for references on how column-oriented stores are implemented, and queries optimized and processed on such stores.
The termdatamining refers loosely to the process of semiautomatically analyzing large databases to find useful patterns.
Like knowledge discovery in artificial intelligence (also called machine learning) or statistical analysis, data mining attempts to discover rules and patterns from data.
However, data mining differs from machine learning and statistics in that it deals with large volumes of data, stored primarily on disk.
That is, data mining deals with “knowledge discovery in databases.”
Some types of knowledge discovered from a database can be represented by a set of rules.
The following is an example of a rule, stated informally: “Young women with annual incomes greater than $50,000 are the most likely people to buy small sports cars.” Of course such rules are not universally true, and have degrees of “support” and “confidence,” as we shall see.
Other types of knowledge are represented by equations relating different variables to each other, or by other mechanisms for predicting outcomes when the values of some variables are known.
There are a variety of possible types of patterns that may be useful, and different techniques are used to find different types of patterns.
We shall study a few examples of patterns and see how they may be automatically derived from a database.
Usually there is amanual component todatamining, consisting of preprocessing data to a form acceptable to the algorithms and postprocessing of discovered patterns to find novel ones that could be useful.
There may also be more than one type of pattern that can be discovered from a given database, and manual interaction may be needed to pick useful types of patterns.
For this reason, data mining is really a semiautomatic process in real life.
However, in our description we concentrate on the automatic aspect of mining.
Themostwidely used applications are those that require some sort of prediction.
For instance, when a person applies for a credit card, the credit-card company wants to predict if the person is a good credit risk.
The prediction is to be based on known attributes of the person, such as age, income, debts, and past debt-repayment history.
Rules for making the prediction are derived from the same attributes of past and current credit-card holders, along with their observed behavior, such as whether they defaulted on their credit-card dues.
Another class of applications looks for associations, for instance, books that tend to be bought together.
If a customer buys a book, an online bookstore may suggest other associated books.
If a person buys a camera, the systemmay suggest accessories that tend to be bought along with cameras.
A good salesperson is aware of such patterns and exploits them to make additional sales.
Other types of associations may lead to discovery of causation.
For instance, discovery of unexpected associations between a newly introduced medicine and cardiac problems led to the finding that the medicine may cause cardiac problems in some people.
For example, over a century ago a cluster of typhoid cases was found around a well, which led to the discovery that the water in the well was contaminated and was spreading typhoid.
As mentioned in Section 20.3, prediction is one of the most important types of data mining.
We describe classification, study techniques for building one type of classifiers, called decision-tree classifiers, and then study other prediction techniques.
Abstractly, the classification problem is this: Given that items belong to one of several classes, and given past instances (called training instances) of items along with the classes to which they belong, the problem is to predict the class to which a new item belongs.
The class of the new instance is not known, so other attributes of the instance must be used to predict the class.
Classification can be done by finding rules that partition the given data into disjoint groups.
For instance, suppose that a credit-card companywants to decide whether or not to give a credit card to an applicant.
The company has a variety of information about the person, such as her age, educational background, annual income, and current debts, that it can use for making a decision.
Some of this information could be relevant to the credit-worthiness of the applicant, whereas some may not be.
To make the decision, the company assigns a credit-worthiness level of excellent, good, average, or bad to each of a sample set of current customers according to each customer’s payment history.
Then, the company attempts to find rules that classify its current customers into excellent, good, average, or bad, on the basis of the information about the person, other than the actual payment history (which is unavailable for new customers)
Let us consider just two attributes: education level (highest degree earned) and income.
Similar ruleswould also be present for the other credit-worthiness levels (average and bad)
The process of building a classifier starts from a sample of data, called a training set.
For each tuple in the training set, the class to which the tuple belongs is already known.
For instance, the training set for a credit-card application may be the existing customers, with their credit-worthiness determined from their payment history.
The actual data, or population, may consist of all people, including those who are not existing customers.
There are several ways of building a classifier, as we shall see.
The decision-tree classifier is a widely used technique for classification.
As the name suggests, decision-tree classifiers use a tree; each leaf node has an associated class, and each internal node has a predicate (or more generally, a function) associated with it.
To classify a new instance, we start at the root and traverse the tree to reach a leaf; at an internal node we evaluate the predicate (or function) on the data instance, to find which child to go to.
For example, if the degree level of a person is masters, and the person’s.
The class at the leaf is “good,” so we predict that the credit risk of that person is good.
The question then is how to build a decision-tree classifier, given a set of training instances.
The most common way of doing so is to use a greedy algorithm, which works recursively, starting at the root and building the tree downward.
Initially there is only one node, the root, and all training instances are associated with that node.
At each node, if all, or “almost all” training instances associatedwith the node belong to the same class, then the node becomes a leaf node associated with that class.
Otherwise, a partitioning attribute and partitioning conditions must be selected to create child nodes.
The data associated with each child node is the set of training instances that satisfy the partitioning condition for that child node.
In our example, the attribute degree is chosen, and four children, one for each value of degree, are created.
The conditions for the four children nodes are degree = none, degree = bachelors, degree = masters, and degree = doctorate, respectively.
The data associated with each child consist of training instances satisfying the condition associated with that child.
The data associated with each node consist of training instances with the degree attribute beingmasters and the income attribute being in each of these ranges, respectively.
Intuitively, by choosing a sequence of partitioning attributes, we start with the set of all training instances, which is “impure” in the sense that it contains instances from many classes, and ends up with leaves which are “pure” in the sense that at each leaf all training instances belong to only one class.
To judge the benefit of picking a particular attribute and condition for partitioning of the data at a node, we measure the purity of the data at the children resulting from partitioning by that attribute.
The attribute and condition that result in the maximum purity are chosen.
The purity of a set S of training instances can be measured quantitatively in several ways.
Suppose there are k classes, and of the instances in S the fraction of instances in class i is pi.
One measure of purity, the Gini measure, is defined as:
The entropyvalue is 0 if all instances are in a single class, and reaches itsmaximum when each class has the same number of instances.
That is, the purity is the weighted average of the purity of the sets Si.
The above formula can be used with both the Gini measure and the entropy measure of purity.
Splits into fewer sets are preferable to splits into many sets, since they lead to simpler and more meaningful decision trees.
The information content of a particular split can be defined in terms of entropy as:
All of this leads to a definition: The best split for an attribute is the one that gives the maximum information gain ratio, defined as:
How do we find the best split for an attribute? How to split an attribute depends on the type of the attribute.
Attributes can be either continuous valued, that is, the values can be ordered in a fashion meaningful to classification, such as age or income, or they can be categorical; that is, they have no meaningful order, such as department names or country names.
We do not expect the sort order of department names or country names to have any significance to classification.
Usually attributes that are numbers (integers/reals) are treated as continuous valued while character string attributes are treated as categorical, but this may be controlled by the user of the system.
In our example, we have treated the attribute degree as categorical, and the attribute income as continuous valued.
We first consider how to find best splits for continuous-valued attributes.
For simplicity, we shall consider only binary splits of continuous-valued attributes, that is, splits that result in two children.
The case of multiway splits is more complicated; see the bibliographical notes for references on the subject.
To find the best binary split of a continuous-valued attribute, we first sort the attribute values in the training instances.
We then compute the information gain obtained by splitting at each value.
The best binary split for the attribute is the split that gives the maximum information gain.
For a categorical attribute, we can have a multiway split, with a child for each value of the attribute.
This works fine for categorical attributes with only a few distinct values, such as degree or gender.
However, if the attribute has many distinct values, such as department names in a large company, creating a child for each value is not a good idea.
In such cases, we would try to combine multiple values into each child, to create a smaller number of children.
See the bibliographical notes for references on how to do so.
The main idea of decision-tree construction is to evaluate different attributes and different partitioning conditions, and pick the attribute and partitioning condition that results in the maximum information-gain ratio.
The same procedure works recursively on each of the sets resulting from the split, thereby recursively constructing a decision tree.
However, often data are noisy, or a set may be so small that partitioning it further may not be justified statistically.
In this case, the recursion stops when the purity of a set is “sufficiently high,” and the class of the resulting leaf is defined as the class of the majority of the elements of the set.
In general, different branches of the tree could grow to different levels.
There are a wide variety of decision-tree construction algorithms, and we outline the distinguishing features of a few of them.
With very large data sets, partitioning may be expensive, since it involves repeated copying.
Several algorithms have therefore been developed to minimize the I/O and computation cost when the training data are larger than available memory.
Several of the algorithms also prune subtrees of the generated decision tree to reduce overfitting: A subtree is overfitted if it has been so highly tuned to the specifics of the training data that it makes many classification errors on other data.
A subtree is pruned by replacing it with a leaf node.
There are different pruning heuristics; one heuristic uses part of the training data to build the tree and another part of the training data to test it.
The heuristic prunes a subtree if it finds that misclassification on the test instances would be reduced if the subtree were replaced by a leaf node.
We can generate classification rules from a decision tree, if we so desire.
For each leaf we generate a rule as follows: The left-hand side is the conjunction of all the split conditions on the path to the leaf, and the class is the class of the majority of the training instances at the leaf.
There are several types of classifiers other than decision-tree classifiers.
Two types that have been quite useful are neural-net classifiers, Bayesian classifiers, and Support Vector Machine classifiers.
Neural-net classifiers use the training data to train artificial neural nets.
There is a large body of literature on neural nets, and we do not consider them further here.
Bayesian classifiers find the distribution of attribute values for each class in the training data; when given a new instance d, they use the distribution information to estimate, for each class c j , the probability that instance d belongs to class c j , denotedby p(c j |d), in amanner outlinedhere.
The classwithmaximum probability becomes the predicted class for instance d.
To find the probability p(c j |d) of instance d being in class c j , Bayesian classifiers use Bayes’ theorem, which says:
Of these, p(d) can be ignored since it is the same for all classes.
In general, multiple attributes need to be considered for classification.
Then, finding p(d|c j ) exactly is difficult, since it requires the distribution of instances of c j , across all combinations of values for the attributes used for classification.
The number of such combinations (for example of income buckets, with degree values and other attributes) can be very large.
With a limited training set used to find the distribution, most combinations would not have even a single training set matching them, leading to incorrect classification decisions.
That is, the probability of the instance d occurring is the product of the probability of occurrence of each of the attribute values di of d, given the class is c j.
The probabilities p(di |c j ) derive from the distribution of values for each attribute i , for each class c j.
This distribution is computed from the training instances that belong to each class c j ; the distribution is usually approximated by a histogram.
For instance, we may divide the range of values of attribute i into equal intervals, and store the fraction of instances of class c j that fall in each interval.
Given a value di for attribute i , the value of p(di |c j ) is simply the fraction of instances belonging to class c j that fall in the interval to which di belongs.
A significant benefit of Bayesian classifiers is that they can classify instances with unknown and null attribute values—unknown or null attributes are just omitted from the probability computation.
In contrast, decision-tree classifiers cannot meaningfully handle situations where an instance to be classified has a null value for a partitioning attribute used to traverse further down the decision tree.
In the simplest case, consider a set of points in a two-dimensional plane, some belonging to class A, and some belonging to class B.
We are given a training set of points whose class (Aor B) is known, and we need to build a classifier of points, using these training points.
This situation is illustrated in Figure 20.5, where the points in class Aare denoted by Xmarks, while those in class B are denoted by O marks.
Suppose we can draw a line on the plane, such that all points in class A lie to one side and all points in line B lie to the other.
Then, the line can be used to classify new points, whose class we don’t already know.
But there may be many possible such lines that can separate points in class A from points in class B.
The Support Vector Machine classifier chooses the line whose distance from the nearest point in either class (from the points in the training data set) is maximum.
This line (called themaximum margin line) is then used to classify other points into class A or B, depending on which side of the line they lie on.
In Figure 20.5, the maximum margin line is shown in bold, while the other lines are shown as dashed lines.
The above intuition canbegeneralized tomore than twodimensions, allowing multiple attributes to be used for classification; in this case, the classifier finds a dividing plane, not a line.
Further, by first transforming the input points using certain functions, called kernel functions, Support Vector Machine classifiers can find nonlinear curves separating the sets of points.
In the presence of noise, some points of one class may lie in the midst of points of the other class.
In such cases, there may not be any line or meaningful curve that separates the points in the two classes; then, the line or curve that most accurately divides the points into the two classes is chosen.
Although the basic formulation of Support Vector Machines is for binary classifiers, i.e., those with only two classes, they can be used for classification into multiple classes as follows: If there are N classes, we build N classifiers, with classifier i performing a binary classification, classifying a point either as in class i or not in class i.
Given a point, each classifier i also outputs a value indicating how related a given point is to class i.
We then apply all N classifiers on a given point, and choose the class for which the relatedness value is the highest.
Regression deals with the prediction of a value, rather than a class.
Xn, we wish to predict the value of a variable Y.
For instance, we could treat the level of education as a number and income as another number, and, on the basis of these two variables, we wish to predict the likelihood of default, which could be a percentage chance of defaulting, or the amount involved in the default.
In general, we wish to find a curve (defined by a polynomial or other formula) that fits the data; the process is also called curve fitting.
The fit may be only approximate, because of noise in the data or because the relationship is not exactly a polynomial, so regression aims to find coefficients that give the best possible fit.
There are standard techniques in statistics for finding regression coefficients.
We do not discuss these techniques here, but the bibliographical notes provide references.
It is important to validate a classifier, that is, to measure its classification error rate, before deciding to use it for an application.
Consider an example of a classification problemwhere a classifier has to predict, based on some inputs (the exact inputs are not relevant here), whether a person is suffering from a particular disease X or not.
A positive prediction says that the person has the disease, and a negative prediction says the person does not have the disease.
The terminology of positive/negative prediction can be used for any binary classification problem, not just disease classification.
A set of test cases where the outcome is already known (in our example, cases where it is already known whether or not the person actually has the disease) is used to measure the quality (that is, the error rate) of the classifier.
A true positive is a case where the prediction was positive, and the person actually had the disease, while a false positive is a case where the prediction was positive, but the person did not have the disease.
True negative and false negative are defined similarly for the case where the prediction was negative.
Given a set of test cases, let t pos, f pos, t neg and f neg denote the number of true positives, false positives, true negatives and false negatives generated.
Let pos and neg denote the actual number of positives and negatives (it is easy to see that pos = t pos + f neg, and neg = f pos + t neg)
The quality of classification can be measured in several different ways:
Accuracy, defined as (t pos + t neg)/(pos+neg), that is, the fraction of the time when the classifier gives the correct classification.
Recall (also known as sensitivity) defined as t pos/pos, that is, howmany of the actual positive cases are classified as positive.
Precision, defined as t pos/(t pos+f pos), that is, how often the positive prediction is correct.
Which of these measures should be used for a specific application depends on the needs of that application.
For example, a high recall is important for a screening test, which is to be followed up by a more precise test, so that patients with the disease are not missed out.
In contrast a researcher who wants to find a few actual patients of the disease for further follow up, but is not interested in finding all patients, may value high precision over recall.
Different classifiers may be appropriate for each of these applications.
A set of test cases where the outcome is already known can be used either to train or tomeasure the quality the classifier.
It is a bad idea to use exactly the same set of test cases to train as well as to measure the quality of the classifier, since the classifier has already seen the correct classification of the test cases during training; this can lead to artificially high measures of quality.
The quality of a classifier must therefore bemeasured on test cases that have not been seen during training.
Retail shops are often interested in associations between different items that people buy.
Someone who buys bread is quite likely also to buy milk.
A person who bought the book Database System Concepts is quite likely also.
When a customer buys a particular book, an online shop may suggest associated books.
A grocery shop may decide to place bread close to milk, since they are often bought together, to help shoppers finish their task faster.
Or, the shop may place them at opposite ends of a row, and place other associated items in between to tempt people to buy those items as well, as the shoppers walk from one end of the row to the other.
A shop that offers discounts on one associated itemmay not offer a discount on the other, since the customer will probably buy the other anyway.
In the context of grocery-store purchases, the rule says that customers who buy bread also tend to buy milk with a high probability.
An association rule must have an associated population: The population consists of a set of instances.
In the grocery-store example, the population may consist of all grocery-store purchases; each purchase is an instance.
In the case of a bookstore, the population may consist of all people who made purchases, regardless of when they made a purchase.
Rules have an associated support, as well as an associated confidence.
Support is a measure of what fraction of the population satisfies both the antecedent and the consequent of the rule.
For instance, suppose only 0.001 percent of all purchases include milk and screwdrivers.
Businesses are usually not interested in rules that have low support, since they involve few customers, and are not worth bothering about.
On the other hand, if 50 percent of all purchases involve milk and bread, then support for rules involving bread and milk (and no other item) is relatively high, and such rules may be worth attention.
Exactly what minimum degree of support is considered desirable depends on the application.
Confidence is a measure of how often the consequent is true when the antecedent is true.
In business applications, rules usually have confidences significantly less than 100 percent, whereas in other domains, such as in physics, rules may have high confidences.
The number of sets grows exponentially, making the procedure just described infeasible if the number of items is large.
Luckily, almost all the sets would normally have very low support; optimizations have been developed to eliminate most such sets from consideration.
These techniques use multiple passes on the database, considering only some sets in each pass.
In the a priori technique for generating large itemsets, only sets with single items are considered in the first pass.
In the second pass, sets with two items are considered, and so on.
One of the major shortcomings is that many associations are not very interesting, since they can be predicted.
For instance, if many people buy cereal and many people buy bread, we can predict that a fairly large number of people would buy both, even if there is no connection between the two purchases.
In fact, even if buying cereal has a mild negative influence on buying bread (that is, customers who buy cereal tend to purchase bread less often than the average customer), the association between cereal and bread may still have a high support.
What would be more interesting is if there is a deviation from the expected co-occurrence of the two.
In statistical terms, we look for correlations between items; correlations can be positive, in that the co-occurrence is higher than would have been expected, or negative, in that the items co-occur less frequently than predicted.
Thus, if purchase of bread is not correlated with cereal, it would not be reported, even if there was a strong association between the two.
There are standard measures of correlation, widely used in the area of statistics.
See a standard textbook on statistics for more information about correlations.
Another important class of data-mining applications is sequence associations (or sequence correlations)
Stock-market analysts want to find associations among stock-market price sequences.
An example of such an association is the following rule: “Whenever bond rates go up, the stock prices go down within 2 days.” Discovering such associations between sequences can help us to make intelligent investment decisions.
See the bibliographical notes for references to research on this topic.
For instance, if a company has been growing at a steady rate each year, a deviation from the usual growth rate is surprising.
See the bibliographical notes for references to research on this topic.
Intuitively, clustering refers to the problem of finding clusters of points in the given data.
The problem of clustering can be formalized from distance metrics in several ways.
One way is to phrase it as the problem of grouping points into k sets (for a given k) so that the average distance of points from the centroid of their assigned cluster is minimized.2 Another way is to group points so that the average distance between every pair of points in each cluster is minimized.
There are other definitions too; see the bibliographical notes for details.
But the intuition behind all these definitions is to group similar points together in a single set.
Another type of clustering appears in classification systems in biology.
Such classification systems do not attempt to predict classes; rather they attempt to cluster related items together.
For instance, leopards and humans are clustered under the class mammalia, while crocodiles and snakes are clustered under reptilia.
Both mammalia and reptilia come under the common class chordata.
The clustering of mammalia has further subclusters, such as carnivora and primates.
Given characteristics of different species, biologists have created a complex hierarchical clustering scheme grouping related species together at different levels of the hierarchy.
Hierarchical clustering is also useful in other domains—for clustering documents, for example.
Internet directory systems (such as the Yahoo! directory) cluster related documents in a hierarchical fashion (see Section 21.9)
Hierarchical clustering algorithms can be classified as agglomerative clustering algorithms, which start by building small clusters and then create higher levels, or divisive.
Database research has provided scalable clustering algorithms that can cluster very large data sets (that may not fit in memory)
Intuitively, data points are inserted into amultidimensional tree structure (based on R-trees, described in Section 25.3.5.3), and guided to appropriate leaf nodes on the basis of nearness to representative points in the internal nodes of the tree.
Nearby points are thus clustered together in leaf nodes, and summarized if there are more points than fit in memory.
The result of this first phase of clustering is to create a partially clustered data set that fits in memory.
Standard clustering techniques can then be executed on the in-memory data to get the final clustering.
See the bibliographical notes for references to the Birch algorithm, and other techniques for clustering, including algorithms for hierarchical clustering.
An interesting application of clustering is to predict what new movies (or books or music) a person is likely to be interested in, on the basis of:
One approach to this problem is as follows: To find people with similar past preferences we create clusters of people based on their preferences for movies.
The accuracy of clustering can be improved by previously clustering movies by their similarity, so even if people have not seen the samemovies, if they have seen similar movies they would be clustered together.
We can repeat the clustering, alternately clustering people, then movies, then people, and so on until we reach an equilibrium.
Given a new user, we find a cluster of users most similar to that user, on the basis of the user’s preferences for movies already seen.
We then predict movies in movie clusters that are popular with that user’s cluster as likely to be interesting to the new user.
In fact, this problem is an instance of collaborative filtering, where users collaborate in the task of filtering information to find information of interest.
For instance, there are tools that form clusters on pages that a user has visited; this helps users when they browse the history of their browsing to find pages they have visited earlier.
The distance between pages can be based, for instance, on commonwords in the pages (see Section 21.2.2)
Another application is to classify pages into a Web directory automatically, according to their similarity with other pages (see Section 21.9)
Data-visualization systems help users to examine large volumes of data, and to detect patterns visually.
A single graphical screen can encode as much information as a far larger number of text screens.
For example, if the user wants to find out whether production problems at plants are correlated to the locations of the plants, the problem locations can be encoded in a special color—say, red—on a map.
The user can then quickly discover locations where problems are occurring.
The user may then form hypotheses about why problems are occurring in those locations, and may verify the hypotheses quantitatively against the database.
As another example, information about values can be encoded as a color, and can be displayed with as little as one pixel of screen area.
To detect associations between pairs of items, we can use a two-dimensional pixel matrix, with each row and each column representing an item.
The percentage of transactions that buy both items can be encoded by the color intensity of the pixel.
Items with high association will show up as bright pixels in the screen—easy to detect against the darker background.
Data-visualization systems do not automatically detect patterns, but they provide system support for users to detect patterns.
Since humans are very good at detecting visual patterns, data visualization is an important component of data mining.
Since most organizations are extensively computerized today, a very large body of information is available for decision support.
Decision-support systems come in various forms, including OLAP systems and data-mining systems.
Warehouses are used for decision support and analysis on historical data, for instance, to predict trends.
Data cleansing from input data sources is often a major task in data warehousing.
Warehouse schemas tend to be multidimensional, involving one or a few very large fact tables and several much smaller dimension tables.
Column-oriented storage systems provide good performance for many data warehousing applications.
Data mining is the process of semiautomatically analyzing large databases to find useful patterns.
There are a number of applications of data mining, such as prediction of values based on past examples, finding of associations between purchases, and automatic clustering of people and movies.
Classification deals with predicting the class of test instances by using attributes of the test instances, based on attributes of training instances, and.
Decision-tree classifiers, which perform classification by constructing a tree based on training instances with leaves having class labels.
The tree is traversed for each test instance to find a leaf, and the class of the leaf is the predicted class.
Several techniques are available to construct decision trees, most of them based on greedy heuristics.
Bayesian classifiers are simpler to construct than decision-tree classifiers, and they work better in the case of missing/null attribute values.
The Support Vector Machine is another widely used classification technique.
Association rules identify items that co-occur frequently, for instance, items that tend to be bought by the same customer.
Other types of data mining include clustering, text mining, and data visualization.
For each of the above classifiers, compute the accuracy, precision, recall and specificity.
If you intend to use the results of classification to perform further screening for the disease, how would you choose between the classifiers.
On the other hand, if you intend to use the result of classification to start medication, where the medication could have harmful effects if given to someone who does not have the disease, how would you choose between the classifiers?
Describe an alternative algorithm that would work better and explain why your solution is better.
Show the final tree, and with each node show the best split for each attribute along with its information gain value.
Write down all the (nontrivial) association rules you can deduce from the above information, giving support and confidence of each rule.
Describe how to find the support for a given collection of itemsets by using a single scan of the data.
Assume that the itemsets and associated information, such as counts, will fit in memory.
Show that no superset of this itemset can have support greater than or equal to j.
A variety of tools are available for each of the applications we have studied in this chapter.
Most database vendors provide OLAP tools as part of their database systems, or as add-on applications.
The Mondrian OLAP server is a public-domain OLAP server.Many companies also provide analysis tools for specific applications, such as customer relationship management.
Major database vendors also offer data warehousing products coupled with their database systems.
These provide support functionality for data modeling, cleansing, loading, and querying.
There is also a wide variety of general-purpose data-mining tools, including data-mining suites from the SAS Institute, IBM IntelligentMiner, andOracle.
There are also several open-source data-mining tools, such as the widely used Weka, and RapidMiner.
The open-source business intelligence suite Pentaho has several components including an ETL tool, the Mondrian OLAP server, and data-mining tools based on Weka.
A good deal of expertise is required to apply general-purpose mining tools for specific applications.
As a result, a large number of mining tools have been developed to address specialized applications.
Mitchell [1997] is a classic textbook on machine learning, and covers classification techniques in detail.
Kohavi and Provost [2001] presents a collection of articles on applications of data mining to electronic commerce.
Algorithms for computing classifiers with large training sets are described by Agrawal et al.
Techniques for integrating data cubes with data mining are described by Sarawagi [2000]
Clustering techniques for large datasets are described by Zhang et al.
Techniques for collaborative filtering of news articles are described by Konstan et al.
Chakrabarti [2000] provides a survey of hypertext mining techniques such as hypertext classification and clustering.
Textual data is unstructured, unlike the rigidly structured data in relational databases.
The term information retrieval generally refers to the querying of unstructured textual data.
However, the emphasis in the field of information systems is different from that in database systems, concentrating on issues such as querying based on keywords; the relevance of documents to the query; and the analysis, classification, and indexing of documents.
Web search engines today go beyond the paradigm of retrieving documents, and address broader issues such as what information to display in response to a keyword query, to satisfy the information needs of a user.
The field of information retrieval has developed in parallel with the field of databases.
In the traditional model used in the field of information retrieval, information is organized into documents, and it is assumed that there is a large number of documents.
Data contained in documents are unstructured, without any associated schema.
The process of information retrieval consists of locating relevant documents, on the basis of user input, such as keywords or example documents.
TheWebprovides a convenientway to get to, and to interactwith, information sources across the Internet.
However, a persistent problem facing the Web is the explosion of stored information, with little guidance to help the user to locate what is interesting.
Information retrieval has played a critical role in making the Web a productive and useful tool, especially for researchers.
The data in such systems are organized as a collection of documents; a newspaper article and a catalog entry (in a library catalog) are examples of documents.
In the context of the Web, usually each HTML page is considered to be a document.
A user of such a system may want to retrieve a particular document or a particular class of documents.
The intended documents are typically described by a set of keywords—for example, the keywords “database system” may be used to locate books on database systems, and the keywords “stock” and “scandal” may be used to locate articles about stock-market scandals.
Documents have associatedwith them a set of keywords, and documents whose keywords contain those supplied by the user are retrieved.
Keyword-based information retrieval can be used not only for retrieving textual data, but also for retrieving other types of data, such as video and audio data, that have descriptive keywords associated with them.
For instance, a video movie may have associated with it keywords such as its title, director, actors, and genre, while an image or video clip may have tags, which are keywords describing the image or video clip, associated with it.
There are several differences between this model and the models used in traditional database systems.
For instance, database systems deal with updates and with the associated transactional requirements of concurrency control and durability.
These matters are viewed as less important in information systems.
For instance, the field of information retrieval has dealt with the issue of querying collections of unstructured documents, focusing on issues such as keyword queries, and of ranking of documents on estimated degree of relevance of the documents to the query.
For example, a user could ask for all documents that contain the keywords “motorcycle and maintenance,” or documents that contain the keywords “computer or microprocessor,” or even documents that contain the keyword “computer but not database.” A query containing keywords without any of the above connectives is assumed to have ands implicitly connecting the keywords.
In full text retrieval, all the words in each document are considered to be keywords.
For unstructured documents, full text retrieval is essential since there may be no information about what words in the document are keywords.
We shall use the word term to refer to the words in a document, since all words are keywords.
More-sophisticated systems estimate relevance of documents to a query so that the documents can be shown in order of estimated relevance.
They use information about term occurrences, as well as hyperlink information, to estimate relevance.
Today, search engines aim to satisfy a user’s information needs, by judging what topic a query is about, and displaying not onlyWeb pages judged as relevant, but also displaying other kinds of information about the topic.
For example, given a query term “cricket”, a search engine may display scores from ongoing or recent cricket matches, rather than just display top-ranked documents related to cricket.
As another example, in response to a query “New York”, a search engine may show a map of New York, and images of New York, in addition to Web pages related to New York.
The set of all documents that satisfy a query expression may be very large; in particular, there are billions of documents on theWeb, andmost keyword queries on a Web search engine find hundreds of thousands of documents containing the keywords.
Full text retrieval makes this problem worse: each document may contain many terms, and even terms that are mentioned only in passing are treated equivalentlywith documentswhere the term is indeed relevant.
Relevance ranking is not an exact science, but there are some well-accepted approaches.
The first question to address is, given a particular term t, how relevant is a particular document d to the term.
One approach is to use the the number of occurrences of the term in the document as a measure of its relevance, on the assumption that relevant terms are likely to be mentioned many times in a document.
One way of measuring TF (d, t), the relevance of a document d to a term t, is:
Observe that this metric takes the length of the document into account.
The relevance grows with more occurrences of a term in the document, although it is not directly proportional to the number of occurrences.
Many systems refine the above metric by using other information.
For instance, if the termoccurs in the title, or the author list, or the abstract, thedocument would be considered more relevant to the term.
Similarly, if the first occurrence of a term is late in the document, the document may be considered less relevant than if the first occurrence is early in the document.
The above notions can be formalized by extensions of the formula we have shown for TF (d, t)
In the information retrieval community, the relevance of a document to a term is referred to as term frequency (TF), regardless of the exact formula used.
The relevance of a document to a query with two or more keywords is estimated by combining the relevance measures of the document to each keyword.
A simpleway of combining themeasures is to add them up.
Suppose a query uses two terms, one of which occurs frequently, such as “database”, and another that is less frequent, such as “Silberschatz”
A document containing “Silberschatz” but not “database” should be ranked higher than a document containing the term “database” but not “Silberschatz”
To fix the above problem, weights are assigned to terms using the inverse document frequency (IDF), defined as:
The relevance of a document d to a set of terms Q is then defined as:
The above approachof using term frequency and inverse document frequency as a measure of the relevance of a document is called the TF–IDF approach.
Almost all text documents (in English) contain words such as “and,” “or,” “a,” and so on, and hence these words are useless for querying purposes since their inverse document frequency is extremely low.
Such words are not used as keywords, and are discarded if present in the keywords supplied by the user.
Another factor taken into account when a query contains multiple terms is the proximity of the terms in the document.
If the terms occur close to each other in the document, the document would be ranked higher than if they occur far apart.
The formula for r (d, Q) can be modified to take proximity of the terms into account.
The name “cosine similarity” comes from the fact that the above formula computes the cosine of the angle between two vectors, one representing each document, defined as follows: Let there be n words overall across all the documents being considered.
An n-dimensional space is defined, with each word as one of the dimensions.
A document d is represented by a point in this space, with the value of the ith coordinate of the point being r (d, ti )
The vector for document d connects the origin (all coordinates = 0) to the point representing the document.
Themodel of documents as points and vectors in an n-dimensional space is called the vector space model.
If the set of documents similar to a query document A is large, the system may present the user a few of the similar documents, allow the user to choose the most relevant few, and start a new search based on similarity to A and to the chosen documents.
The resultant set of documents is likely to be what the user intended to find.
Relevance feedback can also be used to help users find relevant documents from a large set of documents matching the given query keywords.
The resultant set of documents is likely to be what the user intended to find.
An alternative to the relevance feedback approach is to require users to modify the query by adding more keywords; relevance feedback can be easier to use, in addition to giving a better final set of documents as the answer.
In order to show the user a representative set of documents when the number of documents is very large, a search systemmay cluster the documents, based on their cosine similarity.
Then, a few documents from each cluster may be shown, so that more than one cluster is represented in the set of answers.
Clustering was described earlier in Section 20.7, and several techniques have been developed to cluster sets of documents.
See the bibliographical notes for references to more information on clustering.
As a special case of similarity, there are often multiple copies of a document on the Web; this could happen, for example, if a Web site mirrors the contents of another Web site.
In this case, it makes no sense to return multiple copies of a highly ranked document as separate answers; duplicates should be detected, and only one copy should be returned as an answer.
Early Web-search engines ranked documents by using only TF–IDF based relevance measures like those described in Section 21.2
However, these techniques had some limitations when used on very large collections of documents, such as the set of all Web pages.
In particular, many Web pages have all the keywords specified in a typical search engine query; further, some of the pages that users want as answers often have just a few occurrences of the query terms, and would not get a very high TF–IDF score.
However, researchers soon realized that Web pages have very important information that plain text documents do not have, namely hyperlinks.
These can be exploited to get better relevance ranking; in particular, the relevance ranking of a page is influenced greatly by hyperlinks that point to the page.
In this section, we study how hyperlinks are used for ranking of Web pages.
The basic idea of popularity ranking (also called prestige ranking) is to find pages that are popular, and to rank them higher than other pages that contain the specified keywords.
Since most searches are intended to find information from popular pages, ranking such pages higher is generally a good idea.
For instance, the term “google” may occur in vast numbers of pages, but the page google.com is the most popular among the pages that contain the term “google.” The page google.com should therefore be ranked as the most relevant answer to a query consisting of the term “google”
Pages with the highest overall relevance value are returned as the top answers to a query.
This raises the question of how to define and how to find the popularity of a page.
One way would be to find how many times a page is accessed and use the number as a measure of the site’s popularity.
However, getting such information is impossible without the cooperation of the site, and while a few sites may be persuaded to reveal this information, it is difficult to get it for all sites.
Further, sites may lie about their access frequency, in order to get ranked higher.
A very effective alternative is to use hyperlinks to a page as a measure of its popularity.
Many people have bookmark files that contain links to sites that they use frequently.
Sites that appear in a large number of bookmark files can be inferred to be very popular sites.
Bookmark files are usually stored privately and not accessible on the Web.
However, many users do maintain Web pages with links to their favorite Web pages.
Many Web sites also have links to other related sites, which can also be used to infer the popularity of the linked sites.
A Web search engine can fetch Web pages (by a process called crawling, which we describe in Section 21.7), and analyze them to find links between the pages.
A first solution to estimating the popularity of a page is to use the number of pages that link to the page as a measure of its popularity.
However, this by itself has the drawback that many sites have a number of useful pages, yet external links often point only to the root page of the site.
The root page in turn has links to other pages in the site.
These other pages would then be wrongly inferred to be not very popular, and would have a low ranking in answering queries.
One alternative is to associate popularity with sites, rather than with pages.
All pages at a site then get the popularity of the site, and pages other than the root page of a popular site would also benefit from the site’s popularity.
However, the question of what constitutes a site then arises.
For such sites, the popularity of one part of the site does not imply popularity of another part of the site.
A simpler alternative is to allow transfer of prestige from popular pages to pages to which they link.
Under this scheme, in contrast to the one-person onevote principles of democracy, a link from a popular page x to a page y is treated as conferring more prestige to page y than a link from a not-so-popular page z.1
This notion of popularity is in fact circular, since the popularity of a page is defined by the popularity of other pages, and there may be cycles of links between pages.
However, the popularity of pages can be defined by a system of simultaneous linear equations, which can be solved by matrix manipulation.
This is similar in some sense to giving extra weight to endorsements of products by celebrities (such as film stars), so its significance is open to question, although it is effective and widely used in practice.
The linear equations can be defined in such a way that they have a unique and well-defined solution.
It is interesting to note that the basic idea underlying popularity ranking is actually quite old, and first appeared in a theory of social networking developed by sociologists in the 1950s.
In the social-networking context, the goal was to define the prestige of people.
For example, the president of the United States has high prestige since a large number of people know him.
If someone is known by multiple prestigious people, then she also has high prestige, even if she is not known by as large a number of people.
The use of a set of linear equations to define the popularity measure also dates back to this work.
The Web search engine Google introduced PageRank, which is a measure of popularity of a page based on the popularity of pages that link to the page.
Using the PageRank popularity measure to rank answers to a query gave results so much better than previously used ranking techniques that Google became the most widely used search engine, in a rather short period of time.
Note that pages that are pointed to from many Web pages are more likely to be visited, and thus will have a higher PageRank.
Similarly, pages pointed to by Web pages with a high PageRank will also have a higher probability of being visited, and thus will have a higher PageRank.
PageRank can be defined by a set of linear equations, as follows: First, Web pages are given integer identifiers.
The jump probability matrix T is defined with T[i, j] set to the probability that a random walker who is following a link out of page i follows the link to page j.
Assuming that each link from i has an equal probability of being followed T [i, j] = 1/Ni , where Ni is the number of links out of page i.
Most entries of T are 0 and it is best represented as an adjacency list.
Then the PageRank P[ j] for each page j can be defined as:
The set of equations generated as above are usually solved by an an iterative technique, starting with each P[i] set to 1/N.
Each step of the iteration computes new values for each P[i] using the P values from the previous iteration.
Basic measures of popularity such as PageRank play an important role in ranking of query answers, but are by nomeans the only factor.
The TF–IDF scores of a page are used to judge its relevance to the query keywords, and must be combined with the popularity ranking.
Other factors must also be taken into account, to handle limitations of PageRank and related popularity measures.
Information about how often a site is visited would be a useful measure of popularity, but as mentioned earlier it is hard to obtain in general.
However, search engines do track what fraction of times users click on a page, when it is returned as an answer.
This fraction can be used as a measure of the site’s popularity.
To measure the click fraction, instead of providing a direct link to the page, the search engine provides an indirect link through the search engine’s site, which records the page click, and transparently redirects the browser to the original link.2
One drawback of the PageRank algorithm is that it assigns a measure of popularity that does not take query keywords into account.
For example, the page google.com is likely to have a very high PageRank because many sites contain a link to it.
Suppose it contains awordmentioned inpassing, such as “Stanford” (the advanced search page at Google did in fact contain this word at one point several years ago)
A search on the keyword Stanford would then return google.com as the highest-ranked answer, ahead of a more relevant answer such as the Stanford University Web page.
One widely used solution to this problem is to use keywords in the anchor text of links to a page to judge what topics the page is highly relevant to.
The anchor text of a link consists of the text that appears within the HTML a href tag.
If many links to the page stanford.edu have the word Stanford in their anchor text, the page can be judged to be very relevant to the keyword Stanford.
Text near the anchor text may also be taken into account; for example, aWeb site may contain the text “Stanford’s home page is here”, but may have used only the word “here” as anchor text in the link to the StanfordWeb site.
Popularity based on anchor text is combined with other measures of popularity, and with TF–IDF measures, to get an overall ranking for query answers, as discussed in Section 21.3.5
For example when you point the mouse at a link (such as dbbook.com) in a Google query result, the link appears to point directly to the site.
However, at least as of mid 2009, when you actually click on the link, Javascript code associated with the page actually rewrites the link to go indirectly through Google’s site.
If you use the back button of the browser to go back to the query result page, and point to the link again, the change in the linked URL becomes visible.
An alternative approach to taking keywords into account when defining popularity is to compute ameasure of popularity using onlypages that contain the query keywords, instead of computing popularity using all available Web pages.
This approach ismore expensive, since the computation of popularity ranking has to be done dynamicallywhen a query is received,whereas PageRank is computed statically once, and reused for all queries.
Web search engines handling billions of queries per day cannot afford to spend so much time answering a query.
As a result, although this approach can give better answers, it is not very widely used.
The HITS algorithm was based on the above idea of first finding pages that contain the query keywords, and then computing a popularitymeasure using just this set of related pages.
In addition it introduced a notion of hubs and authorities.
A hub is a page that stores links tomany related pages; it may not in itself contain actual information on a topic, but points to pages that contain actual information.
In contrast, an authority is a page that contains actual information on a topic, although it may not store links to many related pages.
The definitions of prestige, as before, are cyclic and are defined by a set of simultaneous linear equations.
Given a query, pageswith highest authority-prestige are rankedhigher than other pages.
Search engine spamming refers to the practice of creating Web pages, or sets of Web pages, designed to get a high relevance rank for some queries, even though the sites are not actually popular sites.
For example, a travel site may want to be ranked high for queries with the keyword “travel”
It can get high TF–IDF scores by repeating the word “travel” many times in its page.3 Even a site unrelated to travel, such as a pornographic site, could do the same thing, and would get highly ranked for a query on the word travel.
In fact, this sort of spamming of TF–IDF was common in the early days of Web search, and there was a constant battle between such sites and search engines that tried to detect spamming and deny them a high ranking.
Repeated words in a Web page may confuse users; spammers can tackle this problem by delivering different pages to search engines than to other users, for the same URL, or by making the repeated words invisible, for example, by formatting the words in small white font on a white background.
Techniques such as using sites instead of pages as the unit of ranking (with appropriately normalized jump probabilities) have been proposed to avoid some spamming techniques, but are not fully effective against other spamming techniques.
The war between search engine spammers and the search engines continues even today.
Thehubs andauthorities approachof theHITS algorithm ismore susceptible to spamming.A spammer can create aWebpage containing links to good authorities on a topic, and gains a high hub score as a result.
In addition the spammer’s Web page includes links to pages that they wish to popularize, which may not have any relevance to the topic.
Because these linked pages are pointed to by a page with high hub score, they get a high but undeserved authority score.
We have seen two broad kinds of features used in ranking, namely TF–IDF and popularity scores such as PageRank.
TF–IDF itself reflects a combination of several factors including raw term frequency and inverse document frequency, occurrence of a term in anchor text linking to the page, and a variety of other factors such as occurrence of the term in the title, occurrence of the term early in the document, and larger font size for the term, among other factors.
How to combine the scores of a page on each these factors, to generate an overall page score, is a major problem that must be addressed by any information retrieval system.
In the early days of search engines, humans created functions to combine scores into an overall score.
But today, search engines use machinelearning techniques to decide how to combine scores.
Typically, a score combining formula is fixed, but the formula takes as parametersweights for different scoring factors.
By using a training set of query results ranked by humans, a machinelearning algorithm can come up with an assignment of weights for each scoring factor that results in the best ranking performance across multiple queries.
We note that most search engines do not reveal how they compute relevance rankings; they believe that revealing their ranking techniques would allow competitors to catch up, and would make the job of search engine spamming easier, resulting in poorer quality of results.
Consider the problem of locating documents about motorcycle maintenance, using the query “motorcycle maintenance”
Suppose that the keywords for each document are the words in the title and the names of the authors.
The document titled Motorcycle Repair would not be retrieved, since the word “maintenance” does not occur in its title.
We can solve that problem by making use of synonyms.
Each word can have a set of synonyms defined, and the occurrence of a word can be replaced by the or of all its synonyms (including the word itself)
Thus, the query “motorcycle and repair” can be replaced by “motorcycle and (repair or maintenance).” This query would find the desired document.
Keyword-based queries also suffer from the opposite problem, ofhomonyms, that is single words with multiple meanings.
For instance, the word “object” has different meanings as a noun and as a verb.
The word “table” may refer to a dinner table, or to a table in a relational database.
In fact, a danger even with using synonyms to extend queries is that the synonyms may themselves have different meanings.
For example, “allowance” is a synonym for one meaning of the word “maintenance”, but has a different meaning than what the user intended in the query “motorcycle maintenance”
Documents that use the synonyms with an alternative intended meaning would be retrieved.
The user is then left wondering why the system thought that a particular retrieved document (for example, using the word “allowance”) is relevant, if it contains neither the keywords the user specified, nor words whose intended meaning in the document is synonymous with specified keywords! It is therefore a bad idea to use synonyms to extend a query without first verifying the synonyms with the user.
A better approach to the above problem is for the system to understand what concept each word in a document represents, and similarly to understand what concepts a user is looking for, and to return documents that address the concepts that the user is interested in.
A system that supports concept-based querying has to analyze each document to disambiguate each word in the document, and replace it with the concept that it represents; disambiguation is usually done by looking at other surrounding words in the document.
For example, if a document contains words such as database or query, the word table probably should be replaced by the concept “table: data” whereas if the document contains words such as furniture, chair, or wood near the word table, the word table should be replaced by the concept “table: furniture.” Disambiguation based on nearby words is usually harder for user queries, since queries contain very few words, so concept-based query systems would offer several alternative concepts to the user, who picks one or more before the search continues.
Concept-based querying has several advantages; for example, a query in one language can retrieve documents in other languages, so long as they relate to the same concept.
Automated translation mechanisms can be used subsequently if the user does not understand the language in which the document is written.
However, the overhead of processing documents to disambiguate words is very high when billions of documents are being handled.
Internet search engines therefore generally did not support concept-based querying initially, but interest in concept-based approaches is growing rapidly.
However, concept-based querying systems have been built and used for other large collections of documents.
Querying based on concepts can be extended further by exploiting concept hierarchies.
For example, suppose a person issues a query “flying animals”; a document containing information about “flying mammals” is certainly relevant, since amammal is an animal.However, the two concepts are not the same, and just matching concepts would not allow the document to be returned as an answer.
Concept-based querying systems can support retrieval of documents based on concept hierarchies.
The most common relationship is the is-a relationship; for example, a leopard is-a mammal, and a mammal is-a animal.
Other relationships, such as part-of are also possible; for example, an airplane wing is part-of an airplane.
TheWordNet systemdefines a large variety of conceptswith associatedwords (called a synset in WordNet terminology)
The words associated with a synset are synonyms for the concept; a word may of course be a synonym for several different concepts.
In addition to synonyms, WordNet defines homonyms and other relationships.
In particular, the is-a and part-of relationships that it defines connect concepts, and in effect define an ontology.
The Cyc project is another effort to create an ontology.
In addition to language-wide ontologies, ontologies have been defined for specific areas to deal with terminology relevant to those areas.
For example, ontologies have been created to standardize terms used in businesses; this is an important step in building a standard infrastructure for handling order processing and other interorganization flow of data.
As another example, consider a medical insurance company that needs to get reports from hospitals containing diagnosis and treatment information.
An ontology that standardizes the terms helps hospital staff to understand the reports unambiguously.
This can greatly help in analysis of the reports, for example to track howmany cases of a particular disease occurred in a particular time frame.
It is also possible to build ontologies that link multiple languages.
For example, WordNets have been built for different languages, and common concepts between languages can be linked to each other.
Such a system can be used for translation of text.
In the context of information retrieval, a multilingual ontology can be used to implement a concept-based search across documents in multiple languages.
The largest effort in using ontologies for concept-based queries is the Semantic Web.
The Semantic Web is led by the World Wide Web Consortium and consists of a collection of tools, standards, and languages that permit data on the Web to be connected based on their semantics, or meaning.
Instead of being a centralized repository, the Semantic Web is designed to permit the same kind of decentralized, distributed growth that has made theWorldWideWeb so successful.
Key to this is the capability to integrate multiple, distributed ontologies.
As a result, anyone with access to the Internet can add to the Semantic Web.
Documents that contain a specified keyword can be located efficiently by using an inverted index that maps each keyword Ki to a list Si of (identifiers of) the documents that contain Ki.
To support relevance ranking based on proximity of keywords, such an index may provide not just identifiers of.
The inverted lists may also include with each document the term frequency of the term.
Such indices must be stored on disk, and each list Si can span multiple disk pages.
To minimize the number of I/O operations to retrieve each list Si , the system would attempt to keep each list Si in a set of consecutive disk pages, so the entire list can be retrievedwith just one disk seek.
A B+-tree index can be used to map each keyword Ki to its associated inverted list Si.
In this case, all documents containing at least one of the words are retrieved (as in the or operation), but are ranked by their relevance measure.
To use term frequency for ranking, the index structure should additionally maintain the number of times terms occur in each document.
To reduce this effort, theymay use a compressed representationwith only a few bits that approximates the term frequency.
The index should also store the document frequency of each term (that is, the number of documents in which the term appears)
If the popularity ranking is independent of the index term (as is the case for PageRank), the list Si canbe sorted on thepopularity ranking (and secondarily, for documents with the same popularity ranking, on document-id)
Then, a simple merge can be used to compute and and or operations.
For the case of the and operation, if we ignore the TF–IDF contribution to the relevance score, andmerely require that the document should contain the given keywords, merging can stop once K answers have been obtained, if the user requires only the top K answers.
In general, the results with highest final score (after including TF–IDF scores) are likely to have high popularity scores, and would appear near the front of the lists.
Techniques have been developed to estimate the best possible scores of remaining results, and these can be used to recognize that answers not yet seen cannot be part of the top K answers.
However, sorting on popularity score is not fully effective in avoiding long inverted list scans, since it ignores the contribution of the TF–IDF scores.
An alternative in such cases is to break up the inverted list for each term into two.
The first part contains documents that have a high TF–IDF score for that term (for example, documents where the term occurs in the document title, or in anchor text referencing the document)
Each part of the list can be sorted in order of (popularity, document-id)
Given a query, merging the first parts of the list for each term is likely to give several answers with an overall high score.
If sufficient high-scoring answers are not found using the first parts of the lists, the second parts of the lists are used to find all remaining answers.
If a document scores high on TF–IDF, it is likely to be found when merging the first parts of the lists.
Each keyword may be contained in a large number of documents; hence, a compact representation is critical to keep space usage of the index low.
Thus, the sets of documents for a keyword are maintained in a compressed form.
So that storage space is saved, the index is sometimes stored such that the retrieval is approximate; a few relevant documents may not be retrieved (called a false drop or false negative), or a few irrelevant documents may be retrieved (called a false positive)
A good index structure will not have any false drops, but may permit a few false positives; the system can filter them away later by looking at the keywords that they actually contain.
InWeb indexing, false positives are not desirable either, since the actual document may not be quickly accessible for filtering.
The first, precision, measures what percentage of the retrieved documents are actually relevant to the query.
The second, recall, measures what percentage of the documents relevant to the query were retrieved.
Precision and recall are also important measures for understanding howwell a particular document-ranking strategy performs.
Ranking strategies can result in false negatives and false positives, but in a more subtle sense.
False negatives may occur when documents are ranked, as a result of relevant documents receiving a low ranking.
If the system fetched all documents down to those with very low ranking there would be very few false negatives.
However, humans would rarely look beyond the first few tens of returned documents, and may thus miss relevant documents because they are not ranked high.
Exactly what is a false negative depends on how many documents are examined.
Therefore instead of having a single number as the measure of recall, we can measure the recall as a function of the number of documents fetched.
False positives may occur because irrelevant documents get higher rankings than relevant documents.
One option is to measure precision as a function of number of documents fetched.
A better and more intuitive alternative for measuring precision is to measure it as a function of recall.
With this combined measure, both precision and recall can be computed as a function of number of documents, if required.
In general, we can draw a graph relating precision to recall.
These measures can be computed for individual queries, then averaged out across a suite of queries in a query benchmark.
Yet another problem with measuring precision and recall lies in how to define which documents are really relevant and which are not.
In fact, it requires understanding of natural language, and understanding of the intent of the query, to decide if a document is relevant or not.
Researchers therefore have created collections of documents and queries, and have manually tagged documents as relevant or irrelevant to the queries.
Different ranking systems can be run on these collections to measure their average precision and recall across multiple queries.
Web crawlers are programs that locate and gather information on the Web.
They recursively follow hyperlinks present in known documents to find other documents.
Crawlers start from an initial set of URLs, which may be created manually.
Each of the pages identified by these URLs are fetched from the Web.
The Web crawler then locates all URL links in these pages, and adds them to the set of URLs to be crawled, if they have not already been fetched, or added to the to-be-crawled set.
This process is again repeated by fetching all pages in the to-be-crawled set, and processing the links in these pages in the same fashion.
By repeating the process, all pages that are reachable by any sequence of links from the initial set of URLs would be eventually fetched.
Since the number of documents on the Web is very large, it is not possible to crawl the whole Web in a short period of time; and in fact, all search engines cover only some portions of the Web, not all of it, and their crawlers may take weeks or months to perform a single crawl of all the pages they cover.
There are usually many processes, running on multiple machines, involved in crawling.
A database stores a set of links (or sites) to be crawled; it assigns links from this set to each crawler process.
New links found during a crawl are added to the database, and may be crawled later if they are not crawled immediately.
Pages have to be refetched (that is, links recrawled) periodically to obtain updated information, and to discard sites that no longer exist, so that the information in the search index is kept reasonably up-to-date.
See the references in the bibliography for a number of practical details in performing a Web crawl, such as infinite sequences of links created by dynamically generated pages (called a spider trap), prioritization of page fetches, and ensuring that Web sites are not flooded by a burst of requests from a crawler.
Pages fetched during a crawl are handed over to a prestige computation and indexing system, which may be running on a different machine.
Pages can be discarded after they are used for prestige computation and added to the index; however, they are usually cached by the search engine, to give search engine users fast access to a cached copy of a page, even if the original Web site containing the page is not accessible.
It is not a good idea to add pages to the same index that is being used for queries, since doing so would require concurrency control on the index, and would affect query and update performance.
Instead, one copy of the index is used to answer queries while another copy is updatedwith newly crawled pages.
At periodic intervals the copies switch over, with the old one being updatedwhile the new copy is being used for queries.
To support very high query rates, the indices may be kept in main memory, and there are multiple machines; the system selectively routes queries to the machines to balance the load among them.
Popular search engines often have tens of thousands of machines carrying out the various tasks of crawling, indexing, and answering user queries.
Web crawlers depend on all relevant pages being reachable through hyperlinks.
However, many sites containing large collections of data may not make all the data available as hyperlinked pages.
Instead, they provide search interfaces, where users can enter terms, or select menu options, and get results.
As an example, a database of flight information is usually made available using such a search interface, without any hyperlinks to the pages containing flight information.
As a result, the information inside such sites is not accessible to a normal Web crawler.
The information in such sites is often referred to as deep Web information.
Deep Web crawlers extract some such information by guessing what terms would make sense to enter, or what menu options to choose, in such search interfaces.
By entering each possible term/option and executing the search interface, they are able to extract pages with data that they would not have been able to find otherwise.
The pages extracted by a deep Web crawl may be indexed just like regular Web pages.
The Google search engine, for example, includes results from deep Web crawls.
People use search engines for many different tasks, from simple tasks such as locating a Web site that they want to use, to a broader goal of finding information on a topic of interest.
Web search engines have become extremely good at the task of locating Web sites that a user wants to visit.
The task of providing information on a topic of interest is much harder, and we study some approaches in this section.
There is also an increasing need for systems that try to understand documents (to a limited extent), and answer questions based on the (limited) understanding.
One approach is to create structured information from unstructured documents.
Another approach applies natural language techniques to find documents related to a question (phrased in natural language) and return relevant segments of the documents as an answer to the question.
Today, search engines do not just return a ranked list of Web pages relevant to a query.
They also return image and video results relevant to a query.
Further, there are a variety of sites providing dynamically changing content such as sports scores, or stock market tickers.
To get current information from such sites, users would have to first click on the query result.
Instead, search engines have created “gadgets,” which take data from a particular domain, such as sports updates, stock prices, or weather conditions, and format them in a nice graphical manner, to be displayed as results for a query.
Search engines have to rank the set of gadgets available in terms of relevance to a query, and display the most relevant gadgets, along with Web pages, images, videos, and other types of results.
Thus a query result has a diverse set of result types.
For example, a query “eclipse” may be referring to a solar or lunar eclipse, or to the integrated development environment (IDE) called Eclipse.
If all the highly ranked pages for the term “eclipse” are about the IDE, a user looking for information about solar or lunar eclipses may be very dissatisfied.
Search engines therefore attempt to provide a set of results that are diverse in terms of their topics, to minimize the chance that a user would be dissatisfied.
To do so, at indexing time the search engine must disambiguate the sense in which a word is used in a page; for example, it must decide whether the use of the word “eclipse” in a page refers to the IDE or the astronomical phenomenon.
Then, given a query, the search engine attempts to provide results that are relevant to the most common senses in which the query words are used.
The results obtained from aWeb page need to be summarized as a snippet in a query result.
Traditionally, search engines provided a few words surrounding the query keywords as a snippet that helps indicate what the page contains.
However, there are many domains where the snippet can be generated in a much more meaningful manner.
For example, if a user queries about a restaurant, a search engine can generate a snippet containing the restaurant’s rating, a phone number, and a link to a map, in addition to providing a link to the restaurant’s home page.
Such specialized snippets are often generated for results retrieved from a database, for example, a database of restaurants.
For example, a real-estate advertisementmay describe attributes of a home in textual form, such as “two-bedroom three-bath house in Queens, $1 million”, from which an information extraction system may extract attributes such as number of bedrooms, number of bathrooms, cost and neighborhood.
The extracted information can be used to structure the data in a standard way.
Thus, a user could specify that he is interested in two-bedroom houses, and a search system would be able to return all relevant houses based on the structured data, regardless of the terms used in the advertisement.
As another example, search engines designed for finding scholarly research articles, such as Citeseer andGoogle Scholar, crawl theWeb to retrieve documents that are likely to be research articles.
They examine some features of each retrieved document, such as the presence of words such as “bibliography”, “references”, and “abstract”, to judge if a document is in fact a scholarly research article.
They then extract the title, list of authors, and the citations at the end of the article, by using information extraction techniques.
The extracted citation information can be used to link each article to articles that it cites, or to articles that cite it; such citation links between articles can be very useful for a researcher.
Several systems have been built for information extraction for specialized applications.
They use linguistic techniques, page structure, and user-defined rules for specific domains such as real estate advertisements or scholarly publications.
For limited domains, such as a specificWeb site, it is possible for a human to specify patterns that can be used to extract information.
Such patterns can be created manually for a limited number of Web sites.
However, on the Web scale with millions of Web sites, manual creation of such patterns is not feasible.
Machine-learning techniques, which can learn such patterns given a set of training examples, arewidely used to automate the process of information extraction.
Information extraction usually has errors in some fraction of the extracted information; typically this is because some page had information in a format that syntactically matched a pattern, but did not actually specify a value (such as the price)
Information extraction using simple patterns, which separately match parts of a page, is relatively error prone.
Machine-learning techniques can perform much more sophisticated analysis, based on interactions between patterns, to minimize errors in the information extracted, while maximizing the amount of information extracted.
See the references in the bibliographical notes for more information.
Information retrieval systems focus on finding documents relevant to a given query.
However, the answer to a query may lie in just one part of a document, or in small parts of several documents.
Question answering systems attempt to providedirect answers to questions posed by users.
Question answering systems targeted at information on the Web typically generate one or more keyword queries from a submitted question, execute the keyword queries against Web search engines, and parse returned documents to find segments of the documents that answer the question.
A number of linguistic techniques and heuristics are used to generate keyword queries, and to find relevant segments from the document.
An issue in answering questions is that different documents may indicate different answers to a question.
For example, if the question is “How tall is a giraffe?” different documents may give different numbers as an answer.
Current-generation question answering systems are limited in power, since they do not really understand either the question or the documents used to answer the question.
However, they are useful for a number of simple question answering tasks.
Structureddata are primarily represented in either relational or XML form.
Several systems have been built to support keyword querying on relational and XML data (see Chapter 23)
A common theme between these systems lies in finding nodes (tuples or XML elements) containing the specified keywords, and finding connecting paths (or common ancestors, in the case of XML data) between them.
For example, a query “Zhang Katz” on a university database may find the name “Zhang” occurring in a student tuple, and the name “Katz” in an instructor tuple, and a path through the advisor relation connecting the two tuples.
Other paths, such as student “Zhang” taking a course taught by “Katz” may also be found in response to this query.
Such queries may be used for ad hoc browsing and querying of data, when the user does not know the exact schema and does not wish to take the effort to write an SQL query defining what she is searching for.
Indeed it is unreasonable to expect lay users to write queries in a structured query language, whereas keyword querying is quite natural.
Since queries are not fully defined, they may have many different types of answers, which must be ranked.
A number of techniques have been proposed to rank answers in such a setting, based on the lengths of connecting paths, and on techniques for assigning directions and weights to edges.
Techniques have also been proposed for assigning popularity ranks to tuples and XML elements, based.
See the bibliographical notes for more information on keyword searching of relational and XML data.
A typical library user may use a catalog to locate a book for which she is looking.
When she retrieves the book from the shelf, however, she is likely to browse through other books that are located nearby.
Libraries organize books in such a way that related books are kept close together.
Hence, a book that is physically near the desired book may be of interest as well, making it worthwhile for users to browse through such books.
To keep related books close together, libraries use a classification hierarchy.
Since there is a relation between mathematics and computer science, relevant sets of books are stored close to each other physically.
At yet another level in the classification hierarchy, computer-science books are broken down into subareas, such as operating systems, languages, and algorithms.
Figure 21.1 illustrates a classification hierarchy that may be used by a library.
Because books can be kept at only one place, each book in a library is classified into exactly one spot in the classification hierarchy.
However, such systems need to organize documents logically so as to permit browsing.
Thus, such a system could use a classification hierarchy similar.
A document that talks of mathematics for computer scientists could be classified under mathematics as well as under computer science.
All that is stored at each spot is an identifier of the document (that is, a pointer to the document), and it is easy to fetch the contents of the document by using the identifier.
As a result of this flexibility, not only can a document be classified under two locations, but also a subarea in the classification hierarchy can itself occur under two areas.
The class of “graph algorithm” documents can appear both under mathematics and under computer science.
Thus, the classification hierarchy is now a directed acyclic graph (DAG), as shown in Figure 21.2
A graph-algorithm document may appear in a single location in the DAG, but can be reached via multiple paths.
Each leaf of the directory stores links to documents on the topic represented by the leaf.
Internal nodesmay also contain links, for example, to documents that cannot be classified under any of the child nodes.
To find information on a topic, a user would start at the root of the directory and follow paths down the DAG until reaching a node representing the desired topic.
While browsing down the directory, the user can find not only documents on the topic he is interested in, but also find related documents and related classes in the classification hierarchy.
The user may learn new information by browsing through documents (or subclasses) within the related classes.
Organizing the enormous amount of information available on the Web into a directory structure is a daunting task.
The first problem is determining what exactly the directory hierarchy should be.
The second problem is, given a document, deciding which nodes of the directory are categories relevant to the document.
To tackle the first problem, portals such as Yahoo! have teams of “Internet librarians” who come up with the classification hierarchy and continually refine it.
The second problem can also be tackled manually by librarians, or Web site maintainers may be responsible for deciding where their sites should lie in the hierarchy.
There are also techniques for deciding automatically the location of documents based on computing their similarity to documents that have already been classified.
Wikipedia, the online encyclopedia, addresses the classification problem in the reverse direction.
Each page in Wikipedia has a list of categories to which it belongs.
For example, as of 2009, the Wikipedia page on giraffes had several categories including “Mammals of Africa”
In turn, the “Mammals of Africa” category itself belongs to the category “Mammals by geography”, which in turn belongs to the category “Mammals”, which in turn has a category “Vertebrates”, and so on.
The category structure is useful to browse other instances of the same category, for example, to find other mammals of Africa, or other mammals.
Conversely, a query that looks for mammals can use the category information to infer that a giraffe is a mammal.
The Wikipedia category structure is not a tree, but is almost a DAG; it is not actually a DAG since it has a few instances of loops, which probably reflect categorization errors.
They use a simpler data model than do database systems, but provide more powerful querying capabilities within the restricted model.
Queries attempt to locate documents that are of interest by specifying, for example, sets of keywords.
Similarity of documents is used to retrieve documents similar to an example document.
The cosine metric is used to define similarity, and is based on the vector space model.
PageRank and hub/authority rank are two ways to assign prestige to pages on the basis of links to the page.
Anchor text information is also used to compute a per-keyword notion of popularity.
Search engine spamming attempts to get (an undeserved) high ranking for a page.
Concept-based querying aims at finding documents containing specified concepts, regardless of the exact words (or language) in which the concept is specified.
Ontologies are used to relate concepts using relationships such as is-a or part-of.
Precision and recall are two measures of the effectiveness of an information.
Web search engines crawl the Web to find pages, analyze them to compute prestige measures, and index them.
Techniques have been developed to extract structured information from textual data, to perform keyword querying on structured data, and to give direct answers to simple questions posed in natural language.
Directory structures and categories are used to classify documents with other similar documents.
Suppose also you have a keyword index that gives you a (sorted) list of identifiers of documents that contain a specified keyword.
Give an efficient algorithm to find the desired set of documents.
You can assume that the concept hierarchy is not very deep, so each concept has only a few generalizations (a concept can, however, have a large number of specializations)
You can also assume that you are provided with a function that returns the concept for each word in a document.
Also suggest how a query using a specialized concept can retrieve documents using a more general concept.
Suggest how merging of inverted lists can stop early if the user wants only the top K answers.
You do not need to compute PageRank, just assume some values for each page.
Does the concept of term frequency make sense in this context? And that of inverse document frequency? Explain your answer.
Also suggest how you can define the similarity of two tuples using TFIDF concepts.
What is the effect of such rings on popularity ranking techniques such as PageRank?
The advertisements supplied are based on the contents of the page.
Suggest how Google might choose which advertisements to supply for a page, given the page contents.
Thus pages that do not contain the keyword but are close (in terms of links) to pages that contain the keyword also get a nonzero rank for that keyword.
Give a formula for computing the relevance of a page to a query containing multiple keywords.
Suggest how such a ranking scheme may be of value in the following applications:
A bibliographic database that has links fromarticles to authors of the articles and links from each article to every article that it references.
A sales database that has links from each sales record to the items that were sold.
Also suggest why prestige ranking can give less than meaningful results in a movie database that records which actor has acted in which movies.
Bharat and Henzinger [1998] presents a refinement of the HITS ranking technique.
These techniques, as well as other popularity-based ranking techniques (and techniques to avoid search engine spamming) are described in detail in Chakrabarti [2002]
Indexing of documents is covered in detail by Witten et al.
Jones and Willet [1997] is a collection of articles on information retrieval.
A number of practical issues in ranking and indexing of Web pages, as done in an early version of the Google search engine, are discussed in Brin and Page [1998]
Unfortunately, there are no publicly available details of how exactly ranking is done currently by any of the leading search engines.
It includes a technique for adjusting the citation ranking based on the age of a publication, to compensate for the fact that citations to a publication increase as time passes; without the adjustment, older documents tend to get a higher ranking than they truly deserve.
It is worth noting that these systems use information extraction techniques to infer the title and list of authors of an article, as well as the citations at the end of the article.
They then create citation links between articles based on (approximate) matching of the article title and author list with the citation text.
Information extraction and question answering have had a fairly long history in the artificial intelligence community.
Jackson and Moulinier [2002] provides textbook coverage of natural language processing techniques with an emphasis on information extraction.
The annual Text Retrieval Conference (TREC) has a number of tracks, each of which defines a problem and infrastructure to test the quality of solutions to the problem.
The goal of the Cyc system is to provide a formal representation of large amounts of human knowledge.
Its knowledge base contains a large number of terms, and assertions about each term.
Cyc also includes a support for natural language understanding and disambiguation.
Information about the Cyc system may be found at cyc.com and opencyc.org.
The evolution of Web search toward concepts and semantics rather than keywords is discussed in Dalvi et al.
The annual International Semantic Web Conference (ISWC) is one of the major conferences where new developments in the Semantic Web are presented.
Keyword querying of XML data is addressed by Florescu et al.
As a result, researchers have developed several data models based on an object-oriented approach, to deal with these application domains.
This model provides the rich type system of object-oriented languages, combined with relations as the basis for storage of data.
The object-relational data model provides a smooth migration path from relational databases, which is attractive to relational database vendors.
As a result, starting with SQL:1999, the SQL standard includes a number of object-oriented features in its type system, while continuing to use the relational model as the underlying model.
Chapter 22 also provides a brief overview of object-oriented databases.
The XML language was initially designed as a way of adding markup information to text documents, but has become important because of its applications in data exchange.
Chapter 23 describes the XML language, and then presents different ways of expressing queries on data represented in XML, including the XQuery XML query language, and SQL/XML, an extension of SQL which allows the creation of nested XML output.
Traditional database applications consist of data-processing tasks, such as banking and payroll management, with relatively simple data types that are well suited to the relational data model.
As database systems were applied to a wider range of applications, such as computer-aided design and geographical information systems, limitations imposed by the relational model emerged as an obstacle.
The solution was the introduction of object-based databases, which allow one to deal with complex data types.
The first obstacle faced by programmers using the relational data model was the limited type system supported by the relational model.
Complex application domains require correspondingly complex data types, such as nested record structures, multivalued attributes, and inheritance, which are supported by traditional programming languages.
Such features are in fact supported in the E-R and extended E-R notations, but had to be translated to simpler SQL data types.
The object-relational data model extends the relational data model by providing a richer type system including complexdata types andobject orientation.
Relational query languages, in particular SQL, need to be correspondingly extended to deal with the richer type system.
Such extensions attempt to preserve the relational foundations—in particular, the declarative access to data—while extending the modeling power.
The second obstacle was the difficulty in accessing database data from programs written in programming languages such as C++ or Java.
Merely extending the type system supported by the database was not enough to solve this problem completely.
Differences between the type system of the database and the type system of the programming language make data storage and retrieval more complicated, and need to be minimized.
Having to express database access using a language (SQL) that is different from the programming language again makes the job of the programmer harder.
In this chapter,wefirst explain themotivation for thedevelopment of complex data types.
Note that most database products support only a subset of the SQL features described here and for supported features, the syntax often differs slightly from the standard.
This is the result of commercial systems introducing object-relational features to the market before the standards were finalized.
Refer to the user manual of the database system you use to find out what features it supports.
We then address the issue of supporting persistence for data that is in the native type systemof an object-oriented programming language.
Build an object-oriented database system, that is, a database system that natively supports an object-oriented type system, and allows direct access to data from an object-oriented programming language using the native type system of the language.
Automatically convert data from the native type systemof the programming language to a relational representation, and vice versa.
Finally, we outline situations in which the object-relational approach is better than the object-oriented approach, and vice versa, and mention criteria for choosing between them.
The basic data items are records that are fairly small and whose fields are atomic—that is, they are not further structured, and first normal form holds (see Chapter 8)
In recent years, demand has grown for ways to deal with more complex data types.
While an entire address could be viewed as an atomic data item of type string, this view would hide details such as the street address, city, state, and postal code, which could be of interest to queries.
On the other hand, if an address were represented by breaking it into the components (street address, city, state, and postal code), writing queries would bemore complicated since theywould have tomention each field.
A better alternative is to allow structureddata types that allowa type addresswith subparts street address, city, state, and postal code.
Such attributes are natural, for example, for representing phone numbers, since people.
The alternative of normalization by creating a new relation is expensive and artificial for this example.
With complex type systems we can represent E-R model concepts, such as composite attributes, multivalued attributes, generalization, and specialization directly, without a complex translation to the relational model.
Recall that a domain is atomic if elements of the domain are considered to be indivisible units.
The assumption of 1NF is a natural one in the database application examples we have considered.
However, not all applications are best modeled by 1NF relations.
For example, rather than view a database as a set of records, users of certain applications view it as a set of objects (or entities)
A simple, easy-to-use interface requires a one-to-one correspondence between the user’s intuitive notion of an object and the database system’s notion of a data item.
Consider, for example, a library application, and suppose we wish to store the following information for each book:
We can see that, if we define a relation for the preceding information, several domains will be nonatomic.
A book may have a list of authors, which we can represent as an array.
Nevertheless, we may want to find all books of which Jones was one of the authors.
Thus, we are interested in a subpart of the domain element “authors.”
If we store a set of keywords for a book, we expect to be able to retrieve all books whose keywords include one or more specified keywords.
Thus, we view the domain of the set of keywords as nonatomic.
Unlike keywords and authors, publisher does not have a set-valued domain.
However, we may view publisher as consisting of the subfields name and branch.
For simplicity,we assume that the title of a book uniquely identifies the book.1 We can then represent the same information using the following schema, where the primary key attributes are underlined:
On the other hand, it may be better to use a first normal form representation in other situations.
For instance, consider the takes relationship in our university example.
Books are usually identified by a 10-digit ISBN number that uniquely identifies each published book.
If we store both, we would have data redundancy (the relationship of a particular student to a particular section would be stored twice)
The ability to use complex data types such as sets and arrays can be useful in many applications but should be used with care.
Before SQL:1999, the SQL type system consisted of a fairly simple set of predefined types.
SQL:1999 added an extensive type system to SQL, allowing structured types and type inheritance.
Structured types allow composite attributes of E-R designs to be represented directly.
For instance, we can define the following structured type to represent a composite attribute name with component attribute firstname and lastname:
Similarly, the following structured type can be used to represent a composite attribute address:
The above definition corresponds to the E-R diagram in Figure 7.4
The final and not final specifications are related to subtyping, which we describe later, in Section 22.3.2.3
We can now use these types to create composite attributes in a relation, by simply declaring an attribute to be of one of these types.
For example, we could create a table person as follows:
To illustrate our earlier note about commercial implementations defining their syntax before the standards were developed, we point out that Oracle requires the keyword object following as.
The final specification for Name indicates that we cannot create subtypes for name, whereas the not final specification for Address indicates that we can create subtypes of address.
The components of a composite attribute can be accessed using a “dot” notation; for instance name.firstname returns the firstname component of the name attribute.
An access to attribute namewould return a value of the structured type Name.
We can also create a table whose rows are of a user-defined type.
For example, we could define a type PersonType and create the table person as follows:4
An alternative way of defining composite attributes in SQL is to use unnamed row types.
For instance, the relation representing person information could have been created using row types as follows:
This definition is equivalent to the preceding table definition, except that the attributes name and address have unnamed types, and the rows of the table also have an unnamed type.
The following query illustrates how to access component attributes of a composite attribute.
The query finds the last name and city of each person.
We declare methods as part of the type definition of a structured type:
Most actual systems, being case insensitive, would not permit name to be used both as an attribute name and a data type.
Note that the for clause indicates which type this method is for, while the keyword instance indicates that thismethod executes on an instance of the Person type.
The variable self refers to the Person instance on which the method is invoked.
The body of the method can contain procedural statements, which we saw earlier in Section 5.2
Methods can update the attributes of the instance on which they are executed.
If we had created a table person of type PersonType, we could invoke the method ageOnDate() as illustrated below, to find the age of each person.
In SQL:1999, constructor functions are used to create values of structured types.A functionwith the samenameas a structured type is a constructor function for the structured type.
For instance, we could declare a constructor for the type Name like this:
We can then use newName(’John’, ’Smith’) to create a value of the typeName.
We can construct a row value by listing its attributes within parentheses.
For instance, if we declare an attribute name as a row type with components firstname.
By default every structured type has a constructor with no arguments, which sets the attributes to their default values.Anyother constructors have tobe created explicitly.
There can be more than one constructor for the same structured type; although they have the same name, they must be distinguishable by the number of arguments and types of their arguments.
The following statement illustrates how we can create a new tuple in the Person relation.
We assume that a constructor has been defined for Address, just like the constructor we defined for Name.
Suppose that we have the following type definition for people:
Wemaywant to store extra information in the database about people who are students, and about people who are teachers.
Since students and teachers are also people, we can use inheritance to define the student and teacher types in SQL:
Both Student and Teacher inherit the attributes of Person—namely, name and address.
Student and Teacher are said to be subtypes of Person, and Person is a supertype of Student, as well as of Teacher.
Methods of a structured type are inherited by its subtypes, just as attributes are.
However, a subtype can redefine the effect of a method by declaring the method again, using overriding method in place of method in the method declaration.
The SQL standard requires an extra field at the end of the type definition, whose value is either final or not final.
The keyword final says that subtypesmay not be created from the given type, while not final says that subtypes may be created.
Now suppose that we want to store information about teaching assistants, who are simultaneously students and teachers, perhaps even in different departments.
We can do this if the type system supportsmultiple inheritance, where a type is declared as a subtype of multiple types.
Note that the SQL standard does not support multiple inheritance, although future versions of the SQL standard may support it, so we discuss the concept here.
For instance, if our type system supports multiple inheritance, we can define a type for teaching assistant as follows:
There is a problem, however, since the attributes name, address, and department are present in Student, as well as in Teacher.
The attributes name and address are actually inherited from a common source, Person.
So there is no conflict caused by inheriting them from Student as well as Teacher.
However, the attribute department is defined separately in Student and Teacher.
In fact, a teaching assistant may be a student of one department and a teacher in another department.
To avoid a conflict between the two occurrences of department, we can rename them by using an as clause, as in this definition of the type TeachingAssistant:
In SQL, as in most other languages, a value of a structured type must have exactly onemost-specific type.That is, each valuemust be associatedwith one specific type, called itsmost-specific type, when it is created.
Bymeans of inheritance, it is also associated with each of the supertypes of its most-specific type.
For example, suppose that an entity has the type Person, as well as the type Student.
Then, the most-specific type of the entity is Student, since Student is a subtype of Person.
However, an entity cannot have the type Student as well as the type Teacher unless it has a type, such as TeachingAssistant, that is a subtype of Teacher, as well as of Student (which is not possible in SQL since multiple inheritance is not supported by SQL)
For instance, suppose we define the people table as follows:
We can then define tables students and teachers as subtables of people, as follows:
The types of the subtables (Student and Teacher in the above example) are subtypes of the type of the parent table (Person in the above example)
As a result, every attribute present in the table people is also present in the subtables students and teachers.
Further, when we declare students and teachers as subtables of people, every tuple present in students or teachers becomes implicitly present in people.
Thus, if a query uses the table people, it will find not only tuples directly inserted into that table, but also tuples inserted into its subtables, namely students and teachers.
However, only those attributes that are present in people can be accessed by that query.
If the only keyword is added to the above statement, tuples that were inserted in subtables are not affected, even if they satisfy the where clause conditions.
Subsequent queries on the supertable would continue to find these tuples.
Conceptually, multiple inheritance is possible with tables, just as it is possible with types.
For example, we can create a table of type TeachingAssistant:
As a result of the declaration, every tuple present in the teaching assistants table is also implicitly present in the teachers and in the students table, and in turn in the people table.
We note, however, that multiple inheritance of tables is not supported by SQL.
Before we state the constraints, we need a definition: we say that tuples in a subtable and parent table correspond if they have the same values for all inherited attributes.
Each tuple of the supertable can correspond to at most one tuple in each of its immediate subtables.
For example, without the first condition, we could have two tuples in students (or teachers) that correspond to the same person.
The second condition rules out a tuple in people corresponding to both a tuple in students and a tuple in teachers, unless all these tuples are implicitly present because a tuple was inserted in a table teaching assistants, which is a subtable of both teachers and students.
Since SQL does not support multiple inheritance, the second condition actually prevents a person from being both a teacher and a student.
Even if multiple inheritancewere supported, the sameproblemwould arise if the subtable teaching assistants were absent.
Obviously it would be useful to model a situation where a person can be a teacher and a student, even if a common subtable teaching assistants is not present.
Thus, it can be useful to remove the second consistency constraint.
Doing so would allow an object to have multiple types, without requiring it to have a most-specific type.
For example, suppose we again have the type Person, with subtypes Student and Teacher, and the corresponding table people, with subtables teachers and students.
We can then have a tuple in teachers and a tuple in students corresponding to the same tuple in people.
There is no need to have a type TeachingAssistant that is a subtype of both Student and Teacher.
We need not create a type TeachingAssistant unless we wish to store extra attributes or redefine methods in a manner specific to people who are both students and teachers.
Since SQL also does not support multiple inheritance, we cannot use inheritance to model a situation where a person can be both a student and a teacher.
As a result, SQL subtables cannot be used to represent overlapping specializations from the E-R model.
In the above example, wewould create tables people, students, and teachers, with the students and teachers tables containing the primary-key.
The people table would contain information about all persons, including students and teachers.
In other words, we can create our own improved implementation of the subtable mechanism using existing features of SQL, with some extra effort in defining the table, as well as some extra effort at query time to specify joins to access required attributes.
We note that SQL defines a privilege called under, which is required in order to create a subtype or subtable under another type or table.
The motivation for this privilege is similar to that for the references privilege.
Recall that a multiset is an unordered collection, where an element may occur multiple times.
Multisets are like sets, except that a set allows each element to occur at most once.
Suppose we wish to record information about books, including a set of keywords for each book.
Suppose also that we wished to store the names of authors of a book as an array; unlike elements in a multiset, the elements of an array are ordered, so we can distinguish the first author from the second author, and so on.
The following example illustrates how these array andmultiset-valued attributes can be defined in SQL:
The first statement defines a type called Publisher with two components: a name and a branch.
The second statement defines a structured type Book that contains a title, an author array, which is an array of up to 10 author names, a publication date, a publisher (of type Publisher), and a multiset of keywords.
Finally, a table books containing tuples of type Book is created.
Note thatwe used an array, instead of amultiset, to store the names of authors, since the ordering of authors generally has some significance, whereas we believe that the ordering of keywords associated with a book is not significant.
In general, multivalued attributes from an E-R schema can be mapped to multiset-valued attributes in SQL; if ordering is important, SQL arrays can be used instead of multisets.
An array of values can be created in SQL:1999 in this way:
Similarly, a multiset of keywords can be constructed as follows:
Thus, we can create a tuple of the type defined by the books relation as:
Here we have created a value for the attribute Publisher by invoking a constructor function for Publisher with appropriate arguments.
Note that this constructor for Publisher must be created explicitly, and is not present by default; it can be declared just like the constructor for Name, which we saw earlier in Section 22.3
If we want to insert the preceding tuple into the relation books, we could execute the statement:
We can access or update elements of an array by specifying the array index, for example author array[1]
We now consider how to handle collection-valued attributes in queries.
An expression evaluating to a collection can appear anywhere that a relation namemay appear, such as in a from clause, as the following paragraphs illustrate.
If we want to find all books that have the word “database” as one of their keywords, we can use this query:
Note that we have used unnest(keyword set) in a position where SQL without nested relations would have required a select-from-where subexpression.
If we know that a particular book has three authors, we could write:
Now, suppose that we want a relation containing pairs of the form “title, author name” for each book and each author of the book.
Note that the tuple variable B is visible to this expression since it is defined earlier in the from clause.
When unnesting an array, the previous query loses information about the ordering of elements in the array.
The unnest with ordinality clause can be used to get this information, as illustrated by the following query.
This query can be used to generate the authors relation,whichwe sawearlier, from the books relation.
Thewith ordinality clause generates an extra attribute which records the position of the element in the array.
A similar query, but without thewith ordinality clause, can be used to generate the keyword relation.
The transformation of a nested relation into a form with fewer (or no) relationvalued attributes is called unnesting.
The books relation has two attributes, author array and keyword set, that are collections, and two attributes, title and publisher, that are not.
Suppose thatwewant to convert the relation into a single flat relation, with nonested relations or structured types as attributes.We canuse the following query to carry out the task:
Figure 22.3 flat books: result of unnesting attributes author array and keyword set of relation books.
The variable B in the from clause is declared to range over books.
The variable A is declared to range over the authors in author array for the book B, and K is declared to range over the keywords in the keyword set of the book B.
Note that the relation flat books is in 1NF, since all its attributes are atomic valued.
The reverse process of transforming a 1NF relation into a nested relation is called nesting.
Nesting can be carried out by an extension of grouping in SQL.
In the normal use of grouping in SQL, a temporary multiset relation is (logically) created for each group, and an aggregate function is applied on the temporary relation to get a single (atomic) value.
The collect function returns the multiset of values, so instead of creating a single value, we can create a nested relation.
The following query nests the relation on the attribute keyword:
If we want to nest the author attribute also into a multiset, we can use the query:
Figure 22.4 A partially nested version of the flat books relation.
Another approach to creating nested relations is to use subqueries in the select clause.
An advantage of the subquery approach is that an order by clause can be used in the subquery to generate results in the order desired for the creation of an array.
The following query illustrates this approach; the keywords array and multiset specify that an array and multiset (respectively) are to be created from the results of the subqueries.
The system executes the nested subqueries in the select clause for each tuple generated by the from and where clauses of the outer query.
Observe that the attribute B.title from the outer query is used in the nested queries, to ensure that only the correct sets of authors and keywords are generated for each title.
SQL:2003 provides a variety of operators on multisets, including a function set(M) that returns a duplicate-free version of a multiset M, an intersection aggregate operation, which returns the intersection of all the multisets in a group, a fusion aggregate operation, which returns the union of all multisets in a group, and a submultiset predicate, which checks if a multiset is contained in another multiset.
The SQL standard does not provide any way to update multiset attributes except by assigning a new value.
For example, to delete a value v from a multiset attribute A, we would have to set it to (A except all multiset[v])
An attribute of a type can be a reference to an object of a specified type.
For example, in SQL we can define a type Department with a field name and a field head that is a reference to the type Person, and a table departments of type Department, as follows:
Here, the reference is restricted to tuples of the table people.
The restriction of the scope of a reference to tuples of a table is mandatory in SQL, and it makes references behave like foreign keys.
We can omit the declaration scope people from the type declaration and instead make an addition to the create table statement:
The referenced table must have an attribute that stores the identifier of the tuple.
We declare this attribute, called the self-referential attribute, by adding a ref is clause to the create table statement:
Here, person id is an attribute name, not a keyword, and the create table statement specifies that the identifier is generated automatically by the database.
In order to initialize a reference attribute, we need to get the identifier of the tuple that is to be referenced.
We can get the identifier value of a tuple by means of a query.
Thus, to create a tuple with the reference value, we may first create the tuple with a null reference and then set the reference separately:
An alternative to system-generated identifiers is to allow users to generate identifiers.
The type of the self-referential attribute must be specified as part of the type definition of the referenced table, and the table definition must specify that the reference is user generated:
When inserting a tuple in people, we must then provide a value for the identifier:
No other tuple for people or its supertables or subtables can have the same identifier.
We can then use the identifier value when inserting a tuple into departments, without the need for a separate query to retrieve the identifier:
It is even possible to use an existing primary-key value as the identifier, by including the ref from clause in the type definition:
Note that the table definition must specify that the reference is derived, and must still specify a self-referential attribute name.
When inserting a tuple for departments, we can then use:
References can be used to hide join operations; in the preceding example, without the references, the head field of department would be declared a foreign key of the table people.
To find the name and address of the head of a department, we would require an explicit join of the relations departments and people.
We can use the operation deref to return the tuple pointed to by a reference, and then access its attributes, as shown below:
Object-relational database systems are basically extensions of existing relational database systems.
Changes are clearly required at many levels of the database system.
To understand how to do this translation, we need look only at how some features of the E-R model are translated into relations.
For instance, multivalued attributes in the E-R model correspond to multiset-valued attributes in the objectrelational model.
The techniques for converting E-R model features to tables, which we saw in Section 7.6, can be used, with some extensions, to translate object-relational data to relational data at the storage level.
Subtables can be stored in an efficient manner, without replication of all inherited fields, in one of two ways:
Each table stores the primary key (which may be inherited from a parent table) and the attributes that are defined locally.
Inherited attributes (other than the primary key) do not need to be stored, and can be derived by means of a join with the supertable, based on the primary key.
When a tuple is inserted, it is stored only in the table in which it is inserted, and its presence is inferred in each of the supertables.
Access to all attributes of a tuple is faster, since a join is not required.
However, in case the type system allows an entity to be represented in two subtables without being present in a common subtable of both, this representation can result in replication of information.
Further, it is hard to translate foreign keys referring to a supertable into constraints on the subtables; to implement such foreign keys efficiently, the supertable has to be defined as a view, and the database system would have to support foreign keys on views.
Implementations may choose to represent array and multiset types directly, or may choose to use a normalized representation internally.
Normalized representations tend to take up more space and require an extra join/grouping cost to collect data in an array or multiset.
The ODBC and JDBC application program interfaces have been extended to retrieve and store structured types.
It is also possible to associate a Java class with an SQL structured type, and JDBC will then convert between the types.
Database languages differ from traditional programming languages in that they directly manipulate data that are persistent—that is, data that continue to exist even after the program that created it has terminated.
A relation in a database and tuples in a relation are examples of persistent data.
In contrast, the only persistent data that traditional programming languages directly manipulate are files.
Access to a database is only one component of any real-world application.
While a data-manipulation language like SQL is quite effective for accessing data, a programming language is required for implementing other components of the application such as user interfaces or communication with other computers.
The traditional way of interfacing database languages to programming languages is by embedding SQL within the programming language.
A persistent programming language is a programming language extended with constructs to handle persistent data.
Persistent programming languages can be distinguished from languages with embedded SQL in at least two ways:
With an embedded language, the type system of the host language usually differs from the type system of the data-manipulation language.
The programmer is responsible for any type conversions between the host language and SQL.
Having the programmer carry out this task has several drawbacks:
The code to convert between objects and tuples operates outside the object-oriented type system, and hence has a higher chance of having undetected errors.
Conversion between the object-oriented format and the relational format of tuples in the database takes a substantial amount of code.
The format translation code, along with the code for loading and unloading data from a database, can form a significant percentage of the total code required for an application.
In contrast, in a persistent programming language, the query language is fully integrated with the host language, and both share the same type system.
Objects can be created and stored in the database without any explicit type or format changes; any format changes required are carried out transparently.
The programmer using an embedded query language is responsible for writing explicit code to fetch data from the database into memory.
If any updates are performed, the programmer must write code explicitly to store the updated data back in the database.
In contrast, in a persistent programming language, the programmer can manipulate persistent data without writing code explicitly to fetch it into memory or store it back to disk.
In this section, we describe how object-oriented programming languages, such as C++ and Java, can be extended to make them persistent programming languages.
These language features allow programmers to manipulate data directly from the programming language, without having to go through a datamanipulation language such as SQL.
They thereby provide tighter integration of the programming languages with the database than, for example, embedded SQL.
There are certain drawbacks to persistent programming languages, however, that we must keep in mind when deciding whether to use them.
Since the programming language is usually a powerful one, it is relatively easy to make programming errors that damage the database.
The complexity of the language makes automatic high-level optimization, such as to reduce disk I/O, harder.
Support for declarative querying is important for many applications, but persistent programming languages currently do not support declarative querying well.
In this section, we describe a number of conceptual issues that must be addressed when adding persistence to an existing programming language.
However, we do not cover details of language extensions; although several standards have been proposed, none has met universal acceptance.
See the references in the bibliographical notes to learn more about specific language extensions and further details of implementations.
Object-oriented programming languages already have a concept of objects, a type system to define object types, and constructs to create objects.
However, these objects are transient—they vanish when the program terminates, just as variables in a Java or C program vanish when the program terminates.
If we wish to turn such a language into a database programming language, the first step is to provide a way to make objects persistent.
The simplest, but least convenient, way is to declare that a class is persistent.
All objects of the class are then persistent objects by default.
In this approach, new syntax is introduced to create persistent objects, by extending the syntax for creating transient objects.
Thus, an object is either persistent or transient, depending on how it was created.
A variant of the preceding approach is to mark objects as persistent after they are created.
All objects are created as transient objects, but, if an object is to persist beyond the execution of the program, it must be marked explicitly as persistent before the program terminates.
This approach, unlike the previous one, postpones the decision on persistence or transience until after the object is created.
One or more objects are explicitly declared as (root) persistent objects.
All other objects are persistent if (and only if) they are reachable from the root object through a sequence of one or more references.
Thus, all objects referenced by (that is, whose object identifiers are stored in) the root persistent objects are persistent.
But also, all objects referenced from these objects are persistent, and objects to which they refer are in turn persistent, and so on.
A benefit of this scheme is that it is easy to make entire data structures persistent by merely declaring the root of such structures as persistent.
In an object-oriented programming language that has not been extended to handle persistence, when an object is created, the system returns a transient object identifier.
Transient object identifiers are valid only when the program that created them is executing; after that program terminates, the objects are deleted, and the identifier is meaningless.
When a persistent object is created, it is assigned a persistent object identifier.
The notion of object identity has an interesting relationship to pointers in programming languages.
A simple way to achieve built-in identity is through pointers to physical locations in storage.
In particular, in many object-oriented languages such as C++, a transient object identifier is actually an in-memory pointer.
However, the association of an object with a physical location in storage may change over time.
Identity persists only during the execution of a single procedure.
Identity persists only during the execution of a single program or query.
Examples of intraprogram identity are global variables in programming languages.
Pointers to file-system data on disk offer interprogram identity, but they may change if the way data is stored in the file system is changed.
Identity persists not only among program executions, but also among structural reorganizations of the data.
It is the persistent form of identity that is required for object-oriented systems.
In persistent extensions of languages such as C++, object identifiers for persistent objects are implemented as “persistent pointers.” A persistent pointer is a type of pointer that, unlike in-memory pointers, remains valid even after the end of a program execution, and across some forms of data reorganization.
A programmer may use a persistent pointer in the same ways that she may use an in-memory pointer in a programming language.
Conceptually, we may think of a persistent pointer as a pointer to an object in the database.
What does it mean to store an object in a database? Clearly, the data part of an object has to be stored individually for each object.
However, many implementations simply store the code in files outside the database, to avoid having to integrate system software such as compilers with the database system.
There are several ways to find objects in the database.
One way is to give names to objects, just as we give names to files.
This approach works for a relatively small number of objects, but does not scale to millions of objects.
A second way is to expose object identifiers or persistent pointers to the objects, which can be stored externally.
Unlike names, these pointers do not have to be mnemonic, and they can even be physical pointers into a database.
A third way is to store collections of objects, and to allow programs to iterate over the collections to find required objects.
Collections of objects can themselves be modeled as objects of a collection type.
Collection types include sets, multisets (that is, sets with possiblymany occurrences of a value), lists, and so on.
A special case of a collection is a class extent, which is the collection of all objects belonging to the class.
If a class extent is present for a class, then, whenever an object of the class is created, that object is inserted in the class extent automatically, and, whenever an object is deleted, that object is removed from the class extent.
Class extents allow classes to be treated like relations in that we can examine all objects in the class, just as we can examine all tuples in a relation.
Most object-oriented database systems support all three ways of accessing persistent objects.
They usually give names only to class extents and other collection objects, and perhaps to other selected objects, but not to most objects.
They usually maintain class extents for all classes that can have persistent objects, but, in many of the implementations, the class extents contain only persistent objects of the class.
There are several object-oriented databases based on persistent extensions to C++ (see the bibliographical notes)
There are differences among them in terms of the system architecture, yet they have many common features in terms of the programming language.
However, it has the drawback that the programmer has to spend much more.
Some persistent C++ implementations support extensions to the C++ syntax to make these tasks easier.
The following aspects need to be addressedwhen adding persistence support to C++ (and other languages):
The ObjectStore database system uses a different approach to persistent pointers.
ObjectStore uses a technique called “hardware swizzling” to address both problems; it prefetches objects from the database into memory, and replaces persistent pointers with in-memory pointers, andwhen data are stored back on disk, in-memory pointers are replaced by persistent pointers.
When ondisk, the value stored in the in-memory pointer field is not the actual persistent pointer; instead, the value is looked up in a table that contains the full persistent pointer value.
Creation of persistent objects: The C++ new operator is used to create persistent objects by defining an “overloaded” version of the operator that takes extra arguments specifying that it should be created in the database.
Thus instead of new T(), one would call new (db) T() to create a persistent object, where db identifies the database.
Class extents: Class extents are created and maintained automatically for each class.
The ODMG C++ standard requires the name of the class to be passed as an additional parameter to the new operation.
This also allows multiple extents to be maintained for a class, by passing different names.
Relationships: Relationships between classes are often represented by storing pointers from each object to the objects to which it is related.
Objects related to multiple objects of a given class store a set of pointers.
Thus if a pair of objects is in a relationship, each should store a pointer to the other.
Persistent C++ systems provide a way to specify such integrity constraints and to enforce them by automatically creating and deleting pointers: For example, if a pointer is created from an object a to an object b, a pointer to a is added automatically to object b.
Iterator interface: Since programs need to iterate over class members, an interface is required to iterate over members of a class extent.
The iterator interface also allows selections to be specified, so that only objects satisfying the selection predicate need to be fetched.
Transactions: Persistent C++ systems provide support for starting a transaction, and for committing it or rolling it back.
That is, a function that operates on an object should not need to know that the object is persistent; the same functions can thus be used onobjects regardless ofwhether they are persistent or not.
However, one resultant problem is that it is difficult to detect when an object has been updated.
Some persistent extensions to C++ require the programmer to specify explicitly that an object has beenmodified by calling a function mark modified()
In addition to increasing programmer effort, this approach increases the chance that programming errors can result in a corrupt database.
If a programmer omits a call to mark modified(), it is possible that one update made by a transaction may never be propagated to the database, while another update made by the same transaction is propagated, violating atomicity of transactions.
Other systems, such as ObjectStore, use memory-protection support provided by the operating system/hardware to detect writes to a block of memory and mark the block as a dirty block that should be written later to disk.
To support more complex queries, persistent C++ systems define a query language.
However, themarket for such databases turned out to be much smaller than anticipated, since most application requirements are more than met by using SQL through interfaces such as ODBC or JDBC.
As a result, most of the object-oriented database systems developed in that period do not exist any longer.
In the 1990s, the Object Data Management Group (ODMG) defined standards for adding persistence to C++ and Java.
ObjectStore and Versant are among the original object-oriented database systems that are still in existence.
Although object-oriented database systems did not find the commercial success that they had hoped for, the motivation for adding persistence to programming language remains.
There are several applications with high performance requirements that run on object-oriented database systems; using SQL would impose too high a performance overhead for many such systems.
With objectrelational database systems now providing support for complex data types, including references, it is easier to store programming language objects in an SQL.
A new generation of object-oriented database systems using objectrelational databases as a backend may yet emerge.
The Java language has seen an enormous growth inusage in recent years.
Demand for support for persistence of data in Java programs has grown correspondingly.
Initial attempts at creating a standard for persistence in Java were led by the ODMG consortium; the consortium wound up its efforts later, but transferred its design to the Java Database Objects (JDO) effort, which is coordinated by Sun Microsystems.
The JDOmodel for object persistence in Java programs differs from the model for persistence support in C++ programs.
Persistence by reachability: Objects are not explicitly created in a database.
Explicitly registering an object as persistent (using the makePersistent() method of the PersistenceManager class) makes the object persistent.
In addition, any object reachable from a persistent object becomes persistent.
Byte code enhancement: Instead of declaring a class to be persistent in the Java code, classes whose objects may be made persistent are specified in a configuration file (with suffix .jdo)
First, it may create structures in a database to store objects of the class.
Second, it modifies the byte code (generated by compiling the Java program) to handle tasks related to persistence.
Any code that accesses an object could be changed to check first if the object is in memory, and if not, take steps to bring it into memory.
Any code that modifies an object is modified to record additionally that the object has been modified, and perhaps to save a pre-updated value used in case the update needs to be undone (that is, if the transaction is rolled back)
Other modifications to the byte code may also be carried out.
Such byte code modification is possible since the byte code is standard across all platforms, and includes much more information than compiled object code.
Database mapping: JDO does not define how data are stored in the back-end database.
For example, a common scenario is to store objects in a relational database.
The enhancer program may create an appropriate schema in the database to store class objects.
How exactly it does this is implementation dependent and not defined by JDO.
Some attributes could be mapped to relational attributes, while others may be stored in a serialized form, treated as a binary object by the database.
Class extents: Class extents are created and maintained automatically for each class declared to be persistent.
All objects made persistent are added automatically to the class extent corresponding to their class.
The Iterator interface provided by Java can be used to create iterators on class extents, and to step through the members of the class extent.
Single reference type: There is no difference in type between a reference to a transient object and a reference to a persistent object.
One approach to achieving such a unification of pointer types would be to load the entire database into memory, replacing all persistent pointers with in-memory pointers.
After updates were done, the process would be reversed, storing updated objects back on disk.
Such an approach would be very inefficient for large databases.
We now describe an alternative approach that allows persistent objects to be fetched automatically into memory when required, while allowing all references contained in in-memory objects to be in-memory references.When an object A is fetched, a hollow object is created for each object Bi that it references, and the in-memory copy of Ahas references to the corresponding hollow object for each Bi.
Of course the system has to ensure that if an object Bi was fetched already, the reference points to the already fetched object instead of creating a new hollow object.
Similarly, if an object Bi has not been fetched, but is referenced by another object fetched earlier, it would already have a hollow object created for it; the reference to the existing hollow object is reused, instead of creating a new hollow object.
Thus, for every object Oi that has been fetched, every reference from Oi is either to an already fetched object or to a hollow object.
Whenever the program actually accesses a hollow object O, the enhanced byte code detects this and fetches the object from the database.
When this object is fetched, the same process of creating hollow objects is carried out for all objects referenced by O.
After this the access to the object is allowed to proceed.5
An in-memory index structure mapping persistent pointers to in-memory references is required to implement this scheme.
In writing objects back to disk, this index would be used to replace in-memory references with persistent pointers in the copy written to disk.
The technique using hollow objects described above is closely related to the hardware swizzling technique (mentioned earlier in Section 22.8.4)
Hardware swizzling is used by some persistent C++ implementations to provide a single pointer type for persistent and in-memory pointers.
Hardware swizzling uses virtual-memory protection techniques provided by the operating system to detect accesses to pages, and fetches the pages from the database when required.
In contrast, the Java version modifies byte code to check for hollow objects, instead of using memory protection, and fetches objects when required, instead of fetching whole pages from the database.
Object-relational mapping systems are built on top of a traditional relational database, and allow a programmer to define a mapping between tuples in database relations and objects in the programming language.
Unlike in persistent programming languages, objects are transient, and there is no permanent object identity.
An object, or a set of objects, can be retrieved based on a selection condition on its attributes; relevant data are retrieved from the underlying database based on the selection conditions, and one ormore objects are created from the retrieved data, based on the prespecified mapping between objects and relations.
The program can optionally update such objects, create new objects, or specify that an object is to be deleted, and then issue a save command; the mapping from objects to relations is then used to correspondingly update, insert or delete tuples in the database.
Object-relational mapping systems in general, and in particular the widely used Hibernate system which provides an object-relational mapping to Java, are described in more detail in Section 9.4.2
The primary goal of object-relational mapping systems is to ease the job of programmers who build applications, by providing them an object-model, while retaining the benefits of using a robust relational database underneath.
As an added benefit, when operating on objects cached in memory, object-relational systems can provide significant performance gains over direct access to the underlying database.
Object-relational mapping systems also provide query languages that allow programmers to write queries directly on the object model; such queries are translated into SQL queries on the underlying relational database, and result objects created from the SQL query results.
On the negative side, object-relational mapping systems can suffer from significant overheads for bulk database updates, and may provide only limited querying capabilities.
However, it is possible to directly update the database, bypassing the object-relational mapping system, and to write complex queries directly in SQL.
The benefits or object-relational models exceed the drawbacks for many applications, and object-relational mapping systems have seenwidespread adoption in recent years.
We have now studied object-relational databases, which are object-oriented databases built on top of the relation model, as well as object-oriented databases, which are built around persistent programming languages, and object-relational.
The declarative nature and limited power (compared to a programming language) of the SQL language provides good protection of data fromprogramming errors, andmakes high-level optimizations, such as reducing I/O, relatively easy.
Object-relational systems aim at making data modeling and querying easier by using complex data types.
Typical applications include storage and querying of complex data, including multimedia data.
A declarative language such as SQL, however, imposes a significant performance penalty for certain kinds of applications that run primarily in main memory, and that perform a large number of accesses to the database.
Persistent programming languages target such applications that have high performance requirements.
They provide low-overhead access to persistent data and eliminate the need for data translation if the data are to be manipulated by a programming language.
However, they are more susceptible to data corruption by programming errors, and they usually do not have a powerful querying capability.
Object-relational mapping systems allow programmers to build applications using an object model, while using a traditional database system to store the data.
Thus, they combine the robustness of widely used relational database systems, with the power of object models for writing applications.
However, they suffer from overheads of data conversion between the object model and the relational model used to store data.
We can summarize the strengths of the various kinds of database systems in this way:
Relational systems: Simple data types, powerful query languages, high protection.
Persistent programming language–based OODBs: Complex data types, integration with programming language, high performance.
Object-relational systems: Complex data types, powerful query languages, high protection.
Object-relational mapping systems: Complex data types integrated with programming languages, designed as a layer on top of a relational database system.
These descriptions hold in general, but keep in mind that some database systems blur the boundaries.
For example, object-oriented database systems built around a persistent programming language can be implemented on top of a relational or object-relational database system.
Such systems may provide lower performance than object-oriented database systems built directly on a storage system, but provide some of the stronger protection guarantees of relational systems.
The object-relational data model extends the relational data model by providing a richer type system including collection types and object orientation.
Collection types include nested relations, sets, multisets, and arrays, and the object-relational model permits attributes of a table to be collections.
Object orientation provides inheritance with subtypes and subtables, as well as object (tuple) references.
The SQL standard includes extensions of the SQL data-definition and query language to deal with new data types and with object orientation.
These include support for collection-valued attributes, inheritance, and tuple references.
Such extensions attempt to preserve the relational foundationsin particular, the declarative access to data—while extending the modeling power.
Object-relational database systems (that is, database systems based on the object-relation model) provide a convenient migration path for users of relational databases who wish to use object-oriented features.
Persistent extensions to C++ and Java integrate persistence seamlessly and orthogonallywith existing programming language constructs and so are easy to use.
The ODMG standard defines classes and other constructs for creating and accessing persistent objects from C++, while the JDO standard provides equivalent functionality for Java.
Object-relational mapping systems provide an object view of data that is stored in a relational database.
Objects are transient, and there is no notion of persistent object identity.
Objects are created on-demand from relational data, and updates to objects are implemented by updating the relational data.
Object-relational mapping systems have been widely adopted, unlike the more limited adoption of persistent programming languages.
We discussed differences between persistent programming languages and object-relational systems, andwemention criteria for choosingbetween them.
For all vehicles, it includes the vehicle identification number, license number, manufacturer, model, date of purchase, and color.
Define the above schema in SQL, with appropriate types for each attribute.
Using the above schema, write the following queries in SQL.
Find the names of all employees who have a child born on or.
Find those employeeswho took an examination for the skill type.
Give an SQL schema definition corresponding to the E-R diagram.
Give constructors for each of the structured types defined above.
Give a schemadefinition in SQLcorresponding to the relational schema, but using references to express foreign-key relationships.
Write each of the queries given in Exercise 6.13 on the above schema, using SQL.
Figure 22.5 E-R diagram with composite, multivalued, and derived attributes.
For eachof the following applications, state what type of database system (relational, persistent programming language–based OODB, object relational; do not specify a commercial product) you would recommend.
A system to track contributions made to candidates for public office.
Write a query to convert data from the schema of Emp to that of EmpA, with the array of children sorted by birthday, the array of skills by the skill type and the array of exams by the year.
Write an SQL statement to perform the same update as above but on the EmpA relation.
Make sure that the array of children remains sorted by year.
Give a relational schema in third normal form that represents the same information.
Recall the constraints on subtables, and give all constraints that must be imposed on the relational schema so that every database instance of the relational schema can also be represented by an instance of the schema with inheritance.
Under what circumstances would you choose to use a reference type?
Give an SQL query to find the names of all people who are not secretaries.
Give an SQL query to print the names of people who are neither employees nor students.
Can you create a person who is an employee and a student with the schema you created? Explain how, or explain why it is not possible.
Suppose a program first dereferences A, then dereferences B by following the reference from A, and then finally dereferences C.
There are considerable differences between database products in their support for object-relational features.
Oracle probably has the most extensive support among the major database vendors.
The Informix database system provides support for many object-relational features.
Information about ObjectStore and Versant, including download of trial versions, may be obtained from their respective Web sites (objectstore.com and versant.com)
A reference implementation of JDO may be obtained from sun.com; use a search engine to get the full URL.
SQL:1999 was the product of an extensive (and long-delayed) standardization effort, which originally started off as adding object-oriented features to SQL and ended up addingmanymore features, such as procedural constructs, which we saw earlier.
Support for multiset types was added as part of SQL:2003
The object database standard ODMG is described in detail in Cattell [2000]
The Extensible Markup Language (XML) was not designed for database applications.
In fact, like the Hyper-Text Markup Language (HTML) on which the World Wide Web is based, XML has its roots in document management, and is derived from a language for structuring large documents known as the Standard Generalized Markup Language (SGML)
However, unlike SGML and HTML, XML is designed to represent data.
It is particularly useful as a data format when an application must communicate with another application, or integrate information from several other applications.
When XML is used in these contexts, many database issues arise, including how to organize, manipulate, and query the XML data.
In this chapter, we introduce XML and discuss both the management of XML data with database techniques and the exchange of data formatted as XML documents.
To understand XML, it is important to understand its roots as a document markup language.
The termmarkup refers to anything in a document that is not intended to be part of the printed output.
For example, a writer creating text that will eventually be typeset in a magazine may want to make notes about how the typesetting should be done.
It would be important to type these notes in a way so that they could be distinguished from the actual content, so that a note like “set this word in large size, bold font” or “insert a line break here” does not end up printed in the magazine.
In electronic document processing, amarkup language is a formal description of what part of the document is content, what part is markup, and what the markup means.
Just as database systems evolved from physical file processing to provide a separate logical view, markup languages evolved from specifying instructions for how to print parts of the document to specifying the function of the content.
For instance, with functional markup, text representing section headings (for this section, the word “Motivation”) would be marked up as being a section heading, instead of being marked up as text to be printed in large size, bold font.
From the viewpoint of typesetting, such functional markup allows the document to be.
It also helps different parts of a large document, or different pages in a large Web site, to be formatted in a uniform manner.More importantly, functional markup also helps recordwhat each part of the text represents semantically, and correspondingly helps automate extraction of key parts of documents.
For the family of markup languages that includes HTML, SGML, and XML, the markup takes the form of tags enclosed in angle brackets, <>
For example, the title of a document might be marked up as follows:
Unlike HTML, XML does not prescribe the set of tags allowed, and the set may be chosen as needed by each application.
This feature is the key to XML’s major role in data representation and exchange, whereas HTML is used primarily for document formatting.
Observe the use of tags such as department, course, instructor, and teaches.
To keep the example short, we use a simplified version of the university schema that ignores section information for courses.
We have also used the tag IID to denote the identifier of the instructor, for reasons we shall see later.
These tags provide context for each value and allow the semantics of the value to be identified.
For this example, the XML data representation does not provide any significant benefit over the traditional relational data representation; however, we use this example as our running example because of its simplicity.
Figure 23.3, which shows how information about a purchase order can be represented in XML, illustrates a more realistic use of XML.
Purchase orders are typically generated by one organization and sent to another.
Traditionally they were printed onpaper by thepurchaser and sent to the supplier; the datawould be manually re-entered into a computer system by the supplier.
This slow process can be greatly sped up by sending the information electronically between the purchaser and supplier.
The nested representation allows all information in a purchase order to be represented naturally in a single document.
Real purchase orders have considerably more information than that depicted in this simplified example.
XMLprovides a standardway of tagging the data; the two organizations must of course agree on what tags appear in the purchase order, and what they mean.
Compared to storage of data in a relational database, the XML representation may be inefficient, since tag names are repeated throughout the document.
However, in spite of this disadvantage, an XML representation has significant advantages when it is used to exchange data between organizations, and for storing complex structured information in files:
First, the presence of the tags makes the message self-documenting; that is, a schema need not be consulted to understand the meaning of the text.
For example, if some sender adds additional information, such as a tag last accessed noting the last date on which an account was accessed, the recipient of the XML data may simply ignore the tag.
The tag is required for items that are ordered by weight or volume, and may be omitted for items that are simply ordered by number.
The ability to recognize and ignore unexpected tags allows the format of the data to evolve over time, without invalidating existing applications.
Similarly, the ability to have multiple occurrences of the same tag makes it easy to represent multivalued attributes.
The purchase order shown in Figure 23.3 illustrates the benefits of having a nested structure.
Each purchase order has a purchaser and a list of items as two of its nested structures.
Each item in turn has an item identifier, description and a price nested within it, while the purchaser has a name and address nested within it.
Such information would have been split into multiple relations in a relational schema.
Item information would have been stored in one relation, purchaser information in a second relation, purchase orders in a third, and the relationship between purchase orders, purchasers, and items would have been stored in a fourth relation.
The relational representation helps to avoid redundancy; for example, item descriptions would be stored only once for each item identifier in a normalized relational schema.
In the XML purchase order, however, the descriptions may be repeated in multiple purchase orders that order the same item.
However, gathering all information related to a purchase order into a single nested structure, even at the cost of redundancy, is attractive when information has to be exchanged with external parties.
Finally, since the XML format is widely accepted, a wide variety of tools are available to assist in its processing, including programming language APIs to create and to read XML data, browser software, and database tools.
We describe several applications for XML data later, in Section 23.7
Just as SQL is the dominant language for querying relational data, XML has become the dominant format for data exchange.
The fundamental construct in an XML document is the element.
An element is simply a pair ofmatching start- and end-tags and all the text that appears between them.
While proper nesting is an intuitive property, wemay define it more formally.
Text is said to appear in the context of an element if it appears between the starttag and end-tag of that element.
Tags are properly nested if every start-tag has a unique matching end-tag that is in the context of the same parent element.
Note that text may be mixed with the subelements of an element, as in Figure 23.4
The ability to nest elements within other elements provides an alternative way to represent information.
The nested representation makes it easy to find all courses offered by a department.
Similarly, identifiers of courses taught by an instructor are nested within the instructor elements.
If an instructor teaches more than one course, there would be multiple course id elements within the correspond.
Details of instructors Brandt and Crick are omitted from Figure 23.5 for lack of space, but are similar in structure to that for Srinivasan.
Although nested representations are natural in XML, they may lead to redundant storage of data.
For example, suppose details of courses taught by an instructor are stored nestedwithin the instructor element as shown in Figure 23.6
If a course is taught by more than one instructor, course information such as title, department, and credits would be stored redundantly with every instructor associated with the course.
Nested representations are widely used in XML data interchange applications to avoid joins.
For instance, apurchase orderwould store the full address of sender and receiver redundantly on multiple purchase orders, whereas a normalized representationmay require a join of purchase order recordswith a company address relation to get address information.
In addition to elements, XML specifies the notion of an attribute.
For instance, the course identifier of a course can be represented as an attribute, as shown in Figure 23.7
The attributes of an element appear as name=value pairs before the closing “>” of a tag.
Furthermore, attributes can appear only once in a given tag, unlike subelements, which may be repeated.
Note that in a document construction context, the distinction between subelement and attribute is important—an attribute is implicitly text that does not appear in the printed or displayed document.
However, in database and data exchange applications of XML, this distinction is less relevant, and the choice of representing data as an attribute or a subelement is frequently arbitrary.
In general, it is advisable to use attributes only to represent identifiers, and to store all other data as subelements.
Since XML documents are designed to be exchanged between applications, a namespace mechanism has been introduced to allow organizations to specify globally unique names to be used as element tags in documents.
The idea of a namespace is to prepend each tag or attribute with a universal resource identifier (for example, a Web address)
Thus, for example, if Yale University wanted to ensure that XML documents it created would not duplicate tags used by any business partner’s XML documents, it could prepend a unique identifier with a colon to each tag name.
Using long unique identifiers in every tag would be rather inconvenient, so thenamespace standardprovides away todefine anabbreviation for identifiers.
In Figure 23.8, the root element (university) has an attribute xmlns:yale, which declares that yale is defined as an abbreviation for the URL given above.
The abbreviation can then be used in various element tags, as illustrated in the figure.
A document can have more than one namespace, declared as part of the root element.
A default namespace can be defined by using the attribute xmlns instead of xmlns:yale in the root element.
Elements without an explicit namespace prefix would then belong to the default namespace.
Sometimes we need to store values containing tags without having the tags interpreted as XML tags.
So that we can do so, XML allows this construct:
Figure 23.8 Unique tag names can be assigned by using namespaces.
Because it is enclosed within CDATA, the text <course> is treated as normal text data, not as a tag.
Databases have schemas, which are used to constrain what information can be stored in the database and to constrain the data types of the stored information.
In contrast, by default, XML documents can be created without any associated schema: an element may then have any subelement or attribute.
While such freedom may occasionally be acceptable given the self-describing nature of the data format, it is not generally useful when XML documents must be processed automatically as part of an application, or even when large amounts of related data are to be formatted in XML.
Here, we describe the first schema-definition language included as part of the XML standard, the Document Type Definition, as well as its more recently defined replacement, XML Schema.
Another XML schema-definition language called Relax NG is also in use, but we do not cover it here; for more information on Relax NG see the references in the bibliographical notes section.
The document type definition (DTD) is an optional part of an XML document.
The main purpose of a DTD is much like that of a schema: to constrain and type the information present in the document.
However, the DTD does not in fact constrain types in the sense of basic types like integer or string.
Instead, it constrains only the appearance of subelements and attributes within an element.
The DTD is primarily a list of rules for what pattern of subelements may appear within an element.
Each declaration is in the form of a regular expression for the subelements of an element.
The course element contains subelements course id, title, dept name, and credits (in that order)
Similarly, department and instructor have the attributes of their relational schema defined as subelements in the DTD.
Finally, the elements course id, title, dept name, credits, building, budget, IID, name, and salary are all declared to be of type #PCDATA.
The keyword #PCDATA indicates text data; it derives its name, historically, from “parsed character data.” Two other special type declarations are empty, which says that the element has no contents, and any, which says that there is no constraint on the subelements of the element; that is, any elements, even those not mentioned in the DTD, can occur as subelements of the element.
The absence of a declaration for an element is equivalent to explicitly declaring the type as any.
The allowable attributes for each element are also declared in the DTD.
Attributes may be specified to be of type CDATA, ID, IDREF, or IDREFS; the type CDATA simply says that the attribute contains character data, while the other three are not so simple; they are explained in more detail shortly.
For instance, the following line from a DTD specifies that element course has an attribute of type course id, and a value must be present for this attribute:
The default declaration can consist of a default value for the attribute or #REQUIRED, meaning.
If an attribute has a default value, for every element that does not specify a value for the attribute, the default value is filled in automatically when the XML document is read.
An attribute of type ID provides a unique identifier for the element; a value that occurs in an ID attribute of an element must not occur in any other element in the same document.
At most one attribute of an element is permitted to be of type ID.
We renamed the attribute ID of the instructor relation to IID in the XML representation, in order to avoid confusion with the type ID.
An attribute of type IDREF is a reference to an element; the attribute must contain a value that appears in the ID attribute of some element in the document.
The type IDREFS allows a list of references, separated by spaces.
The course elements use course id as their identifier attribute; to do so, course id has been made an attribute of course instead of a subelement.
Additionally, each course element also contains an IDREF of the department corresponding to the course, and an IDREFS attribute instructors identifying the instructors who teach the course.
The instructor elements have an identifier attribute called IID, and an IDREF attribute dept name identifying the department to which the instructor belongs.
The ID and IDREF attributes serve the same role as reference mechanisms in object-oriented and object-relational databases, permitting the construction of complex data relationships.
Document typedefinitions are strongly connected to thedocument formatting heritage of XML.
Because of this, they are unsuitable in many ways for serving as the type structure of XML for data-processing applications.
Nevertheless, a number of data exchange formats have been defined in terms of DTDs, since they were part of the original standard.
Here are some of the limitations of DTDs as a schema mechanism:
For instance, the element balance cannot be constrained to be a positive number.
The lack of such constraints is problematic for data processing and exchange applications, which must then contain code to verify the types of elements and attributes.
There is a lack of typing in IDs and IDREFSs.
Thus, there is no way to specify the type of element to which an IDREF or IDREFS attribute should refer.
As a result, the DTD in Figure 23.10 does not prevent the “dept name” attribute of a course element from referring to other courses, even though this makes no sense.
An effort to redress the deficiencies of the DTD mechanism resulted in the development of a more sophisticated schema language, XML Schema.
We provide a brief overview of XML Schema, and then we list some areas in which it improves DTDs.
In addition, it allows user-defined types; these may be simple types with added restrictions, or complex types constructed using constructors such as complexType and sequence.
The first thing to note is that schemadefinitions in XMLSchema are themselves specified in XML syntax, using a variety of tags defined by XML Schema.
Note that any namespace prefix could be used in place of xs; thuswe could replace all occurrences of “xs:” in the schema definition with “xsd:” without changing the meaning of the schema definition.
All types defined by XML Schema must be prefixed by this namespace prefix.
The first element is the root element university, whose type is specified to be UniversityType, which is declared later.
The example then defines the types of elements department, course, instructor, and teaches.
The type of department is defined to be a complex type, which is further specified to consist of a sequence of elements dept name, building, and budget.
Any type that has either attributes or nested subelements must be specified to be a complex type.
Alternatively, the type of an element can be specified to be a predefined type by the attribute type; observe how the XML Schema types xs:string and xs:decimal are used to constrain the types of data elements such as dept name and credits.
Finally the example defines the type UniversityType as containing zero or more occurrences of each of department, course, instructor, and teaches.
Note the use of ref to specify the occurrence of an element defined earlier.
The default for both minimum and maximum occurrences is 1, so these have to be specified explicitly to allow zero or more department, course, instructor, and teaches elements.
For example,we could have defined dept name as an attribute by adding:
Adding the attribute use = “required” to the above attribute specification declares that the attribute must be specified, whereas the default value of use is optional.
Attribute specifications would appear directly under the enclosing complexType specification, even if elements are nested within a sequence specification.
We can use the xs:complexType element to create named complex types; the syntax is the same as that used for the xs:complexType element in Figure 23.12, except thatwe addanattributename = typeName to the xs:complexType element, where typeName is the name we wish to give to the type.
We can then use the named type to specify the type of an element using the type attribute, just as we used xs:decimal and xs:string in our example.
In addition to defining types, a relational schema also allows the specification of constraints.
In SQL, a primary-key constraint or unique constraint ensures that the attribute values do not recur within the relation.
In the context of XML, we need to specify a scope within which values are unique and form a key.
The selector is a path expression that defines the scope for the constraint, and field declarations specify the elements or attributes that form the key.1 To specify that dept name forms a key for department elements under the root university element, we add the following constraint specification to the schema definition:
Correspondingly a foreign-key constraint from course to department may be defined as follows:
Note that the refer attribute specifies the name of the key declaration that is being referenced, while the field specification identifies the referring attributes.
Among the benefits that we have seen in the examples above are these:
It allows the text that appears in elements to be constrained to specific types, such as numeric types in specific formats or complex types such as sequences of elements of other types.
It is integrated with namespaces to allow different parts of a document to.
In addition to the features we have seen, XML Schema supports several other features that DTDs do not, such as these:
It allows types to be restricted to create specialized types, for instance by specifying minimum and maximum values.
It allows complex types to be extended by using a form of inheritance.
Our description of XML Schema is just an overview; to learn more about XML Schema, see the references in the bibliographical notes.
Given the increasing number of applications that use XML to exchange, mediate, and store data, tools for effective management of XML data are becoming increasingly important.
In particular, tools for querying and transformation of XML data are essential to extract information from large bodies of XML data, and to convert data between different representations (schemas) in XML.
Just as the output of a relational query is a relation, the output of an XML query can be an XMLdocument.
As a result, querying and transformation can be combined into a single tool.
In this section, we describe the XPath and XQuery languages:
XPath is a language for path expressions and is actually a building block for XQuery.
It is modeled after SQL but is significantly different, since it has to deal with nested XML data.
The XSLT language is another language designed for transforming XML.
The tools section at the end of this chapter provides references to software that can be used to execute queries written in XPath and XQuery.
A tree model of XML data is used in all these languages.
An XML document is modeled as a tree, with nodes corresponding to elements and attributes.
Element nodes canhave childnodes,which canbe subelements or attributes of the element.
Correspondingly, each node (whether attribute or element), other than the root element, has a parent node, which is an element.
The order of elements and attributes in the XML document is modeled by the ordering of children of nodes of the tree.
The terms parent, child, ancestor, descendant, and siblings are used in the tree model of XML data.
The text content of an element can be modeled as a text-node child of the element.
Elements containing text broken up by intervening subelements can have multiple text-node children.
XPath addresses parts of an XML document by means of path expressions.
The language can be viewed as an extension of the simple path expressions in objectoriented and object-relational databases (see Section 22.6)
The current version of the XPath standard is XPath 2.0, and our description is based on this version.
A path expression in XPath is a sequence of location steps separated by “/” (instead of the “.” operator that separates location steps in SQL)
The result of a path expression is a set of nodes.
For instance, on the document in Figure 23.11, the XPath expression:
Note that this is an abstract root “above” <university-3> that is the document tag.
As a path expression is evaluated, the result of the path at any point consists of an ordered set of nodes from the document.
Initially, the “current” set of elements contains only one node, the abstract root.
When the next step in a path expression is an element name, such as instructor, the result of the step consists of the nodes corresponding to elements of the specified name that are children of elements in the current element set.
These nodes then become the current element set for the next step of the path expression evaluation.
The result of a path expression is then the set of nodes after the last step of.
The nodes returned by each step appear in the same order as their appearance in the document.
Since multiple children can have the same name, the number of nodes in the node set can increase or decrease with each step.
Attribute values may also be accessed, using the “@” symbol.
By default, IDREF links are not followed; we shall see how to deal with IDREFs later.
Selection predicates may follow any step in a path, and are contained in square brackets.
XPath provides several functions that can be used as part of predicates, including testing the position of the current node in the sibling order and the aggregate function count(), which counts the number of nodes matched by the expression to which it is applied.
For example, on the XML representation in Figure 23.6, the path expression:
The function id(“foo”) returns the node (if any) with an attribute of type ID and value “foo”
For example, given datausing the schema fromFigure 23.11,we couldfind theunionofComputer Science and Biology courses using the expression:
However, the | operator cannot be nested inside other operators.
It is also worth noting that the nodes in the union are returned in the order in which they appear in the document.
An XPath expression can skip multiple levels of nodes by using “//”
This example illustrates the ability to find required data without full knowledge of the schema.
A step in the path need not just select from the children of the nodes in the current node set.
In fact, this is just one of several directions along which a step in the path may proceed, such as parents, siblings, ancestors, and descendants.
We omit details, but note that “//”, described above, is a short form for specifying “all descendants,” while “..” specifies the parent.
The built-in function doc(name) returns the root of a named document; the name could be a file name or a URL.
The root returned by the function can then be used in a path expression to access the contents of the document.
Thus, a path expression can be applied on a specified document, instead of being applied on the current default document.
The function collection(name) is similar todoc, but returns a collection of documents identified by name.
The function collection can be used, for example, to open an XML database, which can be viewed as a collection of documents; the following element in the XPath expression would select the appropriate document(s) from the collection.
Inmost of our examples,we assume that the expressions are evaluated in the.
In such cases, we do not need to use the functions doc and collection.
XQuery queries are modeled after SQL queries, but differ significantly from SQL.
They are organized into five sections: for, let, where, order by, and return.
They are referred to as “FLWOR” (pronounced “flower”) expressions, with the letters in FLWOR denoting the five sections.
The for clause is like the from clause of SQL, and specifies variables that range over the results of XPath expressions.
When more than one variable is specified, the results include the Cartesian product of the possible values the variables can take, just as the SQL from clause does.
The let clause simply allows the results of XPath expressions to be assigned to variable names for simplicity of representation.
Thewhere clause, like the SQL where clause, performs additional tests on the joined tuples from the for clause.
The order by clause, like the SQL order by clause, allows sorting of the output.
Finally, the return clause allows the construction of results in XML.
A FLWOR query need not contain all the clauses; for example a query may contain just the for and return clauses, and omit the let, where, and order by clauses.
The preceding XQuery query did not contain an order by clause.
Note further that, since the for clause uses XPath expressions, selections may.
Thus, an equivalent query may have only for and return clauses:
Note also that variables assigned by let clauses may contain sequences with multiple elements or values, if the path expression on the right-hand side returns a sequence of multiple elements or values.
XQueryprovides anotherway of constructing elements using the element and attribute constructors.
For example, if the return clause in the previous query is replaced by the following return clause, the query would return course elements with course id and dept name as attributes and title and credits as subelements.
Note that, as before, the curly brackets are required to treat a string as an expression to be evaluated.
Joins are specified inXQuerymuchas theyare in SQL.The joinofcourse, instructor, and teaches elements in Figure 23.1 can be written in XQuery this way:
The same query can be expressed with the selections specified as XPath selections:
Path expressions in XQuery are the same as path expressions in XPath2.0
Path expressions may return a single value or element, or a sequence of values or elements.
In the absence of schema information, it may not be possible to infer whether a path expression returns a single value or a sequence of values.
XQuery has an interesting definition of comparison operations on sequences.
If this behavior is not appropriate, the operators eq, ne, lt, gt, le, ge can be used instead.
These raise an error if either of their inputs is a sequence with multiple values.
XQuery FLWOR expressions can be nested in the return clause, in order to generate element nestings that do not appear in the source document.
Similarly, $d/text() gives the text content of an element, without the tags.
XQuery provides a variety of aggregate functions such as sum() and count() that can be applied on sequences of elements or values.
The function distinctvalues() applied on a sequence returns a sequence without duplication.
XQuery supports many other functions; see the references in the bibliographical notes for more information.
These functions are actually common to XPath 2.0 and XQuery, and can be used in any XPath path expression.
To avoid namespace conflicts, functions are associated with a namespace:
Thus, these functions can be referred to unambiguously as fn:sum or fn:count.
While XQuery does not provide a group by construct, aggregate queries can be written by using the aggregate functions on path or FLWOR expressions nested within the return clause.
For example, the following query on the university XML schema finds the total salary of all instructors in each department:
Results can be sorted in XQuery by using the order by clause.
For instance, this query outputs all instructor elements sorted by the name subelement:
To sort in descending order, we can use order by $i/name descending.
XQuery provides a variety of built-in functions, such as numeric functions and string matching and manipulation functions.
The following user-defined function takes as input an instructor identifier, and returns a list of all courses offered by the department to which the instructor belongs:
The namespace prefix xs: used in the above example is predefined by XQuery to be associated with the XML Schema namespace, while the namespace local: is predefined to be associated with XQuery local functions.
The type specifications for function arguments and return values are optional, and may be omitted.
The type element allows elements with any tag, while element(course) allows elements.
Types can be suffixed with a * to indicate a sequence of values of that type; for example, the definition of function dept courses specifies the return value as a sequence of course elements.
The following query, which illustrates function invocation, prints out the department courses for the instructor(s) named Srinivasan:
For example, if a numeric value represented by a string is compared to a numeric type, type conversion from string to the numeric type is done automatically.
When an element is passed to a function that expects a string value, type conversion to a string is done by concatenating all the text values contained (nested) within the element.
Thus, the function contains(a,b), which checks if string a contains string b, can be used with its first argument set to an element, in which case it checks if the element a contains the string b nested anywhere inside it.
XQuery offers a variety of other features, such as if-then-else constructs that can be used within return clauses, and existential and universal quantification that can be used in predicates inwhere clauses.
For example, existential quantification can be expressed in the where clause by using:
Universal quantification can be expressed by using every in place of some.
For example, to find departments where every instructor has a salary greater than $50,000, we can use the following query:
Note, however, that if a department has no instructor, it will trivially satisfy the above condition.
The built-in function exists() used in the clause returns true if its input argument is nonempty.
The XQJ standard provides an API to submit XQuery queries to an XML database system and to retrieve the XML results.
With the wide acceptance of XML as a data representation and exchange format, software tools are widely available for manipulation of XML data.
There are two standard models for programmatic manipulation of XML, each available for use with a number of popular programming languages.
Both these APIs can be used to parse an XMLdocument and create an in-memory representationof the document.
They are used for applications that deal with individual XML documents.
Note, however, that they are not suitable for querying large collections of XML data; declarative querying mechanisms such as XPath and XQuery are better suited to this task.
One of the standard APIs formanipulating XML is based on the document object model (DOM), which treats XML content as a tree, with each element represented by a node, called a DOMNode.
Programs may access parts of the document in a navigational fashion, beginning with the root.
We outline here some of the interfaces and methods in the Java API for DOM, to give a flavor of DOM.
The Java DOM API provides an interface called Node, and interfaces Element and Attribute, which inherit from the Node interface.
Attribute values of an element can be accessed by name, using the method getAttribute(name)
The text value of an element is modeled as a Text node, which is a child of the element node; an element node with no subelements has only one such child node.
The method getData() on the Text node returns the text contents.
Many more details are required for writing an actual DOM program; see the bibliographical notes for references to further information.
However, the DOM interface does not support any form of declarative querying.
The second commonly used programming interface, the Simple API for XML (SAX) is an eventmodel, designed to provide a common interface between parsers and applications.
This API is built on the notion of event handlers, which consist of user-specified functions associated with parsing events.
Parsing events correspond to the recognitionof parts of a document; for example, an event is generated when the start-tag is found for an element, and another event is generated when the end-tag is found.
The pieces of a document are always encountered in order from start to finish.
The SAX application developer creates handler functions for each event, and registers them.
When a document is read in by the SAX parser, as each event occurs, the handler function is called with parameters describing the event (such as element tag or text contents)
For example, to construct a tree representing the XML data, the handler functions for an attribute or element start event could add a node (or nodes) to a partially constructed tree.
The start- and end-tag event handlers would also have to keep track of the current node in the tree to which new nodes must be attached; the element start event would set the new element as the node that is the point where further child nodes must be attached.
The corresponding element end event would set the parent of the node as the current node where further child nodes must be attached.
If DOM were used for such applications, there would be unnecessary space and time overhead for constructing the DOM tree.
One way to store XML data is to store it as documents in a file system, while a second is to build a special-purpose database to store XML data.
Another approach is to convert the XML data to a relational representation and store it in a relational database.
Several alternatives for storing XML data are briefly outlined in this section.
There are several alternatives for storing XML data in nonrelational data-storage systems:
Since XML is primarily a file format, a natural storagemechanism is simply a flat file.
In particular, it lacks data isolation, atomicity, concurrent access, and security.
However, the wide availability of XML tools that work on file data makes it relatively easy to access and query XML data stored in files.
Thus, this storage format may be sufficient for some applications.
This allows much of the object-oriented database infrastructure to be reused, while providing a standard XML interface.
The addition of XQuery or other XML query languages provides declarative querying.
Other implementations have built the entire XML storage and querying infrastructure on top of a storage manager that provides transactional support.
Although several databases designed specifically to store XML data have been built, building a full-featured database system from ground up is a very complex task.
Such a database must support not only XML data storage and querying but also other database features such as transactions, security, support for data access from clients, and a variety of administration facilities.
It makes sense to instead use an existing database system to provide these facilities and implement XML data storage and querying either on top of the relational abstraction, or as a layer parallel to the relational abstraction.
Since relational databases are widely used in existing applications, there is a great benefit to be had in storing XML data in relational databases, so that the data can be accessed from existing applications.
Converting XML data to relational form is usually straightforward if the data were generated from a relational schema in the first place and XML is usedmerely as a data exchange format for relational data.
However, there are many applications where the XML data are not generated from a relational schema, and translating the data to relational form for storage may not be straightforward.
In particular, nested elements and elements that recur (corresponding to set-valued attributes) complicate storage of XML data in relational format.
Small XML documents can be stored as string (clob) values in tuples in a relational database.
Large XML documentswith the top-level element havingmany children can be handled by storing each child element as a string in a separate tuple.
For instance, the XML data in Figure 23.1 could be stored as a set of tuples in a relation elements(data), with the attribute data of each tuple storing one XML element (department, course, instructor, or teaches) in string form.
While the above representation is easy to use, the database system does not know the schema of the stored elements.
As a result, it is not possible to query the data directly.
A partial solution to this problem is to store different types of elements in different relations, and also store the values of some critical elements as attributes of the relation to enable indexing.
For instance, in our example, the relationswould be department elements, course elements, instructor elements, and teaches elements, each with an attribute data.
Each relation may have extra attributes to store the values of some subelements, such as dept name, course id, or name.
Thus, a query that requires department elements with a specified department name can be answered efficiently with this representation.
Such an approach depends on type information about XML data, such as the DTD of the data.
Some database systems, such as Oracle, support function indices, which can help avoid replication of attributes between the XML string and relation attributes.
Unlike normal indices, which are on attribute values, function indices can be built on the result of applying user-defined functions on tuples.
For instance, a function index can be built on a user-defined function that returns the value of the dept name subelement of the XML string in a tuple.
The index can then be used in the same way as an index on a dept name attribute.
The above approaches have the drawback that a large part of the XML information is stored within strings.
It is possible to store all the information in relations in one of several ways that we examine next.
Arbitrary XML data can be modeled as a tree and stored using a relation:
Each element and attribute in the XML data is given a unique identifier.
A tuple inserted in the nodes relation for each element and attribute with its identifier (id), the identifier of its parent node (parent id), the type of the node (attribute or element), the name of the element or attribute (label), and the text value of the element or attribute (value)
If order information of elements and attributes must be preserved, an extra attribute position can be added to the nodes relation to indicate the relative position of the child among the children of the parent.
As an exercise, you can represent the XML data of Figure 23.1 by using this technique.
This representation has the advantage that all XML information can be represented directly in relational form, and many XML queries can be translated into relational queries and executed inside the database system.
However, it has the drawback that each element gets broken up intomany pieces, and a large number of joins are required to reassemble subelements into an element.
In this approach, XML elements whose schema is known are mapped to relations and attributes.
Elements whose schema is unknown are stored as strings or as a tree.
A relation is created for each element type (including subelements) whose schema is known and whose type is a complex type (that is, contains attributes or subelements)
The root element of the document can be ignored in this step if it does not have any attributes.
All attributes of these elements are stored as string-valued attributes of the relation.
If a subelement of the element is a simple type (that is, cannot have attributes or subelements), an attribute is added to the relation to represent the subelement.
The type of the relation attribute defaults to a string value, but if the subelement had an XML Schema type, a corresponding SQL typemay be used.
For example, when applied to the element department in the schema (DTD or XML Schema) of the data in Figure 23.1, the subelements dept name, building and budgetof the element department all becomeattributes of a relation department.
Applying this procedure to the remaining elements, we get back the original relational schema that we have used in earlier chapters.
Otherwise, a relation is created corresponding to the subelement (using the same rules recursively on its subelements)
An identifier attribute is added to the relations representing the element.
The identifier attribute is added only once even if an element has several subelements.
An attribute parent id is added to the relation representing the subelement, storing the identifier of its parent element.
If ordering is to be preserved, an attribute position is added to the relation representing the subelement.
For example, if we apply the above procedure to the schema corresponding to the data in Figure 23.5, we get the following relations:
For example, the relations corresponding to subelements that can occur at most once can be “flattened” into the parent relation by moving all their attributes into the parent relation.
The bibliographical notes provide references to different approaches to represent XML data as relations.
When XML is used to exchange data between business applications, the data most often originates in relational databases.
Data in relational databases must be published, that is, converted to XML form, for export to other applications.
Incoming data must be shredded, that is, converted back from XML to normalized relation form and stored in a relational database.
While application code can perform the publishing and shredding operations, the operations are so common that the conversions should be done automatically, without writing application code, where possible.
Database vendors have spent a lot of effort to XML-enable their database products.
An XML-enabled database supports an automatic mechanism for publishing relational data as XML.
The mapping used for publishing data may be simple or complex.
A simple relation to XML mapping might create an XML element for every row of a table, and make each column in that row a subelement of the XML element.
The XML schema in Figure 23.1 can be created from a relational representation of university information, using such a mapping.
Such an XML view of relational data can be treated as a virtual XMLdocument, and XML queries can be executed against the virtual XML document.
A more complicated mapping would allow nested structures to be created.
Extensions of SQL with nested queries in the select clause have been developed to allow easy creation of nested XML output.
Mappings also have to be defined to shred XML data into a relational representation.
For XML data created from a relational representation, the mapping required to shred the data is a straightforward inverse of the mapping used to publish the data.
For the general case, a mapping can be generated as outlined in Section 23.6.2.3
Such systems store XML data as strings or in more efficient binary representations, without converting the data to relational form.
A new data type xml is introduced to represent XML data, although the CLOB and BLOB data types may provide the underlying storage mechanism.
A relation with an attribute of type xml can be used to store a collection of XML documents; each document is stored as a value of type xml in a separate tuple.
They provide an xml data type and allow XQuery queries to be embedded within SQL queries.
AnXQueryquery canbe executedona single XMLdocument andcanbe embedded within an SQL query to allow it to execute on each of a collection of documents, with each document stored in a separate tuple.
While XML is used widely for data interchange, structured data is still widely stored in relational databases.
There is often a need to convert relational data to XML representation.
The SQL/XML standard, developed tomeet this need, defines a standard extension of SQL, allowing the creation of nested XML output.
The standard has several parts, including a standard way of mapping SQL types to XMLSchema types, anda standardway tomap relational schemas to XML schemas, as well as SQL query language extensions.
For example, the SQL/XML representation of the department relation would have an XML schemawith outermost element department, with each tuplemapped to an XML element row, and each relation attribute mapped to an XML element of the same name (with some conventions to resolve incompatibilities with special characters in names)
An entire SQL schema, with multiple relations, can also be mapped to XML in a similar fashion.
SQL/XML adds several operators and aggregate operations to SQL to allow the construction of XML output directly from the extended SQL.
The xmlelement function can be used to create XML elements, while xmlattributes can be used to create attributes, as illustrated by the following query.
The above query creates an XML element for each course, with the course identifier and department name represented as attributes, and title and credits as subelements.
The result would look like the course elements shown in Figure 23.11, but without the instructor attribute.
The xmlattributes operator creates the XML attribute name using the SQL attribute name,which can be changed using an as clause as shown.
Its syntax and behavior are similar to those of xmlattributes, except that it creates a forest (collection) of subelements, instead of a list of attributes.
It takes multiple arguments, creating an element for each argument, with the attribute’s SQL name used as the XML element name.
The xmlconcat operator can be used to concatenate elements created by subexpressions into a forest.
When the SQL value used to construct an attribute is null, the attribute is omitted.
Null values are omitted when the body of an element is constructed.
SQL/XML also provides an aggregate function xmlagg that creates a forest (collection) of XML elements from the collection of values on which it is applied.
The following query creates an element for each department with a course, containing as subelements all the courses in that department.
Since the query has a clause group by dept name, the aggregate function is applied on all courses in each department, creating a sequence of course id elements.
SQL/XML allows the sequence created by xmlagg to be ordered, as illustrated in the preceding query.
See the bibliographical notes for references to more information on SQL/XML.
We now outline several applications of XML for storing and communicating (exchanging) data and for accessing Web services (information resources)
Many applications need to store data that are structured, but are not easily modeled as relations.
Consider, for example, user preferences that must be stored by an application such as a browser.
There are usually a large number of fields, such as home page, security settings, language settings, and display settings, that must be recorded.
Some of the fields aremultivalued, for example, a list of trusted sites, or maybe ordered lists, for example, a list of bookmarks.
Applications traditionally used some type of textual representation to store such data.
Today, a majority of such applications prefer to store such configuration information in XML format.
The ad hoc textual representations used earlier require effort to design and effort to create parsers that can read the file and convert the data into a form that a program can use.
XML-based representations are nowwidely used for storing documents, spreadsheet data and other data that are part of office application packages.
TheOpen Document Format (ODF), supported by the Open Office software suite as well as other office suites, and the Office Open XML (OOXML) format, supported by the Microsoft Office suite, are document representation standards based on XML.
They are the twomost widely used formats for editable document representation.
For example, a database system may represent a query execution plan (a relational-algebra expression with extra information on how to execute operations) by using XML.
This allows one part of the system to generate the query execution plan and another part to display it, without using a shared data structure.
For example, the data may be generated at a server system and sent to a client system where the data are displayed.
XML-based standards for representation of data have been developed for a variety of specialized applications, ranging from business applications such as banking and shipping to scientific applications such as chemistry and molecular biology.
The chemical industry needs information about chemicals, such as their molecular structure, and avariety of important properties, such as boiling and melting points, calorific values, and solubility in various solvents.
In shipping, carriers of goods and customs and tax officials need shipment records containing detailed information about the goods being shipped, from.
For example, the RosettaNet standards for e-business applications define XML schemas and semantics for representing data as well as standards for message exchange.
Using normalized relational schemas to model such complex data requirements would result in a large number of relations that do not correspond directly to the objects that are being modeled.
The relations would often have large numbers of attributes; explicit representation of attribute/element names along with values in XML helps avoid confusion between attributes.
Nested element representations help reduce the number of relations that must be represented, as well as the number of joins required to get required information, at the possible cost of redundancy.
Applications often require data from outside of the organization, or from another department in the same organization that uses a different database.
In many such situations, the outside organization or department is not willing to allow direct access to its database using SQL, but is willing to provide limited forms of information through predefined interfaces.
When the information is to be used directly by a human, organizations provide Web-based forms, where users can input values and get back desired information in HTML form.
However, there are many applications where such information needs to be accessed by software programs, rather than by end users.
Providing the results of a query in XML form is a clear requirement.
In addition, it makes sense to specify the input values to the query also in XML format.
In effect, the provider of the information defines procedures whose input and output are both in XML format.
The HTTP protocol is used to communicate the input and output information, since it is widely used and can go through firewalls that institutions use to keep out unwanted traffic from the Internet.
The Simple Object Access Protocol (SOAP) defines a standard for invoking procedures, using XML for representing the procedure input and output.
Typically, HTTP is used as the transport protocol for SOAP, but a messagebased protocol (such as email over the SMTP protocol) may also be used.
For example, Amazon and Google provide SOAP-based procedures to carry out search and other activities.
These procedures can be invoked by other applications that provide higher-level services to users.
The SOAP standard is independent of the underlying programming language, and it is possible for a site running one language, such as C#, to invoke a service that runs on a different language, such as Java.
A site providing such a collection of SOAP procedures is called a Web service.
The Web Services Description Language (WSDL) is a language used to describe a Web service’s capabilities.
In additionWSDL allows specification of the URL and network port number to be used to invoke the Web service.
There is also a standard called Universal Description, Discovery, and Integration (UDDI) that defines how a directory of available Web services may be created and how a program may search in the directory to find a Web service satisfying its requirements.
An airline may define aWeb service providing a set of procedures that can be invoked by a travel Web site; these may include procedures to find flight schedules and pricing information, as well as to make flight bookings.
The travel Web site may interact with multiple Web services, provided by different airlines, hotels, and other companies, to provide travel information to a customer and tomake travel bookings.
By supporting Web services, the individual companies allow a useful service to be constructed on top, integrating the individual services.
Users can interact with a single Web site to make their travel bookings, without having to contact multiple separate Web sites.
To invoke a Web service, a client must prepare an appropriate SOAP XML message and send it to the service; when it gets the result encoded in XML, the client must then extract information from the XML result.
There are standard APIs in languages such as Java and C# to create and extract information from SOAP messages.
See the bibliographical notes for references to more information on Web services.
Comparison shopping is an example of a mediation application, in which data about items, inventory, pricing, and shipping costs are extracted from a variety of Web sites offering a particular item for sale.
The resulting aggregated information is significantly more valuable than the individual information offered by a single site.
A personal financial manager is a similar application in the context of banking.
Consider a consumer with a variety of accounts to manage, such as bank accounts, credit-card accounts, and retirement accounts.
Suppose that these accounts may be held at different institutions.
XML-based mediation addresses the problem by extracting an XML representation of account information from the respective Web sites of the financial institutions where the individual holds accounts.
This informationmay be extracted easily if the institution exports it in a standard XML format, for example, as a Web service.
For those that do not, wrapper software is used to generate XML data from HTML Web pages returned by the Web site.
Wrapper applications need constant maintenance, since they depend on formatting details of Web pages, which change often.
Nevertheless, the value provided bymediation often justifies the effort required to develop and maintain wrappers.
Once the basic tools are available to extract information from each source, a mediator application is used to combine the extracted information under a single schema.
This may require further transformation of the XML data from each site, since different sites may structure the same information differently.
They may also use different names for the same information (for instance, acct number and account id), or may even use the same name for different information.
The mediatormust decide on a single schema that represents all required information, andmust provide code to transform data between different representations.
Such issues are discussed in more detail in Section 19.8, in the context of distributed databases.
Elements may have subelements nested within them, to any level of nesting.
The choice between representing information as attributes and subelements is often arbitrary in the context of data representation.
Elements may have an attribute of type ID that stores a unique identifier for the element.
Elements may also store references to other elements by using attributes of type IDREF.
Attributes of type IDREFS can store a list of references.
Documents optionally may have their schema specified by a Document Type Declaration (DTD)
The DTD of a document specifies what elements may occur, how they may be nested, and what attributes each element may have.
It provides a large set of basic types, as well as constructs for.
Nesting of elements is reflected by the parent-child structure of the tree representation.
Path expressions can be used to traverse the XML tree structure and locate data.
XPath is a standard language for path expressions, and allows required elements to be specified by a file-system-like path, and additionally allows selections and other features.
The XQuery language is the standard language for querying XML data.
It has a structure not unlike SQL, with for, let,where, order by, and return clauses.
However, it supports many extensions to deal with the tree nature of XML and to allow for the transformation of XML documents into other documents with a significantly different structure.
The DOM and SAX APIs are widely used for programmatic access to XML data.
These APIs are available from a variety of programming languages.
As another alternative, XML data can be mapped to relations in the same way that E-R schemas are mapped to relational schemas.
Native storage of XML in relational databases is facilitated by adding an xml data type to SQL.
Web services provide a remote-procedure call interface, with XML as the mechanism for encoding parameters as well as results.
Also give the DTD or XML Schema for this representation.
Emp = (ename, ChildrenSet setof(Children), SkillsSet setof(Skills)) Children = (name, Birthday) Birthday = (day, month, year) Skills = (type, ExamsSet setof(Exams)) Exams = (year, city)
The relational schema must keep track of the order of author elements.
You can assume that only books and articles appear as top-level elements in XML documents.
Give a small example of data corresponding to this DTD.
Show how to map this DTD to a relational schema.
You can assume that part names are unique; that is, wherever a part appears, its subpart structure will be the same.
Create a schema in XML Schema corresponding to this DTD.
Find the names of all employeeswhohave a childwhohas a birthday in March.
Find those employees who took an examination for the skill type “typing” in the city “Dayton”
That is, at the outermost level of nesting the output must have elements corresponding to authors, and each such elementmust have nestedwithin it items corresponding to all the books written by the author.
Create a separate element type to represent each relationship, but use ID and IDREF to implement primary and foreign keys.
Find all authors who have authored a book and an article in the same year.
Find all books that contain the word “database” in their title and the word “Hank” in an author’s name (whether first or last)
Suggest how to remove redundancy in the relational schema, if item identifiers functionally determine the description and purchase and supplier names functionally determine the purchase and supplier address, respectively.
Check if the key constraint shown in Section 23.3.2 holds.
Check if the keyref constraint shown in Section 23.3.2 holds.
What change would have to be done to the relational schema?
A number of tools to deal with XML are available in the public domain.
Exist (exist-db.org) is an open source XML database, supporting a variety of features.
The World Wide Web Consortium (W3C) acts as the standards body for Webrelated standards, including basic XML and all the XML-related languages such as.
A large number of technical reports defining the XMLrelated standards are available at www.w3.org.
This site also contains tutorials and pointers to software implementing the various standards.
The XQuery language derives from an XML query language called Quilt; Quilt itself included features from earlier languages such as XPath, discussed in Section 23.4.2, and two other XML query languages, XQL and XML-QL.
Integration of keyword querying into XML is outlined by Florescu et al.
A large number of papers have been published in this area.
One of the challenges in indexing is that queries may specify a selection on a path, such as /a/b//c[d=“CSE”]; the index must support efficient retrieval of nodes that satisfy the path specification and the value selection.Work on indexing of XML data includes Pal et al.
If data is shredded and stored in relations, evaluating a path expression maps to computation of a join.
Several techniques have been proposed for efficiently computing such joins, in particular when the path expression specifies any descendant (//)
Several techniques for numbering of nodes in XML data have been proposed that can be used to efficiently check if a node is a descendant of another; see, for example, O’Neil et al.
Work on optimization of XML queries includes McHugh and Widom [1999], Wu et al.
It then discusses standard benchmarks that are used as measures of commercial database-system performance.
The chapter concludeswith anoverview of the standardization process and existing database-language standards.
Chapter 25 describes spatial and temporal data types, and multimedia data, and the issues in storing such data in databases.
Database issues related tomobile computing systems are also described in this chapter.
In fact, it is common to find that once an application has been built, it runs slower than the designers wanted, or handles fewer transactions per second than they required.
An application that takes an excessive amount of time to perform requested actions can cause user dissatisfaction at best and be completely unusable at worst.
Applications can be made to run significantly faster by performance tuning, which consists of finding and eliminating bottlenecks and adding appropriate hardware such as memory or disks.
There are many things an application developer can do to tune the application, and there are things that a database-system administrator can do to speed up processing for an application.
Benchmarks are standardized sets of tasks that help to characterize the performance of database systems.
They are useful to get a rough idea of the hardware and software requirements of an application, even before the application is built.
Testing requires generation of database states and test inputs, and verifying that the outputsmatch the expected outputs.
Legacy systems are application systems that are outdated and usually based on older-generation technology.
However, they are often at the core of organizations, and runmissioncritical applications.
We outline issues in interfacing with and issues in migrating away from legacy systems, replacing them with more modern systems.
Standards are very important for application development, especially in the age of the Internet, since applications need to communicate with each other to perform useful tasks.
Tuning the performance of a system involves adjusting various parameters and design choices to improve its performance for a specific application.
Each of these aspects can be adjusted so that performance is improved.
When SQL queries are executed from an application program, it is often the case that a query is executed frequently, but with different values for a parameter.
Each call has an overhead of communication with the server, in addition to processing overheads at the server.
For example, consider a program that steps through each department, invoking an embedded SQL query to find the total salary of all instructors in the department:
If the instructor relation does not have a clustered index on dept name, each such query will result in a scan of the relation.
Even if there is such an index, a random I/O operation will be required for each dept name value.
Instead, we can use a single SQL query to find total salary expenses of each department:
This query can be evaluated with a single scan of the instructor relation, avoiding random I/O for eachdepartment.
The results can be fetched to the client side using a single round of communication, and the client program can then step through the results to find the aggregate for each department.
Combining multiple SQL queries into a single SQL query as above can reduce execution costs greatly in many cases–for example, if the instructor relation is very large and has a large number of departments.
The JDBCAPI also provides a feature calledbatch update that allows a number of inserts to be performed using a single communication with the database.
The code shown in the figure requires only one round of communication with the database, when the executeBatch() method is executed, in contrast to similar code without the batch update feature that we saw earlier in Figure 5.2
In the absence of batch update, as many rounds of communication with the database are required as there are instructors to be inserted.
The batch update feature also enables the database to process a batch of.
Another technique used widely in client–server systems to reduce the cost of communication and SQL compilation is to use stored procedures, where queries are stored at the server in the form of procedures, which may be precompiled.
Clients can invoke these stored procedures, rather than communicate a series of queries.
Another aspect of improving set orientation lies in rewriting queries with nested subqueries.
Today’s advanced optimizers can transform even badly written queries and execute them efficiently, so the need for tuning individual queries is less important than it used to be.
However, complex queries containing nested subqueries are not optimized very well by many optimizers.
We saw techniques for nested subquery decorrelation in Section 13.4.4
If a subquery is not decorrelated, it gets executed repeatedly, potentially resulting in a great deal of random I/O.
In contrast, decorrelation allows efficient set-oriented operations such as joins to be used,minimizing random I/O.Most database query optimizers incorporate some forms of decorrelation, but some can handle only very simple nested subqueries.
If the optimizer has not succeeded in decorrelating a nested subquery, the query can be decorrelated by rewriting it manually.
When loading a large volume of data into a database (called a bulk load operation), performance is usually very poor if the inserts are carried out a separate SQL insert statements.
One reason is the overhead of parsing each SQL query; a more important reason is that performing integrity constraint checks and index.
To support bulk load operations, most database systems provide a bulk import utility, and a corresponding bulk export utility.
The bulk-import utility reads data from a file, and performs integrity constraint checking as well as index maintenance in a very efficient manner.
Common input and output file format supported by such bulk import/export utilities include text files with characters such as commas or tabs separating attribute values, with each record in a line of its own (such file formats are referred to as comma-separated values or tab-separated values formats)
Database specific binary formats, as well as XML formats are also supported by bulk import/export utilities.
The names of the bulk import/export utilities differ by database.
In PostgreSQL, the utilities are called pg dump and pg restore (PostgreSQL also provides an SQL command copywhich provides similar functionality)
We now consider the case of tuning of bulk updates.
Suppose we have a relation funds received(dept name, amount) that stores funds received (say, by electronic funds transfer) for each of a set of departments.
Suppose now that we want to add the amounts to the balances of the corresponding department budgets.
In order to use the SQL update statement to carry out this task, we have to perform a look up on the funds received relation for each tuple in the department relation.
We can use subqueries in the update clause to carry out this task, as follows: We assume for simplicity that the relation funds received contains at most one tuple for each department.
Note that the condition in the where clause of the update ensures that only accounts with corresponding tuples in funds received are updated, while the subquery within the set clause computes the amount to be added to each such department.
There are many applications that require updates such as that illustrated above.
Typically, there is a table, whichwe shall call themaster table, and updates to the master table are received as a batch.
SQL:2003 provides a special construct, called themerge construct, to simplify the task of performing such merging of information.
For example, the above update can be expressed usingmerge as follows:
When a record from the subquery in the using clause matches a record in the department relation, the when matched clause is executed, which can execute an update on the relation; in this case, the matching record in the department relation is updated as shown.
The merge statement can also have a when not matched then clause, which permits insertion of new records into the relation.
In the above example, when there is no matching department for a funds received tuple, the insertion action could create a new department record (with a null building) using the following clause:
Although not very meaningful in this example,1 the when not matched then clause can be quite useful in other cases.
For example, suppose the local relation is a copy of a master relation, and we receive updated as well as newly inserted records from themaster relation.
Themerge statement can updatematched records (these would be updated old records) and insert records that are not matched (these would be new records)
Not all SQL implementations support the merge statement currently; see the respective system manuals for further details.
Theperformance ofmost systems (at least before they are tuned) is usually limited primarily by the performance of one or a few components, called bottlenecks.
Improving the performance of a component that is not a bottleneck does little to improve the overall speed of the system; in the example, improving the speed of the rest of the code cannot lead to more than a.
A better action here would have been to insert these records into an error relation, but that cannot be done with the merge statement.
In a well-balanced system, no single component is the bottleneck.
If the system contains bottlenecks, components that are not part of the bottleneck are underutilized, and could perhaps have been replaced by cheaper components with lower performance.
For simple programs, the time spent in each region of the code determines the overall execution time.
However, database systems are much more complex, and can be modeled as queueing systems.
A transaction requests various services from the database system, starting from entry into a server process, disk reads during execution, CPU cycles, and locks for concurrency control.
Each of these services has a queue associated with it, and small transactions may spend most of their time waiting in queues—especially in disk I/O queues—instead of executing code.
Figure 24.2 illustrates some of the queues in a database system.
As a result of the numerous queues in the database, bottlenecks in a database system typically show up in the form of long queues for a particular service, or, equivalently, in high utilizations for a particular service.
If requests are spaced exactly uniformly, and the time to service a request is less than or equal to the.
Unfortunately, the arrival of requests in a database system is never so uniform and is instead random.
Assuming uniformly randomly distributed arrivals, the length of the queue (and correspondingly the waiting time) go up exponentially with utilization; as utilization approaches 100 percent, the queue length increases sharply, resulting in excessively long waiting times.
The utilization of a resource should be kept low enough that queue length is short.
To learnmore about the theory of queueing systems, generally referred to as queueing theory, you can consult the references cited in the bibliographical notes.
Database administrators can tune a database system at three levels.
Options for tuning systems at this level include adding disks or using a RAID system if disk I/O is a bottleneck, adding more memory if the disk buffer size is a bottleneck, or moving to a faster processor if CPU use is a bottleneck.
The second level consists of the database-system parameters, such as buffer size and checkpointing intervals.
Most database-system manuals provide information on what database-system parameters can be adjusted, and how you should choose values for the parameters.
Well-designed database systems perform as much tuning as possible automatically, freeing the user or database administrator from the burden.
For instance, in many database systems the buffer size is fixed but tunable.
If the system automatically adjusts the buffer size by observing indicators such as page-fault rates, then the database administrator will not have to worry about tuning the buffer size.
The administrator can tune the design of the schema, the indices that are created, and the transactions that are executed, to improve performance.
The three levels of tuning interact with one another; we must consider them together when tuning a system.
For example, tuning at a higher level may result in the hardware bottleneck changing from the disk system to the CPU, or vice versa.
Even in a well-designed transaction processing system, each transaction usually has to do at least a few I/O operations, if the data required by the transaction.
An important factor in tuning a transaction processing system is to make sure that the disk subsystem can handle the rate at which I/O operations are required.
The onlyway to supportmore transactions per second is to increase the number of disks.
Notice here that the limiting factor is not the capacity of the disk, but the speed at which random data can be accessed (limited in turn by the speed at which the disk arm can move)
The number of I/O operations per transaction can be reduced by storing more data in memory.
If all data are in memory, there will be no disk I/O except for writes.
Keeping frequently used data in memory reduces the number of disk I/Os, and is worth the extra cost of memory.
Keeping very infrequently used data in memory would be a waste, since memory is much more expensive than disk.
The question is, for a given amount of money available for spending on disks or memory, what is the best way to spend the money to achieve the maximum number of transactions per second.
Thus, if a particular page is accessed n times per second, the saving due to keeping it in memory is n times the above value.
We can rearrange the equation and substitute current values for each of the above parameters to get a value for n; if a page is accessedmore frequently than this, it is worth buying enough memory to store it.
In other words, some years ago, the rule suggested buying.
Today, it is worth buying enough memory to cache all pages that are accessed at least once in 2 hours on average.
For data that are accessed less frequently, buy enough disks to support the rate of I/O required for the data.
For data that are sequentially accessed, significantly more pages can be read per second.
Surprisingly, this figure has not changed all that much over the years, since disk transfer rates have increased greatly, even though the price of a megabyte of memory has reduced greatly compared to the price of a disk.
The 5-minute rule of thumb and its variants take only the number of I/O operations into account, and do not consider factors such as response time.
Some applications need to keep even infrequently used data in memory, to support response times that are less than or comparable to disk-access time.
With the wide availability of flash memory, and “solid-state disks” based on flash memory, system designers can now choose to store frequently used data in flash storage, instead of storing it on disk.
Alternatively, in the flash-as-buffer approach, flash storage is used as a persistent buffer, with each block having a permanent location on disk, but stored in flash instead of being written to disk as long as it is frequently used.
When flash storage is full, a block that is not frequently used is evicted, and flushed back to disk if it was updated after being read from disk.
The flash-as-buffer approach requires changes in the database system itself.
Even if a database system does not support flash as a buffer, a database administrator can control the mapping of relations or indices to disks, and allocate frequently used relations/indices to flash storage.
The tablespace feature, supported bymost database systems, can be used to control themapping, by creating a tablespace on flash storage and assigning desired relations and indices to that tablespace.
Controlling the mapping at a finer level of granularity than a relation, however, requires changes to the database-system code.
The “5-minute” rule has been extended to the case where data can be stored on flash, in addition to main memory and disk.
We can then calculate the number of disks required to support the required I/O operations per.
For many applications, r and w are large enough that the (r + w)/100 disks can easily hold two copies of all the data.
Within the constraints of the chosen normal form, it is possible to partition relations vertically.
Within the constraints of the normal forms (BCNF and third normal forms), we can partition the course relation into two relations:
The two representations are logically equivalent, since course id is a key, but they have different performance characteristics.
If most accesses to course information look at only the course id and credits, then they can be run against the course credit relation, and access is likely to be somewhat faster, since the title and dept name attributes are not fetched.
For the same reason, more tuples of course credit will fit in the buffer than corresponding tuples of course, again leading to faster performance.
This effect would be particularly marked if the title and dept name attributes were large.
Hence, a schema consisting of course credit and course title dept would be preferable to a schema consisting of the course relation in this case.
On the other hand, if most accesses to course information require both dept name and credits, using the course relation would be preferable, since the cost of the join of course credit and course title dept would be avoided.
Also, the storage overheadwould be lower, since therewould be only one relation, and the attribute course id would not be replicated.
The column store approach to storing data is based on vertical partitioning, but takes it to the limit by storing each attribute (column) of the relation in a separate file.
Column stores have been shown to perform well for several datawarehouse applications.
Another trick to improveperformance is to store a denormalized relation, suchas a join of instructor and department, where the information about dept name, building, and budget is repeated for every instructor.
More effort has to be expended to make sure the relation is consistent whenever an update is carried out.
However, a query that fetches the names of the instructors and the associated buildings will be speeded up, since the join of instructor and department will have been.
If such a query is executed frequently, and has to be performed as efficiently as possible, the denormalized relation could be beneficial.
Materialized views can provide the benefits that denormalized relations provide, at the cost of some extra storage; we describe performance tuning of materialized views in Section 24.1.8
A major advantage to materialized views over denormalized relations is that maintaining consistency of redundant data becomes the job of the database system, not the programmer.
Thus, materialized views are preferable, whenever they are supported by the database system.
Another approach to speed up the computation of the join without materializing it, is to cluster records that would match in the join on the same disk page.
We can tune the indices in a database system to improve performance.
If queries are the bottleneck, we can often speed them up by creating appropriate indices on relations.
If updates are the bottleneck, there may be too many indices, which have to be updatedwhen the relations are updated.
The choice of the type of index also is important.
Some database systems support different kinds of indices, such as hash indices and B-tree indices.
If range queries are common, B-tree indices are preferable to hash indices.
Whether to make an index a clustered index is another tunable parameter.
Only one index on a relation can be made clustered, by storing the relation sorted on the index attributes.
Generally, the index that benefits the greatest number of queries and updates should be made clustered.
To help identify what indices to create, and which index (if any) on each relation should be clustered, most commercial database systems provide tuning wizards; these are described in more detail in Section 24.1.9
These tools use the past history of queries and updates (called the workload) to estimate the effects of various indices on the execution time of the queries and updates in the workload.
Recommendations on what indices to create are based on these estimates.
Maintaining materialized views can greatly speed up certain types of queries, in particular aggregate queries.
Recall the example from Section 13.5 where the total salary for each department (obtained by summing the salary of each instructor in the department) is required frequently.
As we saw in that section, creating a materialized view storing the total salary for each department can greatly speed up such queries.
Materialized views should be used with care, however, since there is not only space overhead for storing them but, more important, there is also time overhead for maintaining materialized views.
In the case of immediate viewmaintenance, if the updates of a transaction affect the materialized view, the materialized view must be updated as part of the same transaction.
In the case of deferred view maintenance, the materialized view is.
For instance, the materialized view may be brought upto-date when a query uses the view, or periodically.
The database administrator is responsible for the selection of materialized views and for view-maintenance policies.
The database administrator can make the selection manually by examining the types of queries in the workload, and finding out which queries need to run faster and which updates/queries may be executed more slowly.
From the examination, the database administrator may choose an appropriate set of materialized views.
For instance, the administrator mayfind that a certain aggregate is used frequently, and choose tomaterialize it, or may find that a particular join is computed frequently, and choose to materialize it.
However, manual choice is tedious for even moderately large sets of query types, andmaking a good choice may be difficult, since it requires understanding the costs of different alternatives; only the query optimizer can estimate the costs with reasonable accuracy, without actually executing the query.
Thus a good set of views may be found only by trial and error—that is, by materializing one or more views, running the workload, and measuring the time taken to run the queries in theworkload.
The administrator repeats the process until a set of views is found that gives acceptable performance.
A better alternative is to provide support for selecting materialized views within the database system itself, integrated with the query optimizer.
This approach is described in more detail in Section 24.1.9
Most commercial database systems today provide tools to help the database administrator with index andmaterialized view selection, and other tasks related to physical database design such as how to partition data in a parallel database system.
These tools examine the workload (the history of queries and updates) and suggest indices and views to be materialized.
The database administrator may specify the importance of speeding up different queries, which the tool takes into account when selecting views to materialize.
Often tuning must be done before the application is fully developed, and the actual database contents may be small on the development database, but expected to be much larger on a production database.
Thus, some tuning tools also allow the database administrator to specify information about the expected size of the database and related statistics.
Microsoft’s Database Tuning Assistant, for example, allows the user to ask “what if” questions, whereby the user can pick a view, and the optimizer then estimates the effect of materializing the view on the total cost of the workload and on the individual costs of different types of queries and updates in the workload.
Since the number of design alternatives may be extremely large, as also the workload, the selection techniques must be designed carefully.
This is usually done by recording all the queries and updates that are executed during some time period.
Next, the selection tools perform workload compression, that is, create a representation of the workload using a small number of updates and queries.
For example, updates of the same form can be represented by a single update with a weight corresponding to howmany times the update occurred.
Queries of the same form can be similarly replaced by a representative with appropriate weight.
After this, queries that are very infrequent and do not have a high cost may be discarded from consideration.
The most expensive queries may be chosen to be addressed first.
Heuristics are used to reduce the space of alternatives, that is, to reduce the number of combinations considered.
Greedy heuristics for index and materialized view selection operate as follows: They estimate the benefits of materializing different indices or views (using the optimizer’s cost estimation functionality as a subroutine)
They then choose the index or view that gives either the maximum benefit or the maximum benefit per unit space (that is, benefit divided by the space required to store the index or view)
The cost of maintaining the index or view must be taken into account when computing the benefit.
Once the heuristic has selected an index or view, the benefits of other indices or views may have changed, so the heuristic recomputes these, and chooses the next best index or view for materialization.
The process continues until either the available disk space for storing indices or materialized views is exhausted, or the cost of maintaining the remaining candidates is more than the benefit to queries that could use the indices or views.
Real-world index and materialized-view selection tools usually incorporate some elements of greedy selection, but use other techniques to get better results.
They also support other aspects of physical database design, such asdecidinghow to partition a relation in a parallel database, or what physical storage mechanism to use for a relation.
Concurrent execution of different types of transactions can sometimes lead to poor performance because of contention on locks.
We first consider the case of read-write contention, which is more common, and then consider the case of write-write contention.
As an example of read-write contention, consider the following situation on a banking database.
Suppose that a large query that computes statistics on branches is run at the same time.
If the query performs a scan on a relation, it may block out all updates on the relation while it runs, and that can have a disastrous effect on the performance of the system.
Several database systems—Oracle, PostgreSQL, and Microsoft SQL Server, for example— support snapshot isolation, whereby queries are executed on a snapshot of the data, and updates can go on concurrently.
Snapshot isolation should be used, if available, for large queries, to avoid lock contention in the above situation.
In Oracle and PostgreSQL, using the keyword serializable in place of the keyword snapshot in the above command has the same effect, since both these systems actually use snapshot isolation when the isolation level is set to serializable.
If snapshot isolation is not available, an alternative option is to execute large queries at times when updates are few or nonexistent.
However, for databases supporting Web sites, there may be no such quiet period for updates.
Another alternative is to use weaker levels of consistency, such as the read committed isolation level, whereby evaluation of the query has aminimal impact on concurrent updates, but the query results are not guaranteed to be consistent.
The application semantics determine whether approximate (inconsistent) answers are acceptable.
Data items that are updated very frequently can result in poor performance with locking, with many transactions waiting for locks on those data items.
Update hot spots can cause problems even with snapshot isolation, causing frequent transaction aborts due to write validation failures.
A commonly occurring situation that results in an update hot spot is as follows: transactions need to assign unique identifiers to data items being inserted into the database, and to do so they read and increment a sequence counter stored in a tuple in the database.
If inserts are frequent, and the sequence counter is locked in a two-phase manner, the tuple containing the sequence counter becomes a hot spot.
Oneway to improve concurrency is to release the lock on the sequence counter immediately after it is read and incremented; however, after doing so, even if the transaction aborts, the update to the sequence counter should not be rolled back.
Most databases provide a special construct for creating sequence counters that implement early, non-two-phase, lock release, coupled with special case treatment of undo logging so that updates to the counter are not rolled back if the transaction aborts.
The SQL standard allows a sequence counter to be created using the command:
In the above command, counter1 is the name of the sequence; multiple sequences can be created with different names.
The SQL standardprovides analternative tousinganexplicit sequence counter, which is useful when the goal is to give unique identifiers to tuples inserted into a relation.
To do so, the keyword identity can be added to the declaration of an integer attribute of a relation (usually this attribute would also be declared as the primary key)
If the value for that attribute is left unspecified in an insert statement for that relation, a unique new value is created automatically for each newly inserted tuple.
A non-two-phase locked sequence counter is used internally to implement the identity declaration, with the counter incremented each time a tuple is inserted.
Several databases including DB2 and SQL Server support the identity declaration, although the syntax varies.
PostgreSQL supports a data type called serial, which provides the same effect; the PostgreSQL type serial is implemented by transparently creating a non-two-phase locked sequence.
It isworth noting that since the acquisition of a sequence number by a transaction cannot be rolled back if the transaction aborts (for reasons discussed earlier), transaction aborts may result in gaps in the sequence numbers in tuple inserted in the database.
Such gaps are not acceptable in some applications; for example, some financial applications require that there be no gaps in bill or receipt numbers.
Database provided sequences and automatically incremented attributes should not be used for such applications, since they can result in gaps.A sequence counter stored in normal tuple, which is locked in a two-phase manner, would not be susceptible to such gaps since a transaction abort would restore the sequence counter value, and the next transaction would get the same sequence number, avoiding a gap.
Long update transactions can cause performance problems with system logs, and can increase the time taken to recover from system crashes.
If a transaction performs many updates, the system log may become full even before the transaction completes, in which case the transaction will have to be rolled back.
If an update transaction runs for a long time (even with few updates), it may block deletion of old parts of the log, if the logging system is not well designed.
Again, this blocking could lead to the log getting filled up.
To avoid such problems, many database systems impose strict limits on the number of updates that a single transaction can carry out.
For example, a transaction that gives a raise to every employee in a large corporation could be split up into a series of small transactions, each of which updates a small range of employeeids.
First, if there are concurrent updates on the set of employees, the result of the set of smaller transactionsmaynot be equivalent to that of the single large transaction.
Second, if there is a failure, the salaries of some of the employees would have been increased by committed transactions, but salaries of other employees would not.
To avoid this problem, as soon as the system recovers from failure, we must execute the transactions remaining in the batch.
Long transactions, whether read-only or update, can also result in the lock table becoming full.
If a single query scans a large relation, the query optimizer would ensure that a relation lock is obtained instead of acquiring a large number of tuple locks.
However, if a transaction executes a large number of small queries or updates, it may acquire a large number of locks, resulting in the lock table becoming full.
To avoid this problem, some databases provide for automatic lock escalation; with this technique, if a transaction has acquired a large number of tuple locks, tuple locks are upgraded to page locks, or even full relation locks.
On databases that do not support lock escalation, it is possible for the transaction to explicitly acquire a relation lock, thereby avoiding the acquisition of tuple locks.
Each service shown in Figure 24.2, such as the CPU, each disk, the buffer, and the concurrency control, is modeled in the simulation.
Instead of modeling details of a service, the simulation model may capture only some aspects of each service, such as the service time—that is, the time taken tofinishprocessing a request once processing has begun.
Thus, the simulation can model a disk access from just the average disk-access time.
Since requests for a service generally have to wait their turn, each service has an associated queue in the simulation model.
The requests are queued up as they arrive, and are serviced according to the policy for that service, such as first come, first served.
The models for services such as CPU and the disks conceptually operate in parallel, to account for the fact that these subsystems operate in parallel in a real system.
Once the simulation model for transaction processing is built, the system administrator can run a number of experiments on it.
The administrator can use experiments with simulated transactions arriving at different rates to find how the systemwould behave under various load conditions.
The administrator could run other experiments that vary the service times for each service to find out how sensitive the performance is to each of them.
System parameters, too, can be varied, so that performance tuning can be done on the simulation model.
As database servers become more standardized, the differentiating factor among the products of different vendors is those products’ performance.
Performance benchmarks are suites of tasks that are used to quantify the performance of software systems.
Since most software systems, such as databases, are complex, there is a good deal of variation in their implementation by different vendors.
As a result, there is a significant amount of variation in their performance on different tasks.
One system may be the most efficient on a particular task; another may be the most efficient on a different task.
Hence, a single task is usually insufficient to quantify the performance of the system.
Instead, the performance of a system is measured by suites of standardized tasks, called performance benchmarks.
Combining the performance numbers frommultiple tasks must be done with care.
Suppose also that a workload has an equal mixture of the two types of transactions.
The example shows that a simple measure of performance is misleading if there is more than one type of transaction.
The right way to average out the numbers is to take the time to completion for the workload, rather than the average throughput for each transaction type.
We can then compute system performance accurately in transactions per second for a specified workload.
Thus, system B is approximately 25 times faster than system A on a workload consisting of an equal mixture of the two example types of transactions.
Online transaction processing (OLTP) and decision support, including online analytical processing (OLAP), are two broad classes of applications handled by database systems.
High concurrency and clever techniques to speed up commit processing are required for supporting a high rate of update transactions.
On the other hand, good queryevaluation algorithms and query optimization are required for decision support.
The architecture of some database systems has been tuned to transaction processing; that of others, such as the Teradata series of parallel database systems, has been tuned to decision support.
Other vendors try to strike a balance between the two tasks.
Hence, which database system is best for an application depends on what mix of the two requirements the application has.
Suppose that we have throughput numbers for the two classes of applications separately, and the application at hand has amix of transactions in the two classes.
We must be careful even about taking the harmonic mean of the throughput numbers, because of interference between the transactions.
For example, a longrunning decision-support transaction may acquire a number of locks, which may prevent all progress of update transactions.
The harmonic mean of throughputs should be used only if the transactions do not interfere with one another.
The Transaction Processing Performance Council (TPC) has defined a series of benchmark standards for database systems.
They define the set of relations and the sizes of the tuples.
They define the number of tuples in the relations not as a fixed number, but rather as a multiple of the number of claimed transactions per second, to reflect that a larger rate of transaction execution is likely to be correlated with a larger number of accounts.
The performance metric is throughput, expressed as transactions per second (TPS)
When its performance is measured, the system must provide a response time within certain bounds, so that a high throughput cannot be obtained at the cost of very long response times.
Hence, the TPC benchmark also measures performance in terms of price per TPS.
A large system may have a high number of transactions per second, but may be expensive (that is, have a high price per TPS)
Moreover, a company cannot claim TPC benchmark numbers for its systems without an external audit that ensures that the system faithfully follows the definition of the benchmark, including full support for the ACID properties of transactions.
This benchmark simulates a typical bank application by a single type of transaction that models cash withdrawal and deposit at a bank teller.
The transaction updates several relations—such as the bank balance, the teller’s balance, and the customer’s balance—and adds a record to an audit trail relation.
The benchmark also incorporates communication with terminals, to model the end-to-end performance of the system realistically.
The TPC-B benchmark was designed to test the core performance of the database system, along with the operating system on which the system runs.
It removes the parts of the TPC-A benchmark that deal with users, communication, and terminals, to focus on the back-end database server.
The TPC-C benchmark was designed to model a more complex system than the TPC-A benchmark.
The TPC-C benchmark concentrates on the main activities in an order-entry environment, such as entering and delivering orders, recording payments, checking status of orders, and monitoring levels of stock.
The TPC-C benchmark is still widely used for benchmarking online transaction processing (OLTP) systems.
The more recent TPC-E benchmark is also aimed at OLTP systems, but is based on amodel of a brokerage firm, with customers who interact with the firm and generate transactions.
The firm in turn interacts with financial markets to execute transactions.
The TPC-D benchmark was designed to test the performance of database systems on decision-support queries.
The TPC-D benchmark schema models a sales/distribution application, with parts, suppliers, customers, and orders, along with some auxiliary information.
The sizes of the relations are defined as a ratio, and database size is the total size of all the relations, expressed in gigabytes.
The benchmark workload consists of a set of 17 SQL queries modeling common tasks executed on decision-support systems.
Some of the queries make use of complex SQL features, such as aggregation and nested queries.
The benchmark’s users soon realized that the various TPC-D queries could be significantly speeded up by using materialized views and other redundant information.
There are applications, such as periodic reporting tasks, where the queries are known in advance and materialized view can be carefully selected.
It is necessary, however, to account for the overhead of maintaining materialized views.
The TPC-H benchmark (where H represents ad hoc) is a refinement of the TPC-D benchmark.
In addition, there are two updates, a set of inserts, and a set of deletes.
TPC-H prohibits materialized views and other redundant information, and permits indices only on primary and foreign keys.
This benchmark models ad hoc queryingwhere the queries are not known beforehand, so it is not possible to create appropriate materialized views ahead of time.
A variant, TPC-R (where R stands for “reporting”), which is no longer in use, allowed the use of materialized views and other redundant information.
TPC-Hmeasures performance in thisway: Thepower test runs the queries and updates one at a time sequentially, and 3600 seconds divided by the geometric mean of the execution times of the queries (in seconds) gives a measure of queries per hour.
The throughput test runs multiple streams in parallel, with each stream executing all 22 queries.
Here the total time for the entire run is used to compute the number of queries per hour.
The composite query per hour metric, which is the overall metric, is then obtained as the square root of the product of the power and throughput metrics.
A composite price/performance metric is defined by dividing the system price by the composite metric.
The TPC-W Web commerce benchmark is an end-to-end benchmark that models Web sites having static content (primarily images) and dynamic content generated from a database.
Caching of dynamic content is specifically permitted, since it is very useful for speeding up Web sites.
The benchmark models an electronic bookstore, and like other TPC benchmarks, provides for different scale factors.
In this section, we discuss two issues in application development: testing of applications, and migration of applications.
Testing of programs involves designing a test suite, that is, a collection of test cases.
Testing is not a one-time process, since programs evolve continuously, and bugs may appear as an unintended consequence of a change in the program; such a bug is referred to as program regression.
Thus, after every change to a program, the program must be tested again.
It is usually infeasible to have a human perform tests after every change to a program.
Instead, expected test outputs are stored with each test case in a test suite.
Regression testing involves running the program on each test case in a test suite, and checking that the program generates the expected test output.
In the context of database applications, a test case consists of two parts: a database state, and an input to a specific interface of the application.
If a test case performs an update on the database, to check that it executed properly one must verify that the contents of the database match the expected contents.
Thus, the expected output consists not only of data displayed on the user’s screen, but also (updates to) the database state.
Since the database state can be rather large, multiple test cases would share a common database state.
Testing is complicated by the fact that if a test case performs an update on the database, the results of other test cases run subsequently on the same database may not match the expected results.
The other test caseswould then be erroneously reported as having failed.
To avoid this problem, whenever a test case performs an update, the database state must be restored to its original state after running the test.
Testing can also be used to ensure that an application meets performance requirements.
To carry out such performance testing, the test database must be of the same size as the real database would be.
In some cases, there is already existing data on which performance testing can be carried out.
In other cases, a test database of the required size must be generated; there are several tools available for generating such test databases.
These tools ensure that the generated data satisfies constraints such as primary and foreign key constraints.
They may additionally generate data that looks meaningful, for example, by populating a name attribute using meaningful names instead of random strings.
Even if there is an existing database, organizations usually do not want to reveal sensitive data to an external organization that may be carrying out the performance tests.
In such a situation, a copy of the real database may be made, and the values in the copy may be modified in such a way that any sensitive data, such as credit-card numbers, social-security numbers, or dates of birth, are obfuscated.
Obfuscation is done in most cases by replacing a real value with a randomly generated value (taking care to also update all references to that value, in case the value is a primary key)
On the other hand, if the application execution depends on the value, such as the date of birth in an application that performs different actions based on the date of birth, obfuscation may make small random changes in the value instead of replacing it completely.
Legacy systems are older-generation application systems that are in use by an organization, but that the organization wishes to replace with a different application.
For example, many organizations developed applications in house, but may decide to replace themwith a commercial product.
In some cases, a legacy system may use old technology that is incompatible with current-generation standards and systems.
Some legacy systems in operation today are several decades old and are based on technologies such as databases that use the network or hierarchical datamodels, or use Cobol and file systemswithout a database.
Such systemsmay still contain valuable data, and may support critical applications.
Replacing legacy applications with new applications is often costly in terms of both time and money, since they are often very large, consisting of millions of lines of code developed by teams of programmers, often over several decades.
They contain large amounts of data that must be ported to the new application, which may use a completely different schema.
Switchover from an old to a new application involves retraining large numbers of staff.
Switchover must usually be done without any disruption, with data entered in the old system available through the new system as well.
Many organizations attempt to avoid replacing legacy systems, and instead try to interoperate them with newer systems.
One approach used to interoperate between relational databases and legacy databases is to build a layer, called a wrapper, on top of the legacy systems that can make the legacy system appear to be a relational database.
The wrapper may provide support for ODBC or other interconnection standards such as OLE-DB,which can be used to query andupdate the legacy system.
The wrapper is responsible for converting relational queries and updates into queries and updates on the legacy system.
When an organization decides to replace a legacy system with a new system, it may follow a process called reverse engineering, which consists of going over the code of the legacy system to come up with schema designs in the required data model (such as an E-R model or an object-oriented data model)
Reverse engineering also examines the code to find out what procedures and processes were implemented, in order to get a high-level model of the system.
Reverse engineering is needed because legacy systems usually do not have high-level documentation of their schema and overall system design.
When coming up with the design of a new system, developers review the design, so that it can be improved rather than just reimplemented as is.
Extensive coding is required to support all the functionality (such as user interface and reporting systems) that was provided by the legacy system.
When a new system has been built and tested, the systemmust be populated with data from the legacy system, and all further activities must be carried out on the new system.However, abruptly transitioning to a new system,which is called the big-bang approach, carries several risks.
First, users may not be familiar with the interfaces of the new system.
Second, there may be bugs or performance problems in the new system that were not discovered when it was tested.
Such problems may lead to great losses for companies, since their ability to carry out.
In some extreme cases the new system has even been abandoned, and the legacy system reused, after an attempted switchover failed.
An alternative approach, called the chicken-little approach, incrementally replaces the functionality of the legacy system.
For example, the new user interfaces may be used with the old system in the back end, or vice versa.
Another option is to use the new system only for some functionality that can be decoupled from the legacy system.
In either case, the legacy and new systems coexist for some time.
There is therefore a need for developing and using wrappers on the legacy system to provide required functionality to interoperate with the new system.
This approach therefore has a higher development cost associated with it.
Today, database systems are complex, and are oftenmade up of multiple independently created parts that need to interact.
For example, client programs may be created independently of back-end systems, but the two must be able to interact with each other.
A company that has multiple heterogeneous database systems may need to exchange data between the databases.
Formal standards are those developed by a standards organization or by industry groups, through a public process.
Dominant products sometimes become de facto standards, in that they become generally accepted as standards without any formal process of recognition.
In other cases, the standards, or parts of the standards, are reactionary standards, in that they attempt to standardize features that some vendors have already implemented, and that may even have become de facto standards.
SQL-89 was in many ways reactionary, since it standardized features, such as integrity checking, that were already present in the IBM SAA SQL standard and in other databases.
Formal standards committeesmeet periodically, and members present proposals for features to be added to or modified in the standard.
After a (usually extended) period of discussion, modifications to the proposal, andpublic review,members vote onwhether to accept or reject a feature.
Some time after a standard has been defined and implemented, its shortcomings become clear and new requirements become apparent.
This cycle usually repeats every few years, until eventually (perhaps many years later) the standard becomes technologically irrelevant, or loses its user base.
The DBTGCODASYL standard for network databases, formulated by the Database Task Group, was one of the early formal standards for databases.
In recent years, Microsoft has created a number of specifications that also have become de facto standards.
A notable example is ODBC, which is now used in non-Microsoft environments.
JDBC, whose specification was created by Sun Microsystems, is another widely used de facto standard.
This section gives a very high-level overview of different standards, concentrating on the goals of the standard.
The bibliographical notes at the end of the chapter provide references to detailed descriptions of the standards mentioned in this section.
Since SQL is the most widely used query language, much work has been done on standardizing it.
The SQL:1999 version of the SQL standard added a variety of features to SQL.
We have seen many of these features in earlier chapters.
Part 4: SQL/PSM (Persistent Stored Modules) defines extensions to SQL to make it procedural.
Part 9: SQL/MED (Management of External Data) defines standards or interfacing an SQL system to external sources.
Part 11: SQL/Schemata (Information and Definition Schema) defines a standard catalog interface.
Part 13: SQL/JRT (Java Routines and Types) defines standards for accessing routines and types in Java.
The missing numbers cover features such as temporal data, distributed transaction processing, and multimedia data, for which there is as yet no agreement on the standards.
The ODBC standard is a widely used standard for communication between client applications and database systems.
The ODBC API defines a CLI, an SQL syntax definition, and rules about permissible sequences of CLI calls.
The standard also defines conformance levels for the CLI and the SQL syntax.
For example, the core level of the CLI has commands to connect to a database, to prepare and execute SQL statements, to get back results or status values, and to manage transactions.
A distributed system provides a more general environment than a clientserver system.
The X/Open consortium has also developed the X/Open XA standards for interoperation of databases.
The XA standards are independent of the data model and of the specific interfaces between clients and databases to exchange data.
There are many data sources that are not relational databases, and in fact may not be databases at all.
Just like ODBC, OLE-DB provides constructs for connecting to a data source, starting a session, executing commands, and getting back results in the form of a rowset, which is a set of result rows.
To support data sources with limited feature support, features in OLE-DB are divided into a number of interfaces, and a data source may implement only a subset of the interfaces.
An OLE-DB program can negotiate with a data source to find what interfaces are supported.
In OLE-DB, commands may be in any language supported by the data source; while some sources may support SQL, or a limited subset of SQL, other sourcesmay provide only simple capabilities such as accessing data in a flat file, without any query capability.
Another major difference of OLE-DB from ODBC is that a rowset is an object that can be shared by multiple applications through shared memory.
A rowset object can be updated by one application, and other applications sharing that object will get notified about the change.
The Active Data Objects (ADO) API, also created by Microsoft, provides an easy-to-use interface to the OLE-DB functionality, which can be called from scripting languages, such as VBScript and JScript.
The newer ADO.NET API is designed for applications written in the .NET languages such as C# and Visual Basic.NET.
In addition toproviding simplified interfaces, it provides anabstraction called the DataSet that permits disconnected data access.
Standards in the area of object-oriented databases have so far been driven primarily by OODB vendors.
The Object Request Broker (ORB) is a component of the OMA architecture that providesmessage dispatch to distributed objects transparently, so the physical location of the object is not important.
A wide variety of standards based on XML (see Chapter 23) have been defined for a wide variety of applications.
They include standards promulgated by nonprofit consortia and corporate-backed efforts to create de facto standards.
Supply-chain management refers to the purchases of material and services that an organization needs to function.
Supply-chain management requires standardization of a variety of things such as:
Global company identifier: RosettaNet specifies a system for uniquely identifying companies, using a 9-digit identifier called Data Universal Numbering System (DUNS)
Global product identifier: RosettaNet specifies a 14-digit Global Trade Item Number (GTIN) for identifying products and services.
Global class identifier: This is a 10-digit hierarchical code for classifying products and services called the United Nations/Standard Product and Services Code (UN/SPSC)
Interfaces between trading partners: RosettaNet Partner Interface Processes (PIPs) define business processes between partners.
PIPs are system-to-system XML-based dialogs: They define the formats and semantics of business documents involved in a process and the steps involved in completing a transaction.
Examples of steps could include getting product and service information, purchase orders, order invoicing, payment, order status requests, inventory management, post-sales support including service warranty, and so on.
Exchange of design, configuration, process, and quality information is also possible to coordinate manufacturing activities across organizations.
These systems may use different data models, data formats, and data types.
Furthermore, there may be semantic differences (metric versus English measure, distinct monetary currencies, and so forth) in the data.
These XML wrappers form the basis of a unified view of data across all of the participants in the marketplace.
Simple Object Access Protocol (SOAP) is a remote procedure call standard that uses XML to encode data (both parameters and results), and uses HTTP as the transport protocol; that is, a procedure call becomes an HTTP request.
Queries can be tuned to improve set-orientation, while bulk-loading utilities can greatly speed up data import into a database.
Tuning is best done by identifying bottlenecks and eliminating them.
Database systemsusually have a variety of tunable parameters, such as buffer sizes, memory size, and number of disks.
The set of indices and materialized views can be appropriately chosen to minimize overall cost.
Transactions can be tuned to minimize lock contention; snapshot isolation, and sequence numbering facilities supporting early lock release areuseful tools for reducing read-write and write-write contention.
Performance benchmarks play an important role in comparisons of database systems, especially as systems become more standards compliant.
The TPC benchmark suites are widely used, and the different TPC benchmarks are useful for comparing the performance of databases under different workloads.
Applications need to be tested extensively as they are developed, and before they are deployed.
Testing is used to catch errors, as well as to ensure that performance goals are met.
Legacy systems are systems based on older-generation technologies such as nonrelational databases or even directly on file systems.
Interfacing legacy systems with new-generation systems is often important when they run mission-critical systems.
Migrating from legacy systems to new-generation systems must be done carefully to avoid disruptions, which can be very expensive.
Standards are important because of the complexity of database systems and their need for interoperation.
De facto standards, such as ODBC and JDBC, and standards adopted by industry groups, such as CORBA, have played an important role in the growth of client–server database systems.
If a sequence counter is locked in two-phase manner, it can become a concurrency bottleneck.
Many database systems support built-in sequence counters that are not locked in two-phase manner; when a transaction requests a sequence number, the counter is locked, incremented and unlocked.
Explainwhy theremay be gaps in the sequence numbers belonging to the final set of committed transactions.
Give an example of a situation under which the performance of equality selection queries on attribute a can be greatly affected by how r is clustered.
Suppose you also had range selection queries on attribute b.
Can you cluster r in such a way that the equality selection queries on r.a and the range selection queries on r.b can both be answered efficiently? Explain your answer.
If clustering as above is not possible, suggest how both types of queries can be executed efficiently by choosing appropriate indices, assuming your database supports index-only plans (that is, if all information required for a query is available in an index, the database can generate a plan that uses the index but does not access the relation)
Does that mean the application cannot be tuned further? Explain your answer.
What is the average transaction throughput of the system, assuming there is no interference between the transactions?
What factors may result in interference between the transactions of different types, leading to the calculated throughput being incorrect?
You may also be able to get information about CPU and I/O utilization from the operating system.
What are the three broad levels at which a database system can be tuned to improve performance?
Give two examples of how tuning can be done for each of the levels.
Assume that all internal nodes of the B+-tree are inmemory, but only a very small fraction of the leaf pages can fit in memory.
Explain how to calculate the minimum number of disks required to support a workload of 1000 transactions per second.
Also calculate the required number of disks, using values for disk parameters given in Section 10.2
An early proposal for a database-system benchmark (the Wisconsin benchmark) was made by Bitton et al.
The TPC-A, -B, and -C benchmarks are described in Gray [1991]
An online version of all the TPC benchmark descriptions, as well as benchmark results, is available on theWorldWideWeb at the URL.
Shasha and Bonnet [2002] provides detailed coverage of database tuning.
O’Neil and O’Neil [2000] provides a very good textbook coverage of performance measurement and tuning.
Index selection and materialized view selection are addressed by Ross et al.
Awealth of information on XML-based standards and tools is available online on the Web site www.w3c.org.
Umar [1997] covers re-engineering and issues in dealing with legacy systems.
For most of the history of databases, the types of data stored in databases were relatively simple, and this was reflected in the rather limited support for data types in earlier versions of SQL.
Over time, however, there developed increasing need for handling more complex data types in databases, such as temporal data, spatial data, and multimedia data.
Another major trend has created its own set of issues: the growth of mobile computers, starting with laptop computers and pocket organizers and extending in more recent years to mobile phones with built-in computers and a variety of wearable computers that are increasingly used in commercial applications.
In this chapter, we study several data types and other database issues dealing with these applications.
Before we address each of the topics in detail, we summarize the motivation for, and some important issues in dealing with, each of these types of data.
Most database systems model the current state of the world, for instance, current customers, current students, and courses currently being offered.
In many applications, it is very important to store and retrieve information about past states.
Historical information can be incorporated manually into a schema design.
However, the task is greatly simplified by database support for temporal data, which we study in Section 25.2
Applications of spatial data initially stored data as files in a file system, as did early-generation business applications.
But as the complexity and volume of the data, and the number of users, have grown, ad hoc approaches to storing and retrieving data in a file.
Spatial-data applications require facilities offered by a database systemin particular, the ability to store and query large amounts of data efficiently.
Some applications may also require other database features, such as atomic updates to parts of the stored data, durability, and concurrency control.
In Section 25.3, we study the extensions needed in traditional database systems to support spatial data.
In Section 25.4, we study the features required in database systems that storemultimedia data such as image, video, and audio data.
The main distinguishing feature of video and audio data is that the display of the data requires retrieval at a steady, predetermined rate; hence, such data are called continuous-media data.
In Section 25.5,we study the database requirements ofmobile computing systems, such as laptop and netbook computers and high-end cell phones that are connected to base stations via wireless digital communication networks.
They also have limited storage capacity, and thus require special techniques for memory management.
A database models the state of some aspect of the real world outside itself.
Typically, databases model only one state—the current state—of the real world, and do not store information about past states, except perhaps as audit trails.
When the state of the real world changes, the database gets updated, and information about the old state gets lost.
However, in many applications, it is important to store and retrieve information about past states.
For example, a patient database must store information about the medical history of a patient.
A factory monitoring system may store information about current and past readings of sensors in the factory, for analysis.
Databases that store information about states of the real world across time are called temporal databases.
When considering the issue of time in database systems, we must distinguish between time as measured by the system and time as observed in the real world.
The valid time for a fact is the set of time intervals during which the fact is true in the real world.
The transaction time for a fact is the time interval during which the fact is current within the database system.
This latter time is based on the transaction serialization order and is generated automatically by the system.
Note that valid-time intervals, being a real-world concept, cannot be generated automatically and must be provided to the system.
A temporal relation is one where each tuple has an associated time when it is true; the time may be either valid time or transaction time.
Of course, both valid time and transaction time can be stored, in which case the relation is said.
To simplify the representation, each tuple has only one time interval associated with it; thus, a tuple is represented once for every disjoint time interval in which it is true.
Intervals are shown here as a pair of attributes from and to; an actual implementation would have a structured type, perhaps called Interval, that contains both fields.
Note that some of the tuples have a “*” in the to time column; these asterisks indicate that the tuple is true until the value in the to time column is changed; thus, the tuple is true at the current time.
The type time contains two digits for the hour, two digits for the minute, and two digits for the second, plus optional fractional digits.
The seconds field can go beyond 60, to allow for leap seconds that are added during some years to correct for small variations in the speed of rotation of Earth.
The type timestamp contains the fields of date and time, with six fractional digits for the seconds field.
This notion differs from the notion of interval we used.
Adatabase relationwithout temporal information is sometimes called a snapshot relation, since it reflects the state in a snapshot of the real world.
Thus, a snapshot of a temporal relation at a point in time t is the set of tuples in the relation that are true at time t, with the time-interval attributes projected out.
The snapshot operation on a temporal relation gives the snapshot of the relation at a specified time (or the current time, if the time is not specified)
A temporal selection is a selection that involves the time attributes; a temporal projection is a projection where the tuples in the projection inherit their times from the tuples in the original relation.
A temporal join is a join, with the time of a tuple in the result being the intersection of the times of the tuples from which it is derived.
If the times do not intersect, the tuple is removed from the result.
The predicates precedes, overlaps, and contains can be applied on intervals; their meanings should be clear.
The intersect operation can be applied on two intervals, to give a single (possibly empty) interval.
However, the union of two intervals may or may not be a single interval.
Functional dependencies must be used with care in a temporal relation, as we saw in Section 8.9
Although the instructor IDmay functionally determine the salary at any given point in time, obviously the salary can change over time.
Several proposals have been made for extending SQL to improve its support of temporal data, but at least until SQL:2008, SQL has not provided any special support for temporal data beyond the time-related data types and operations.
Spatial data support in databases is important for efficiently storing, indexing, and querying of data on the basis of spatial locations.
For example, suppose that we want to store a set of polygons in a database and to query the database to find all polygons that intersect a given polygon.
We cannot use standard index structures, such as B-trees or hash indices, to answer such a query efficiently.
Efficient processing of the above query would require special-purpose index structures, such as R-trees (which we study later) for the task.
Other im1Many temporal database researchers feel this type should have been called span since it does not specify an exact start or end time, only the time span between the two.
Geographic data such as roadmaps, land-usagemaps, topographic elevation maps, political maps showing boundaries, land-ownership maps, and so on.
Geographic information systems are special-purpose databases tailored for storing geographic data.
Support for geographic data has been added to many database systems, such as the IBM DB2 Spatial Extender, the Informix Spatial Datablade, and Oracle Spatial.
Figure 25.2 illustrates how various geometric constructs can be represented in a database, in a normalized fashion.
We stress here that geometric information can be represented in several different ways, only some of which we describe.
A line segment can be represented by the coordinates of its endpoints.
For example, in amap database, the two coordinates of a point would be its latitude and longitude.
A polyline (also called a linestring) consists of a connected sequence of line segments and can be represented by a list containing the coordinates of the endpoints of the segments, in sequence.
We can approximately represent an arbitrary curve by polylines, by partitioning the curve into a sequence of segments.
This representation is useful for two-dimensional features such as roads; here, the width of the road is small enough relative to the size of the full map that it can be considered to be a line.
Some systems also support circular arcs as primitives, allowing curves to be represented as sequences of arcs.
We can represent a polygon by listing its vertices in order, as in Figure 25.2.2 The list of vertices specifies the boundary of a polygonal region.
In an alternative representation, a polygon can be divided into a set of triangles, as shown in Figure 25.2
This process is called triangulation, and any polygon can be triangulated.
The complex polygon can be given an identifier, and each of the triangles into which it is divided carries the identifier of the polygon.
Circles and ellipses can be represented by corresponding types, or can be approximated by polygons.
List-based representations of polylines or polygons are often convenient for query processing.
So that we can use fixed-size tuples (in first normal form) for representing polylines, we can give the polyline or curve an identifier, and can represent each segment as a separate tuple that also carrieswith it the identifier of the polyline or curve.
Similarly, the triangulated representation of polygons allows a first normal form relational representation of polygons.
The representation of points and line segments in three-dimensional space is similar to their representation in two-dimensional space, the only difference being that points have an extra z component.
Similarly, the representation of planar figures—such as triangles, rectangles, and other polygons—does not change.
Some references use the term closed polygon to refer to what we call polygons, and refer to polylines as open polygons.
Tetrahedrons and cuboids can be represented in the same way as triangles and rectangles.
We can represent arbitrary polyhedra by dividing them into tetrahedrons, just as we triangulate polygons.
We can also represent them by listing their faces, each of which is itself a polygon, along with an indication of which side of the face is inside the polyhedron.
The drawbacks of such a scheme include the cost (programming complexity, as well as time cost) of transforming data from one form to another, and the need to read in an entire file even if only parts of it are required.
For large designs, such as the design of a large-scale integrated circuit or the design of an entire airplane, it may be impossible to hold the complete design in.
Designers of object-oriented databases were motivated in large part by the database requirements of CAD systems.
Object-oriented databases represent components of the design as objects, and the connections between the objects indicate how the design is structured.
The objects stored in a design database are generally geometric objects.
Simple two-dimensional geometric objects include points, lines, triangles, rectangles, and, in general, polygons.
Complex two-dimensional objects can be formed from simple objects by means of union, intersection, and difference operations.
Similarly, complex three-dimensional objectsmaybe formed fromsimpler objects such as spheres, cylinders, and cuboids, by union, intersection, and difference operations, as in Figure 25.3
Three-dimensional surfaces may also be represented by wireframemodels, which essentiallymodel the surface as a set of simpler objects, such as line segments, triangles, and rectangles.
Design databases also store nonspatial information about objects, such as the material from which the objects are constructed.
We can usually model such information by standard data-modeling techniques.
For instance, the designer may want to retrieve that part of the design that corresponds to a particular region of interest.
Spatial-index structures, discussed in Section 25.3.5, are useful for such tasks.
Spatial-index structures are multidimensional, dealing with two- and three-dimensional data, rather than dealing with just the simple one-dimensional ordering provided by the B+-trees.
Spatial-integrity constraints, such as “two pipes should not be in the same location,” are important in design databases to prevent interference errors.
Such errors often occur if the design is performedmanually, and are detected onlywhen a prototype is being constructed.
As a result, these errors can be expensive to fix.
Database support for spatial-integrity constraints helps people to avoid design.
Implementing such integrity checks again depends on the availability of efficient multidimensional index structures.
Geographic data are spatial in nature, but differ from design data in certain ways.
Maps and satellite images are typical examples of geographic data.
Maps may provide not only location information—about boundaries, rivers, and roads, for example—but also much more detailed information associated with locations, such as elevation, soil type, land usage, and annual rainfall.
Web-based road map services form a very widely used application of map data.
At the simplest level, these systems can be used to generate online road maps of a desired region.
An important benefit of online maps is that it is easy to scale the maps to the desired size—that is, to zoom in and out to locate relevant features.
Users can query online information about services to locate, for example, hotels, gas stations, or restaurants with desired offerings and price ranges.
In recent years, several Web-based map services have defined APIs that allow programmers to create customized maps that include data from the map service along with data from other sources.
Such customized maps can be used to display, for example, houses available for sale or rent, or shops and restaurants, in a particular area.
Vehicle-navigation systems are systems that are mounted in automobiles and provide roadmaps and trip-planning services.
They include aGlobal Positioning System (GPS) unit, which uses information broadcast from GPS satellites to find the current location with an accuracy of tens of meters.
With such a system, a driver can never3 get lost—the GPS unit finds the location in terms of latitude, longitude, and elevation and the navigation system can query the geographic database to find where and on which road the vehicle is currently located.
Geographic databases for public-utility information have become very important as the network of buried cables and pipes has grown.
Without detailed maps, work carried out by one utility may damage the cables of another utility, resulting in large-scale disruption of service.
Geographic databases, coupledwith accurate location-finding systems, help avoid such problems.
Such data consist of bit maps or pixel maps, in two or more dimensions.A typical example of a two-dimensional raster image is a satellite image of an area.
In addition to the actual image, the data includes the location of the image, specified for example by the latitude and longitudeof its corners, and the resolution, specified either by the total number of pixels, or, more commonly in the context of geographic data, by the area covered by each pixel.
Raster data is often represented as tiles, each covering a fixed sized area.
A larger area can be displayed by displaying all the tiles that overlap with the area.
To allow the display of data at different zoom levels, a separate set of tiles is created for each zoom level.
Once the zoom level is set by the user interface (for example aWeb browser), tiles at that zoom level, which overlap the area being displayed, are retrieved and displayed.
Time could form another dimension—for example, the surface temperature measurements at different points in time.
Vector data are constructed from basic geometric objects, such as points, line segments, polylines, triangles, and other polygons in two dimensions, and cylinders, spheres, cuboids, and other polyhedrons in three dimensions.
In the context of geographic data, points are usually represented by latitude and longitude, and where the height is relevant, additionally by elevation.
Geographic features, such as large lakes, or even political features such as states and countries, are represented as complex polygons.
Some features, such as rivers, may be represented either as complex curves or as complex polygons, depending on whether their width is relevant.
Geographic information related to regions, such as annual rainfall, can be represented as an array—that is, in raster form.
For space efficiency, the array can be stored in a compressed form.
In Section 25.3.5.2, we study an alternative representation of such arrays by a data structure called a quadtree.
The vector representation is more compact than the raster representation in some applications.
It is also more accurate for some tasks, such as depicting roads, where dividing the region into pixels (which may be fairly large) leads to a loss of precision in location information.
However, the vector representation is unsuitable for applications where the data are intrinsically raster based, such as satellite images.
Topographical information, that is information about the elevation (height) of each point on a surface, can be represented in raster form.
Alternatively it can be represented in vector form by dividing the surface into polygons covering regions of (approximately) equal elevation, with a single elevation value associated with each polygon.
As another alternative, the surface can be triangulated (that is, divided into triangles), with each triangle represented by the latitude, longitude, and elevation of each of its corners.
The latter representation, called the triangulated irregular network (TIN) representation, is a compact representation which is particularly useful for generating three-dimensional views of an area.
Geographic information systems usually contain both raster and vector data, and can merge the two kinds of data when displaying results to users.
For example, maps applications usually contain both satellite images and vector data about roads, building and other landmarks.
A map display usually overlays different kinds of information; for example, road information can be overlaid on a background satellite image, to create a hybrid display.
In fact, a map typically consists of multiple layers, which are displayed in bottom-to-top order; data from higher layers appears on top of data from lower layers.
It is also interesting to note that even information that is actually stored in vector form may be converted to raster form before it is sent to a user interface such as a Web browser.
One reason is that even Web browsers that do not support scripting languages (required to interpret and display vector data) can then display map data; a second reason may be to prevent end users from extracting and using the vector data.
Map services such as Google Maps and Yahoo! Maps provide APIs that allow users to create specialized map displays, containing application specific data overlaid on top of standardmap data.
For example, aWeb sitemay show amap of an areawith information about restaurants overlaid on themap.
The overlays can be constructed dynamically, displaying only restaurants with a specific cuisine, for example, or allowing users to change the zoom level, or pan the display.
The maps APIs for a specific language (typically JavaScript or Flash) are built on top of a Web service that provides the underlying map data.
There are a number of types of queries that involve spatial locations.
Nearness queries request objects that lie near a specified location.
A query to find all restaurants that lie within a given distance of a given point is an example of a nearness query.
The nearest-neighbor query requests the object that is nearest to a specified point.
For example, we may want to find the nearest gasoline station.
Note that this query does not have to specify a limit on the distance, and hence we can ask it even if we have no idea how far the nearest gasoline station lies.
Such a query can ask for objects that lie partially or fully inside a specified region.
A query to find all retail shops within the geographic boundaries of a given town is an example.
For example, given region information, such as annual rainfall and population density, a query may request all regions with a low annual rainfall as well as a high population density.
In general, given two relations, each containing spatial objects, the spatial join of the two relations generates either pairs of objects that intersect, or the intersection regions of such pairs.
Several join algorithms efficiently compute spatial joins on vector data.
Although nested-loop join and indexed nested-loop join (with spatial indices) can be used, hash joins and sort–merge joins cannot be used on spatial data.
Researchers have proposed join techniques based on coordinated traversal of spatial index structures on the two relations.
In general, queries on spatial data may have a combination of spatial and nonspatial requirements.
For instance, wemaywant to find the nearest restaurant that has vegetarian selections and that charges less than $10 for a meal.
Since spatial data are inherently graphical, we usually query them by using a graphical query language.
Results of such queries are also displayed graphically, rather than in tables.
The user can invoke various operations on the interface, such as choosing an area to be viewed (for example, by pointing and clicking on suburbs west of Manhattan), zooming in and out, choosing what to display on the basis of selection conditions (for example, houses with more than three bedrooms), overlay of multiple maps (for example, houses with more than three bedrooms overlaid on a map showing areas with low crime rates), and so on.
Extensions of SQL have been proposed to permit relational databases to store and retrieve spatial information efficiently, and also to allow queries to mix spatial and nonspatial conditions.
Extensions include allowing abstract data types, such as lines, polygons, and bit maps, and allowing spatial conditions, such as contains or overlaps.
Traditional index structures, such as hash indices and B-trees, are not suitable, since they deal only with one-dimensional data, whereas spatial data are typically of two or more dimensions.
To understand how to index spatial data consisting of two or more dimensions, we consider first the indexing of points in one-dimensional data.
Tree structures, such as binary trees and B-trees, operate by successively dividing space into smaller parts.
Points that lie in the left partition go into the left subtree; points that lie in the right partition go into the right subtree.
In a balanced binary tree, the partition is chosen so that approximately one-half of the points stored in the subtree fall in each partition.
Similarly, each level of a B-tree splits a one-dimensional interval into multiple parts.
We can use that intuition to create tree structures for two-dimensional space, as well as in higher-dimensional spaces.
Each level of ak-d tree partitions the space into two.
The partitioning is done along one dimension at the node at the top level of the tree, along another dimension in nodes at the next level, and so on, cycling through the dimensions.
The partitioning proceeds in such a way that, at each node, approximately one-half of the points stored in the subtree fall on one side and one-half fall on the other.
Partitioning stops when a node has less than a given maximum number of points.
Figure 25.4 shows a set of points in two-dimensional space, and a k-d tree representation of the set of points.
Each line in the figure (other than the outside box) corresponds to a node in the k-d tree.
The numbering of the lines in the figure indicates the level of the tree at which the corresponding node appears.
The k-d-B tree extends the k-d tree to allow multiple child nodes for each internal node, just as a B-tree extends a binary tree, to reduce the height of the tree.
An example of the division of space by a quadtree appears in Figure 25.5
Each node of a quadtree is associated with a rectangular region of space.
Leaf nodes have between zero and some fixedmaximum number of points.
Correspondingly, if the region corresponding to a node has more than the maximum number of points, child nodes are created for that node.
This type of quadtree is called a PR quadtree, to indicate it stores points, and that the division of space is divided based on regions, rather than on the actual set of points stored.
We can use region quadtrees to store array (raster) information.
A node in a region quadtree is a leaf node if all the array values in the region that it covers are the same.
Otherwise, it is subdivided further into four children of equal area, and is therefore an internal node.
Each node in the region quadtree corresponds to a subarray of values.
The subarrays corresponding to leaves either contain just a single array element or have multiple array elements, all of which have the same value.
There are extensions of k-d trees and quadtrees for this task.
However, a line segment or polygon may cross a partitioning line.
If it does, it has to be split and represented in each of the subtrees in which its pieces occur.
Multiple occurrences of a line segment or polygon can result in inefficiencies in storage, as well as inefficiencies in querying.
A storage structure called an R-tree is useful for indexing of objects such as points, line segments, rectangles, and other polygons.
However, instead of a range of values, a rectangular bounding box is associated with each tree node.
The bounding box of a leaf node is the smallest rectangle parallel to the axes that contains all objects stored in the leaf node.
The bounding box of internal nodes is, similarly, the smallest rectangle parallel to the axes that contains the bounding boxes of its child nodes.
The bounding box of an object (such as a polygon) is defined, similarly, as the smallest rectangle parallel to the axes that contains the object.
Each internal node stores the bounding boxes of the child nodes along with the pointers to the child nodes.
Each leaf node stores the indexed objects, and may optionally store the bounding boxes of the objects; the bounding boxes help speed up checks for overlaps of the rectanglewith the indexed objects—if a query rectangle does not overlap with the bounding box of an object, it cannot overlap with the object, either.
If the indexed objects are rectangles, there is of course no need to store bounding boxes, since they are identical to the rectangles.
Figure 25.6 shows an example of a set of rectangles (drawn with a solid line) and the bounding boxes (drawn with a dashed line) of the nodes of an R-tree for the set of rectangles.
Note that the bounding boxes are shown with extra space inside them, to make them stand out pictorially.
In reality, the boxes would be smaller and fit tightly on the objects that they contain; that is, each side of a bounding box B would touch at least one of the objects or bounding boxes that are contained in B.
The R-tree itself is at the right side of Figure 25.6
The figure refers to the coordinates of bounding box i as BBi in the figure.
We shall now see how to implement search, insert, and delete operations on an R-tree.
As the figure shows, the bounding boxes associated with sibling nodes may overlap; in B+-trees, k-d trees, and quadtrees, in contrast, the ranges do not overlap.
A search for objects containing a point therefore has to follow all child nodes whose associated bounding boxes contain the point; as a result, multiple paths may have to be searched.
Similarly, a query to find all objects that intersect a given object has to go down every node where the associated rectangle intersects the given object.
When we insert an object into an R-tree, we select a leaf node to hold the object.
Ideally we should pick a leaf node that has space to hold a new entry, and whose bounding box contains the bounding box of the object.
However, such a node may not exist; even if it did, finding the node may be very expensive, since it is not possible to find it by a single traversal down from the root.
At each internal node we may find multiple children whose bounding boxes contain the bounding box of the object, and each of these children needs to be explored.
Therefore, as a heuristic, in a traversal from the root, if any of the child nodes has a bounding box containing the bounding box of the object, the R-tree algorithm chooses one of them arbitrarily.
If none of the children satisfy this condition, the algorithm chooses a child node whose bounding box has the maximum overlap with the bounding box of the object for continuing the traversal.
Once the leaf nodehas been reached, if thenode is already full, the algorithm.
Just as with B+-tree insertion, the Rtree insertion algorithm ensures that the tree remains balanced.
Additionally, it ensures that the bounding boxes of leaf nodes, as well as internal nodes, remain consistent; that is, bounding boxes of leaves contain all the bounding boxes of the objects stored at the leaf, while the bounding boxes for internal nodes contain all the bounding boxes of the children nodes.
The main difference of the insertion procedure from the B+-tree insertion procedure lies in how the node is split.
In a B+-tree, it is possible to find a value such that half the entries are less than the midpoint and half are greater than the value.
This property does not generalize beyond one dimension; that is, for more than one dimension, it is not always possible to split the entries into two sets so that their bounding boxes do not overlap.
The cost of finding splits with minimum total area or overlap can itself be large, so cheaper heuristics, such as the quadratic split heuristic, are used.
The heuristic gets is name from the fact that it takes time quadratic in the number of entries.
The quadratic split heuristic works this way: First, it picks a pair of entries a and b from S such that putting them in the same node would result in a bounding box with the maximum wasted space; that is, the area of the.
The heuristic then adds all unassigned entries to the set with fewer entries.
Deletion can be performed like a B+-tree deletion, borrowing entries from sibling nodes, or merging sibling nodes if a node becomes underfull.
An alternative approach redistributes all the entries of underfull nodes to sibling nodes, with the aim of improving the clustering of entries in the R-tree.
The storage efficiency of R-trees is better than that of k-d trees or quadtrees, since an object is stored only once, and we can ensure easily that each node is at least half full.
However, querying may be slower, since multiple paths have to be searched.
Spatial joins are simpler with quadtrees than with R-trees, since all quadtrees on a region are partitioned in the same manner.
However, because of their better storage efficiency, and their similarity to B-trees, R-trees and their variants have proved popular in database systems that support spatial data.
Multimedia data, such as images, audio, and video—an increasingly popular form of data—are today almost always stored outside the database, in file systems.
This kind of storage is not a problem when the number of multimedia objects is relatively small, since features provided by databases are usually not important.
However, database features become important when the number of multimedia objects stored is large.
Issues such as transactional updates, querying facilities, and indexing then become important.
Multimedia objects often have descriptive attributes, such as those indicating when they were created, who created them, and to what category they belong.
One approach to building a database for such multimedia objects is to use databases for storing the descriptive attributes and for keeping track of the files in which the multimedia objects are stored.
However, storingmultimedia outside the databasemakes it harder to provide database functionality, such as indexing on the basis of actual multimedia data content.
It can also lead to inconsistencies, such as a file that is noted in the database, but whose contents are missing, or vice versa.
It is therefore desirable to store the data themselves in the database.
Several issues must be addressed if multimedia data are to be stored in a database.
The database must support large objects, since multimedia data such as videos can occupy up to a few gigabytes of storage.
Many database systems do not support objects larger than a few gigabytes.
Larger objects could be split into smaller pieces and stored in the database.
Alternatively, the multimedia object may be stored in a file system, but the database may contain a pointer to the object; the pointer would typically be a file name.
The SQL/MED standard (MED stands for Management of External Data) allows external data, such as files, to be treated as if they are part of the database.
With SQL/MED, the object would appear to be part of the database, but can be stored externally.
The retrieval of some types of data, such as audio and video, has the requirement that data delivery must proceed at a guaranteed steady rate.
Such data are sometimes called isochronous data, or continuous-media data.
For example, if audio data are not supplied in time, there will be gaps in the sound.
If the data are supplied too fast, system buffers may overflow, resulting in loss of data.
For example, in a database that stores fingerprint images, a query fingerprint image is provided, and fingerprints in the database that are similar to the query fingerprint must be retrieved.
Index structures such as B+trees and R-trees cannot be used for this purpose; special index structures need to be created.
Because of the large number of bytes required to represent multimedia data, it is essential that multimedia data be stored and transmitted in compressed form.
For image data, the most widely used format is JPEG, named after the standards body that created it, the Joint Picture Experts Group.
We can store video data by encoding each frame of video in JPEG format, but such an encoding is wasteful, since successive frames of a video are often nearly the same.
The Moving Picture Experts Group has developed theMPEG series of standards for encoding video and audio data; these encodings exploit commonalities among a sequence of frames to achieve a greater degree of compression.
However, MPEG-1 encoding introduces some loss of video quality, to a level roughly comparable.
Themost important types of continuous-media data are video and audio data (for example, a database of movies)
Data must be delivered sufficiently fast that no gaps in the audio or video result.
Data must be delivered at a rate that does not cause overflow of system buffers.
This need arises, for example, when the video of a person speaking must show lips moving synchronously with the audio of the person speaking.
Extensive research on delivery of continuous-media data has dealt with such issues as handling arrays of disks and dealing with disk failure.
Current systems are based on file systems, because existing database systems do not provide the real-time response that these applications need.
Multimedia data are stored on several disks (usually in a RAID configuration)
Systems containing a large volume of data may use tertiary storage for less frequently accessed data.
People view multimedia data through various devices, collectively referred to as terminals.
Examples are personal computers and televisions attached to a small, inexpensive computer called a set-top box.
Transmission of multimedia data from a server to multiple terminals requires a high-capacity network.
In many multimedia applications, data are described only approximately in the database.
Pictorial data.Twopictures or images that are slightlydifferent as represented in the databasemay be considered the same by a user.
When a new trademark is to be registered, the system may need first to identify all similar trademarks that were registered previously.
Speech-based user interfaces are being developed that allow the user to give a command or identify a data item by speaking.
The input from the user must then be tested for similarity to those commands or data items stored in the system.
Handwritten data.Handwritten input can be used to identify a handwritten data item or command stored in the database.
The notion of similarity is often subjective and user specific.
However, similarity testing is often more successful than speech or handwriting recognition, because the input can be compared to data already in the system and, thus, the set of choices available to the system is limited.
Several algorithms exist for finding the best matches to a given input by similarity testing.
Many voice-activated systems have been deployed commercially, particularly for phone applications and in-vehicle controls.
Large-scale, commercial databases have traditionally been stored in central computing facilities.
In distributed database applications, there has usually been strong central database and network administration.
Several technology trends have combined to create applications in which this assumption of central control and administration is not entirely correct:
The widespread use of cell phones with the capabilities of a computer.
The development of a relatively low-cost wireless digital communication infrastructure, based on wireless local-area networks, cellular digital packet networks, and other technologies.
Many business travelers use laptop computers so that they can work and access data en route.
Emergencyresponse services use mobile computers at the scene of disasters, medical emergencies, and the like to access information and to enter data pertaining to the situation.
Cell phones are increasingly becoming devices that provide not only phone services, but are also mobile computers allowing email and Web access.
Wireless computing creates a situation where machines no longer have fixed locations and network addresses.
Location-dependent queries are an interesting class of queries that are motivated by mobile computers; in such queries, the location of the user (computer) is a parameter of the query.
The value of the location parameter is provided by a global positioning system (GPS)
An example is a traveler’s information system that provides data on hotels, roadside services, and the like to motorists.
Processing of queries about services that are ahead on the current route must be based on knowledge of the user’s location, direction of motion, and speed.
Increasingly, navigational aids are being offered as a built-in feature in automobiles.
Energy (battery power) is a scarce resource for most mobile computers.
Among themore interesting consequences of the need for energy efficiency is that small mobile devices spend most of their time sleeping, waking up for a fraction of a second every second or so to check for incoming data and to send outgoing data.
Theuse of scheduled data broadcasts to reduce the need for mobile systems to transmit queries is another way to reduce energy requirements.
Increasing amounts of data may reside on machines administered by users, rather than by database administrators.
Inmany cases, there is a conflict between the user’s need to continue to work while disconnected and the need for global data consistency.
A user is likely to use more than one mobile device.
Such users need to be able to view their data in its most up-to-date version regardless of which device is being used at a given time.
Often, this capability is supported by some variant of cloud computing, which we discussed in Section 19.9
The mobile-computing environment consists of mobile computers, referred to as mobile hosts, and a wired network of computers.
Each mobile support station manages those mobile hosts within its cell—that is, the geographical area that it covers.
Mobile hosts may move between cells, thus necessitating a handoff of control from one mobile support station to another.
Sincemobile hosts may, at times, be powered down, a host may leave one cell and rematerialize later at some distant cell.
Therefore, moves between cells are not necessarily between adjacent cells.
It is possible for mobile hosts to communicate directly without the intervention of a mobile support station.
Initially conceived as a replacement for cables, Bluetooth’s greatest benefit is in easy ad hoc connection of mobile computers, PDAs, mobile phones, and so-called intelligent appliances.
The network infrastructure formobile computing consists in large part of two technologies: wireless local-area networks and packet-based cellular telephony networks.
Early cellular systems used analog technology and were designed for voice communication.
In these networks, voice is just one of many applications (albeit an economically important one)
Fourth-generation (4G) technologies include Wi-Max as well as several competitors.
While such communication itself does not fit the domain of a usual database application, the accounting, monitoring, and management data pertaining to this communication generate huge databases.
The immediacy of wireless communication generates a need for real-time access tomany of these databases.
This need for timeliness adds another dimension to the constraints on the system—a matter we shall discuss further in Section 26.4
The size and power limitations of many mobile computers have led to alternative memory hierarchies.
Instead of, or in addition to, disk storage, flash memory, which we discussed in Section 10.1, may be included.
If the mobile host includes a hard disk, the disk may be allowed to spin down when it is not in use, to save energy.
The same considerations of size and energy limit the type and size of the display used in a mobile device.
Designers of mobile devices often create special-purpose user interfaces to work within these constraints.
However, the need to presentWeb-based data has necessitated the creation of presentation standards.
Wireless application protocol (WAP) is a standard for wireless Internet.
WAP-based browsers access special Web pages that use wireless markup language (WML), an XML-based language designed for the constraints of mobile and wireless Web browsing.
The route between a pair of hosts may change over time if one of the two hosts is mobile.
This simple fact has a dramatic effect at the network level, since locationbased network addresses are no longer constants within the system.
As we saw in Chapter 19, we must consider the communication costs when we choose a distributed query-processing strategy.
Mobility results in dynamically changing communication costs, thus complicating the optimization process.
User time is a highly valuable commodity in many business applications.
Connection time is the unit by whichmonetary charges are assigned in some.
Number of bytes, or packets, transferred is the unit by which charges are computed in some digital cellular systems.
Time-of-day-based charges vary, depending on whether communication occurs during peak or off-peak periods.
Often, battery power is a scarce resource whose use must be optimized.
A basic principle of radio communication is that it requires less energy to receive than to transmit radio signals.
Thus, transmission and reception of data impose different power demands on the mobile host.
It is often desirable for frequently requested data to be broadcast in a continuous cycle by mobile support stations, rather than transmitted to mobile hosts on demand.
A typical application of such broadcast data is stock-market price information.
First, the mobile host avoids the energy cost for transmitting data requests.
Second, the broadcast data can be received by a large number of mobile hosts at once, at no extra cost.
A mobile host can then receive data as they are transmitted, rather than consuming energy by transmitting a request.
The mobile host may have local nonvolatile storage available to cache the broadcast data for possible later use.
If the cached data are insufficient, there are two options: wait for the data to be broadcast, or transmit a request for data.
To make this decision, the mobile host must know when the relevant data will be broadcast.
Broadcast data may be transmitted according to a fixed schedule or a changeable schedule.
In the former case, the mobile host uses the known fixed schedule to determine when the relevant data will be transmitted.
In the latter case, the broadcast schedule must itself be broadcast at a well-known radio frequency and at well-known time intervals.
In effect, the broadcast medium can be modeled as a disk with a high latency.
Requests for data can be thought of as being serviced when the requested data are broadcast.
The bibliographical notes list recent research papers in the area of broadcast data management.
Since wireless communication may be paid for on the basis of connection time, there is an incentive for certain mobile hosts to be disconnected for substantial periods.
Mobile computers without wireless connectivity are disconnected most of the timewhen they are beingused, except periodicallywhen they are connected to their host computers, either physically or through a computer network.
During these periods of disconnection, the mobile host may remain in operation.
The user of the mobile host may issue queries and updates on data that reside or are cached locally.
Recoverability: Updates entered on a disconnected machine may be lost if the mobile host experiences a catastrophic failure.
Since the mobile host represents a single point of failure, stable storage cannot be simulated well.
Consistency: Locally cached data may become out-of-date, but the mobile host cannot discover this situation until it is reconnected.
Likewise, updates occurring in the mobile host cannot be propagated until reconnection occurs.
We explored the consistency problem in Chapter 19, where we discussed network partitioning, and we elaborate on it here.
In wired distributed systems, partitioning is considered to be a failure mode; in mobile computing, partitioning via disconnection is part of the normalmode of operation.
It is therefore necessary to allow data access to proceed despite partitioning, even at the risk of some loss of consistency.
For data updated by only the mobile host, it is a simple matter to propagate the updates when the mobile host reconnects.
However, if the mobile host caches read-only copies of data that may be updated by other computers, the cached data may become inconsistent.
When the mobile host is connected, it can be sent invalidation reports that inform it of out-of-date cache entries.
However, when the mobile host is disconnected, it may miss an invalidation report.
A simple solution to this problem is to invalidate the entire cache on reconnection, but such an extreme solution is highly costly.
If updates can occur at both themobile host and elsewhere, detecting conflicting updates is more difficult.
These schemes do not guarantee that the updates will be consistent.
Rather, they guarantee that, if two hosts independently update the same version of a document, the clash will be detected eventually, when the hosts exchange information either directly or through a common host.
The version-vector scheme detects inconsistencies when copies of a document are independently updated.
This scheme allows copies of a document to be stored at multiple hosts.
Although we use the term document, the scheme can be applied to any other data items, such as tuples of a relation.
Whenever two hosts i and j connect with each other, they exchange updated documents, so that both obtain new versions of the documents.
However, before exchanging documents, the hosts have to discover whether the copies are consistent:
If the version vectors are the sameon both hosts—that is, for each k,Vd,i [k] = Vd, j [k]—then the copies of document d are identical.
If there are a pair of hosts k and m such that Vd,i [k] < Vd, j [k] and Vd,i [m] > Vd, j [m], then the copies are inconsistent; that is, the copy of d at i contains updates performed by host k that have not been propagated to host j , and, similarly, the copy of d at j contains updates performed by hostm that have not been propagated to host i.
Then, the copies of d are inconsistent, since two or more updates have been performed on d independently.
The version-vector scheme was initially designed to deal with failures in distributed file systems.
The scheme gained importance because mobile computers often store copies of files that are also present on server systems, in effect constituting a distributed file system that is often disconnected.
Another application of the scheme is in groupware systems, where hosts are connected periodically, rather than continuously, and must exchange updated documents.
The version-vector scheme also has applications in replicated databases, where it can be applied to individual tuples.
For example, if a calendar or address book is maintained on a mobile device as well as on a host, inserts, deletes and updates can happen either on the mobile device or on the host.
By applying the version-vector scheme to individual calendar entries or contacts, it is easy to handle situations where a particular entry has been updated on the mobile device.
However, if the same entry is updated independently at both places, a conflict would be detected by the version-vector scheme.
The version-vector scheme, however, fails to address the most difficult and most important issue arising from updates to shared data—the reconciliation of inconsistent copies of data.
Many applications can perform reconciliation automatically by executing in each computer those operations that had performed updates on remote computers during the period of disconnection.
This solution works if update operations commute—that is, they generate the same result, regardless of the order in which they are executed.
Alternative techniques may be available in certain applications; in the worst case, however, it must be left to the users to resolve the inconsistencies.
Dealing with such inconsistency automatically, and assisting users in resolving inconsistencies that cannot be handled automatically, remains an area of research.
Anotherweakness is that the version-vector scheme requires substantial communication between a reconnecting mobile host and that host’s mobile support station.
Consistency checks can be delayed until the data are needed, although this delay may increase the overall inconsistency of the database.
Often, it is preferable to let users prepare transactions on mobile hosts, but to require that, instead of executing the transactions locally, they submit transactions to a server for execution.
Transactions that span more than one computer and that include amobile host face long-term blocking during transaction commit, unless disconnectivity is rare or predictable.
Whereas most databases model the state of the real world at a point in time (at the current time), temporal databases model the states of the real world across time.
Facts in temporal relations have associated times when they are valid, which can be represented as a union of intervals.
Temporal query languages simplify modeling of time, as well as time-related queries.
Design data are stored primarily as vector data; geographic data consist of a combination of vector and raster data.
Special-purpose index structures are particularly important for accessing spatial data, and for processing spatial queries.
Issues such as similaritybased retrieval and delivery of data at guaranteed rates are topics of current research.
Mobile computing systems have become common, leading to interest in database systems that can run on such systems.
Query processing in such systems may involve lookups on server databases.
The query costmodelmust include the cost of communication, including monetary cost and battery-power cost, which is relatively high for mobile systems.
Broadcast is much cheaper per recipient than is point-to-point communication, and broadcast of data such as stock-market data helps mobile systems to pick up data inexpensively.
Disconnected operation, use of broadcast data, and caching of data are three important issues being addressed in mobile computing.
Suppose also that the only queries that will be asked are of the following form: The query specifies a point, and asks if there is a restaurant exactly at that point.
Which type of index would be preferable, R-tree or B-tree? Why?
Describe an algorithm to find the nearest neighbor by making use of multiple region queries.
If a line segment is not parallel to the axes, the bounding box for it can be large, containing a large empty area.
Describe the effect on performance of having large bounding boxes on queries that ask for line segments intersecting a given region.
Briefly describe a technique to improve performance for such queries and give an example of its benefit.
Hint: Use bounding boxes to check if leaf entries under a pair of internal nodes may intersect.
Showhow the version-vector scheme can ensure proper updating of the central database and mobile computers when a mobile computer reconnects.
Is it possible to convert such vector data to raster data? If so, what are the drawbacks of storing raster data obtained by such conversion, instead of the original vector data?
A schema to represent the geographic location of restaurants along with features such as the cuisine served at the restaurant and the level of expensiveness.
A query to findmoderately priced restaurants that serve Indian food and are within 5 miles of your house (assume any location for your house)
A query to find for each restaurant the distance from the nearest restaurant serving the same cuisine and with the same level of expensiveness.
Samet and Aref [1995] provides an overview of spatial data models, spatial operations, and the integration of spatial and nonspatial data.
Revesz [2002] provides textbook coverage of the area of constraint databases; temporal intervals and spatial regions can be thought of as special cases of constraints.
Indexing of multimedia data is discussed in Faloutsos and Lin [1995]
Imielinski and Korth [1996] presents an introduction to mobile computing and a collection of research papers on the subject.
The version-vector scheme for detecting inconsistency in distributed file systems is described by Popek et al.
We discussed in those chapters a variety of techniques for ensuring the ACID properties in an environment where failure can occur, and where the transactions may run concurrently.
The CICS TP monitor from IBM was one of the earliest TP monitors, and has been very widely used.
Web application server architectures, including servlets, which we studied earlier in Section 9.3, support many of the features of TP monitors and are sometimes referred to as “TP lite.” Web application servers are in widespread use, and have supplanted traditional TP monitors for many applications.
However, the concepts underlying them, which we study in this section, are essentially the same.
One way of building such systems is to have a server process for each client; the server performs authentication, and then executes actions requested by the client.
This model presents several problems with respect to memory utilization and processing speed:
Even if memory for program code is shared by all processes, each process consumes memory for local data and open file descriptors, as well as for operating-system overhead, such as page tables to support virtual memory.
The operating system divides up available CPU time among processes by switching among them; this technique is called multitasking.
Each context switch between one process and the next has considerable CPU overhead; even on today’s fast systems, a context switch can take hundreds of microseconds.
Remote clients send requests to the server process, which then executes those requests.
This model is also used in client–server environments, where clients send requests to a single-server process.
The server process handles tasks, such as user authentication, that would normally be handled by the operating system.
To avoid blocking other clients when processing a long request for one client, the server process is multithreaded: The server process has a thread of control for each client, and, in effect, implements its own low-overhead multitasking.
It executes code on behalf of one client for a while, then saves the internal context and switches to the code for another client.
Unlike the overhead of full multitasking, the cost of switching between threads is low (typically only a few microseconds)
Systems based on the single-server model, such as the original version of the IBM CICS TP monitor and file servers such as Novel’s NetWare, successfully provided high transaction rateswith limited resources.
However, they had problems, especially when multiple applications accessed the same database:
Since all the applications run as a single process, there is no protection among them.
A bug in one application can affect all the other applications as well.
It would be best to run each application as a separate process.
However, concurrent threads within a process can be supported in a shared-memory multiprocessor system.
This is a serious drawback in large organizations, where parallel processing is critical for handling large workloads, and distributed data are becoming increasingly common.
One way to solve these problems is to run multiple application-server processes that access a common database, and to let the clients communicate with the application through a single communication process that routes requests.
This model is called themany-server, single-router model, illustrated in Figure 26.1c.
This model supports independent server processes for multiple applications; further, each application can have a pool of server processes, any one of which can handle a client session.
The request can, for example, be routed to themost lightly loaded server in a pool.
As before, each server process can itself bemultithreaded, so that it can handlemultiple clients concurrently.
The above architecture is also widely used in Web servers.
A Web server has a main process that receives HTTP requests, and then assigns the task of handling each request to a separate process (chosen from among a pool of processes)
The use of safe programming languages, such as Java, C#, or Visual Basic, allowsWeb application servers to protect threads from errors in other threads.
In contrast, with a language like C or C++, errors such as memory allocation errors in one thread can cause other threads to fail.
A more general architecture has multiple processes, rather than just one, to communicate with clients.
The client communication processes interact with one or more router processes, which route their requests to the appropriate server.
Very high performance Web-server systems also adopt such an architecture.
The router processes are often network routers that direct traffic addressed to the same Internet address to different server computers, depending on where the traffic comes from.
What looks like a single server with a single address to the outside world may be a collection of servers.
The detailed structure of a TP monitor appears in Figure 26.2
A TP monitor does more than simply pass messages to application servers.
When messages arrive, they may have to be queued; thus, there is a queue manager for incoming messages.
The queue may be a durable queue, whose entries survive system failures.
Using a durable queue helps ensure that once received and stored in the queue, the messages will be processed eventually, regardless of system failures.
Authorization and application-server management (for example, server start-up and routing of messages to servers) are further functions of a TP monitor.
Recall that persistent messaging (Section 19.4.3) provides a guarantee that the message will be delivered if (and only if) the transaction commits.
In addition to these facilities, many TP monitors also provided presentation facilities to create menus/forms interfaces for dumb clients such as terminals;
They may also have to interact with legacy systems, such as special-purpose data-storage systems built directly on file systems.
Finally, they may have to communicate with users or other applications at remote sites.
It is important to be able to coordinate data accesses, and to implement ACID properties for transactions across such systems.
Modern TPmonitors provide support for the construction and administration of such large applications, built up from multiple subsystems such as databases, legacy systems, and communication systems.
A TP monitor treats each subsystem as a resource manager that provides transactional access to some set of resources.
The interface between the TPmonitor and the resourcemanager is defined by a set of transaction primitives, such as begin transaction, commit transaction, abort transaction, and prepare to commit transaction (for two-phase commit)
Of course, the resource manager must also provide other services, such as supplying data, to the application.
Many database systems support the X/Open standards, and can act as resourcemanagers.
TPmonitors—as well as other products, such as SQL systems, that support the X/Open standards—can connect to the resource managers.
In addition, services provided by a TP monitor, such as persistent messaging and durable queues, act as resource managers supporting transactions.
The TP monitor can act as coordinator of two-phase commit for transactions that access these services as well as database systems.
For example, when a queued update transaction is executed, an output message is delivered, and the request transaction is removed from the request queue.
Two-phase commit between the database and the resource managers for the durable queue and persistent messaging helps ensure that, regardless of failures, either all these actions occur or none occurs.
We can also use TP monitors to administer complex client–server systems consisting of multiple servers and a large number of clients.
The TP monitor coordinates activities such as system checkpoints and shutdowns.
It administers server pools by adding servers or removing servers without interruption of the the database system.
If a server fails, the TPmonitor can detect this failure, abort the transactions in progress, and restart the transactions.
If a node fails, the TP monitor can migrate transactions to servers at other nodes, again backing out incomplete transactions.
When failed nodes restart, the TP monitor can govern the recovery of the node’s resource managers.
TPmonitors canbeused tohidedatabase failures in replicated systems; remote backup systems (Section 16.9) are an example of replicated systems.
Transaction requests are sent to the TP monitor, which relays the messages to one of the.
If one site fails, the TP monitor can transparently route messages to a backup site, masking the failure of the first site.
As far as the client code that invokes the RPC is concerned, the call looks like a local procedure-call invocation.
In such an interface, the RPC mechanism provides calls that can be used to enclose a series of RPC calls within a transaction.
Thus, updates performed by an RPC are carried out within the scope of the transaction, and can be rolled back if there is any failure.
A workflow is an activity in which multiple tasks are executed in a coordinated way by different processing entities.
A task defines some work to be done and can be specified in a number of ways, including a textual description in a file or electronic-mail message, a form, a message, or a computer program.
Figure 26.3 showsa fewexamples ofworkflows.A simple example is that of an electronic-mail system.
The delivery of a singlemail messagemay involve several mail systems that receive and forward themailmessage, until themessage reaches its destination, where it is stored.
Other terms used in the database and related literature to refer to workflows include task flow andmultisystem applications.
The person who wants a loan fills out a form, which is then checked by a loan officer.
An employee who processes loan applications verifies the data in the form, using sources such as credit-reference bureaus.
When all the required information has been collected, the loan officer may decide to approve the loan; that decisionmay.
Workﬂow Typical Typical processing application task entity electronic-mail routing electronic-mail message mailers.
Each human here performs a task; in a bank that has not automated the task of loan processing, the coordination of the tasks is typically carried out by passing of the loan application, with attached notes and other information, from one employee to the next.
Other examples of workflows include processing of expense vouchers, of purchase orders, and of credit-card transactions.
Today, all the information related to aworkflow ismore than likely to be stored in a digital form on one or more computers, and, with the growth of networking, information can be easily transferred from one computer to another.
Hence, it is feasible for organizations to automate their workflows.
For example, to automate the tasks involved in loan processing, we can store the loan application and associated information in a database.
The workflow itself then involves handing of responsibility from one human to the next, and possibly even to programs that can automatically fetch the required information.
Humans can coordinate their activities by means such as electronic mail.
Workflows are becoming increasingly important for multiple reasons within as well as between organizations.
Many organizations today have multiple software systems that need to work together.
For example, when an employee joins an organization, information about the employee may have to be provided to the payroll system, to the library system, to authentication systems that allow the user to log on to computers, to a system that manages cafeteria accounts, an so on.
Updates, such as when the employee changes status or leaves the organization, also have to be propagated to all the systems.
Organizations are increasingly automating their services; for example, a supplier may provide an automated system for customers to place orders.
Several tasks may need to be carried out when an order is placed, including reserving production time to create the ordered product and delivery services to deliver the product.
We have to address two activities, in general, to automate a workflow.
The first is workflow specification: detailing the tasks that must be carried out and defining the execution requirements.
The second problem is workflow execution, which we must do while providing the safeguards of traditional database.
For example, it is not acceptable for a loan application or a voucher to be lost, or to be processed more than once, because of a system crash.
The idea behind transactional workflows is to use and extend the concepts of transactions to the context of workflows.
Workflow activities may require interactions among several such systems, each performing a task, as well as interactions with humans.
A number of workflow systems have been developed in recent years.
Here, we study properties of workflow systems at a relatively abstract level, without going into the details of any particular system.
Internal aspects of a taskdonotneed tobemodeled for thepurposeof specification and management of a workflow.
In an abstract view of a task, a task may use parameters stored in its input variables, may retrieve and update data in the local system, may store its results in its output variables, and may be queried about its execution state.
At any time during the execution, the workflow state consists of the collection of states of the workflow’s constituent tasks, and the states (values) of all variables in the workflow specification.
The coordination of tasks can be specified either statically or dynamically.
A static specification defines the tasks—and dependencies among them—before the execution of the workflow begins.
For instance, the tasks in an expensevoucher workflow may consist of the approvals of the voucher by a secretary, a manager, and an accountant, in that order, and finally the delivery of a check.
The dependencies among the tasks may be simple—each task has to be completed before the next begins.
A generalization of this strategy is to have a precondition for execution of each task in the workflow, so that all possible tasks in a workflow and their dependencies are known in advance, but only those tasks whose preconditions are satisfiedare executed.
Execution states of other tasks—for example, “task ti cannot start until task tj has ended,” or “task ti must abort if task tj has committed.”
Output values of other tasks—for example, “task ti can start if task tj returns a value greater than 25,” or “the manager-approval task can start if the secretary-approval task returns a value of OK.”
We can combine the dependencies by the regular logical connectors (or, and, not) to form complex scheduling preconditions.
An example of dynamic scheduling of tasks is an electronic-mail routing system.
The next task to be scheduled for a given mail message depends on what the destination address of the message is, and on which intermediate routers are functioning.
The traditional notion of failure atomicity would require that a failure of any task result in the failure of the workflow.
However, a workflow can, in many cases, survive the failure of one of its tasks—for example, by executing a functionally equivalent task at another site.
Therefore, we should allow the designer to define failure-atomicity requirements of a workflow.
The system must guarantee that every execution of a workflow will terminate in a state that satisfies the failure-atomicity requirements defined by the designer.
We call those states acceptable termination states of a workflow.
All other execution states of a workflow constitute a set of nonacceptable termination states, in which the failure-atomicity requirements may be violated.
An acceptable termination state can be designated as committed or aborted.
A committed acceptable termination state is an execution state in which the objectives of a workflow have been achieved.
In contrast, an aborted acceptable termination state is a valid termination state in which a workflow has failed to achieve its objectives.
If an aborted acceptable termination state has been reached, all undesirable effects of the partial execution of the workflow must be undone in accordance with that workflow’s failure-atomicity requirements.
A workflow must reach an acceptable termination state even in the presence of system failures.
Thus, if a workflow is in a nonacceptable termination state at the time of failure, during system recovery it must be brought to an acceptable termination state (whether aborted or committed)
For example, in the loan-processing workflow, in the final state, either the loan applicant is told that a loan cannot be made or the loan is disbursed.
In case of failures such as a long failure of the verification system, the loan application could be returned to the loan applicant with a suitable explanation; this outcome would constitute an aborted acceptable termination.
A committed acceptable termination would be either the acceptance or the rejection of the loan.
In general, a task can commit and release its resources before the workflow reaches a termination state.
However, if the multitask transaction later aborts, its failure atomicity may require that we undo the effects of already completed tasks (for example, committed subtransactions) by executing compensating tasks (as subtransactions)
The semantics of compensation requires that a compensating transaction eventually complete its execution successfully, possibly after a number of resubmissions.
If the voucher is later rejected, whether because of failure or for other reasons, the budget may have to be restored by a compensating transaction.
A task agent controls the execution of a task by a processing entity.
A scheduler is a program that processes workflows by submitting various tasks for execution, monitoring various events, and evaluating conditions related to intertask dependencies.
A scheduler may submit a task for execution (to a task agent), or may request that a previously submitted task be aborted.
In accordance with the workflow specifications, the scheduler enforces the scheduling dependencies and is responsible for ensuring that tasks reach acceptable termination states.
There are three architectural approaches to the development of a workflowmanagement system.A centralized architecturehas a single scheduler that schedules the tasks for all concurrently executing workflows.
The partially distributed architecture has one scheduler instantiated for each workflow.
When the issues of concurrent execution can be separated from the scheduling function, the latter option is a natural choice.
A fully distributed architecture has no scheduler, but the task agents coordinate their execution by communicating with one another to satisfy task dependencies and other workflow execution requirements.
The simplest workflow-execution systems follow the fully distributed approach just described and are based on messaging.
Messaging may be implemented by persistent messaging mechanisms, to provide guaranteed delivery.
Some implementations use email for messaging; such implementations provide many of the features of persistent messaging, but generally do not guarantee atomicity of message delivery and transaction commit.
Each site has a task agent that executes tasks received through messages.
Execution may also involve presenting messages to humans, who have then to carry out some action.
When a task is completed at a site, and needs to be processed at another site, the task agent dispatches a message to the next site.
The message contains all relevant information about the task to be performed.
Such message-based workflow systems are particularly useful in networks that may be disconnected for part of the time.
The centralized approach is used in workflow systems where the data are stored in a central database.
The scheduler notifies various agents, such as humans or computer programs, that a task has to be carried out, and keeps track of task completion.
It is easier to keep track of the state of a workflow with a centralized approach than it is with a fully distributed approach.
The scheduler must guarantee that a workflow will terminate in one of the specified acceptable termination states.
Ideally, before attempting to execute a workflow, the scheduler should examine that workflow to check whether the.
If the scheduler cannot guarantee that a workflow will terminate in an acceptable state, it should reject such specifications without attempting to execute the workflow.
Therefore, such a workflow specification is unsafe, and should be rejected.
Safety checks such as the one just describedmay be impossible or impractical to implement in the scheduler; it then becomes the responsibility of the person designing the workflow specification to ensure that the workflows are safe.
The objective ofworkflow recovery is to enforce the failure atomicity of theworkflows.
For example, the scheduler could continue processing after failure and recovery, as though nothing happened, thus providing forward recoverability.
Otherwise, the scheduler could abort the whole workflow (that is, reach one of the global abort states)
In either case, some subtransactions may need to be committed or even submitted for execution (for example, compensating subtransactions)
We assume that the processing entities involved in the workflow have their own recovery systems and handle their local failures.
Therefore, the appropriate status information must be logged on stable storage.
We also need to consider the contents of the message queues.
When one agent hands off a task to another, the handoff should be carried out exactly once: If the handoff happens twice a task may get executed twice; if the handoff does not occur, the task may get lost.
Persistent messaging (Section 19.4.3) provides exactly the features to ensure positive, single handoff.
Workflows are often hand coded as part of application systems.
For instance, enterprise resource planning (ERP) systems, which help coordinate activities across an entire enterprise, have numerous workflows built into them.
In today’s world of interconnected organizations, it is not sufficient to manage workflows only within an organization.
For instance, consider an order placed by an organization and communicated to another organization that fulfills the order.
In each organization there may be a workflow associated with the order, and it is important that the workflows be able to interoperate, in order to minimize human intervention.
The term business process management is used to refer to the management of workflows related to business processes.
Today, applications are increasingly making their functionality available as services that can be invoked by other applications, often using a Web service architecture.
A system architecture based on invoking services provided by multiple applications is referred to as a service oriented architecture SOA.
Such services are the base layer on top of which workflowmanagement is implemented today.
The process logic that controls the workflow by invoking the services is referred to as orchestration.
The Business Process Modeling Notation (BPMN), is a standard for graphical modeling of business processes in a workflow, and XML Process Definition Language (XPDL) is an XML based representation of business process definitions, based on BPMN diagrams.
E-commerce refers to the process of carrying out various activities related to commerce, through electronic means, primarily through the Internet.
Presale activities, needed to inform the potential buyer about the product or service being sold.
The sale process, which includes negotiations on price and quality of service, and other contractual matters.
The marketplace: When there are multiple sellers and buyers for a product, a marketplace, such as a stock exchange, helps in negotiating the price to be paid for the product.
Auctions are used when there is a single seller and multiple buyers, and reverse auctions are used when there is a single buyer and multiple sellers.
Any e-commerce site provides users with a catalog of the products and services that the site supplies.
At the minimum, an e-catalog must provide browsing and search facilities to help customers find the product for which they are looking.
To help with browsing, products should be organized into an intuitive hierarchy, so a few clicks on hyperlinks can lead customers to the products in which they are interested.
Keywords provided by the customer (for example, “digital camera” or “computer”) should speed up the process of finding required products.
E-catalogs should also provide a means for customers to easily compare alternatives from which to choose among competing products.
For instance, a retailer may have an agreement with a large company to supply some products at a discount.
An employee of the company, viewing the catalog to purchase products for the company, should see prices with the negotiated discount, instead of the regular prices.
Because of legal restrictions on sales of some types of items, customerswho are underage, or from certain states or countries, should not be shown items that cannot legally be sold to them.
Catalogs can also be personalized to individual users, on the basis of past buying history.
For instance, frequent customers may be offered special discounts on some items.
Supporting such customization requires customer information as well as special pricing/discount information and sales restriction information to be stored in a database.
There are also challenges in supporting very high transaction rates, which are often tackled by caching of query results or generated Web pages.
When there are multiple sellers or multiple buyers (or both) for a product, a marketplace helps in negotiating the price to be paid for the product.
In a reverse auction system a buyer states requirements, and sellers bid for supplying the item.
In a closed bidding system, the bids are not made public, whereas in an open bidding system the bids are made public.
In an auction there are multiple buyers and a single seller.
For simplicity, assume that there is only one instance of each item being sold.
Buyers bid for the items being sold, and the highest bidder for an item gets to buy the item at the bid price.
If the items will be of no value if they are not sold (for instance, airline seats, which must be sold before the plane leaves), the seller simply picks a set of bids that maximizes the income.
In an exchange, such as a stock exchange, there are multiple sellers and multiple buyers.
Buyers can specify the maximum price they are willing to pay, while sellers specify the minimum price they want.
There is usually a market maker who matches buy and sell bids, deciding on the price for each trade (for instance, at the price of the sell bid)
Bidders need to be authenticated before they are allowed to bid.
Bids (buy or sell) need to be recorded securely in a database.
Delays in broadcasting bids can lead to financial losses to some participants.
The volumes of tradesmay be extremely large at times of stockmarket volatility, or toward the end of auctions.
Thus, very high performance databases with large degrees of parallelism are used for such systems.
After items have been selected (perhaps through an electronic catalog) and the price determined (perhaps by an electronic marketplace), the order has to be settled.
Settlement involves payment for goods and the delivery of the goods.
A simple but unsecure way of paying electronically is to send a credit-card number.
First, credit-card fraud is possible.When a buyer pays for physical goods, companies can ensure that the address for delivery matches the cardholder’s address, so no one else can receive the goods, but for goods delivered electronically no such check is possible.
Second, the seller has to be trusted to bill only for the agreed-on item and to not pass on the card number to unauthorized people who may misuse it.
Several protocols are available for secure payments that avoid both the problems listed above.
In addition, they provide for better privacy, whereby the seller may not be given any unnecessary details about the buyer, and the credit-card.
All information transmitted must be encrypted so that anyone intercepting the data on the network cannot find out the contents.
Impersonation can be perpetrated by passing off a fake key as someone else’s public key (the bank’s or credit-card company’s, or the merchant’s or the buyer’s)
Impersonation is prevented by a system of digital certificates, whereby public keys are signed by a certification agency, whose public key is well known (or which in turn has its public key certified by another certification agency and so on up to a key that is well known)
From the well-known public key, the system can authenticate the other keys by checking the certificates in reverse sequence.
Several novel payment systems were developed in the early days of the Web.
One of thesewas a secure payment protocol called the Secure Electronic Transaction (SET) protocol.
The protocol requires several rounds of communication between the buyer, seller, and the bank, in order to guarantee safety of the transaction.
There were also systems that provide for greater anonymity, similar to that providedbyphysical cash.
TheDigiCashpayment systemwas one such system.When a payment is made in such a system, it is not possible to identify the purchaser.
In contrast, identifying purchasers is very easy with credit cards, and even in the case of SET, it is possible to identify the purchaser with the cooperation of the credit-card company or bank.
However, none of these systems was successful commercially, for both technical and non-technical reasons.
Unlike the SET or DigiCash protocols, there is no software running on the purchasers machine, except a Web browser; as a result this approach has found wide success where the earlier approaches failed.
An alternative approach which is used by the PayPal system is for both the purchaser and the merchant to have an account on a common platform, and the money transfer happens entirely within the common platform.
The purchaser first loads her account with money using a credit card, and can then transfer money to the merchants account.
This approach has been very successful with small merchants, since it does not require either the purchaser or the merchant to run any software.
To allow a high rate of transaction processing (hundreds or thousands of transactions per second), we must use high-performance hardware, and must exploit.
These techniques alone, however, are insufficient to obtain very low response times, since disk I/O remains a bottleneck—about 10 milliseconds are required for each I/O, and this number has not decreased at a rate comparable to the increase in processor speeds.
Disk I/O is often the bottleneck for reads, as well as for transaction commits.
The long disk latency increases not only the time to access a data item, but also limits the number of accesses per second.1
We can make a database system less disk bound by increasing the size of the database buffer.Advances inmain-memory technology let us construct largemain memories at relatively low cost.
Today, commercial 64-bit systems can support main memories of tens of gigabytes.
Additional information on main-memory databases is given in the references in the bibliographical notes.
For some applications, such as real-time control, it is necessary to store data in main memory to meet performance requirements.
The memory size required for most such systems is not exceptionally large, although there are at least a few applications that require multiple gigabytes of data to be memory resident.
Since memory sizes have been growing at a very fast rate, an increasing number of applications can be expected to have data that fit into main memory.
Large main memories allow faster processing of transactions, since data are memory resident.
Log records must be written to stable storage before a transaction is committed.
The improved performance made possible by a large main memory may result in the logging process becoming a bottleneck.
We can reduce commit time by creating a stable log buffer in main memory, using nonvolatile RAM (implemented, for example, by battery-backed-up memory)
The overhead imposed by logging can also be reduced by the group-commit technique discussed later in this section.
Throughput (number of transactions per second) is still limited by the data-transfer rate of the log disk.
Buffer blocks marked as modified by committed transactions still have to be written so that the amount of log that has to be replayed at recovery time is reduced.
If the update rate is extremely high, the disk data-transfer rate may become a bottleneck.
If the system crashes, all of main memory is lost.
On recovery, the system has an empty database buffer, and data items must be input from disk when they are accessed.
Therefore, even after recovery is complete, it takes some time before the database is fully loaded in main memory and high-speed processing of transactions can resume.
On the other hand, a main-memory database provides opportunities for optimizations:
Write latency for flash depends on whether an erase operation must be done first.
Since memory is costlier than disk space, internal data structures in mainmemory databases have to be designed to reduce space requirements.
However, data structures can have pointers crossing multiple pages, unlike those in disk databases, where the cost of the I/Os to traversemultiple pages would be excessively high.
For example, tree structures in main-memory databases can be relatively deep, unlike B+-trees, but should minimize space requirements.
However, the speed difference between cache memory and main-memory, and the fact that data is transferred between main-memory and cache in units of a cache-line (typically about 64 bytes), results in a situation where the relationship between cache and main-memory is not dissimilar to the relationship between main-memory and disk (although with smaller speed differences)
As a result, B+-trees with small nodes that fit in a cache line have been found quite useful even in main-memory databases.
There is no need to pin buffer pages in memory before data are accessed, since buffer pages will never be replaced.
Query-processing techniques should be designed to minimize space overhead, so that main-memory limits are not exceeded while a query is being evaluated; that situation would result in paging to swap area, and would slow down query processing.
Once the disk I/O bottleneck is removed, operations such as locking and latching may become bottlenecks.
Such bottlenecks must be eliminated by improvements in the implementation of these operations.
Recovery algorithms can be optimized, since pages rarely need to be written out to make space for other pages.
The process of committing a transaction T requires these records to be written to stable storage:
All log records associated with T that have not been output to stable storage.
These output operations frequently require the output of blocks that are only partially filled.
To ensure that nearly full blocks are output, we use the groupcommit technique.
Instead of attempting to commit T when T completes, the system waits until several transactions have completed, or a certain period of time has passed since a transaction completed execution.
It then commits the group of transactions that are waiting, together.
Blocks written to the log on stable storage would contain records of several transactions.
By careful choice of group size and maximum waiting time, the system can ensure that blocks are full when they are written to stable storage without making transactions wait excessively.
This technique results, on average, in fewer output operations per committed transaction.
Although group commit reduces the overhead imposed by logging, it results in a slight delay in commit of transactions that perform updates.
These delays can be eliminated if disks or disk controllers support nonvolatile RAM buffers for write operations.
Transactions can commit as soon as the write is performed on the nonvolatile RAM buffer.
In this case, there is no need for group commit.
Note that group commit is useful even in databases with disk-resident data, not just for main-memory databases.
If flash storage is used instead of magnetic disk for storing log records, the commit delay is significantly reduced.
However, group commit can still be useful since it minimizes the number of pages written; this translates to performance benefits in flash storage, since pages cannot be overwritten, and the erase operation is expensive.
Flash storage systems remap logical pages to a pre-erased physical page, avoiding delay at the time a page is written, but the erase operation must be performed eventually as part of garbage collection of old versions of pages.
The integrity constraints that we have considered thus far pertain to the values stored in the database.
In certain applications, the constraints include deadlines by which a task must be completed.
Examples of such applications include plant management, traffic control, and scheduling.
When deadlines are included, correctness of an execution is no longer solely an issue of database consistency.
Rather, we are concerned with how many deadlines are missed, and by how much time they are missed.
Serious problems, such as system crash, may occur if a task is not completed by its deadline.
The task has zero value if it is completed after the deadline.
The task has diminishing value if it is completed after the.
Transaction management in real-time systems must take deadlines into account.
In such cases, it may be preferable to pre-empt the transaction holding the lock, and to allow Ti to proceed.
Pre-emption must be used with care, however, because the time lost by the pre-empted transaction (due to rollback and restart) may cause the pre-empted transaction to miss its deadline.
Unfortunately, it is difficult to determinewhether rollback orwaiting is preferable in a given situation.
Amajor difficulty in supporting real-time constraints arises from the variance in transaction execution time.
In the best case, all data accesses reference data in.
In the worst case, each access causes a buffer page to be written to disk (preceded by the requisite log records), followed by the reading from disk of the page containing the data to be accessed.
Because the two or more disk accesses required in the worst case take several orders of magnitude more time than the main-memory references required in the best case, transaction execution time can be estimated only very poorly if data are resident on disk.
Hence, main-memory databases are often used if real-time constraints have to be met.
They have extended locking protocols to provide higher priority for transactions with early deadlines.
They have found that optimistic concurrency protocols performwell in real-time databases; that is, these protocols result in fewer missed deadlines than even the extended locking protocols.
The bibliographical notes provide references to research in the area of real-time databases.
In real-time systems, deadlines, rather than absolute speed, are the most important issue.
Designing a real-time system involves ensuring that there is enough processing power to meet deadlines without requiring excessive hardware resources.
Achieving this objective, despite the variance in execution time resulting from transaction management, remains a challenging problem.
The transaction concept developed initially in the context of data-processing applications, inwhichmost transactions are noninteractive and of short duration.
Once a human interacts with an active transaction, that transaction becomes a long-duration transaction from the perspective of the computer, since human response time is slow relative to computer speed.
Furthermore, in design applications, the human activity may involve hours, days, or an even longer period.
Thus, transactions may be of long duration in human terms, as well as in machine terms.
Data generated and displayed to a user by a long-duration transaction are uncommitted, since the transaction may abort.
Thus, users—and, as a result, other transactions—may be forced to read uncommitted data.
If several users are cooperating on a project, user transactions may need to exchange data prior to transaction commit.
An interactive transactionmay consist of a set of subtasks initiated by the user.
The usermaywish to abort a subtaskwithout necessarily causing the entire transaction to abort.
It is unacceptable to abort a long-duration interactive transaction because of a system crash.
The active transaction must be recovered to a state that existed shortly before the crash so that relatively little human work is lost.
Good performance in an interactive transaction system is defined as fast response time.
This definition is in contrast to that in a noninteractive system, in which high throughput (number of transactions per second) is the goal.
Systems with high throughput make efficient use of system resources.
However, in the case of interactive transactions, the most costly resource is the user.
If the efficiency and satisfaction of the user is to be optimized, response time should be fast (from a human perspective)
In those caseswhere a task takes a long time, response time should be predictable (that is, the variance in response times should be low), so that users can manage their time well.
The properties that we discussed make it impractical to enforce the requirement used in earlier chapters that only serializable schedules be permitted.
When a lock cannot be granted, the transaction requesting the lock is forced to wait for the data item in question to be unlocked.
The duration of this wait is proportional to the duration of the transaction holding the lock.
If the data item is locked by a short-duration transaction, we expect that the waiting time will be short (except in case of deadlock or extraordinary system load)
However, if the data item is locked by a longduration transaction, the wait will be of long duration.
Long waiting times lead to both longer response time and an increased chance of deadlock.
Graph-based protocols allow for locks to be released earlier than under the two-phase locking protocols, and they prevent deadlock.
Transactions must lock data items in amanner consistent with this ordering.
As a result, a transaction may have to lock more data than it needs.
Furthermore, a transaction must hold a lock until there is no chance that the lock will be needed again.
However, they do require transactions to abort under certain circumstances.
If a long-duration transaction is aborted, a substantial amount of.
For noninteractive transactions, this lost work is a performance issue.
For interactive transactions, the issue is also one of user satisfaction.
It is highly undesirable for a user to find that several hours’ worth of work have been undone.
Like timestamp-based protocols, validation protocols enforce serializability by means of transaction abort.
Thus, it appears that the enforcement of serializability results in long-duration waits, in abort of long-duration transactions, or in both.
There are theoretical results, cited in the bibliographical notes, that substantiate this conclusion.
Further difficulties with the enforcement of serializability arise when we consider recovery issues.We previously discussed the problem of cascading rollback, in which the abort of a transaction may lead to the abort of other transactions.
If locking is used, exclusive locks must be held until the end of the transaction, if cascading rollback is to be avoided.
This holding of exclusive locks, however, increases the length of transaction waiting time.
Thus, it appears that the enforcement of transaction atomicity must either lead to an increased probability of long-duration waits or create a possibility of cascading rollback.
The latter protocol was in fact designed specifically to deal with long duration transactions that involve user interaction.
Although it does not guarantee serializability, optimistic concurrency control without read validation is quite widely used.
However, when transactions are of long duration, conflicting updates are more likely, resulting in additional waits or aborts.
These considerations are the basis for the alternative concepts of correctness of concurrent executions and transaction recovery that we consider in the remainder of this section.
The fundamental goal of database concurrency control is to ensure that concurrent execution of transactions does not result in a loss of database consistency.
The concept of serializability can be used to achieve this goal, since all serializable schedules preserve consistency of the database.
However, not all schedules that preserve consistency of the database are serializable.
For an example, consider again a bank database consisting of two accounts A and B, with the consistency requirement that the sum A + B be preserved.
Although the schedule of Figure 26.5 is not conflict serializable, it nevertheless preserves the sum of A + B.
It also illustrates two important points about the concept of correctness without serializability.
Correctness depends on the specific consistency constraints for the database.
In general it is not possible to perform an automatic analysis of low-level operations by transactions and check their effect on database consistency constraints.
One is to use the database consistency constraints as the basis for a split of the database into subdatabases onwhich concurrency can be managed separately.
Another is to treat some operations besides read and write as fundamental low-level operations and to extend concurrency control to deal with them.
The bibliographical notes reference other techniques for ensuring consistency without requiring serializability.
Many of these techniques exploit variants of multiversion concurrency control (see Section 15.6)
For older data-processing applications that need only one version, multiversion protocols impose a high space overhead to store the extra versions.
A long-duration transaction can be viewed as a collection of related subtasks or subtransactions.
By structuring a transaction as a set of subtransactions, we are able to enhance parallelism, since itmaybe possible to run several subtransactions in parallel.
Furthermore, it is possible to deal with failure of a subtransaction (due to abort, system crash, and so on) without having to roll back the entire long-duration transaction.
Nesting may be several levels deep, representing a subdivision of a transaction into subtasks, subsubtasks, and so on.
At the lowest level of nesting, we have the standard database operations read and write that we have used previously.
If a subtransaction of T is permitted to release locks on completion, T is called amultilevel transaction.
When a multilevel transaction represents a longduration activity, the transaction is sometimes referred to as a saga.
Alternatively, if locks held by a subtransaction ti of T are automatically assigned to T on completion of ti , T is called a nested transaction.
Although the main practical value of multilevel transactions arises in complex, long-duration transactions, we shall use the simple example of Figure 26.5 to show how nesting can create higher-level operations that may enhance concurrency.
To reduce the frequency of long-duration waiting, we arrange for uncommitted updates to be exposed to other concurrently executing transactions.
However, the exposure of uncommitted data creates the potential for cascading rollbacks.
The concept of compensating transactions helps us to deal with this problem.
Now, if the outer-level transaction T has to be aborted, the effect of its subtransactions must be undone.
We can undo the effects of tk+1 by aborting that.
Instead,we execute a new subtransaction cti , called a compensating transaction, to undo the effect of a subtransaction ti.
Each subtransaction ti is required to have a compensating transaction cti.
Consider the schedule of Figure 26.5, which we have shown to be correct, although not conflict serializable.
Consider a database insert by transaction Ti that, as a side effect, causes a B+-tree index to be updated.
The insert operationmay have modified several nodes of the B+-tree index.
Other transactions may have read these nodes in accessing data other than the record inserted by Ti.
As mentioned in Section 16.7, we can undo the insertion by deleting the record inserted by Ti.
The result is a correct, consistent B+-tree, but is not necessarily one with exactly the same structure as the one we had before Ti started.
Instead of undoing all of Ti , we compensate for the failure of Ti,3 by deleting the old hotel reservation and making a new one.
If the system crashes in the middle of executing an outer-level transaction, its subtransactions must be rolled back when it recovers.
The techniques described in Section 16.7 can be used for this purpose.
Compensation for the failure of a transaction requires that the semantics of the failed transaction be used.
For certain operations, such as incrementation or insertion into a B+-tree, the corresponding compensation is easily defined.
For more complex transactions, the application programmers may have to define the correct form of compensation at the time that the transaction is coded.
For complex interactive transactions, it may be necessary for the system to interact with the user to determine the proper form of compensation.
The transaction concepts discussed in this section create serious difficulties for implementation.
We present a few of them here, and discuss howwe can address these problems.
In typical database systems, such internal system data as lock tables and transaction timestamps are kept in volatile storage.
For a long-duration transaction to be resumed after a crash, these data must be restored.
Therefore, it is necessary to log not only changes to the database, but also changes to internal system data pertaining to long-duration transactions.
Logging of updates is made more complex when certain types of data items exist in the database.
A data item may be a CAD design, text of a document, or another form of composite design.
Thus, storing both the old and new values of the data item in a log record is undesirable.
There are two approaches to reducing the overhead of ensuring the recoverability of large data items:
Only the operation performed on the data item and the data-item name are stored in the log.
We perform undo using the inverse operation and redo using the operation itself.
Recovery through operation logging is more difficult, since redo and undo are not idempotent.
Further, using logical logging for an operation that updates multiple pages is greatly complicated by the fact that some, but not all, of the updated pages may have been written to the disk, so it is hard to apply either the redo or the undo of the operation on the disk image during recovery.
Using physical redo logging and logical undo logging, as described in Section 16.7, provides the concurrency benefits of logical logging while avoiding the above pitfalls.
Logging is used formodifications to small data items, but large data items are often made recoverable via a shadowing, or copy-on-write, technique.
When we use shadowing, it is possible to reduce the overheadbykeeping copies of only those pages that are actuallymodified.
Regardless of the technique used, the complexities introduced by long-duration transactions and large data items complicate the recovery process.
Thus, it is desirable to allow certain noncritical data to be exempt from logging, and to rely instead on offline backups and human intervention.
Workflows are activities that involve the coordinated execution of multiple tasks performed by different processing entities.
They exist not just in computer applications, but also in almost all organizational activities.
With the growth of networks, and the existence of multiple autonomous database systems, workflows provide a convenient way of carrying out tasks that involve multiple systems.
Although the usual ACID transactional requirements are too strong or are unimplementable for such workflow applications, workflows must satisfy a.
They provide services such as durable queueing of client requests and server responses, routing of client messages to servers, persistent messaging, load balancing, and coordination of two-phase commit when transactions access multiple servers.
Catalogmanagement, especially personalization of the catalog, is done with databases.
Electronic marketplaces help in pricing of products through auctions, reverse auctions, or exchanges.
Orders are settled by electronic payment systems, which also need high-performance database systems to handle very high transaction rates.
Large main memories are exploited in certain systems to achieve high system throughput.
Under the groupcommit concept, the number of outputs to stable storage can be reduced, thus releasing this bottleneck.
The efficient management of long-duration interactive transactions is more complex, because of the long-duration waits and because of the possibility of aborts.
A long-duration transaction is represented as anested transactionwith atomic database operations at the lowest level.
Active long-duration transactions resume once any short-duration transactions have recovered.
A compensating transaction is needed to undo updates of nested transactions that have committed, if the outer-level transaction fails.
In systems with real-time constraints, correctness of execution involves not only database consistency but also deadline satisfaction.
Loading the entire database back into main memory before resuming transaction processing.
Different threads may run concurrently, attempting to deliver different messages.
In case of a delivery failure, the messagemust be restored in the queue.Model the actions that each thread carries out as a multilevel transaction, so that locks on the queue need not be held until a message is delivered.
Also, explain any differences that result if we allow multilevel transactions.
Give a high-level picture of the workflow starting from the student application procedure.
Indicate acceptable termination states and which steps involve human intervention.
Indicate possible errors (including deadline expiry) and how they are dealt with.
Study how much of the workflow has been automated at your university.
Explain why electronic transactions carried out using credit-card numbers may be insecure.
An alternative is to have an electronic payment gatewaymaintained by the credit-card company, and the site receiving payment redirects customers to the gateway site to make the payment.
Explain what benefits such a system offers if the gateway does.
Explain what further benefits are offered if the gateway has a.
Some credit-card companies offer a one-time-use credit-cardnumber as a more secure method of electronic payment.
Customers connect to the credit-card company’s Web site to get the one-time-use number.
Does either of the above systems guarantee the same privacy that is available when payments are made in cash? Explain your answer.
Explain why this presents a problem to designers of real-time database systems.
Hint: consider the case when the disk buffer is full.
Our description of workflows follows the model of Rusinkiewicz and Sheth [1995]
Loeb [1998] provides a detailed description of secure electronic transactions.
Garcia-Molina and Salem [1992] provides an overview of main-memory databases.
A storage manager for main-memory databases is described in Jagadish et al.
Concurrency control and scheduling in real-time databases are discussed by Haritsa et al.
Ozsoyoglu and Snodgrass [1995] is a survey of research in real-time and temporal databases.
The concept of Saga was introduced in Garcia-Molina and Salem [1987]
These three represent three of the most widely used commercial database systems.
Each of these chapters highlights unique features of each database system: tools, SQL variations and extensions, and system architecture, including storage organization, query processing, concurrency control and recovery, and replication.
The chapters cover only key aspects of the database products they describe, and therefore should not be regarded as a comprehensive coverage of the product.
Furthermore, since products are enhanced regularly, details of the product may change.
When using a particular product version, be sure to consult the user manuals for specific details.
Keep in mind that the chapters in this part often use industrial rather than academic terminology.
For instance, they use table instead of relation, row instead of tuple, and column instead of attribute.
It is a descendant of one of the earliest such systems, the POSTGRES system developed under ProfessorMichael Stonebraker at theUniversity ofCalifornia, Berkeley.
The name “postgres” is derived from the name of a pioneering relational database system, Ingres, also developed under Stonebraker at Berkeley.
Currently, PostgreSQL supports many aspects of SQL:2003 and offers features such as complex queries, foreign keys, triggers, views, transactional integrity, full-text searching, and limited data replication.
In addition, users can extend PostgreSQL with new data types, functions, operators, or index methods.
PostgreSQL supports a variety of programming languages (including C, C++, Java, Perl, Tcl, and Python) as well as the database interfaces JDBC and ODBC.
Another notable point of PostgreSQL is that it, along with MySQL, is one of the two most widely used open-source relational database systems.
PostgreSQL is released under the BSD license, which grants permission to anyone for the use, modification, and distribution of the PostgreSQL code and documentation for any purpose without fee.
In the course of two decades, PostgreSQL has undergone several major releases.
The first prototype system, under the name POSTGRES, was demonstrated at the 1988 ACM SIGMOD conference.
The first version, distributed to users in 1989, provided features such as extensible data types, a preliminary rule system, and a query language named POSTQUEL.
After the subsequent versions added a new rule system, support for multiple storage managers, and an improved query executor, the system developers focused on portability and performance until 1994, when an SQL language interpreter was added.
Today, PostgreSQL is used to implement several different research andproduction applications (such as the PostGIS system for geographic information) and an educational tool at several universities.
The system continues to evolve through the contributions of a community of about 1000 developers.
In this chapter, we explain how PostgreSQL works, starting from user interfaces and languages and continuing into the heart of the system (the data structures and the concurrencycontrol mechanism)
The standard distribution of PostgreSQL comes with command-line tools for administering the database.
However, there is a wide range of commercial and open-source graphical administration and design tools that support PostgreSQL.
Software developers may also access PostgreSQL through a comprehensive set of programming interfaces.
Like most database systems, PostgreSQL offers command-line tools for database administration.
The main interactive terminal client is psql, which is modeled after the Unix shell and allows execution of SQL commands on the server, as well as several other operations (such as client-side copying)
The user can substitute (“interpolate”) psql variables into regular SQL statements by placing a colon in front of the variable name.
PostgreSQL may also be accessed from a Tcl/Tk shell, which provides a flexible scripting language commonly used for rapid prototyping.
This functionality is enabled in Tcl/Tk by loading the pgtcl library,which is distributed as an optional extension to PostgreSQL.
The standard distribution of PostgreSQL does not contain any graphical tools.
However, several graphical user interface tools exist, and users can choose among.
There are graphical tools for administration, including pgAccess and pgAdmin, the latter of which is shown in Figure 27.1
Tools for database design include TORA and Data Architect, the latter of which is shown in Figure 27.2
The libpq library supports both synchronous and asynchronous execution of SQL commands and prepared statements, through a reentrant and thread-safe interface.
The connection parameters of libpq may be configured in several flexible ways, such as.
The current version of PostgreSQL supports almost all entry-level SQL-92 features, as well as many of the intermediate- and full-level features.
In fact, some features of the current SQL standard (such as arrays, functions, and inheritance) were pioneered by PostgreSQL or its ancestors.
It lacks OLAP features (most notably, cube and rollup), but data from PostgreSQL can be easily loaded into open-source external OLAP servers (such as Mondrian) as well as commercial products.
PostgreSQL has support for several nonstandard types, useful for specific application domains.
Furthermore, users can define new types with the create type.
This includes new low-level base types, typically written in C (see Section 27.3.3.1)
Base types are also known as abstract data types; that is, modules that encapsulate both state and a set of operations.
These are implemented below the SQL level, typically in a language such as C (see Section 27.3.3.1)
Examples are int4 (already included in PostgreSQL) or complex (included as an optional extension type)
A base type may represent either an individual scalar value or a variable-length array of values.
For each scalar type that exists in a database, PostgreSQL automatically creates an array type that holds values of the same scalar type.
These correspond to table rows; that is, they are a list of field names and their respective types.
A composite type is created implicitly whenever a table is created, but users may also construct them explicitly.
Domains.Adomain type is defined by coupling a base typewith a constraint that values of the type must satisfy.
Values of the domain type and the associated base type may be used interchangeably, provided that the constraint is satisfied.
A domainmay also have an optional default value, whose meaning is similar to the default value of a table column.
These are similar to enum types used in programming languages such as C and Java.
An enumerated type is essentially a fixed list of named values.
In PostgreSQL, enumerated types may be converted to the textual representation of their name, but this conversion must be specified explicitly in some cases to ensure type safety.
For instance, values of different enumerated types may not be compared without explicit conversion to compatible types.
Currently, PostgreSQL supports the following pseudotypes: any, anyarray, anyelement, anyenum, anynonarray cstring, internal, opaque, language handler, record, trigger, and void.
These cannot be used in composite types (and thus cannot be used for table columns), but can be used as argument and return types of user-defined functions.
Four of the pseudotypes anyelement, anyarray, anynonarray, and anyenum are collectively known as polymorphic.
Functions with arguments of these types (correspondingly calledpolymorphic functions)may operate on any actual type.
The types described in this section are included in the standard distribution.
Furthermore, thanks to the open nature of PostgreSQL, there are several contributed extension types, such as complex numbers, and ISBN/ISSNs (see Section 27.3.3)
Geometric data types (point, line, lseg, box, polygon, path, circle) are used in geographic information systems to represent two-dimensional spatial objects such as points, line segments, polygons, paths, and circles.
Numerous functions and operators are available in PostgreSQL to perform various geometric operations such as scaling, translation, rotation, and determining intersections.
Full-text searching is performed in PostgreSQL using the tsvector type that represents a document and the tsquery type that represents a full-text query.
A tsvector stores the distinct words in a document, after converting variants of each word to a common normal form (for example, removingword stems)
PostgreSQL provides functions to convert raw text to a tsvector and concatenate documents.
A tsquery specifieswords to search for in candidate documents, withmultiplewords connected by Boolean operators.
These data types allow network-management applications to use a PostgreSQL database as their data store.
For those familiar with computer networking, we provide a brief summary of this feature here.
Their main difference is in input/output formatting, as well as the restriction that classless Internet domain routing (CIDR) addresses do not accept values with nonzero bits to the right of the netmask.
The macaddr type is used to store MAC addresses (typically, Ethernet card hardware addresses)
PostgreSQL supports indexing and sorting on these types, as well as a set of operations (including subnet testing, and mapping MAC addresses to hardware manufacturer names)
PostgreSQL supports SQL constraints and triggers (and stored procedures; see Section 27.3.3)
Furthermore, it features query-rewriting rules that can be declared on the server.
PostgreSQL allows check constraints, not null constraints, and primary-key and foreign-key constraints (with restricting and cascading deletes)
Like many other relational database systems, PostgreSQL supports triggers, which are useful for nontrivial constraints and consistency checking or enforcement.
Trigger functions can bewritten in a procedural language such as PL/pgSQL (see Section 27.3.3.4) or in C, but not in plain SQL.
Triggers can execute before or after insert, update, or delete operations and either once per modified row, or once per SQL statement.
The PostgreSQL rules system allows users to define query-rewrite rules on the database server.Unlike storedprocedures and triggers, the rule system intervenes between the query parser and the planner andmodifies queries on the basis of the set of rules.
After the original query tree has been transformed into one or more trees, they are passed to the query planner.
Thus, the planner has all the necessary information (tables to be scanned, relationships between them, qualifications, join information, and so forth) and can come upwith an efficient execution plan, even when complex rules are involved.
The rest of this section provides examples that illustrate the rule system’s capabilities.
More details on how rules are matched to query trees and how the latter are subsequently transformed can be found in the PostgreSQL documentation (see the bibliographical notes)
The rule system is implemented in the rewrite phase of query processing and explained in Section 27.6.1
Queries onmyview are transformed before execution to queries on the underlying table mytab.
The create view syntax is considered better programming form in this case, since it is more concise and it also prevents creation of views that.
However, rules can be used to define update actions on views explicitly (create view statements do not allow this)
As another example, consider the casewhere theuserwants to log all increases of instructor salaries.
Assume that pending salary increases are stored in a table salary increases(name, increase)
We can declare a “dummy” table approved increases with the same fields and then define the following rule:
Since the instead keyword was specified in the rule, the approved increases table is unchanged.
There is some overlap between the functionality provided by rules and perrow triggers.
The PostgreSQL rule system can be used to implementmost triggers, but some kinds of constraints (in particular, foreign keys) cannot be implemented by rules.
Also, triggers have the added ability to generate errormessages to signal constraint violations, whereas a rule may only enforce data integrity by silently suppressing invalid values.
On the other hand, triggers cannot be used for the update or delete actions that rules enable on views.
Since there is no real data in a view relation, the trigger would never be called.
An important difference between triggers and views is that a trigger is executed iteratively for every affected row.
A rule, on the other hand, manipulates the query tree before query planning.
So if a statement affects many rows, a rule is far more efficient than a trigger.
The implementation of triggers and constraints in PostgreSQL is outlined briefly in Section 27.6.4
Like most relational database systems, PostgreSQL stores information about databases, tables, columns, and so forth, in what are commonly known as system catalogs, which appear to the user as normal tables.
Other relational database systems are typically extended by changing hard-coded procedures in the source code or by loading special extension modules written by the vendor.
Unlikemost relational database systems, PostgreSQLgoes one step further and stores much more information in its catalogs: not only information about tables and columns, but also information about data types, functions, access methods, and so on.
Therefore, PostgreSQL is easy for users to extend and facilitates rapid prototyping of new applications and storage structures.
PostgreSQL can also incorporate user-written code into the server, through dynamic loading of shared objects.
This provides an alternative approach to writing extensions that can be used when catalog-based extensions are not sufficient.
PostgreSQL allows users to define composite types, enumeration types, and even new base types.
A composite-type definition is similar to a table definition (in fact, the latter implicitly does the former)
Enumeration types are easy to define, by simply listing the names of the values.
The following example creates an enumerated type to represent the status of a software product.
The order of listed names is significant in comparing values of an enumerated type.
Adding base types to PostgreSQL is straightforward; an example can be found.
The base type can be declared in C, for example:
The next step is to define functions to read and write values of the new type in text format (see Section 27.3.3.2)
Subsequently, the new type can be registered using the statement:
The user has the option of defining binary I/O functions as well (for more efficient data dumping)
Extension types can be used like the existing base types of PostgreSQL.
In fact, their only difference is that the extension types are dynamically loaded and linked into the server.
Furthermore, indices may be extended easily to handle new base types; see Section 27.3.3.3
PostgreSQL allows users to define functions that are stored and executed on the server.
PostgreSQL also supports function overloading (that is, functions may be declared by using the same name but with arguments of different types)
Functions can be written as plain SQL statements, or in several procedural languages (covered in Section 27.3.3.4)
Finally, PostgreSQL has an application programmer interface for adding functions written in C (explained in this section)
User-defined functions can be written in C (or a language with compatible calling conventions, such as C++)
The actual coding conventions are essentially the same for dynamically loaded, user-defined functions, as well as for internal functions (which are statically linked into the server).Hence, the standard internal function library is a rich source of coding examples for user-defined C functions.
Once the shared library containing the function has been created, a declaration such as the following registers it on the server:
The entry point to the shared object file is assumed to be the same as the SQL function name (here, complex out), unless otherwise specified.
The application program interface hides most of PostgreSQL’s internal details.
Hence, the actual C code for the above text output function of complex values is quite simple:
The first line declares the function complex out, and the following lines implement the output function.
More details may be found in the PostgreSQL documentation (see the bibliographical notes)
Aggregate functions in PostgreSQL operate by updating a state value via a state transition function that is called for each tuple value in the aggregation group.
For example, the state for the avg operator consists of the running sum and the count of values.
As each tuple arrives, the transition function should simply add its value to the running sum and increment the count by one.
Optionally, a final function may be called to compute the return value based on the state information.
For example, the final function for avg would simply divide the running sum by the count and return it.
Thus, defining a new aggregate is as simple as defining these two functions.
For the complex type example, if complex add is a user-defined function that takes two complex arguments and returns their sum, then the sum aggregate operator can be extended to complex numbers using the simple declaration:
Note the use of function overloading: PostgreSQL will call the appropriate sum aggregate function, on the basis of the actual type of its argument during invocation.
The basetype is the argument type and stype is the state value type.
In this case, a final function is unnecessary, since the return value is the state value itself (that is, the running sum in both cases)
User-defined functions can also be invoked by using operator syntax.
Beyond simple “syntactic sugar” for function invocation, operator declarations can also providehints to the query optimizer in order to improveperformance.
These hints may include information about commutativity, restriction and join selectivity estimation, and various other properties related to join algorithms.
PostgreSQL currently supports the usual B-tree and hash indices, as well as two index methods that are unique to PostgreSQL: the Generalized Search Tree (GiST) and the Generalized Inverted Index (GIN), which is useful for full-text indexing (these index structures are explained in Section 27.5.2.1)
Finally, PostgreSQL provides indexing of two-dimensional spatial objects with an R-tree index, which is implemented using a GiST index behind the scenes.
All of these can be easily extended to accommodate new base types.
Adding index extensions for a type requires definition of an operator class, which encapsulates the following:
These are a set of operators that can be used as qualifiers inwhere clauses.
A hash index allows only equality testing and an R-tree index allows a number of spatial relationships (for example contained, to-the-left, and so forth)
The above set of operators is typically not sufficient for the operation of the index.
For example, a hash index requires a function to compute the hash value for each object.
An R-tree index needs to be able to compute intersections and unions and to estimate the size of indexed objects.
For example, if the following functions and operators are defined to compare the magnitude of complex numbers (see Section 27.3.3.1), then we can make such objects indexable by the following declaration:
The operator statements define the strategymethods and the function statements define the support methods.
Stored functions and procedures can be written in a number of procedural languages.
Furthermore, PostgreSQL defines an application programmer interface for hooking up any programming language for this purpose.
Programming languages can be registered on demand and are either trusted or untrusted.
The latter allow unlimited access to the DBMS and the file system, and writing stored functions in them requires superuser privileges.
This is a trusted language that adds procedural programming capabilities (for example, variables and control flow) to SQL.
Although code cannot be transferred verbatim from one to the other, porting is usually simple.
These leverage the power of Tcl, Perl, and Python to write stored functions and procedures on the server.
The first two come in both trusted and untrusted versions (PL/Tcl, PL/Perl and PL/TclU, PL/PerlU, respectively), while PL/Python is untrusted at the time of this writing.
Each of these has bindings that allow access to the database system via a language-specific interface.
The server programming interface (SPI) is an application programmer interface that allows user-defined C functions (see Section 27.3.3.2) to run arbitrary SQL commands inside their functions.
This gives writers of user-defined functions the ability to implement only essential parts in C and easily leverage the full power of the relational database system engine to do most of the work.
Transaction management in PostgreSQL uses both both snapshot isolation and two-phase locking.Which one of the two protocols is used depends on the type of statement being executed.
Concurrency control for DDL statements, on the other hand, is based on standard two-phase locking.
Since the concurrency control protocol used by PostgreSQL depends on the isolation level requested by the application, we begin with an overview of the isolation levels offered by PostgreSQL.
We then describe the key ideas behind the MVCC scheme, followed by adiscussion of their implementation in PostgreSQL and some of the implications of MVCC.We conclude this sectionwith an overviewof locking for DDL statements and a discussion of concurrency control for indices.
The SQL standard defines three weak levels of consistency, in addition to the serializable level of consistency, on which most of the discussion in this book is based.
The purpose of providing the weak consistency levels is to allow a higher degree of concurrency for applications that don’t require the strong guarantees that serializability provides.
Examples of such applications include long-running transactions that collect statistics over the database andwhose results do not need to be precise.
The SQL standard defines the different isolation levels in terms of three phenomena that violate serializability.
The three phenomena are called dirty read, nonrepeatable read, and phantom read, and are defined as follows:
The transaction reads values written by another transaction that hasn’t committed yet.
A transaction reads the same object twice during execution and finds a different value the second time, although the transaction has not changed the value in the meantime.
A transaction re-executes a query returning a set of rows that satisfy a search condition and finds that the set of rows satisfying the condition has changed as a result of another recently committed transaction.
A more detailed explanation of the phantom phenomenon, including the.
A DML statement is any statement that updates or reads data within a table, that is, select, insert, update, fetch, and copy.
Figure 27.5 Definition of the four standard SQL isolation levels.
It should be obvious that each of the above phenomena violates transaction isolation, and hence violates serializability.
Figure 27.5 shows the definition of the four SQL isolation levels specified in the SQL standard—read uncommitted, read committed, repeatable read, and serializable—in terms of these phenomena.
PostgreSQL supports two of the four different isolation levels, read committed (which is the default isolation level in PostgreSQL) and serializable.
However, the PostgreSQL implementation of the serializable isolation level uses snapshot isolation, which does not truly ensure serializability as we have seen earlier in Section 15.7
The MVCC scheme used in PostgreSQL is an implementation of the snapshot isolation protocol which we saw in Section 15.7
The key idea behind MVCC is to maintain different versions of each row that correspond to instances of the row at different points in time.
This allows a transaction to see a consistent snapshot of the data, by selecting the most recent version of each row that was committed before taking the snapshot.
TheMVCCprotocol uses snapshots to ensure that every transaction sees a consistent view of the database: before executing a command, the transaction chooses a snapshot of the data and processes the row versions that are either in the snapshot or created by earlier commands of the same transaction.
This view of the data is “consistent” since it only takes full transactions into account, but the snapshot is not necessarily equal to the current state of the data.
The motivation for using MVCC is that readers never block writers and vice versa.
Readers access the most recent version of a row that is part of the transaction’s snapshot.
Writers create their own separate copy of the row to be updated.
Section 27.4.1.3 shows that the only conflict that causes a transaction to be blocked arises if two writers try to update the same row.
In contrast, under the standard two-phase locking approach, both readers and writers might be blocked, since there is only one version of each data object and both read and write operations are required to obtain a lock before accessing any data.
The MVCC scheme in PostgreSQL implements the first-updater-wins version of the snapshot isolation protocol, by acquiring exclusive locks on rows that are written, but using a snapshot (without any locking) when reading rows;
At the core of PostgreSQL MVCC is the notion of tuple visibility.
A PostgreSQL tuple refers to a version of a row.
Tuple visibility defines which of the potentially many versions of a row in a table is valid within the context of a given statement or transaction.
A transaction determines tuple visibility based on a database snapshot that is chosen before executing a command.
A tuple is visible for a transaction T if the following two conditions hold:
The tuple was created by a transaction that committed before transaction T took its snapshot.
To be precise, a tuple is also visible to T if it was created by T and not subsequently updated by T.
We omit the details of this special case for simplicity.
The goal of the above conditions is to ensure that each transaction sees a consistent view of the data.
PostgreSQL maintains the following state information to check these conditions efficiently:
A transaction ID, which at the same time serves as a timestamp, is assigned to every transaction at transaction start time.
PostgreSQL uses a logical counter (as described in Section 15.4.1) for assigning transaction IDs.
A log file called pg clog contains the current status of each transaction.
The status can be either in progress, committed, or aborted.
A SnapshotData data structure is created either at transaction start time or at query start time, depending on the isolation level (described in more detail below)
Its main purpose is to decide whether a tuple is visible to the current command.
TheSnapshotData stores information about the state of transactions at the time it is created, which includes a list of active transactions and xmax, a value equal to 1 + the highest ID of any transaction that has started so far.
The value xmax serves as a “cutoff” for transactions that may be considered visible.
The department table has three columns, the name of the department, the building where the department is located, and the budget of the department.
Figure 27.6 shows a fragment of the department table containing only the (versions of) the row corresponding to the Physics department.
Figure 27.6 also shows a fragment of the corresponding pg clog file.
Given the above state information, the two conditions that need to be satisfied for a tuple to be visible can be rewritten as follows:
Moreover, transaction 106 is still in progress, which violates another one of the conditions.
The second version of the row meets all the conditions for tuple visibility.
The details of how PostgreSQL MVCC interacts with the execution of SQL statements depends on whether the statement is an insert, select, update, or delete statement.
The simplest case is an insert statement, which may simply create a new tuple based on the data in the statement, initialize the tuple header (the creation ID), and insert the new tuple into the table.
When the systemexecutes a select,update, ordelete statement the interaction with theMVCCprotocol dependson the isolation level specifiedby the application.
If the isolation level is read committed, the processing of a new statement begins with creating a new SnapshotData data structure (independent of whether the statement starts a new transaction or is part of an existing transaction)
Next, the system identifies target tuples, that is, the tuples that are visible with respect to the SnapshotData and that match the search criteria of the statement.
In the case of a select statement, the set of target tuples make up the result of the query.
In the case of an update or delete statement in read committedmode, an extra step is necessary after identifying the target tuples, before the actual update or.
The reason is that visibility of a tuple ensures only that the tuple has been created by a transaction that committed before the update/delete statement in question started.
However, it is possible that, since query start, this tuple has been updated or deleted by another concurrently executing transaction.
This can be detected by looking at the expire-transaction ID of the tuple.
If the expire-transaction ID corresponds to a transaction that is still in progress, it is necessary towait for the completion of this transaction first.
If the transaction aborts, the update or delete statement can proceed and perform the actualmodification.
If the transaction commits, the search criteria of the statement need to be evaluated again, and only if the tuple still meets these criteria can the row be modified.
If the row is to be deleted, the main step is to update the expire-transaction ID of the old tuple.
It would then re-evaluate the search condition and, only if it is still met, proceed with its update.
Using the protocol described above for update and delete statements provides only the read-committed isolation level.
Since each query within a transaction may see a different snapshot of the database, a query in a transaction might see the effects of an update command completed in the meantime that weren’t visible to earlier queries within the same transaction.
Following the same line of thought, phantom reads are possible when a relation is modified between queries.
In order to provide the PostgreSQL serializable isolation level, PostgreSQL MVCC eliminates violations of serializability in two ways: First, when it is determining tuple visibility, all queries within a transaction use a snapshot as of the start of the transaction, rather than the start of the individual query.
This way successive queries within a transaction always see the same data.
Second, the way updates and deletes are processed is different in serializable mode compared to read-committed mode.
As in read-committed mode, transactionswait after identifying a visible target row thatmeets the search condition and is currently updated or deleted by another concurrent transaction.
Therefore, the waiting transaction is rolled back and returns with the error message “could not serialize access due to concurrent update.”
It is up to the application to handle an error message like the above appropriately, by aborting the current transaction and restarting the entire transaction from the beginning.
It is still the case that select statements never conflict with any other transactions.
The implications of PostgreSQL MVCC are described in more detail below.
Creating and storing multiple versions of every row can lead to excessive storage consumption.
To alleviate this problem, PostgreSQL frees up space when possible by identifying and deleting versions of tuples that cannot be visible to any active or future transactions, and are therefore no longer needed.
The task of freeing space is nontrivial, because indices may refer to the location of an unneeded tuple, so these references need to be deleted before reusing the space.
To lessen this issue, PostgreSQL avoids indexing multiple versions of a tuple that have identical index attributes.
This allows the space taken by nonindexed tuples to be freed efficiently by any transaction that finds such a tuple.
Formore aggressive space reuse, PostgreSQL provides the vacuum command, which correctly updates indices for each freed tuple.
PostgreSQL employs a background process to vacuum tables automatically, but the command can also be executed by the user directly.
The vacuum command offers two modes of operation: Plain vacuum simply identifies tuples that are not needed, and makes their space available for reuse.
This form of the command can operate in parallel with normal reading and writing of the table.
Vacuum full does more extensive processing, including moving of tuples across blocks to try to compact the table to the minimum number of disk blocks.
This form is much slower and requires an exclusive lock on each table while it is being processed.
Because of the use ofmultiversion concurrency control in PostgreSQL, porting applications fromother environments to PostgreSQLmight require someextra care to ensure data consistency.
As an example, consider a transaction TA executing a select statement.
Since readers in PostgreSQL don’t lock data, data read and selected by TA can be overwritten by another concurrent transaction TB , while TA is still running.
As a result some of the data that TA returns might not be current anymore at the time of completion of TA.
To ensure the current validity of a row and protect it against concurrent updates, an application must either use select for share or explicitly acquire a lock with the appropriate lock table command.
PostgreSQL’s approach to concurrency control performs best for workloads containing many more reads than updates, since in this case there is a very low chance that two updates will conflict and force a transaction to roll back.
The MVCC mechanisms described in the previous section do not protect transactions against operations that affect entire tables, for example, transactions that drop a table or change the schema of a table.
Toward this end, PostgreSQL provides explicit locks that DDL commands are forced to acquire before starting their execution.
These locks are always table based (rather than row based) and are acquired and released in accordance with the strict two-phase locking protocol.
The names of the lock types are often historical anddon’t necessarily reflect the use of the lock.
For example, all the locks are table-level locks, although some contain the word “row” in the name.
These three lock types are compatible with each other, since MVCC takes care of protecting these operations against each other.
While their main purpose is providing PostgreSQL internal concurrency control for DDL commands, all locks in Figure 27.8 can also be acquired explicitly by PostgreSQL applications through the lock table command.
Locks are recorded in a lock table that is implemented as a shared-memory hash table keyed by a signature that identifies the object being locked.
If a transaction wants to acquire a lock on an object that is held by another transaction in a conflicting mode, it needs to wait until the lock is released.
Lock waits are implemented through semaphores, each of which is associated with a unique transaction.
When waiting for a lock, a transaction actually waits on the semaphore associatedwith the transaction holding the lock.
Once the lock holder releases the lock, it will signal the waiting transaction(s) through the semaphore.
By implementing lock waits on a per-lock-holder basis, rather than on a per-object basis, PostgreSQL requires at most one semaphore per concurrent transaction, rather than one semaphore per lockable object.
By default, deadlock detection is triggered if a transaction has been waiting for a lock for more than 1 second.
The deadlock-detection algorithm constructs a wait-for graph based on the information in the lock table and searches this graph for circular dependencies.
If it finds any,meaning a deadlockwas detected, the transaction that triggered the deadlock detection aborts and returns an error to the user.
If no cycle is detected, the transaction continues waiting on the lock.
Unlike some commercial systems, PostgreSQL does not tune the lock time-out parameter dynamically, but it allows the administrator to tune it manually.
Ideally, this parameter should be chosen on the order of a transaction lifetime, in order to optimize the trade-off between the time it takes to detect a deadlock and the work wasted for running the deadlock detection algorithm when there is no deadlock.
All current types of indices in PostgreSQL allow for concurrent access by multiple transactions.
This is typically enabled by page-level locks, so that different transactions may access the index in parallel if they do not request conflicting locks on a page.
These locks are usually held for a short time to avoid deadlock, with the exception of hash indices, which lock pages for longer periods and may participate in deadlock.
Historically, PostgreSQL did not use write-ahead logging (WAL) for recovery, and therefore was not able to guarantee consistency in the case of crash.
The approach is similar to standard recovery techniques such as ARIES (Section 16.8), but recovery in PostgreSQL is simplified in some ways because of the MVCC protocol.
First, under PostgreSQL, recovery doesn’t have to undo the effects of aborted transactions: an aborting transaction makes an entry in the pg clog file, recording the fact that it is aborting.
Consequently, all versions of rows it leaves behind will never be visible to any other transactions.
The only case where this approach could potentially lead to problems is when a transaction aborts because of a crash of the corresponding PostgreSQL process and the PostgreSQL process doesn’t have a chance to create the pg clog entry before the crash.
PostgreSQL handles this as follows: Before checking the status of another transaction in the pg clog file, it checks whether the transaction is running on any of the PostgreSQL processes.
If no PostgreSQL process is currently running the transaction and the pg clog file shows the transaction as still running, it is safe to assume that the transaction crashed and the transaction’s pg clog entry is updated to “aborted”
Second, recovery is simplified by the fact that PostgreSQLMVCC already keeps track of some of the information required by WAL logging.
More precisely, there is no need for logging the start, commit, and abort of transactions, since MVCC logs the status of every transaction in the pg clog.
As a step toward these goals, PostgreSQL relies on “cooked” file systems, instead of handling the physical layout of data on raw disk partitions by itself.
PostgreSQL maintains a list of directories in the file hierarchy to use for storage, which are conventionally referred to as tablespaces.
Each PostgreSQL installation is initializedwith adefault tablespace, and additional tablespaces may be added at any time.
When creating a table, index, or entire database, the user may specify any existing tablespace in which to store the related files.
It is particularly useful to create multiple tablespaces if they reside on different physical devices, so that the faster devices may be dedicated to data that are in higher demand.
Moreover, data that are stored on separate disks may be accessed in parallel more efficiently.
The design of the PostgreSQL storage system potentially leads to some performance limitations, due to clashes between PostgreSQL and the file system.
The use of cooked file systems results in double buffering, where blocks are first fetched from disk into the file system’s cache (in kernel space) before being copied to PostgreSQL’s buffer pool.
Performance can also be limited by the fact that PostgreSQL stores data in 8-KB blocks, which may not match the block size used by the kernel.
It is possible to change the PostgreSQL block size when the server is installed, but this may have undesired consequences: small blocks limit.
On the other hand, modern enterprises increasingly use external storage systems, such as network-attached storage and storage-area networks, instead of disks attached to servers.
The philosophy here is that storage is a service that is easily administered and tuned for performance separately.
One approach used by these systems is RAID, which offers both parallelism and redundant storage as explained in Section 10.3
PostgreSQLmay directly leverage these technologies because of its reliance on cooked file systems.
Thus, the feeling of many PostgreSQL developers is that, for a vast majority of applications, and indeed PostgreSQL’s audience, the performance limitations are minimal and justified by the ease of administration and management, as well as simplicity of implementation.
The primary unit of storage in PostgreSQL is a table.
These files use a form of the standard slotted-page format described in Section 10.5
In each page, a header is followed by an array of “line pointers.”A line pointer holds the offset (relative to the start of the page) and length of a specific tuple in the page.
The actual tuples are stored in reverse order of line pointers from the end of the page.
A record in a heap file is identified by its tuple ID (TID)
The slot ID is an index into the line pointer array that in turn is used to access the tuple.
Although this infrastructure permits tuples in a page to be deletedor updated, under PostgreSQL’sMVCC approach, neither operation actually deletes or replaces old versions of rows immediately.
As explained in Section 27.4.1.4, expired tuples may be physically deleted by later commands, causing holes to be formed in a.
The indirection of accessing tuples through the line pointer array permits the reuse of such holes.
The length of a tuple is normally limited by the length of a data page.
When PostgreSQL encounters such a large tuple, it tries to “toast” individual large attributes.
In some cases, toasting an attribute may be accomplished by compressing the value.
If this does not shrink the tuple enough to fit in the page (often the case), the data in the toasted attribute is replaced with a reference to a copy that is stored outside the page.
A PostgreSQL index is a data structure that provides a dynamic mapping from search predicates to sequences of tuple IDs from a particular table.
The returned tuples are intended to match the search predicate, although in some cases the predicatemust be rechecked in the heapfile.
PostgreSQL supports several different index types, including those that are based on user-extensible access methods.
Although an access method may use a different page format, all the indices available in PostgreSQL use the same slotted-page format described above in Section 27.5.1
These indices are useful for equality and range queries on sortable data and also for certain pattern-matching operations such as the like expression.
PostgreSQL’s hash indices are an implementation of linear hashing (for more information on hash indices, see Section 11.6.3)
Moreover, hash indices are the only indices in PostgreSQL that do not support crash recovery.
Thus it is almost always preferable to use B-tree indices instead of hash indices.
Examples of some indices built using GiST include B-trees and R-trees, as well as less conventional indices for multidimensional cubes and full-text search.
New GiST access methods can be implemented by creating an operator class as explained in Section 27.3.3.3
Operator classes for GiST are different from Btrees, as each GiST operator class may have a different set of strategies that.
GiST also relies on seven support functions for operations such as testing set membership and splitting sets of entries for page overflows.
This allowed R-trees to take advantage of the WAL logging and concurrency capabilities that were added to GiST in version 8.1
Since the original Rtree implementation did not have these features, this change illustrates the benefits of an extensible indexing method.
See the bibliographical notes for references to more information on the GiST index.
The newest type of index in PostgreSQL is the Generalized Inverted Index (GIN)
A GIN index interprets both index keys and search keys as sets, making the index type appropriate for set-oriented operators.
One of the intended uses of GIN is to index documents for full-text search, which is implemented by reducing documents and queries to sets of search terms.
Like GiST, a GIN index may be extended to handle any comparison operator by creating an operator class with appropriate support functions.
To evaluate a search, GIN efficiently identifies index keys that overlap the search key, and computes a bitmap indicating which searched-for elements are members of the index key.
This is accomplished using support functions that extract members from a set and compare individual members.
Another support function decides if the search predicate is satisfied based on the bitmap and the original predicate.
If the search predicate cannot be resolved without the full indexed attribute, the decision function must report a match and recheck the predicate in the heap file.
For some of the index types described above, PostgreSQL supports more complex variations such as:
These are useful for conjuncts of predicates over multiple columns of a table.
Multicolumn indices are only supported for B-tree and GiST indices.
Unique and primary-key constraints can be enforced by using unique indices in PostgreSQL.
In PostgreSQL, it is possible to create indices on arbitrary scalar expressions of columns, and not just specific columns, of a table.
This is especially useful when the expressions in question are “expensive” —say, involving complicated user-defined computation.
An example is to support case-insensitive comparisons by defining an index on the expression lower(column) and using the predicate lower(column) = ’value’ in queries.
One disadvantage is that the maintenance costs of indices on expressions is high.
The specific comparison functions used to build, maintain, and use an index on a column are tied to the data type of that column.
Each data type has associated with it a default “operator class” (described in Section 27.3.3.3) that identifies the actual operators that would normally be used for it.
While this default operator class is normally sufficient for most uses, some data types might possess multiple “meaningful” classes.
For instance, in dealing with complex numbers, it might be desirable to index either the real or imaginary component.
PostgreSQL provides some built-in operator classes for pattern-matching operations (such as like) on text data that do not use the standard locale-specific collation rules (in other words, language specific sort orders)
These are indices built over a subset of a table defined by a predicate.
The index contains only entries for tuples that satisfy the predicate.
Partial indices are suited for cases where a column might contain a large number of occurrences of a very small number of values.
In such cases, the common values are not worth indexing, since index scans are not beneficial for queries that require a large subset of the base table.
A partial index that excludes the common values is small and incurs less I/O.
The partial indices are less expensive to maintain, as a large fraction of inserts do not participate in the index.
An index may be added to the database using the create index command.
For example, the following DDL statement creates a B-tree index on instructor salaries.
This statement is executed by scanning the instructor relation to find row versions that might be visible to a future transaction, then sorting their index attributes and building the index structure.
During this process, the building transaction holds a lock on the instructor relation that prevents concurrent insert, delete, and update statements.
Once the process is finished, the index is ready to use and the table lock is released.
The lock acquired by the create index command may present a major inconvenience for some applications where it is difficult to suspend updates while the index is built.
For these cases, PostgreSQL provides the create index concurrently variant, which allows concurrent updates during index construction.
This is achieved by a more complex construction algorithm that scans the base table twice.
The first table scan builds an initial version of the index, in a way similar to normal index construction described above.
This index may be missing tuples if the table was concurrently updated; however, the index is well formed, so it is flagged as being ready for insertions.
Finally, the algorithm scans the table a second time and inserts all tuples it finds that still need to be indexed.
This scan may also miss concurrently updated tuples, but the algorithm synchronizes with other transactions to guarantee that tuples that are updated during the second.
Hence, the index is ready to use after the second table scan.
Since this two-pass approach can be expensive, the plain create index command is preferred if it is easy to suspend table updates temporarily.
When PostgreSQL receives a query, it is first parsed into an internal representation, which goes through a series of transformations, resulting in a query plan that is used by the executor to process the query.
The first stage of a query’s transformation is rewrite and it is this stage that is responsible for the PostgreSQL rules system.
As explained in Section 27.3, in PostgreSQL, users can create rules that are fired ondifferent events such asupdate, delete, insert, and select statements.
A view is implemented by the system by converting a view definition into a select rule.
When a query involving a select statement on the view is received, the select rule for the view is fired, and the query is rewritten using the definition of the view.
A rule is registered in the system using the create rule command, at which point information on the rule is stored in the catalog.
This catalog is then used during query rewrite to uncover all candidate rules for a given query.
The rewrite phase first deals with all update, delete, and insert statements by firing all appropriate rules.
Subsequently, all the remaining rules involving only select statements are fired.
Since the firing of a rule may cause the query to be rewritten to a form that may require another rule to be fired, the rules are repeatedly checked on each form of the rewritten query until a fixed point is reached and no more rules need to be fired.
There exist no default rules in PostgreSQL—only those defined explicitly by users and implicitly by the definition of views.
Once the query has been rewritten, it is subject to the planning and optimization phase.
Here, each query block is treated in isolation and a plan is generated for it.
This planning begins bottom-up from the rewritten query’s innermost subquery, proceeding to its outermost query block.
The optimizer in PostgreSQL is, for the most part, cost based.
The idea is to generate an access planwhose estimated cost isminimal.
The cost model includes as parameters the I/O cost of sequential and random page fetches, as well as the CPU costs of processing heap tuples, index tuples, and simple predicates.
The actual process of optimization is based on one of the following two forms:
The standard planner uses the the bottom-up dynamic programming algorithm for join order optimization, originally used in System R, the pioneering relational system developed by IBM research in the 1970s.
The System R dynamic programming algorithm is described in detail in Section 13.4.1
The algorithm is used on a single query block at a time.
Genetic query optimizer.When the number of tables in a query block is very large, System R’s dynamic programming algorithm becomes very expensive.
Unlike other commercial systems that default to greedy or rule-based techniques, PostgreSQL uses a more radical approach: a genetic algorithm that was developed initially to solve traveling-salesman problems.
There exists anecdotal evidence of the successful use of genetic query optimization in production systems for queries with around 45 tables.
Since the planner operates in a bottom-up fashion on query blocks, it is able to perform certain transformations on the query plan as it is being built.
One example is the common subquery-to-join transformation that is present in many commercial systems (usually implemented in the rewrite phase)
When PostgreSQL encounters a noncorrelated subquery (such as one caused by a query on a view), it is generally possible to “pull up” the planned subquery andmerge it into the upper-level query block.
However, transformations that push duplicate elimination into lower-level query blocks are generally not possible in PostgreSQL.
The query-optimization phase results in a query plan that is a tree of relational operators.
Each operator represents a specific operation on one or more sets of tuples.
The operators can be unary (for example, sort, aggregation), binary (for example, nested-loop join), or n-ary (for example, set union)
Crucial to the cost model is an accurate estimate of the total number of tuples thatwill be processed at each operator in the plan.
This is inferred by the optimizer on the basis of statistics that are maintained on each relation in the system.
In addition, PostgreSQL also maintains a statistical correlation between the physical and logical row orderings of a column’s values —this indicates the cost of an index scan to retrieve tuples that pass predicates on the column.
The DBA must ensure that these statistics are current by running the analyze command periodically.
The executor module is responsible for processing a query plan produced by the optimizer.
The executor follows the iterator model with a set of four functions implemented for each operator (open, next, rescan, and close)
Iterators are also discussed as part of demand-driven pipelining in Section 12.7.2.1
PostgreSQL iterators have an extra function, rescan, which is used to reset a subplan (say for an inner loop of a join) with parameters such as index key ranges.
Some of the important operators of the executor can be categorized as follows:
The actual access methods that are used to retrieve data from on-disk objects in PostgreSQL are sequential scans of heap files, index scans, and bitmap index scans.
The tuples of a relation are scanned sequentially from the first to last blocks of the file.
Each tuple is returned to the caller only if it is “visible” according to the transaction isolation rules in Section 27.4.1.3
Given a search condition such as a range or equality predicate, this access method returns a set of matching tuples from the associated heap file.
The operator processes one tuple at a time, starting by reading an entry from the index and then fetching the corresponding tuple from the heap file.
This can result in a random page fetch for each tuple in the worst case.
A bitmap index scan reduces the danger of excessive random page fetches in index scans.
The first phase reads all index entries and stores the heap tuple IDs in a bitmap, and the second phase fetches the matching heap tuples in sequential order.
This guarantees that each heap page is accessed only once, and increases the chance of sequential page fetches.
Moreover, bitmaps from multiple indexes can be merged and intersected to evaluate complex Boolean predicates before accessing the heap.
PostgreSQL supports three join methods: sorted merge joins, nested-loop joins (including index-nested loop variants for the inner), and a hybrid hash join (Section 12.5)
Sort.External sorting is implemented in PostgreSQLby algorithms explained in Section 12.4
The input is divided into sorted runs that are thenmerged in a polyphase merge.
The initial runs are formed using replacement selection, using a priority tree instead of a data structure that fixes the number of inmemory records.
This is because PostgreSQL may deal with tuples that vary considerably in size and tries to ensure full utilization of the configured sort memory space.
Grouped aggregation in PostgreSQL can be either sort-based or hash-based.
When the estimated number of distinct groups is very large the former is used and otherwise the hash-based approach is preferred.
In PostgreSQL (unlike some commercial systems) active-database features such as triggers and constraints are not implemented in the rewrite phase.
Instead they are implemented as part of the query executor.
The executor processes an update, delete, and insert statement by repeatedly generating tuple changes for a relation.
For each row modification, the executor explicitly identifies, fires, and enforces candidate triggers and constraints, before or after the change as required.
A running PostgreSQL site is managed by a central coordinating process, called the postmaster.
The postmaster process is responsible for initializing and shutting down the server and also for handling connection requests from new clients.
The postmaster assigns each new connecting client to a back-end server process that is responsible for executing the queries on behalf of the client and for returning the results to the client.
Client applications can connect to the PostgreSQL server and submit queries through one of the many database application programmer interfaces supported by PostgreSQL (libpq, JDBC, ODBC, Perl DBD) that are provided as client-side libraries.
An example client application is the command-line psql program, included in the standard PostgreSQL distribution.
The postmaster is responsible for handling the initial client connections.
For this, it constantly listens for new connections on a known port.
After performing initialization steps such as user authentication, the postmaster will spawn a new back-end server process to handle the new client.
After this initial connection, the client interacts only with the back-end server process, submitting queries and receiving query results.
The back-end server process is responsible for executing the queries submitted by the client by performing the necessary query-execution steps, including parsing, optimization, and execution.
In order to execute more than one query in parallel, an application must maintain multiple connections to the server.
At any given time, there may be multiple clients connected to the system and thus multiple back-end server processes may be executing concurrently.
The back-end server processes access database data through the main-memory buffer pool, which is placed in shared memory, so that all the processes have the same view of the data.
Shared memory is also used to implement other forms of synchronization between the server processes, for example, the locking of data items.
The use of shared memory as a communication medium suggests that a PostgreSQL server should run on a single machine; a single-server site cannot be spread across multiple machines without the assistance of third-party packages, such as the Slony replication tool.However, it is possible to build a shared-nothing parallel database systemwith an instance of PostgreSQL running on each node; in fact, several commercial parallel database systems have been built with exactly this architecture, as described in Section 18.8
This Web site is the authoritative source for information on new releases of PostgreSQL, which occur on a frequent basis.
Until PostgreSQL version 8, the only way to run PostgreSQL under Microsoft Windows was by using Cygwin.
Cygwin is a Linuxlike environment that allows rebuilding of Linux applications from source to run under Windows.
Rules as used in PostgreSQL are presented in Stonebraker et al.
These include the pgtcl library and the pgAccess administration tool mentioned in this chapter.
An open-source alternative to PostgreSQL is MySQL, which is available for noncommercial use under the GNU General Public License.
MySQL may be embedded in commercial software that does not have freely distributed source code, but this requires a special license to be purchased.
Comparisons between themost recent versions of the two systems are readily available on the Web.
The company, which was later renamed Oracle, set out to build a relational database management system as a commercial product, and became a pioneer of the RDBMS market and has held a leading position in this market ever since.
Over the years, its product and service offerings have grown beyond the relational database server to include middleware and applications.
In addition to products developed inside the company, Oracle’s offerings include software that was originally developed in companies that Oracle acquired.
Oracle’s acquisitions have ranged from small companies to large, publicly traded ones, including Peoplesoft, Siebel, Hyperion, and BEA.
As a result of these acquisitions, Oracle has a very broad portfolio of enterprise software products.
This chapter is focused onOracle’smain relational database server and closely related products.
New versions of the products are being developed continually, so all product descriptions are subject to change.
The feature set described here is based on the first release of Oracle11g, which is Oracle’s flagship database product.
Oracle provides a variety of tools for database design, querying, report generation and data analysis, including OLAP.
These tools, along with various other application development tools, are part of a portfolio of software products called Oracle FusionMiddleware.
Products include both traditional tools using Oracle’s PL/SQL programming language and newer ones based on Java/J2EE technologies.
The Model and Business Services layers handle the interaction with the data sources and contains the business logic.
The View layer handles the user interface, and the Controller layer handles the flow of the application and the interaction between the other layers.
Oracle Designer is a database design tool, which translates business logic and data flows into schema definitions and procedural scripts for application logic.
It supports such modeling techniques as E-R diagrams, information engineering, and object analysis and design.
Oracle also has an application development tool for data warehousing, Oracle Warehouse Builder.
Warehouse Builder is a tool for design and deployment of all aspects of a data warehouse, including schema design, data mapping and transformations, data load processing, and metadata management.
Oracle Warehouse Builder supports both 3NF and star schemas and can also import designs from Oracle Designer.
This tool, in conjunction with database features, such as external tables and table functions, typically eliminates the need for third-party extraction, transformation, and loading (ETL) tools.
Oracle provides tools for ad hoc querying, report generation, and data analysis, including OLAP.
Oracle Business Intelligence Suite (OBI) is a comprehensive suite of tools sharing a common service-oriented architecture.
Components include a Business Intelligence server and tools for ad hoc querying, dashboard generation, reporting, and alerting.
The components share infrastructure and services for data access and metadata management and have a common security model and administration tool.
The component for ad hoc querying, Oracle BI Answers, is an interactive tool that presents the user with a logical view of the data hiding the details of the physical implementation.
Objects available to the user are displayed graphically and the user can build a query with a point-and-click interface.
This logical query is sent to theOracle BI Server component,which then generates the physical query or queries.
Multiple physical data sources are supported, and a query could combine data stored in relational databases, OLAP sources, and Excel spreadsheets.
Results can be presented as charts, reports, pivot tables, or dashboards that are drillable and can be saved and later modified.
These are used to store objects while providing a relational view of the attributes of the objects.
These are functions that produce sets of rows as output, and can be used in the from clause of a query.
If a table function is used to express some formof data transformation, nesting multiple functions allows multiple transformations to be expressed in a single statement.
These provide a virtual object table view of data stored in a regular relational table.
They allowdata to be accessed or viewed in an objectoriented style even if the data are really stored in a traditional relational format.
Oracle XML DB provides in-database storage for XML data and support for a broad set of XML functionality including XML Schema and XQuery.
It is built on the XMLType abstract data type, which is treated as a native Oracle data type.
This format is usually space efficient and allows the use of a variety of standard relational features, such as B-tree indices, but incurs some overhead when mapping XML documents to the storage format and back.
It is mainly suitable for XML data that are highly structured and the mapping includes a manageable number of relational tables and joins.
This representation does not require any mapping and provides high throughput when inserting or retrieving an entire XML document.
However, it is usually not very space efficient and provides for less intelligent processing when operating on parts of an XML document.
It is more space efficient than unstructured storage and can handle operations against parts of an XML document.
It is also better than the structured format at handling data that are highly unstructured, but may not always be as space efficient.
This format may make the processing of XQuery statements less efficient than when the structured format is used.
Both the binary andunstructured representation can be indexedwith a special type of index called XMLIndex.
This type of index allows document fragments to be indexed based on their corresponding XPath expression.
Storing XML data inside the database means that they get the benefit of Oracle’s functionality in areas such as backup, recovery, security, and query processing.
It allows for accessing relational data as part of doing XML processing as well as accessing XML data as part of doing SQL processing.
An XSLT process that lets XSL transformations be performed inside the database.
An XPath rewrite optimization that can speed up queries against data stored in object-relational representation.
By translating an expression used in an XQuery into conditions directly on the object-relational columns, regular indices on these columns can be used to speed up query processing.
PL/SQL was Oracle’s original language for stored procedures and it has syntax similar to that used in the Ada language.
Java is supported through a Java virtual machine inside the database engine.
Oracle provides a package to encapsulate related procedures, functions, and variables into single units.
Oracle supports SQLJ (SQL embedded in Java) and JDBC, and provides a tool to generate Java class definitions corresponding to user-defined database types.
Dimensional modeling is a commonly used design technique for relational star schemas as well as multidimensional databases.
Oracle supports the creation of dimensions as metadata objects in order to support query processing against databases designed based on this technique.
The metadata objects can be used to store information about various kinds of attributes of a dimension, but perhaps more importantly, about hierarchical relationships.
Oracle provides support for analytical database processing in several different ways.
Oracle provides native multidimensional storage inside the relational database server.
The multidimensional data structures allow for array-based access to the data, and, in the right circumstances, this type of access can be vastly more efficient than traditional relational access methods.
Using these data structures as an integrated part of a relational database provides a choice of storing data in a relational or multidimensional format while still taking advantage of Oracle features in areas such as backup and recovery, security, and administration tools.
Oracle provides storage containers for multidimensional data known as analytic workspaces.
An analytic workspace contains both the dimensional data and measures (or facts) of an OLAP cube and is stored inside an Oracle table.
From a traditional relational perspective, a cube stored inside a table would be an opaque object where the data could not normally be interpreted directly in terms of the table’s rows and columns.
However, Oracle’s OLAP server inside the database has the knowledge to interpret and access the data and makes it possible to give SQL access to it as if it had been stored in a regular table format.
Hence, it is possible to store data in either a multidimensional format or a traditional relational format, depending on what is optimal, and still be able to join data stored in both types of representations in a single SQLquery.
In addition to Oracle’s OLAP support inside its relational database, Oracle’s product suite includes Essbase.
Essbase is a widely used multidimensional database that came to be part of Oracle with the acquisition of Hyperion.
Oracle provides several types of triggers and several options for when and how they are invoked.
See Section 5.3 for an introduction to triggers in SQL.
Triggers can be written in PL/SQL or Java or as C callouts.
For triggers that execute on DML statements such as insert, update, and delete, Oracle supports row triggers and statement triggers.
Row triggers execute once for every row that is affected (updated or deleted, for example) by the DML operation.
In each case, the trigger can be defined as either a before or after trigger, depending on whether it is to be invoked before or after the DML operation is carried out.
Oracle allows the creation of instead of triggers for views that cannot be subject to DML operations.
Depending on the view definition, it may not be possible for Oracle to translate a DML statement on a view to modifications of the underlying base tables unambiguously.
Hence, DML operations on views are subject to numerous restrictions.
A user can create an instead of trigger on a view to specify manually what operations on the base tables are to be carried out in response to a DML operation on the view.Oracle executes the trigger instead of the.
Oracle also has triggers that execute on a variety of other events, such as database start-up or shutdown, server error messages, user logon or logoff, and DDL statements such as create, alter and drop statements.
In Oracle parlance, a database consists of information stored in files and is accessed through an instance, which is a sharedmemory area and a set of processes that interact with the data in the files.
The control file is a small file that contains some very high-level metadata required to start or operate an instance.
The storage structure of the regular data andmetadata is described in the next section.
A database consists of one or more logical storage units called table spaces.
Each table space, in turn, consists of one or more physical structures called data files.
These may be either part of a file system or raw devices.
The system and the auxiliary sysaux table spaces, which are always created.
They contain the data dictionary tables and storage for triggers and stored procedures.
While user data can be stored in the system table space, it is often desirable to separate the user data from the system data.
Usually, the decision about what other table spaces should be created is based on performance, availability, maintainability, and ease of administration.
For example, having multiple table spaces can be useful for partial backup and recovery operations.
The undo table space, which is used solely for storing undo information for transaction management and recovery.
Many database operations require sorting the data, and the sort routine may have to store data temporarily on disk if the sort cannot be done in memory.
Temporary table spaces are allocated for sorting and hashing to make the space management operations involved in spilling to disk more efficient.
Table spaces can also be used as a means of moving data between databases.
For example, it is common to move data from a transactional system to a data warehouse at regular intervals.
Oracle allows moving all the data in a table space from one system to the other by simply copying the data files and exporting and importing a small amount of data-dictionary metadata.
These operations can be much faster than unloading the data from one database and then using a loader.
The space in a table space is divided into units, called segments, each of which contains data for a specific data structure.
Each table in a table space has its own data segment where the table data are stored unless the table is partitioned; if so, there is one data segment per partition.
Each index in a table space has its own index segment, except for partitioned indices, which have one index segment per partition.
These are segments used when a sort operation needs to write data to disk or when data are inserted into a temporary table.
These segments contain undo information so that an uncommitted transaction can be rolled back.
These segments are automatically allocated in a special undo table space.
In older implementations of Oracle’s undo management, the term “rollback segment”was used.
Below the level of segment, space is allocated at a level of granularity called an extent.
Each extent consists of a set of contiguous database blocks.
A database block is the lowest level of granularity at which Oracle performs disk I/O.
A database block does not have to be the same as an operating system block in size, but should be a multiple thereof.
Oracle provides storage parameters that allow for detailed control of how space is allocated and managed, parameters such as:
The size of a new extent that is to be allocated to provide room for rows that are inserted into a table.
The percentage of space utilization at which a database block is considered full and at which no more rows will be inserted into that block.
Leaving some free space in a block can allow the existing rows to grow in size through updates, without running out of space in the block.
A standard table in Oracle is heap organized; that is, the storage location of a row in a table is not based on the values contained in the row, and is fixed when the row is inserted.
However, if the table is partitioned, the content of the row affects the partition in which it is stored.
Heap tables can optionally be compressed, as described in Section 28.3.3.2
Oracle supports nested tables; that is, a table can have a column whose data type is another table.
The nested table is not stored in line in the parent table, but is stored in a separate table.
Oracle supports temporary tables where the duration of the data is either the transaction inwhich the data are inserted, or the user session.
The data are private to the session and are automatically removed at the end of its duration.
A cluster is another form of file organization for table data, described earlier in Section 10.6.2where it is calledmultitable clustering.
The use of the term “cluster” in this context, should not be confused with other meanings of the word cluster, such as those relating to hardware architecture.
In a cluster file organization, rows from different tables are stored together in the same block on the basis of some common columns.
For example, a department table and an employee table could be clustered so that each row in the department table is stored together with all the employee rows for those employees who work in that department.
The primary key/foreign key values are used to determine the storage location.
The cluster organization implies that a row belongs in a specific place; for example, a new employee row must be inserted with the other rows for the same department.
Here, Oracle computes the location of a row by applying a hash function to the value for the cluster column.
The hash function maps the row to a specific block in the hash cluster.
Since no index traversal is needed to access a row according to its cluster column value, this organization can save significant amounts of disk I/O.
In an index-organized table (IOT), records are stored in an Oracle B-tree index instead of in a heap; this file organization is described earlier in Section 11.4.1, where it is called B+-tree file organization.
An IOT requires that a unique key be identified for use as the index key.
While an entry in a regular index contains the key value and row-id of the indexed row, an IOT replaces the row-id with the column values for the remaining columns of the row.
Compared to storing the data in a regular heap table and creating an index on the key columns, using an IOT can improve both performance and space utilization.
Consider looking up all the column values of a row, given its primary key value.
For a heap table, that would require an index probe followed by a table access by row-id.
Secondary indices on nonkey columns of an index-organized table are different from indices on a regular heap table.
In a heap table, each row has a fixed row-id that does not change.
However, a B-tree is reorganized as it grows or shrinks when entries are inserted or deleted, and there is no guarantee that a row will stay in a fixed place inside an IOT.
Hence, a secondary index on an IOT contains not normal row-ids, but logical row-ids instead.
A logical row-id consists of two parts: a physical row-id corresponding to where the row was when the index was created or last rebuilt and a value for the unique key.
The physical row-id is referred to as a “guess” since it could be incorrect if the row has been.
If so, the other part of a logical row-id, the key value for the row, is used to access the row; however, this access is slower than if the guess had been correct, since it involves a traversal of the B-tree for the IOT from the root all the way to the leaf nodes, potentially incurring several disk I/Os.
However, if a table is highly volatile and a large percentage of the guesses are likely to be wrong, it can be better to create the secondary index with only key values (as described in Section 11.4.1), since using an incorrect guess may result in a wasted disk I/O.
Oracle’s compression feature allows data to be stored in a compressed format, something that can drastically reduce the amount of space needed to store the data and the number of I/O operations needed to retrieve it.
Oracle’s compression method is a lossless dictionary-based algorithm that compresses each block individually.
All the information needed to uncompress a block is contained in that block itself.
The algorithm works by replacing repeated occurrences of a value in that blockwith pointers to an entry for that value in a symbol table (or dictionary) in the block.
Entries can be based on repeated values for individual columns or a combination of columns.
Oracle’s original table compression generated the compressed block format as the data were bulk-loaded into a table and was mainly intended for data warehousing environments.
A newer OLTP compression feature supports compression in conjunction with regular DML operations as well.
In the latter case, Oracle compresses blocks only after certain thresholds have been reached for how much data have been written into the block.
As a result, only transactions that cause a threshold to be passed will occur any overhead for compressing a block.
In addition to regular access control features such as passwords, user privileges, and user roles, Oracle supports several features to protect the data from unauthorized access, including:
Encryption can be enabled for an entire database or just for individual table columns.
The main motivation for this feature is to protect sensitive data outside the normally protected environment, such as when backup media is sent to a remote location.
This feature is aimed at providing a separation of duties for users with access to the database.
A database administrator is a highly privileged user that typically can do almost anythingwith the database.
However, it may be inappropriate or illegal to let that person access sensitive corporate financial data or personal information about other employees.
Database vault includes a variety of mechanisms that can be used to restrict or monitor access to sensitive data by highly privileged database users.
This feature, described earlier in Section 9.7.5, allows additional predicates to be automatically added to the where clause of a query that accesses a given table or view.
Typically, the feature would be used so that the additional predicate filters out all the rows that the user does not have the right to see.
For example, two users could submit identical queries to find all the employee information in the entire employee table.
However, if a policy exists that limits each user to seeing only the information for the employee number that matches the user ID, the automatically added predicateswill ensure that each query only returns the employee information for the user who submitted the query.
Hence, each user will be left with the impression of accessing a virtual database that contains only a subset of the data of the physical database.
The most commonly used type is a B-tree index, created on one ormultiple columns.
Oracle can optionally compress the prefix of the entry to save space.
Bitmap indices (described in Section 11.9) use a bitmap representation for index entries, which can lead to substantial space saving (and therefore disk I/O savings), when the indexed column has a moderate number of distinct values.
Bitmap indices in Oracle use the same kind of B-tree structure to store the entries as a regular index.
However, where a regular index on a column would have entries of the form <col1><row-id>, a bitmap index entry has the form:
The bitmap conceptually represents the space of all possible rows in the table between the start and end row-id.
The number of such possible rows in a block depends on howmany rows can fit into a block, which is a function of the number.
Each bit in the bitmap represents one such possible row in a block.
It is possible that the row does not actually exist because a table block may well have a smaller number of rows than the number that was calculated as the maximum possible.
If the difference is large, the result may be long strings of consecutive zeros in the bitmap, but the compression algorithm deals with such strings of zeros, so the negative effect is limited.
The compression algorithm is a variation of a compression technique called Byte-Aligned Bitmap Compression (BBC)
Essentially, a section of the bitmap where the distance between two consecutive 1s is small enough is stored as verbatim bitmaps.
Bitmap indices allowmultiple indices on the same table to be combined in the same access path if there aremultiple conditions on indexed columns in thewhere clause of a query.
Bitmaps from the different indices are retrieved and combined using Boolean operations corresponding to the conditions in the where clause.
All Boolean operations are performed directly on the compressed representation of the bitmaps—no decompression is necessary—and the resulting (compressed) bitmap represents those rows that match all the logical conditions.
The ability to use the Boolean operations to combine multiple indices is not limited to bitmap indices.
Oracle can convert row-ids to the compressed bitmap representation, so it can use a regular B-tree index anywhere in a Boolean tree of bitmap operation simply by putting a row-id-to-bitmap operator on top of the index access in the execution plan.
As a rule of thumb, bitmap indices tend to bemore space efficient than regular B-tree indices if the number of distinct key values is less than half the number of rows in the table.
For columns with a very small number of distinct values—for example, columns referring to properties such as country, state, gender, marital status, and various status flags—a bitmap index might require only a small fraction of the space of a regular B-tree index.
Any such space advantage can also give rise to corresponding performance advantages in the form of fewer disk I/Os when the index is scanned.
A function-based index can be created as either a bitmap or a B-tree index.
A join index is an index where the key columns are not in the table that is referenced by the row-ids in the index.
Oracle supports bitmap join indices primarily for use with star schemas (see Section 20.2.2)
For example, if there is a column for product names in a product dimension table, a bitmap join index on the fact table with this key column could be used to retrieve the fact table rows that correspond to a product with a specific name, although the name is not stored in the fact table.
How the rows in the fact and dimension tables correspond is based on a join condition that is specified when the index is created, and becomes part of the index metadata.
When a query is processed, the optimizer will look for the same join condition in the where clause of the query in order to determine if the join index is applicable.
Oracle can combine a bitmap join index on a fact table with other indices on the same table—whether join indices or not—by using the operators for Boolean bitmap operations.
Oracle allows tables to be indexed by index structures that are not native to Oracle.
This extensibility feature of the Oracle server allows software vendors to develop so-called cartridges with functionality for specific application domains, such as text, spatial data, and images, with indexing functionality beyond that provided by the standard Oracle index types.
In implementing the logic for creating, maintaining, and searching the index, the index designer must ensure that it adheres to a specific protocol in its interaction with the Oracle server.
A domain index must be registered in the data dictionary, together with the operators it supports.
Oracle’s optimizer considers domain indices as one of the possible access paths for a table.
Oracle allows cost functions to be registeredwith the operators so that the optimizer can compare the cost of using the domain index to those of other access paths.
For example, a domain index for advanced text searches may support an operator contains.
Once this operator has been registered, the domain index will be considered as an access path for a query like:
The domain index can be stored in either an external data file or inside an Oracle index-organized table.
A domain index can be combined with other (bitmap or B-tree) indices in the same access path by converting between the row-id and bitmap representation and using Boolean bitmap operations.
Oracle supports various kinds of horizontal partitioning of tables and indices, and this feature plays a major role in Oracle’s ability to support very large databases.
The ability to partition a table or index has advantages in many areas.
Backup and recovery are easier and faster, since they can be done on individual partitions rather than on the table as a whole.
Loading operations in a data warehousing environment are less intrusive: data can be added to a newly created partition, and then the partition added to a table,which is an instantaneous operation.
Likewise, dropping apartition with obsolete data froma table is very easy in adatawarehouse thatmaintains a rolling window of historical data.
Query performance benefits substantially, since the optimizer can recognize that only a subset of the partitions of a table need to be accessed in order to resolve a query (partition pruning)
Also, the optimizer can recognize that in a join, it is not necessary to try to match all rows in one table with all rows in the other, but that the joins need to be done only between matching pairs of partitions (partitionwise join)
An index on a partitioned table can be either a global index or a local index.
Entries in a global index can refer to rows in any partition.
A locally indexed table has one physical index for each partition that only contains entries for that partition.
Unless partition pruning restricts a query to a single partition, a table accessed through a local index will require multiple individual physical index probes.However, a local indexhas advantages indatawarehousing environments where new data can be loaded into a new partition and indexedwithout the need tomaintain any existing index.
Loading followed by index creation ismuchmore efficient than maintaining an existing index while the data are being loaded.
Similarly, dropping an old partition and the physical part of its local index can be done without causing any index maintenance.
Each row in a partitioned table is associated with a specific partition.
This association is based on the partitioning column or columns that are part of the definition of a partitioned table.
There are several ways to map column values to partitions, giving rise to several types of partitioning, each with different characteristics: range, hash, list, and composite partitioning.
In range partitioning, the partitioning criteria are ranges of values.
This type of partitioning is especially well suited to date columns, in which case all rows in the same date range, say a day or a month, belong in the same partition.
In a data warehouse where data are loaded from the transactional systems at regular intervals, range partitioning can be used to implement a rolling window of historical data efficiently.
Each data load gets its own new partition, making the loading process faster and more efficient.
The system actually loads the data into a separate table with the same column definition as the partitioned table.
It can then check the data for consistency, cleanse them, and index them.
After that, the system canmake the separate table a new partition of the partitioned table, by a simple change to the metadata in the data dictionary—a nearly instantaneous operation.
Up until the metadata change, the loading process does not affect the existing data in the partitioned table in any way.
There is no need to do any maintenance of existing indices as part of the loading.
Old data can be removed from a table by simply dropping its partition; this operation does not affect the other partitions.
In addition, queries in a data warehousing environment often contain conditions that restrict them to a certain time period, such as a quarter or month.
If date-range partitioning is used, the query optimizer can restrict the data access to those partitions that are relevant to the query, and avoid a scan of the entire table.
Partitions can either be created with explicitly set end points or be defined based on a fixed range, such as a day or a month.
In the latter case, called interval partitioning, the creation of the partition happens automatically under the covers when trying to insert a row with a value in a previously nonexistent interval.
In hash partitioning, a hash function maps rows to partitions according to the values in the partitioning columns.
This type of partitioning is primarily useful when it is important to distribute the rows evenly among partitions or when partitionwise joins are important for query performance.
In list partitioning, the values associated with a particular partition are stated in a list.
This type of partitioning is useful if the data in the partitioning column have a relatively small set of discrete values.
For instance, a table with a state column can be implicitly partitioned by geographical region if each partition list has the states that belong in the same region.
In composite partitioning, tables that are range, interval, or list partitioned can be subpartitioned by range, list, or hash.
For example, a table may be range partitioned on a date column and hash subpartitioned on a column that is frequently.
The subpartitioning allows partition-wise joins to be used when the table is joined.
In reference partitioning, the partitioning key is resolved based on a foreignkey constraint with another table.
The dependency between the tables allows maintenance operations to be automatically cascaded.
In addition, Oracle maintains thematerialized result, updating itwhen the tables thatwere referenced in the query are updated.
Materialized views are used in data warehousing to speed up query processing, but the technology is also used for replication in distributed and mobile environments.
In datawarehousing, a commonusage formaterialized views is to summarize data.
Oracle supports automatic query rewrites that take advantage of any useful materialized view when resolving a query.
The rewrite consists of changing the query to use the materialized view instead of the original tables in the query.
In addition, the rewrite may add additional joins or aggregate processing as may be required to get the correct result.
For example, if a queryneeds sales byquarter, the rewrite can take advantage of a view that materializes sales by month, by adding additional aggregation to roll up the months to quarters.
Oracle has a type of metadata object called dimension that allows hierarchical relationships in tables to be defined.
For example, for a time-dimension table in a star schema,Oracle can define a dimensionmetadata object to specify howdays roll up tomonths,months to quarters, quarters to years, and so forth.
Likewise, hierarchical properties relating to geography can be specified—for example, how sales districts roll up to regions.
The query rewrite logic looks at these relationships since they allow a materialized view to be used for wider classes of queries.
The container object for a materialized view is a table, which means that a materialized view can be indexed, partitioned, or subjected to other controls, to improve query performance.
When there are changes to the data in the tables referenced in the query that defines a materialized view, the materialized view must be refreshed to reflect those changes.
Oracle supports both complete refresh of a materialized view and fast, incremental refresh.
In a complete refresh, Oracle recomputes the materialized view from scratch, which may be the best option if the underlying tables have had significant changes, for example, changes due to a bulk load.
In a fast refresh, Oracle updates the view using the records that were changed in the underlying tables.
The refresh to the view can be executed on commit as part of.
Fast refresh may be better if the number of rows that were changed is low.
There are some restrictions on the classes of queries for which a materialized view can be incrementally refreshed (and others for when a materialized view can be created at all)
A materialized view is similar to an index in the sense that, while it can improve query performance, it uses up space, and creating and maintaining it consumes resources.
To help resolve this trade-off, Oracle provides an advisor that can help a user create the most cost-effective materialized views, given a particular query workload as input.
Oracle supports a large variety of processing techniques in its query processing engine.
Some of the more important ones are described here briefly.
Data can be accessed through a variety of access methods:
The query processor scans the entire table by getting information about the blocks that make up the table from the extent map, and scanning those blocks.
The processor creates a start and/or stop key from conditions in the query and uses it to scan to a relevant part of the index.
If there are columns that need to be retrieved, that are not part of the index, the index scan would be followed by a table access by index row-id.
If no start or stop key is available, the scan would be a full index scan.
The processor scans the extents the same way as the table extent in a full table scan.
If the index contains all the table columns that are needed for that table, and there are no good start/stop keys that would significantly reduce that portion of the index that would be scanned in a regular index scan, this method may be the fastest way to access the data.
This is because the fast full scan can take full advantage of multiblock disk I/O.
However, unlike a regular full scan, which traverses the index leaf blocks in order, a fast full scan does not guarantee that the output preserves the sort order of the index.
If a query needs only a small subset of the columns of a wide table, but no single index contains all those columns, the processor can use an index join to generate the relevant informationwithout accessing the table, by joining several indices that together contain the needed columns.
It performs the joins as hash joins on the row-ids from the different indices.
The processor accesses the data by using the cluster key.
Oracle has several ways to combine information from multiple indices in a single access path.
This ability allows multiple where-clause conditions to be used together to compute the result set as efficiently as possible.
The functionality includes the ability to performBoolean operations and, or, andminus on bitmaps representing row-ids.
There are also operators that map a list of row-ids into bitmaps and vice versa, which allows regular B-tree indices and bitmap indices to be used together in the same access path.
In addition, for many queries involving count(*) on selections on a table, the result can be computed by just counting the bits that are set in the bitmap generated by applying thewhere clause conditions, without accessing the table.
Oracle supports several types of joins in the execution engine: inner joins, outer joins, semijoins, and antijoins.
An antijoin in Oracle returns rows from the left-hand side input that do not match any row in the right-hand side input; this operation is called anti-semijoin in other literature.
It evaluates each type of join by one of three methods: hash join, sort–merge join, or nested-loop join.
One such step is to perform various query transformations and rewrites that fundamentally change the structure of the query.
Another step is to perform access path selection to determine access paths, join methods, and join order.
Since some transformations are not always beneficial, Oracle uses cost-based query transformations where the transformations and access path selection are interleaved.
For each transformation that is tried, access path selection is performed in order to generate a cost estimate, and the transformation is accepted or rejected based on the cost for the resulting execution plan.
Some of themajor types of transformations and rewrites supported by Oracle are as follows:
A view reference in a query is replaced by the viewdefinition.
Oracle offers this feature for certain classes of views that are not subject to regular viewmerging because they have a group by or select distinct in the view definition.
If such a view is joined to other tables, Oracle can commute the joins and the sort or hash operation used for the group by or distinct.
Oracle has a variety of transformations that convert various classes of subqueries into joins, semijoins, or antijoins.
Such conversion is also called decorrelation, and is described briefly in Section 13.4.4
Oracle has the ability to rewrite a query automatically to take advantage ofmaterialized views.
If some part of the query can be matched up with an existing materialized view, Oracle can replace that part of the query with a reference to the table in which the view is materialized.
If need be, Oracle adds join conditions or group by operations to preserve the semantics of the query.
If multiple materialized views are applicable, Oracle picks the one that gives the greatest advantage in reducing the amount of data that have to be processed.
In addition, Oracle subjects both the rewritten query and the original version to the full optimization process producing an execution plan and an associated cost estimate for each.
Oracle then decides whether to execute the rewritten or the original version of the query on the basis of the cost estimates.
Oracle supports a technique for evaluating queries against star schemas, known as the star transformation.
When a query contains a join of a fact table with dimension tables, and selections on attributes from the dimension tables, the query is transformed by deleting the join condition between the fact table and the dimension tables, and replacing the selection condition on each dimension table by a subquery of the form:
One such subquery is generated for each dimension that has some constraining predicate.
If the dimension has a snowflake schema (see Section 20.2), the subquery will contain a join of the applicable tables that make up the dimension.
Oracle uses the values that are returned from each subquery to probe an index on the corresponding fact table column, getting a bitmap as a result.
The bitmaps generated from different subqueries are combined by a bitmap and operation.
The resultant bitmap can be used to access matching fact table rows.
Hence, only those rows in the fact table that simultaneously match the conditions on the constrained dimensions will be accessed.
Both the decision on whether the use of a subquery for a particular dimension is cost-effective, and the decision on whether the rewritten query is better than the original, are based on the optimizer’s cost estimates.
Oracle has a cost-based optimizer that determines join order, join methods, and access paths.
Each operation that the optimizer considers has an associated cost.
In estimating the cost of an operation, the optimizer relies on statistics that have been computed for schema objects such as tables and indices.
The statistics contain information about the size of the object, the cardinality, the data distribution of table columns, and so forth.
Height-balanced histograms are also referred to as equi-depth histograms, and are described in Section 13.3.1
To facilitate the collection of optimizer statistics, Oracle can monitor modification activity on tables and keep track of those tables that have been subject to enough changes that recalculating the statistics may be appropriate.
Oracle also tracks what columns are used in where clauses of queries, which makes them potential candidates for histogram creation.
With a single command, a user can tell Oracle to refresh the statistics for those tables that were marked as sufficiently changed.
Oracle uses sampling to speed up the process of gathering the new statistics and automatically chooses the smallest adequate sample percentage.
It also determines whether the distribution of the marked columns merits the creation of histograms; if the distribution is close to uniform, Oracle uses a simpler representation of the column statistics.
In some cases, it may be impossible for the optimizer to accurately estimate the selectivity of a condition in the where clause of a query just based on simple column statistics.
Another class of problematic queries is those that have multiple predicates on columns that have some form of correlation.
Assessing the combined selectivity of those predicates may be hard.
Oracle therefore allows statistics to be created for expressions as well as for groups of columns.
In addition, Oracle can address these issues through dynamic sampling.
The optimizer can randomly sample a small portion of a table and apply all the relevant predicates to the sample to see the percentage of the rows that match.
This feature can also handle temporary tableswhere the lifespan and visibility of the datamay prevent regular statistics collection.
Oracle uses both CPU cost and disk I/Os in the optimizer cost model.
To balance the two components, it stores measures about CPU speed and disk I/O performance as part of the optimizer statistics.
For queries involving a nontrivial number of joins, the search space is an issue for a query optimizer.
The optimizer generates an initial join order and thendecides on the best joinmethods and access paths for that join order.
It then changes the order of the tables and determines the best join methods and access paths for the new join order and so forth, while keeping the best plan that has been found so far.
Oracle cuts the optimization short if the number of different join orders that have been considered becomes so large that the time spent in the optimizer may be noticeable compared to the time it would take to execute the best plan found so far.
Since this cutoff depends on the cost estimate for the best plan found so far, finding a good plan early is important so that the optimization can be stopped after a smaller number of.
Oracle uses several initial ordering heuristics to increase the likelihood that the first join order considered is a good one.
For each join order that is considered, the optimizer may make additional passes over the tables to decide join methods and access paths.
Such additional passes would target specific global side effects of the access path selection.
For instance, a specific combination of join methods and access paths may eliminate the need to perform an order by sort.
Since such a global side effect may not be obvious when the costs of the different join methods and access paths are considered locally, a separate pass targeting a specific side effect is used to find a possible execution plan with a better overall cost.
For partitioned tables, the optimizer tries to match conditions in thewhere clause of a query with the partitioning criteria for the table, in order to avoid accessing partitions that are not needed for the result.
For example, if a table is partitioned by date range and the query is constrained to data between two specific dates, the optimizer determines which partitions contain data between the specified dates and ensures that only those partitions are accessed.
This scenario is very common, and the speedup can be dramatic if only a small subset of the partitions are needed.
In addition to the regular optimization process, Oracle’s optimizer can be used in tuning mode as part of the SQL Tuning Advisor in order to generate more efficient execution plans than it normally would.
This feature is especially useful for packaged applications that generate the same set of SQL statements repeatedly so that effort to tune these statements for performance can have future benefits.
Such statements are logical candidates for tuning since their impact on the system is the greatest.
The SQL Tuning Advisor can be used to improve the performance of these statements by making making various kinds of recommendations that fall into the following different categories:
Oracle checks whether statistics needed by the optimizer are missing or stale and makes recommendations for collecting them.
When running the optimizer in tuning mode to create a profile, the optimizer tries to verify that its assumptions are correct using dynamic sampling and partial evaluation of the SQL statement.
If it finds that there are steps in the optimization process where the optimizer’s assumptions are wrong, it will generate a correction factor for that step that will become part of the profile.
Optimizing in tuning mode can be very time-consuming, but it can be worthwhile if the use of the profile significantly improves the performance of the statement.
If a profile is created, it will be stored persistently and used whenever the statement is optimized in the future.
Profiles can be used to tune SQL statementswithout changing the text of the statement, something that is important since it is often impossible for the database administrator to modify statements generated by an application.
Based on analysis by the optimizer, Oracle suggests the creation of additional indices that could speed up the statement.
Oracle suggests changes in the structure of the SQLstatement that would allow for more efficient execution.
Packaged applications often generate a large number of SQL statements that are executed repeatedly.
If the application is performing adequately, it is common that database administrators are averse to changes in database behavior.
If the change results in better performance, there is limited perceived upside since the performance was already good enough.
On the other hand, if the change leads to a performance degradation, it may break an application if a critical query deteriorates to a response time that is unacceptable.
An example of a change of behavior is a change of an execution plan for a query.
Such a change may be a perfectly legitimate reflection of changes to properties of the data, such as a table having grown much larger.
But the change could also be an unintended consequence of a number of other actions, such as a change in the routines for collecting optimizer statistics or an upgrade to a new version of the RDBMS with new optimizer behavior.
Oracle’s SQL Plan Management feature addresses the risk associated with execution plan changes by maintaining a set of trusted execution plans for a workload and phasing in plans changed by the query optimizer only after they have been verified not to cause any performance degradations.
Oracle can capture execution plans for a workload and store a plan history for each SQL statement.
The plan baseline is a set of plans for a workload with trusted performance characteristics and against which future plan changes can be compared.
SQLplanbaseline selection.After the optimizer generates a plan for an SQL statement, it checks whether there exists a baseline plan for the statement.
If the statement exists in the baseline but the new plan is different from any existing one, the baseline plan that the optimizer considers to be the best will be used.
The newly generated plan will be added to the plan history for the statement and could become part of a future baseline.
Periodically, it may make sense to try to make newly generated execution plans part of the trusted plans in the baseline.
Oracle supports adding new plans to the baseline with or without verification.
If verification is the chosen option, Oracle will execute a newly generated plan and compare its performance to the baseline in order to make sure it does not cause performance regressions.
This feature is especiallyuseful for computationally intensive operations thatwouldotherwise take an unacceptably long time to perform.
Representative examples are decision support queries that need to process large amounts of data, data loads in a data warehouse, and index creation or rebuild.
In order to achieve good speedup through parallelism, it is important that the work involved in executing the statement be divided into granules that can be processed independently by the different parallel processors.
Depending on the type of operation, Oracle has several ways to split up the work.
For operations that access base objects (tables and indices), Oracle can divide the work by horizontal slices of the data.
For some operations, such as a full table scan, each such slice can be a range of blocks—each parallel query process scans the table from the block at the start of the range to the block at the end.
For some operations on a partitioned table, such as an index range scan, the slice would be a partition.
Parallelism based on block ranges is more flexible since these can be determined dynamically based on a variety of criteria and are not tied to the table definition.
For example, if a large table is joined to a small one by a hash join, Oracle divides the large table among the processes and broadcasts a copy of the small table to each process, which then joins its slice with the smaller table.
If both tables are large, it would be prohibitively expensive to broadcast one of them to all processes.
In that case, Oracle achieves parallelism by partitioning the data among processes by hashing on the values of the join columns (the partitioned hash-join method of Section 18.5.2.1)
Each table is scanned in parallel by a set of processes and each row in the output is passed on to one of a set of processes that are to perform the join.
Which one of these processes gets the row is determined by a hash function on the values of the join column.
Hence, each join process gets only rows that could potentially match, and no rows that could match could end up in different processes.
Oracle parallelizes sort operations by value ranges of the column on which the sort is performed (that is, using the range-partitioning sort of Section 18.5.1)
Each process participating in the sort is sent rows with values in its range, and it sorts the rows in its range.
To maximize the benefits of parallelism, the rows need to be divided as evenly as possible among the parallel processes, and the problem of determining range boundaries that generates a good distribution then arises.
Oracle solves the problem by dynamically sampling a subset of the rows in the input to the sort before deciding on the range boundaries.
The processes involved in the parallel execution of an SQL statement consist of a coordinator process and a number of parallel server processes.
The coordinator is responsible for assigning work to the parallel servers and for collecting and returning data to the user process that issued the statement.
The degree of parallelism is the number of parallel server processes that are assigned to execute a primitive operation as part of the statement.
The degree of parallelism is determined by the optimizer, but can be throttled back dynamically if the load on the system increases.
When a sequence of operations is needed to process a statement, the producer set of servers performs the first operation and passes the resulting data to the consumer set.
If a subsequent operation is needed, such as another sort, the roles of the two sets of servers switch.
The servers that originally performed the table scan take on the role of consumers of the output produced by the first sort and use it to perform the second sort.
Hence, a sequence of operations proceeds by passing data back and forth between two sets of servers that alternate in their roles as producers and consumers.
The servers communicate with each other through memory buffers on shared-memory hardware and through high-speed network connections on MPP (shared nothing) configurations and clustered (shared disk) systems.
For shared-nothing systems, the cost of accessing data on disk is not uniform among processes.
A process running on a node that has direct access to a device is able to process data on that device faster than a process that has to retrieve the data over a network.
Oracle uses knowledge about device-to-node and device-toprocess affinity—that is, the ability to access devices directly—when distributing work among parallel execution servers.
Oracle’s result caching feature allows the result of a query or query block (e.g., a view referenced in a query) to be cached in memory and reused if the same query is executed again.
Updates of the data in the underlying tables invalidate the cached results, so this feature works best for queries against tables that are relatively static and where the result sets are relatively small.
For such an application, result cachingwould be amuchmore lightweight alternative to using materialized views, which would require explicitly creating and administering new persistent database objects.
Oracle supports concurrency control and recovery techniques that provide a number of useful features.
Oracle’s multiversion concurrency control mechanism is based on the snapshot isolation protocol described in Section 15.7
Read-only queries are given a readconsistent snapshot, which is a view of the database as it existed at a specific point in time, containing all updates that were committed by that point in time, and not containing any updates that were not committed at that point in time.
Thus, read locks are not used and read-only queries do not interfere with other database activity in terms of locking.
Oracle supports both statement- and transaction-level read consistency: at the beginning of the execution of either a statement or a transaction (depending on what level of consistency is used), Oracle determines the current system change number (SCN)
The SCN essentially acts as a timestamp, where the time is measured in terms of transaction commits instead of wall-clock time.
If in the course of a query a data block is found that has a higher SCN than the one being associated with the query, it is evident that the data block has been modified after the time of the original query’s SCN by some other transaction that may or may not have committed.
Hence, the data in the block cannot be included in a consistent view of the database as it existed at the time of the query’s SCN.
Instead, an older version of the data in the block must be used; specifically, the one that has the highest SCN that does not exceed the SCN of the query.
Oracle retrieves that version of the data from the undo segment (undo segments are described in Section 28.5.2)
Hence, provided that the undo space is sufficiently large, Oracle can return a consistent result of the query even if the data items have been modified several times since the query started execution.
Should the block with the desired SCN no longer exist in the undo, the query will return an error.
It would be an indication that the undo table space has not been properly sized, given the activity on the system.
In the Oracle concurrency model, read operations do not block write operations and write operations do not block read operations, a property that allows a high degree of concurrency.
In particular, the scheme allows for long-running queries (for example, reporting queries) to run on a system with a large amount of transactional activity.
This kind of scenario is often problematic for database systems where queries use read locks, since the query may either fail to acquire.
An alternative that is used in some systems is to use a lower degree of consistency, such as degree-two consistency, but that could result in inconsistent query results.
Oracle’s concurrency model is used as a basis for the flashback feature.
This feature allows a user to set a certain SCN number or wall-clock time in his session and perform operations on the data that existed at that point in time (provided that the data still exist in the undo)
Normally in a database system, once a change has been committed, there is no way to get back to the previous state of the data other than performing point-in-time recovery from backups.
However, recovery of a very large database can be very costly, especially if the goal is just to retrieve some data item that had been inadvertently deleted by a user.
The flashback feature provides a much simpler mechanism to deal with user errors.
The flashback feature includes the ability to restore a table or an entire database to an earlier point in time without recovering from backups, the ability to perform queries on the data as they existed at an earlier point in time, the ability to track howone ormore rowshave changed over time, and the ability to examine changes to the database at the transaction level.
It may be desirable to be able to track changes to a table beyond what would be possible through normal undo retention.
For instance, corporate governance regulations may require that such changes be trackable for a certain number of years.
For this purpose, a table can be tracked by the flashback archive feature, which creates an internal, history version of the table.
A background process converts the undo information into entries in the history table, which can be used to provide flashback functionality for arbitrarily long periods of time.
Oracle supports two ANSI/ISO isolation levels, read committed and serializable.
There is no support fordirty reads since it is not needed.
Statement-level read consistency corresponds to the read committed isolation level, while transactionlevel read consistency corresponds to the serializable isolation level.
The isolation level can be set for a session or an individual transaction.
Statement-level read consistency (that is, read committed) is the default.
If two writers attempt to modify the same row, one waits until the other either commits or is rolled back, and then it can either return a write-conflict error or go ahead and modify the row; write-conflict errors are detected based on the first-updaterwins version of snapshot isolation, described in Section 15.7
Section 15.7 also describes certain cases of non-serializable execution that can occur with snapshot isolation, and outlines techniques for preventing such problems.
In addition to row-level locks that prevent inconsistencies due to DML activity, Oracle uses table locks that prevent inconsistencies due to DDL activity.
These locks prevent one user from, say, dropping a table while another user has an uncommitted transaction that is accessing that table.
Oracle does not use lock escalation to convert row locks to table locks for the purpose of its regular concurrency control.
Oracle detects deadlocks automatically and resolves them by rolling back one of the transactions involved in the deadlock.
Oracle supports autonomous transactions, which are independent transactions generated within other transactions.
When Oracle invokes an autonomous transaction, it generates a new transaction in a separate context.
The new transaction can be either committed or rolled back before control returns to the calling transaction.
Oracle’s Flashback technology, described in Section 28.5.1, can be used as a recovery mechanism, but Oracle also supports media recovery where files are backed up physically.
We describe this more traditional form of backup and recovery here.
In order to understand how Oracle recovers from a failure, such as a disk crash, it is important to understand the basic structures that are involved.
In addition to the data files that contain tables and indices, there are control files, redo logs, archived redo logs, and undo segments.
The control file contains various metadata that are needed to operate the database, including information about backups.
Oracle records any transactional modification of a database buffer in the redo log, which consists of two or more files.
It logs the modification as part of the operation that causes it and regardless of whether the transaction eventually commits.
It logs changes to indices and undo segments as well as changes to table data.
As the redo logs fill up, they are archived by one or several background processes (if the database is running in archivelogmode)
The undo segment contains information about older versions of the data (that is, undo information)
In addition to its role in Oracle’s consistency model, the information is used to restore the old version of data items when a transaction that has modified the data items is rolled back.
To be able to recover from a storage failure, the data files and control files should be backed up regularly.
The frequency of the backup determines the worst-case recovery time, since it takes longer to recover if the backup is old.
Oracle supports hot backups—backups performed on an online database that is subject to transactional activity.
During recovery from a backup, Oracle performs two steps to reach a consistent state of the database as it existed just prior to the failure.
First, Oracle rolls forward by applying the (archived) redo logs to the backup.
This action takes the database to a state that existed at the time of the failure, but not necessarily a consistent state since the redo logs include uncommitted data.
Second, Oracle rolls back uncommitted transactions by using the undo segment data.
Recovery on a database that has been subject to heavy transactional activity since the last backup can be time-consuming.
Oracle supports parallel recovery in which several processes are used to apply redo information simultaneously.
Oracle provides a GUI tool, Recovery Manager, which automates most tasks associated with backup and recovery.
To ensure high availability, Oracle provides a standby database feature, data guard.
This feature is the same as remote backups, described in Section 16.9
A standby database is a copy of the regular database that is installed on a separate system.
If a catastrophic failure occurs on the primary system, the standby system is activated and takes over, thereby minimizing the effect of the failure on availability.
Oracle keeps the standby database up-to-date by constantly applying archived redo logs that are shipped from the primary database.
The backup database can be brought online in read-only mode and used for reporting and decision support queries.
Whenever a database application executes an SQL statement, there is an operating systemprocess that executes code in the database server.Oracle can be configured so that the operating system process is dedicated exclusively to the statement it is processing or so that the process can be shared among multiple statements.
The latter configuration, known as the shared server, has somewhat different properties with regard to the process and memory architecture.
We shall discuss the dedicated server architecture first and the multithreaded server architecture later.
The memory used by Oracle falls mainly into three categories: software code areas, which are the parts of the memory where the Oracle server code resides, the system global area (SGA), and the program global area (PGA)
A PGA is allocated for each process to hold its local data and control information.
This area contains stack space for various session data and the private memory for the SQL statement that it is executing.
It also contains memory for sorting and hashing operations that may occur during the evaluation of the statement.
The performance of such operations is sensitive to the amount of memory that is available.
For example, a hash join that can be performed in memory will be faster than if it is necessary to spill to disk.
Since there can be a large number of sorting and hashing operations active simultaneously (because of multiple queries as well as multiple operations within each query), deciding how much memory should be allocated to each operation is nontrivial, especially as the load on the system may fluctuate.
Underallocation of memory can lead to extra disk I/Os if an operation needlessly spills to disk and overallocation of memory can lead to thrashing.
Oracle lets the database administrator specify a target parameter for the total amount of memory that should be considered available for these operations.
The size of this target would typically be based on the total amount of.
Oracle will dynamically decide the best way to divide thememory available within the target between the active operations in order to maximize throughput.
The memory allocation algorithm knows the relationship betweenmemory and performance for the different operations and seeks to ensure that the available memory is used as efficiently as possible.
The SGA is a memory area for structures that are shared among users.
It is made up of several major structures, including the following.
This cache keeps frequently accessed data blocks (from tables aswell as indices) inmemory to reduce the need to performphysical disk I/O.
A least recently used replacement policy is used except for blocks accessed during a full table scan.
However, Oracle allows multiple buffer pools to be created that have different criteria for aging out data.
SomeOracle operations bypass the buffer cache and read data directly from disk.
This buffer contains the part of the redo log that has not yet been written to disk.
Oracle seeks to maximize the number of users that can use the database concurrently by minimizing the amount of memory that is needed for each user.
One important concept in this context is the ability to share the internal representation of SQL statements and procedural code written in PL/SQL.
Whenmultiple users execute the same SQL statement, they can share most data structures that represent the execution plan for the statement.
Only data that are local to each specific invocation of the statement need to be kept in private memory.
The sharable parts of the data structures representing the SQL statement are stored in the shared pool, including the text of the statement.
The caching of SQL statements in the shared pool also saves compilation time, since a new invocation of a statement that is already cached does not have to go through the complete compilation process.
The determination of whether an SQL statement is the same as one existing in the shared pool is based on exact text matching and the setting of certain session parameters.
Oracle can automatically replace constants in an SQL statement with bind variables; future queries that are the same except for the values of constants will then match the earlier query in the shared pool.
Caching dictionarymetadata is important for speeding up the compilation time for SQL statements.
In addition, the shared pool is used for Oracle’s result cache feature.
Some of these processes are optional, and in some cases, multiple processes of the same type can be used for performance reasons.
Oracle can generate about two dozen different types of background processes.
When a buffer is removed from the buffer cache, it must be written back to disk if it has beenmodified since it entered the cache.
This task is performed by the database writer processes, which help the performance of the system by freeing up space in the buffer cache.
The log-writer process writes entries in the redo log buffer to the redo log file on disk.
It also writes a commit record to disk whenever a transaction commits.
The checkpoint process updates the headers of the data file when a checkpoint occurs.
It also performs some space management to reclaim unused space in temporary segments.
This process performs process recovery for server processes that fail, releasing resources and performing various cleanup operations.
The recoverer process resolves failures and conducts cleanup for distributed transactions.
The archiver copies the online redo log file to an archived redo log every time the online log file fills up.
The shared-server configuration increases the number of users that a given number of serverprocesses can support by sharing serverprocesses among statements.
It differs from the dedicated server architecture in these major aspects:
A background dispatch process routes user requests to the next available server process.
In doing so, it uses a request queue and a response queue in the SGA.
The dispatcher puts a new request in the request queue where it will be picked up by a server process.
As a server process completes a request, it puts the result in the response queue to be picked up by the dispatcher and returned to the user.
Since a server process is shared among multiple SQL statements, Oracle does not keep private data in the PGA.
Oracle Real Application Clusters (RAC) is a feature that allows multiple instances of Oracle to run against the same database.
Recall that, in Oracle terminology, an instance is the combination of background processes and memory areas.
This feature enables Oracle to run on clustered and MPP (shared disk and shared nothing) hardware architectures.
The ability to cluster multiple nodes has important benefits for scalability and availability that are useful in both OLTP and data warehousing environments.
The scalability benefits of the feature are obvious, since more nodes mean more processing power.
On shared-nothing architectures, adding nodes to a cluster typically requires redistributing the data between the nodes.
Oracle uses a shared-disk architecture where all the nodes have access to all the data and as a result, more nodes can be added to a RAC cluster without worrying how the data should be divided between the nodes.
Oracle further optimizes the use of the hardware through features such as affinity and partitionwise joins.
If one node fails, the remaining ones are still available to the application accessing the database.
The remaining instances will automatically roll back uncommitted transactions that were being processed on the failed node in order to prevent them from blocking activity on the remaining nodes.
Oracle’s shared-disk architecture avoids many of the issues that sharednothing architectures have with data on disk either being local to a node or not.
Still, having multiple instances run against the same database gives rise to some technical issues that do not exist on a single instance.
While it is sometimes possible to partition an application among nodes so that nodes rarely access the same data, there is always the possibility of overlaps, which affects cache management.
In order to achieve efficient cache management over multiple nodes, Oracle’s cache fusion feature allows data blocks to flow directly among caches on different instances using the interconnect, without being written to disk.
The Automatic Storage Manager (ASM) is a volume manager and file system developed by Oracle.
While Oracle can be used with other volume managers and file systems as well as raw devices, ASM is specifically designed to simplify storage management for the Oracle database while optimizing performance.
Recall that an Oracle table space is defined in terms of data files.
Examples of what could constitute ASM disks include disks or partitions of disk arrays, logical volumes, and network attached files.
If the disk configuration changes, e.g., when more disks are added to increase storage capacity, a disk group may need to be rebalanced so that the data are spread evenly over all the disks.
Exadata is a set of Oracle libraries that can run on the storage array CPUs on certain types of storage hardware.
While Oracle is fundamentally based on a shared-disk architecture, Exadata contains a shared-nothing flavor in that some operations that would normally be executed on the database server are moved to storage cells that can only access data that are local to each cell.
Each storage cell consists of a number of disks and several multicore CPUs.
The are major advantages to offloading certain types of processing to storage CPUs:
It allows a large, but relatively inexpensive, expansion of the amount of processing power that is available.
The amount of data that needs to be transferred from a storage cell to the database server can be dramatically reduced, which can be very important since the bandwidth between the storage cell and database server is usually expensive and often a bottleneck.
When executing a query against Exadata storage, the reduction of the amount of data that needs to be retrieved comes from several techniques that can be pushed to the storage cells and executed there locally:
A table may have hundreds of columns, but a given query may only need to access a very small subset of them.
The storage cells can project out the unneeded columns and only send the relevant ones back to the database server.
The database server can send a list of predicates that are local to a table to the storage cells and only rows matching these predicates get sent back to the server.
The filtering mechanism allows for predicates that are Bloom filters allowing rows to be filtered out based on join conditions as well.
In combination, offloading these techniques to the storage cells can speed up query processing by orders of magnitude.
It requires that the storage cells, in addition to sending back regular, unaltered database blocks to the server, can send back a compacted version where certain columns and rows have been removed.
This ability in turn requires the storage software to understandOracle’s block format and data types, and to include Oracle’s expression and predicate evaluation routines.
In addition to providing benefits for query processing, Exadata can also speed up incremental backups by performing block-level change tracking and only.
Also, the work of formatting extents when creating a new table space is offloaded to Exadata storage.
Exadata storage supports all regular Oracle features, and it is possible to have a database that includes both Exadata and non-Exadata storage.
Oracle provides support for replication and distributed transactions with twophase commit.
In one form, data in a master site are replicated to other sites in the form of materialized views.
A materialized view does not have to contain all the master data—it can, for example, exclude certain columns from a table for security reasons.
Oracle supports two types of materialized views for replication: read-only and updatable.
An updatable materialized view can be modified and the modifications propagated to the master table.
However, read-only materialized views allow for a wider range of view definitions.
For instance, a read-only materialized view can be defined in terms of set operations on tables at the master site.
Changes to the master data are propagated to the replicas through the materialized view refresh mechanism.
Oracle also supports multiple master sites for the same data, where all master sites act as peers.
A replicated table can be updated at any of the master sites and the update is propagated to the other sites.
For asynchronous replication, the update information is sent in batches to the other master sites and applied.
Since the same data could be subject to conflicting modifications at different sites, conflict resolution based on some business rules might be needed.Oracle provides a number of built-in conflict resolutionmethods and allows users to write their own if need be.
With synchronous replication, an update to one master site is propagated immediately to all other sites.
Oracle supports queries and transactions spanning multiple databases on different systems.With the use of gateways, the remote systems can include non-Oracle databases.
Oracle has built-in capability to optimize a query that includes tables at different sites, retrieve the relevant data, and return the result as if it had been a normal, local query.
Oracle also transparently supports transactions spanning multiple sites by a built-in two-phase-commit protocol.
The most common usage is in data warehousing when large amounts of data are regularly loaded from a transactional system.
Oracle has a direct-load utility, SQL*Loader, that supports fast parallel loads of large amounts of data from external files.
It supports a variety of data formats and it can perform various filtering operations on the data being loaded.
Oracle allows external data sources, such as flat files, to be referenced in the from clause of a query as if they were regular tables.
An external table is defined by metadata that describe the Oracle column types and the mapping of the external data into those columns.
An access driver is also needed to access the external data.
The external table feature is primarily intended for extraction, transformation, and loading (ETL) operations in a data warehousing environment.
Data can be loaded into the data warehouse from a flat file using.
By addingoperations on thedata in either the select list orwhere clause, transformations and filtering can be done as part of the same SQL statement.
Since these operations can be expressed either in native SQL or in functions written in PL/SQL or Java, the external table feature provides a very powerful mechanism for expressing all kinds of data transformation and filtering operations.
For scalability, the access to the external table can be parallelized by Oracle’s parallel execution feature.
Oracle provides an export utility for unloading data and metadata into dump files.
These files are regular files using a proprietary format that can be moved to another system and loaded into another Oracle database using the corresponding import utility.
Oracle provides users a number of tools and features for system management and application development.
In recent releases of Oracle, a lot of emphasis was put on the concept of manageability, that is, reducing the complexity of all.
This effort covered a wide variety of areas, including database creation, tuning, space management, storage management, backup and recovery, memory management, performance diagnostics, and workload management.
Oracle Enterprise Manager (OEM) is Oracle’s main tool for database systems management.
It provides an easy-to-use graphical user interface for most tasks associated with administering an Oracle database including configuration, performance monitoring, resource management, security management, and access to the various advisors.
In addition to database management, OEM provides integrated management of Oracle’s applications and middleware software stack.
The Automatic Workload Repository (AWR) is one of the central pieces of infrastructure for Oracle’s manageability effort.
Oracle monitors the activity on the database system and records a variety of information relating to workloads and resource consumption and records them in AWR at regular intervals.
By tracking the characteristics of a workload over time, Oracle can detect and help diagnose deviations from normal behavior such as a significant performance degradation of a query, lock contention, and CPU bottlenecks.
The information recorded in AWR provides a basis for a variety of advisors that provide analysis of various aspects of the performance of the system and advice for how it can be improved.
Oracle has advisors for SQL tuning, creating access structures, such as indices and materialized views, and memory sizing.
Oracle also provides advisors for segment defragmentation and undo sizing.
A database administrator needs to be able to control how the processing power of the hardware is divided among users or groups of users.
Some groups may execute interactive queries where response time is critical; others may execute long-running reports that can be run as batch jobs in the background when the system load is low.
It is also important to be able to prevent a user from inadvertently submitting an extremely expensive ad hoc query that will unduly delay other users.
Oracle’s Database Resource Management feature allows the database administrator to divide users into resource consumer groups with different priorities and properties.
For example, a group of high-priority, interactive users may be guaranteed at least 60 percent of the CPU.
The remainder, plus any part of the 60 percent not used up by the high-priority group, would be allocated among resource consumer groups with lower priority.
A really low-priority group could get assigned 0 percent, which would mean that queries issued by this group would run only when there are spare CPU cycles available.
Limits for the degree of parallelism for parallel execution can be set for each group.
When a user submits a statement, the resource manager estimates how long it would take to execute it and returns an error if the statement violates the limit.
The resource manager can also limit the number of user sessions that can be active concurrently for each resource consumer group.
Other resources that can be controlled by the resource manager include undo space.
Oracle Data Mining provides a variety of algorithms that embed the data mining process inside the database both for building amodel on a training set of data and for applying the model for scoring the actual production data.
The fact the data never needs to leave the database is a significant advantage compared to using other data mining engines.
Having to extract and insert potentially very large data sets into a separate engine is cumbersome, costly, andmay prevent new data from being scored instantaneously as they are entered into the database.
Oracle provides algorithms for both supervised and unsupervised learning including:
In addition, Oracle provides a wide range of statistical functions inside the database covering areas including linear regression, correlation, cross tabs, hypothesis testing, distribution fitting, and Pareto analysis.
Oracle provides two interfaces to the data mining functionality, one based on Java and one that is based onOracle’s procedural language PL/SQL.
Once amodel has been built on an Oracle database, it can be shipped to be deployed on other Oracle databases.
Up-to-date product information, including documentation, on Oracle products can be found at the Web sites http://www.oracle.com and http://technet.oracle.com.
IBM’s DB2 Universal Database family of products consists of flagship database servers and suites of related products for business intelligence, information integration, and content management.
The DB2 Universal Database Server is available on a variety of hardware and operating-system platforms.
The list of server platforms supported includes high-end systems such as mainframes, massively parallel processors (MPP), and large symmetric multiprocessors (SMP) servers; medium-scale systems such as four-way and eight-way SMPs; workstations; and even small handheld devices.
Applications can migrate seamlessly from the low-end platforms to high-end servers because of the portability of the DB2 interfaces and services.
Besides the core database engine, the DB2 family consists of several other products that provide tooling, administration, replication, distributed data access, pervasive data access, OLAP, and many other features.
All of these support a common subset of data-definition language, SQL, and administration interfaces.
However, the engines have somewhat different features due to their platform origins.
In this chapter, the focus is on the DB2 Universal Database (UDB) engine that supports Linux, Unix, and Windows.
Specific features of interest in other DB2 systems are highlighted in appropriate sections.
Most industry database-design and CASE tools can be used to design a DB2 database.
In particular, data modeling tools such as ERWin and Rational Rose allow the designer to generate DB2-specific DDL syntax.
DB2 provides support for many logical and physical database features using SQL.
Likewise, certain physical database features such as tablespaces, bufferpools, and partitioning are also supported by using SQL statements.
The Control Center GUI tool for DB2 allows a designer or an administrator to issue the appropriate DDL for these features.Another tool, db2look, allows the administrator to obtain a full set of DDL statements for a database including tablespaces, tables, indices, constraints, triggers, and functions that can be used to create an exact replica of the database schema for testing or replication.
For design, the Control Center provides a tree view of a server, its databases, tables, views, and all other objects.
It also allows users to define new objects, create ad hoc SQL queries, and view query results.
Design tools for ETL, OLAP, replication, and federation also integrate into the Control Center.
The entire DB2 family supports the Control Center for database definition as well as related tools.
DB2 also provides plug-in modules for application development in the IBM Rational Application Developer product as well as in the Microsoft Visual Studio product.
DB2 provides support for a rich set of SQL features for various aspects of database processing.
A rich set of XML functions have been included in DB2
The following is a list of several important XML functions that can be used in SQL, as part of the SQL/XML extension to SQL (described earlier in Section 23.6.3):
For example the function call, xmlelement(book) creates the book element.
Returns the concatenation of a variable number of XML arguments.
The XML functions can be incorporated into SQL effectively to provide extensive XML manipulation capabilities.
In Figure 29.2, we show an SQL query with XML extensions that can be used to create such a purchase order.
Specialized storage, indexing, query processing and optimization techniques have been introduced for efficient processing of XML data and queries in the XQuery language, and APIs have been extended to deal with XML data and XQuery.
Distinct data types are based on DB2 built-in data types.
However, the user can define additional or alternative semantics for these new types.
For example, the user can define a distinct data type called us dollar, using:
Subsequently, the user can create a field (e.g., price) in a table with type us dollar.
Queries may now use the typed field in predicates such as the following:
Structured data types are complex objects that usually consist of two or more attributes.
For example, one can use the following DDL to create a structured type called department t:
One can create a type hierarchy and tables in the hierarchy that can inherit specificmethods andprivileges.
Structured types can also beused todefinenested attributes inside a column of a table.
Although such a definition would violate normalization rules, it may be suitable for object-oriented applications that rely on encapsulation and well-defined methods on objects.
Another important feature is the ability for users to define their own functions and methods.
These functions can subsequently be included in SQL statements and queries.
Functions can generate scalars (single attribute) or tables (multiattribute row) as their result.
Users can register functions (scalar or table) using the create function statement.
The functions can be written in common programming languages such as C or Java or scripts such as REXX or PERL.
User-defined functions (UDFs) can operate in fenced or unfenced modes.
In fenced mode, the functions are executed by a separate thread in its own address space.
UDFs can define a scratch pad (work) area where they can maintain local and static variables across different invocations.
Thus, UDFs can perform powerful manipulations of intermediate rows that are its inputs.
Methods are another feature that define the behavior of objects.
Unlike UDFs, they are tightly encapsulated with a particular structured data type.
New database applications require the ability to manipulate text, images, video, and other types of data that are typically quite large in size.
DB2 supports these requirements by providing three different large object (LOB) types.
Each LOB can be as large as two gigabytes in size.
Users can register UDFs that manipulate these LOBs according to application requirements.
A recent feature of DB2 enables users to create index extensions to generate keys from structured data types by using the create index extension statement.
For example, one can create an index on an attribute based on the department t data type defined earlier by generating keys, using the department name.
Finally, users can take advantage of the rich set of constraint checking features available in DB2 for enforcing object semantics such as uniqueness, validity, and inheritance.
A Web service can be defined to invoke DB2, using SQL statements.
The resultant Web-service call is processed by an embedded Web-service engine in DB2 and the appropriate SOAP response generated.
The following SQL shows DB2 acting as a consumer of a Web service.
In this example, the GetQuote() user-defined function is a Web service.
DB2 makes the Web-service call using an embedded Web-service engine.
In this case, GetQuote returns a numeric quote value for each ticker id in the portfolio table.
These UDFs can be incorporated in SQL for reading from or writing to message queues.
This allows complete flexibility in allocating table partitions to different nodes in a system.
For example, large tables may be partitioned across all nodes in a system while small tables may reside on a single node.
A tablespace consists of one or more containers, which are references to directories, devices, or files.
A tablespace may contain zero or more database objects such as tables, indices, or LOBs.
In this figure, two tablespaces have been defined for a nodegroup.
The humanres tablespace is assigned four containers, while the sched tablespace has only one container.
The employee and department tables are assigned to the humanres tablespace, while the project table is in the sched tablespace.
Striping is used to allocate fragments (extents) of the employee and department table to the containers of the humanres tablespace.
DB2 permits the administrator to create either system-managed or DBMS-managed tablespaces.
System-managed spaces (SMS) are directories or file systems that are maintained by the underlying operating system.
Data-managed spaces (DMS) are raw devices or preallocated files that are then controlled by DB2
The size of these containers can never grow or shrink.
In both cases, an extent of pages is the unit of space management.
The administrator can choose the extent size for a tablespace.
DB2 supports striping across the different containers as a default behavior.
For example, when data are inserted into a newly created table, the first extent is assigned to a container.
Once the extent is full, the next data items are allocated to the next container in round-robin fashion.
Striping provides two significant benefits: parallel I/O and load balancing.
One or more buffer pools may be associated with each tablespace for managing different objects such as data and indices.
The buffer pool is a common shared data area that maintains memory copies of objects.
These objects are typically organized as pages for management in the buffer pool.
DB2 allows buffer pools to be defined by SQL statements.
An administrator can add more pages to a buffer pool or decrease its size without quiescing the database activity.
The data manager component triggers prefetch of data and index pages based on the query access patterns.
For instance, a table scan always triggers prefetch of data pages.
Index scans can trigger prefetch of index pages as well as data pages if they are being accessed in a clustered fashion.
The number of prefetchers and the prefetch size are configurable parameters that need to be initialized according to the number of disks or containers in the tablespace.
Figure 29.7 shows the logical view of a table and an associated index.
Each page consists of a set of records that are either user data records or special system records.
Page zero of the table contains special system records about the table and its status.
DB2 uses a space-map record called free space control record (FSCR) to find free space in the table.
The FSCR record usually contains a space map for 500 pages.
The FSCR entry is a bit mask that provides a rough indication of the possibility of free space in a page.
The insert or update algorithm must validate the FSCR entries by performing a physical check of the available space in a page.
Indices are also organized as pages containing index records and pointers to child and sibling pages.
The indices have bidirectional pointers at the leaf level to support forward and reverse scans.
Leaf pages contain index entries that point to records in the table.
Each record in the table can be uniquely identified by using its page and slot information, which are called the record ID or RID.
Exception: Any space reserved by an uncommited delete is not usable.
The included index columns enable DB2 to extend the use of “index-only” query-processing techniques whenever possible.
Additional directives such as minpctused and pctfree can be used to control the merge and initial space allocation of index pages.
The slot directory is an array of 255 entries that points to record offsets in the page.
However, each page may contain only 255 user records in it.
Larger page sizes are useful in applications such as data warehousing, where the table contains many columns.
Smaller page sizes are useful for operational data with frequent updates.
This section provides a brief overview of the main features of MDC.
With this feature, a DB2 table may be created by specifying one or more keys as dimensions.
DB2 includes a clause called organize by dimensions for this purpose.
For example, the following DDL describes a sales table organized by storeId, year(orderDate), and itemId attributes as dimensions.
Each of these dimensions may consist of one or more columns, similar to index keys.
In fact, a “dimension block index” (described below) is automatically created for each of the dimensions specified and is used to access data quickly and efficiently.
A composite block index, containing all dimension key columns, is created automatically if necessary, and is used to maintain the clustering of data over insert and update activity.
Every unique combination of dimension values forms a logical “cell,” that is physically organized as blocks of pages, where a block is a set of consecutive pages on disk.
The set of blocks that contain pages with data having a certain key value of one of the dimension block indices is called a “slice.” Every page of the table is part of exactly one block, and all blocks of the table consist of the same number of pages, namely, the block size.
DB2 has associated the block size with the extent size of the tablespace so that block boundaries line up with extent boundaries.
This MDC table is clustered along the dimensions year(orderDate),1 region, and itemId.
The figure shows a simple logical cube with only two values for each dimension attribute.
In reality, dimension attributes can easily extend to large numbers of values without requiring any administration.
Logical cells are represented by the subcubes in the figure.
Records in the table are stored in blocks, which contain an extent’s worth of consecutive pages on disk.
In the diagram, a block is represented by a shaded oval, and is numbered according to the logical order of allocated extents in the table.
We show only a few blocks of data for the cell identified by the dimension values <1997,Canada,2>
A column or row in the grid represents a slice for a particular dimension.
For example, all records containing the value “Canada” in the region dimension are found in the blocks contained in the slice defined by the “Canada” column in the cube.
In fact, each block in this slice only contains records having “Canada” in the region field.
Figure 29.9 Logical view of physical layout of an MDC table.
In our example, a dimension block index is created on each of the year(orderDate), region, and itemId attributes.
Each dimension block index is structured in the same manner as a traditional B-tree index except that, at the leaf level, the keys point to a block identifier (BID) instead of a record identifier (RID)
Since each block contains potentially many pages of records, these block indices are much smaller than RID indices and need be updated only when a new block is added to a cell or existing blocks are emptied and removed from a cell.
A slice, or the set of blocks containing pages with all records having a particular key value in a dimension, are represented in the associated dimension block index by a BID list for that key value.
Figure 29.10 illustrates slices of blocks for specific values of region and itemId dimensions, respectively.
In the example above, to find the slice containing all records with “Canada” for the region dimension, wewould look up this key value in the region dimension block index and find a key as shown in Figure 29.10a.
This key points to the exact set of BIDs for the particular value.
This map records the state of each block belonging to the table.
A block may be in a number of states such as in use, free, loaded, requiring constraint enforcement.
Figure 29.11 shows an example block map for a table.
Its availability status is “U,” indicating that it is in use.
However, it is a special block and does not contain any user records.
Block 12 was previously loaded and requires that a constraint check be performed on it.
A crucial aspect of MDC is to choose the right set of dimensions for clustering a table and the right block size parameter to minimize the space utilization.
If the dimensions and block sizes are chosen appropriately, then the clustering benefits translate into significant performance and maintenance advantages.
On the other hand, if chosen incorrectly, the performance may degrade and the space utilization could be significantly worse.
There are a number of tuning knobs that can be exploited to organize the table.
These include varying the number of dimensions, and varying the granularity of one or more dimensions, varying the.
One or more of these techniques can be used jointly to identify the best organization of the table.
It is natural to askwhether the newMDC feature has an adverse impact or disables some existing features of DB2 for normal tables.
All existing features such as secondary RID indices, constraints, triggers, defining materialized views, and query processing options, are available for MDC tables.
Hence, MDC tables behave just like normal tables except for their enhanced clustering andprocessing aspects.
DB2 queries are transformed into a tree of operations by the query compiler.
The query operator tree is used at execution time for processing.
DB2 supports a rich set of query operators that enables it to consider the best processing strategies and provides the flexibility to execute complex query tasks.
The query is a representative complex query (query 5) from the TPC-H benchmark and contains several joins and aggregations.
The query plan chosen for this particular example is rather simple since many indices and other auxiliary structures such as materialized views were not defined for these tables.
DB2 provides various “explain” facilities including a powerful visual explain feature in the Control Center that can help users understand the details of a query-execution plan.
The query plan shown in the figure is based on the visual explain for the query.
All SQL queries and statements, however complex they may be, are transformed into a query tree.
The base or leaf operators of the query tree manipulate records in database tables.
Intermediate operations of the tree include relational-algebra operations such as join, set operations, and aggregation.
The root of the tree produces the results of the query or SQL statement.
DB2 supports a comprehensive set of access methods on relational tables.
This is themost basicmethod andperforms apage-by-page access of all records in the table.
An index is used to select the specific records that satisfy the query.
The qualifying records are accessed using the RIDs in the index.
DB2 detects opportunities to prefetch data pages when it observes a sequentialaccess pattern.
One of the block indices is used to scan a specific set of MDC data blocks.
The qualifying blocks are accessed and processed in block table scan operations.
In this case, the index contains all the attributes that are required by the query.
This access method is chosen for an unclustered index scan with a significant number of RIDs.
DB2 has a sort operation on the RIDs and performs a fetch of the records in sorted order from the data pages.
Sorted access changes the I/O pattern from random to sequential and also enables prefetching opportunities.
List prefetch has been extended to deal with block indices as well.
This method is used when DB2 determines that more than one index can be used to constrain the number of satisfying records in a base table.
The most selective index is processed to generate a list of BIDs or RIDs.
The next selective index is then processed to return the BIDs or RIDs that it qualifies.
A BID or RID qualifies for further processing only if it is present in the intersection (AND operation) of the index scan results.
The result of an index AND operation is a small list of qualifying BIDs or RIDs which are used to fetch the corresponding records from the base table.
This strategy is used if two or more block or record indices can be used to satisfy query predicates that are combined by using the OR operator.
DB2 eliminates duplicate BIDs or RIDs by performing a sort and then fetching the resulting set of records.
Index ORing has been extended to consider block and RID index combinations.
All the selection and projection predicates of a query are usually pushed down to the access methods.
In addition, DB2 performs certain operations such as sorting and aggregation in “pushed down” mode in order to reduce instruction paths.
This MDC feature takes advantage of the new set of access-method improvements for block index scans, block index prefetch, block index ANDing, and block index ORing to process blocks of data.
For join, DB2 can choose between nested-loop, sort-merge, and hash-join techniques.
The nested-loop technique is useful if the inner table is very small or can be accessed by using an index on a join predicate.
Sort-merge-join and hash-join techniques are used for joins involving large outer and inner tables.
Set operations are implemented by using sorting and merging techniques.
The merging technique eliminates duplicates in the case of unionwhile duplicates are forwarded in the case of intersection.
DB2 processes aggregation operations in early or “push-down” mode whenever possible.
For instance, a group by aggregation can be performed by incorporating the aggregation into the sort phase.
The join andaggregation algorithms can take advantage of superscalar processing in modern CPUs using block-oriented and cache-conscious techniques.
One of the most important aspects of DB2 is that it uses the query-processing infrastructure in an extensible fashion to support complex SQL operations.
The complex SQL operations include support for deeply nested and correlated queries as well as constraints, referential integrity, and triggers.
Because most of these actions are built into the query plan, DB2 is able to scale and provide support for a larger number of these constraints and actions.
Constraints and integrity checks are built as query tree operations on insert, delete, or update SQL statements.
DB2 also supports maintenance of materialized view by using built-in triggers.
DB2 extends the base set of query operations with control and data exchange primitives to support SMP (that is, sharedmemory), MPP (that is, shared nothing), and SMP cluster (that is, shared disk) modes of query processing.
DB2 uses a “tablequeue” abstraction for data exchange between threads on different nodes or on the same node.
The tablequeue is used as a buffer that redirects data to appropriate receivers using broadcast, one-to-one, or directedmulticast methods.
Control operations are used to create threads and coordinate the operation of different processes and threads.
In all these modes, DB2 employs a coordinator process to control the query operations and final result gathering.
An example is the global aggregation operation to combine the local aggregation results.
Subagents or slave threads perform the base database operations in one or more nodes.
In SMP mode, the subagents use shared memory to synchronize between themselves when sharing data.
In an MPP, the tablequeue mechanisms provide buffering and flow control to synchronize across different nodes during execution.
DB2 employs extensive techniques to optimize and process queries efficiently in an MPP or SMP environment.
Figure 29.14 shows a simple query executing in a fournode MPP system.
The query is executed by spawning agents that execute at each.
DB2’s query compiler uses an internal representation of the query, called the query-graph model (QGM), in order to perform transformations and optimizations.
After parsing the SQL statement, DB2 performs semantic transformations on the QGM to enforce constraints, referential integrity, and triggers.
Next, DB2 attempts to perform query rewrite transformations that are considered mostly beneficial.
Rewrite rules are fired if applicable to perform the required transformations.
The query optimizer component uses this enhanced and transformed QGM as its input for optimization.
The optimizer is cost based and uses an extensible, rule-driven framework.
The optimizer can be configured to operate at different levels of complexity.
At an intermediate level, the optimizer does not consider certain plans, access methods (e.g., index ORing), or rewrite rules.
At the lowest level of complexity, the optimizer uses a simple greedy heuristic to choose a good but not necessarily optimal query plan.
The optimizer uses detailed models of the query-processing operations, including memory sizes and prefetching, to obtain accurate estimates of the I/O and CPU costs.
It relies on the statistics of the data to estimate the cardinality and selectivities of the operations.
DB2 allows the user to obtain detailed histograms of column-level distributions and combinations of columns using the runstats utility.
The detailed histograms contain information about the most frequent value occurrences as well as quantile-based frequency distributions of the attributes.
The optimizer generates an internal query plan that is considered the.
This query plan is converted into threads of query operators and associated data structures for execution by the query-processing engine.
Materialized views are supported in DB2 in Linux, Unix, andWindows as well as on the z/OS platforms.
A materialized view can be any general view definition on one or more tables or views.
A materialized view is useful since it maintains a persistent copy of the view data to enable faster query processing.
In DB2 a materialized view is called amaterialized query table (MQT)
MQTs are specified by using the create table statement as shown by the example in Figure 29.15
In DB2, MQTs can reference other MQTs to create a tree or forest of dependent views.
These MQTs are highly scalable as they can be partitioned in an MPP environment and can have MDC clustering keys.
MQTs are most valuable if the database engine can route queries to them seamlessly and also if the database engine can maintain them efficiently whenever possible.
The query-compiler infrastructure in DB2 is ideally suited to leverage the full power of MQTs.
The internal QGM model allows the compiler to match the input query against the available MQT definitions and choose appropriate MQTs for consideration.
They include the base query as well as suitable MQT reroute versions.
The optimizer loops through these options before choosing the optimal version for execution.
The entire flow of the reroute and optimization is shown in Figure 29.16
MQTs are useful only if the database engine provides efficient techniques for maintenance.
In the time dimension, the two choices are immediate or deferred.
If one selects immediate, then internal triggers are created and compiled.
In the case of deferred maintenance, the updated tables aremoved into an integritymode and an explicit refresh statement must be issued to perform the maintenance.
In the size dimension, the choices are incremental or full.
Incremental maintenance implies that only the recently updated rows should be used for maintenance.
Full maintenance implies that the entire MQT be refreshed from its sources.
The matrix in Figure 29.17 shows the two dimensions and the options that are most useful along these dimensions.
For instance, immediate and full maintenance are not compatible unless the sources are extremely small.
DB2 also allows for the MQTs to be maintained by user.
In this case, the refresh of the MQTs is determined by users performing explicit processing using SQL or utilities.
The following commandsprovide one simple example of performingdeferred maintenance for the emp dept materialized view after a load operation to one of its sources.
DB2 UDB provides features for simplifying the design and manageability of databases.
Autonomic computing encompasses a set of techniques that allow the computing environment to manage itself and reduce the external dependencies in the face of external and internal changes in security, system load, or other factors.
The following sections briefly describe the configuration and optimization areas.
DB2 is providing support for automatic tuning of various memory and system configuration parameters.
For instance, parameters such as buffer pool sizes and sort heap sizes can be specified as automatic.
In this case, DB2 monitors the system and slowly grows or shrinks these heap memory sizes, depending on the workload characteristics.
Auxiliary data structures (indices, MQTs) and data organization features (partitioning, clustering) are important aspects of improving the performance of database processing in DB2
In the past, the database administrator (DBA) had to use experience and known guidelines to choose meaningful indices, MQTs, partition keys, and clustering keys.
Given the potential number of choices, even the best experts are not capable of finding the right mix of these features for a given workload in a short time.
DB2 includes a Design Advisor that provides workload-based advice for all of these features.
The Design Advisor tool automatically analyzes a workload, using optimization techniques to present a set of recommendations.
The “-m ” parameter allows the user to specify the following options:
The advisor uses the full power of the DB2 query-optimization framework in these recommendations.
It uses an input workload and constraints on size and time of advise as its parameters.
Given that it leverages the DB2 optimization framework, it has full knowledge of the schema and statistics of the underlying data.
The advisor uses several combinatorial techniques to identify indices, MQTs, MDCs, and partitioning keys to improve the performance of the given workload.
Another aspect of optimization is balancing the processing loadon the system.
In particular, utilities tend to increase the load on a system and cause significant reduction in user workload performance.
Given the trend toward online utilities, there is a need to balance the load consumption of utilities.
It continually adjusts and throttles the performance of the backup utility, using specific control parameters.
DB2 provides a number of tools for ease of use and administration.
This core set of tools is augmented and enhanced by a large number of tools from vendors.
It is organized from data objects such as servers, databases, tables, and indices.
It contains task-oriented interfaces to perform commands and allows users to generate SQL scripts.
Figure 29.18 shows a screen shot of the main panel of the Control Center.
This screen shot shows a list of tables in the Sampledatabase in theDB2 instance on node Crankarm.
The administrator can use the menu to invoke a suite of component tools.
The main components of the Control Center include command center, script center, journal, license management, alert center, performance monitor, visual explain, remote database management, storage management, and support for replication.
The command center allows users and administrators to issue database commands and SQL.
The script center allows users to run SQL scripts constructed interactively or from a file.
The performance monitor allows users to monitor various events in the database system and obtain snapshots of performance.
A stored-procedure builder helps the user to develop and install stored procedures.
Visual explain allows the user to obtain graphical views of the query-execution plan.
An index wizard helps the administrator by suggesting indices for performance.
While the Control Center is an integrated interface for many of the tasks, DB2 also provides direct access to most tools.
For users, tools such as the explain facility, explain tables, and graphical explain provide a detailed breakdown of the query plans.
Users are also allowed to modify statistics (if permitted) in order to generate the best query plans.
DB2 provides comprehensive support for load, import, export, reorg, redistribute, and other data-related utilities.
DB2’s utilities are all fully enabled to run in parallel mode.
Audit facility for maintaining the audit trace of database actions.
Governor facility for controlling the priority and execution times of different.
Query patroller facility for managing the query jobs in the system.
Event monitoring facilities for tracking the resources and events during system execution.
For isolation, DB2 supports the repeatable read (RR), read stability (RS), cursor stability (CS), and uncommitted read (UR) modes.
The RS isolation mode locks only the rows that an application retrieves in a unit of work.
On a subsequent scan, the application is guaranteed to see all these rows (like RR) but might also see new rows that qualify.
However, this might be an acceptable trade-off for some applications with respect to strict RR isolation.
Applications can choose their level of isolation at the binding stage.
Most commercially available applications are bound using most isolation levels, enabling users to choose the right version of the application for their requirement.
A separate lock-table data structure is maintained with the lock information.
DB2 escalates from record-level to table-level locks if the space in the lock table becomes tight.
Write locks or update locks are held until commit or rollback time.
Figure 29.19 shows the different lock modes and their descriptions.
The set of lock modes supported includes intent locks at the table.
Also, DB2 implements next-key locking and variant schemes for updates affecting index scans to eliminate the Halloween and phantom-read problems.
The transaction can set the lock granularity to table level by using the lock table statement.
This is useful for applications that know their desired level of isolation is at the table level.
Also, DB2 chooses the appropriate locking granularities for utilities such as reorg and load.
The offline versions of these utilities usually lock the table in exclusive mode.
The online versions of the utilities allow other transactions to proceed concurrently by acquiring row locks.
A deadlock detection agent is activated for each database and periodically checks for deadlocks between transactions.
In case of a deadlock, the agent chooses a victim and aborts it with a deadlock SQL error code.
Applications can commit or roll back by using explicit commit or rollback statements.
Applications can also issue begin transaction and end transaction statements to control the scope of transactions.
Normally, DB2 releases all locks that it holds on behalf of a transaction at commit or rollback.
However, if a cursor statement has been declared by using the with hold clause, then some locks are maintained across commits.
Write-ahead logging is employed to flush log records to the persistent log file before data pages are written or at commit time.
DB2 supports two types of log modes: circular logging and archive logging.
In circular logging, a predefined set of primary and secondary log files is used.
Circular logging is useful for crash recovery or application failure recovery.
In archival logging, DB2 creates new log files and the old log files must be archived in order to free up space in the file system.
In both cases, DB2 allows the user to configure the number of log files and the sizes of the log files.
In update-intensive environments, DB2 can be configured to look for group commits in order to bunch log writes.
DB2 supports transaction rollback and crash recovery as well as point-in-time or roll-forward recovery.
In the case of crash recovery, DB2 performs the standard phases of undo processing and redo processing up to and from the last checkpoint in order to recover the proper committed state of the database.
For point-in-time recovery, the database can be restored from a backup and can be rolled forward to a specific point in time, using the archived logs.
The roll-forward recovery command supports both database and tablespace levels.
It can also be issued on specific nodes on a multinode system.
A parallel recovery scheme improves the performance in SMP systems by utilizing many CPUs.
DB2 performs coordinated recovery across MPP nodes by implementing a global checkpointing scheme.
Remote client applications connect to the database server by using communication agents such as db2tcpcm.
Each application is assigned an agent (coordinator agent in MPP or SMP environments) called the db2agent thread.
Each database has a set of processes or threads that performs tasks such as prefetching, page cleaning from buffer pool, logging, and deadlock detection.
Finally, there is a set of agents at the level of the server to perform tasks such as crash detection, license server, process creation, and control of system resources.
DB2 provides configuration parameters to control the number of threads and processes in a server.
Almost all the different types of agents can be controlled by using the configuration parameters.
Private memory in agents or threads ismainly used for local variables and data structures that are relevant only for the current activity.
For example, a private sort could allocate memory from the agent’s private heap.
Shared memory is partitioned into server shared memory, database shared memory, and application shared memory.
The database-level shared memory contains useful data structures such as the buffer pool, lock lists, application package caches, and shared sort areas.
The server and application sharedmemory areas are primarily used for common data structures and communication buffers.
Buffer pools can be created by using the create bufferpool statement and can be associated with tablespaces.
Multiple buffer pools are useful for a variety of reasons but they should be defined after a careful analysis of the workload requirements.
DB2 supports a comprehensive list of memory configuration and tuning parameters.
It consists of capture and apply components, which are controlled by administration interfaces.
The change-capture mechanisms are either “log-based” for DB2 tables or “trigger-based” in the case of other data sources.
The captured changes are stored in temporary staging table areas under the control of DB2 Replication.
SQL-based transformations can be performed on the intermediate staging tables by using filtering conditions as well as aggregations.
The resulting rows can be applied to one ormore target tables.
All of these actions are controlled by the administration facility.
Queue (Q) replication creates a queue transportmechanism using IBM’s message-queue product to ship captured log records as messages.
These messages are extracted from the queues at the receiving end and applied against targets.
The apply process can be parallelized and allows for user-specified conflict resolution rules.
The federated edition integrates tables in remote DB2 or other relational databases into a single distributed database.
Users and developers can access various nonrelational data sources in tabular format, using wrapper technology.
The federation engine provides a cost-based method for query optimization across the different data sites.
DB2 supports user-defined table functions that enable access to nonrelational and external data sources.
User-defined table functions are created by using the create function statement with the clause returns table.
Using these features, DB2 is able to participate in the OLE DB protocols.
Finally, DB2 provides full support for distributed transaction processing using the two-phase commit protocol.
DB2 can act as the coordinator or agent for distributed XA support.
As a coordinator, DB2 can perform all stages of the twophase commit protocol.
As a participant, DB2 can interact with any commercial distributed transaction manager.
Data Warehouse Edition has at its foundation the DB2 engine, and enhances it with features for ETL, OLAP,mining, and online reporting.
In the MPP mode, DB2 can support configurations that can scale to several hundreds of nodes for large database sizes (terabytes)
Additionally, features such as MDC and MQT provide support for the complex query-processing requirements of business intelligence.
Another aspect of business intelligence is online analytical processing or OLAP.
Cube views provide modeling support for multidimensional cubes and provides a mapping mechanism to a relational star schema.
This model is then used to recommend appropriate MQTs, indices, and MDCdefinitions to improve theperformance ofOLAPqueries against the database.
In addition, cube views can take advantage of DB2’s native support for the cube by and rollup operations for generating aggregated cubes.
The OLAP engine from the Essbase product is used in the DB2 OLAP server.
DB2 Alphablox is a new feature that provides online, interactive, reporting, and analysis capabilities.
A very attractive feature of the Alphablox feature is the ability to construct newWeb-based analysis forms rapidly, using a building block approach called blox.
For deep analytics, DB2 Intelligent Miner provides various components for modeling, scoring, and visualizing data.
Mining enables users to perform classification, prediction, clustering, segmentation, and association against large data sets.
The latest release, SQL Server 2008, is available in express, standard, and enterprise editions and localized for many languages around the world.
Its Analysis Services, an integral part of the system, includes online analytical processing (OLAP) and data-mining facilities.
Many development environments support SQL Server, including Microsoft’s Visual Studio and related products, in particular the .NET products and services.
While designing a database, the database administrator creates database objects such as tables, columns, keys, indices, relationships, constraints, and views.
To help create these objects, the SQL Server Management Studio provides access to visual database tools.
These tools provide three mechanisms to aid in database design: the Database Designer, the Table Designer, and the View Designer.
The Database Designer is a visual tool that allows the database owner or the owner’s delegates to create tables, columns, keys, indices, relationships, and constraints.
Within this tool, a user can interact with database objects through database diagrams, which graphically show the structure of the database.
The ViewDesigner provides a visual query tool that allows theuser to create ormodify SQL views through the use of Windows drag-and-drop capabilities.
Figure 30.1 shows a view opened from the Management Studio.
Queries and stored procedures can be developed and tested using the integrated Query Editor.
The Query Editor supports creating and editing scripts for a variety of environments, including Transact-SQL, the SQL Server scripting language SQLCMD, the multidimensional expression language MDX which is used for data analysis, the SQL Server data-mining language DMX, the XML-analysis language XMLA, and SQL Server Mobile.
Database tuning recommendations are provided by the Database Tuning Advisor.
The integrated Query Editor provides a simple graphical user interface for running SQL queries and viewing the results.
The Query Editor also provides a graphical representation of showplan, the steps chosen by the optimizer for query execution.
The Query Editor is integrated with Management Studio’s Object Explorer, which lets a user drag and drop object or table names into a query window and helps build select, insert, update, or delete statements for any table.
A database administrator or developer can use Query Editor to:
Analyze queries: Query Editor can show a graphical or textual execution plan for any query, as well as displaying statistics regarding the time and resources required to execute any query.
Use templates for stored procedures, functions, and basic SQL statements: The Management Studio comes with dozens of predefined templates for building DDL commands, or users can define their own.
Figure 30.2 shows the Management Studio with the Query Editor displaying the graphical execution plan for a query involving a four-table join and an aggregation.
Figure 30.2 A showplan for a four-table join with group by aggregation.
Dozens of different events can be captured, and dozens of data items can be captured for each event.
Using SQL Profiler, a user can choose to save the captured data to a file or a table, in addition to displaying it in the Profiler User Interface (UI)
The Profiler tool displays every event that meets the filter criteria as it occurs.
Once trace data are saved, SQL Profiler can read the saved data for display or analysis purposes.
On the server side is the SQL trace facility, which manages queues of events generated by event producers.
A consumer thread reads events from the queues and filters them before sending them to the process that requested them.
Events are the main unit of activity as far as tracing is concerned, and an event can be anything that happens inside SQL Server, or between SQL Server and a client.
For example, creating or dropping an object is an event, executing a stored procedure is an event, acquiring or releasing a lock is an event, and sending a Transact-SQL batch from a client to the SQL Server is an event.
There is a set of stored system procedures to define which events should be traced, what data for each event are interesting, and where to save the information collected from the events.
Filters applied to the events can reduce the amount of information collected and stored.
Queries and updates can often executemuch faster if an appropriate set of indices is available.
Designing the best possible indices for the tables in a large database is a complex task; it not only requires a thorough knowledge of how SQL Server uses indices and how the query optimizer makes its decisions, but how the data will actually be used by applications and interactive queries.
The SQL Server Database Tuning Advisor (DTA) is a powerful tool for designing the best possible indices and indexed (materialized) views based on observed query and update workloads.
In addition to providing access to the database design and visual database tools, the easy-to-use SQL Server Management Studio supports centralized management of all aspects of multiple installations of the SQL Server Database Engine, Analysis Services, Reporting Services, Integration Services, and SQL Server Mobile, including security, events, alerts, scheduling, backup, server configuration, tuning, full-text search, and replication.
Because multiple installations of SQL Server can be organized into groups and treated as a unit, SQL Server Management Studio can manage hundreds of servers simultaneously.
Although it can runon the same computer as the SQLServer engine, SQLServer Management Studio offers the same management capabilities while running on.
In addition, the efficient client–server architecture of SQL Server makes it practical to use the remote-access (dial-up networking) capabilities of Windows for administration and management.
It provides wizards to guide the database administrator through the process of setting up and maintaining an installation of SQL Server.
Management Studio’s interface is shown in Figure 30.3 and illustrates how a script for database backup can be created directly from its dialogs.
Transact-SQL supports most of the mandatory DDL query and data modification statements and constructs in the.
In addition to the mandatory features, Transact-SQL also supports many optional features in the SQL:2003 standard such as recursive queries, common table expressions, user-defined functions, and relational operators such as intersect and except among others.
An XML type, described in Section 30.11, which is used to store XML data inside a table column.
The XML type can optionally have an associated XML schema collection specifying a constraint that the instances of the type should adhere to one of the XML types defined in the schema collection.
This type is used by applications that need to store data whose type cannot be anticipated at data-definition time.
Internally, the system keeps track of the original type of the data.
It is possible to filter, join, and sort on sql variant columns.
The system function sql variant property returns details on the actual data stored in a column of type sql variant, including the base type and size information.
Hierarchical data are defined as a set of data items related to one another by hierarchical relationships where one item of data is the parent of another item.
Common examples include: an organizational structure, a hierarchical file system, a set of tasks in a project, a taxonomy of language terms, a singleinheritance type hierarchy, part-subpart relationships, and a graph of links among Web pages.
Common models of these data are the planar and geodetic coordinate systems.
The main distinction between these two systems is that the latter takes into account the curvature of the earth.
Server supports geometry and geography, which correspond to the planar and geodetic models.
In addition, SQL Server supports a table type and a cursor type that cannot be used as columns in a table, but can be used in the Transact-SQL language as variables:
A table type enables a variable to hold a set of rows.
An instance of this type is used primarily to hold temporary results in a stored procedure or as the return value of a table-valued function.
It has a well-defined scope, which is the function, stored procedure, or batch in which it is declared.Within its scope, a table variable may be used like a regular table.
It may be applied anywhere a table or table expression is used in select, insert, update, and delete statements.
A cursor type that enables references to a cursor object.
The cursor type can be used to declare variables, or routine input/output arguments to reference cursors across routine calls.
In addition to the SQL relational operators such as inner join and outer join, SQL Server supports the relational operators pivot, unpivot, and apply.
The name column from the input is called the pivot column.
The user needs to indicatewhich names to transpose from the input into individual columns in the output.
The following query, using the pivot operator, returns the SalesQty for each of the months Jan, Feb, and Mar as separate columns.
Note that the pivot operator also performs an implicit aggregation on all the other columns in the table and an explicit aggregation on the pivot column.
The apply operator is similar to join, except its right input is an expression that may contain references to columns in the left input, for example a tablevalued function invocation that takes as arguments oneormore columns from the left input.
The set of columns produced by the operator is the union of the columns from its two inputs.
The apply operator can be used to evaluate its right input for each row of its left input and perform a union all of the rows across all these evaluations.
There are two flavors of the apply operator similar to join, namely, cross and outer.
In the case of cross apply, this causes the corresponding row from the left input to not appear in the result.
In the case of outer apply, the row appears from the left input with NULL values for the columns in the right input.
Consider a table-valued function called FindReports that takes as input the ID of a given employee and returns the set of employees reporting directly or indirectly to that employee in an organization.
The following query calls this function for the manager of each department from the Departments table:
Users can write routines that run inside the server process as scalar or table functions, stored procedures, and triggers using Transact-SQL or a .NET language.
All these routines are defined to the database by using the corresponding create [function, procedure, trigger] DDL statement.
Scalar functions can be used in any scalar expression inside an SQL DML or DDL statement.
Table-valued functions can be used anywhere a table is allowed in a select statement.
Transact-SQL tablevalued functions whose body contains a single SQL select statement are treated as a view (expanded inline) in the query that references the function.
Since tablevalued functions allow input arguments, inline table-valued functions can be considered parameterized views.
In addition to traditional views as defined in ANSI SQL, SQL Server supports indexed (materialized) views.
Indexed views can substantially enhance the performance of complex decision support queries that retrieve large numbers of base table rows and aggregate large amounts of information into concise sums, counts, and averages.
Once a view is indexed, the optimizer can use its indices in queries that reference the view or its base tables.
There is no need for queries to refer to the view explicitly for the indexed view to be used in the query plan, as the matching is done automatically from the view definition.
This way, existing queries can benefit from the improved efficiency of retrieving data directly from the indexed view without having to be rewritten.
The indexed view is maintained consistent with the base tables by automatically propagating all updates.
A filtered index is an optimized nonclustered index, especially suited to cover queries that select from a well-defined subset of data.
It uses a filter predicate to index a portion of rows in the table.
Filtered indices can provide the following advantages over full-table indices:
A well-designed filtered index improves query performance and execution plan quality because it is smaller than a full-table nonclustered index and has filtered statistics.
The filtered statistics aremore accurate than full-table statistics because they cover only the rows in the filtered index.
An index is maintained only when data manipulation language (DML) statements affect the data in the index.
A filtered index reduces index maintenance costs compared to a full-table nonclustered index because it is smaller and is only maintained when the data in the index are affected.
It is possible to have a large number of filtered indices, especially when they contain data that are affected infrequently.
Similarly, if a filtered index contains only the frequently affected data, the smaller size of the index reduces the cost of updating the statistics.
Creating afiltered index can reducedisk storage for nonclustered indices when a full-table index is not necessary.
You can replace a full-table nonclustered index with multiple filtered indices without significantly increasing the storage requirements.
Filtered statistics can also be created explicitly, independently from filtered indices.
Generally, views canbe the target ofupdate, delete, or insert statements if the data modification applies to only one of the view’s base tables.
Updates to partitioned views can be propagated to multiple base tables.
For data modifications that affect more than one base table, the view can be updated if there is an instead trigger defined for the operation; instead triggers for insert, update, or delete operations can be defined on a view, to specify the updates that must be performed on the base tables to reflect the corresponding modifications on the view.
Triggers are Transact-SQL or .NET procedures that are automatically executed when either a DML (update, insert, or delete) or DDL statement is issued against a base table or view.
Triggers are mechanisms that enable enforcement of business logic automatically when data are modified or when DDL statements are executed.
Triggers can be classified into DML and DDL triggers depending on the kind of event that fires the trigger.
Triggers can be classified into after and instead triggers according to when the trigger gets invoked relative to the action that fires the trigger.
After triggers execute after the triggering statement and subsequent declarative constraints are enforced.
Instead triggers can be thought of as similar to before triggers, but they actually replace the triggering action.
In SQL Server, DML after triggers can be defined only on base tables, while DML instead triggers can be defined on base tables or views.
Instead triggers allow practically any view to bemade updatable via user-provided logic.
In SQL Server, a database refers to a collection of files that contain data and are supported by a single transaction log.
The database is the primary unit of administration in SQL Server and also provides a container for physical structures such as tables and indices and for logical structures such as constraints and views.
In order to manage space effectively in a database, the set of data files in a database is divided into groups called filegroups.
Every database has at least one filegroup known as the primary filegroup.
This filegroup contains all the metadata for the database in system tables.
If additional, user-defined filegroups have been created, a user can explicitly control the placement of individual tables, indices, or the large-object columns of a table by placing them in a particular filegroup.
For example, the user may choose to store performance critical indices on a filegroup located on solid state disks.
Likewise they may choose to place varbinary(max) columns containing video data on an I/O subsystem optimized for streaming.
One of the main purposes for filegroups is to allow for effective space management.
All data files are divided into fixed-size 8-kilobyte units called pages.
The allocation system is responsible for allocating these pages to tables and indices.
The goal of the allocation system is to minimize the amount of space wasted while, at the same time, keeping the amount of fragmentation in the database to a minimum to ensure good scan performance.
In order to achieve this goal, the allocationmanager usually allocates and deallocates all the pages in units of eight contiguous pages called extents.
These bitmaps allow the allocation system to find a page or an extent for allocation quickly.
These bitmaps are also used when a full table or index scan is executed.
The advantage of using allocation-based bitmaps for scanning is that it allows disk-order traversals of all the extents belonging to a table or index-leaf level, which significantly improves the scan performance.
If there is more than one file in a filegroup, the allocation system allocates extents for any object on that filegroup by using a “proportional fill” algorithm.
Each file is filled up in the proportion of the amount of free space in that file compared to other files.
This fills all the files in a filegroup at roughly the same rate and allows the system to utilize all the files in the filegroup evenly.
Files can also be configured to grow automatically if the filegroup is running out of space.
In order to shrink a data file, SQL Server moves all the data from the physical end of the file to a point closer to the beginning of the file and then actually shrinks the file, releasing space back to the operating system.
In a heaporganized table, the location of every row of the table is determined entirely by the system and is not specified in any way by the user.
The rows of a heap have a fixed identifier known as the row (RID), and this value never changes unless the file is shrunk and the row is moved.
If the row becomes large enough that it cannot fit in the page in which it was originally inserted, the record is moved to a different place but a forwarding stub is left in the original place so that the record can still be found by using its original RID.
In a clustered-index organization for a table, the rows of the table are stored in a B+-tree sorted by the clustering key of the index.
The clustered-index key also serves as the unique identifier for each row.
The key for a clustered index can be defined to be nonunique, in which case SQL Server adds an additional hidden column to make the key unique.
The clustered index also serves as a search structure to identify a row of the table with a particular key or scan a set of rows of the table with keys within a certain range.
A clustered index is the most common type of table organization.
Queries that refer only to columns that are available through secondary indices are processed by retrieving pages from the leaf level of the indices without having to retrieve data from the clustered index or heap.
Thus, the clustered index rows canmove to adifferent page (via splits, defragmentation, or even index rebuilds) without requiring changes to the nonclustered indices.
A computed column is a column whose value is an expression, usually based on the value of other columns in that row.
A partitioned index ismade up ofmultiple B+-trees, one per partition.
A partitioned table without an index (a heap) is made up of multiple heaps, one per partition.
For brevity, we refer only to partitioned indices (clustered or nonclustered) and ignore heaps for the rest of this discussion.
Partitioning a large index allows an administrator more flexibility in managing the storage for the index and can improve some query performance because the partitions act as a coarse-grained index.
The partitioning for an index is specified by providing both a partitioning function and a partitioning scheme.
A partitioning function maps the domain of a partitioning column (any column in the index) to partitions numbered 1 to N.
A partitioning schememaps partition numbers produced by a partitioning function to specific filegroups where the partitions are stored.
Building new indices and rebuilding existing indices on a table can be performed online, i.e., while select, insert, delete, and update operations are being performed on the table.
The creation of a new index happens in three phases.
The first phase is simply creating an empty B+-tree for the new index with the catalog showing the new index is available for maintenance operations.
That is, the new index must be maintained by all subsequent insert, delete, and update operations, but it is not available for queries.
The second phase consists of scanning the table to retrieve the index columns for each row, sorting the rows and inserting them into the new B+-tree.
These inserts must be careful to interact with the other rows in the new B+-tree placed there by index maintenance operations from updates on the base table.
The scan is a snapshot scan that, without locking, ensures the scan sees the entire table with only the results of committed transactions as of the start of the scan.
This is achieved by using the snapshot isolation technique described in Section 30.5.1
The final phase of the index build involves updating the catalog to indicate the index build is complete and the index is available for queries.
Execution of queries in SQL Server can involve a variety of different scanmodes on the underlying tables and indices.
Each of the scan modes has a read-ahead mechanism that tries to keep the scan ahead of the needs of the query execution, in order to reduce seek and latency overheads and utilize disk idle time.
The SQL Server read-ahead algorithm uses the knowledge from the query-execution plan in order to drive the read-ahead and make sure that only data that are actually needed by the query are read.
Also, the amount of read-ahead is automatically scaled according to the size of the buffer pool, the amount of I/O the disk subsystem can sustain, and the rate at which the data are being consumed by query execution.
Row compression uses a variable-length format for data types such as integers that are traditionally considered fixed-length.
Page compression removes common prefixes on columns and builds a per-page dictionary for common values.
The query processor of SQL Server is based on an extensible framework that allows rapid incorporation of new execution and optimization techniques.
Any SQL query can be expressed as a tree of operators in an extended relational algebra.
Abstracting operators of this algebra into iterators, query execution encapsulates data-processing algorithms as logical units that communicate with each other by using a GetNextRow() interface.
Starting out with an initial query tree, the query optimizer generates alternatives by using tree transformations and estimates their execution cost by taking into account iterator behavior and statistical models to estimate the number of rows to process.
Complex queries present significant optimization opportunities that require reordering operators across query block boundaries and selecting plans solely on the basis of estimated costs.
To go after these opportunities, the query optimizer deviates from traditional query-optimization approaches used in other commercial systems in favor of a more general, purely algebraic framework that is based on the Cascades optimizer prototype.
Query optimization is part of the querycompilation process, which consists of four steps:
After parsing, the binder resolves table and column names by using the catalogs.
If no cached plan is available, an initial operator tree is generated.
The optimizer applies simplification rules on the operator tree to obtain a normal, simplified form.
During simplification, the optimizer determines and loads statistics required for cardinality estimation.
The optimizer applies exploration and implementation rules to generate alternatives, estimates execution cost, and chooses the plan with the cheapest anticipated cost.
Exploration rules implement reordering for an extensive set of operators, including join and aggregation reordering.
Implementation rules introduce execution alternatives such as merge join and hash join.
To achieve best results, cost-based optimization is not divided into phases that optimize different aspects of the query independently; also, it is not restricted to a single dimension such as join enumeration.
Instead, a collection of transformation rules defines the space of interest, and cost estimation is used uniformly to select an efficient plan.
During simplification, only transformations that are guaranteed to generate less costly substitutes are applied.
The optimizer pushes selects down the operator tree as far as possible; it checks predicates for contradictions, taking into account declared constraints.
It uses contradictions to identify subexpressions that can be removed from the tree.
A common scenario is the elimination of union branches that retrieve data from tables with different constraints.
A number of simplification rules are context dependent; that is, the substitution is valid only in the context of utilization of the subexpression.
For example, an outer join can be simplified into an inner join if a later filter operation will discard nonmatching rows that were padded with null.
Another example is the elimination of joins on foreign keys, when there are no later uses of columns from the referenced table.
A third example is the context of duplicate insensitivity, which specifies that delivering one or multiple copies of a row does not affect the query result.
Subexpressions under semijoins and under distinct are duplicate insensitive, which allows turning union into union all, for example.
For grouping and aggregation, the GbAgg operator is used, which creates groups and optionally applies an aggregate function on each group.
Duplicate removal, expressed in SQL by the distinct keyword, is simply a GbAgg with no aggregate functions to compute.
During simplification, information about keys and functional dependencies is used to reduce grouping columns.
Subqueries are normalized by removing correlated query specifications and using some join variant instead.
A variety of execution strategies is then considered during cost-based optimization.
In SQL Server, transformations are fully integrated into the cost-based generation and selection of execution plans.
The query optimizer includes about 350 logical and physical transformation rules.
In addition to inner-join reordering, the query optimizer employs reordering transformations for the operators outer join, semijoin, and antisemijoin, from the standard relational algebra (with duplicates, for SQL)
GbAgg is reordered as well, by moving it below or above joins when possible.
Partial aggregation, that is, introducing a new GbAggwith grouping on a superset of the columns of a subsequent GbAgg, is considered below joins and union all, and also in parallel plans.
See the references given in the bibliographical notes for details.
Correlated execution is considered during plan exploration, the simplest case being index-lookup join.
Apply executes E for each row of T , which provides parameter values.
Correlated execution is considered as an execution alternative, regardless of the use of subqueries in the original SQL formulation.
It is a very efficient strategy when table T is small and indices support efficient parameterized execution of E(t)
Furthermore, we consider reduction on the number of executions of E(t) when there are duplicate parameter values, by means of two techniques: Sort T on parameter values so that a single result of E(t) is reused while the parameter value remains the same, or else use a hash table that keeps track of the result of E(t) for (some subset of) earlier parameter values.
Some applications select rows on the basis of some aggregate result for their group.
For example, “Find customers whose balance is more than twice the average for their market segment.” The SQL formulation requires a self-join.
During exploration, this pattern is detected and per-segment execution over a single scan is considered as an alternative to self-join.
View matching interacts with operator reordering in that utilization may not be apparent until some other reordering has taken place.
When a view is found to match some subexpression, the table that contains the view result is added as an alternative for the corresponding expression.
To estimate the execution cost of a plan, the model takes into account the number of times a subexpression is executed, as well as the row goal, which is the number of rows expected to be consumed by the parent operator.
The row goal can be less than the cardinality estimate in the case of top-n queries, and for Apply/semijoin.
For example, Apply/semijoin outputs row t from T as soon as a single row is produced by E(t) (that is, it tests exists E(t))
Update plans optimize maintenance of indices, verify constraints, apply cascading actions, and maintain materialized views.
For index maintenance, instead of taking each row andmaintaining all indices for it, update plans may apply modifications per index, sorting rows and applying the update operation in key order.
This minimizes random I/O, especially when the number of rows to update is large.
Constraints are handled by an assert operator, which executes a predicate and raises an error if the result is false.
Referential constraints are defined by exists predicates, which in turn become semijoins and are optimized by considering all execution algorithms.
As a result of the update, rows will move forward in the index andwill be found and updated again, leading to an infinite loop.
One way to address this problem is to separate processing into two phases: First read all rows that will be updated and make a copy of them in some temporary place, then read from this place and apply all updates.
Another alternative is to read from a different index where rows will not move as a result of the update.
Some execution plans provide phase separation automatically, if they sort or build a hash table on the rows to be updated.
Multiple plans that provide the required property are considered, and one is selected on the basis of estimated execution cost.
The computation of result size estimates is based on statistics for columnsused in a given expression.
These statistics consist ofmax-diff histograms on the column values and a number of counters that capture densities and row sizes, among others.
Database administrators may create statistics explicitly by using extended SQL syntax.
If no statistics are available for a given column, however, SQL Server’s optimizer puts the ongoing optimization on hold and gathers statistics as needed.
As soon as the statistics are computed, the original optimization is resumed, leveraging the newly created statistics.
Typically, after a short period of time, statistics for frequently used columns have been created and interruptions to gather new statistics become infrequent.
By keeping track of the number of rows modified in a table, a measure of staleness is maintained for all affected statistics.
Once the staleness exceeds a certain threshold the statistics are recomputed and cached plans are recompiled to take changed data distributions into account.
Statistics can be recomputed asynchronously, which avoids potentially long compile times caused by synchronous computation.
However, subsequent queries are able to leverage the recomputed statistics.
This allows striking an acceptable balance between time spent in optimization and the quality of the resulting query plan.
Cost-based query optimizers face the issue of search-space explosion because applications do issue queries involving dozens of tables.
To address this, SQL Server uses multiple optimization stages, each of which uses query transformations to explore successively larger regions of the search space.
There are simple and complete transformations geared toward exhaustive optimization, as well as smart transformations that implement various heuristics.
Smart transformations generate plans that are very far apart in the search space, while simple transformations explore neighborhoods.
Optimization stages apply a mix of both kinds of transformations, first emphasizing smart transformations, and later transitioning to simple transformations.
Optimum results on subtrees are preserved, so that later stages can take advantage of results generated earlier.
Exhaustive generation of alternatives: To generate the complete space, the optimizer uses complete, local, nonredundant transformations—a transformation rule that is equivalent to a sequence ofmoreprimitive transformations would only introduce additional overhead.
Heuristic generation of candidates: A handful of interesting candidates (selected on the basis of estimated cost) are likely to be far apart in terms of primitive transformation rules.
Optimization can be terminated at any point after a first plan has been generated.
Such termination is based on the estimated cost of the best plan found and the time spent already in optimization.
For example, if a query requires only looking up a few rows in some indices, a very cheap plan will likely be produced quickly in the early stages, terminating optimization.
This approach enabled adding new heuristics easily over time, without compromising either cost-based selection of plans, or exhaustive exploration of the search space, when appropriate.
Execution algorithms support both sort-based and hash-based processing, and their data structures are designed to optimize use of processor cache.
Hash operations support basic aggregation and join, with a number of optimizations, extensions, and dynamic tuning for data skew.
The flow-distinct operation is a variant of hash-distinct, where rows are output early, as soon as a new distinct value is found, instead of waiting to process the complete input.
Correlated plans specify executing E(t), often including some index lookup based on the parameter, for each row t of a table T.
Asynchronous prefetching allows issuing multiple index-lookup requests to the storage engine.
It is implemented this way: A nonblocking index-lookup request is made for a row t of T , then t is placed in a prefetch queue.
Rows are taken out of the queue and used by apply to execute E(t)
Execution of E(t) does not require that data be already in the buffer pool, but having outstanding prefetch operations maximizes hardware utilization and increases performance.
The size of the queue is determined dynamically as a function of cache hits.
If no ordering is required on the output rows of apply, rows from the queue may be taken out of order, to minimize waiting on I/O.
Parallel execution is implemented by the exchange operator, which manages multiple threads, partitions or broadcasts data, and feeds the data to multiple processes.
The query optimizer decides exchange placement on the basis of estimated cost.
The degree of parallelism is determined dynamically at runtime, according to the current system utilization.
Index plans are made up of the pieces described earlier.
For example, we consider the use of an index join to resolve predicate conjunctions (or index union, for disjunctions), in a cost-based way.
Such a join can be done in parallel, using any of SQL Server’s join algorithms.
We also consider joining indices for the sole purpose of assembling a rowwith the set of columns needed on a query, which is sometimes faster than scanning a base table.
Taking record IDs from a secondary index and locating the corresponding row in a base table is effectively equivalent to performing index-lookup join.
For this, we use our generic correlated execution techniques such as asynchronous prefetch.
Communication with the storage engine is done through OLE-DB, which allows accessing other data providers that implement this interface.
OLE-DB is the mechanism used for distributed and remote queries, which are driven directly by the query processor.
Data providers are categorized according to the range of functionality they provide, ranging from simple rowset providers with no indexing capabilities to providers with full SQL support.
In SQL Server all statements are atomic and applications can specify various levels of isolation for each statement.
A single transaction can include statements that not only select, insert, delete, or update records, but also create or drop tables, build indices, and bulk-import data.
When transactions are spread across servers, SQL Server uses a.
Windows operating-system service called the Microsoft Distributed Transaction Coordinator (MSDTC) to perform two-phase commit processing.
Concurrency control based on locking is the default for SQL Server.
Optimistic concurrency control is based on the assumption that resource conflicts betweenmultiple users are unlikely (but not impossible), and allows transactions to execute without locking any resources.
Only when attempting to change data does SQL Server check resources to determine if any conflicts have occurred.
If a conflict occurs, the application must read the data and attempt the change again.
Applications can choose to detect changes either by comparing values or by checking a special row version column on a row.
In addition, SQL Server supports two snapshot-based isolation levels (snapshot isolation is described earlier in Section 15.7)
Snapshot: Specifies that data read by any statement in a transaction will be the transactionally consistent version of the data that existed at the start of the transaction.
The effect is as if the statements in a transaction see a snapshot of the committed data as it existed at the start of the transaction.
Writes are validated using the validation steps described in Section 15.7, and permitted to complete only if the validation is successful.
Read committed snapshot: Specifies that each statement executed within a transaction sees a transactionally consistent snapshot of the data as it existed at the start of the statement.
This contrasts with read committed isolation where the statement may see committed updates of transactions that commit while the statement is executing.
Locking is the primary mechanism used to enforce the semantics of the isolation levels.
All updates acquire sufficient exclusive locks held for the duration of the transaction to prevent conflicting updates from occurring.
Shared locks are held for various durations to provide the different SQL isolation levels for queries.
To minimize the cost of locking, SQL Server locks resources automatically at a granularity appropriate to the task.
Locking at a smaller granularity, such as rows, increases concurrency, but has a higher overhead because more locks must be held if many rows are locked.
Row identifier; used to lock a single row within a table Row lock within an index; protects key ranges in serializable transactions Contiguous group of eight data pages or index pages Entire table, including all data and indices Database.
Fine-granularity locking can improve concurrency at the cost of extra CPU cycles andmemory to acquire and holdmany locks.
For many queries, a coarser locking granularity provides better performancewith no (orminimal) loss of concurrency.
Database systems have traditionally required query hints and table options for applications to specify locking granularity.
In addition, there are configuration parameters (often static) for how much memory to dedicate to the lock manager.
In SQL Server, locking granularity is optimized automatically for optimal performance and concurrency for each index used in a query.
In addition, the memory dedicated to the lock manager is adjusted dynamically on the basis of feedback from other parts of the system, including other applications on the machine.
Lock granularity is optimized before query execution for each table and index used in the query.
The lock optimization process takes into account isolation level (that is, how long locks are held), scan type (range, probe, or entire table), estimated number of rows to be scanned, selectivity (percentage of visited rows that qualify for the query), row density (number of rows per page), operation type (scan, update), user limits on the granularity, and available systemmemory.
Once a query is executing, the lock granularity is escalated automatically to table level if the system acquires significantly more locks than the optimizer expected or if the amount of available memory drops and cannot support the number of locks required.
Threads and communication buffers can also be involved in deadlocks.
When SQL Server detects a deadlock, it chooses as the deadlock victim the transaction thatwould be.
If deadlocks are infrequent, the detection algorithm runs every 5 seconds.
If they are frequent it will begin checking every time a transaction waits for a lock.
The two snapshot-based isolation levels use row versioning to achieve isolation for queries while not blocking the queries behind updates and vice versa.
Under snapshot isolation, update and delete operations generate versions of the affected rows and store them in a temporary database.
The versions are garbage-collected when there are no active transactions that could require them.
Therefore, a query run under snapshot isolation does not need to acquire locks and instead can read the older versions of any record that gets updated/deletedby another transaction.
Row versioning is also used to provide a snapshot of a table for online index build operations.
SQLServer is designed to recover fromsystemandmedia failures, and the recovery system can scale to machines with very large buffer pools (100 gigabytes) and thousands of disk drives.
Logically, the log is a potentially infinite stream of log records identified by log sequence numbers (LSNs)
Physically, a portion of the stream is stored in log files.
Log records are saved in the log files until they have been backed up and are no longer needed by the system for rollback or replication.
Log files grow and shrink in size to accommodate the records that need to be stored.
Additional log files can be added to a database (on new disks, for example) while the system is running and without blocking any current operations, and all logs are treated as if they were one continuous file.
The server dynamically adjusts the checkpoint frequency to reduce recovery time to within the recovery interval.
Checkpoints flush all dirty pages from the buffer pool and adjust to the capabilities of the I/O system and its current workload to effectively eliminate any impact on running transactions.
Upon start-up after a crash, the system starts multiple threads (automatically scaled to the number of CPUs) to start recovering multiple databases in parallel.
The first phase of recovery is an analysis pass on the log,which builds a dirty page table and active transaction list.
The next phase is a redo pass starting from the last checkpoint and redoing all operations.
During the redo phase, the dirty page table is used to drive read-ahead of data pages.
The final phase is an undo phase where incomplete transactions are rolled back.
The undophase is actually divided into two parts as SQL Server uses a two-level recovery scheme.
Transactions at the first level (those involving internal operations such as space allocation and page splits) are rolled back first, followed by user transactions.
Once the transactions at the first level are rolled back, the database is brought online and is available for new user transactions to start while the final rollback operations are performed.
This is achieved by having the redo pass reacquire locks for all incomplete user transactions that will be rolled back in the undo phase.
Additionally, backing up and restoring databases is useful for other purposes, such as copying a database from one server to another andmaintaining standby systems.
By specifying a recoverymodel, an administrator declares the type of recovery capabilities required (such as point-in-time restore and log shipping) and the required backups to achieve them.
Backups can be taken on databases, files, file-groups, and the transaction log.
All backups are fuzzy and completely online; that is, they do not block any DML or DDL operations while they execute.
Restores can also be done online such that only the portion of the database being restored (e.g., a corrupt disk block) is taken offline.
Backup and restore operations are highly optimized and limited only by the speed of the media onto which the backup is targeted.
Databasemirroring involves immediately reproducing everyupdate to adatabase (the principal database) onto a separate, complete copy of the database (themirror database) generally located on another machine.
In the event of a disaster on the primary server or even just maintenance, the system can automatically failover to themirror in amatter of seconds.
The communication library used by applications is aware of the mirroring and will automatically reconnect to the mirror machine in the event of a failover.
A tight coupling between the primary database and the mirror is achieved by sending blocks of transaction log to the mirror as it is generated on the primary and redoing the log records on the mirror.
In full-safety mode, a transaction cannot commit until the log records for the transaction have made it to disk on the mirror.
An SQL Server instance is a single operating-system process that is also a named endpoint for requests for SQL execution.
Applications interact with SQL Server via various client-side libraries (like ODBC, OLE-DB, and ADO.NET) in order to execute SQL.
In order to minimize the context switching on the server and to control the degree of multiprogramming, the SQL Server process maintains a pool of threads that execute client requests.
As requests arrive from the client, they are assigned a thread on which to execute.
The thread executes the SQL statements issued by the client and sends the results back to it.
Once the user request completes, the thread is returned back to the thread pool.
In addition to user requests, the thread pool is used to assign threads for internal background tasks such as:
Lazywriter: This thread is dedicated to making sure a certain amount of the buffer pool is free and available at all times for allocation by the system.
The thread also interacts with the operating system to determine the optimal amount of memory that should be consumed by the SQL Server process.
Checkpoint: This thread periodically checkpoints all databases in order to maintain a fast recovery interval for the databases on server restart.
Deadlock monitor: This thread monitors the other threads, looking for a deadlock in the system.
It is responsible for the detection of deadlocks and also picking a victim in order to allow the system to make progress.
When the query processor chooses a parallel plan to execute a particular query, it can allocate multiple threads that work on behalf of the main thread to execute the query.
Since the Windows NT family of operating systems provides native thread support, SQL Server uses NT threads for its execution.
However, SQL Server can be configured to run with user-mode threads in addition to kernel threads in very high-end systems to avoid the cost of a kernel context switch on a thread switch.
There are many different uses of memory within the SQL Server process:
The biggest consumer of memory in the system is the buffer pool.
The buffer pool maintains a cache of the most recently used database pages.
The buffers also obey the write-ahead logging protocol to ensure correctness of crash and media recovery.
This is the memory that is allocated dynamically to execute requests submitted by the user.
This cache stores the compiled plans for various queries that have been previously executed by users in the system.
This allows various users to share the same plan (saving memory) and also saves on query compilation time for similar queries.
These are for query operators that consume large amounts of memory, such as hash join and sort.
A single memory manager centrally manages all the memory used by SQL Server.
The memory manager is responsible for dynamically partitioning and redistributing the memory between the various consumers of memory in the system.
It distributes this memory in accordance with an analysis of the relative cost benefit of memory for any particular use.
A generalized LRU infrastructure mechanism is available to all components.
This caching infrastructure tracks not only the lifetime of cached data but also the relative CPU and I/O costs incurred to create and cache it.
This information is used to determine the relative costs of various cached data.
Thememorymanager focuses on throwing out the cached data that have not been touched recently and were cheap to cache.
As an example, complex query plans that require seconds of CPU time to compile are more likely to stay in memory than trivial plans, given equivalent access frequencies.
The memory manager interacts with the operating system to decide dynamically how much memory it should consume out of the total amount of memory in the system.
This allows SQL Server to be quite aggressive in using the memory on the system but still return memory back to the system when other programs need it without causing excessive page faults.
In addition thememorymanager is aware of the CPU andmemory topology of the system.
Specifically, it leverages the NUMA (nonuniform memory access) that many machines employ and attempts to maintain locality between the processor that a thread is executing on and the memory it accesses.
Authentication can be either through a username–password pair managed by SQL Server, or through a Windows OS account.
Authorization is managed by permission grants to schema objects or covering permissions on container objects such as the database or server instance.
Audits records are written either in a file or to the Windows Security Log.
Transparent Data Encryption encrypts all data pages and log pages when written to disk and decrypts when read from the disk so that the data are encrypted at rest on the disk but is plaintext to SQL Server users without application modification.
Transparent Data Encryption can be more CPU efficient than manual encryption as data is only encrypted when written to disk and it is done in larger units, pages, rather than individual cells of data.
The quality of the code base is enhanced by using the Security Development Lifecycle.
All developers and testers of the product go through security training.
All features are threat modeled to assure assets are appropriately protected.
Wherever possible, SQL Server utilizes the underlying security features of the operating system rather than implementing its own, such asWindows OS Authorization and theWindows Security Log for an audit record.
Furthermore, numerous internal tools areutilized to analyze the codebase looking for potential security flaws.
Security is verified using fuzz testing2 and testing of the threat model.
Before release, there is a final security review of the product and a response plan is in place for dealing with security issues found after release which is then executed as issues are discovered.
A number of features are provided to help users secure the system properly.
One such feature is a fundamental policy called off-by-default, where many less commonly used components or those requiring extra care for security, are completely disabled by default.
Another feature is a best-practices analyzer that warns users about configurations of system settings that could lead to a security vulnerability.
Policy-based management further allows users to define what the settings should be and either warns of or prevents changes that would conflict with the approved settings.
This is Microsoft’s implementation of the standard SQL:1999 calllevel interface (CLI)
It includes object models—Remote Data Objects (RDOs) and Data Access Objects (DAOs)—that make it easier to program multitier database applications from programming languages like Visual Basic.
This is a low-level, systems-oriented API designed for programmers building database components.
The interface is architected according to the Microsoft Component Object Model (COM), and it enables the encapsulation of low-level database services such as rowset providers, ISAM providers, and query engines.
OLE-DB is used inside SQL Server to integrate the relational query processor and the storage engine and to enable replication and distributed access to SQL and other external data sources.
Like ODBC, OLE-DB includes a higher-level object model called ActiveX Data Objects (ADO) to make it easier to program database applications from Visual Basic.
This is an API designed for applications written in .NET languages such as C# and Visual Basic.NET.
This interface simplifies some common data access patterns supported by ODBC and OLE-DB.
In addition, it provides a new data setmodel to enable stateless, disconnected data access applications.
ADO.NET includes the ADO.NET Entity Framework, which is a platform for programming against data that raises the level of abstraction from the logical (relational) level to the conceptual (entity) level, and thereby significantly reduces the impedance mismatch for applications and data services such as reporting, analysis, and replication.
The conceptual data model is implemented using an extended relational model, the Entity Data Model (EDM) that embraces entities and relationships as first-class concepts.
It includes a query language for the EDM called Entity SQL, a comprehensive mapping engine that translates from the conceptual to the logical (relational) level, and a set of model-driven tools that help developers define mappings between objects and entities to tables.
The query expressions are not processed by an external tool or language preprocessor but instead are first-class expressions of the languages themselves.
C# and Visual Basic also support query comprehensions, i.e., language syntax extensions that leverage the standard query operators.
The DB-Library for C API that was developed specifically to be used with earlier versions of SQL Server that predate the SQL-92 standard.
Applications can use HTTP/SOAP requests to invoke SQL Server queries and procedures.
The URL can contain an XPath query, a Transact-SQL statement, or an XML template.
Objects in these linked servers can be referenced inTransact-SQL statements using the four-part name convention described below.
For example, if a linked server name of DeptSQLSrvr is defined against another copy of SQL Server, the following statement references a table on that server:
An OLE-DB data source is registered in SQL Server as a linked server.
Once a linked server is defined, its data can be accessed using the four-part name:
The following example establishes a linked server to an Oracle server via an OLE-DB provider for Oracle:
In addition, SQL Server supports built-in, parameterized table-valued functions called openrowset and openquery, which allow sending uninterpreted queries to a provider or linked server, respectively, in the dialect supported by the provider.
The following query combines information stored in an Oracle server and a Microsoft Index Server.
It lists all documents and their author containing the words Data and Access ordered by the author’s department and name.
The relational engine uses the OLE-DB interfaces to open the rowsets on linked servers, to fetch the rows, and tomanage transactions.
For eachOLE-DBdata source accessed as a linked server, an OLE-DB provider must be present on the server running SQL Server.
The set of Transact-SQL operations that can be used against a specific OLE-DB data source depends on the capabilities of the OLE-DB provider.
Whenever it is cost-effective, SQL Server pushes relational operations such as joins, restrictions, projections, sorts, and group by operations to the OLE-DB data source.
The publisher is a server that makes data available for replication to other servers.
The publisher can have one or more publications, each representing a logically related set of data and database objects.
The addition of an article to a publication allows for extensive customizing of the way the object is replicated, e.g., restrictions on which users can subscribe to receive its data and how the data set should be filtered on the basis of a projection or selection of a table, by a “horizontal” or a “vertical” filter, respectively.
Subscribers are servers that receive replicated data from a publisher.
Subscribers can conveniently subscribe to only the publications they require from one or more publishers regardless of the number or type of replication options each implements.
Depending on the type of replication options selected, the subscriber either can be used as a read-only replica or can make data changes that are automatically propagated back to the publisher and subsequently to all other replicas.
Subscribers can also republish the data they subscribe to, supporting as flexible a replication topology as the enterprise requires.
The distributor is a server that plays different roles, depending on the replication options chosen.
At a minimum it is used as a repository for history and error state information.
In other cases, it is used additionally as an intermediate store-and-forward queue to scale up the delivery of the replicated payload to all the subscribers.
Microsoft SQL Server replication offers a wide spectrum of replication options.
To decide on the appropriate replication options to use, a database designer must determine the application’s needs with respect to autonomous operation of the sites involved and the degree of transactional consistency required.
Snapshot replication copies and distributes data and database objects exactly as they appear at a moment in time.
Snapshot replication does not require continuous change tracking because changes are not propagated incrementally to subscribers.
Subscribers are updated with a complete refresh of the data set defined by the publication on a periodic basis.
Options available with snapshot replication can filter published data and can enable subscribers to modify replicated data and propagate those changes back to the publisher.
This type of replication is best suited for smaller sizes of data and when updates typically affect enough of the data that replicating a complete refresh of the data is efficient.
With transactional replication, the publisher propagates an initial snapshot of data to subscribers, then forwards incremental data modifications to subscribers as discrete transactions and commands.
Incremental change tracking occurs inside the core engine of SQL Server, which marks transactions affecting replicated objects in the publishing database’s transaction log.
A replication process called the log reader agent reads these transactions from the database transaction log, applies an optional filter, and stores them in the distribution database, which acts as the reliable queue supporting the store-and-forward mechanism of transactional replication.
Reliable queues are the same as durable queues, described in Section 26.1.1
Another replication process, called the distribution agent, then forwards the changes to each subscriber.
Like snapshot replication, transactional replicationoffers subscribers the option tomakeupdates that eitheruse two-phase.
This type of replication is suitable when intermediate states between multiple updates need to be preserved.
Merge replication allows each replica in the enterprise to work with total autonomy whether online or offline.
The system tracks metadata on the changes to published objects at publishers and subscribers in each replicated database, and the replication agent merges those data modifications together during synchronization between replicated pairs and ensures data convergence through automatic conflict detection and resolution.
Numerous conflict resolution policy options are built into the replication agent used in the synchronization process, and custom conflict resolution can be written by using stored procedures or by using an extensible component object model (COM) interface.
This type of replication does not replicate all intermediate states but only the current state of the data at the time of synchronization.
It is suitable when replicas require the ability to make autonomous updates while not connected to any network.
The ability to run application code inside the database adds flexibility to the design of application architectures that require business logic to execute close to the data and cannot afford the cost of shipping data to a middle-tier process to perform computation outside the database.
The runtime usesmetadata to locate and load classes, lay out instances in memory, resolve method invocations, generate native code, enforce security, and set runtime context boundaries.
Application code is deployed inside the database by using assemblies, which are the units of packaging, deployment, and versioning of application code in .NET.
Deployment of application code inside the database provides a uniform way to administer, back up, and restore complete database applications (code and data)
Once an assembly is registered inside the database, users can expose entry points within the assembly via SQL DDL statements, which can act as scalar or table functions, procedures, triggers, types, and aggregates, by usingwell-defined extensibility contracts enforced during the execution of these DDL statements.
Stored procedures, triggers, and functions usually need to execute SQL queries and updates.
This is achieved through a component that implements the ADO.NET data-access API for use inside the database process.
In the .NET framework, a programmer writes program code in a high-level programming language that implements a class defining its structure (e.g., the fields or properties of the class) and methods.
The compilation of the program produces a file, called an assembly, containing the compiled code in the Microsoft Intermediate Language (MSIL), and a manifest containing all references to dependent assemblies.
The manifest is an integral part of every assembly that renders the assembly self-describing.
The assembly manifest contains the assembly’s metadata, which describes all structures, fields, properties, classes, inheritance relationships, functions, andmethods defined in the program.
The manifest establishes the assembly identity, specifies the files that make up the assembly implementation, specifies the types and resources that make up the assembly, itemizes the compile-time dependencies on other assemblies, and specifies the set of permissions required for the assembly to run properly.
This information is used at runtime to resolve references, enforce version-binding policy, and validate the integrity of loaded assemblies.
The .NET framework supports an out-of-band mechanism called custom attributes for annotating classes, properties, functions and methods with additional information or facets the application may want to capture in metadata.
All .NET compilers consume these annotations without interpretation and store them in the assembly’s metadata.
All these annotations can be examined in the same way as any other metadata by using a common set of reflection APIs.Managed code refers to MSIL executed in the CLR rather than directly by the operating system.
Managed-code applications gain common-language runtime services such as automatic garbage collection, runtime type checking, and security support.
At execution time, a just-in-time (JIT) compiler translates the MSIL into native code (e.g., Intel X86 code)
During this translation, code must pass a verification process that examines the MSIL and metadata to find out whether the code can be determined to be type safe.
If user code running inside the DBMS can directly call the operating-system (OS) threading primitives, then it does not integrate well with the SQL Server task scheduler and can degrade the scalability of the system.
The different models for threading, scheduling, and memory management present an integration challenge for a DBMS that scales to support thousands of concurrent user sessions.
The CLR calls low-level primitives implemented by SQL Server for threading, scheduling, synchronization, and memory management (see Figure 30.5)
In order to synchronize between multiple threads, the CLR calls SQL Server synchronization objects.
This allows SQL Server scheduler to schedule other tasks when a thread is waiting on a synchronization object.
For instance, when the CLR initiates garbage collection, all of its threads wait for garbage collection to finish.
Further, this enables SQL Server to detect deadlocks that involve locks taken by CLR synchronization objects and employ traditional techniques for deadlock removal.
The SQL Server scheduler has the ability to detect and stop threads that have not yielded for a significant amount of time.
The ability to hook CLR threads to SQL Server threads implies that the SQL Server scheduler can identify runaway threads running in the CLR andmanage their priority, so that they do not consume significant CPU resources, thereby affecting the throughput of the system.
Such runaway threads are suspended and put back in the queue.
Repeat offenders are not allowed timeslices that are unfair to other executing workers.
The CLR calls SQL Server primitives for allocating and deallocating its memory.
Since the memory used by the CLR is accounted for in the total memory usage of the system, SQL Server can stay within its configured memory limits and ensure the CLR and SQL Server are not competing with each other for memory.
All user-managed code runningwithin the SQLServer process interactswithDBMS components as an extension.
Current extensions include scalar functions, table functions, procedures, triggers, scalar types, and scalar aggregates.
For each extension there is a mutual contract defining the properties or services user code must implement to act as one of these extensions as well as the services the extension can expect from the DBMS when the managed code is called.
All relational and assembly metadata are processed inside the SQL engine through a uniform set of interfaces and data structures.
When data-definition language (DDL) statements registering a particular extension function, type, or aggregate are processed, the system ensures the user code implements the appropriate contract by analyzing its assembly metadata.
If the contract is implemented, then the DDL statement succeeds, otherwise it fails.
The next subsections describe key aspects of the specific contracts currently enforced by SQL Server.
We classify scalar functions, procedures, and triggers generically as routines.
Routines, implemented as static class methods, can specify the following properties through custom attributes.
If this Boolean property is false, then it indicates the routine body involves imprecise computations such as floating-point operations.
If the value of this property is read, then the routine reads user-data tables.
Otherwise, the value of the property is None indicating the routine does not access data.
Queries that do not access any user tables (directly or indirectly through views and functions) are not considered to have user-data access.
If the value of this property is read, then the routine reads system catalogs or virtual system tables.
If this property is true, then the routine is assumed to produce the same output value given the same input values, state of the local database, and execution context.
If the value of this property is true, then the routine accesses resources outside SQL Server such as files, network, Web access, and registry.
A class implementing a table-valued function must implement an interface IEnumerable to enable iteration over the rows returned by the function, a method to describe the schema of the table returned (i.e., columns, types), a method to describe what columns can be unique keys, and a method to insert rows into the table.
This is the maximum size of the serialized binary representation of type instances in bytes.
This is a Boolean property specifying whether the instances of the type have fixed or variable length.
This is a Boolean property indicating whether the serialized binary representation of the type instances is binary ordered.
When this property is true, the system can perform comparisons directly against this representation without the need to instantiate type instances as objects.
Nullability.All UDTs in our systemmust be capable of holding the null value by supporting the INullable interface containing the Boolean IsNullmethod.
All UDTs must implement conversions to and from character strings via the ToString and Parsemethods.
In addition to supporting the contract for types, user-defined aggregates must implement four methods required by the query-execution engine to initialize the computation of an aggregate instance, to accumulate input values into the function provided by the aggregate, to merge partial computations of the aggregate, and to retrieve the final aggregate result.
Aggregates can declare additional properties, via custom attributes, in their class definition; these properties are used by the query optimizer to derive alternative plans for the aggregate computation.
If this property is true, then null rows can be discarded from the input.
However, care must be taken in the context of group by operations not to discard entire groups.
If this property is true, then the query processor can ignore order by clauses and explore plans that avoid having to sort the data.
Relational database systems have embraced XML inmany differentways in recent years.
First-generation XML support in relational database systems was mainly concerned with exporting relational data as XML (“publish XML”), and to import relational data in XML markup form back into a relational representation (“shred XML”)
The main usage scenario supported by these systems is information exchange in contexts where XML is used as the “wire format” and where the relational and XML schemas are often predefined independently of each other.
In order to cover this scenario,Microsoft SQL Server provides extensive functionality such as the for xml publishing rowset aggregator, the OpenXML rowset provider, and the XML view technology based on annotated schemas.
Shredding of XML data into a relational schema can be quite difficult or inefficient for storing semistructured data whose structure may vary over time, and for storing documents.
To support such applications SQL Server implements native XML based on the SQL:2003 xml data type.
It consists of the ability to store XML natively, to constrain and type the stored XML data with collections of XML schemas, and to query and update the XML data.
In order to provide efficient query executions, several types of XML-specific indices are provided.
Finally, the native XML support also integrateswith the “shredding” and “publishing” to and from relational data.
The data type can be used for parameters of stored procedures, for variables, and as a column type.
The internal binary format provides efficient retrieval and reconstruction of the original XML document, in addition to some space savings (on average, 20 percent)
The indices support an efficient query mechanism that can utilize the relational query engine and optimizer; more details are provided later, in Section 30.11.3
These query and modification capabilities are supported by usingmethods defined on the xmldata type.
Someof thesemethods are described in the rest of this section.
Each method takes a string literal as the query string and potentially other arguments.
The XML data type (on which the method is applied) provides the context item for the path expressions and populates the in-scope schema definitions with all the type information provided by the associated XML schema collection (if no collection is provided, the XML data is assumed to be untyped)
The SQL Server XQuery implementation is statically typed, thereby supporting early detection of path expression typing mistakes, type errors, and cardinality mismatch, as well as some additional optimizations.
The query method takes an XQuery expression and returns an untyped XML data type instance (that can then be cast to a target schema collection if the data need to be typed)
In XQuery specification terminology, we have set the construction mode to “strip.” The following example shows a simple XQuery expression that summarizes a complex Customer element in a trip report document that contains among other information a name, anIDattribute, and sales-lead information that are contained in themarked-up actual trip report notes.
The summary shows the name and sales leads for Customer elements that have sales leads.
The above XQuery query gets executed on the XML value stored in the doc attribute of each rowof the tableTripReports.
Each row in the result of the SQL query contains the result of executing the XQuery query on the data in one input row.
The valuemethod takes an XQuery expression and an SQL type name, extracts a single atomic value from the result of the XQuery expression, and casts its lexical form into the specified SQL type.
If the XQuery expression results in a node, the typed value of the node will implicitly be extracted as the atomic value to be cast into the SQL type (in XQuery terminology the node will be “atomized”; the result is cast to SQL)
Note that the value method performs a static type check that at most one value is being returned.
Finally, the modify method provides a mechanism to change an XML value at the subtree level, inserting new subtrees at specific locations inside a tree, changing thevalue of an element or attribute, anddeleting subtrees.
The following example deletes all customer saleslead elements of years previous to the year given by an SQL variable or parameter with the name @year:
Asmentioned earlier, the XML data are stored in an internal binary representation.
However, in order to execute the XQuery expressions, the XML data type is internally transformed into a so-called node table.
The internal node table basically uses a row to represent a node.
Each node receives an OrdPath identifier as its nodeID (an OrdPath identifier is a modified Dewey decimal numbering scheme; see the bibliographical notes for references to more information on OrdPath)
Each node also contains key information to point back to the original SQL row to which the node belongs, information about the name and type (in a tokenized form), values, andmore.
Since the OrdPath encodes both the document order and the hierarchy information, the node table then is clustered on the basis of the key.
All XQuery and update expressions are then translated into an algebraic operator tree against this internal node table; the tree uses the common relational operators and some operators specifically designed for the XQuery algebraization.
The resulting tree is then grafted into the algebra tree of the relational expression so that in the end, the query-execution engine receives a single execution tree that it can optimize and execute.
In order to avoid costly runtime transformations, a user can prematerialize the node table by using the primary XML index.
The path index provides support for simple types of path expressions.
The properties index provides support for the common scenario of propertyvalue comparisons.
The value index is well suited if the query uses wild-cards in comparisons.
See the bibliographical notes for references to more information on XML indexing and query processing in SQL Server.
One common approach to asynchronous processing is to use work tables.
Instead of performing all of the work for a business process in a single database transaction, an application makes a change indicating that outstandingwork is present and then inserts a record of thework to be performed into a work table.
As resources permit, the application processes the work table and completes the business process.
Service Broker is a part of the database server that directly supports this approach for application development.
These allow programmatic access to Service Broker objects frommanaged code.
Previous message-queuing technologies concentrated on individual messages.With Service Broker, the basic unit of communication is the conversationa persistent, reliable, full-duplex stream of messages.
Messages from conversations with higher priority are sent and received faster than messages from conversations with a lower priority.
Related conversations can be associated with the same conversation group.
Messages are strongly typed, i.e., each message has a specific type.
A contract defines the message types that are allowable for a conversation, and which participant in the conversation may send messages of that type.
These tables are not directly accessible; instead, SQL Server exposes queues as views of those internal tables.
A receive operation returns one or moremessages from the same conversation group.
By controlling access to the underlying table, SQL Server can efficiently enforce message ordering, correlation of relatedmessages, and locking.
Because queues are internal tables, queues require no special treatment for backup, restore, failover, or database mirroring.
Both application tables and the associated, queued messages are backed up, restored, and failed-over with the database.
Broker conversations that exist in mirrored databases continue where they left off when the mirrored failover is complete even if the conversationwas between two services that live in separate databases.
The locking granularity for Service Broker operations is the conversation group rather than a specific conversation or individual messages.
By enforcing locking on the conversation group, Service Broker automatically helps applications avoid concurrency issues while processing messages.
When a queue contains multiple conversations, SQL Server guarantees that only one queue reader at a time can process messages that belong to a given conversation group.
This eliminates the need for the application itself to include deadlock-avoidance logic — a common source of errors in many messaging applications.
Another nice side effect of this locking semantic is that applications may choose to use the conversation group as a key for storing and retrieving application state.
These programming-model benefits are just two examples of the advantages that derive from the decision to formalize the conversation as the communication primitive versus the atomic message primitive found in traditional message-queuing systems.
To scale the number of running stored procedures to the incoming traffic, the activation logic monitors the queue to see if there is useful work for another queue reader.
The stored procedure to be activated, the security context of the stored procedure, and themaximumnumber of instances to be started are configured for an individual queue.
This feature allows an application outside of SQL Server to be activated when newmessages are inserted into a queue.
By doing this, CPU-intensivework can be offloaded out of SQL Server to an application, possibly in a different computer.
Also, long-duration tasks, e.g., invoking a Web service, can be executed.
The External Activator follows the same logic as internal activation, and can be configured to activate multiple instances of an application when messages accumulate in a queue.
Conversations can occur within a single instance of SQL Server or between two instances of SQL Server.
Security and routing are configured declaratively, without requiring changes to the queue readers.
A conversation that spans instances of SQL Server can be secured both at the networking level (point to point) and at the conversation level (end to end)
When end-to-end security is used, the contents of the message remain encrypted until the message reaches the final destination, while theheaders are available to each SQLServer instance that themessage travels through.
The protocol fragments large messages and permits interleaved fragments from multiple messages.
Fragmentation allows SQL Server to quickly transmit smaller messages even in cases where a large message is in the process of being transmitted.
The binary protocol does not use distributed transactions or two-phase commit.
Instead, the protocol requires that a recipient acknowledge message fragments.
Acknowledgments are most often included as part of the headers of a return message, although dedicated return messages are used if no return message is available.
The tool can run in either configuration or runtime mode.
In configuration mode, the tool checks whether a pair of services can exchange messages and returns any configuration errors.
Examples of these errors are disabled queues and missing return routes.
In the second mode, the tool connects to two or more SQL Server instances and monitors SQL Profiler events to discover Service Broker problems at runtime.
The tool output can be sent into a file for automated processing.
Integration Services, Analysis Services, and Reporting Services are each implemented in separate servers and can be installed independently from one another on the same or different machines.
They can connect to a variety of data sources, such as flat files, spreadsheets, or a variety of relational database systems, through native connectors, OLE-DB, or ODBC drivers.
Together they provide an end-to-end solution for extracting, transforming, and loading data, then modeling and adding analytical capability to the data, and finally building and distributing reports on the data.
The different Business Intelligence components of SQL Server can integrate and leverage each others’ capability.
Here are a few common scenarios that will leverage a combination of components:
Build an SSIS package that cleanses data, using patterns generated by SSAS data mining.
Use SSIS to load data to an SSAS cube, process it, and execute reports against the SSAS cube.
Build an SSRS report to publish the findings of a mining model or the data contained in an SSAS OLAP component.
The following sections give an overview of the capabilities and architecture of each of these server components.
Microsoft SQL Server Integration Services (SSIS) is an enterprise data transformation and data integration solution that you can use to extract, transform, aggregate, and consolidate data from disparate sources and move it to single or multiple destinations.
These provide the ability to build large, robust, and complex data transformation solutions without any custom programming.
However, anAPI and programmable objects are availablewhen they are needed to create custom elements or integrate data transformation capabilities into custom applications.
The SSIS data-flow engine provides the in-memory buffers that move data from source to destination and calls the source adapters that extract data from files and relational databases.
The engine also provides the transformations that modify data and the destination adapters that loaddata into data stores.
Duplicate elimination based on fuzzy (approximate) match is an example of a transformation provided by SSIS.
Figure 30.7 shows an example of how various transformations can be combined to cleanse and load book sales information; the book titles from the sales data.
Information about confidence and data lineage is stored with the cleansed data.
The Analysis Services component delivers online analytical processing (OLAP) and data-mining functionality for business intelligence applications.
The calculation engine is on the server, so queries are resolved on the server, avoiding the need to transfer large amounts of data between the client and the server.
Analysis Services utilizes a Unified Dimensional Model (UDM), which bridges the gap between traditional relational reporting and OLAP ad hoc analysis.
The role of a Unified Dimensional Model (UDM) is to provide a bridge between the user and the data sources.
A UDM is constructed over one or more physical data sources, and then the end user issues queries against the UDM, using one of a variety of client tools, such as Microsoft Excel.
More than simply a dimensionmodeling layer of theDataSource schemas, the UDM provides a rich environment for defining powerful yet exhaustive business logic, rules, and semantic definition.
Users can browse and generate reports on the UDM data in their native language (for example, French or Hindi) by defining local language translation of the metadata catalog as well as the dimensional data.
The UDM allows users to define business-oriented perspectives, each one presenting only a specific subset of the model (measures, dimensions, attributes, business rules, and so forth) that is relevant to a particular group of users.
Businesses often define key performance indicators (KPIs) that are important metrics used to measure the health of the business.
Examples of such KPIs include sales, revenue per employee, and customer retention rate.
The UDM allows such KPIs to be defined, enabling a much more understandable grouping and presentation of data.
Clustering techniques such as expectation maximization and K-means (coupled with techniques for sequence clustering)
In addition, SQL Server provides an extensible architecture for plugging in third-party data mining algorithms and visualizers.
With DMX, models can be created and trained and then stored in an Analysis Services database.
The model can then be browsed to look at patterns or, by using a special prediction join syntax, applied against new data to perform predictions.
The DMX language supports functions and constructs to easily determine a predicted class along with its confidence, predict a list of associated items as in a recommendation engine, or even return information and supporting facts about a prediction.
Data mining in SQL Server can be used against data stored in relational or multidimensional data sources.
Other data sources are supported as well through specialized tasks and transforms, allowing data mining directly in the operational data pipeline of Integration Services.
Data-mining results can be exposed in graphical controls, special data-mining dimensions for OLAP cubes, or simply in Reporting Services reports.
Reporting Services is a server-based reporting platform that can be used to create and manage tabular, matrix, graphical, and free-form reports that contain data from relational and multidimensional data sources.
The reports that you create can be viewed and managed over a Web-based connection.
Matrix reports can summarize data for high-level reviews, while providing supporting detail in drilldown reports.
Parameterized reports can be used to filter data on the basis of values that are provided at runtime.
Users can choose from a variety of viewing formats to render reports on the fly in preferred formats for data manipulation or printing.
An API is also available to extend or integrate report capabilities into custom solutions.
Server-based reporting provides a way to centralize report storage and management, set policies and secure access to reports and folders, control how reports are processed and distributed, and standardize how reports are used in your business.
Additional information on the self-tuning aspects of SQL Server are discussed by Chaudhuri et al.
Under this scheme, the optimizer detects the pattern and considers per-segment execution.
It argues that this sometimes is faster than scanning a base table.
Rys [2004] provides an overview of the extensions to the for xml aggregation.
For information on XML capabilities that can be used on the client side or inside CLR, refer to the collection ofwhite papers at http://msdn.microsoft.com/XML/BuildingXML/XMLandDatabase/default.aspx.
Rys [2003] provides an overview of implementation techniques for XQuery in the context of relational databases.
The OrdPath numbering scheme is described in O’Neil et al.
The DDL and sample data are also available on the Web site of the book, db-book.com, for use in laboratory exercises.
The remaining appendices are not part of the printed book, but are available online on the Web site of the book, db-book.com.
The project-join normal form, which is based on a type of constraint called join dependency is presentednext; join dependencies are a generalization of multivalued dependencies.
The chapter concludes with another normal form called the domain-key normal form.
Appendix C (Other Relational Query Languages) first presents the relational query language Query-by-Example (QBE), which was designed to be used by non-programmers.
In QBE, queries look like a collection of tables containing an example of data to be retrieved.
The graphical query language ofMicrosoft Access, which is based on QBE, is presented next, followed by the Datalog language,which has a syntaxmodeled after the logic-programming language Prolog.
Appendix D (Network Model), and Appendix E (Hierarchical Model), cover the network and hierarchical data models.
Both these data models predate the relational model, and provide a level of abstraction that is lower than the relational model.
They abstract away some, but not all, details of the actual data structures used to store data on disks.
These models are only used in a few legacy applications.
For appendices B through E, we illsutrate our concepts using a bank enterprise with the schema shown in Figure 2.15
In this appendix, we present the full details of our running-example university database.
In Section A.1 we present the full schema as used in the text and the E-R diagram that corresponds to that schema.
In Section A.2 we present a relatively complete SQL data definition for our running university example.
Besides listing a datatype for each attribute, we include a substantial number of constraints.
Finally, in Section A.3 we present sample data that correspond to our schema.
The full schema of the University database as used in the text is shown in Figure A.1
The E-R diagram that corresponds to that schema, and used throughout the text, is shown in Figure A.2
In this section, we present a relatively complete SQL data definition for our example.
Besides listing a datatype for each attribute, we include a substantial number of constraints.
In the above DDL we add the on delete cascade specification to a foreign key constraint if the existence of the tuple depends on the referenced tuple.
In other foreign key constraints we either specify on delete set null, which allows deletion of a referenced tuple by setting the referencing value to null, or do not add any specification, which prevents the deletion of any referenced tuple.
For example, if a department is deleted, we would not wish to delete associated instructors; the foreign key constraint from instructor to department instead sets the dept name attribute to null.
On the other hand, the foreign key constraint for the prereq relation, shown later, prevents the deletion of a course that is required as a prerequisite for another course.
For the advisor relation, shown later, we allow i ID to be set to null if an instructor is deleted, but delete an advisor tuple if the referenced student is deleted.
The following create table statement for the table time slot can be run onmost database systems, but does not work on Oracle (at least as of Oracle version 11), since Oracle does not support the SQL standard type time.
Since Oracle does not support the time type, for Oracle we use the following schema instead:
The difference is that start time has been replaced by two attributes start hr and start min, and similarly end time has been replaced by attributes end hr and end min.
These attributes also have constraints that ensure that only numbers representing valid time values appear in those attributes.
Note that although Oracle supports the datetime datatype, datetime includes a specific day, month, and year as well as a time, and is not appropriate here since we want only a time.
There are two alternatives to splitting the time attributes into an hour and a minute component, but neither is desirable.
The first alternative is to use a varchar type, but that makes it hard to enforce validity constraints on the string as well as to perform comparison on time.
The second alternative is to encode time as an integer representing a number of minutes (or seconds) frommidnight, but this alternative requires extra code with each query to covert values between the standard time representation and the integer encoding.
In this section we provide sample data for each of the relations defined in the previous section.
Figure A.14 The time slot relation with start and end time separated into hour and minute.
Narasayya, “Random sampling for histogram construction: how much is enough?”, In Proc.
Bernstein, “Compiling mappings to bridge applications and databases”, In Proc.
