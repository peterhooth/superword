Over 150 recipes to design and optimize large-scale Apache Cassandra deployments.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Edward Capriolo is a member of the Apache Software Foundation and a committer for the Hadoop-Hive project.
He has experience as a developer as well Linux and network administrator and enjoys the rich world of Open Source software.
Edward Capriolo is currently working as a System Administrator at Media6degrees where he designs and maintains distributed data storage systems for the Internet advertising industry.
To my educators and employers who gave me the tools.
To those who have contributed to Cassandra, including (but not limited to) Jonathon Ellis, Brandon Williams, Jake Luciani, and others who have developed an amazing project to learn and write about.
Also deserving of recognition is the amazing community, including (but not limited to) Robert Coli (my Cassandra sensei) and Jeremy Hanna.
Special thanks to Matt Landolf, Mubarak Seyed, Tyler Hobbs, and Eric Tamme who contributed the knowledge and free time to add to the content of the book.
They did an excellent job by beautifully redrafting my crude drawings, fixing my grammatical errors, and reviewing and suggesting content.
The book got better with every new person who became involved.
Vineet Daniel is a Systems Architect and has worked at various startups and managed high traffic web applications.
He has over eight years of experience in Software development, managing servers/cloud, and team.
The experience has made him a learned individual in technologies like optimization, high-availability, and scalability.
He loves to use Linux commands and has a never ending appetite for penetration testing.
This is for my Parents, brother, Annie, and two wonderful kids in my life Ana and Aman for encouraging me to go ahead with the task.
I would like to thank the team at Packt publishing especially Michelle for guiding me and providing me with this wonderful opportunity to work with Packt Publishing.
He has been a software engineer in Intelligence Corps, Israel Defense Force (IDF), 2005-2008, working on a variety of military IT systems, and later a software engineer and a team leader in web-based startup named AnyClip, making a dream of "Find any moment from any film, instantly" to come true.
His experience covers aspects of the architecture, design, and development of high performance distributed web and data analysis systems.
His background includes a wide range of programming languages (including Java and C#), search engines (including Lucene), databases, and NoSQL distributed data stores.
Matthew's research interests include search engines, distributed computing, image processing, computer vision, and machine learning.
I would like to thank my beloved girlfriend, Luba, for her thoughtful feedback and support, during the review process of this book.
Jing Song has been working in the IT industry as an engineer for more than 12 years after she graduated school.
She enjoys solving problems and learning about new technologies in computer science space.
Her interests and experiences lie across multiple tiers, from web frontend GUI to middleware, from middleware to backend SQL RDBMS and NoSQL data storage.
In the last five years, she has mainly focused on the enterprise application performance and cloud computing areas.
Jing currently works for Apple as a tech lead with the Enterprise Technology Service group, leading various Java applications from design, to implementation, to performance tuning.
She was one of the contributors to the internal private cloud application last year.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Why Subscribe? f Fully searchable across every book published by Packt.
Preface Apache Cassandra is a fault-tolerant, distributed data store which offers linear scalability allowing it to be a storage platform for large high volume websites.
This book provides detailed recipes that describe how to use the features of Cassandra and improve its performance.
Recipes cover topics ranging from setting up Cassandra for the first time to complex multiple data center installations.
The recipe format presents the information in a concise actionable form.
The book describes in detail how features of Cassandra can be tuned and what the possible effects of tuning can be.
Recipes include how to access data stored in Cassandra and use third party tools to help you out.
The book also describes how to monitor and do capacity planning to ensure it is performing at a high level.
Towards the end, it takes you through the use of libraries and third-party applications with Cassandra and Cassandra integration with Hadoop.
What this book covers Chapter 1, Getting Started: The recipes in this chapter provide a whirlwind tour of Cassandra.
Setup recipes demonstrate how to download and install Cassandra as a single instance or simulating multiple instance clusters.
Trouble-shooting recipes show how to run Cassandra with more debugging information and how to use management tools.
Also included are recipes for end users which connect with the command like interface and setup an environment to build code to access Cassandra.
Chapter 2, Command-line Interface: This chapter provides recipes on using Cassandra's command line interface.
Recipes cover how the CLI is used to make changes to the metadata such as key spaces, column families, and cache settings.
Additionally recipes show how to use the CLI to set, get and scan data.
Chapter 3, Application Programmer Interface: Cassandra provides an application programmer interface for programs to insert and access data.
The chapter has recipes for doing common operations like inserting fetching, deleting, and range scanning data.
Also covered in this chapter are recipes for batch mutate and multi-get which are useful in batch programs.
Chapter 4, Performance Tuning: Many configuration knobs and tunable settings exist for Cassandra.
Additionally hardware choices and operating system level tuning effect performance.
The recipes in this chapter show configuration options and how changing them optimizes performance.
Chapter 5, Consistency, Availability, and Partition Tolerance with Cassandra: Cassandra is designed from the ground up to store and replicate data across multiple nodes.
This chapter has recipes that utilize tunable consistency levels and configure features like read repair.
These recipes demonstrate how to use features of Cassandra that make available even in the case of failures or network partitions.
Chapter 6, Schema Design: The Cassandra data model is designed for storing large amounts of data across many nodes.
This chapter has recipes showing how common storage challenges can be satisfied using Cassandra.
Recipes include techniques for serializing data, storing large objects, time series, normalized, and de-normalized data.
Chapter 7, Administration and Cluster Management: Cassandra allows nodes to be added and remove from the cluster without downtime.
This chapter contains recipes for adding, moving, and removing nodes as well as administrative techniques for backing up and restoring data.
Also covered administrative techniques such as backing up or restoring data.
Chapter 8, Multiple Datacenter Deployments: Cassandra is designed to work both when nodes are deployed in a local area network and when nodes are separated by larger geographical distances such as a wide area network.
The recipes in this chapter show how to configure and use features that control and optimize how Cassandra works in multiple datacenter environments.
Chapter 9, Coding and Internals: This chapter covers programming recipes that go beyond the typical application programmer interface, including building Cassandra from source, creating custom types for use with Cassandra, and modifying tools like the JSON export tools.
Chapter 10, Third-party Libraries and Applications: A variety of libraries and applications exist for Cassandra.
This chapter introduces tools that make coding easier such as the high-level client Hector, ot the object mapping tool Kundera.
Recipes also show how to setup and use applications built on top of Cassandra such as the full text search engine solandra.
Chapter 11, Hadoop and Cassanda: Hadoop is a distributed file system, HDFS that provides high throughput and redundant storage and MapReduce, a software framework for distributed processing of large data sets on compute clusters.
This chapter provides recipes with tips on setting up Hadoop and Cassandra both individually and on shared hardware.
Recipes show how to use Cassandra as the input or output of map reduce jobs, as well as common tasks like counting or joining data that can be done with Cassandra data inside Hadoop.
Chapter 12, Collecting and Analyzing Statistics: This chapter covers techniques for collecting performance data from the Cassandra and the operating system.
Recipes collect and display performance data and how to interpret that data and use the information tune Cassandra servers.
Chapter 13, Monitoring: The monitoring chapter has recipes which show how to install and use tools to help understand the performance of Cassandra.
Recipes include how to forward log events to a central server for aggregation.
Othere recipes show how to monitor logs for dangerous conditions.
What you need for this book To run the examples in this book the following software will be required:
Additionally the following tools are helpful, but are not strictly required:
Who this book is for This book is designed for administrators, developers, and data architects who are interested in Apache Cassandra for redundant, highly performing, and scalable data storage.
Typically these users should have experience working with a database technology, multiple node computer clusters, and high availability solutions.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "Click on the attributes and the Scores information will appear in the right panel."
Warnings or important notes appear in a box like this.
Let us know what you think about this bookwhat you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code for this book You can download the example code files for all Packt books you have purchased from your account at http://www.PacktPub.com.
If you purchased this book elsewhere, you can visit http://www.PacktPub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Introduction The Apache Cassandra Project develops a highly scalable second-generation distributed database, bringing together a fully distributed design and a ColumnFamily-based data model.
The chapter contains recipes that allow users to hit the ground running with Cassandra.
It also contains recipes for connecting to Cassandra and executing commands both from the application programmer interface and the command-line interface.
Also described are the Java profiling tools such as JConsole.
The recipes in this chapter should help the user understand the basics of running and working with Cassandra.
A simple single node Cassandra installation Cassandra is a highly scalable distributed database.
While it is designed to run on multiple production class servers, it can be installed on desktop computers for functional testing and experimentation.
This recipe shows how to set up a single instance of Cassandra.
Getting ready Visit http://cassandra.apache.org in your web browser and find a link to the latest binary release.
Choose a base directory that the user will run as he has read and write access to:
Cassandra defaults to wanting to save data in /var/lib/cassandra and logs in /var/log/cassandra.
These locations will likely not exist and will require root-level privileges to create.
To avoid permission issues, carry out the installation in user-writable directories.
Use the echo command to display the path to your home directory.
Cassandra comes as a compiled Java application in a tar file.
By default, it is configured to store data inside /var.
By changing options in the cassandra.yaml configuration file, Cassandra uses specific directories created.
After startup, Cassandra detaches from the console and runs as a daemon.
The nodetool program communicates with the JMX port to confirm that the server is alive.
Due to the distributed design, many of the features require multiple instances of Cassandra running to utilize.
For example, you cannot experiment with Replication Factor, the setting that controls how many nodes data is stored on, larger than one.
Replication Factor dictates what Consistency Level settings can be used for.
The next recipe, Reading and writing test data using the command-line interface.
The command-line interface (CLI) presents users with an interactive tool to communicate with the Cassandra server and execute the same operations that can be done from client server code.
This recipe takes you through all the steps required to insert and read data.
New clusters do not have any preexisting keyspaces or column families.
These need to be created so data can be stored in them: [default@unknown] create keyspace testkeyspace.
The CLI is a helpful interactive facade on top of the Cassandra API.
After connecting, users can carry out administrative or troubleshooting tasks.
Chapter 2, Command-line Interface is dedicated to CLI recipes defined in the preceding statements in greater detail.
While it can be run on a single node, simulating a production cluster of multiple nodes is best done by running multiple instances of Cassandra.
This recipe is similar to A simple single node Cassandra installation earlier in this chapter.
However in order to run multiple instances on a single machine, we create different sets of directories and modified configuration files for each node.
Use the echo command to display the path to your home directory.
Cassandra uses JMX (Java Management Extensions), which allows you to configure an explicit port but always binds to all interfaces on the system.
As a result, each instance will require its own management port.
At this point your cluster is comprised of single node.
The Thrift port has to be the same for all instances in a cluster.
Thus, it is impossible to run multiple nodes in the same cluster on one IP address.
These addresses do not usually need to be configured explicitly.
Following this recipe you can run as many instances on your computer as you wish, or even multiple distinct clusters.
You are only limited by resources such as memory, CPU time, and hard disk space.
The next recipe, Scripting a multiple instance installation does this process with a single script.
Scripting a multiple instance installation Cassandra is an active open source project.
Setting up a multiple-node test environment is not complex, but has several steps and smaller errors happen.
Each time you wish to try a new release, the installation process will have to be repeated.
This recipe achieves the same result of the Running multiple instances on a single machine recipe, but only involves running a single script.
Copy the tar to the base directory and then use pushd to change to that directory.
This script uses borne shell scripting to handle tasks such as creating directories and extracting tars, and uses the sed utility to locate sections of the file that need to be modified to correspond to the directories created.
Setting up a build and test environment for tasks in this book.
Cassandra does not have a standardized data access language such as SQL or XPATH.
Cassandra has support for Thrift, which generates bindings for a variety of languages.
Since Cassandra is written in Java, these bindings are well established, part of the Cassandra distribution, and stable.
Thus, it makes sense to have a build environment capable of compiling and running Java applications to access Cassandra.
This recipe shows you how to set up this environment.
Other recipes in the book that involve coding will assume you have this environment setup.
Create a top-level folder and several sub folders for this project:
From the JUnit installation, copy the junit.jar into your library path.
By convention, properties that represent critical paths to the build are typically specified at the top of the file:
This will compile source code and build a JAR file: $ ant dist.
The jar command will build empty JAR files with no indication that you had specified the wrong path.
Use the run target to run the A class: $ ant -DclassToRun=A run.
A target can be a task such as compiling code, testing code, or producing a final JAR.
As a result, you do not have to run a list of targets sequentially; the dist target will run its dependents such as compile and init and their dependencies in proper order.
If you want to work with an IDE, the NetBeans IDE has a type of project called Free-Form project.
You can use the preceding build.xml with the Free-Form project type.
When working with new software or troubleshooting an issue, every piece of information can be valuable.
Cassandra has the capability to both run in the foreground and to run with specific debugging levels.
This recipe will show you how to run in the foreground with the highest possible debugging level.
Start the instance in the foreground using –f: $ bin/cassandra -f.
Without the -f option, Cassandra disassociates itself from the starting console and runs like a system daemon.
With the -f option, Cassandra runs as a standard Java application.
Setting a global DEBUG level is only appropriate for testing and troubleshooting because of the overhead incurred by writing many events to a single file.
Cassandra uses a Consistent Hashing to divide data across the ring.
Each node has an Initial Token which represents the node's logical position in the ring.
Using the row key of data, the partitioner calculates a token.
The node whose Initial Token is closest without being larger than the data's token is where the data is stored along with the other replicas.
For a five node cluster, the initial token for the 3rd node would be:
For larger clusters of 20 or more nodes, determining the ideal Initial Token for each node in a cluster is a time consuming process.
The following Java program calculates the Initial Tokens for each node in the cluster.
Getting ready You can easily build and run this example following Setting up a build and test environment earlier in this chapter.
Set the environment variable tokens to the number of nodes in the cluster.
How it works Generating numbers equidistant from each other helps keep the amount of data on each node in the cluster balanced.
This technique for calculating Initial Tokens is ideal for the Random Partitioner, which is the default partitioner.
When using the Order Preserving Partitioner, imbalances in key distribution may require adjustments to the Initial Tokens to balance out the load.
If a Cassandra node has already joined the cluster, see in Chapter 7, Administration, the recipe Nodetool Move: Move a node to a specific ring location to see how to move a node to an initial token.
Choosing Initial Tokens for use with Partitioners that preserve ordering.
If the distribution of keys is uneven, some nodes will have more data than others.
For names from a phonebook, some letters may be more common than others.
Names such as Smith are very common while names such as Capriolo are very rare.
For a cluster of eight nodes, choose initial tokens that will divide the list roughly evenly.
Information on calculating distributions using spreadsheets can be found online: http://www.wisc-online.com/objects/ViewObject.
Partitioners that preserve order can range scan across keys and return data in a natural order.
The trade off is that users and administrators have to plan for and track the distribution of data.
If a Cassandra node has already joined the cluster, see the recipe in Chapter 7, Administration, the recipe Nodetool Move: Move a node to a specific ring location to see how to move a node to an initial token.
In addition to JVM internals, applications can maintain their own counters and provide operations that the user can trigger remotely.
Cassandra has numerous counters and the ability to trigger operations such as clearing the Key Cache or disabling compaction over JMX.
This recipe shows how to connect to Cassandra instances using JConsole.
It requires a windowing system such as X11 to run on the system you start JConsole from, not on the server it will connect to.
In the Remote Process box, enter the host and port of your instance:
Click on the Memory tab to view information about the virtual memory being used by the JVM:
JConsole can connect to local processes running as your user without host and port information by selecting the process in the Local Process list.
Connecting to processes on other machines requires you to enter host and port information in the Remote Process.
The recipe Connecting with JConsole over a SOCKS Proxy shows how to use JConsole with a host only reachable by SSH.
Often, you would like to run JConsole on your desktop and connect to a server on a remote network.
Applications that use RMI typically have trouble running on more secure networks.
This recipe shows how to create a dynamic proxy over SSH and how to have JConsole use the proxy instead of direct connections.
Getting ready On your management system you will need an SSH client from OpenSSH.
Windows users can try Cygwin to get an OpenSSH client.
A dynamic SOCKS proxy is opened up on the target server and tunneled to a local port on your workstation.
JConsole is started up and configured to use this proxy.
When JConsole attempts to open connections, they will happen through the proxy.
Destination hosts will see the source of the traffic as your proxy system and not as your local desktop.
Connecting to Cassandra with Java and Thrift Cassandra clients communicate with servers through API classes generated by Thrift.
The API allows clients to perform data manipulation operations as well as gain information about the cluster.
This recipe shows how to connect from client to server and call methods that return cluster information.
Getting ready This recipe is designed to work with the build environment from the recipe Setting up a build and test environment.
You also need to have a system running Cassandra, as in the Simulating multiple node clusters recipe.
RandomPartitioner [java] cluster name Test Cluster [java] keyspace Keyspace1 [java] keyspace system.
Cassandra clusters are symmetric in that you can connect to any node in the cluster and perform operations.
After choosing the correct transports and other connection settings, users can instantiate a Cassandra.Client instance.
In Chapter 5, Consistency, Availability, and Partition Tolerance with Cassandra, the recipe Working with the formula for strong consistency shows how to create a simple wrapper that reduces the repeated code when connecting to Cassandra.
Connecting to Cassandra with the CLI Users can connect to any node in the cluster and issue requests.
This recipe shows how to connect to a node in the cluster.
Use the connect statement and specify a host and port to connect to.
The CLI presents an interactive interface to execute operations with Cassandra.
The underlying communication between the CLI and Cassandra uses the same Thrift interface that client applications use.
Creating a keyspace from the CLI A keyspace is a top-level organizational unit that can hold one or more column families.
An important setting for creating a keyspace is replication factor, which controls how many replicas of data will be in the cluster.
Replication factor can not exceed the number of nodes in the cluster.
However, all the column families inside it inherit configuration from it.
This is important because two column families inside the same keyspace must use the same replication factor and replication strategy.
It is suggested to create a keyspace for each column family since there are no benefits in sharing one.
These options are described in the multiple data center chapter.
Creating a column family with the CLI A column family is a container for columns.
To insert and read data, you first need to create a column family.
Getting ready Columns families need to be created inside a keyspace.
See the previous recipe, Creating a keyspace from the CLI.
Column families have several configurable parameters that are specified in a with clause and separated by and:
There are many parameters for a column family that significantly change how it operates.
Describing a keyspace The describe keyspace command shows all the properties of a keyspace, including the information of each column family inside it.
Use the describe keyspace command and specify the name of a keyspace:
The information about keyspaces and column families is meta-information that is stored and replicated across all nodes in the cluster.
Writing data with the CLI The command-line interface has a set command that is used for inserting data.
Getting ready To insert data, a target keyspace and column family must already exist.
An insert also requires a row key, a column name, and a column value information.
Set command will create the column if it does not already exist.
If the column does exist, the old value will be overwritten with the new value.
Cassandra automatically writes the key to the proper node regardless of which node the CLI connects to.
The CLI uses microseconds since epoch time as the value of timestamp when setting columns.
Reading data with the CLI The get operation allows users to retrieve data through the CLI.
Getting ready To retrieve data using the get operation, a row key is required and a column name is optional.
Cassandra uses the row key to locate and fetch the requested data from the proper node.
The client does not need to specifically connect to the node with the data.
In this chapter, the recipe Using the assume keyword to decode column names or column values demonstrates how to force the CLI to display column names and values in more human-readable formats.
In this chapter, the recipe Using column metadata and comparators for schema enforcement stores meta-information inside the schema so it displays properly.
Deleting rows and columns from the CLI Known rows and columns can be deleted by specifying the row key and/or the column name with the del (delete) command.
Supplying only a row key with no column will result in all the columns for that row being deleted.
Issue a delete for all the columns of the row key server01:
Deletes in Cassandra are implemented as special writes known as tombstones.
The data is not removed from disk until a time in the future, but reads done after the delete will not find this data.
Getting ready Insert some sample data into a column family.
Use the list keyword to show the first entries in the column family:
Run list using a start key of 'a' and an end key of 'h':
List uses range scanning to display data in a column family.
If using RandomParitioner, the data will not be in a lexicographical order, but it will be in a constant order.
The list moves through the data and provides commands to limit results, start at a specific key, and stop scanning at a specific key.
Dropping a keyspace or a column family The drop statement can be used to drop a column family or a keyspace.
Dropping a column family or a keyspace will remove all data in it for all nodes in the cluster.
The drop command removes a keyspace or a column family across the cluster.
After executing this statement, writes and reads to the entity will not be possible.
The CLI allows inserts to super columns much like inserts of normal columns.
They can be read with get, written with set, and deleted with del.
The super column version of these commands uses an extra ['xxx'] to represent the extra level of the map called the sub-column.
Use assume so CLI formats the columns as ASCII text, and then fetch all the columns of the 'mynewcar' row:
Super columns bring an extra level of nesting to the data model.
When working with super columns from the CLI, an extra '[]' specifies the sub-index level.
Internally, super columns must be completely serialized and de-serialized to be accessed.
This makes them inefficient for super columns with a large number of columns.
While super columns look like an attractive option, it is almost always better to append the column and the super column together with a deliminator between them.
The extra serialization involved in using super columns and extra space used makes them less efficient.
Using the assume keyword to decode column names or column values.
The assume keyword does not modify data or column family metadata.
Instead, it decodes and helps display results of get and list requests inside the command-line interface.
By default, columns with no metadata are displayed in a hex format.
This is done because row keys, column names, and column values are byte arrays.
These could have non-printable characters inside them such as a tab or newline that would affect the CLI output.
Cassandra has built-in types that can be used with assume.
Time To Live (TTL) is a setting that makes a column self-delete a specified number of seconds after the insertion time.
Append with ttl clause to a set statement that will expire a row after ten seconds:
Wait ten seconds or longer before reading again and the column will be deleted:
Using built-in CLI functions By default, Cassandra treats data as byte arrays.
However, support is offered for types such as Long, which is a serialized 64 bit integer.
The CLI provides built-in functions that convert the user-supplied data from the CLI into other types such as a conversion from a string to a long.
Other functions create values of timeuuid(), which are normally generated by a program.
Use the help command to determine which functions are available:
Insert a column that uses the timeuuid() method as a column name and uses the long() method to turn the literal string '7' into an encoded long:
Functions are useful for converting strings into the other types from the CLI.
Cassandra is designed to store and retrieve simple byte arrays.
It is normally up to the user to encode and decode their data.
Cassandra does have support for built-in types such as timeuuid, ASCII, long, and a few others.
When creating or updating a column family, the user can supply column metadata that instructs the CLI on how to display data and help the server enforce types during insertion operations.
Create a column family named cars specifying the comparator as LongType:
Cassandra defaults to not enforcing types and accepts arbitrary byte data.
Column metadata as well as comparators allow users to ensure the integrity of data during write operations.
It also serves as meta information for users reading the data so it can be decoded properly.
Changing the consistency level of the CLI The Cassandra data mode stores data across many nodes and data centers.
When operating on data, users choose the consistency level of the operation per requests.
The default consistency level used by the CLI is ONE.
This recipe shows how to use the consistencylevel keyword to change consistency level.
After changing the level, do set, get, and list operations as normal:
Changing the consistency level only affects the current CLI session.
Doing this is helpful when trying to troubleshoot errors that users may be reporting.
Consistency level ONE is forgiving in that write or read operations will succeed with multiple node failures, while other levels such as ALL are less forgiving.
This feature is also useful when working in multiple data center environments with levels such as LOCAL_QUORUM.
In Chapter 8, Multiple Data Center Deployments, the recipe Changing consistency level from the CLI to test various consistency levels with multiple data center deployments.
Getting help from the CLI The CLI has built-in documentation that is accessed using the help statement.
This recipe shows how to get help from the CLI.
Run the command 'help del' for help on the delete command:
The default help statement displays information about all the other statements available.
When following help with the name of another statement such as del or list, the statement issues more details on that specific command.
Loading CLI statements from a file The Cassandra CLI has a batch utility that processes commands from a file.
Create a file with a list of commands for the Cassandra CLI:
Start the Cassandra CLI using the -b argument to specify the batch file:
The batch mode has access to the same commands as the CLI.
However, it is good practice to create a batch file for all meta operations done to the cluster for change management.
The -B,--batch switch enables batch mode, which suppresses output and stops processing on any error.
Introduction Programmatic access to a cluster of Cassandra servers is done though the Application Programmer Interface.
With Thrift, structures, exception, services, and methods are specified in a language-neutral file called an interface file.
Thrift's code generation takes the interface file as input and generates network RPC clients in many languages.
The multiple language code generation allows programs written in C++ or Perl to call the same methods as a Java client.
The Java client is generated and comes packaged with Cassandra.
The Application Programmer Interface provided by Cassandra provides methods for users to create, modify, and remove the meta structures for storing data, keyspaces, and column families, as well as methods for inserting, removing, and fetching data from column families.
The clients' Thrift generates are more generic because they have to work with many languages.
These high-level clients are typically suggested because they insulate users from the details of the generated Thrift code.
This chapter uses the Thrift API as this is the most language-neutral way to present the material.
Connecting to a Cassandra server The first step is connecting to a node in the Cassandra cluster.
The process of opening and closing connections involves a few lines of code that are repeated often.
This recipe demonstrates the connection opening and closing code and abstracts that code into a class for reuse.
This is due to different options and transports available to Thrift.
The final product of the connection steps are an instance of type TProtocol.
The TProtocol instance is used in the constructor of the Cassandra.Client class.
There's more Initializing connections in this manner does not account for server fail-over or retries.
This is one of the reasons higher level clients are preferred.
The top level element in the storage element is a keyspace; the column family is the structure that holds data.
It is common to have an application detect if the proper metadata is created and, if they are not, to create them.
Method that is used retrieve environment or -D options passed from the user */
It produces a KsDef instance with an initialized CfDef instance inside it.
Using MultiGet to limit round trips and overhead MultiGet should be used as an alternative to multiple get operations when each get operation uses the same SlicePredicate.
By using MultiGet, the number of requests and network round trips are reduced versus doing one get operation per row key.
The time savings from MultiGet is mostly attributed to saving network round trip time between the application and the server.
This is significant when reading small rows in a linear manner.
It is important to note that using MultiGet does not change how the data is located in caches or a disk; that time is the same when using both methods.
The ability to bring up a fully functioning instance inside user code is a clear advantage to having to manage the code base and Cassandra service separately.
This approach is ideal when a large number of developers are sharing a project, or with continuous integration tools such as Hudson that build and test code in a completely automated and unattended manner.
Also, ensure that the test.classpath is included in the test target.
Inside the hpcbuild directory, create a directory for test configuration files and copy a stock configuration directory to it:
Many Cassandra internals not re-entrant and can only be used with the static modifier (one per JVM)
The Cassandra threads have their daemon status set to true.
When only daemon threads are running, the JVM will close.
Running Cassandra in an embedded manner makes it fast and easy to develop applications that store their data inside Cassandra such as a custom middle-ware layer.
It is possible to drop individual keys, column families, and keyspaces, as well as truncating column families through the Thrift API.
In some cases, it may be easier to remove all the data directories at the beginning of a unit test.
A tool to handle this is inside a contrib sub project.
This recipe shows how to build the contrib that contains the data cleaner and then uses it inside a test case.
This recipe builds on the previous recipe, Writing unit tests with an embedded Cassandra server, and enhances it.
The contrib projects are not currently built into the binary distributions.
The class needed is part of the javautils contrib project.
By building the contrib using ant and then adding the resulting JAR to the classpath, unit tests have a simple way to ensure they always run with a clean state.
Generating Thrift bindings for other languages (C++, PHP, and others)
The low level client for Cassandra is generated using Thrift.
This recipe shows how to download and install Thrift to generate bindings for multiple languages.
Getting ready A compiler such as GCC and several other development tools are needed to build Thrift.
For example, generating Ruby binding may require ruby and ruby-devel packages to be installed.
Download a matching version of Thrift and compile it: $ wget http://apache.imghat.com/
Thrift uses the cassandra.thrift file to generate bindings for each language specified using -gen arguments.
Clients from a variety of different languages can interact with Cassandra.
Each of them have access to the same Remote Procedure Call (RPC) methods provided by the Thrift interface.
This allows a PHP web application, a Python batch program, as well as Java applications to access the same functionality!
The higher level clients such as Thrift have a more stable API as well as slightly more overhead.
Still, a user who requires more access to the Cassandra internals can use the Storage Proxy API directory.
The Storage Proxy API is not guaranteed to stay consistent even between minor versions.
This recipe shows how to use the Storage Proxy API.
Using StorageProxy and StorageService not intended for the average user, using them is closer to running an Embedded Cassandra server than connecting with a client.
When a node using StorageService joins the cluster, it will not store any data, but it does open up Thrift and storage ports on a particular IP address.
Once connected, this API gives you  access to more Cassandra internals.
A StorageProxy instance cannot be open from the same IP as a node in the cluster.
Network-wise, the StorageProxy has to be able to reach the other nodes as if it were part of the cluster.
The StorageService can be initialized once in the life of the JVM.
The primary operations used in Cassandra are get and insert operations.
In many applications, data can become stale and is no longer needed.
In these type of application, a  process can be used to iterate all the data on the node using range scans.
This recipe shows how to use range scans to iterate all the data in a cluster and remove data older than a user-supplied number of seconds.
One of the parameters needed for get_range_slices is a SlicePredicate.
Set the SlicePredicate so that it has a large size and it includes any column by specifying an empty byte array as the start and finish:
Range scans allow you to move through the data set by using the last token or key from the first range scan as the start token for the next range scan.
Range scanning can take a long time and be processor intensive.
Choosing a larger key range size or slice range size causes each operation to read through more data.
There are several ways to reduce the intensity of the program.
For large clusters, it may be more effective to run several scanning programs on smaller sections of the ring instead of one program.
Also, the program can be written to work during low traffic hours.
Iterating all the columns of a large key In some designs, a particular row key may have a large number of associated columns.
It may be impractical or impossible to retrieve all the columns in a single operation.
This recipe shows how to iterate the columns of a key a few columns at a time using a slice predicate.
Simulate a large list of names under a key "friends" with an array of names.*/
The get_slice method uses a SlicePredicate with SliceRange to select columns of a key.
The columns of a key are sorted, thus it is possible to move through the list several elements at a time in order.
Moving through the list of columns is done by taking the last column seen and using it as the start of the next slice.
The last element of the previous slice is the first element of the next slice.
Slicing columns in reverse The columns of a key are ordered in a Sorted Map structure.
When using the get_slice to select columns of a row key, the natural ordering of the columns can be leveraged.
This recipe shows how to reverse the ordering of the slice results.
Getting ready This recipe requires the code from the previous recipe, Iterating all the columns of a large key.
Reversing the order of result columns is useful in several instances.
One such instance is when it is desired to fetch the largest column.
If the column is a number, it would be the largest number.
If the column was a text string, it would be the value that is alphabetically last.
If the column was a time stamp, it would be the newest data.
Batch mutations have several advantages over doing multiple insert operations.
Larger messages will result in less network overhead transmitting data between the client and Cassandra.
For example, if code is doing a series of operations, multiple try-catch blocks with retry logic at every step will cause clutter.
Instead, with batch mutations a larger list of operations can be built up and submitted.
If the mutations fail, it is safe to submit the entire list again as mutations are idempotent due to the time stamp associated with them.
A second mutation is like the first, except the value is squared:
There are less network round trips and less total data to transfer.
Time stamps are what allow inserts in Cassandra to be idempotent; the update entry with the largest time stamp is the final value of the column.
Thus, if a batch insert fails partially, it is safe to replay the entire mutation without the fear of reverting any data that may have changed outside the mutation.
In Chapter 10, Libraries and Applications, the recipe Doing batch mutations with Hector shows how the Hector high-level library makes mutations easier.
This process of range scanning with get_range_slices through data just to find data to remove is intensive.
It can lower your cache hit rate, and adds more load to the cluster.
An alternative to this is setting the time-to-live (TTL) property for a column.
Once the time-to-live has passed, the column will automatically be removed.
This recipe shows how to use the time-to-live property with a mock messaging application to automatically clear old messages.
This will return all the messages for the key accept those that are past their ttl or deleted.
Use sleep or just wait before running the command a third time.
Once a TTL is set, a column past the expiration time will not show up in the results of any get or get_slice request.
TTLs use less space then storing the column twice and are more efficient then having to range scan through data to find old entries.
Working with secondary indexes The primary ordering and sharding is done by the row key.
This makes searching on the value of a key very fast.
Columns associated with a row key are sorted by their column name.
Secondary indexes allow searching on the values of the columns.
This recipe shows how to create and use secondary indexes.
Getting ready This recipe demonstrates secondary indexes with a limited CRM application.
For a given entry, the names of the customers, the states they live in, and their phone numbers will be stored.
Ask which users live in 'New York': [default@ks33] get customers where state = 'New York';
Secondary indexes require more disk space as they have to maintain another ordering for data.
Cassandra also uses more processing time managing and updating indexes.
Secondary indexes are not atomic; they are built and managed in the background.
Chapter 6, Schema Design, the recipe Developing secondary data orderings or indexes.
In this chapter, we present recipes for Cassandra, Java, and system-level tuning.
There are some practical points that should affect your decision.
This recipe shows the important topics to consider before choosing.
Other JVM implementations and ports exist for other operating systems.
However, the licensing and maturity of these other JVMs vary.
The Java Native Architecture is a component that allows an application to directly interact with system libraries.
Several features of Cassandra use this to avoid using swap, create snapshot files, and optimize performance.
These packages make it easy to install and run Cassandra.
The de facto standard deployment of Cassandra is on 2.6 Linux Kernels.
Other operating systems such as Solaris, FreeBSD, or Windows do work, but are less often deployed in production.
If you employ a non-typical choice, you may encounter rare bugs or edge cases and are difficult for others to reproduce.
Thus, choosing a version of Java is important to performance.
There are several different virtual machines that are compatible with the Java standard.
This recipe shows the important factors you should consider when choosing which version of Java to install.
It should have support for Java Native Architecture and low pause garbage collection.
The Java SE JVM is used for recipes throughout the book.
Cassandra has specific code to gather and log garbage collection statistics from the Oracle Java SE JVM.
Licensing issues prevents distribution of the Oracle JVM in most RPM repositories.
As a result, Oracle offers OpenJDK: http://openjdk.java.net/, which is GPLv2 licensed.
Using a dedicated Commit Log disk Write operations are done sequentially to a commit log on disk and modify a sorted structure in memory called a Memtable.
When thresholds are reached, a Memtable is flushed to disk in a sorted format called an SSTable.
After the flush, the Commit Log is no longer needed and is deleted.
The Commit Log is only read on startup to recover write operations that were never flushed to disk.
Getting ready For this recipe, your system would need a separate physical disk from your data disk.
This disk should be formatted as you would format a normal disk.
Use the df command to list mounted partitions on your system:
Ensure the cassandra user has ownership of this directory and the directory has appropriate file access permissions:
When moving the commit log directory, make sure to copy the files from the old directory to the new directory before restarting.
Having a separate Commit Log improves performance for applications with high levels of write activity.
It does this by isolating the disk traffic for Commit Log activity from the traffic used for reads as well as flush Memtables and compact SSTables.
The Commit Log disk does not need to be large.
It only needs to be large enough to hold the Memtable data that is unflushed.
Commit logs have sync intervals and this does not block writes.
The Commit Log directory and underlying disk need to be fast enough to keep up with write traffic.
The speed of this disk is not an issue because the sequential write speed of even a single disk is normally sufficient.
Choosing a high performing RAID level Cassandra handles replication of data internally.
A Replication Factor of two will ensure data is written to two separate nodes.
Because Cassandra handles replication, disk systems can be optimized for more performance versus redundancy.
This recipe shows which RAID levels are commonly used with Cassandra.
Getting ready The fastest option for reads and writes is RAID-0, calling striping.
A single disk failure in a RAID-0 results in complete data loss for that node.
This configuration allows 100 percent of the storage capacity of the disks in the array.
RAID-1 typically uses two disks and mirrors data to both disks.
This option does not speed up writing, but reads can normally be striped.
Because data is copied to both disks, the storage capacity is 50 percent the total capacity of the disk.
It can survive a single disk failure However, a failed disk will result in degraded performance.
The parity information in this configuration reduces your overall storage.
It typically performs better then RAID-5 in reads and writes and can survive multiple disk failures.
You get 50 percent of the storage capacity of your disks using this RAID level.
Just a Bunch Of Disks (JBOD) can be used because Cassandra allows multiple data directories.
However, it is hard to balance out hotspots across the disks and is rarely deployed.
Consider the risk of disk failure and the downtime associated with one.
In a large enough cluster, failures are the norm not the exception.
In a hardware failure, the performance of a node may be degraded or it might be entirely offline.
While RAID cards offload processing, RAID can be done with software as well.
With the multiple options of RAID types, levels, and disks, having tools to determine which configurations work the best are available.
Hardware RAID is provided by cards that offload CPU processing and perform better than software RAID.
Read specifications carefully because not all RAID cards support all RAID levels.
Disk performance testing Disk performance tools such as Bonnie++: http://www.coker.com.au/bonnie++/, or IOZone: http://www.iozone.org/ can be used to test the performance of your disk system.
In Chapter 12, Collecting and Analyzing Performance Statistics, the recipe Monitoring disk utilization and having a performance baseline.
The file system for a device as well as the mount options chosen affect performance.
Several file system options exist, including (but not limited to) EXT, JFS, and XFS.
Most modern distributions of Linux have support for EXT4, which has impressive performance numbers and stability.
This recipe will show how to format and mount an EXT4 file system.
A quick check is to look for /sbin/mkfs.ext4 on your system.
To configure your ext4 file system for strong performance, while making less data integrity sacrifices, enable the following mount options:
You can apply mount options without having to unmount by using the remount option.
However, some changes might require a complete unmount and remount: $ mount -o remount /var.
Noatime does not update inode information each time it is read.
Since access time information is not used by Cassandra and reads are frequent, this option should be used.
Write barriers enforce proper on-disk ordering of journal commits, making volatile disk write caches safe to use, with some performance penalty.
If your disks are battery-backed in one way or another, disabling barriers may safely improve performance.
The commit=x option syncs all data and metadata every 'X' seconds.
With data=writeback, unlike the default ordered mode, the ordering of metadata and file data writes are not preserved.
In the event of a crash, old data can appear in files.
Cassandra data files are typically write once and written in a linear fashion.
Thus, writeback may not be as large of a problem as would be an application that edits files in place.
This mode is considered to have the best raw performance.
The Key Cache stores keys and their locations in each SStable in heap memory.
Since keys are typically small, you can store a large cache without using much RAM.
This recipe shows how to enable the Key Cache for a Column Family.
In most cases, you want to use an absolute size as percentages grow with your data and change your memory profile.
Getting ready Ensure the Cassandra service is not close to the memory limit by using nodetool info.
This value should be sampled over time because JVM garbage collection is a background process on a separate thread.
Using a percentage is generally not suggested because table growth results are increasing memory used.
Use nodetool cfstats to see the effectiveness of the Key Cache.
Remember, the cache hit rate may not be high right away.
The larger the cache, the longer it will take to warm up.
Once the Key cache size reaches the Key cache capacity, you should have a good idea of the hit rate.
Make the changes permanent by updating the column family metadata with the CLI:
How it works Each cache hit results in less disk activity.
A high key cache hit ratio makes searches less intensive, freeing resources for other operations.
Heap memory controlled with the Xmx JVM option should only be a portion of your total system memory.
A ratio of half JVM to free memory is suggested as a good starting point.
This is suggested because key cache works well in tandem with the VFS cache, and for the VFS cache to be effective, memory must be free for the Operating System to use.
This setting migrates caches so that after compaction, the key cache is not cold.
The next recipe, Boosting Read Performance with the Row Cache shows how to enable the row cache that stores key and all associated columns.
In this chapter, the recipe for JVM tuning to avoid system pauses describes how Xmx memory should typically be used with Key Cache.
In Chapter 12, the recipe Profiling the effectiveness of caches with cache graphs.
The Row Cache stores a key and all its associated columns in memory.
Using the Row Cache can save two or more seeks per request.
The benefit of the Row Cache is that a request can be served entirely from memory without accessing the disk.
This can allow for a high rate of low latency reads.
This change only takes effect on the server you have specified.
You will have to run this command for each server.
Use nodetool cfstats to see the effectiveness of the Row Cache.
Remember, the cache hit rate will not be high right away.
The larger the cache, the longer it will take to 'warm' up.
Once the Row Cache Size reaches the Row Cache Capacity, you should have a good idea of the hit rate.
The nodetool setcachecapacity command reconfigures the cache size on a single node for testing.
The nodetool cfstats is used to determine how effective the cache is.
Once the cache settings are optimal, the cassandra-cli can be used to change the column metadata definitions across all nodes in the cluster.
Row Cache requires more memory than the equivalent sized key cache.
However, accessing memory is significantly faster than accessing the hard disk even if the disk read is completely from the VFS cache.
There are times where it can be problematic to use the Row Cache.
One such situation is when keys have a large number of associated columns.
Another instance is when the values inside columns are very large.
Columns with a high write-to-read ratio are also not good candidates for the Row Cache.
These scenarios move a lot of data into and out of the heap causing memory pressure.
Also, sizing the cache can be difficult since the cache is based on the number of items, not the size of the items.
Many users of Cassandra choose it for low latency read-and-write performance.
Even if the operative system is not low on memory, it may decide to swap parts of memory, called pages, to disk.
When these pages need to be accessed, they will have to be read in from a disk, which takes significantly longer than if they were still in main memory.
Find any lines with swap in column two or three and place a # character in the first column to comment them out:
The swapoff command disables all swap memory that may be currently in use.
Editing the /etc/fstab file ensures swap will not be reactivated on operating system startup.
The next recipe, Stopping Cassandra from using swap without disabling it system-wide to see an alternative to turning off all swap memory.
For example, if the system is not dedicated to running Cassandra, other processes on the system may benefit from Swap Memory.
This recipe shows how to install the Java Native Architecture, which allows Java to lock itself in memory making it inevitable.
For this recipe, Cassandra must run as the root user.
Confirm this configuration has taken effect by checking to see if a large portion of memory is Unevictable:
Memory mapped file IO is more efficient for reading and writing than Standard IO.
Do this by following either the Disabling Swap Memory for predictable performance or the Stopping Cassandra from using Swap without disabling it system-wide recipes in this chapter.
Check if the operating system is 64 bit using the uname command:
If the operating system is not 64 bit it cannot efficiently memory map large files.
However, the larger the data, index, and bloom filter file, the more heap memory is required.
The alternative to mmap and mmap_index_only is standard, which uses direct IO.
Tuning Memtables for write-heavy workloads Cassandra is designed so that all disk write operations are serial.
Write operations are written to a sorted structure in memory called a Memtable (and to a Commit Log only used to replay writes on startup)
When a Memtable reaches threshold criteria it is flushed to disk.
A flush writes the Memtable to its on-disk representation: a Sorted String Table (SSTable)
This recipe shows how to modify the Memtable settings so they will flush less often.
Setting the flush settings larger causes Memtables to flush less often.
For workloads where the same column is repeatedly modified, large Memtables will absorb multiple writes, thus saving writes to disk.
Less flushing and therefore less compacting should make the VFS Cache (Virtual File System Cache) more effective.
Consider your settings carefully if you have many Column Families as each Column Family has its own Memtable.
When Cassandra is storing many column families, managing Memtables settings on a per column family basis can be difficult.
When all memtables use more memory than this, value with the largest memtable will be flushed.
In Chapter 12, Collecting and Analysing Performance Statistics, the recipe Using Memtable graphs to profile when and why they flush.
An OOP is normally the same size as the machine pointer.
Compressed OOPs replaces some of the OOPs with managed pointers.
Getting ready This option may not be available on all JVMs.
The Oracle JVM has supported this option since version 1.6.0_14
Using less memory benefits the system overall as the system bus has less data to shuffle.
Less memory usage means more memory is available for use as Key Cache, Row Cache, and Memtables.
Each request allocates temporary objects, so better memory usage equates to a higher theoretical maximum request rate.
Messages enter a stage and an event handler is called.
This recipe will show you how to tune the Concurrent Reader and Concurrent Writer stages.
It is common to calculate the number of Concurrent Readers by taking the number of cores and multiplying it by two.
Locate ROW-READ-STAGE and select Attributes, and change the CorePoolSize by entering in the textbox on the right.
The Concurrent Readers and Concurrent Writers control the maximum number of threads allocated to a particular stage.
In cases where data size exceeds main memory, this normally makes the disk the bottleneck for reads rather than number of read threads.
Writes do not have to make in-place changes to data files.
Raising these values beyond the limits of your hardware causes more contention and decreases performance.
Setting compaction thresholds Cassandra does not do in-place writes or updates.
Writes are done to Memtables, which are periodically flushed to disk as SSTables.
As a result of this approach, the number of SSTables grows over time.
Having multiple SSTables causes read operations to be less efficient as columns for an associated key may be spread over multiple SSTables.
Cassandra uses Compaction to merge multiple SSTables into a single larger one.
Update the column family metadata to make this change permanent across the cluster:
Ensure the two are configured with respect to each other.
The JVM has a variety of options that drastically affect how it operates.
For applications such as SWING Graphical User Interface applications, periodic application Garbage Collections are not a large issue.
This recipe shows how to tune JVM settings to limit (and hopefully remove) JVM pauses.
Determine how much physical memory your system has by searching through the /proc/meminfo file:
A good starting point is to use half your free memory.
Do not include swap in this calculation since it is suggested to disable swap anyway.
It does this by using multiple threads to mark objects to be garbage collected in parallel.
Concurrent Mark Sweep uses heuristics to decide when to start a concurrent collection.
The heuristics are not aggressive enough, especially under heavy load.
ParNewGC (or the parallel young generation collector) is important to Cassandra.
Cassandra allocates temporary objects during operation, and being able to process the young generation quickly stops these objects from being promoted to the older generations.
The JVM options in this recipe are a suggested starting point.
Large memory systems One situation where the standard advice does not apply is systems with large amounts of memory.
Because Java actively manages memory and garbage collections, large heaps represent a challenge for the JVM.
There's more The Garbage-First garbage collector (G1) is a next generation collector designed to work for medium-to-large machines with large heaps.
Raising the open file limit to deal with many clients.
Each client connection, as well as the connections between servers in the cluster, uses socket resources.
The defaults for a Unix/Linux system limit the number of open files.
This recipe shows how to raise the open file limit to serve multiple connections under high load.
When the hard limit is reached, the system will deny the process the ability to open more files.
This results in exceptions that usually shutdown the Cassandra process.
However, there are other ways to keep the number of sockets low.
They should issue multiple requests over the same connection or they should be using a Connection Pooling implementation such as Hector.
Increasing performance by scaling up Cassandra is designed to be scaled out by adding more nodes to the cluster.
Keep in mind that motherboards have a limited number of slots, but higher density DIMMs are more expensive.
If your system has multiple network cards, they can be bonded together to double their performance.
Larger servers may even be able to utilize 10 gigabit Ethernet.
Solid State Drives (SSD) do not have physical parts and can seek extremely quickly.
However, they are still new and more costly then spinning disks.
Network Time Protocol (NTP) is a distributed hierarchical system used to keep system clocks in sync.
Clients to Cassandra require NTP because clients need to set the timestamp filed of an insert themselves.
Servers running Cassandra require the correct time as well as they need to correctly evaluate if the lifetime of a tombstone or time-to-live column has passed.
This recipe shows how to set up Network Time Protocol.
Getting ready It is ideal to have one or two NTP servers locally (on the same subnet or in the same LAN network) synchronizing to NTP server pools on the Internet.
This is preferred over having each server sync to the NTP server pools individually.
The NTP service will not sync a clock that is drastically out of sync with the contacted server.
The NTP Daemon runs continuously and sends messages to its configured NTP servers.
It uses the reply information as well as the calculated latency involved in transmitting that data to adjust or "groom" the clock.
This is especially useful under high load where CPUs tend to drift more.
Introduction Distributed systems such as Cassandra face challenges that single-system data stores do not have to face.
The CAP theorem describes distributed systems with respect to consistency, availability, and partition tolerance.
Distributed systems can at best achieve two out of three.
Cassandra has a versatile data model that allows the user to choose the aspects of CAP enforced per request.
The recipes in this chapter require multiple Cassandra nodes to execute.
In Chapter 1, Getting Started, the recipe Scripting a multiple instance installation will quickly set up the prerequisites.
Cassandra provides consistency when the read replica count (R) plus write replica count (W) is greater than the replication factor (N)
Based on the replication factor and the number of live nodes, some consistency levels may not be achievable.
Cassandra allows users to submit consistency level per request, allowing the user to trade-off between consistency, performance, and failed node tolerance.
Getting ready The table describes the strength of consistency based on the levels chosen for read and write operations.
Users of this application will specify a host and port to connect to Cassandra with.
The program also requires a value from the user that will be written to a column.
Down a node in our cluster by killing it, and use nodetool to confirm its status.
Start up the down node and run the application again:
First, the program detects if the required keyspace and column family exists.
The replication factor of this keyspace is set to five, so the write must succeed on five nodes, otherwise an exception will be returned to the user.
All of the needed parameters except for the colVal are specified by the users at the start of the program.
The write operation will either succeed and continue to the next line in the program or take the exception branch if a problem arises.
In Chapter 3, Application Programmer Interface we should know how to use transactional style locking using Cages.
When two columns for a row key have the same name, the timestamps of the columns are compared and the value of the column with the highest timestamp is the final value.
This recipe shows how to set the timestamp of a column.
Create a column, calculate the current time, multiply by 1,000, and then pass the result to the setTimestamp method.
The timestamp is used to enforce the idempotent behavior of a column.
The column with the highest timestamp is the final value.
This is useful in situations where a write operation may be received out of order.
A column with a smaller timestamp will not overwrite a column with a larger timestamp.
The timestamp column does not have to be a numeric representation of a date and time.
It is a 64 bit integer that is used to resolve conflicts, and the user is free to choose any value.
Other options are incrementing the previous value or use an auto-increment value from another source.
Remember that a delete operation creates a tombstone that will need to have a higher timestamp than the current data to remove it.
Disabling the hinted handoff mechanism Cassandra determines which nodes data should be written to using the row key (and the replication factor, partition, strategy, and options)
If a node that should receive a write is down, a hint is written to another node.
When the downed node comes back online, the hints are redelivered to it.
Long-term outages can be a problem as storing hints on other nodes can begin to negatively impact their performance.
Hinted handoff writes do not count towards the strong consistency calculation discussed in the section Working with the formula for strong consistency.
If there are failures, data could remain out of sync until it is repaired either with anti-entropy repair or read repair.
This could result in stale reads depending on the read consistency level chosen, especially when using Read.ONE.
When hinted handoff is enabled, two variables fine-tune how it operates:
This is used to ensure that a node that is down for extended periods of time will not cause other nodes to store hints indefinitely.
This option adds small delays in between each hinted handoff row that is delivered.
This is helpful in two ways: first, the revived node is not overloaded with a burst of hinted handoff messages.
Secondly, the sending node does not consume a large amount of resources trying to deliver the hinted messages.
Read consistency levels QUORUM and ALL are always synchronized before data is returned to the client.
Data read at consistency level ONE uses a different code path.
As soon as a natural endpoint finds data, it is returned to the client.
Read repair keeps data consistent by comparing and updating the data across all the replicas.
Each column family has a read_repair_chance property that controls the chance of a read repair being triggered.
This recipe shows how to adjust read_repair_chance for better performance.
Getting ready A quick way to set read repair chance is by using the CLI.
The following recipe manipulates the value using a program to demonstrate the functionality in the API.
Use the describe_keyspace method to find a keyspace by the user-specified name.
Each keyspace has a list of CfDef, or column family definition, objects.
Start the application passing it the required environment variables, including the keyspace, column family, and the chance:
Lowering read_repair_chance will result in less read traffic across the cluster.
If data is requested multiple times, the savings are cumulative.
Disabling read_repair_chance and hinted handoff greatly increases the risk of data being out of sync.
Cassandra is a peer-to-peer distributed system and schema changes will propagate quickly, but not instantly, to all nodes.
This recipe shows how to check that all nodes are at the same schema level.
This is helpful for applications that need to create keyspaces and column families and write to them once they are available on all nodes.
If the numbers of keys in the map is greater than one, two versions of the schema are currently present in the cluster and the changes are not completely propagated.
Each time a schema changing operation is issued, an ID is calculated.
If only one key in the schema map exists, all nodes in the cluster are at the same schema version.
A schema change has to propagate to every node in the cluster.
To avoid issues, it is best not to make too many rapid schema changes or make changes when nodes in the cluster are down.
A quorum is the number of nodes that need to be in agreement to reach a consensus.
The formula to determine the nodes needed for a quorum is:
When using a replication factor of one, data only exists on a single node and it is always consistent, but not redundant.
When using a replication factor of two or higher, operations at level quorum are used to achieve consistency.
This recipe shows how to create keyspaces at different replication levels and how quorum operations work at different replication levels.
Stop one of the nodes and confirm it is down using nodetool:
Additionally, the operation can throw a TimedOutException if the nodes are UP according to cluster gossip, but do not respond in a timely fashion.
When replication factor is two, QUORUM is equal to ALL.
This majority is only possible with replication factor of two when both nodes are UP.
Using write consistency QUORUM, read consistency QUROUM for strong consistency recipe in this chapter to see the performance characteristics with using QUORUM.
Using write consistency ONE, read consistency ONE for low latency operations.
Consistency level ONE is the lowest latency consistency level available for clients to read and write with.
It is low latency because the operation is acknowledgment to the client by only one of the natural endpoints for the data.
This recipe shows how to use consistency level ONE, and what the CAP trade-offs are for using this level.
Users will supply a host list and internally the application will open a connection to each.
Close up all connections to the cluster by iterating the list of connections .*/
Retry the application, this time specifying the retryRead environment variable:
This application shows that using consistency level ONE for write and read operations results in eventual consistency.
Because ONE has the lowest consistency guarantee, in the first run of the application, 117 of the inserts had not propagated to the other replicas before a get request for that data was issued.
In the next run with the retryRead option enabled, only three keys were not found.
The data would eventually arrive at the destination, but there is no guarantee how long it will take.
The following diagram illustrates a client reading from a node before a write has propagated to it:
It is important to understand that "eventual" can be small deltas in time.
From the output above, using Cassandra at the weakest level only caused a stale read 117/100000 times in a contrived scenario designed to detect it.
The term tunable consistency is often used to describe Cassandra as the consistency is chosen on a per request basis.
Using write consistency QUORUM, read consistency QUORUM for strong consistency.
When combined with a replication factor of three or higher, QUORUM operations still succeed even when some of the nodes are down.
Using quorum is favored in many cases because it brings consistency, good performance, and failure tolerance.
Getting ready This recipe requires the application from the last recipe: Using write consistency ONE, read consistency ONE for low latency operations.
This requires a keyspace and a column family to work with.
Down a node and confirm it is down using nodetool:
When reading and writing at QUORUM data, consistency is strong.
Using this combination, the NotFoundCount counter should always be 0 unless enough nodes in your cluster are down that a quorum cannot be achieved.
While reading at QUORUM, clients are blocked until a majority confirms the data is consistent.
This means the read time is longer than that of reading at ONE.
The following diagram illustrates how reading and writing at QUORUM ensures clients always have a consistent view of data:
Since the read operations are typically more intensive than write operations, read-heavy applications try to optimize reads in any way possible.
This recipe shows performance advantages and the consistency trade-off for this combination of quorum levels.
Trading off consistency for performance may not be an acceptable solution.
Consider other techniques for enhancing performance before giving up on consistency.
Getting ready Complete the code in the recipe Using write consistency ONE, read consistency ONE for low latency operations.
Read operations are more intensive than write operations especially as the data per node increases.
The run of the program shows the performance of this combination is strong.
However, due to eventual consistency, this combination can result in stale reads if data is read from a natural endpoint quickly before the write was replicated to it.
Since QUORUM acknowledges the write on two nodes, the NotFoundCount would be lower than if the data was written at ONE.
Using the consistency level ALL always has strong consistency regardless of what other level it is paired with, with one important exception: writing at ANY and reading at ALL is not strongly consistent.
Remove the downed node from the host list and run the application again:
The drawback of using ALL is any node failure will cause operations involving that node to fail.
From the example, the high exceptions count shows many writes fail when even a single node is down.
Normal operation cannot continue even with a single node being down.
Some use cases may wish for Cassandra to accept writes even if all the natural endpoint nodes are down.
Cassandra has a consistency level that can be only used for write operations called ANY.
When using ANY, the write is delivered to any node in the cluster to be redelivered later via the hinted handoff mechanism.
The number of NotFoundCount events shows that the consistency is unpredictable when using this level for writing regardless of the level used for reading.
This level is best used for applications that do not want to miss write operations, do not care about the consistency of data, and do not care about delivery delays.
Demonstrating how consistency is not a lock or a transaction.
The classic way to illustrate this functionality is with a counter that needs to be manipulated by multiple threads at once.
Along with connection information of host and port, users must supply the number of threads and the number of inserts per thread */
For the first run of the program, one thread does 70 inserts.
This happened because multiple clients are operating on the column simultaneously and have no way to lock the value so it cannot be read or changed by others.
In Chapter 10, Third-party Libraries and Applications, the recipes Setting up Zookeeper to support Cages for transactional locking and Using Cages to implement an atomic read and set.
Introduction A critical component of performance understands how to utilize the data model that Cassandra provides.
The recipes in this chapter show ways by which data can be modeled to be stored and accessed efficiently when using Cassandra.
The columns associated with a key are stored in a sorted map data structure.
This is different than data stores that use column separators or fixed width rows.
One advantage of this is that all entries can have differing columns.
However, due to this design the column names and values need to be stored on disk for each key.
This recipe demonstrates the advantages of using smaller column names.
This allows us to demonstrate how the column name affects the overall size of data files:
Run the ColSize application specifying a colname that is a large string:
Run the ColumnSize application again, this time specifying a column name of just a single character s:
Flush the data to the disk and use the ls command to display file size information:
A column is stored as a tuple of name, value, and timestamp.
These larger columns also use more memory inside key caches, row caches, and Memtables.
The savings using smaller column names can be significant when the size of a column name is a large portion of the column size.
If all or most of the data for a key will be read with each request, it may be better to serialize all the columns of a key into a single column.
This recipe will use delimited text as an alternative to storing multiple columns under the same key.
Use the CLI to see that the data is stored in a single column: [default@parking] assume parking validator as ascii;
Each column is a tuple consisting of a column name, a column value, and a timestamp.
When column sizes are very small, the overhead of the tuple can be significant.
Beyond the data size of the column, index overhead exists as well.
This method reduces overhead in some cases, but can use more storage in others.
This can happen if this data is updated often as each update will be the entire row rather than a small column.
Storing time series data effectively Modeling time series information has wide appeal.
Typically, Network Management Systems (NMS) use time series data to store system performance information.
This type of storage is useful for trending statistics over time such as the weather or stock market prices.
This recipe shows how to take advantage of the distributed nature of Cassandra while using a Comparator to organize numeric data inside a column.
The DateFormat classes allow calendar information to be displayed in a user-defined format.
The date format is used to group data into the same key.
Use the Counter class and its utility methods to write two counters.
These counters represent the CPU usage of a fictitious processor core:
Make sure to set the comparator to LongType: [default@unknown] create keyspace perfdata;
The ability to construct long composite keys combined with the sharding capabilities of the data model provides a horizontally scalable storage system for performance counters.
Comparators allow the column data to be ordered in a user-defined manner since data in Cassandra is a byte []
Using Super Columns for nested maps Super Columns are columns where the value is a sorted map of keys and values.
This is different than a standard column where the value is a byte array.
Super Columns add another level of nesting, allowing storage of more complex objects without encoding data into composite columns.
For this recipe, we use a Super Column to store information about movies and reviews of the movie.
Super Columns provide another level of associated arrays in the value of the column.
This can be used as an alternative to generating composite column keys.
This recipe uses two Super Columns: cast to store actors in the movie, and reviews to store comments on what the users thought of the movie.
Super Columns allow us to store this information without using multiple Standard Column Families or composite keys.
Because of the de-serialization overhead, avoid storing a large number of sub columns in a Super Column family column.
Using a lower Replication Factor for disk space saving and performance enhancements.
The Replication Factor can be set on keyspace (not per column family)
The higher the Replication Factor, the more nodes data is stored on and the more fault tolerant your cluster is.
A larger Replication Factor means the storage capacity of the cluster is diminished.
A higher Replication Factor also means write and read operations take more resources on more hardware.
Using a Replication Factor of 2 for the bulkload keyspace uses less disk space and uses less resources, creating less overall utilization.
This results in a performance gain at the cost of some redundancy.
This is because Quorum requires a majority that cannot be achieved with one of two replicas.
An important decision when setting up your cluster is choosing a Partitioner.
The Random Partitioner hashes the user supplied key and uses the result to place this data on nodes in the cluster.
The hash function distributes the data roughly evenly across the cluster in a pseudo random order.
The Order Preserving Partitioner uses the key itself to determine which nodes on the cluster the data should be stored on.
This stores the keys in order, which allows ordered Range Scans on keys.
If the keys do not have an even distribution, nodes can become unbalanced and some will have more data than others.
This recipe shows how to use an Order Preserving Partitioner and hash the values on the client side, achieving a similar effect to the Random Partitioner.
Getting ready The partitioner used is a global setting that can only be set when the cluster is first initialized.
To change the partitioner, you must remove all data in your cluster and start again.
Left pad the name with two characters from the hash:
Change to the user keyspace and list the user column family:
By generating an MD5 check-sum from the key, we randomize which node the data for that key will be stored on.
This achieves a similar effect to that of using RandomPartitioner.
Range scans performed on the phonebook column family return results in alphabetical order.
Range scans performed on the user column family return data in a pseudo random order, which should balance data across the cluster as the RandomPartitioner would.
Choosing a partitioner is an important decision that cannot be changed.
To experiment with this option, using a multiple node cluster in your test environment can help determine the distribution of your row keys.
Using different hash algorithms This recipe used the md5hash() method used internally by Cassandra.
Different algorithms can be used that may be less computationally expensive while still achieving good entropy.
Storing large objects The row data stored in Cassandra is typically smaller in size, between a few bytes to a few thousand bytes.
Some use cases may wish to store entire file or binary data that could be megabytes or gigabytes in size.
Storing large files with a single set operation is difficult.
This is because the underlying transport layer does not have streaming capability and is designed around smaller objects.
This recipe demonstrates how to store large objects or files by breaking them up into small parts and storing the parts across multiple columns of a single key.
Open an input stream to this file to the user specified file:
As long as the file loaded was not a binary file, the CLI can be configured to display the data as ascii: [default@filedata] assume filedata comparator as ascii;
List the column family to display the contents of the file:
Rather than attempting to store a file under a single key and column, the file is broken up into smaller pieces and stored across multiple columns.
This is done because the transport protocol has to buffer objects completely in memory to transmit them, and because the frame size for that transport is smaller than some files.
This approach stores all the blocks of a file under a single row key and thus on a single set of nodes.
Another possible design may be to store the blocks of the file in different row keys.
This will spread the blocks of a single file across multiple nodes.
Using Cassandra for distributed caching Cassandra has several variables that can be configured for each keyspace and column family that drastically change the profile of how it operates.
In typical caching use cases, the amount of write and update traffic is low compared to the amount of read traffic.
Additionally, the size of the data may be relatively small.
This recipe shows how to set up a keyspace and column family for this type of workload.
Create a keyspace with a high replication factor (assume this is a 10-node cluster):
With a replication factor of 10, each write uses resources on each node.
This does not scale writes because each node must maintain a copy of all the data in the keyspace.
Because read_repair_chance is set to a low value, read repairs will rarely be triggered as long as clients use consistency level ONE on read operations.
By increasing replication factor and reading at ONE, more nodes can be used to serve the same data set.
Storing large or infrequently accessed data in a separate column family.
In many cases it is suggested to store all data related to a key in a single column family.
This allows applications to fetch data by having to seek only a single key.
This is not always the best option, however, and this recipe demonstrates a case where storing the data in two separate column families is preferred.
For this recipe, assume the source data to be inserted as the following table:
For the example data, of the five columns that need to be stored, the quote data is many times larger than all the other columns combined.
Since this data is not accessed as frequently as the other columns, it is stored in the extra_info column family.
The extra_info family uses a lower replication factor, which saves disk space.
Each keyspace corresponds to a folder under the data directory.
Also, by separating the data into two column families, two separate caching policies are utilized.
The user_info has a small and predictable row size and a row cache was used.
The extra_info data can have an unpredictable size, thus the key cache was used.
Graph databases are used to store structures based on data from graph theory.
Graph databases have a concept of nodes that are entries and edges that connect one node to another.
Typically, graph databases are used to determine data that is closely related, as in the following image:
This recipe shows how to use, store, and traverse a simple graph database.
Getting ready Create a keyspace and column family to store the graph data and insert some sample data into it.
To traverse a graph, a few pieces of information are needed: the first is a starting point, the next a stopping condition, and the third a list of already seen nodes to prevent cycles, and a depth counter to control how many recursions.
The only way to call it is through the kickoff method defined previously.
Run the application, supplying a node to start at using the startAt parameter and a depth for recursion into the graph:
Each node is a single column, the row key is the name of the node, each column is the name of a node this node is related to, and the value is unneeded and left empty.
Inside the recursive traverse method, getSlice is called against the start node.
It returns the key, if it exists, and the columns returned are the related nodes.
If the related node does not exist in the set of already seen nodes, and the max depth has not been exceeded, a recursive called will be done using the related node.
The row key determines which nodes the data is stored on.
The row key is also used to locate the data on disk in the underlying data files.
The columns of a row key are sorted by the column name, making searching for specific columns inside a specific key optimal.
At times, one or more ordering of data may be required.
Storing data multiple times in different ways can be done to support different requests.
This recipe shows how to create two orderings of a mailbox—the first being efficient to search messages by time, and another to search messages by user.
Getting ready Create the required keyspace and column families for this recipe.
The searchFrom method is used to quickly locate messages to and from some specific users.
Run the program from the command line searching for messages to and from specific users:
This recipe stores data from messages in two separate columns families.
Storing data in multiple times increases the overall disk usage.
However, the cost can be justified because the data can be stored in a way that is optimal for specific requests.
The subject column family is useful for looking up messages for a user by time.
The fromIndex column family is designed to help find messages from a specific user.
Cassandra has no single point of failure or master nodes.
Instead, it uses an internal process called Gossip to communicate changes about the topology of the Ring between nodes.
Seeds are a list that a node attempts to contact on startup to begin gossiping.
Getting ready The technique for defining Seeds can vary based on the type and size of your deployment.
For multiple datacenter deployments, place one or more seed in each datacenter, refer to Chapter 8, Multiple Datacenter Deployments.
However, there are a few subtle things to keep in mind when configuring them.
This should allow the cluster to continue functioning even with DNS issues since Cassandra communicates exclusively by IP address.
Keep the seed list synchronized The seed list should be the same for every node in your deployment.
This is not a strict requirement for the gossip protocol to work, but there are cases during failures where using different nodes can cause strange results.
Consider using a configuration management tool such as Puppet to keep configuration files in sync.
Seed nodes do not auto bootstrap At least one seed node needs to be specified during initial deployment.
For a joining node to receive data from other nodes, do not include its hostname or IP in its seed list.
After it has bootstrapped and joined the cluster, it can be put in any seed list for any node, including itself.
Choosing the correct number of seed nodes The number of seed nodes should be some factor of the cluster size.
Nodes starting up will attempt to contact all configured seed nodes to learn about the topology of the cluster.
Ensure at least one seed is running at all times.
For deployments of less than ten, two or three seed nodes are sufficient.
Nodetool Move: Moving a node to a specific ring location.
Cassandra uses a Consistent Hashing to divide the data in a cluster between nodes.
Each node has an Initial Token that represents its logical position on the ring and what data it should hold.
There are many situations where a node is not placed at an optimal position in the ring.
This may be because of the addition or removal of other nodes.
This recipe shows how to use nodetool move to adjust a node's position.
Getting ready Use the recipe Calculating Correct Initial Tokens from Chapter 1 to determine what the ideal tokens should be.
Run nodetool ring and determine how unbalanced the cluster is:
Notice that in the Owns column, some nodes own larger portions of the ring than others.
Unless the number of nodes in the cluster is growing by exactly double, not specifying the InitialToken will likely result in a non-optimal number being picked.
Moving large amounts of data can take a long time.
The more data a node has on it, the longer an operation such as move will take.
Also, these operations are intensive and are best done at times of low traffic.
Run nodetool ring after the nodetool move operations have completed:
Nodetool Move is a one-step way to accomplish the two-part process of removing and adding a node.
Behind the scenes, nodetool move causes the node to move data to other nodes before leaving the cluster.
The node then re-joins at the location specified by the user.
Other nodes compute data that belongs on the rejoined node and transmit that data back to it.
In the example, using nodetool move corrects imbalances of data across the cluster.
Once all the proper move operations are complete, the owned percentage would be 20 percent across all nodes.
It is important to keep clusters as close to balanced as possible.
Nodetool Remove: Removing a downed node If a node fails, it is not assumed lost forever.
The other nodes in the cluster become aware that it is down through gossip.
If enabled, the other nodes will store Hinted Handoff messages, which are writes destined to downed system.
They will attempt to re-deliver the hinted messages when they detect the destination node is up again.
If a node is lost forever, either because of hardware failure or because it is no longer needed, the administrator must actively acknowledge the failure using nodetool removetoken.
You can run the command again with the optional status argument at that end to see the progress.
When the removal is complete, the list should be one element shorter:
When a node is removed, Cassandra begins actively replicating the missing data until it is all stored on the number of nodes specified by the replication factor.
The nodetool removetoken command can only be run on a node in the Down state.
The next recipe, Nodetool Decommission: Removing a live node shows how to remove a node in the Up state.
Decommission is the process for which a node in the UP state is removed.
Use nodetool ring and find the Address of a node to remove:
Select node 127.0.0.3 for decommission and remove it with nodetool decomission.
The host nodetool connects to with nodetool decommission will be the one removed.
With removetoken, the node with the specified token will be removed.
Depending on the amount of data in your cluster, this operation can take a long time.
The machine first anti-compacts the data it has and then streams the data to the node that will now be responsible for that data.
Use nodetool ring over time and the decommissioned host will eventually vanish from your list.
Review the logs of the nodes to see how the compaction and streaming are progressing.
You can also use nodetool compactionstats and nodetool streams to view the progress.
Nodetool decommission has the same effect as Nodetool remove, but is more efficient because the node leaving the cluster helps calculate and transmit the data that will need to be moved to other nodes before leaving.
By default, the nodes of a cluster have auto_bootstrap set to false.
With this setting when a node starts up, it does not begin migrating data to itself from other nodes.
This recipe shows how to use this setting and explains the caveats that come with it.
Several recipes in this chapter will use password-less access to the OpenSSL tools such as ssh, scp, and rsync.
This recipe shows how to generate a public private SSH key pair.
With auto_bootstrap set to false, the node joins the cluster immediately and starts serving requests.
Setting auto_bootstrap to true means other nodes in the cluster recompute and transfer data for the new node before it comes online.
The data of a node that did not auto_bootstrap can be populated in several ways:
Normal write traffic After the node is joined, it is responsible for a part of the Token Ring.
Write operations will be delivered to this node as normal.
This keeps newly written data in sync, but does nothing for already existing data that this node is now responsible for.
The Natural Endpoints for the data are compared and the most recent timestamp is the final value.
For read repair to deliver the consistent data, applications must be reading at consistency level Quorum or higher.
Reading at One causes the first read to return an empty or stale result and the data is repaired after the result is returned to the client.
Anti-Entropy Repair Anti-Entropy Repair is an intensive process that calculates the differences between data on nodes and then streams the differing data between nodes.
The anti-entropy repair ends up being more computational work on the cluster than joining a node using auto-bootstrap.
Append the public key to the authorized_keys file and ensure the permissions are strong.
If done correctly, connecting to the destination server with ssh, rsync, and scp should not require a password.
This ability to run commands on remote machines makes it a fast and simple solution for ad-hoc distributed computing.
It is commonly used to run commands on remote computers typically from non-interactive jobs such as rsync-based backups through cron.
These is more Even though SSH keys are cryptographically secure, they do represent a security risk as they can be used to move from one server to the next or execute remote commands.
If a machine is compromised, a key with no passphrase would allow a user to spring to another server.
It is highly advised to limit commands an SSH key can execute.
See the SSH manual pages for information on restricting these keys.
Copying the data directory to new hardware The Bootstrap and Anti Entropy Repair processes are generally the best ways to move data to new nodes.
In some cases, it is more efficient to move data around with a file copy tool such as rsync.
This method is efficient when doing a one-to-one move from old hardware to new hardware.
This recipe requires an SSH Server and SSH Client, but any method of transferring binary data such as FTP is sufficient.
Create an executable script /root/sync.sh that uses the rsync command:
On the source server, cassandra05, stop the Cassandra process and run the sync again.
It will take much less time than the first run because rsync only transfers changes in the files.
Disable Cassandra so it will not be accidentally started again:
The rsync command is an intelligent copy tool that calculates rolling check sums and only transfers new or changed parts of files.
One of the advantages of rsync is that runs after the first one only need to transfer changes.
This is ideal for the structured log format of Cassandra's SSTable.
This technique requires less computation and data movement compared to the process of nodetool decommission, new node bootstrap, and then subsequent nodetool cleanup.
However, it is slightly more intensive on the administrative side.
This method is best used when the volume of data per node is very large.
As far as the other nodes in the cluster know, the node was offline for a few minutes and is now restarted.
They are unaware that the data, IP, and host name moved to new hardware.
There's more rsync has a myriad of options that can change how it works.
One option is the ability to exclude data that does not need to be copied with expressions.
The bwlimit knob throttles the network usage and, in effect, the disk usage.
This can be used so that the added disk activity will not diminish the serving capabilities of the node.
Cassandra uses Consistent Hashing to decide which nodes data should be stored on.
When a new node joins the cluster, it becomes responsible for a section of data.
That data is calculated and transferred to that node during the bootstrap process.
That transferred data is always a subset of data on the node logicall right in the ring.
One way to achieve a node join is to transfer the data yourself and then run cleanup on both nodes.
Getting ready Review the Generating SSH Keys for password-less interaction recipe for moving data between different physical nodes.
Use the nodetool ring tool to locate a section of the ring with the most data:
Start the node and confirm it has correctly joined the cluster using nodetool ring:
Just as with auto-bootstrap, some nodes in the cluster are now carrying extra data that they are no longer responsible for.
Use nodetool cleanup to remove data that is no longer served by other nodes.
This technique uses rsync to carry out roughly the same process of an auto-bootstrap.
The built-in Cassandra bootstrap is the ideal solution in most situations.
This solution can be faster particularly if the node is under heavy usage as normally Cassandra would have to use resources to anti-compact and stream the data.
Unlike the bootstrap process, rsync can be stopped and restarted.
Anti-Entropy Repair, also called Anti-Entropy Service or AES, is a process where nodes compare their data and ensure data is replicated properly and up-to-date.
This recipe explains how to do an anti-entropy repair and the conditions for which it should be ran.
Anti-entropy repair is intensive for disk, CPU, and network resources.
It is optimal to run this at times of low traffic.
It can create excess copies of data on your nodes.
If the storage on nodes grows significantly as a result of AES, use nodetool compact.
Major compaction should remove duplicate data that resulted from the repair.
There are also other situations when this operation should be run.
Raising the Replication Factor If the Replication Factor of a Column Family is raised from two to three, data previously stored on two nodes should now be stored on three nodes.
Read repair fixes this over time but AES should be run to ensure the node has the proper data as quickly as possible.
Joining nodes without auto-bootstrap If a node without auto-bootstrap starts with no data, AES will ensure this node gets populated with the correct data.
Loss of corrupted files If data files such as SSTables, Indexes, Commit Logs, or Bloom Filters are lost or corrupt, AES fixes this by re-synchronizing with other nodes.
This is more effective than removing and rejoining the node again.
Nodetool Drain: Stable files on upgrade Cassandra is designed to be fail-fast.
It is designed without elaborate shutdown procedures to be resilient and start up properly even after an unplanned shutdown.
Sending the daemon process a standard kill should shut the node down properly.
A special Nodetool operation called Drain exists as an alternative to the standard kill.
This recipe shows how to use drain and under what circumstances it should be used.
Use the nodetool drain command to shut down a selected node:
Drain can be almost instantaneous or can take a long time depending on the Commit Log and Memtable settings.
Check for the presence of the final message to ensure the drain is complete.
Future versions of drain may self-terminate and remove the need to run kill.
It flushes the Commit Logs to Memtables and then flushes Memtables to SSTables on disk.
The next time a node starts up it will not need to re-read its commit logs since those will be empty.
Users who are paranoid about losing a write should use drain as the log actively acknowledges a clean shutdown.
Instead, a delete is another write entry known as a tombstone.
During the compaction process, old versions of columns as well as their tombstones older then GCGraceSeconds can finally be removed.
This recipe shows how to lower GCGraceSeconds and what this change implies.
From the CLI, issue the update column family operation and change the grace seconds time.
Lowering this value is important for data sets that are completely rewritten often.
The worst case scenario would be a data set that is completely rewritten every day as the disk would hold many tombstones or old data with newer versions in other SSTables.
Lowering this setting to three days allows compactions to reclaim the disk space sooner.
Also, it allows some lead time; if an outage happens on a Friday night, you will hopefully be able to enjoy your weekend before dealing with the troubled node.
One of the drawbacks of lowering gc_grace deals with data resurrection.
Cassandra is a distributed system, and distributed deletes have a complex lifecycle of their own.
Data resurrection Envision a three-node cluster with nodes A, B, C, a Replication Factor of three, and gc_grace set to one day.
On Friday night, node A suffers a power supply failure.
On Saturday, a key X is deleted and Tombstones are written to node B and node C.
On Sunday, node B and node C compactions remove all data and Tombstones for key X.
Key X does exist on node A and it happily returns it to the client and Read Repair (if enabled) would recreate the key on nodes B and C!
If a node is shutdown longer than gc_grace, you must rebootstrap it to avoid resurrecting deleted data like previously described!
Scheduling Major Compaction Because SSTables are written once and not modified, delete operations are implemented as a special write called a Tombstone.
A Tombstone is a marker that signifies a column is deleted.
Compaction is a process that merges multiple SSTables together, rows Tombstoned are removed, while other rows are moved to the same location on disk for more efficiency.
After the compaction is done, the source tables are deleted.
Normal Compaction can remove all traces of a key if the Tombstone is older than the gc_grace and if the Bloom Filters confirm that the key exists in no other SSTables.
Data that is marked as deleted and older than GCGracePeriod will not be present in the new SSTable.
SSTables normally compact only when there are a few like-sized tables.
This means that data can still be resident on disk long after it is deleted, waiting for compaction to finally remove it.
This recipe shows how to run Major Compaction on a schedule forcing the removal of Tombstones.
Set the script to be executable, then use the nohup command to run the script detached from the console:
The more data to be compacted, the longer this process takes.
In a cluster of many nodes, the compaction of a few nodes at a time is absorbed.
It compacts a node, then it sleeps, and then it compacts the next in the list.
The nohup command detached the process from this way as the user closing that terminal does not stop the script from running.
Normally, users will have access to an enterprise-wide task scheduler or configuration management system.
For those using Puppet for configuration management, it can handle randomizing when compaction runs.
Using nodetool snapshot for backups One of the benefits of the write-once data file format of Cassandra is that a point in time snapshot backup of data is easy to achieve.
Snapshot makes hard-links of files in the data directory to a subfolder.
These files are not removed until the snapshot is cleared.
SSTables are written once and never edited until they are compacted into other tables and removed.
Snapshots maintain a folder with hard links to the original SSTables at the time of the snapshot.
These files will not be removed until the snapshots are cleared.
This can easily be used to back up Cassandra's data files to a remote system.
Snapshots are used to make point in time backups of data on a specific node.
Each time a snapshot is taken, a folder with hard links to data files are made.
When multiple data files are compacted, the old files are no longer needed.
However, if snapshots still exist with references to the files, they can now be removed.
Getting ready Check for existing snapshots in your Cassandra data directory.
If you need to create one, refer to the previous recipe, Using nodetool snapshot for backups.
Use the nodetool clearsnapshot command to remove all existing snapshots:
Check to make sure the snapshot directory has been removed:
The nodetool clearsnapshot feature removes all existing snapshot directories and unlinks all the files in them.
This allows the data files to be removed and the space to be freed for new data.
Restoring from a snapshot A snapshot is a copy of the Cassandra data files at a point in time.
These files can be the results of a snapshot command or another backup method.
On startup, Cassandra scans its data directories for data files and loads them for serving.
By stopping the server and changing the files in the directory, restarting Cassandra will begin serving the data from those files.
Data is typically stored on more than a single node.
Multiple nodes will have to be restored at once so that updates are not reapplied.
Exporting data to JSON with sstable2json Exporting data can be used as a backup or to move data to another system.
This recipe shows how to use the sstable2json tool for exporting data.
Use the sstable2json tool to display the data in an SSTable.
This tool exports the binary data of an sstable to a JSON format that can be read by other tools.
Sstable2json has several options that can help it be used for troubleshooting as well as exporting data.
Extracting specific keys Sstable2json can extract one or more specific keys using -k arguments.
Excluding specific keys Sstable2json can exclude certain rows using -x arguments.
Saving the exported JSON to a file Send the output to standard out to save the results.
Using the xxd command to decode hex values When a column is row stored only as bytes with no comparator or validator, it is displayed as hex.
If the system has the xxd tool, you can decode these.
Nodetool cleanup: Removing excess data When a node is added to a Cassandra cluster or an existing node is moved to a new position on the token ring, other systems still retain copies of data they are not responsible for.
Nodetool cleanup removes data that does not belong on this node.
Use the IP and JMX port as arguments to nodetool cleanup.
If called with no arguments, cleanup is run on all keyspaces and column families.
The keyspace and column family can be specified at the end of the command to limit the data cleaned up.
Cleanup is a special type of compaction that removes data that does not belong on the node.
Cleanup is intensive because it has to examine large portions of the data on disk.
Topology changes The first and most common reason cleanup is needed is a node being added or moved from the token ring.
These changes cause the nodes that some data in the cluster belong to shift.
Cassandra was designed not to automatically remove data from the old location.
The reasoning behind this logic is that if a node has faulty hardware, it will likely fail quickly and keeping the data in the old place for some protects against this.
Hinted handoff and write consistency ANY The second reason where cleanup is needed is when using hinted handoff and write consistency ANY.
During a write, if all the nodes data belongs on are down, a write can be delivered to any node in the cluster.
This write will be re-delivered later to the node in which it belongs.
No process is in place to remove the hinted handoff data automatically.
Nodetool Compact: Defragment data and remove deleted data from disk.
The data files in Cassandra are stored in a structured log format.
Row keys with columns written over time can be spread across many data files.
Frequently updated columns may be stored multiple times on disk.
The process that merges these tables over time is called compaction.
This recipe shows how to force a major compaction that combines multiple data files into a single one.
Use the IP and JMX port as arguments to nodetool compact.
If called with no arguments, compact is run for all keyspaces and column families.
The keyspace and column family can be specified at the end of the command to limit the data compacted.
Major compaction joins multiple data files into a single one.
If a row was fragmented across multiple physical data files, it will now be in a single file, which should make reading more efficient.
The gc_grace are removed along with their data they mark as deleted, which shrinks the size of the data files.
The tunable consistency model of Cassandra extends beyond a single datacenter to complex multiple datacenter scenarios.
This chapter discusses the features inside Cassandra that are designed for this type of deployment.
Changing debugging to determine where read operations are being routed.
Cassandra replicates data to multiple nodes; because of this, a read operation can be served by multiple nodes.
If a read at QUORUM or higher is submitted, a Read Repair is executed, and the read operation will involve more than a single server.
In a simple flat network which nodes have chosen for digest reads, are not of much consequence.
However, in multiple datacenter or multiple switch environments, having a read cross a switch or a slower WAN link between datacenters can add milliseconds of latency.
This recipe shows how to debug the read path to see if reads are being routed as expected.
In another display, open an instance of the Cassandra CLI and use it to insert data.
Changing the logging property level to DEBUG causes Cassandra to print information as it is handling reads internally.
Later in this chapter, the recipe Quorum operations in multiple datacenter environments describes a scenario where being able to troubleshoot the read path is critical to performance.
Using IPTables to simulate complex network scenarios in a local environment.
While it is possible to simulate network failures by shutting down Cassandra instances, another failure you may wish to simulate is a failure that partitions your network.
A failure in which multiple systems are UP but cannot communicate with each other is commonly referred to as a split brain scenario.
This state could happen if the uplink between switches fails or the connectivity between two datacenters is lost.
Getting ready When editing any firewall, it is important to have a backup copy.
Testing on a remote machine is risky as an incorrect configuration could render your system unreachable.
IPTables is a complete firewall that is a standard part of current Linux kernel.
It has extensible rules that can permit or deny traffic based on many attributes, including, but not limited to, source IP, destination IP, source port, and destination port.
This recipe uses the traffic blocking features to simulate network failures, which can be used to test how Cassandra will operate with network failures.
A snitch is Cassandra's way of mapping a node to a physical location in the network.
It helps determine the location of a node relative to another node in order to ensure efficient request routing.
Restart the Cassandra instance for this change to take effect.
Cassandra uses this information to determine which hosts are ‘closest'
It is assumed that ‘closer' nodes will have more bandwidth and less latency between them.
Cassandra uses this information to send Digest Reads to the closest nodes and route requests efficiently.
It is also rigid in that if a single machine does not adhere to the convention, the snitch will fail to work properly.
The next recipe, Manually specifying Rack and Datacenter configuration with property file snitch to see how to use a configuration file to set the topology information.
Scripting a multiple datacenter installation Testing out some multiple datacenter capabilities of Cassandra can sometimes require a large number of instances.
This recipe installs and creates all the configuration files required to run a multiple datacenter simulation of Cassandra locally.
Getting ready This recipe is an enhanced version of the recipe Scripting a multiple instance installation in Chapter 1, Getting Started.
It creates many instances of Cassandra, and each instance uses a minimum of 256 MB RAM.
A Cassandra release in tar.gz format needs to be in the same directory as the script.
This script takes command-line arguments and uses those to set up multiple Cassandra instances using specific IP addresses.
Determining natural endpoints, datacenter, and rack for a given key.
When troubleshooting, it is valuable to know which rack and datacenter your snitch believes a node belongs to.
Also, knowing which machines would store a specific key is important when troubleshooting specific failures or determining how your strategy is spreading data across the cluster.
This recipe shows how to use JConsole to find this information.
Enter the IP address of a node to find rack information in the text box next to the button.
Supply the IP address of a node in the text box next to the button and then click OK.
Manually specifying Rack and Datacenter configuration with a property file snitch in this chapter.
Manually specifying Rack and Datacenter configuration with a property file snitch.
The job of the Snitch is to determine which nodes are in the same Rack or Datacenter.
The property file snitch allows the administrator to define a property file that Cassandra uses to determine what datacenter and rack nodes are a part of.
This recipe shows how to configure the property file snitch.
We will be using the same theoretical network for this recipe.
Replicate this file to all hosts in your cluster and restart the Cassandra process.
The property file snitch reads this information on startup and uses it to route requests.
This optimization attempts to handle digest reads to a host on the same switch or datacenter.
See the previous recipe, Determining natural endpoints, datacenter, and rack for a given key to see how to test if you have performed this setup correctly.
The Dynamic Snitch is a special snitch that wraps another snitch such as the PropertyFileSnitch.
Internally, Cassandra measures latency of read traffic on each host and attempts to avoid sending requests to nodes that are performing slowly.
This recipe shows how to use JConsole to find and display the scores that the snitch has recorded.
Getting ready The recipe in Chapter 1, Connecting to Cassandra with JConsole shows you how to connect.
An Mbean with a randomly chosen number will be below, which you need to expand again.
Click on the attributes and the Scores information will appear in the right panel.
When Cassandra nodes are under CPU or IO load due to a heavy number of requests, compaction, or an external factor such as a degraded RAID volume, that node should have a higher score.
With dynamic snitch enabled nodes coordinating read operations will send fewer requests to slow servers.
Most applications use Cassandra for its capability to perform low-latency read and write operations.
When a cluster is all located in a single physical location, the network latency is low and bandwidth does not (typically) have a cost.
However, when a cluster is spread across different geographical locations, latency and bandwidth costs are factors that need to be considered.
See the recipe Scripting a multiple datacenter installation for information on setting up that environment.
READ.EACH_QUORUM returns the record with the most recent timestamp once a majority of replicas within each datacenter have replied.
For example, reading at QUORUM in a multi-datacenter configuration would have to wait for a quorum of nodes across several datacenters to respond before returning a result to the client.
Since requests across a WAN link could have high latency (40ms and higher), this might not be acceptable for an application that returns results to the clients quickly.
Those clients can use LOCAL_QUORUM for a stronger read then ONE while not causing excess delay.
The same can be said for write operations at LOCAL_QUORUM, although it is important to point out that writes are generally faster than reads.
It is also important to note how these modes react in the face of network failures.
EACH_ QUORUM will only succeed if each datacenter is reachable and QUORUM can be established in each.
LOCAL_QUORUM can continue serving requests even with the complete failure of a datacenter.
Internet communication over long distances has inherent latency due to the speed of light; however, some areas of the world have more robust network links and better peering.
A common tool used to check for network latency is traceroute.
Use traceroute to test the path to an Internet-based host.
Traceroute tracks the route packets taken from an IP network on their way to a given host.
It utilizes the IP protocol's time to live (TTL) field and attempts to elicit an ICMP TIME_EXCEEDED response from each gateway along the path to the host.
By analyzing the response time of each device, hops that are taking a long time can be identified as the source of problems.
Depending on where the slowness occurs, you can take appropriate action.
That may be contacting your network administrator or your ISP.
For clusters with a few nodes, it is generally advisable to place these nodes on a single switch for simplicity.
Multiple rack deployments are suggested when the number of nodes is more than the ports on a typical switch or for redundancy.
Thus, when nodes are divided across switches, ensure that the links between switches DO not become choke points.
Monitor the traffic on all interfaces, especially on the uplink interfaces between switches using a NMS such as mrtg or cacti.
Know the maximum capacity of your network gear and plan for growth.
If the uplinks between switches are network contention points, there are several solutions.
One option is to spread your nodes out over more switches.
Enterprise-level switches often support Link Aggregation Groups (LAG), which bundle multiple interfaces together in an active/active fashion to make a single logical interface that is as fast as the sum of the links aggregated.
Cassandra servers in a cluster have a maximum timeout that they will use when communicating with each other.
This is different than the socket timeout used for Thrift clients talking to Cassandra.
Operations will have more latency with nodes communicating across large distances.This recipe shows how to adjust this timeout.
When clusters are spread out over large geographical distances, intermittent outages have a greater chance of making requests exceed the timeout value.
Remember, if a client is using a consistency level such as ONE, they may receive a result quickly, but the cluster may still be working to write that data to replicas in other datacenters.
Raising the timeout gives the cluster more time to complete the process in the background.
Changing consistency level from the CLI to test various consistency levels with multiple datacenter deployments.
By default, the command-line interface reads and writes data at consistency level ONE.
When using Cassandra with multiple node and multiple datacenter environments, being able to execute operations at other consistency levels is essential to testing and troubleshooting problems.
This recipe shows to change the consistency level while using the CLI.
Getting ready This recipe assumes a multiple data setup such as the one created in the recipe Scripting a multiple datacenter installation.
Down all the nodes that are in one of the datacenters.
The consistencylevel statement in the command-line interface changes the level operations run at.
The default level of ONE will succeed as long as a single natural endpoint for the data acknowledges the operation.
For LOCAL_QUORUM, a quorum of nodes in the local datacenters must acknowledge the operation for it to succeed.
With EACH_QUROUM, a quorum of nodes in all datacenters much acknowledge the operation for it to succeed.
If the CLI displays null after a set or get, the operation failed.
In multiple datacenter scenarios, replication factors higher than three are common.
In some of these cases, users want durability of writing to multiple nodes, but do not want to use ONE or QUORUM.
Write consistency levels make the following guarantees before reporting success to the client: ...
Read consistency levels make the following guarantees before returning successful results to the client: ...
Getting ready This recipe requires a multiple datacenter installation as described in the recipe Scripting a multiple datacenter installation.
Create a two-datacenter cluster with two nodes in each datacenter:
Set the consistency level to TWO and then insert and read a row:
Replication factors such as TWO and THREE can be helpful.
An example of this is a two-datacenter deployment with a replication factor of four.
Consistency level TWO would allow the first two acknowledgments, local or remote, to successfully complete the operation.
For absolute control, use PropertyFileSnitch to specify which Cassandra nodes are in which datacenter and rack.
For this example, assume two datacenters each with two nodes.
Calculate the tokens for the nodes in a datacenter as if they were the entire ring.
Now, for the second datacenter, do the exact same process, but no two nodes can have the same token, so offset the tokens by adding 1 to the token value.
Cassandra determines which node will get the write in the remote datacenter the same way it would do for primary insertion.
Cassandra will write to the node whose Initial Token is closest without being larger than the data's token.
When using Network Topology strategy, Cassandra only has nodes in the remote datacenter to choose from when placing the replica, not the entire ring.
More than two datacenters If there are more than two datacenters, follow the same steps but keep incrementing the offset so that no nodes have the same Initial Token.
Follow the recipe of computing the tokens for that datacenter independently, and then check to make sure there are no token collisions on any other node in any datacenter.
If the numbers collide, increment the token on one of those nodes.
Endpoint Snitch Furthermore, using a different, or improperly configured Endpoint Snitch, will not guarantee you even replication.
In this chapter, the recipe Specifying Rack and Datacenter configuration with a property file snitch.
For a reference on how Cassandra uses tokens to select nodes to write to, see the recipe in Chapter 1, Getting Started, Calculating Ideal Initial Tokens for use with Random Partitioner.
Introduction Cassandra has a simple and powerful API and data model.
There are some components that are designed for user extensibility such as custom types and practitioners.
Working with these components requires writing code that builds against the Cassandra code.
Also, like many open source projects, users often become developers by searching for bugs or adding new features.
Installing common development tools Several common tools are used to develop Java applications.
They can be downloaded and installed individually, but installing with the yum tool is faster.
Building Cassandra from source The Cassandra code base is active and typically has multiple branches.
It is a good practice to run official releases, but at times it may be necessary to use a feature or a bug fix that has not yet been released.
Building and running Cassandra from source allows for a greater level of control of the environment.
Having the source code, it is also possible to trace down and understand the context or warning or error messages you may encounter.
This recipe shows how to checkout Cassandra code from Subversion (SVN) and build it.
To build the release tar, move into the folder created and run:
Subversion (SVN) is a revision control system commonly used to manage software projects.
This recipe is using the command-line client to checkout code from the repository.
In this chapter, the recipe Generating a diff using subversion's diff feature.
Cassandra stores columns as byte arrays and does not enforce any restrictions on them by default.
This design principal allows users to quickly serialize and store data, but there are some drawbacks to this approach.
For the high-level user using the CLI, raw byte arrays display as hex strings.
Those working on the backend may be used to a storage system providing data integrity checks such as length or type of data.
This recipe shows how to write a custom type in Cassandra by extending AbstractType.
Also, define a compare method, which is used to sort entries:
By creating types, users can control how columns are sorted by writing custom implementations of the compare method.
Display is controlled by providing a custom implementation of getString.
This type removes the pipe characters used as delimiters for a display that is easier to read.
The next recipe, Using the validation to check data on insertion.
A default validation class can be specified for a column family.
The validation class can be overridden for columns of specific names.
This recipe shows how to create a sub class of AbstractType and use this as a validation class.
Getting ready This recipe is an enhancement on the previous recipe, Creating your own type by sub classing abstract type.
Build the project and copy the hpcas.jar to the lib directory of Cassandra.
If a validation class is enabled, during an insert the data being inserted is passed to the validate method.
If the data is invalid, the method should throw a MarshalException and supply information as to why the validation failed.
The ability to validate data on insertion helps incorrect data from being inserted.
This technique can be used to enforce restrictions on the length or content of data.
Communicating with the Cassandra developers and users through IRC and e-mail.
E-mail and IRC allows people across the world to collaborate.
Amazingly, most of the project coordination is done over these mediums.
This makes Cassandra more than just code with an open source license; anyone can become involved and become part of the project.
There are several mailing lists; send an e-mail to the specified addresses and a reply e-mail will be sent to you with further instructions on joining the list.
High-level clients such as Thrift, Hector, or clients for other languages.
Always attempt to search for the answer first before asking a question on a mailing list or IRC.
Remember, most people on the lists are enthusiasts and are volunteering their time to help.
For those interested in developing or fixing features in Cassandra, it is common to join the dev mailing list and chat room.
Typically, the key committers have insights into the problem and the code base and help code get committed faster.
Any changes made to the source code after checkout will be tracked.
A diff is a file that stores the changes made.
This recipe shows how to use subversion to create diff files.
Generate a diff, which captures the changes you have made to the source files:
Subversion with diff capabilities provide a way to track changes and share code with others.
Lines that start with a minus sign ( - ) indicate a line has been removed.
Lines that start with a plus sign (+) indicate a line has been added.
Lines above the change are also stored in the file.
This can help resolve issues when the patch offsets do not match exactly due to other changes since the diff had been generated.
The next recipe, Applying a diff using the patch command shows how to apply a diff to your copy of the code.
Applying a diff using the patch command A diff file represents the comparison of a file before and after a change.
Unreleased source code updates or patches typically come in the form of diff files.
This recipe shows how to use the patch command with a diff file to apply changes to a branch of Cassandra source code.
Ensure the software being patched is the exact same revision of the software the patch was based on.
Patches may apply incorrectly if it is not applied to the correct source.
The output of the previous recipe, Generating a diff using subversion's diff feature can be applied with this recipe.
If the patch did not apply correctly, svn revert will restore the contents of files back to the repository state.
Run the patch command and use the shell's input redirection to feed it the contents of the patch file:
If any messages are displayed such as 'skipping hunk', this means that the diff file is not applying cleanly.
The patch command takes the content of a diff file and applies those changes to the local code.
All the updates to the Cassandra code base are always done by generating diff files.
Using strings and od to quickly search through data files.
A user may wish to review Cassandra data files directly.
This is a fast alternative to searching through data with range scanning using the API or exporting the data to JSON format.
Because the Cassandra data files are written in a binary format, using standard text editing tools can be difficult.
Two command-line tools that are helpful in this process are the command-line octal dump utility, od, and the strings utility, which displays human readable strings inside binary files.
Run strings against a file in your data directory that matches the pattern '*Data*':
Command-line tools such as od and strings are a fast way to extract data when troubleshooting, while tools such as SSTable2JSON have the startup overhead involved in sampling the index files.
Od and strings can be combined with other command-line tools such as pipelines or grep.
Command-line utilities to hex or octal dump still have shortcomings in their ability to decode data as they require a lower-level understanding of data files.
The format of these files is also subject to change.
This recipe shows how to customize the sstable2json program to output data in a user-defined format.
This recipe is only applicable when your data is ASCII or UTF-8
The default export program outputs data as Hex encoded strings.
This recipe uses the helper functions in the ByteBufferUtil class to convert byte data into strings.
This assumption is not a good valid if the data stored is binary data.
This application could be easily customized to produce XML, an SQL file to be bulk loaded, or a batch of inserts to replicate the data to another Cassandra cluster.
The index_interval controls the sampling of row keys for each SSTable.
Index sampling happens during node startup and that data stays in memory until the SSTable is removed.
The memory used by index_interval is independent of the key cache.
Raising the index_interval uses less memory, but makes the index less effective.
A common use of this feature is systems that store a large amount of data but do not have much RAM memory.
This knob is also useful because different use cases have different key size, key count, and key-to-row ratio.
Raising the ratio can also help a node start up faster.
The failure detector monitors gossip traffic, and if a node has not participated in the process for an interval, it marks the node as dead.
Cassandra must be restarted for this change to take effect.
This setting should be changed when networks are unstable or with virtual machines that sometimes have resources stolen by other instances on the same hardware.
Using the Cassandra maven plugin With maven, it is simple to create a software project that has Cassandra support built in.
The Cassandra maven plugin fetches all the dependencies and provides goals for starting and stopping Cassandra.
This is an easy way to create self-contained projects that work with Cassandra.
This recipe shows how to use the Cassandra maven plugin.
Getting ready The recipe in this chapter, Installing common development tools is a pre-requisite.
A pom file contains information on the project, including dependencies and plugin configuration information.
Maven repositories across the Internet store project JARs and information on their dependencies.
The Cassandra plugin for maven provides goals to start and stop Cassandra without having to explicitly bring Cassandra-related libraries into the project.
This makes it easy to prototype and distribute an application using Cassandra.
More information on the Cassandra maven plugin can be found at http://mojo.codehaus.org/cassandra-maven-plugin/
Introduction Cassandra's popularity has led to several pieces of software that have developed around it.
Some of these are libraries and utilities that make working with Cassandra easier.
Other software applications have been built completely around Cassandra to take advantage of its scalability.
Stress is an easy-to-use command-line tool for stress testing and benchmarking Cassandra.
It can be used to generate a large quantity of requests in short periods of time, and it can also be used to generate a large amount of data to test performance with.
This recipe shows how to build it from the Cassandra source.
Getting ready Before running this recipe, complete the Building Cassandra from source recipe discussed in Chapter 9, Coding and Internals.
Then, change to the contrib/stress directory and run ant again.
The next recipe, Inserting and reading data with the stress tool.
The stress tool is a multithreaded load tester specifically for Cassandra.
It is a command-line program with a variety of knobs that control its operation.
See the previous recipe, Building the contrib stress tool for benchmarking before doing this recipe.
The stress tool is an easy way to do load testing against a cluster.
It can insert or read data and report on the performance of those operations.
This is also useful in staging environments where significant volumes of disk data are needed to test at scale.
Generating data is also useful to practice administration techniques such as joining new nodes to a cluster.
It is best to run the load testing tool on a different node than on the system being tested and remove anything else that causes other unnecessary contention.
The next recipe, Running the Yahoo! Cloud Serving Benchmark for a more sophisticated load testing system.
The Yahoo! Cloud Serving Benchmark (YCSB) provides benchmarking for the bases of comparison between NoSQL systems.
It works by generating random workloads with varying portions of insert, get, delete, and other operations.
This recipe shows how to build and run the YCSB.
Use the Cassandra CLI to create the required keyspace and column family.
Run the script ant pipe the output to more command to control pagination:
An important configuration option is -P, which chooses the workload.
The workload describes the portion of read, write, and update percentage.
Cassandra has historically been one of the strongest performers in the YCSB.
Hector, a high-level client for Cassandra It is suggested that when available, clients should use a higher level API.
Hector is one of the most actively developed higher level clients.
It works as a facade over the Thrift API, and in many cases condenses what is a large section of Thrift code into a shorter version using Hector's helper methods and design patterns.
This recipe shows how to use Hector to communicate with Cassandra.
Download the Hector JAR and place it in your applications classpath.
The role of a Serializer is to take the encoding burden away from the user.
This is an alternative to working with the Column in a JavaBean-like style.
One way to read data is by using a ColumnQuery object.
ColumnQuery uses a builder pattern where set operations return an instance to the ColumnQuery object instead of void.
Firstly, remember that the bindings generated by Thrift are cross-platform and designed for compatibility.
Higher level clients such as Hector bring more abstraction and take more advantage of language features such as Java's generics.
For example, the HFactory class provides methods that reduce four lines of Thrift code to a single line factory method call.
Hector also provides client-side load balancing because detecting and automatically failing-over between servers is important to achieve good uptime.
The next recipe, Doing batch mutates with Hector shows how Hector's API design makes operations such as batch mutate easier.
Doing batch mutations with Hector In an earlier chapter, we showed how batch mutations are much more efficient than doing individual inserts.
However, the long complex method signature of the batch_mutate method is difficult to read and assembling that structure may clutter code.
This recipe shows how to use the Hector API for the batch mutate operation.
Create a mutator as you would for a single insert and make multiple calls to the addInsertion method.
Hector's mutator concept is more straightforward than the elaborate nested object needed to execute a batch mutation through Thrift.
Writing less lines of code to carry out a task is better in numerous ways as there is less code to review and less chance to make a mistake.
Data in memory being used by an application is typically in a different format than it's on-disk representation.
Serialization and deserialization take data from an in-memory form and persist it to a back-end data store.
This work can be done by hand.The Java Persistence Architecture (JPA) allows you to annotate a Java object and use JPA to handle the serialization and de serialization automatically.
This recipe show how to use JPA annotation to persist data to Cassandra.
This recipe requires the mvm command provided by the maven2 package.
Use subversion to download the kundera source code and maven to build it.
All annotated fields of the object are automatically populated with data from Cassandra.
While this does remove some of the burden, it also takes away some level of control.
Since JPA can provide access to many types of data stores such as relational databases, it also makes it easy to switch between backend storage without having to make large code changes.
Setting up Solandra for full text indexing with a Cassandra backend.
Lucene is a reverse index system designed for full text search.
Solr is a popular frontend that provides a web service for Lucene as well as caching warming and other advanced capabilities.
Solandra integrates with both tools by storing Lucene's data inside Cassandra, allowing for a high level of scalability.
Use git to obtain a copy of the Solandra source code and use ant to build it.
Prepare a temporary directory that Solandra will use to store data.
Then, run these steps to start Solandra, download, and load sample data.
Place text in the search box to search for occurrences of it inside the documents loaded into Solandra.
Solandra takes the data that Solr would normally store on local disk and instead stores it inside Cassandra.
It does this by providing custom implementations of Lucene's IndexReader and IndexWriter and also runs Solr and Cassandra inside the same JVM.
Cages API is used for distributed read and write locks.
This recipe shows how to set up a single instance of Zookeeper to support Cages.
Apache ZooKeeper is an effort to develop and maintain an open source server that enables highly reliable distributed coordination.
Create the dataDir directory you referenced in the preceding configuration.
It is typically installed on one to seven nodes so it is highly available and capable of managing a large number of locks and watches.
Cassandra and Zookeeper are an interesting pairing: Cassandra providing high availability and high performance with Zookeeper providing synchronization.
The next recipe, Using Cages to implement an atomic read and set uses the zookeeper instance setup in this recipe.
In Chapter 5, Consistency, Availability, and Partition Tolerance with Cassandra, the recipe Consistency is not locking or a transaction shows what can happen when multiple applications read and update the same piece of data without synchronization.
In the previous recipe, we set up Apache Zookeeper, a system for distributed synchronization.
The Cages library provides a simple API to synchronize access to rows.
Getting ready Review the recipe Demonstrating how consistency is not a lock or a transaction discussed in Chapter 5, Consistency, Availability, and Partition Tolerance with Cassandra.
To do this recipe, you must complete the previous recipe, Setting up Zookeeper to support Cages for transactional locking.
Use subversion to checkout a copy of the Cages source code and binary JAR.
Copy the cages and zookeeper JARs to the library directory of the build root.
Cages and Zookeeper provide a way for external processes to synchronize.
When each thread is initialized, it opens a Zookeeper session.
The critical section of the code reads, increments, and finally updates a column.
Surround the critical section of the code with a Zookeeper Write Lock that prevents all other threads from updating this value while the current thread operates on it.
Synchronization incurs extra overhead; it should only be used when necessary.
Zookeeper does scale out to several nodes, but it does not scale out indefinitely.
This is because writes to Zookeeper have to be synchronized across all nodes.
Using Groovandra as a CLI alternative Groovy is an agile and dynamic language for the Java Virtual Machine.
Groovandra is a library designed to work with Groovy for rapid exploration of data in Cassandra.
It can be used for tasks the Cassandra CLI cannot do and that coding and deploying a Java application may not make much sense.
Code can be written line by line or in Groovy scripts that do not need to be compiled and packaged before running.
Groovandra is a simple way to interact with Cassandra without having to go through the steps of compiling, deploying, and running Java applications.
Groovy allows users to approach the application line by line.
This allows ad hoc programming and debugging and is helpful for accessing the features of Cassandra that are not accessible from the CLI such as setting up a call to the multiget_slice method, which requires numerous parameters to be set.
Searchable log storage with Logsandra Logsandra is a project based around log storage in Cassandra.
Logsandra is a project that provides a set of tools to parse logs, store them in Cassandra in a searchable fashion, and search for or graph the occurrence of keywords in logs.
The second runs a web server that allows you to search for occurrences of keywords in logs or graph their frequency.
Getting ready Logsandra needs a running instance of Cassandra to connect to and store data.
This recipe also requires Python and the Python installer pip.
Elevate to root to install the requirements and then drop back to a standard user.
Loading sample data for the following keywords: foo, bar, baz.
Logsandra presents a graph with occurrences of this keyword over time.
Logsandra creates and uses a keyspace name logsandra with a column family inside it named keyword.
It primarily retrieves events by looking up all logs containing a keyword from a range of time.To make this efficient, the event timeline is denormalized to produce one timeline per keyword.
For each keyword that appears in a log, a separate copy of the log event will be appended to the corresponding timeline.
Each timeline gets its own row, and within the row, each column holds one log event.
The columns are sorted chronologically, using unique IDs (UUIDs) for column names to avoid clashes.
Although this denormalization strategy uses more space on disk, a lookup query by Logsandra will only read a single contiguous portion of one row in Cassandra, which is very efficient.
Logsandra shows a versatile way to store and access log data in Cassandra.
It is also important to note that Logsandra is written in Python, which demonstrates adoption for Cassandra outside the Java world.
Inside the logsandra/conf directory, the logsandra.yaml file can be used to control which host and port the Logsandra web interface binds to, host and port information to connect to the Cassandra cluster, and directives that instruct it as to which folders to watch for log events.
Introduction The Apache Hadoop project develops open source software for reliable, scalable, and distributed computing.
Hadoop is commonly used to store and process huge data sets.
This allows MapReduce programs in Hadoop to read from and write to Cassandra.
The pairing of Hadoop and Cassandra complement each other because Cassandra excels at low latency reading and writing and Hadoop provides a system to perform data mining and advanced searching.
A pseudo-distributed Hadoop setup A production Hadoop cluster can span from a single node to thousands of computers.
The communication between the components is depicted in the following image:
For Hadoop to be effective at grid computing, it needs to be installed on multiple machines, but the stack can be set up on a single node in a pseudo-distributed cluster.
This recipe shows how to set up a pseudo-distributed cluster.
JobTracker web interface and confirming one node listed in the Nodes column:
Use the hadoop command-line tool to test the file system:
Each Hadoop component uses information in core-site.xml and either mapred-site.
There are hundreds of configuration knobs for Hadoop and many features.
Hadoop can then be used to perform many different types of algorithms on the data.
This recipe shows how to use a map-only job to locate any key with a specific column and convert the value of the column to uppercase.
Entry point programs for Hadoop typically extend Configured and implement Tool.
This means that processing large column families can take a long time and be very intensive.
The result of this planning is a list of splits.
The larger the cluster, the more splits that can be processed in parallel.
Cassandra also implements a Hadoop OutputFormat allowing the results of MapReduce jobs to be written data directly to Cassandra.
This recipe shows how to read data from Cassandra, update it, and then write it back using MapReduce.
The OutputFormat uses Avro and more JARs are required to run the job.
Add the following files to the -libjar list and run the program again.
Confirm that the values of the column are in uppercase:
The OutputFormat receives data from MapReduce and writes the data to Cassandra.
Having support for both InputFormat and OutputFormat allows users to mix and match how they approach problems.
This job reads from Cassandra, processes using Hadoop, and then writes the data back to Cassandra.
However, users can read data from Hadoop and write to Cassandra or vice-versa.
Using MapReduce to do grouping and counting with Cassandra input and output.
Many types of grid computing systems can divide a problem into smaller sub-problems and distribute this across many nodes.
MapReduce has a map phase, a shuffle sort that uses a Partitioner to guarantee that identical keys go to the same reducer, and finally a reduce phase.
This recipe shows a word_count application in the Cassandra contrib.
Grouping and counting is a problem ideal for MapReduce to solve.
The mapper takes a column and breaks it into tokens (individual words) using StringTokenizer, a class that splits strings on common tokens such as spaces and columns.
Equal keys are guaranteed to be processed by the same reducer.
This reducer counts how many times a given key occurs.
This application counts the number of times words appear in text.
However, much of this code can be used to count hits to a website, or build a reverse index storing the positions of words in a body of text.
Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, ad hoc querying, and analysis of large data sets stored in Hadoop files.
It provides a mechanism to put structure on this data and provides a simple query language called Hive QL, which is based on SQL and which enables users familiar with SQL to query this data.
Hive has an API for Storage Handlers that allows data in other systems outside of HDFS to be used as input or output.
This recipe shows how to set up Hive with the Cassandra Storage Handler.
Getting ready You will need to have a working Hadoop deployment.
Refer to the recipe A pseudo-distributed Hadoop setup in this chapter for more information.
The Cassandra Storage Handler has not yet been integrated into a Hive release.
Copy all the JAR files from your Cassandra distribution to auxlib.
Remove libraries that are not needed and may conflict with Hive versions:
The HADOOP_HOME environment variable needs to be exported so Hive can use it to find required libraries as well as site-specific Hadoop settings such as the hostname and port of the NameNode and JobTracker.
Hive is designed on the concept that rows in the same table have a fixed number of columns, while a Cassandra row has multiple key value pairs associated with it.
You can think of the process of overlaying Hive on Cassandra as turning a map into a fixed array based only on certain key value pairs in the map.
For this recipe, columns such as favorite_book will not be accessible.
Additionally, if a row does not have a required column such as favorite_movie, a null value will be returned.
Tables that use the Cassandra Storage Handler must be external.
External tables are not physically removed when they are dropped from Hive:
After the table is set up, run a simple query that should select all rows and then order the results by the key:
The parameters specified when the table is created are stored as the definition of the Hive table.
Hive uses multiple mappers to fetch all the contents of the column family in parallel.
The result of the map phase is then passed into a single reducer; the outputs of a reducer are naturally sorted.
Joining two Column Families with Hive Joining two data sets on a value is a common operation.
This operation is typically done in SQL databases where data is in a normalized form.
Data in Cassandra is typically stored in a de-normalized form; however, there are many cases where a user wishes to join columns from two-column families together on a key.
Create entries in two column families that have the same row key:
Issue a JOIN clause to combine both data sets on the row key:
After this data is read, it is no different than any other data from Hive.
It can be joined with another Cassandra table as is done in this example, or it can be joined with a table of HDFS data.
Equality joins are efficient in MapReduce as data produced from the map phase is naturally moved to a reducer based on the key.
Once a schema is defined inside Hive, many different ad hoc queries can be run against it.
Based on the submitted query, Hive generates a plan that may be one or more MapReduce jobs.
This recipe shows how to group and count on the values of a specified column using Hive.
Insert some entries ensuring that the 'favorite_movie' column is populated:
Create an HQL query that will count values of the favorite_movie column and then order the counts in ascending order:
Comparing this recipe with the recipe A MapReduce program that uses grouping with Cassandra input and output demonstrates the benefit of the code generation capability of Hive.
Coding, compiling, testing, and deploying a MapReduce program can sometimes be replaced by a two-line Hive query!
In this chapter, the recipe A MapReduce program that uses grouping with Cassandra input and output.
When multiple applications are run on the same hardware, they affect the performance of each other by competing for the same resources.
MapReduce jobs commonly run from several minutes to hours while processing gigabytes of data.
Cassandra requests are low latency operations on small amounts of data.
One of the important concepts leveraged by Hadoop is that moving data is more intensive than moving processing.
When a Hadoop job is divided into tasks, the scheduler attempts to run the task on a node where the data is.
This recipe shows how to achieve data locality when using Hadoop and Cassandra, as well as configuration suggestions so they run on the same hardware while isolating them from each other.
If your system has multiple disks, consider isolating the TaskTracker to its own disk(s)
They work with large amounts of data, and during operations repeatedly spill data to disk.
Both Cassandra and Hadoop will also compete for the CPU of the system.
For example, if a system has eight CPU cores, you may only want to dedicate four of them to the TaskTracker, leaving the remaining four for Cassandra.
The next recipe, Setting up a "Shadow" data center for running only MapReduce Jobs shows how to use Cassandra's built-in replication to partition a cluster with nodes dedicated for ETL-type workloads and others dedicated for serving low-latency requests.
Setting up a "Shadow" data center for running only MapReduce jobs.
MapReduce and other Extract Translate Load (ETL) processing can be intensive, which can interfere with the ability of Cassandra to serve other requests promptly.
This recipe shows how to set up a second Cassandra data center for ETL, as depicted in the following image:
Getting ready Review the chapter on Multi datacenter deployments for recipes on multi-data centers setups.
This division of resources allows ETL-type processes to run without having an impact on the nodes that serve requests to application servers.
The hardware in the Shadow data center does not have to be the same class of hardware used for request serving.
For example, the primary data center might have ten servers with fasts SCSI disks and large amounts of RAM to handle user requests.
However, in your shadow data center, three servers with large SATA drives and less RAM may be sufficient for MapReduce or ETL workloads.
Brisk contains versions of Cassandra, Hadoop, and Hive that are combined into a single package.
This makes it fast and easy to deploy and manage these components as one entity.
How it works Brisk removes the need to run separate NameNode, SecondaryNameNode, and DataNodes components by storing data directly in Cassandra File System (CFS), which works as a drop-drop-in replacement for HDFS.
The Hive metadata, typically stored in a relational datastore, is also stored directly in Cassandra.
Brisk is ideal for those wishing to use Cassandra and who do not wish to manage Hadoop components separately.
Cassandra offers built-in support for performance counters that provide in-depth information into how the system is doing.
Recording the information from these counters is an invaluable asset when troubleshooting and capacity planning.
Cassandra provides access to this information through standard JMX MBeans (Java Management eXtension Managed Bean)
MBeans make it possible for a variety of applications to collect, report, and alert on this information.
This chapter looks in depth at techniques for both standard system monitoring and Cassandra-specific monitoring.
It shows which information is important to capture and how to analyze this information.
Rather than spawning a thread per request, the requests are transferred between queues of bounded size called thread pools.
If thread pools are filled, requests get backlogged and clients will begin experiencing delays or exceptions.
A good first step in diagnosing a performance problem is running tpstats (thread pool stats) and determining if any stage is backlogged.
Use nodetool tpstats to connect to the JMX port of the server you would like to gather statistics on.
Any stage that has a non-zero number in the Active or Pending column is backlogged.
A healthy system shows near zero at most times in all stages for both the Active and Pending states.
Some stages such as the AntiEntropyStage are entered by administrative actions such as running nodetool repair.
The FlushStage happens periodically when Memtables hit their thresholds and flush to disk.
Sustained non-zero values for active and pending inside this stage are rare.
Since flushing to disk is a serial operation, this would indicate an extremely overburdened disk subsystem.
Because the write stage is highly optimized, backlog in this stage would indicate either an extreme volume of write activity and that the disk with the commit log cannot sustain the write traffic.
Each column family has a number of performance counters that provide in-depth diagnostics.
The cfstats (column family statistics) option shows a high level summary of the column family information.
Some values in the cfstats output represent values that are a current size such as SSTable count.
Other fields such as Write Count represent counters that need to be sampled over time to determine a rate.
Averages of specific variables such as Read Count are used to summarize the activity of the keyspace.
Further information on many of these fields is described in other recipes across this chapter.
Monitoring CPU utilization The CPU activity is one of the most important factors of performance.
This recipe shows the CPU graph and describes how to interpret the following CPU graph:
The following table lists descriptions of the major CPU states:
State Description user Shows the percentage of CPU utilization that occurred while executing at the user level.
IOWait Shows the  percentage of time that the CPU or CPUs were idle and the system did not.
Sites that use Cassandra in a caching role have small data sets compared to RAM size.
Under load, they will typically see the User and System states increase and become a bottleneck.
For sites that use Cassandra to store significantly more data than RAM, IOWait will usually become the limiting factor.
The Java garbage collection process utilizes multiple threads to sweep through memory often and avoids pauses.
Generally, if a system is spending time in Idle state there is spare capacity.
However, the IOWait state, wa when using the top command, indicates the system is waiting on disk or network IO.
Because the system is waiting on those resources, it can not fully utilize the CPU.
Some NMS systems provide CPU graphs that do not record IOWait and other system states.
High IOWait will prevent you from fully utilizing your CPU.
That reason could be that 300 percent of the CPU time is spent waiting for IO!
High IOWait usually means that your disk subsystem is overworked.
Review the Monitoring disk utilization and having a performance baseline recipe to learn how to monitor hard disk activity.
Each column family tracks the read and write requests to it.
This recipe describes the CFStores Read/Write graph and the information it provides, which is depicted as follows:
In this graph, the ReadCount is represented as an area in blue and the WriteCount is an area in green stacked above it.
The more requests on a given column family, the more system resources are being used.
Tracking, trending, and having a baseline for your read and write activity is critical.
For example, a software release could accidentally triple read operations, and without knowing the normal rate, detecting the issue would be difficult.
Knowing which column families are the most active is important when deciding where to allocate larger caches.
The hinted handoff stores records that were destined to downed nodes.
Currently, hinted handoff reads from the source system to find the data that needs to be replayed to the other nodes.
This causes read traffic to temporarily increase on the nodes storing hints after a failed node comes back online.
Using Memtable graphs to profile when and why they flush.
Memtables are in-memory sorted structures that data is written to.
The Memtables flush when their time, size, or activity thresholds are triggered.
This recipe describes how to interpret activity from the following Memtable graph:
In this graph, the orange area represents MemtableDataSize, which is the size of the Memtable including all keys and columns.
The size of the table grows until a flush to disk is triggered and the size and column count are reset to zero.
If a column is written twice to a Memtable, it will be overridden.
If Memtable settings are set higher, Memtables will flush less.
This increases the chances that data will not be written to disk multiple times.
Flushing often creates multiple SSTables, which may in turn trigger more compaction.
If this disk is flushing and compacting often, it has less resources to spend responding to user's requests.
The recipe Monitoring compaction by graphing its activity in this chapter.
Graphing SSTable count Cassandra's SSTables are written once and never modified.
For an active column family with frequent writes and/or deletes, new SSTables are created often.
The compaction manager has thresholds that are triggered and combine multiple SSTables into one.
This recipe shows how to interpret the data from the SSTable graph, which is shown as follows:
Each read may have to check for data in all the SSTables in a column family.
SSTables that have to be part of a snapshot may still exist on disk, but are not counted by this graph.
The compaction thresholds try to ensure that the SSTable count stays low.
It may grow temporarily during a large compaction, or possibly if compaction is disabled for a bulk load.
However, if SSTable count begins to grow, it may be time to tune Memtable or compaction settings, or get more hardware.
For deployments where the data on disk is larger than the amount of RAM on system, disk performance becomes a larger factor in performance.
This recipe shows how to monitor the activity of a hard disk, as depicted in the following graph:
The area of the graph in blue is bytesRead that represents the bytes read from disk per second.
The area stacked on top of bytesRead in green is bytesWritten, the bytes written to disk per second.
Disks on platters are capable of reading and writing fast serially.
The majority of use cases exhibit a random read pattern.
Random reads cause the hard disk to spend more time seeking than reading and thus limit throughput.
Solid State Drives (SSD) offer an interesting solution to random read challenge.
A solid state drive has no moving parts and never has to seek access to any data location.
This technology is new and more expensive than standard spinning disks.
See the recipes Using a dedicated commit log disk, Choosing a high performing RAID Level, and File system optimization for hard disk performance in Chapter 4, Performance Tuning for information on tuning physical disks.
Towards the bottom of the graph, the Cache Hit Rate represents the number of Requests divided by the number of Cache Hits.
The higher the Cache Hit Rate, the more effective the cache.
Caching can make a drastic difference in performance if the situation is right and the caches are employed correctly.
Active set can be considered as the portion of your data that is in use at a given time.
At any given time, a small portion of those users may be active, such as five percent.
If you employ correctly sized caches, a small amount of memory can effectively cache the five percent of active users, making the service responsive for them.
Cache tuning involves making caches large enough to achieve a high hit rate.
The goal is to keep as much of the active set in memory as possible as this lowers disk activity.
The law of diminishing returns may apply to cache sizes.
It may not make sense to double the memory in that case to achieve only a two percent higher rate.
Compaction is a necessary process in the life cycle of the data in Cassandra's structured log format.
Compaction removes old data and optimizes the data on disk.
Rows marked for deletion with tombstones are candidates to be removed entirely.
Due to the way compaction counters are kept, it is possible that a graph sampling at five-minute intervals could miss a compaction event.
The blue area renders ByteCompacted, which is the current progress.
When the blue area meets with the red line, the compaction is done.
An absence of graph elements indicate no compaction is happening at that time.
After all, compaction removes old data and optimizes the data on disk.
However, if systems are in a compaction state often, user requests will have more latency.
In most cases, it is desirable for compactions to finish as quickly as possible, and short quick spikes in the compaction graph show just that.
A health system should be able to compact many gigabytes of data in a short time period.
If a system is beginning to become overloaded, its compaction process could become long and drawn out.
Adjusting your Memtable settings so that they flush less should cause less compaction.
It is also possible to manually run major compaction at specified times.
This lowers the chance a larger compaction will automatically trigger during peak request load.
In Chapter 4, Performance Tuning the recipe Setting compaction thresholds shows how to change the criteria that cause compaction.
The next recipe, Using nodetool compaction stats to check the progress of compaction for a command-line alternative to this graph.
Using nodetool compaction stats to check the progress of compaction.
They can compact automatically when the compaction thresholds are reached.
A major compaction is a compaction of all the SSTables for a column family that is triggered by the user.
Joining and leaving nodes trigger anti compactions, as does anti entropy repairs.
This recipe shows how to check and monitor compaction using nodetool.
The nodetool compactionstats command allows you to quickly see if a compaction is in progress.
This command is a quick way to determine if a node is compacting and monitoring how close it is to finishing.
Use this to determine if a compaction is the reason for performance degradation or to see how a node join is progressing.
Rows can have between a single column and up to two billion columns inside them.
During compaction, information is collected about the rows that were compacted.
This recipe shows how to interpret this graph and the implications of row size in Cassandra, which is depicted as follows:
The red, green, and blue lines represent max, mean, and min row compacted size.
Using the row cache creates memory pressure when rows are very large.
Remember that all the columns of a row must be cached when using the row cache.
Latency is an important factor when serving data to clients.
Cassandra tracks latency, which does not include the network latency; it counts time the request is received to the time it is found or inserted to disk.
When this value is divided by the read count, the result is average read latency per request.
Because the write path of Cassandra involves writing to a sorted-in memory table and serially to a disk, write operations typically have very low latency and are constant.
Latency is a function of many things: data size, disk search speed, load from other requests, and caching.
This means that smaller column families will search faster than larger ones.
Disks capable of more Revolutions Per Minute, such as SCSI, will search faster than SATA disks.
More simultaneous requests will cause more contention and more latency.
Caching in the form of Cassandra's built-in key cache and row cache as well as VFS cache (system RAM) reduce latency by serving some or possibly all of the data from memory.
Tracking the physical disk size of each column family over time.
It is common to graph your system's total disk usage.
Each column family has its own statistics that record disk size.
This recipe shows how to interpret data from the Column Family Store graph, as depicted in the following graph:
The blue area, LiveDiskSpaceUsed, represents storage being used by Data, Index, and Bloom Filter files.
Column Family size is also important to the performance of read and write operations.
Thus, it is important to be able to correlate column family size with other information such as latency and request rate.
Using nodetool cfhistograms to see the distribution of query latencies.
Nodetool provides the cfhistograms command to display the latency information of requests.
This is helpful for determining the performance of requests without having to record information in an external NMS.
Histograms are useful for seeing the distribution of request times.
This is helpful in cases where knowing the average request time is not enough.
For example, a request hitting the row cache may have low latency where requests not cached may be considerably slower.
In this chapter, the recipe Using latency graphs to profile time to seek keys shows a way to visualize latency information such as the cfhistogram stats.
Tracking open networking connections Cassandra is a client server application.
In the operating system, each open socket requires CPU and memory to manage it.
Inside the virtual machine each thread uses resources as well.
This recipe shows how to interpret what the following TCP connection graphs are showing.
In the Current Established graph, the tcpCurrEstab area shows how many sockets are open across the system.
Use the information to monitor how many listening sockets are open on the system.
Numerous counters are available that can be used to get performance information and troubleshoot the TCP stack.
This information is important to Cassandra because of the high request rate and amount of internode communication.
Introduction Getting Cassandra running at maximum efficiency involves understanding how it is utilizing the hardware and the operating system.
It also requires understanding the inner workings of Cassandra to know when things are working non-optimally.
This chapter focuses on applying conventional and Cassandra-specific monitoring techniques.
Forwarding Log4j logs to a central sever The faster a problem can be diagnosed and corrected, the better.
In environments with only a few systems, connection to the server over SSH and using command-line tools to examine logfiles is usually sufficient.
Since a Cassandra cluster can range from one to a few hundred nodes, a better way to aggregate and review logs is needed.
This recipe shows how to configure Cassandra’s logging mechanism, Log4J, to send events to its local logfile as well as a remote syslog server.
Getting ready Syslog is a simple text-based protocol designed to transfer log messages over UDP.
Modern Linux distributions have a syslog server installed by default.
Designate a system as a syslog server and prepare it to accept remote messages.
Check to make sure the UDP port 514 is listening.
Log4j is a versatile logging framework that is used by numerous projects.
Log4j uses Java property files for configuration and has many options to control how logs are formatted and how large they can get before a new one is created.
Log4j also has a number of builtin appenders such as the SyslogAppender used in this recipe.
The SyslogAppender transmits messages using Syslog protocol to a remove logging host.
By aggregating logs from multiple Cassandra servers to a single host, events from multiple servers can be correlated when troubleshooting.
Syslog is a simple protocol for sending text-based log messages over UDP.
There are more advanced syslog servers that have more features such as syslog-ng.
Using top to understand overall performance top gathers a variety of performance information from across the system.
It uses this information and updates the console display on an interval.
This information combined with knowledge of Cassandra’s inner workings is invaluable in understanding how to optimize your deployment.
This recipe shows how to use top to determine how Cassandra is operating.
The section demonstrates top output on two separate server class machines with the same RAM, disk, and CPU running Cassandra.
For the purpose of this example, the second system either has more data or is seeing more requests.
This was run against a server with moderate load: $ top.
However, it is generally described as the number of active processes.
Load averages below one indicate there is ample spare processing.
Low user (us) and low wait (wa) indicate that the system is not CPU or disk-bound.
This should mean that your node can handle more requests than it currently is handling.
Cache is not removed until other programs need memory or other items are added to the cache.
Any disk information in cache can be read from memory instead of the hard disk.
The following is the output against a server with heavy load: $top -H.
In the thread mode, top displays thread counts instead of process counts.
While the user state (us) looks low, the wait is 12 percent.
Processes in the D state are in an unimplementable sleep state.
These are likely to be threads waiting for data from the disk.
Top gathers information from numerous sources and displays it in a display that refreshes every few seconds.
Top is one of the best ways to determine what resources are being used and by which processes.
For terminals that are capable of displaying color and have other advanced features, the htop command can be used in combination with top.
The iostat command uses counter information from the /proc file system to calculate system utilization.
For those using Cassandra for data sets significantly larger them main memory, disk performance is a major factor in the read and write throughput of Cassandra.
This recipe shows how to use the iostat command to examine disk performance.
If the package is not installed, install it with yum (or similar package fetch tools)
Run iostat at five-second intervals for three intervals; use awk to limit the columns displayed:
The first output from iostat is the sum of information since system startup.
After that, the statistics are calculated by the interval specified (five seconds)
The columns rsec and wsec show blocks read and written per second respectively.
As the disk utilization becomes closer to 100 percent servicing, read and write requests will begin to take longer.
Short bursts of high disk activity are normal during compaction.
However, systems with high utilization may not have enough disk IO.
Solutions to high disk utilization may include dedicating more memory to caching.
It also may be an indication that new nodes need to be added to the cluster, or that the current cluster nodes need a high performing disk subsystem.
The next recipe, Using sar to review performance over time.
Using sar to review performance over time Most sites have traffic patterns that vary throughout the day and week.
The system activity data collector, sadc, collects and stores performance information over time.
This recipe shows how to use sar to understand Cassandra system utilization over time.
Getting ready The data collection may not be enabled by default.
The information provided by sar can be used in several ways.
First, it can help determine if CPU or IOWait is high during times of the day.
The best time to schedule intensive operations such as joining new nodes, scheduling major compaction, or running anti-entropy repair is during lulls so they have the lowest impact.
This recipe shows how to use JMXTerm to connect to a JMX application and retrieve statistics from it.
Get information on the bean with the info command: $>info.
Because JMXTerm can be run on a machine without a windowing subsystem, it can typically be run from the machine Cassandra is running on or another node on the same local network.
It provides a way to access all the same JMX attributes and call JMX operations that can be accessed with JConsole.
Monitoring the garbage collection events In Java programs do not explicitly deallocate or free objects from memory.
Garbage collection is a background process that navigates the objects in memory to determine which are no longer reachable.
If the object creation rate exceeds the object delete rate, the JVM might have a pause, often called stop-the-world.
This recipe shows how to watch logs for garbage collection events.
Use grep to look for the string “GC inspection” inside the Cassandra log.
When the JVM garbage collector returns from a pause, it logs a message containing the information on how long the garbage collection took as well as how much memory was freed during the collection.
If these events happen often, this indicates that the system may be overworked.
A solution for avoiding pauses is assigning Cassandra more heap memory.
However, garbage collections may be caused by inadequate Memtable, cache, or other settings.
Using tpstats to find bottlenecks Cassandra is written using a SEDA architecture.
This architecture is designed to control resource utilization in a high concurrency environment.
This recipe shows how to use the tpstats command to diagnose performance bottlenecks in your cluster.
Use the nodetool tpstats command to supply the hostname and JMX port:
In a healthy cluster, the Active and Pending columns should be near zero at all times.
If any column has a high number of pending operations, this generally indicates a bottleneck.
A high number of pending operations ROW-READ-STAGE could indicate the hard disk is over-utilized.
Buildup in the ROW-MUTATION-STAGE would mean the write path is bottlenecked.
Nagios is the de facto standard Network Monitoring System (NMS)
Nagios uses executable programs or scripts to probe the state of services and typically sends e-mails if the services are down.
This recipe shows how to build an executable that can be used by Nagios to check Cassandra.
Even if you are not using Nagios, you may be able to use this script with your system.
Keep an eye out for large rows with compaction limits.
In some use cases, a row can have several columns.
However, in other use cases such as time series data, a row can have thousands or millions of columns.
Use the grep command to search for the string Compacting large row inside the Cassandra logfile.
For use cases that have fixed columns, the limit should never be exceeded.
Setting this value can work as a sanity check to ensure that processes are not inadvertently writing to many columns to the same key.
Keys with many columns can also be problematic when using the row cache because it requires the entire row to be stored in memory.
Reviewing network traffic with IPTraf Issues that manifest on one system are sometimes caused by another.
These type of issues include clients not closing connections, faulty network cards, and applications unintentionally degrading the service by over-utilizing it.
IPTraf is a ncurses-based application to view network statistics in real time.
Getting ready IPTraf is a popular utility and likely packaged with your distribution.
The result is an interactive display that shows network activity.
IPTraf allows users to filter traffic and display the results in real time.
Keep on the lookout for dropped messages Cassandra has a concept of back pressure.
Back pressure is a technique used in staged SEDA architectures in which, if a stage is already full of requests, it will not accept requests from earlier stages.
As a result of back pressure, Cassandra will drop already timed-out requests without processing, and log an error.
Clients receiving this exception should try the failed operation again.
If dropped messages appear frequently in logs, this indicates that the server is not able to keep up with the request load.
If compaction does not become re-enabled, this can end up being bad for performance as multiple sstables slow down the read path.
This recipe shows how to check for these dangerous conditions.
Nodetool uses Remote Method Invocation (RMI) to invoke methods on the server.
Cassandra instances connecting, using Jconsole  23-25 disk space, saving by using small column.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Alex Clark's book will get you managing and enhancing your Plone website like a seasoned expert.
Learn how to use Buildout to develop, deploy, and maintain a modern Plone site.
Enhance the functionality and appearance of your web site by using third-party add-ons.
Integrate the flexibility of Python and the power of MySQL to boost the productivity of your Python applications.
See how to make MySQL take the processing burden from your programs.
Learn how to employ Python with MySQL to power your websites and desktop applications.
Apply your knowledge of MySQL and Python to real-world problems instead of hypothetical scenarios.
Chapter 2: The Command-line Interface Connecting to Cassandra with the CLI Creating a keyspace from the CLI Creating a column family with the CLI Describing a keyspace Writing data with the CLI Reading data with the CLI Deleting rows and columns from the CLI Listing and paginating all rows in a column family Dropping a keyspace or a column family CLI operations with super columns Using the assume keyword to decode column names or column values Supplying time to live information when inserting columns Using built-in CLI functions Using column metadata and comparators for type enforcement Changing the consistency level of the CLI Getting help from the CLI Loading CLI statements from a file.
Chapter 3: Application Programmer Interface Introduction Connecting to a Cassandra server Creating a keyspace and column family from the client Using MultiGet to limit round trips and overhead Writing unit tests with an embedded Cassandra server Cleaning up data directories before unit tests Generating Thrift bindings for other languages (C++, PHP, and others) Using the Cassandra Storage Proxy "Fat Client" Using range scans to find and remove old data Iterating all the columns of a large key Slicing columns in reverse Batch mutations to improve insert performance and code robustness Using TTL to create columns with self-deletion times Working with secondary indexes.
Chapter 4: Performance Tuning Introduction Choosing an operating system and distribution Choosing a Java Virtual Machine Using a dedicated Commit Log disk Choosing a high performing RAID level File system optimization for hard disk performance Boosting read performance with the Key Cache Boosting read performance with the Row Cache Disabling Swap Memory for predictable performance Stopping Cassandra from using swap without disabling it system-wide Enabling Memory Mapped Disk modes Tuning Memtables for write-heavy workloads Saving memory on 64bit architectures with compressed pointers Tuning Concurrent Readers and Writers for throughput Setting compaction thresholds Garbage collection tuning to avoid JVM pauses Raising the open file limit to deal with many clients Increasing performance by scaling up.
Chapter 5: Consistency, Availability, and Partition Tolerance with Cassandra Introduction Working with the formula for strong consistency Supplying the timestamp value with write requests Disabling the hinted handoff mechanism Adjusting read repair chance for less intensive data reads Confirming schema agreement across the cluster Adjusting replication factor to work with quorum Using write consistency ONE, read consistency ONE for low latency operations Using write consistency QUORUM, read consistency QUORUM for strong consistency Mixing levels write consistency QUORUM, read consistency ONE Choosing consistency over availability consistency ALL Choosing availability over consistency with write consistency ANY Demonstrating how consistency is not a lock or a transaction.
Chapter 6: Schema Design Introduction Saving disk space by using small column names Serializing data into large columns for smaller index sizes Storing time series data effectively Using Super Columns for nested maps Using a lower Replication Factor for disk space saving and performance enhancements Hybrid Random Partitioner using Order Preserving Partitioner Storing large objects Using Cassandra for distributed caching Storing large or infrequently accessed data in a separate column family Storing and searching edge graph data in Cassandra Developing secondary data orderings or indexes.
Chapter 10: Libraries and Applications Introduction Building the contrib stress tool for benchmarking Inserting and reading data with the stress tool Running the Yahoo! Cloud Serving Benchmark Hector, a high-level client for Cassandra Doing batch mutations with Hector Cassandra with Java Persistence Architecture (JPA) Setting up Solandra for full text indexing with a Cassandra backend Setting up Zookeeper to support Cages for transactional locking Using Cages to implement an atomic read and set Using Groovandra as a CLI alternative Searchable log storage with Logsandra.
Chapter 12: Collecting and Analyzing Performance Statistics Finding bottlenecks with nodetool tpstats Using nodetool cfstats to retrieve column family statistics Monitoring CPU utilization Adding read/write graphs to find active column families Using Memtable graphs to profile when and why they flush Graphing SSTable count Monitoring disk utilization and having a performance baseline Monitoring compaction by graphing its activity Using nodetool compaction stats to check the progress of compaction Graphing column family statistics to track average/max row sizes Using latency graphs to profile time to seek keys Tracking the physical disk size of each column family over time Using nodetool cfhistograms to see the distribution of query latencies Tracking open networking connections.
Chapter 13: Monitoring Cassandra Servers Introduction Forwarding Log4j logs to a central sever Using top to understand overall performance Using iostat to monitor current disk performance Using sar to review performance over time Using JMXTerm to access Cassandra JMX Monitoring the garbage collection events Using tpstats to find bottlenecks Creating a Nagios Check Script for Cassandra Keep an eye out for large rows with compaction limits Reviewing network traffic with IPTraf Keep on the lookout for dropped messages Inspecting column families for dangerous conditions.
