OpenStack offers open source software for cloud administrators to manage and troubleshoot an OpenStack cloud.
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.
Except where otherwise noted, this document is licensed under Creative Commons Attribution ShareAlike 3.0 License.
The OpenStack project is an open source cloud computing platform for all types of clouds, which aims to be simple to implement, massively scalable, and feature rich.
Developers and cloud computing technologists from around the world create the OpenStack project.
OpenStack provides an Infrastructure as a Service (IaaS) solution through a set of interrelated services.
Each service offers an application programming interface (API) that facilitates this integration.
Depending on your needs, you can install some or all services.
The following table describes the OpenStack services that make up the OpenStack architecture:
Dashboard Horizon Provides a web-based self-service portal to interact with underlying OpenStack services, such as launching an instance, assigning IP addresses and configuring access controls.
Compute Nova Manages the lifecycle of compute instances in an OpenStack environment.
Responsibilities include spawning, scheduling and decomissioning of machines on demand.
Networking Neutron Enables network connectivity as a service for other OpenStack services, such as OpenStack Compute.
Provides an API for users to define networks and the attachments into them.
Has a pluggable architecture that supports many popular networking vendors and technologies.
Swift Stores and retrieves arbitrary unstructured data objects via a RESTful, HTTP based API.
It is highly fault tolerant with its data replication and scale out architecture.
Its implementation is not like a file server with mountable directories.
Block Storage Cinder Provides persistent block storage to running instances.
Its pluggable driver architecture facilitates the creation and management of block storage devices.
Keystone Provides an authentication and authorization service for other OpenStack services.
Image Service Glance Stores and retrieves virtual machine disk images.
Ceilometer Monitors and meters the OpenStack cloud for billing, benchmarking, scalability, and statistical purposes.
Conceptual architecture The following diagram shows the relationships among the OpenStack services:
Logical architecture To design, install, and configure a cloud, cloud administrators must understand the logical architecture.
On Linux platforms, a daemon is usually installed as a service.
For example, the run_tests.sh script installs and optionally tests a virtual environment for a service.
Enables users to submit API calls to OpenStack services through easy-to-use commands.
The following diagram shows the most common, but not the only, architecture for an OpenStack cloud:
Compute service The Compute service is a cloud computing fabric controller, which is the main part of an IaaS system.
Compute interacts with the Identity Service for authentication, Image Service for images, and the Dashboard for the user and administrative interface.
Access to images is limited by project and by user; quotas are limited per project (for example, the number of instances)
The Compute service scales horizontally on standard hardware, and downloads images to launch instances as required.
The Compute Service is made up of the following functional areas and their underlying components:
Also, initiates most orchestration activities, such as running an instance, and enforces some policies.
The novaapi-metadata service is generally only used when you run in multi-host mode with nova-network installations.
On Debian systems, it is included in the nova-api package, and can be selected through debconf.
A worker daemon that creates and terminates virtual machine instances through hypervisor APIs.
The process by which it does so is fairly complex but the basics are simple: Accept actions from the queue and perform a series of system commands, like launching a KVM instance, to carry them out while updating state in the database.
Takes a virtual machine instance request from the queue and determines on which compute server host it should run.
Aims to eliminate direct accesses to the cloud database made by novacompute.
However, do not deploy it on any nodes where nova-compute runs.
Similar to nova-compute, it accepts networking tasks from the queue and performs tasks to manipulate the network, such as setting up bridging interfaces or changing iptables rules.
This functionality is being migrated to OpenStack Networking, which is a separate OpenStack service.
Tracks IP address leases and records them in the database by using the dnsmasq dhcp-script facility.
This service must be running for console proxies to work.
Many proxies of either type can be run against a single novaconsoleauth service in a cluster configuration.
Provides a proxy for accessing running instances through a VNC connection.
A proxy for accessing running instances through a VNC connection.
Provides an S3 interface for registering images with the Image Service.
A set of command-line interpreter commands for managing cloud resources.
Though not an OpenStack module, you can configure nova-api to support this EC2 interface.
Enables users to submit commands as a tenant administrator or end user.
Usually implemented with RabbitMQ, but could be any AMPQ message queue, such as Apache Qpid or Zero MQ.
Stores most build-time and runtime states for a cloud infrastructure.
Includes instance types that are available for use, instances in use, available networks, and projects.
Theoretically, OpenStack Compute can support any database that SQLAlchemy supports, but the only databases widely used are sqlite3 databases (only appropriate for test and development work), MySQL, and PostgreSQL.
The Compute Service interacts with other OpenStack services: Identity Service for authentication, Image Service for images, and the OpenStack dashboard for a web interface.
Storage concepts The OpenStack stack uses the following storage types:
Used for adding additional persistent storage to a virtual machine (VM)
Persists until VM is terminated Persists until deleted Persists until deleted.
Access associated with a VM Access associated with a VM Available from anywhere.
OpenStack Object Storage is not used like a traditional hard drive.
Object storage is all about relaxing some of the constraints of a POSIX-style file system.
The access to it is APIbased (and the API uses http)
This is a good idea as if you don't have to provide atomic operations (that is, you can rely on eventual consistency), you can much more easily scale a storage system and avoid a central point of failure.
The OpenStack Image Service is used to manage the virtual machine images in an OpenStack cluster, not store them.
Instead, it provides an abstraction to different methods for storage - a bridge to the storage, not the storage itself.
The Object Storage (swift) product can be used independently of the Compute (nova) product.
Object Storage service The Object Storage service is a highly scalable and durable multi-tenant object storage system for large amounts of unstructured data at low cost through a RESTful HTTP API.
Accepts Object Storage API and raw HTTP requests to upload files, modify metadata, and create containers.
It also serves file or container listings to web browsers.
To improve performance, the proxy server can use an optional cache usually deployed with memcache.
Manage a mapping of containers, or folders, within the Object Storage service.
Manage actual objects, such as files, on the storage nodes.
The replication services ensure consistency and availability through the cluster.
Accepts API requests and routes them to cinder-volume for action.
Responds to requests to read from and write to the Block Storage database to maintain state, interacting with other processes (like cinder-scheduler) through a message queue and directly upon block storage providing hardware or software.
It can interact with a variety of storage providers through a driver architecture.
Like the nova-scheduler, picks the optimal block storage provider node on which to create the volume.
Like many OpenStack services, OpenStack Networking is highly configurable due to its plug-in architecture.
Accepts and routes API requests to the appropriate OpenStack Networking plug-in for action.
Plugs and unplugs ports, creates networks or subnets, and provides IP addressing.
These plug-ins and agents differ depending on the vendor and technologies used in the particular cloud.
Most OpenStack Networking installations make use of a messaging queue to route information between the neutron-server and various agents as well as a database to store networking state for particular plug-ins.
OpenStack Networking interacts mainly with OpenStack Compute, where it provides networks and connectivity for its instances.
Dashboard The dashboard is a modular Django web application that provides a graphical interface to OpenStack services.
You can modify the dashboard code to make it suitable for different sites.
From a network architecture point of view, this service must be accessible to customers and the public API for each OpenStack service.
To use the administrator functionality for other services, it must also connect to Admin API endpoints, which should not be accessible by customers.
Provides a catalog of available services with their API endpoints.
To understand the Identity Service, you must understand the following concepts:
User Digital representation of a person, system, or service who uses OpenStack cloud services.
The Identity Service validates that incoming requests are made by the user who claims to be making the call.
Users have a login and may be assigned tokens to access resources.
Users can be directly assigned to a particular tenant and behave as if they are contained in that tenant.
Credentials Data that is known only by a user that proves who they are.
In the Identity Service, examples are: User name and password, user name and API key, or an authentication token provided by the Identity Service.
Authentication The act of confirming the identity of a user.
The Identity Service confirms an incoming request by validating a set of credentials supplied by the user.
These credentials are initially a user name and password or a user name and API key.
In response to these credentials, the Identity Service issues an authentication token to the user, which the user provides in subsequent requests.
Token An arbitrary bit of text that is used to access resources.
Each token has a scope which describes which resources are accessible with it.
A token may be revoked at any time and is valid for a finite duration.
While the Identity Service supports token-based authentication in this release, the intention is for it to support additional protocols in the future.
The intent is for it to be an integration service foremost, and not aspire to be a full-fledged identity store and management solution.
Tenant A container used to group or isolate resources and/or identity objects.
Depending on the service operator, a tenant may map to a customer, account, organization, or project.
Provides one or more endpoints through which users can access resources and perform operations.
Endpoint A network-accessible address, usually described by a URL, from where you access a service.
If using an extension for templates, you can create an endpoint template, which represents the templates of all the consumable services that are available across the regions.
Role A personality that a user assumes that enables them to perform a specific set of operations.
A user assuming that role inherits those rights and privileges.
In the Identity Service, a token that is issued to a user includes the list of roles that user has.
Services that are being called by that user determine how they interpret the set of roles a user has and to which operations or resources each role grants access.
Accepts Image API calls for image discovery, retrieval, and storage.
A number of periodic processes run on the Image Service to support caching.
As shown in the section called “Conceptual architecture” [2], the Image Service is central to the overall IaaS picture.
It accepts API requests for images or image metadata from end users or Compute components and can store its disk files in the Object Storage Service.
Efficiently collects the metering data about the CPU and network costs.
Collects data by monitoring notifications sent from services or by polling the infrastructure.
Configures the type of collected data to meet various operating requirements.
Accessing and inserting the metering data through the REST API.
Expands the framework to collect custom usage data by additional plug-ins.
Runs on each compute node and polls for resource utilization statistics.
There may be other types of agents in the future, but for now we will focus on creating the compute agent.
Runs on a central management server to poll for resource utilization statistics for resources not tied to instances or compute nodes.
Runs on one or more central management servers to monitor the message queues (for notifications and for metering data coming from the agent)
Telemetry messages are written to the data store without modification.
Runs on one or more central management servers to allow settting alarms based on threshold evaluation for a collection of samples.
A database capable of handling concurrent writes (from one or more collector instances) and reads (from the API server)
Runs on one or more central management servers to provide access to the data from the data store.
Only the collector and API server have access to the data store.
These services communicate by using the standard OpenStack messaging bus.
Only the collector and API server have access to the data store.
Orchestration service overview The Orchestration service provides a template-based orchestration for describing a cloud application by running OpenStack API calls to generate running cloud applications.
The software integrates other core components of OpenStack into a one-file template system.
The templates enable you to create most OpenStack resource types, such as instances, floating IPs, volumes, security groups, users, and so on.
Also, provides some more advanced functionality, such as instance high availability, instance auto-scaling, and nested stacks.
By providing very tight integration with other OpenStack core projects, all OpenStack core projects could receive a larger user base.
The service enables deployers to integrate with the Orchestration service directly or through custom plug-ins.
End developers could also use the Orchestration REST API directly.
Provides an OpenStack-native REST API that processes API requests by sending them to the heat-engine over RPC.
Provides an AWS Query API that is compatible with AWS CloudFormation and processes API requests by sending them to the heat-engine over RPC.
Orchestrates the launching of templates and provides events back to the API consumer.
You initialize data into the Identity Service by using the keystone command-line client.
Has associated information such as user name, password, and email.
When you make requests to OpenStack services, you must specify a tenant.
For example, if you query the Compute service for a list of running instances, you get a list of all running instances in the tenant that you specified in your query.
Because the term project was used instead of tenant in earlier versions of OpenStack Compute, some command-line tools use --project_id instead of --tenant-id or --os-tenant-id to refer to a tenant ID.
Captures the operations that a user can perform in a given tenant.
Individual services, such as Compute and the Image Service, assign meaning to roles.
In the Identity Service, a role is simply a name.
The Identity Service assigns a tenant and a role to a user.
You might assign the computeuser role to the alice user in the acme tenant:
For example, Alice might also have the admin role in the Cyberdyne tenant.
A user can also have multiple roles in the same tenant.
The default policy.json files in the Compute, Identity, and Image service recognize only the admin role: all operations that do not require the admin role are accessible by any user that has any role in a tenant.
Service management The Identity Service provides identity, token, catalog, and policy services.
Starts both the service and administrative APIs in a single process to provide Catalog, Authorization, and Authentication services for OpenStack.
Each has a pluggable back end that allows different ways to use the particular service.
The Identity Service also maintains a user that corresponds to each service, such as, a user named nova for the Compute service, and a special service tenant called service.
For information about how to create services and endpoints, see the OpenStack Admin User Guide.
Then, rather than assign a role to each user individually, assign a role to the group.
Groups were introduced with version 3 of the Identity API (the Grizzly release of Identity Service)
For example, if using the Identity server with the LDAP Identity back end and group updates are disabled, then a request to create, delete, or update a group fails.
Domains A domain defines administrative boundaries for the management of Identity entities.
A domain may represent an individual, company, or operator-owned space.
It is used for exposing administrative activities directly to the system users.
A domain is a collection of tenants, users, and roles.
A domain administrator may create tenants, users, and groups within a domain and assign roles to users and groups.
User CRUD The Identity Service provides a user CRUD filter that can be added to the public_api pipeline.
This user CRUD filter enables users to use a HTTP PATCH to change their own password.
Each user can then change their own password with a HTTP PATCH:
In addition to changing their password, all of the user's current tokens are deleted (if the back-end is KVS or sql)
Logging You configure logging externally to the rest of the Identity Service.
The file specifying the logging configuration is in the [DEFAULT] section of the keystone.conf file under log_config.
To route logging through syslog, set use_syslog=true option in the [DEFAULT] section.
Like other OpenStack projects, the Identity Service uses the Python logging module, which includes extensive configuration options that let you define the output levels and formats.
For example, each server application has its own configuration file.
For example in Compute, you can remove the middleware parameters from api-paste.ini, as follows:
Monitoring The Identity Service provides some basic request/response monitoring statistics out of the box.
Enable data collection by defining a stats_monitoring filter and including it at the beginning of any desired WSGI pipelines:
Start the Identity Service To start the services for the Identity Service, run the following command:
This command starts two wsgi.Server instances configured by the keystone.conf file as described previously.
One of these wsgi servers is admin (the administration API) and the other is main (the primary/public API interface)
Example usage The keystone client is set up to expect commands in the general form of keystone command argument, followed by flag-like keyword arguments to provide additional (often optional) information.
If admin_token is specified, it is used only if the specified token is still valid.
In Compute, for example, you can remove the middleware parameters from api-paste.ini, as follows:
The admin user is granted access to the admin role on the admin tenant.
The logs show the components that have come in to the WSGI request, and ideally show an error that explains why an authorization request failed.
If you do not see the request in the logs, run keystone with --debug parameter.
Debug PKI middleware If you receive an Invalid OpenStack Identity Credentials message when you talk to an OpenStack service, it might be caused by the changeover from UUID tokens to PKI tokens in the Grizzly release.
The PKI-based token validation scheme relies on certificates from the Identity Service that are fetched through HTTP and stored in a local directory.
The location for this directory is specified by the signing_dir configuration option.
In your services configuration file, look for a section like this:
If no value is specified for this directory, it defaults to a secure temporary directory.
Initialization code for the service checks that the directory exists and is writable.
If it does not exist, the code tries to create it.
The first thing to check is that the signing_dir does, in fact, exist.
If it does, check for the presence of the certificate files inside there:
This directory contains two certificates and the token revocation list.
If these files are not present, your service cannot fetch them from the Identity Service.
To troubleshoot, try to talk to the Identity Service to make sure it is serving out the files, as follows:
The token revocation list is updated once a minute, but the certificates are not.
One possible problem is that the certificates are the wrong files or garbage.
You can remove these files and run another command against your server: They are fetched on demand.
The Identity Service log should show the access of the certificate files.
Set debug = True and verbose = True in your Identity Service configuration file and restart the Identity Service server.
If the files do not appear in your directory after this, it is likely one of the following issues:
Your service is configured incorrectly and cannot talk to the Identity Service.
Use the chmod command to change its permissions so that the service (POSIX) user can write to it.
SELinux troubles often when you use Fedora/RHEL-based packages and you choose configuration options that do not match the standard policy.
If that makes a different, you should relabel the directory.
If you are using a sub-directory of the /var/cache/ directory, run the following command:
If you are not using a /var/cache sub-directory, you should.
Modify the signing_dir configuration option for your service and restart.
Set back to setenforce enforcing to confirm that your changes solve the problem.
If your certificates are fetched on demand, the PKI validation is working properly.
Most likely, the token from the Identity Service is not valid for the operation you are attempting to perform, and your user needs a different role for the operation.
Debug signing key file errors If an error occurs when the signing key file opens, it is possible that the person who ran the keystone-manage pki_setup command to generate certificates and keys did not use the correct user.
This can present a problem when you run the Identity Service daemon under the keystone user account (nologin) when you try to run PKI.
Unless you run the chown command against the files keystone:keystone or run the keystone-manage pki_setup command with the --keystone-user and --keystone-group parameters, you get an error, as follows:
The OpenStack dashboard is a web-based interface that allows you to manage OpenStack resources and services.
The dashboard allows you to interact with the OpenStack Compute cloud controller using the OpenStack APIs.
For more information about installing and configuring the dashboard, see the OpenStack Installation Guide for your operating system.
You can customize the dashboard with your own colors, logo, and site title through a CSS file.
The text TGen Cloud in this example is rendered through .png files of multiple sizes created with a graphics program.
Edit your CSS file to override the Ubuntu customizations in the ubuntu.css file.
Change the colors and image file names as appropriate, though the relative directory paths should be the same.
Reload the dashboard in your browser to view your changes.
Set up session storage for the dashboard The dashboard uses Django sessions framework to handle user session data.
The following sections describe the pros and cons of each option as it pertains to deploying the dashboard.
Local memory cache Local memory storage is the quickest and easiest session back end to set up, as it has no external dependencies whatsoever.
The local memory back end is enabled as the default for Horizon solely because it has no dependencies.
It is not recommended for production use, or even for serious development work.
Key-value stores You can use applications such as Memcached or Redis for external caching.
These applications offer persistence and shared storage and are useful for small-scale deployments and/or development.
Memcached is an high-performance and distributed memory object caching system providing in-memory key-value store for small chunks of arbitrary data.
Redis is an open source, BSD licensed, advanced key-value store.
It is often referred to as a data structure server.
Initialize and configure the database Database-backed sessions are scalable, persistent, and can be made high-concurrency and highly-available.
However, database-backed sessions are one of the slower session storages and incur a high overhead under heavy usage.
Proper configuration of your database deployment can also be a substantial undertaking and is far beyond the scope of this documentation.
Create a MySQL user for the newly-created dash database that has full control of the database.
After configuring the local_settings as shown, you can run the manage.py syncdb command to populate this newly-created database.
On Ubuntu: If you want to avoid a warning when you restart apache2, create a blackhole directory in the dashboard directory, as follows:
Restart Apache to pick up the default site and symbolic link settings:
On Ubuntu, restart the nova-api service to ensure that the API server can connect to the dashboard without error:
Cached database To mitigate the performance issues of database queries, you can use the Django cached_db session back end, which utilizes both your database and caching infrastructure to perform write-through caching and efficient retrieval.
Enable this hybrid setting by configuring both your database and cache, as discussed previously.
This back end stores session data in a cookie, which is stored by the user’s browser.
The back end uses a cryptographic signing technique to ensure session data is not tampered with during transport.
This is not the same as encryption; session data is still readable by an attacker.
The pros of this engine are that it requires no additional dependencies or infrastructure overhead, and it scales indefinitely as long as the quantity of session data being stored fits into a normal cookie.
The biggest downside is that it places session data into storage on the user’s machine and transports it over the wire.
It also limits the quantity of session data that can be stored.
It gives you control over instances and networks, and allows you to manage access to the cloud through users and projects.
Instead, it defines drivers that interact with underlying virtualization mechanisms that run on your host operating system, and exposes functionality over a web-based API.
Selecting the best hypervisor to use can be difficult, and you must take budget, resource constraints, supported features, and required technical specifications into account.
However, the majority of OpenStack development is done on systems using KVM and Xen-based hypervisors.
You can also orchestrate clouds using multiple hypervisors in different availability zones.
The types of virtualization standards that can be used with Compute include:
For more information about hypervisors, see the Hypervisors section in the OpenStack Configuration Reference.
Tenants, users, and roles The Compute system is designed to be used by different consumers in the form of tenants on a shared system, and role-based access assignments.
Roles control the actions that a user is allowed to perform.
Tenants are isolated resource containers that form the principal organizational structure within the Compute service.
They consist of an individual VLAN, and volumes, instances, images, keys, and users.
A user can specify the tenant by appending :project_id to their access key.
If no tenant is specified in the API request, Compute attempts to use a tenant with the same ID as the user.
This allows instances to have the same publicly accessible IP addresses.
This allows instances to have the same publicly or privately accessible IP addresses.
Roles control the actions a user is allowed to perform.
By default, most actions do not require a particular role, but you can configure them by editing the policy.json file for user roles.
For example, a rule can be defined so that a user must have the admin role in order to be able to allocate a public IP address.
Keypairs granting access to an instance are enabled for each user, but quotas are set, so that each tenant can control resource consumption across available hardware resources.
Earlier versions of OpenStack used the term project instead of tenant.
Because of this legacy terminology, some command-line tools use -project_id where you would normally expect to enter a tenant ID.
Images and instances Disk images provide templates for virtual machine file systems.
Instances are the individual virtual machines that run on physical compute nodes.
Users can launch any number of instances from the same image.
Each launched instance runs from a copy of the base image so that any changes made to the instance do not affect the base image.
You can take snapshots of running instances to create an image based on the current disk state of a particular instance.
For more information about creating and troubleshooting images, see the Manage Images section of the OpenStack Admin User Guide.
For more information about image configuration options, see the Image Services section of the OpenStack Configuration Reference.
When you launch an instance, you must choose a flavor, which represents a set of virtual resources.
Flavors define how many virtual CPUs an instance has and the amount of RAM and size of its ephemeral disks.
OpenStack provides a number of predefined flavors that you can edit or add to.
Users must select from the set of available flavors defined on their cloud.
For more information about flavors, see the Flavors section in the OpenStack Operations Guide.
You can add and remove additional resources from running instances, such as persistent volume storage, or public IP addresses.
The example used in this chapter is of a typical virtual system within an OpenStack cloud.
It uses the cinder-volume service, which provides persistent block storage, instead of the ephemeral storage provided by the selected instance flavor.
This diagram shows the system state prior to launching an instance.
The image store, fronted by the image service, Glance, has a number of predefined images.
Inside the cloud, a compute node contains the available vCPU, memory, and local disk resources.
Additionally, the cinder-volume service provides a number of predefined volumes.
To launch an instance, select an image, a flavor, and other optional attributes.
The selected flavor provides a root volume, labeled vda in this diagram, and additional ephemeral storage, labeled vdb.
In this example, the cinder-volume store is mapped to the third virtual disk on this instance, vdc.
The base image is copied from the image store to the local disk.
The local disk is the first disk that the instance accesses, and is labeled vda.
By using smaller images, your instances start up faster as less data needs to be copied across the network.
This is an empty ephemeral disk, which is destroyed when you delete the instance.
The compute node is attached to the cinder-volume using iSCSI, and maps to the third disk, vdc.
The vCPU and memory resources are provisioned and the instance is booted from vda.
The instance runs and changes data on the disks as indicated in red in the diagram.
Some of the details in this example scenario might be different in your environment.
Specifically, you might use a different type of back-end storage or different network protocols.
One common variant is that the ephemeral storage used for volumes vda and vdb could be backed by network storage rather than a local disk.
When the instance is deleted, the state is reclaimed with the exception of the persistent volume.
The ephemeral storage is purged, memory and vCPU resources are released.
The cloud controller represents the global state and interacts with the other components.
The API server acts as the web services front end for the cloud controller.
The compute controller provides compute server resources and usually also contains the Compute service.
The object store is an optional component that provides storage services.
An auth manager provides authentication and authorization services when used with the Compute system, or you can use the identity service as a separate authentication service instead.
A volume controller provides fast and permanent block-level storage for the compute servers.
The network controller provides virtual networks to enable compute servers to interact with each other and with the public network.
The scheduler is used to select the most suitable compute controller to host an instance.
All major components exist on multiple servers, including the compute, volume, and network controllers, and the object store or image service.
The state of the entire system is stored in a database.
The cloud controller communicates with the internal object store using HTTP, but it communicates with the scheduler, network controller, and volume controller using AMQP (advanced message queueing protocol)
To avoid blocking a component while waiting for a response, Compute uses asynchronous calls, with a callback that is triggered when a response is received.
Block storage OpenStack provides two classes of block storage: ephemeral storage and persistent volumes.
Volumes are persistent virtualized block devices independent of any particular instance.
Ephemeral storage is associated with a single unique instance, and it exists only for the life of that instance.
The amount of ephemeral storage is defined by the flavor of the instance.
Generally, the root file system for an instance will be stored on ephemeral storage.
It persists across reboots of the guest operating system, but when the instance is deleted, the ephemeral storage is also removed.
This is presented as a raw block device with no partition table or file system.
Cloud-aware operating system images can discover, format, and mount these storage devices.
For example, the cloud-init package included in Ubuntu's stock cloud images format this space as an ext3 file system and mount it on / mnt.
This is a feature of the guest operating system you are using, and is not an OpenStack mechanism.
Persistent volumes are created by users and their size is limited only by the user's quota and availability limits.
Upon initial creation, volumes are raw block devices without a partition table or a file system.
To partition or format volumes, you must attach them to an instance.
Once they are attached to an instance, you can use persistent volumes in much the same way as you would use external hard disk drive.
You can attach volumes to only one instance at a time, although you can detach and reattach volumes to as many different instances as you like.
Persistent volumes can be configured as bootable and used to provide a persistent virtual instance similar to traditional non-cloud-based virtualization systems.
Typically, the resulting instance can also still have ephemeral storage depending on the flavor selected, but the root file system can be on the persistent volume and its state maintained even if the instance is shut down.
For more information about this type of configuration, see the OpenStack Configuration Reference.
Note Persistent volumes do not provide concurrent access from multiple instances.
That type of configuration requires a traditional network file system like NFS or CIFS, or a cluster file system such as GlusterFS.
These systems can be built within an OpenStack cluster or provisioned outside of it, but OpenStack software does not provide these features.
Image management The OpenStack Image service discovers, registers, and retrieves virtual machine images.
The service also includes a RESTful API that allows you to query VM image metadata and retrieve the actual image with HTTP requests.
The OpenStack Image service can be controlled using a command line tool.
For more information about the OpenStack Image command line tool, see the  Image Management section in the OpenStack User Guide.
Virtual images that have been made available through the Image service can be stored in a variety of ways.
In order to use these services, you must have a working installation of the Image service, with a working endpoint, and users that have been created in the Identity service.
Additionally, you must meet the environment variables required by the Compute and Image clients.
File system The OpenStack Image service stores virtual machine images in the file system back-end by default.
Rados block device (RBD) Stores images inside of a Ceph storage cluster using Ceph's RBD interface.
Instance management tools OpenStack provides command line, web-based, and API-based instance management tools.
Additionally, a number of third party management tools are available, using either the native API or the provided EC2-compatible API.
The OpenStack python-novaclient package provides a basic command line utility, which uses the nova command.
This is available as a native package for most Linux distributions, or you can install the latest version using the pip python package installer:
The OpenStack Configuration Reference lists configuration options for customizing this compatibility API on your OpenStack cloud.
Numerous third party tools and language-specific SDKs can be used to interact with OpenStack clouds, using both native and compatibility APIs.
Hybridfox A Firefox browser add-on that provides a graphical interface to many popular public and private cloud technologies, including OpenStack.
It can be used to access OpenStack through the EC2 compatibility API.
For more information, see the  boto project page on GitHub.
It provides methods for interacting with a large number of cloud and virtualization platforms, including OpenStack.
Building blocks In OpenStack the base operating system is usually copied from an image stored in the OpenStack Image service.
This is the most common case and results in an ephemeral instance that starts from a known template state and loses all accumulated states on shutdown.
It is also possible to put an operating system on a persistent volume in the Nova-Volume or Cinder volume system.
This gives a more traditional persistent system that accumulates states, which are preserved across restarts.
Server For images that are created as snapshots of running instances, this is the UUID of the instance the snapshot derives from.
For a list of flavors that are available on your system:
Flavors Authorized users can use the nova flavor-create command to create flavors.
To modify an existing flavor in the dashboard, you must delete the flavor and create a modified one with the same name.
This is an ephemeral disk that the base image is copied into.
When booting from a persistent volume it is not used.
The "0" size is a special case which uses the native base image size as the size of the ephemeral root volume.
Ephemeral Specifies the size of a secondary ephemeral data disk.
This is an empty, unformatted disk and exists only for the life of the instance.
This factor is multiplied by the rxtx_base property of the network.
Default value is 1.0 (that is, the same as attached network)
Is_Public Boolean value, whether flavor is available to all users or private to the tenant it was created in.
This is implemented as key/value pairs that must match against the corresponding key/value pairs on compute nodes.
Can be used to implement things like special resources (e.g., flavors that can only run on compute nodes with GPU hardware)
Flavor customization can be limited by the hypervisor in use, for example the libvirt driver enables quotas on CPUs available to a VM, disk tuning, bandwidth I/O, and instance VIF traffic control.
You can configure the CPU limits with control parameters with the nova tool.
There are CPU control parameters for weight shares, enforcement intervals for runtime quotas, and a quota for maximum allowed bandwidth.
The optional cpu_shares element specifies the proportional weighted share for the domain.
If this element is omitted, the service defaults to the OS provided defaults.
It is a relative measure based on the setting of other VMs.
The optional cpu_period element specifies the enforcement interval (unit: microseconds) for QEMU and LXC hypervisors.
Within a period, each VCPU of the domain is not allowed to consume more than the quota worth of runtime.
The optional cpu_quota element specifies the maximum allowed bandwidth (unit: microseconds)
A domain with a quota with a negative value indicates that the domain has infinite bandwidth, which means that it is not bandwidth controlled.
You can use this feature to ensure that all vcpus run at the same speed.
Through disk I/O quotas, you can set maximum disk write to 10 MB per second for a VM user.
The bandwidth element can have at most one inbound and at most one outbound child element.
Leaving any of these children element out result in no quality of service (QoS) applied on that traffic direction.
So, when you want to shape only the network's incoming traffic, use inbound only, and vice versa.
It specifies average bit rate on the interface being shaped.
Then there are two optional attributes: peak, which specifies maximum rate at which bridge can send data, and burst, amount of bytes that can be burst at peak speed.
Accepted values for attributes are integer numbers, The units for average and peak attributes are kilobytes per second, and for the burst just kilobytes.
The rate is shared equally within domains connected to the network.
This example configures a bandwidth limit for instance network traffic:
By default, a flavor is public and available to all projects.
Private flavors are only accessible to those on the access list and are invisible to other projects.
To create and assign a private flavor to a project, run these commands:
Control where instances run The OpenStack Configuration Reference provides detailed information on controlling where your instances run, including ensuring a set of instances run on different compute nodes for service resiliency or on the same node for high performance inter-instance communications.
Admin password injection You can configure Compute to generate a random administrator (root) password and inject that password into the instance.
If this feature is enabled, a user can ssh to an instance without an ssh keypair.
The random password appears in the output of the nova boot command.
You can also view and set the admin password from the dashboard.
Dashboard The dashboard is configured by default to display the admin password and allow the user to modify it.
Libvirt-based hypervisors (KVM, QEMU, LXC) For hypervisors such as KVM that use the libvirt backend, admin password injection is disabled by default.
When enabled, Compute will modify the password of the root account by editing the / etc/shadow file inside of the virtual machine instance.
Note Users will only be able to ssh to the instance using the admin password if:
The virtual machine has been configured to allow users to ssh as the root user.
This is not the case for Ubuntu cloud images, which disallow ssh to the root account by default.
XenAPI (XenServer/XCP) Compute uses the XenAPI agent to inject passwords into guests when using the XenAPI hypervisor backend.
The virtual machine image must be configured with the agent for password injection to work.
Windows images (all hypervisors) To support the admin password for Windows virtual machines, you must configure the Windows image to retrieve the admin password on boot by installing an agent such as cloudbase-init.
Volumes Depending on the setup of your cloud provider, they may give you an endpoint to use to manage volumes, or there may be an extension under the covers.
In either case, you can use the nova CLI to manage volumes.
Networking with nova-network Understanding the networking configuration options helps you design the best configuration for your Compute instances.
For each VM instance, Compute assigns to it a private IP address.
Currently, Compute with nova-network only supports Linux bridge networking that enables the virtual interfaces to connect to the outside network through the physical interface.
The network controller with nova-network provides virtual networks to enable compute servers to interact with each other and with the public network.
Currently, Compute with nova-network supports these kinds of networks, implemented in different “Network Manager” types:
However, because you can't yet select the type of network for a given project, you cannot configure more than one type of network in a given Compute installation.
All networking options require network connectivity to be already set up between OpenStack physical nodes.
OpenStack automatically creates all network bridges (for example, br100) and VM virtual interfaces.
The internal network interface is used for communication with VMs, it shouldn't have an IP address attached to it before OpenStack installation (it serves merely as a fabric where the actual endpoints are VMs and dnsmasq)
Also, the internal network interface must be put in promiscuous mode, because it must receive packets whose target MAC address is of the guest VM, not of the host.
All the network managers configure the network using network drivers.
The driver isn't tied to any particular network manager; all network managers use the same driver.
The driver usually initializes (creates bridges and so on) only when the first VM lands on this host node.
All network managers operate in either single-host or multi-host mode.
In single-host mode, a single nova-network service provides a default gateway for VMs and hosts a single DHCP server (dnsmasq)
In multi-host mode, each compute node runs its own nova-network service.
In both cases, all traffic between VMs and the outer world flows through nova-network.
Compute makes a distinction between fixed IPs and floating IPs for VM instances.
Fixed IPs are IP addresses that are assigned to an instance on creation and stay the same until the instance is explicitly terminated.
By contrast, floating IPs are addresses that can be dynamically associated with an instance.
A floating IP address can be disassociated and associated with another instance at any time.
A user can reserve a floating IP for their project.
The IP addresses for VM instances are grabbed from the subnet, and then injected into the image on launch.
Each instance receives a fixed IP address from the pool of available addresses.
A system administrator may create the Linux networking bridge (typically named br100, although this configurable) on the systems running the nova-network service.
All instances of the system are attached to the same bridge, configured manually by the network administrator.
In Flat DHCP Mode, OpenStack starts a DHCP server (dnsmasq) to pass out IP addresses to VM instances from the specified subnet in addition to manually configuring the networking bridge.
Like Flat Mode, all instances are attached to a single bridge on the compute node.
In addition a DHCP server is running to configure instances (depending on single-/multihost mode, alongside each nova-network)
It also runs and configures dnsmasq as a DHCP server listening on this bridge, usually on IP address 10.0.0.1 (see DHCP server: dnsmasq)
For every instance, nova allocates a fixed IP address and configure dnsmasq with the MAC/IP pair for the VM.
For example, dnsmasq doesn't take part in the IP address allocation process, it only hands out IPs according to the mapping done by nova.
These IPs are not assigned to any of the host's network interfaces, only to the VM's guest-side interface.
In any setup with flat networking, the host(-s) with nova-network on it is (are) responsible for forwarding traffic from the private network.
Compute can determine the NAT entries for each network, though sometimes NAT is not used, such as when configured with all public IPs or a hardware router is used (one of the HA options)
Such host(-s) needs to have br100 configured and physically connected to any other nodes that are hosting VMs.
Compute nodes have iptables/ebtables entries created for each project and instance to protect against IP/MAC address spoofing and ARP poisoning.
In single-host Flat DHCP mode you will be able to ping VMs through their fixed IP from the nova-network node, but you cannot ping them from the compute nodes.
In this mode, Compute creates a VLAN and bridge for each project.
For multiple machine installation, the VLAN Network Mode requires a switch that supports VLAN tagging (IEEE 802.1Q)
The project gets a range of private IPs that are only accessible from inside the VLAN.
In order for a user to access the instances in their project, a special VPN instance (code named cloudpipe) needs to be created.
Compute generates a certificate and key for the user to access the VPN and starts the VPN automatically.
It provides a private network segment for each project's instances that can be accessed through a dedicated VPN connection from the Internet.
In this mode, each project gets its own VLAN, Linux networking bridge, and subnet.
The subnets are specified by the network administrator, and are assigned dynamically to a project when required.
A DHCP Server is started for each VLAN to pass out IP addresses to VM instances from the subnet assigned to the project.
OpenStack Compute creates the Linux networking bridges and VLANs when required.
The nova-network service is responsible for starting up dnsmasq processes.
The behavior of dnsmasq can be customized by creating a dnsmasq configuration file.
See the  OpenStack Configuration Reference for an example of how to change the behavior of dnsmasq using a dnsmasq configuration file.
The dnsmasq documentation has a more comprehensive dnsmasq configuration file example.
Dnsmasq also acts as a caching DNS server for instances.
The following example would configure dnsmasq to use Google's public DNS server:
Dnsmasq logging output goes to the syslog (typically /var/log/syslog or /var/ log/messages, depending on Linux distribution)
The dnsmasq logging output can be useful for troubleshooting if VM instances boot successfully but are not reachable over the network.
This reservation only affects which IP address the VMs start at, not the fixed IP addresses that the nova-network service places on the bridges.
The Compute service uses a special metadata service to enable virtual machine instances to retrieve instance-specific data.
The metadata service supports two sets of APIs: an OpenStack metadata API and an EC2-compatible API.
To retrieve a list of supported versions for the OpenStack metadata API, make a GET request to.
To retrieve a list of supported versions for the EC2-compatible metadata API, make a GET request to.
If you write a consumer for one of these APIs, always attempt to access the most recent API version supported by your consumer first, then fall back to an earlier version if the most recent one is not available.
Here is the same content after having run through a JSON pretty-printer:
You can retrieve a listing of these elements by making a GET query to:
Instances can retrieve the public SSH key (identified by keypair name when a user requests a new instance) by making a GET request to:
Instances can retrieve user data by making a GET request to:
The metadata service is implemented by either the nova-api service or the nova-apimetadata service.
The default enabled_apis configuration setting includes the metadata service, so you should not need to modify it.
The metadata_host configuration option must be an IP address, not a host name.
The default Compute service settings assume that the nova-network service and the nova-api service are running on the same host.
Set the metadata_host configuration option to the IP address of the host where the nova-api service runs.
Enable ping and SSH on VMs Be sure you enable access to your VMs by using the euca-authorize or nova secgroup-addrule command.
These commands enable you to ping and ssh to your VMs:
You must run these commands as root only if the credentials used to interact with nova-api are in /root/.bashrc.
If the EC2 credentials are the .bashrc file for another user, you must run these commands as the user.
If you still cannot ping or SSH your instances after issuing the nova secgroup-add-rule commands, look at the number of dnsmasq processes that are running.
If you have a running instance, check to see that TWO dnsmasq processes are running.
This section describes how to configure floating IP addresses if you opt to use nova-network instead of neutron for OpenStack Networking.
Every virtual instance is automatically assigned a private IP address.
The term floating IP refers to an IP address, typically public, that you can dynamically add to a running virtual instance.
Because floating IPs are implemented by using a source NAT (SNAT rule in iptables), security groups can show inconsistent behavior if VMs use their floating IP to communicate with other VMs, particularly on the same physical host.
Traffic from VM to VM across the fixed network does not have this issue, and so this is the recommended path.
To ensure that traffic does not get SNATed to the floating range, explicitly set.
The x.x.x.x/y value specifies the range of floating IPs for each pool of floating IPs that you define.
If the VMs in the source group have floating IPs, this configuration is also required.
Enable IP forwarding By default, IP forwarding is disabled on most Linux distributions.
To use the floating IP feature, you must enable IP forwarding.
You must enable IP forwarding on only the nodes that run the nova-network service.
If you use multi_host mode, make sure to enable it on all compute nodes.
Otherwise, enable it on only the node that runs the nova-network service.
To check if the forwarding is enabled, run this command:
To make the changes permanent, edit the /etc/sysctl.conf file and update the IP forwarding setting:
Save the file and run this command to apply the changes:
You can also update the setting by restarting the network service.
Nova maintains a list of floating IP addresses that you can assign to instances.
Use the novamanage floating create command to add entries to this list.
You can use the following nova-manage commands to perform floating IP operations:
Creates specific floating IPs for either a single address or a subnet.
Removes floating IP addresses using the same parameters as the create command.
For information about how administrators can associate floating IPs with instances, see Manage IP addresses in the OpenStack Admin User Guide.
You can configure the nova-network service to automatically allocate and assign a floating IP address to virtual instances when they are launched.
If you enable this option and all floating IP addresses have already been allocated, the nova boot command fails.
Remove a network from a project You cannot remove a network that has already been associated to a project by simply deleting it.
To determine the project ID you must have admin rights.
You can disassociate the project from the network with a scrub command and the project ID as the final parameter:
Multiple interfaces for your instances (multinic) The multi-nic feature allows you to plug more than one interface to your instances, making it possible to make several use cases available:
Each VIF is representative of a separate network with its own IP block.
Every network mode introduces it's own set of changes regarding the mulitnic usage:
In order to use the multinic feature, first create two networks, and attach them to your project:
Now every time you spawn a new instance, it gets two IP addresses from the respective DHCP servers:
Make sure to power up the second interface on the instance, otherwise that last won't be reachable through its second IP.
If the Virtual Network Service Neutron is installed, it is possible to specify the networks to attach to the respective interfaces by using the --nic flag when invoking the nova command:
If you cannot reach your instances through the floating IP address, make sure the default security group allows ICMP (ping) and SSH (port 22), so that you can reach the instances:
Ensure the NAT rules have been added to iptables on the node that nova-network is running on, as root:
Note that you cannot SSH to an instance with a public IP from within the same server as the routing configuration won't allow it.
You can use tcpdump to identify if packets are being routed to the inbound interface on the compute host.
If the packets are reaching the compute hosts but the connection is failing, the issue may be that the packet is being dropped by reverse path filtering.
For example, if the inbound interface is eth2, as root:
If this solves your issue, add this line to /etc/sysctl.conf so that the reverse path filter is disabled the next time the compute host reboots:
We strongly recommend you remove this line to re-enable the firewall once your networking issues have been resolved.
If you can SSH to your instances but you find that the network interactions to your instance is slow, or if you find that running certain operations are slower than they should be (for example, sudo), then there may be packet loss occurring on the connection to the instance.
Packet loss can be caused by Linux networking configuration settings related to bridges.
One way to check if this is the issue in your setup is to open up three terminals and run the following commands:
In the first terminal, on the host running nova-network, use tcpdump to monitor DNSrelated traffic (UDP, port 53) on the VLAN interface.
In the second terminal, also on the host running nova-network, use tcpdump to monitor DNS-related traffic on the bridge interface.
In the third terminal, SSH inside of the instance and generate DNS requests by using the nslookup command:
The symptoms may be intermittent, so try running nslookup multiple times.
If the network configuration is correct, the command should return immediately each time.
If it is not functioning properly, the command hangs for several seconds.
If the nslookup command sometimes hangs, and there are packets that appear in the first terminal but not the second, then the problem may be due to filtering done on the bridges.
If this solves your issue, add this line to /etc/sysctl.conf so that these changes take effect the next time the host reboots:
Some administrators have observed an issue with the KVM hypervisor where instances running Ubuntu 12.04 sometimes loses network connectivity after functioning properly for a period of time.
This kernel module may also improve network performance on KVM.
See the OpenStack Configuration Reference for information about configuring volume drivers and creating and attaching volumes to server instances.
System administration By understanding how the different installed nodes interact with each other you can administer the Compute installation.
The Compute cloud works through the interaction of a series of daemon processes named nova-* that reside persistently on the host machine or machines.
These binaries can all run on the same machine or be spread out on multiple boxes in a large deployment.
The responsibilities of Services, Managers, and Drivers, can be a bit confusing at first.
Here is an outline the division of responsibilities to make understanding the system a little bit easier.
Currently, Services are nova-api, nova-objectstore (which can be replaced with Glance, the OpenStack Image Service), nova-compute, and nova-network.
Managers are responsible for a certain aspect of the system.
It is a logical grouping of code relating to a portion of the system.
In general other components should be using the manager to make changes to the components that it is responsible for.
Receives xml requests and sends them to the rest of the system.
It is a wsgi app that routes and authenticate requests.
There is a nova-api.conf file created when you install Compute.
It can be replaced with OpenStack Image Service and a simple image manager or use OpenStack Object Storage as the virtual machine image storage facility.
It loads a Service object which exposes the public methods on ComputeManager through Remote Procedure Call (RPC)
Responsible for managing floating and fixed IPs, DHCP, bridging and VLANs.
It loads a Service object which exposes the public methods on one of the subclasses of NetworkManager.
Different networking strategies are available to the service by changing the network_manager configuration option to FlatManager, FlatDHCPManager, or VlanManager (default is VLAN if no other is specified)
Compute service architecture These basic categories describe the service architecture and what's going on within the cloud controller.
At the heart of the cloud framework is an API server.
This API server makes command and control of the hypervisor, storage, and networking programmatically available to users in realization of the definition of cloud computing.
The API endpoints are basic HTTP web services which handle authentication, authorization, and basic command and control functions using various API interfaces under the Amazon, Rackspace, and related models.
This enables API compatibility with multiple existing tool sets created for interaction with offerings from other vendors.
A messaging queue brokers the interaction between compute nodes (processing), the networking controllers (software which controls network infrastructure), API endpoints, the scheduler (determines which physical hardware to allocate to a virtual resource), and similar components.
Communication to and from the cloud controller is by HTTP requests through multiple API endpoints.
A typical message passing event begins with the API server receiving a request from a user.
The API server authenticates the user and ensures that the user is permitted to issue the subject command.
Availability of objects implicated in the request is evaluated and, if available, the request is routed to the queuing engine for the relevant workers.
Workers continually listen to the queue based on their role, and occasionally their type host name.
When such listening produces a work request, the worker takes assignment of the task and begins its execution.
Upon completion, a response is dispatched to the queue which is received by the API server and relayed to the originating user.
Database entries are queried, added, or removed as necessary throughout the process.
The API dispatches commands to compute workers to complete these tasks:
The Network Controller manages the networking resources on host machines.
The API server dispatches commands through the message queue, which are subsequently processed by Network Controllers.
The user’s access key needs to be included in the request, and the request must be signed with the secret.
Upon receipt of API requests, Compute verifies the signature and runs commands on behalf of the user.
To begin using Compute, you must create a user with the Identity Service.
Manage the cloud A system administrator can use these tools to manage a cloud; the nova client, the novamanage command, and the Euca2ools commands.
The nova-manage command can only be run by cloud administrators.
Both nova client and euca2ools can be used by all users, though specific commands might be restricted by Role Based Access Control in the Identity Service.
You install the client, and then provide your user name and password, set as environment variables for convenience, and then you can have the ability to send commands to your cloud on the commandline.
Now that you have installed the python-novaclient, confirm the installation by entering:
This command returns a list of nova commands and parameters.
Set the required parameters as environment variables to make running commands easier.
You can add --os-username, for example, on the nova command, or set it as environment variables:
Using the Identity Service, you are supplied with an authentication endpoint, which nova recognizes as the OS_AUTH_URL.
The nova-manage command may be used to perform many essential functions for administration and ongoing maintenance of nova, such as network creation or user manipulation.
The man page for nova-manage has a good explanation for each of its functions, and is recommended reading for those starting out.
For administrators, the standard pattern for executing a nova-manage command is:
Run without arguments to see a list of available command categories:
You can also run with a category argument such as user to see a list of all commands in that category:
Show usage statistics for hosts and instances You can show basic statistics on resource usage for hosts and instances.
Note For more sophisticated monitoring, see the Ceilometer project, which is under development.
You can also use tools, such as Ganglia or Graphite, to gather more detailed data.
List the hosts and the nova-related services that run on them:
Get a summary of resource usage of all of the instances running on the host.
The cpu column shows the sum of the virtual CPUs for instances running on the host.
The memory_mb column shows the sum of the memory (in MB) allocated to the instances that run on the hosts.
The disk_gb column shows the sum of the root and ephemeral disk sizes (in GB) of the instances that run on the hosts.
The used_now row shows the sum of the resources allocated to the instances that run on the host plus the resources allocated to the virtual machine of the host itself.
The used_max row shows the sum of the resources allocated to the instances that run on the host.
These values are computed by using only information about the flavors of the instances that run on the hosts.
This command does not query the CPU usage, memory usage, or hard disk usage of the physical host.
Get CPU, memory, I/O, and network statistics for an instance.
The file must contain a section called logger_nova, for example:
This example sets the debugging level to INFO (which less verbose than the default DEBUG setting)
See the Python documentation on logging configuration file format for more details on this file, including the meaning of the handlers and quaname variables.
Syslog You can configure OpenStack Compute services to send logging information to syslog.
This is useful if you want to use rsyslog, which forwards the logs to a remote machine.
You need to separately configure the Compute service (Nova), the Identity Service (Keystone), the Image Service (Glance), and, if you are using it, the Block Storage Service (Cinder) to send log messages to syslog.
In addition to enabling syslog, these settings also turn off more verbose output and debugging output from the log.
For example, you may want to capture logging info at different severity levels for different services.
Rsyslog Rsyslog is a useful tool for setting up a centralized log server across multiple machines.
We briefly describe the configuration to set up an rsyslog server; a full treatment of rsyslog is beyond the scope of this document.
We assume rsyslog has already been installed on your hosts, which is the default on most Linux distributions.
The example below use compute-01 as an example of a compute host name:
Once you have created this file, restart your rsyslog daemon.
Error-level log messages on the compute hosts should now be sent to your log server.
Secure with root wrappers The root wrapper enables the Compute unprivileged user to run a number of actions as the root user in the safest manner possible.
Historically, Compute used a specific sudoers file that listed every command that the Compute user was allowed to run, and used sudo to run that command as root.
However this was difficult to maintain (the sudoers file was in packaging), and did not enable complex filtering of parameters (advanced filters)
A generic sudoers entry lets the Compute user run nova-rootwrap as root.
The nova-rootwrap code looks for filter definition directories in its configuration file, and loads command filters from them.
Then it checks if the command requested by Compute matches one of those filters, in which case it executes the command (as root)
Security model The escalation path is fully controlled by the root user.
A sudoers entry (owned by root) allows Compute to run (as root) a specific rootwrap executable, and only with a specific configuration file (which should be owned by root)
The configuration file (also root-owned) points to root-owned filter definition directories, which contain root-owned filters definition files.
This chain ensures that the Compute user itself is not in control of the configuration or modules used by the nova-rootwrap executable.
Details of rootwrap.conf You configure nova-rootwrap in the rootwrap.conf file.
Because it's in the trusted security path, it must be owned and writable by only the root user.
It uses an INI file format with these sections and parameters:
Directories defined on this line should all exist, be owned and writable only by the root user.
Filters definition files contain lists of filters that nova-rootwrap will use to allow or deny a specific command.
Since they are in the trusted security path, they need to be owned and writable only by the root user.
It uses an INI file format with a [Filters] section and several lines, each with a unique parameter name (different for each filter that you define):
Migration provides a scheme to migrate running instances from one OpenStack Compute server to another OpenStack Compute server.
Look at the running instances, to get the ID of the instance you wish to migrate.
Look at information associated with that instance - our example is vm1 from above.
In this example, HostC can be picked up because nova-compute is running on it.
If instances are still running on HostB, check log files (src/dest nova-compute and nova-scheduler) to determine why.
While the nova command is called live-migration, under the default Compute configuration options the instances are suspended before migration.
Recover from a failed compute node If you have deployed Compute with a shared file system, you can quickly recover from a failed compute node.
Of the two methods covered in these sections, the evacuate API is the preferred method even in the absence of shared storage.
The evacuate API provides many benefits over manual recovery, such as re-attachment of volumes and floating IPs.
Evacuate instances If a cloud compute node fails due to a hardware malfunction or another reason, you can evacuate instances to make them available again.
To preserve user data on server disk, you must configure shared storage on the target host.
Also, you must validate that the current VM host is down.
To find a different host for the evacuated instance, run this command to list hosts:
You can pass the instance password to the command by using the --password <pwd> option.
If you do not specify a password, one is generated and printed after the command finishes successfully.
The command evacuates an instance from a down host to a specified host.
The instance is booted from a new disk, but preserves its configuration including its ID, name, uid, IP address, and so on.
To preserve the user disk data on the evacuated server, deploy OpenStack Compute with shared file system.
Manual recovery For KVM/libvirt compute node recovery, see the previous section.
You can review the status of the host by using the nova database.
This example converts an EC2 API instance ID into an OpenStack ID - if you used the nova commands, you can substitute the ID directly.
You can find the credentials for your database in /etc/nova.conf.
Armed with the information of VMs on the failed host, determine to which compute host the affected VMs should move.
Run the following database command to move the VM to np-rcc46:
The important changes to make are to change the DHCPSERVER value to the host ip address of the Compute host that is the VMs new home, and update the VNC IP if it isn't already 0.0.0.0
In theory, the above database update and nova reboot command are all that is required to recover the VMs from a failed host.
Recover from a UID/GID mismatch When running OpenStack compute, using a shared file system or an automated configuration tool, you could encounter a situation where some files on your compute node are using the wrong UID or GID.
This causes a raft of errors, such as being unable to live migrate, or start virtual machines.
This basic procedure runs on nova-compute hosts, based on the KVM hypervisor, that could help to restore the situation:
Make sure you don't use numbers that are already used for some other user/group.
Set the nova uid in /etc/passwd to the same number in all hosts (for example, 112)
Set the libvirt-qemu uid in /etc/passwd to the same number in all hosts (for example, 119)
Set the nova group in /etc/group file to the same number in all hosts (for example, 120)
Set the libvirtd group in /etc/group file to the same number in all hosts (for example, 119)
Change all the files owned by user nova or by group nova.
Repeat the steps for the libvirt-qemu owned files if those were needed to change.
Now you can run the find command to verify that all files using the correct identifiers.
Compute disaster recovery process In this section describes how to manage your cloud after a disaster, and how to easily back up the persistent storage volumes.
A disaster could happen to several components of your architecture: a disk crash, a network loss, a power cut, and so on.
The disaster example is the worst one: a power loss.
Let's see what runs and how it runs before the crash:
From the SAN to the cloud controller, we have an active iscsi session (used for the "cindervolumes" LVM's VG)
From the cloud controller to the compute node we also have active iscsi sessions (managed by cinder-volume)
From the cloud controller to the compute node, we also have iptables/ ebtables rules which allows the access from the cloud controller to the running instance.
Now, after the power loss occurs and all hardware components restart, the situation is as follows:
From the SAN to the cloud, the ISCSI session no longer exists.
From the cloud controller to the compute node, the ISCSI sessions no longer exist.
From the cloud controller to the compute node, the iptables and ebtables are recreated, since, at boot, nova-network reapply the configurations.
From the cloud controller, instances turn into a shutdown state (because they are no longer running)
Into the database, data was not updated at all, since Compute could not have guessed the crash.
Before going further, and to prevent the admin to make fatal mistakes, the instances won't be lost, because no "destroy" or "terminate" command was invoked, so the files for the instances remain on the compute node.
Any extra step would be dangerous at this stage :
Get the current relation from a volume to its instance, so that you can recreate the attachment.
In other words, go from a shutdown to running state.
After the restart, you can reattach the volumes to their respective instances.
That step, which is not a mandatory one, exists in an SSH into the instances to reboot them.
You must get the current relationship from a volume to its instance, because we recreate the attachment.
Note that nova client includes the ability to get volume information from cinder.
You must restore for every volume, uses these queries to clean up the database:
Then, when you run nova volume-list commands, all volumes appear.
You can restart the instances through the nova reboot $instance.
At that stage, depending on your image, some instances completely reboot and become reachable, while others stop on the "plymouth" stage.
Do not reboot the ones that are stopped at that stage (see the fourth step)
In fact it depends on whether you added an /etc/fstab entry for that volume.
Images built with the cloud-init package remain in a pending state, while others skip the missing volume and start.
The idea of that stage is only to ask nova to reboot every instance, so the stored state is preserved.
After the restart, you can reattach the volumes to their respective instances.
Now that nova has restored the right status, it is time to perform the attachments through a nova volume-attach.
At that stage, instances that were pending on the boot sequence (plymouth) automatically continue their boot, and restart normally, while the ones that booted see the volume.
If some services depend on the volume, or if a volume has an entry into fstab, it could be good to simply restart the instance.
This restart needs to be made from the instance itself, not through nova.
So, we SSH into the instance and perform a reboot:
By completing this procedure, you can successfully recover your cloud.
Use the  errors=remount parameter in the fstab file, which prevents data corruption.
The system would lock any write to the disk if it detects an I/O error.
This configuration option should be added into the cinder-volume server (the one which performs the ISCSI connection to the SAN), but also into the instances' fstab file.
Do not add the entry for the SAN's disks to the cinder-volume's fstab file.
Some systems hang on that step, which means you could lose access to your cloudcontroller.
To re-run the session manually, you would run the following command before performing the mount:
For your instances, if you have the whole /home/ directory on the disk, instead of emptying the /home directory and map the disk on it, leave a user's directory with the user's bash files and the authorized_keys file.
This enables you to connect to the instance, even without the volume attached, if you allow only connections through public keys.
You can download from here a bash script which performs these steps:
The "test mode" allows you to perform that whole sequence for only one instance.
To reproduce the power loss, connect to the compute node which runs that same instance and close the iscsi session.
Do not detach the volume through nova volumedetach, but instead manually close the iscsi session.
In this example, the iscsi session is number 15 for that instance:
Troubleshoot Compute Common problems for Compute typically involve misconfigured networking or credentials that are not sourced properly in the environment.
Also, most flat networking configurations do not enable ping or ssh from a compute node to the instances that run on that node.
Compute log files Compute stores a log file for each service in /var/log/nova.
For example, novacompute.log is the log for the nova-compute service.
You can set the following options to format log strings for the nova.log module in the nova.conf file:
For information about what variables are available for the formatter see: http://docs.python.org/library/logging.html#formatter.
You have two options for logging for OpenStack Compute based on configuration settings.
Alternatively you can set use_syslog=1 so that the nova daemon logs to syslog.
Bugs are constantly being fixed, so online resources are a great way to get the most up-to-date errors and fixes.
Get get the novarc file from the project ZIP file, save existing credentials in case of override.
Generates novarc from the project ZIP file and sources it for you.
When you run nova-api the first time, it generates the certificate authority information, including openssl.cnf.
If you start the CA services before this, you might not be able to create your ZIP file.
When your CA information is available, create your ZIP file.
Also, check your HTTP proxy settings to see whether they cause problems with novarc creation.
Sometimes a particular instance shows pending or you cannot SSH to it.
For example, when you use flat manager networking, you do not have a DHCP server and certain images do not support interface injection; you cannot connect to them.
The fix for this problem is to use an image that does support this method, such as Ubuntu, which obtains an IP address correctly with FlatManager network settings.
To troubleshoot other possible problems with an instance, such as an instance that stays in a spawning state, check the directory for the particular instance under /var/lib/nova/ instances on the nova-compute host and make sure that these files are present:
If any files are missing, empty, or very small, the nova-compute service did not successfully download the images from the Image Service.
Reset the state of an instance If an instance remains in an intermediate state, such as deleting, you can use the nova reset-state command to manually reset the state of an instance to an error state.
You can also use the --active parameter to force the instance back to an active state instead of an error state.
Injection problems If instances do not boot or boot slowly, investigate file injection as a cause.
If you have not enabled the configuration drive and you want to make userspecified files available from the metadata server for to improve performance and avoid boot failure if injection fails, you must disable injection.
Introduction to Object Storage OpenStack Object Storage (code-named Swift) is open source software for creating redundant, scalable data storage using clusters of standardized servers to store petabytes of accessible data.
It is a long-term storage system for large amounts of static data that can be retrieved, leveraged, and updated.
Object Storage uses a distributed architecture with no central point of control, providing greater scalability, redundancy, and permanence.
Objects are written to multiple hardware devices, with the OpenStack software responsible for ensuring data replication and integrity across the cluster.
Should a node fail, OpenStack works to replicate its content from other active nodes.
Because OpenStack uses software logic to ensure data replication and distribution across different devices, inexpensive commodity hard drives and servers can be used in lieu of more expensive equipment.
It provides a fully distributed, API-accessible storage platform that can be integrated directly into applications or used for backup, archiving, and data retention.
HDD/node failure agnostic Self-healing, reliable, data redundancy protects from failures.
Unlimited storage Large and flat namespace, highly scalable read/write access, able to serve content directly from storage system.
A configurable number of accounts, containers and object copies for high availability.
Easily add capacity (unlike RAID resize) Elastic data scaling with ease.
Expiring objects Users can set an expiration time or a TTL on an object to control access.
Direct object access Enable direct browser access to content, such as for a control panel.
Realtime visibility into client requests Know what users are requesting.
Restrict containers per account Limit access to control usage by user.
Support for NetApp, Nexenta, SolidFire Unified support for block volumes using a variety of storage systems.
Snapshot and backup API for block volumes Data protection and recovery for VM data.
Standalone volume API available Separate endpoint and API for integration with other compute systems.
Integration with Compute Fully integrated with Compute for attaching block volumes and reporting on usage.
Developers interact with the object storage system through a RESTful HTTP API.
The cluster scales by adding additional nodes without sacrificing performance, which allows a more cost-effective linear storage expansion than fork-lift upgrades.
Data doesn't have to be migrate to an entirely new storage system.
New nodes can be added to the cluster without downtime.
Failed nodes and disks can be swapped out without downtime.
It runs on industry-standard hardware, such as Dell, HP, and Supermicro.
Developers can either write directly to the Swift API or use one of the many client libraries that exist for all of the popular programming languages, such as Java, Python, Ruby, and C#
Users new to object storage systems will have to adjust to a different approach and mindset than those required for a traditional filesystem.
Components The components that enable Object Storage to deliver high availability, high durability, and high concurrency are:
Proxy servers Proxy servers are the public face of Object Storage and handle all of the incoming API requests.
Proxy servers also coordinate responses, handle failures, and coordinate timestamps.
Proxy servers use a shared-nothing architecture and can be scaled as needed based on projected workloads.
A minimum of two proxy servers should be deployed for redundancy.
Rings A ring represents a mapping between the names of entities stored on disk and their physical locations.
When other components need to perform any operation on an object, container, or account, they need to interact with the appropriate ring to determine their location in the cluster.
The ring maintains this mapping using zones, devices, partitions, and replicas.
Each partition in the ring is replicated, by default, three times across the cluster, and partition locations are stored in the mapping maintained by the ring.
The ring is also responsible for determining which devices are used for handoff in failure scenarios.
Each partition replica is guaranteed to reside in a different zone.
A zone could represent a drive, a server, a cabinet, a switch, or even a data center.
The partitions of the ring are equally divided among all of the devices in the Object Storage installation.
When partitions need to be moved around (for example, if a device is added to the cluster), the ring ensures that a minimum number of partitions are moved at a time, and only one replica of a partition is moved at a time.
Weights can be used to balance the distribution of partitions on drives across the cluster.
This can be useful, for example, when differently sized drives are used in a cluster.
The ring is used by the proxy server and several background processes (like replication)
These rings are externally managed, in that the server processes themselves do not modify the rings, they are instead given new rings modified by other tools.
The ring uses a configurable number of bits from a path’s MD5 hash as a partition index that designates a device.
The number of bits kept from the hash is known as the partition power, and 2 to the partition power indicates the partition count.
Partitioning the full MD5 hash ring allows other parts of the cluster to work in batches of items at once which ends up either more efficient or at least less complex than working with each item separately or the entire cluster all at once.
Another configurable value is the replica count, which indicates how many of the partitiondevice assignments make up a single ring.
For a given partition number, each replica’s device will not be in the same zone as any other replica's device.
Zones can be used to group devices based on physical locations, power separations, network separations, or any other attribute that would improve the availability of multiple replicas at the same time.
Zones Object Storage allows configuring zones in order to isolate failure boundaries.
Each data replica resides in a separate zone, if possible.
At the smallest level, a zone could be a single drive or a grouping of a few drives.
If there were five object storage servers, then each server would represent its own zone.
Larger deployments would have an entire rack (or multiple racks) of object servers, each representing a zone.
The goal of zones is to allow the cluster to tolerate significant outages of storage servers without losing all replicas of the data.
As mentioned earlier, everything in Object Storage is stored, by default, three times.
This means that when chosing a replica location, Object Storage chooses a server in an unused zone before an unused server in a zone that already has a replica of the data.
When a disk fails, replica data is automatically distributed to the other zones to ensure there are three copies of the data.
Accounts and containers Each account and container is an individual SQLite database that is distributed across the cluster.
An account database contains the list of containers in that account.
A container database contains the list of objects in that container.
To keep track of object data locations, each account in the system has a database that references all of its containers, and each container database references each object.
Partitions A partition is a collection of stored data, including account databases, container databases, and objects.
Think of a partition as a bin moving throughout a fulfillment center warehouse.
The system treats that bin as a cohesive entity as it moves throughout the system.
A bin is easier to deal with than many little things.
As the system scales up, its behavior continues to be predictable because the number of partitions is a fixed number.
Implementing a partition is conceptually simple#a partition is just a directory sitting on a disk with a corresponding hash table of what it contains.
Replicators In order to ensure that there are three copies of the data everywhere, replicators continuously examine each partition.
For each local partition, the replicator compares it against the replicated copies in the other zones to see if there are any differences.
The replicator knowd if replication needs to take plac by examining hashes.
A hash file is created for each partition, which contains hashes of each directory in the partition.
For a given partition, the hash files for each of the partition's copies are compared.
If the hashes are different, then it is time to replicate, and the directory that needs to be replicated is copied over.
With fewer things in the system, larger chunks of data are transferred around (rather than lots of little TCP connections, which is inefficient) and there is a consistent number of hashes to compare.
The cluster eventually has a consistent behavior where the newest data has a priority.
If a zone goes down, one of the nodes containing a replica notices and proactively copies data to a handoff location.
Use cases The following sections show use cases for object uploads and downloads and introduce the components.
A client uses the REST API to make a HTTP request to PUT an object into an existing container.
First, the system must figure out where the data is going to go.
To do this, the account name, container name, and object name are all used to determine the partition where this object should live.
Then a lookup in the ring figures out which storage nodes contain the partitions in question.
The data is then sent to each storage node where it is placed in the appropriate partition.
At least two of the three writes must be successful before the client is notified that the upload was successful.
Next, the container database is updated asynchronously to reflect that there is a new object in it.
Using the same consistent hashing, the partition name is generated.
A lookup in the ring reveals which storage nodes contain that partition.
A request is made to one of the storage nodes to fetch the object and, if that fails, requests are made to the other nodes.
Ring-builder Use the swift-ring-builder utility to build and manage rings.
This utility assigns partitions to devices and writes an optimized Python structure to a gzipped, serialized file on disk for transmission to the servers.
The server processes occasionally check the modification time of the file and reload in-memory copies of the ring structure as needed.
If you use a slightly older version of the ring, one of the three replicas for a partition subset will be incorrect because of the way the ring-builder manages changes to the ring.
The ring-builder also keeps its own builder file with the ring information and additional data required to build future rings.
It is very important to keep multiple backup copies of these builder files.
One option is to copy the builder files out to every server while copying.
Another is to upload the builder files into the cluster itself.
If you lose the builder file, you have to create a new ring from scratch.
Nearly all partitions would be assigned to different devices and, therefore, nearly all of the stored data would have to be replicated to new locations.
So, recovery from a builder file loss is possible, but data would be unreachable for an extended time.
Ring data structure The ring data structure consists of three top level fields: a list of devices in the cluster, a list of lists of device ids indicating partition to device assignments, and an integer indicating the number of bits to shift an MD5 hash to calculate the partition for the hash.
Partition assignment list This is a list of array(‘H’) of devices ids.
Each array(‘H’) has a length equal to the partition count for the ring.
Each integer in the array(‘H’) is an index into the above list of devices.
So, to create a list of device dictionaries assigned to a partition, the Python code would look like:
That code is a little simplistic because it does not account for the removal of duplicate devices.
If a ring has more replicas than devices, a partition will have more than one replica on a device.
Replica counts To support the gradual change in replica counts, a ring can have a real number of replicas and is not restricted to an integer number of replicas.
A fractional replica count is for the whole ring and not for individual partitions.
It indicates the average number of replicas for each partition.
You must rebalance the replica ring in globally distributed clusters.
Operators of these clusters generally want an equal number of replicas and regions.
Therefore, when an operator adds or removes a region, the operator adds or removes a replica.
You can gradually increase the replica count at a rate that does not adversely affect cluster performance.
Additionally, swift-ring-builder X.builder create can now take a decimal argument for the number of replicas.
Partition shift value The partition shift value is known internally to the Ring class as _part_shift.
This value is used to shift an MD5 hash to calculate the partition where the data for that hash should reside.
Only the top four bytes of the hash is used in this process.
Build the ring The ring builder process includes these high-level steps:
The utility calculates the number of partitions to assign to each device based on the weight of the device.
One thousand devices of equal weight will each want 1,048.576 partitions.
The devices are sorted by the number of partitions they desire and kept in order throughout the initialization process.
Note Each device is also assigned a random tiebreaker value that is used when two devices desire the same number of partitions.
This tiebreaker is not stored on disk anywhere, and so two different rings created with the same parameters will have different partition assignments.
The ring builder assigns each partition replica to the device that requires most partitions at that point while keeping it as far away as possible from other replicas.
If no such region is available, the ring builder searches for a device in a different zone, or on a different server.
If it does not find one, it looks for a device with no replicas.
Finally, if all options are exhausted, the ring builder assigns the replica to the device that has the fewest replicas already assigned.
The ring builder assigns multiple replicas to one device only if the ring has fewer devices than it has replicas.
When building a new ring from an old ring, the ring builder recalculates the desired number of partitions that each device wants.
The ring builder unassigns partitions and gathers these partitions for reassignment, as follows:
The ring builder unassigns any assigned partitions from any removed devices and adds these partitions to the gathered list.
The ring builder unassigns any partition replicas that can be spread out for better durability and adds these partitions to the gathered list.
The ring builder unassigns random partitions from any devices that have more partitions than they need and adds these partitions to the gathered list.
The ring builder reassigns the gathered partitions to devices by using a similar method to the one described previously.
When the ring builder reassigns a replica to a partition, the ring builder records the time of the reassignment.
The ring builder uses this value when it gathers partitions for reassignment so that no partition is moved twice in a configurable amount of time.
The RingBuilder class knows this configurable amount of time as min_part_hours.
The ring builder ignores this restriction for replicas of partitions on removed devices because removal of a device happens on device failure only, and reassignment is the only choice.
Theses steps do not always perfectly rebalance a ring due to the random nature of gathering partitions for reassignment.
Large-scale deployments segment off an access tier, which is considered the Object Storage system's central hub.
The access tier fields the incoming API requests from clients and moves data in and out of the system.
This tier consists of front-end load balancers, sslterminators, and authentication services.
It runs the (distributed) brain of the Object Storage system#the proxy server processes.
Because access servers are collocated in their own tier, you can scale out read/write access regardless of the storage capacity.
For example, if a cluster is on the public Internet, requires SSL termination, and has a high demand for data access, you can provision many access servers.
However, if the cluster is on a private network and used primarily for archival purposes, you need fewer access servers.
Since this is an HTTP addressable storage service, you may incorporate a load balancer into the access tier.
Typically, the tier consists of a collection of 1U servers.
These machines use a moderate amount of RAM and are network I/O intensive.
For most publicly facing deployments as well as private deployments available across a wide-reaching corporate network, you use SSL to encrypt traffic to the client.
Storage nodes In most configurations, each of the five zones should have an equal amount of storage capacity.
Storage nodes use a reasonable amount of memory and CPU.
Metadata needs to be readily available to return objects quickly.
The object stores run services not only to field incoming requests from the access tier, but to also run replicators, auditors, and reapers.
You can provision object stores provisioned with single gigabit or 10 gigabit network interface depending on the expected workload and desired performance.
You can use desktop-grade drives if you have responsive remote hands in the datacenter and enterprisegrade drives if you don't.
Factors to consider You should keep in mind the desired I/O performance for single-threaded requests.
This system does not use RAID, so a single disk handles each request for an object.
To achieve apparent higher throughput, the object storage system is designed to handle concurrent uploads/downloads.
Replication Because each replica in Object Storage functions independently and clients generally require only a simple majority of nodes to respond to consider an operation successful, transient failures like network partitions can quickly cause replicas to diverge.
These differences are eventually reconciled by asynchronous, peer-to-peer replicator processes.
The replicator processes traverse their local file systems and concurrently perform operations in a manner that balances load across physical disks.
Replication uses a push model, with records and files generally only being copied from local to remote replicas.
This is important because data on the node might not belong there (as in the case of hand offs and ring changes), and a replicator cannot know which data it should pull in from elsewhere in the cluster.
Any node that contains data must ensure that data gets to where it belongs.
To replicate deletions in addition to creations, every deleted record or file in the system is marked by a tombstone.
The replication process cleans up tombstones after a time period known as the consistency window.
This window defines the duration of the replication and how long transient failure can remove a node from the cluster.
Tombstone cleanup must be tied to replication to reach replica convergence.
If a replicator detects that a remote drive has failed, the replicator uses the get_more_nodes interface for the ring to choose an alternate node with which to synchronize.
The replicator can maintain desired levels of replication during disk failures, though some replicas might not be in an immediately usable location.
The replicator does not maintain desired levels of replication when failures such as entire node failures occur; most failures are transient.
Database replication Database replication completes a low-cost hash comparison to determine whether two replicas already match.
Normally, this check can quickly verify that most databases in the system are already synchronized.
If the hashes differ, the replicator synchronizes the databases by sharing records added since the last synchronization point.
This synchronization point is a high water mark that notes the last record at which two databases were known to be synchronized, and is stored in each database as a tuple of the remote database ID and record ID.
Database IDs are unique across all replicas of the database, and record IDs are monotonically increasing integers.
After all new records are pushed to the remote database, the entire synchronization table of the local database is pushed, so the remote database can guarantee that it is synchronized with everything with which the local database was previously synchronized.
If a replica is missing, the whole local database file is transmitted to the peer by using rsync(1) and is assigned a new unique ID.
In practice, database replication can process hundreds of databases per concurrency setting per second (up to the number of available CPUs or disks) and is bound by the number of database transactions that must be performed.
Object replication The initial implementation of object replication performed an rsync to push data from a local partition to all remote servers where it was expected to reside.
While this worked at small scale, replication times skyrocketed once directory structures could no longer be held in RAM.
This scheme was modified to save a hash of the contents for each suffix directory to a per-partition hashes file.
The hash for a suffix directory is no longer valid when the contents of that suffix directory is modified.
The object replication process reads in hash files and calculates any invalidated hashes.
Then, it transmits the hashes to each remote server that should hold the partition, and only suffix directories with differing hashes on the remote server are rsynced.
After pushing files to the remote server, the replication process notifies it to recalculate hashes for the rsynced suffix directories.
The number of uncached directories that object replication must traverse, usually as a result of invalidated suffix directory hashes, impedes performance.
To provide acceptable replication speeds, object replication is designed to invalidate around 2 percent of the hash space on a normal node each day.
An OpenStack Object Storage cluster is a collection of many daemons that work together across many nodes.
With so many different components, you must be able to tell what is going on inside the cluster.
Tracking server-level metrics like CPU utilization, load, memory consumption, disk usage and utilization, and so on is necessary, but not sufficient.
What are different daemons are doing on each server? What is the volume of object replication on node8? How long is it taking? Are there errors? If so, when did they happen?
In such a complex ecosystem, you can use multiple approaches to get the answers to these questions.
Swift Recon The Swift Recon middleware (see http://swift.openstack.org/admin_guide.html#clustertelemetry-and-monitoring) provides general machine statistics, such as load average, socket statistics, /proc/meminfo contents, and so on, as well as Swift-specific metrics:
Count of each type of quarantined file: Account, container, or object.
Swift Recon is middleware that is installed in the object servers pipeline and takes one required option: A local cache directory.
To track async_pendings, you must set up an additional cron job for each object server.
You access data by either sending HTTP requests directly to the object server or using the swift-recon command-line client.
There are some good Object Storage cluster statistics but the general server metrics overlap with existing server monitoring systems.
To get the Swift-specific metrics into a monitoring system, they must be polled.
The process that feeds metrics to your statistics system, such as collectd and gmond, probably already runs on the storage node.
So, you can choose to either talk to Swift Recon or collect the metrics directly.
I am not sure what the Etsy server does but our StatsD server turns timing metrics into five derivative metrics with new segments appended, so it probably works as coded.
This is good for getting a feel for the quality of service clients are experiencing with the timing metrics, as well as getting a feel for the volume of the various permutations of request server type, command, and response code.
Swift-Informant also requires no change to core Object Storage code because it is implemented as middleware.
However, it gives you no insight into the workings of the cluster past the proxy server.
If the responsiveness of one storage node degrades, you can only see that some of your requests are bad, either as high latency or error status codes.
You do not know exactly why or where that request tried to go.
Maybe the container server in question was on a good node but the object server was on a different, poorly-performing node.
Statsdlog Florian’s Statsdlog project increments StatsD counters based on logged events.
Like SwiftInformant, it is also non-intrusive, but statsdlog can track events from all Object Storage daemons, not just proxy-server.
The daemon listens to a UDP stream of syslog messages and StatsD counters are incremented when a log line matches a regular expression.
Metric names are mapped to regex match patterns in a JSON file, allowing flexible configuration of what metrics are extracted from the log stream.
Currently, only the first matching regex triggers a StatsD counter increment, and the counter is always incremented by one.
There is no way to increment a counter by more than one or send timing data to StatsD based on the log line content.
The tool could be extended to handle more metrics for each line and data extraction, including timing data.
But a coupling would still exist between the log textual format and the log parsing regexes, which would themselves be more complex to support multiple matches for each line and data extraction.
Also, log processing introduces a delay between the triggering event and sending the data to StatsD.
It would be preferable to increment error counters where they occur and send timing data as soon as it is known to avoid coupling between a log string and a parsing regex and prevent a time delay between events and sending data to StatsD.
The next section describes another method for gathering Object Storage operational metrics.
Swift StatsD logging StatsD (see http://codeascraft.etsy.com/2011/02/15/measure-anything-measureeverything/) was designed for application code to be deeply instrumented; metrics are sent in real-time by the code that just noticed or did something.
The overhead of sending a metric is extremely low: a sendto of one UDP packet.
If that overhead is still too high, the StatsD client library can send only a random portion of samples and StatsD approximates the actual number when flushing metrics upstream.
To avoid the problems inherent with middleware-based monitoring and after-the-fact log processing, the sending of StatsD metrics is integrated into Object Storage itself.
Details of the metrics tracked are in the Administrator's Guide.
The sending of metrics is integrated with the logging framework.
You can also specify the port and a default sample rate.
The specified default sample rate is used unless a specific call to a statsd logging method (see the list below) overrides it.
Then the LogAdapter object returned by get_logger(), usually stored in self.logger, has these new methods:
This is used when you need to add or subtract.
Note that these logging methods may safely be called anywhere you have a logger object.
If StatsD logging has not been configured, the methods are no-ops.
This avoids messy conditional logic each place a metric is recorded.
The development team of StatsD wanted to use the pystatsd client library (not to be confused with a similar-looking project also hosted on GitHub), but the released version on PyPi was missing two desired features the latest version in GitHub had: the ability to configure a metrics prefix in the client object and a convenience method for sending timing data between “now” and a “start” timestamp you already have.
So they just implemented a simple StatsD client library from scratch with the same interface.
This has the nice fringe benefit of not introducing another external library dependency into Object Storage.
Troubleshoot Object Storage For Object Storage, everything is logged in /var/log/syslog (or messages on some distros)
Drive failure In the event that a drive has failed, the first step is to make sure the drive is unmounted.
This will make it easier for Object Storage to work around the failure until it has been resolved.
If the drive is going to be replaced immediately, then it is just best to replace the drive, format it, remount it, and let replication fill it up.
If the drive can’t be replaced immediately, then it is best to leave it unmounted, and remove the drive from the ring.
This will allow all the replicas that were on that drive to be replicated elsewhere until the drive is replaced.
Once the drive is replaced, it can be readded to the ring.
Server failure If a server is having hardware issues, it is a good idea to make sure the Object Storage services are not running.
This will allow Object Storage to work around the failure while you troubleshoot.
If the server just needs a reboot, or a small amount of work that should only last a couple of hours, then it is probably best to let Object Storage work around the failure and get the machine fixed and back online.
When the machine comes back online, replication will make sure that anything that is missing during the downtime will get updated.
If the server has more serious issues, then it is probably best to remove all of the server’s devices from the ring.
Once the server has been repaired and is back online, the server’s devices can be added back into the ring.
It is important that the devices are reformatted before putting them back into the ring as it is likely to be responsible for a different set of partitions than before.
There is a script called swift-drive-audit that can be run via cron to.
If errors are detected, it will unmount the bad drive, so that Object Storage can work around it.
The script takes a configuration file with the following settings:
This script has only been tested on Ubuntu 10.04, so if you are using a different distro or OS, some care should be taken before using in production.
Emergency recovery of ring builder files You should always keep a backup of Swift ring builder files.
However, if an emergency occurs, this procedure may assist in returning your cluster to an operational state.
Using existing Swift tools, there is no way to recover a builder file from a ring.gz file.
However, if you have a knowledge of Python, it is possible to construct a builder file that is pretty close to the one you have lost.
This procedure is a last-resort for emergency circumstances#it requires knowledge of the swift python code and may not succeed.
First, load the ring and a new ringbuilder object in a Python REPL:
Now, start copying the data we have in the ring into the builder.
For min_part_hours you'll either have to remember what the value you used was, or just make up a new one.
Try some validation: if this doesn't raise an exception, you may feel some hope.
The OpenStack Block Storage service works though the interaction of a series of daemon processes named cinder-* that reside persistently on the host machine or machines.
The binaries can all be run from a single node, or spread across multiple nodes.
They can also be run on the same node as other OpenStack services.
Introduction to Block Storage To administer the OpenStack Block Storage service, it is helpful to understand a number of concepts.
You must make certain choices when you configure the Block Storage service in OpenStack.
The bulk of the options come down to two choices, single node or multi-node install.
You can read a longer discussion about storage decisions in Storage Decisions in the OpenStack Operations Guide.
The OpenStack Block Storage service is not a shared storage solution like a Storage Area Network (SAN) of NFS volumes, where you can attach a volume to multiple servers.
With the OpenStack Block Storage service, you can attach a volume to only one instance at a time.
The OpenStack Block Storage service also provides drivers that enable you to use several vendors' back-end storage devices, in addition to or instead of the base LVM implementation.
This high-level procedure shows you how to create and attach a volume to a server instance.
You must configure both OpenStack Compute and the OpenStack Block Storage service through the cinder.conf file.
Attach the volume to an instance through the nova volume-attach command.
This command creates a unique iSCSI IQN that is exposed to the compute node.
The compute node, which runs the instance, now has an active ISCSI session and new local storage (usually a /dev/sdX disk)
The instance get a new disk, usually a /dev/vdX disk.
For this particular walk through, one cloud controller runs nova-api, nova-scheduler, nova-objectstore, nova-network and cinder-* services.
The walk through uses a custom partitioning scheme that carves out 60 GB of space and labels it as LVM.
To set up Compute to use volumes, ensure that Block Storage is installed along with lvm2
This guide describes how to troubleshoot your installation and back up your Compute volumes.
Boot from volume In some cases, instances can be stored and run from inside volumes.
For information, see the Launch an instance from a volume section in the OpenStack End User Guide.
Configure a multiple-storage back-end This section presents the multi back-end storage feature introduced with the Grizzly release.
Multi back-end allows the creation of several back-end storage solutions serving the same OpenStack Compute configuration.
In that case, the scheduler properly decides which back-end the volume has to be created in.
When a volume is created, the scheduler chooses an appropriate back-end to handle the request, according to the volume type specified by the user.
To enable a multi back-end configuration, you must set the enabled_backends flag in the cinder.conf file.
This flag defines the names (separated by a comma) of the configuration groups for the different back-ends: one name is associated to one configuration group for a back-end (such as, [lvmdriver-1])
The options for a configuration group must be defined in the group (or default options are used)
Configuration values in the [DEFAULT] configuration group are not used.
You must enable the filter_scheduler option to use multi back-end.
The CapacityWeigher attributes higher scores to back-ends with the most available.
The scheduler uses the filtering and weighing process to pick the best back-end to handle the request, and explicitly creates volumes on specific back-ends through the use of volume types.
To enable the filter scheduler, add this line to the cinder.conf configuration file:
While the Block Storage Scheduler defaults to filter_scheduler in Grizzly, this setting is not required.
Before using it, a volume type has to be declared to Block Storage.
When you create a volume, you must specify the volume type.
Back up Block Storage Service disks While you can use the LVM snapshot to create snapshots, you can also use it to back up your volumes.
By using LVM snapshot, you reduce the size of the backup; only existing data is backed up instead of the entire volume.
To back up a volume, you must create a snapshot of it.
An LVM snapshot is the exact copy of a logical volume, which contains data in a frozen state.
This prevents data corruption, because data cannot be manipulated during the volume creation process.
Remember that the volumes created through a nova volume-create command exist in an LVM logical volume.
Before you create the snapshot, you must have enough space to save it.
As a precaution, you should have at least twice as much space as the potential snapshot size.
If insufficient space is available, the snapshot might become corrupted.
This example uses these commands to back up only those 4 GB:
You can apply this process to volumes of any size.
Create the snapshot; you can do this while the volume is attached to an instance:
Use the --snapshot configuration option to tell LVM that you want a snapshot of an already existing volume.
The command includes the size of the space reserved for the snapshot volume, the name of the snapshot, and the path of an already existing volume.
The size does not have to be the same as the volume of the snapshot.
The size parameter defines the space that LVM reserves for the snapshot volume.
As a precaution, the size should be the same as that of the original volume, even if the whole space is not currently used by the snapshot.
To exploit the snapshot with the tar command, mount your partition on the Block Storage Service server.
You can use it to view partitions that are created inside the instance.
Without using the partitions created inside instances, you cannot see its content and create efficient backups.
Note On a Debian-based distribution, you can also use the apt-get install kpartx command.
If the tools successfully find and map the partition table, no errors are returned.
If a message prompts you for a partition or you cannot mount it, determine whether enough space was allocated for the snapshot or the kpartx command failed to discover the partition table.
Allocate more space to the snapshot and try the process again.
This command creates a tar.gz file that contains the data, and data only.
This ensures that you do not waste space by backing up empty sectors.
You should always have the checksum for your backup files.
When you transfer the same file over the network, you can run a checksum calculation to ensure that your file was not corrupted during its transfer.
Run this command to run a checksum for your file and save the result to a file:
Use the sha1sum command carefully because the time it takes to complete the calculation is directly proportional to the size of the file.
Now that you have an efficient and consistent backup, use this command to clean up the file system:
Because more and more volumes might be allocated to your Block Storage service, you might want to automate your backups.
Launch this script from the server that runs the Block Storage Service.
The script also enables you to SSH to your instances and run a mysqldump command into them.
To make this work, enable the connection to the Compute project keys.
Migrate volumes The Havana release of OpenStack introduces the ability to migrate volumes between back-ends.
Migrating a volume transparently moves its data from the current back-end for the volume to a new one.
This is an administrator function, and can be used for functions including storage evacuation (for maintenance or decommissioning), or manual optimizations (for example, performance, reliability, or cost)
If the storage can migrate the volume on its own, it is given the opportunity to do so.
This allows the Block Storage driver to enable optimizations that the storage might be able to perform.
If the back-end is not able to perform the migration, the Block Storage Service uses one of two generic flows, as follows.
If the volume is not attached, the Block Storage Service creates a volume and copies the data from the original to the new volume.
Note: While most back-ends support this function, not all do.
See driver documentation in the OpenStack Configuration Reference for more details.
If the volume is attached to a VM instance, the Block Storage Service creates a volume, and calls Compute to copy the data from the original to the new volume.
Currently this is supported only by the Compute libvirt driver.
As an example, this scenario shows two LVM back-ends and migrates an attached volume from one to the other.
Next, as the admin user, you can see the current status of the volume (replace the example ID with your own):
During the course of a migration, if you create a volume and copy over the data, the volume get the new name but keeps its original ID.
You can use the cinder show command to see the status of the migration.
While migrating, the migstat attribute shows states such as migrating or completing.
On error, migstat is set to None and the host attribute shows the original host.
Note that migstat is None, host is the new host, and name_id holds the ID of the volume created by the migration.
The migration is not visible to non-admin users (for example, through the volume status)
If a user performs such an action during a migration, an error is returned.
Troubleshoot your installation This section provides useful tips to help troubleshoot your Block Storage Service installation.
Troubleshoot the Block Storage configuration This section helps you solve some basic and common errors that you might encounter during setup and configuration of the Block Storage Service.
The most important thing to know is where to look in case of a failure.
Two log files are especially helpful for solving volume creation failures, the cinderapi log and the cinder-volume log.
The cinder-api log is useful for determining if you have endpoint or connectivity issues.
If you send a request to create a volume and it fails, review the cinder-api log to determine whether the request made it to the Block Storage service.
If the request is logged and you see no errors or trace-backs, check the cinder-volume log for errors or trace-backs.
Print debugging output (set logging level to DEBUG instead # of default WARNING level)
Print more verbose output (set logging level to INFO instead # of default WARNING level)
Please see the Python logging module # documentation for details on logging configuration files.
The OpenStack Block Storage uses tgtd as the default iscsi helper and implements persistent targets.
This means that in the case of a tgt restart or even a node reboot your existing volumes on that node will be restored automatically with their original IQN.
In order to make this possible the iSCSI target information needs to be stored in a file on creation that can be queried in case of restart of the tgt daemon.
By default, Block Storage uses a state_path variable, which if installing with Yum or APT should be set to /var/lib/cinder/
While this should all be handled by the installer, it can go wrong.
If you have trouble creating volumes and this directory does not exist you should see an error message in the cinder-volume log indicating that the volumes_dir does not exist, and it should provide information about which path it was looking for.
Along with the volumes_dir option, the iSCSI target driver also needs to be configured to look in the correct place for the persist files.
This is a simple entry in the /etc/tgt/ conf.d file that you should have set when you installed OpenStack.
If the file is not present, create it with this command:
This is most likely going to be a minor adjustment to your nova.conf file.
Restart tgt and cinder-* services so they pick up the new configuration.
This warning occurs in the Compute log if you do not have the optional multipath-tools package installed on the Compute node.
This is an optional package and the volume attachment does work without the multipath tools installed.
If the multipath-tools package is installed on the Compute node, it is used to perform the volume attachment.
The IDs in your message are unique to your system.
Solution Run the following command on the Compute node to install the multipath-tools packages.
Failed to attach volume to an instance, sg_scan file not found.
This warning and error occur when the sg3-utils package is not installed on the Compute node.
The IDs in your message are unique to your system:
Run this command on the Compute node to install the sg3-utils package:
Unless you really know what you're doing, make sure that only one greenthread can read any particular socket.
You need to update your copy of the hp_3par_fc.py driver which contains the synchronization code.
The JSON serializable issue is caused by an RPC response timeout.
Make sure your iptables allow port 3260 communication on the ISC controller.
If the port communication is properly configured, you can try running this command.
If you continue to get the RPC response time out, your ISC controller and KVM host might be incompatible.
This error may be caused by a volume being exported outside of OpenStack using a host name different from the system name that OpenStack expects.
This error could be displayed with the IQN if the host was exported using iSCSI.
Change the 3PAR host name to match the one that OpenStack expects.
The 3PAR host constructed by the driver uses just the local hostname, not the fully qualified domain name (FQDN) of the compute host.
Failed to attach a volume after detaching the same volume.
You must change the device name on the nova-attach command.
The VM might not clean up after a nova-detach command runs.
This example shows how the nova-attach command fails when you use the vdb, vdc, or vdd device names:
You might also have this problem after attaching and detaching the same volume from the same VM with the same mount point multiple times.
This warning and error occurs if you do not have the required sysfsutils package installed on the Compute node.
Run the following command on the Compute node to install the sysfsutils packages.
Compute node failed to connect to a volume in a Fibre Channel (FC) SAN configuration.
The WWN may not be zoned correctly in your FC SAN that links the Compute host to the storage array.
The network administrator must configure the FC SAN fabric by correctly zoning the WWN (port names) from your Compute node HBAs.
When you attempt to create a VM, the error shows the VM is in the BUILD then ERROR state.
Follow the instructions in the  enabling KVM section of the Configuration Reference to enable hardware virtualization support in your BIOS.
This error could be caused by a volume being exported outside of OpenStack using a host name different from the system name that OpenStack expects.
This error could be displayed with the IQN if the host was exported using iSCSI.
Host names constructed by the driver use just the local hostname, not the fully qualified domain name (FQDN) of the Compute host.
This error occurs if the 3PAR host exists with the correct host name that the OpenStack Block Storage drivers expect but the volume was created in a different Domain.
Learn Networking concepts, architecture, and basic and advanced neutron and nova command-line interface (CLI) cloud.
Introduction to Networking The Networking service, code-named Neutron, provides an API that lets you define network connectivity and addressing in the cloud.
The Networking service enables operators to leverage different networking technologies to power their cloud networking.
The Networking service also provides an API to configure and manage a variety of network services ranging from L3 forwarding and NAT to load balancing, edge firewalls, and IPSEC VPN.
For a detailed description of the Networking API abstractions and their attributes, see the OpenStack Networking API v2.0 Reference.
Networking API Networking is a virtual network service that provides a powerful API to define the network connectivity and IP addressing used by devices from other services, such as Compute.
The Compute API has a virtual server abstraction to describe computing resources.
Similarly, the Networking API has virtual network, subnet, and port abstractions to describe networking resources.
Network An isolated L2 segment, analogous to VLAN in the physical networking world.
Port A connection point for attaching a single device, such as the NIC of a virtual server, to a virtual network.
Also describes the associated network configuration, such as the MAC and IP addresses to be used on that port.
You can configure rich network topologies by creating and configuring networks and subnets, and then instructing other OpenStack services like Compute to attach virtual devices to ports on these networks.
In particular, Networking supports each tenant having multiple private networks, and allows tenants to choose their own IP addressing scheme (even if those IP addresses overlap with those used by other tenants)
Enables advanced cloud networking use cases, such as building multi-tiered web applications and allowing applications to be migrated to the cloud without changing IP addresses.
Offers flexibility for the cloud administrator to customize network offerings.
Over time, the extended functionality becomes part of the core Networking API.
Plug-in architecture The original Compute network implementation assumed a basic model of isolation through Linux VLANs and IP tables.
Networking introduces the concept of a plug-in, which is a backend implementation of the Networking API.
A plug-in can use a variety of technologies to implement the logical API requests.
Some Networking plug-ins might use basic Linux VLANs and IP tables, while others might use more advanced technologies, such as L2-in-L3 tunneling or OpenFlow, to provide similar benefits.
Plug-ins can have different properties for hardware requirements, features, performance, scale, or operator tools.
Because Networking supports a large number of plug-ins, the cloud administrator can weigh options to decide on the right networking technology for the deployment.
The Open vSwitch and Linux Bridge plug-ins are deprecated in the Havana release and will be removed in the Icehouse release.
All features have been ported to the ML2 plug-in in the form of mechanism drivers.
Not all Networking plug-ins are compatible with all possible Compute drivers:
For configurations options, see Networking configuration options in Configuration Reference.
If you use the Open vSwitch (OVS) plug-in in a deployment with multiple hosts, you will need to use either tunneling or vlans to isolate traffic from multiple networks.
Tunneling is easier to deploy because it does not require configuring VLANs on network switches.
If you use the neutron DHCP agent, add these lines to the /etc/neutron/ dhcp_agent.ini file:
After performing that change on the node running neutron-server, restart neutron-server to apply the new settings:
While the instructions in this section refer to the Nicira NVP platform, they also apply to VMware NSX.
A set of parameters need to establish and configure the connection with the controller cluster.
Such parameters include NVP API endpoints, access credentials, and settings for HTTP redirects and retries in case of connection failures.
In order to ensure correct operations nvp_user shoud be a user with administrator credentials on the NVP platform.
A controller API endpoint consists of the controller's IP address and port; if the port is omitted, port 443 will be used.
If multiple API endpoints are specified, it is up to the user to ensure that all these endpoints belong to the same controller cluster; The Openstack Networking Nicira NVP plugin does not perform this check, and results might be unpredictable.
When multiple API endpoints are specified, the plugin will load balance requests on the various API endpoints.
The UUID of the NVP Transport Zone that should be used by default when a tenant creates a network.
Ubuntu packaging currently does not update the neutron init script to point to the NVP configuration file.
To debug nvp.ini configuration issues, run this command from the host that runs neutron-server:
This command tests whether neutron-server can log into all of the NVP Controllers and the SQL server, and whether all UUID values are correct.
The NVP LBaaS and FWaaS services use the standard OpenStack API with the exception of requiring routed-insertion extension support.
Below are the main differences between the NVP implementation and the community reference implementation of these services:
The NVP LBaaS and FWaaS plugins require the routed-insertion extension, which adds the router_id attribute to the VIP (Virtual IP address) and firewall resources and binds these services to a logical router.
The community reference implementation of LBaaS only supports a one-arm model, which restricts the VIP to be on the same subnet as the backend servers.
The NVP LBaaS plugin only supports a two-arm model between north-south traffic, meaning that the VIP can only be created on the external (physical) network.
The community reference implementation of FWaaS applies firewall rules to all logical routers in a tenant, while the NVP FWaaS plugin applies firewall rules only to one logical router according to the router_id of the firewall entity.
Optional, if not specified, a default global deployment container will be used.
The openflow_rest_api is used to tell where Ryu is listening for REST API.
The ovsdb_interface is used for Ryu to access the ovsdb-server.
If you want to change this value irrespective of the interface name, ovsdb_ip can be specified.
If you use a non-default port for ovsdb-server, it can be specified by ovsdb_port.
The IP address is derived from the network interface name.
You can use the same configuration file for many Compute nodes by using a network interface name with a different IP address:
Configure neutron agents Plug-ins typically have requirements for particular software that must be run on each node that handles data packets.
A data-forwarding node typically has a network interface with an IP address on the “management network” and another interface on the “data network”
This section shows you how to install and configure a subset of the available plug-ins, which may include the installation of switching software (for example, Open vSwitch) as well as agents used to communicate with the neutron-server process running elsewhere in the data center.
This section also applies to the ML2 plugin when Open vSwitch is used as a mechanism driver.
Install the OVS agent package (this pulls in the Open vSwitch software as a dependency):
If you use the Nicira NVP plug-in, you must also install Open vSwitch on each dataforwarding node.
However, you do not need to install an additional agent on each node.
It is critical that you are running an Open vSwitch version that is compatible with the current version of the NVP Controller software.
Do not use the Open vSwitch version that is installed by default on Ubuntu.
Instead, use the Open Vswitch version that is provided on the Nicira support portal for your NVP Controller version.
Ensure each data-forwarding node has an IP address on the "management network," and an IP address on the "data network" that is used for tunneling data traffic.
For full details on configuring your forwarding node, see the NVP Administrator Guide.
After following the NVP Administrator Guide, use the page for this Hypervisor in the NVP Manager GUI to confirm that the node is properly connected to the NVP.
Controller Cluster and that the NVP Controller Cluster can see the br-int integration bridge.
If you use the Ryu plug-in, you must install both Open vSwitch and Ryu, in addition to the Ryu agent package:
Install Ryu (there isn't currently an Ryu package for ubuntu):
Configure DHCP agent The DHCP service agent is compatible with all existing plug-ins and is required for all deployments where VMs should automatically receive IP addresses through DHCP.
You must configure the host running the neutron-dhcp-agent as a "data forwarding node" according to the requirements for your plug-in (see the section called “Configure neutron agents” [131])
Important If you reboot a node that runs the DHCP agent, you must run the neutron-ovscleanup command before the neutron-dhcp-agent service starts.
However, on Debian-based systems such as Ubuntu, you must manually run this command or write your own system script that runs on boot before the  neutron-dhcp-agent service starts.
Big Switch/Floodlight plug-in, which supports both the open source Floodlight controller and the proprietary Big Switch controller.
When using Floodlight as your OpenFlow controller, L3 functionality is not available.
Do not configure or use neutron-l3-agent if you use one of these plug-ins.
To uplink the node that runs neutron-l3-agent to the external network, create a bridge named "br-ex" and attach the NIC for the external network to this bridge.
For example, with Open vSwitch and NIC eth1 connected to the external network, run:
Do not manually configure an IP address on the NIC connected to the external network for the node running neutron-l3-agent.
Rather, you must have a range of IP addresses from the external network that can be used by OpenStack Networking for routers that uplink to the external network.
This range must be large enough to have an IP address for each router in the deployment, as well as each floating IP.
In order to support multiple routers with potentially overlapping IP addresses, neutron-l3-agent defaults to using Linux network namespaces to provide isolated forwarding contexts.
As a result, the IP addresses of routers will not be visible simply by running ip addr list or ifconfig on the node.
Similarly, you will not be able to directly ping fixed IPs.
To do either of these things, you must run the command within a particular router's network namespace.
However, on Debian-based systems such as Ubuntu, you must manually run this command or write your own system script that runs on boot before the neutron-l3-agent service starts.
Set this parameter in the neutron.conf file on the host that runs neutronserver:
You can enable the FWaaS functionality by setting the configuration, as follows.
Set this parameter in the neutron.conf file on the host that runs neutronserver:
To use the reference implementation, you must also add a FWaaS driver configuration to the neutron.conf file on every node where the Neutron L3 agent is deployed:
Networking architecture Before you deploy Networking, it helps to understand the Networking components and how these components interact with each other and other OpenStack services.
Overview Networking is a standalone service, just like other OpenStack services such as Compute, Image service, Identity service, or the Dashboard.
Like those services, a deployment of Networking often involves deploying several processes on a variety of hosts.
The Networking server uses the neutron-server daemon to expose the Networking API and to pass user requests to the configured Networking plug-in for additional processing.
Typically, the plug-in requires access to a database for persistent storage (also similar to other OpenStack services)
If your deployment uses a controller host to run centralized Compute components, you can deploy the Networking server on that same host.
However, Networking is entirely standalone and can be deployed on its own host as well.
Depending on your deployment, Networking can also include the following agents.
The agent that runs depends on the plug-in that you use, and some plug-ins do not require an agent.
These agents interact with the main neutron process through RPC (for example, rabbitmq or qpid) or through the standard Networking API.
Networking relies on the Identity service (Keystone) for the authentication and authorization of all API requests.
The Dashboard (Horizon) integrates with the Networking API, enabling administrators and tenant users to create and manage network services through a web-based GUI.
Place services on physical hosts Like other OpenStack services, Networking enables cloud administrators to run one or more services on one or more physical devices.
At one extreme, the cloud administrator can run all service daemons on a single physical host for evaluation purposes.
Alternatively the cloud administrator can run each service on its own physical host and, in some cases, can replicate services across multiple hosts for redundancy.
However, if you expect VMs to send significant traffic to or from the Internet, a dedicated network gateway host helps avoid CPU contention between the neutron-l3-agent and other OpenStack services that forward packets.
A standard Networking set up has one or more of the following distinct physical data center networks.
Data network Provides VM data communication within the cloud deployment.
The IP addressing requirements of this network depend on the Networking plug-in that is used.
External network Provides VMs with Internet access in some deployment scenarios.
Anyone on the Internet can reach IP addresses on this network.
The get_id() function stores the ID of created objects, and removes the need to copy and paste object IDs in later steps:
The way that you create a Networking endpoint entry depends on whether you are using the SQL or the template catalog driver:
You must provide admin user credentials that Compute and some internal Networking components can use to access the Networking API.
Create a special service tenant and a neutron user within this tenant, and assign an admin role to this role.
Compute If you use Networking, do not run the Compute nova-network service (like you do in traditional Compute deployments)
Compute proxies tenant-facing API calls to manage security groups and floating IPs to Networking APIs.
However, operator-facing tools such as novamanage, are not proxied and should not be used.
Do not rely on Compute networking documentation or past experience with Compute.
If a nova command or configuration option related to networking is not mentioned in this guide, the command is probably not supported for use with Networking.
In particular, you cannot use CLI tools like nova-manage and nova to manage networks or IP addressing, including both fixed and floating IPs, with Networking.
Uninstall nova-network and reboot any physical nodes that have been running nova-network before using them to run Networking.
Inadvertently running the nova-network process while using Networking can cause problems, as can stale iptables rules pushed down by previously running novanetwork.
To ensure that Compute works properly with Networking (rather than the legacy novanetwork mechanism), you must adjust settings in the nova.conf configuration file.
Networking API and credential configuration Each time you provision or de-provision a VM in Compute, nova-* services communicate with Networking using the standard API.
For this to happen, you must configure the following items in the nova.conf file (used by each nova-compute and nova-api instance)
This is the Identity (keystone) admin API server IP and port value, and not the Identity service API IP and port.
Configure security groups The Networking Service provides security group functionality using a mechanism that is more flexible and powerful than the security group capabilities built into Compute.
Therefore, if you use Networking, you should always disable built-in security groups and proxy all security group calls to the Networking API.
If you do not, security policies will conflict by being simultaneously applied by both services.
To proxy security groups to Networking, use the following configuration values in nova.conf:
Networking supports proxying those requests to nova-api, even when the requests are made from isolated networks, or from multiple networks that use overlapping IP addresses.
To enable proxying the requests, you must update the following fields in nova.conf.
The default value of an empty string in both files will allow metadata to function, but will not be secure if any non-trusted entities have access to the metadata APIs exposed by nova-api.
Instead, you should run a dedicated set of nova-api instances for metadata that are available only on your management network.
Whether a given nova-api instance exposes metadata APIs is determined by the value of enabled_apis in its nova.conf.
Example nova.conf (for nova-compute and nova-api) Example values for the above settings, assuming a cloud controller node running Compute and Networking with an IP address of 192.168.1.2:
Networking scenarios This chapter describes two networking scenarios and how the Open vSwitch plug-in and the Linux bridging plug-in implement these scenarios.
Open vSwitch This section describes how the Open vSwitch plug-in implements the Networking abstractions.
This example uses VLAN isolation on the switches to isolate tenant networks.
Under the service tenant, create the shared router, define the public network, and set it as the default gateway of the router.
Configure it to use VLAN ID 101 on the physical switch.
The following figure shows how to configure various Linux networking devices on the compute host:
There are four distinct type of virtual networking devices: TAP devices, veth pairs, Linux bridges, and Open vSwitch bridges.
A TAP device, such as vnet0 is how hypervisors such as KVM and Xen implement a virtual network interface card (typically called a VIF or vNIC)
An ethernet frame sent to a TAP device is received by the guest operating system.
A veth pair is a pair of directly connected virtual network interfaces.
An ethernet frame sent to one end of a veth pair is received by the other end of a veth pair.
Networking uses veth pairs as virtual patch cables to make connections between virtual bridges.
A Linux bridge behaves like a hub: you can connect multiple (physical or virtual) network interfaces devices to a Linux bridge.
Any ethernet frames that come in from one interface attached to the bridge is transmitted to all of the other devices.
An Open vSwitch bridge behaves like a virtual switch: network interface devices connect to Open vSwitch bridge's ports, and the ports can be configured much like a physical switch's ports, including VLAN configurations.
The br-int OpenvSwitch bridge is the integration bridge: all guests running on the compute host connect to this bridge.
Networking implements isolation across these guests by configuring the br-int ports.
The Open vSwitch agent is responsible for configuring flow rules on br-int and br-eth1 to do VLAN translation.
Ideally, the TAP device vnet0 would be connected directly to the integration bridge, br-int.
Unfortunately, this isn't possible because of how OpenStack security groups are currently implemented.
OpenStack uses iptables rules on the TAP devices such as vnet0 to implement security groups, and Open vSwitch is not compatible with iptables rules that are applied directly on TAP devices that are connected to an Open vSwitch port.
Networking uses an extra Linux bridge and a veth pair as a workaround for this issue.
Instead of connecting vnet0 to an Open vSwitch bridge, it is connected to a Linux bridge, qbrXXX.
This bridge is connected to the integration bridge, br-int, through the (qvbXXX, qvoXXX) veth pair.
The following figure shows the network devices on the network host:
An additional Open vSwitch bridge, br-ex, connects to the physical interface that is connected to the external network.
While the integration bridge and the external bridge are connected by a veth pair (int-br-ex, phy-br-ex), this example uses layer 3 connectivity to route packets from the internal networks to the public network: no packets traverse that veth pair in this example.
Internal ports enable you to assign one or more IP addresses to an Open vSwitch bridge.
In previous example, the br-int bridge has four internal ports: tapXXX, qr-YYY, qr-ZZZ, and tapWWW.
Each internal port has a separate IP address associated with it.
By default, The Networking DHCP agent uses a process called dnsmasq to provide DHCP services to guests.
Networking must create an internal port for each network that requires DHCP services and attach a dnsmasq process to that port.
The Networking L3 agent uses Open vSwitch internal ports to implement routing and relies on the network host to route the packets across the interfaces.
Because each of these interfaces is visible to the network host operating system, the network host routes the packets across the interfaces, as long as an administrator has enabled IP forwarding.
The L3 agent uses iptables to implement floating IPs to do the network address translation (NAT)
One problem with using the host to implement routing is that one of the Networking subnets might overlap with one of the physical networks that the host uses.
If end users are permitted to create their own logical networks and subnets, you must design the system so that such collisions do not occur.
Networking uses Linux network namespaces to prevent collisions between the physical networks on the network host, and the logical networks used by the virtual machines.
It also prevents collisions across different logical networks that are not routed to each other, as the following scenario shows.
A network namespace is an isolated environment with its own networking stack.
A network namespace has its own network interfaces, routes, and iptables rules.
Consider it a chroot jail, except for networking instead of for a file system.
Networking creates network namespaces on the network host to avoid subnet collisions.
In this example, there are three network namespaces, as shown in the figure above:
This allows overlapping IPs between net01_subnet01 and any other subnets on the network host.
This allows overlapping IPs between net02_subnet01 and any other subnets on the network host.
In this scenario, tenant A and tenant B each have a network with one subnet and one router that connects the tenants to the public Internet.
Under the tenantA user tenant, create the tenant router and set its gateway for the public network.
Similarly, for tenantB, create a router and another network, using VLAN ID 102 on the physical switch:
The following figure shows how to configure Linux networking devices on the Compute host:
However, in scenario 1, a guest connects to two subnets while in this scenario, the subnets belong to different tenants.
The following figure shows the network devices on the network host for the second scenario.
In this configuration, the network namespaces are organized to isolate the two subnets from each other as shown in the following figure.
In this scenario, there are four network namespaces (qhdcp-aaa, qrouter-bbbb, qrouter-cccc, and qhdcp-dddd), instead of three.
Since there is no connectivity between the two networks, and so each router is implemented by a separate namespace.
This example uses VLAN isolation on the switches to isolate tenant networks.
Under the service tenant, create the shared router, define the public network, and set it as the default gateway of the router.
Configure it to use VLAN ID 101 on the physical switch.
The following figure shows how to configure the various Linux networking devices on the compute host.
Note There are three distinct type of virtual networking devices: TAP devices, VLAN devices, and Linux bridges.
A TAP device, such as vnet0 is how hypervisors such as KVM and Xen implement a virtual network interface card (typically called a VIF or vNIC)
An ethernet frame sent to a TAP device is received by the guest operating system.
A VLAN device is associated with a VLAN tag attaches to an existing interface device and adds or removes VLAN tags.
A Linux bridge behaves like a hub: you can connect multiple (physical or virtual) network interfaces devices to a Linux bridge.
Any ethernet frames that come in from one interface attached to the bridge is transmitted to all of the other devices.
The following figure shows the network devices on the network host.
The following figure shows how the Linux Bridge plug-in uses network namespaces to provide isolation.
Each tenant has a network with one subnet, and each one has a router that connects them to the public Internet.
Under the tenantA user tenant, create the tenant router and set its gateway for the public network.
Similarly, for tenantB, create a router and another network, using VLAN ID 102 on the physical switch:
The following figure shows how the various Linux networking devices would be configured on the compute host under this scenario.
The only real difference is that scenario 1 had a guest that was connected to two subnets, and in this scenario, the subnets belong to different tenants.
The following figure shows the network devices on the network host for the second scenario.
The main difference between the configuration in this scenario and the previous one is the organization of the network namespaces, in order to provide isolation across the two subnets, as shown in the following figure.
In this scenario, there are four network namespaces (qhdcp-aaa, qrouter-bbbb, qrouter-cccc, and qhdcp-dddd), instead of three.
Since there is no connectivity between the two networks, and so each router is implemented by a separate namespace.
It currently includes drivers for the local, flat, vlan, gre and vxlan network types and works with the existing Open vSwitch, Linux Bridge, and HyperV L2 agents.
The ML2 plug-in can be extended through mechanism drivers, multiple mechanisms can be used simultaneously.
This section describes different ML2 plug-in / agents configurations with different type drivers and mechanism drivers.
Current Open vSwitch and Linux Bridge tunneling implementations broadcast to every agent, even if they don’t host the corresponding network as illustrated below.
As broadcast emulation on overlay is costly, it may be better to avoid its use for mac learning and ARP resolution.
This supposes the use of proxy ARP on the agent to answer VM requests, and to populate forwarding table.
The prepopulation limits L2 broadcasts in overlay, however it may anyway be necessary to provide broadcast emulation.
This is achieved by sending broadcasts packets over unicasts only to the relevant agents as illustrated below.
The partial-mesh is available with the Open vSwitch and the Linux Bridge agent.
The following scenarios will use the L2 population mechanism driver with an Open vSwitch agent and a Linux Bridge agent.
To enable the l2 population driver we have to add it in the list of mechanism drivers.
We also need to have at least one tunneling type driver enabled, either GRE, VXLAN or both.
Below configuration options that we have to set in ml2_conf.ini:
Advanced configuration options This section describes advanced configuration options for various system components.
For example, configuration options where the default works but that the user wants to customize options.
It is responsible for loading a plug-in and passing the API calls to the plug-in for processing.
The neutronserver should receive one of more configuration files as it its input, for example:
The plug-in that is run on the service is loaded through the core_plugin configuration parameter.
In some cases a plug-in might have an agent that performs the actual networking.
After you install and start the database server, set a password for the root account and delete the anonymous accounts:
Once the above is done you can update the settings in the relevant plug-in configuration files.
Some plug-ins have a L2 agent that performs the actual networking.
That is, the agent will attach the virtual machine NIC to the OpenStack Networking network.
Each node should have an L2 agent running on it.
Two things need to be done prior to working with the plug-in:
Some Linux packages might provide installation utilities that configure these.
All plug-in configuration files options can be found in the Appendix - Configuration File Options.
When a subnet is created, by default, the subnet has DHCP enabled.
Currently the DHCP agent uses dnsmasq to perform that static address assignment.
A driver needs to be configured that matches the plug-in running on the service.
By default the DHCP agent makes use of Linux network namespaces in order to support overlapping IP addresses.
Requirements for network namespaces support are described in the Limitations section.
If the Linux installation does not support network namespace, you must disable using network namespace in the DHCP agent config file (The default value of use_namespaces is True)
A driver needs to be configured that matches the plug-in running on the service.
This field must be empty (or the bridge name for the external network)
The L3 agent communicates with the OpenStack Networking server via the OpenStack Networking API, so the following configuration is required:
Namespace By default the L3 agent makes use of Linux network namespaces in order to support overlapping IP addresses.
Requirements for network namespaces support are described in the Limitation section.
When use_namespaces is set to False, only one router ID can be supported per node.
If use_namespaces is set to False then the agent can only configure one router.
In OpenStack Networking, a floating IP pool is represented as an external network and a floating IP is allocated from a subnet associated with the external network.
You can run multiple L3 agent instances on one host.
Since the default value of this parameter is True, you need to configure it carefully.
In general case the metering agent should be launched on all nodes that run the L3 agent:
A driver needs to be configured that matches the plug-in running on the service.
The driver is used to add metering to the routing interface.
The metering agent and the L3 agent have to have the same configuration regarding to the network namespaces setting.
A driver which implements the metering abstraction needs to be configured.
Currently there is only one implementation which is based on iptables.
To enable L3 metering you have to be sure to set the following parameter in neutron.conf on the host that runs neutron-server:
No equivalent for nova-network --multi_host flag: Nova-network has a model where.
OpenStack Networking now support running multiple l3agent and dhcp-agents with load being split across those agents, but the tight coupling of that scheduling with the location of the VM is not supported in Grizzly.
The Havana release is expected to include an exact replacement for the --multi_host flag in novanetwork.
Linux network namespace required on nodes running neutron-l3-agent or neutron-dhcp-agent if overlapping IPs are in use:
In order to support overlapping IP addresses, the OpenStack Networking DHCP and L3 agents use Linux network namespaces by default.
To check whether your host supports namespaces try running the following as root:
If the preceding commands do not produce errors, your platform is likely sufficient to use the dhcp-agent or l3-agent with namespace.
It may be possible to upgrade the iproute2 package on a platform that does not support namespaces by default.
If you need to disable namespaces, make sure the neutron.conf used by neutronserver has the following setting:
If the host does not support namespaces then the neutron-l3-agent and neutron-dhcp-agent should be run on different hosts.
This is due to the fact that there is no isolation between the IP addresses created by the L3 agent and by the DHCP agent.
By manipulating the routing the user can ensure that these networks have access to one another.
If you run both L3 and DHCP services on the same node, you should enable namespaces to avoid conflicts with routes:
Currently, there are no errors provided if you configure IPv6 addresses via the API.
ZeroMQ is an available option in the configuration file, but has not been tested and should be considered experimental.
In particular, issues might occur with ZeroMQ and the dhcp agent.
MetaPlugin is experimental: This release includes a MetaPlugin that is intended to support multiple plug-ins at the same time for different API requests, based on the content of those API requests.
The core team has not thoroughly reviewed or tested this functionality.
Consider this functionality to be experimental until further validation is performed.
Scalable and highly available DHCP agents This section describes how to use the agent management (alias agent) and scheduler (alias agent_scheduler) extensions for DHCP agents scalability and HA.
Use the neutron ext-list client command to check if these extensions are enabled:
OpenStack Controller host - controlnode Runs the Neutron, Keystone, and Nova services that are required to deploy VMs.
The node must have at least one network interface that is connected to the Management Network.
Note that nova-network should not be running because it is replaced by Neutron.
Commands in agent management and scheduler extensions The following commands require the tenant running the command to have an admin role.
These are used by the various clients to access Keystone.
Every agent which supports these extensions will register itself with the neutron server when it starts up.
In some deployments, one DHCP agent is not enough to hold all network data.
In addition, you must have a backup for it even when the deployment is small.
The same network can be assigned to more than one DHCP agent and one DHCP agent can host more than one network.
This command is to show which networks a given dhcp agent is managing.
You do not need to synchronize all agents to this time for this extension to run correctly.
This agent is a DHCP agent and it hosts one network, one subnet, and three ports.
The following output shows information for a Linux bridge agent:
The output shows bridge-mapping and the number of virtual network devices on this L2 agent.
When you create a network with one port, you can schedule it to an active DHCP agent.
If many active DHCP agents are running, select one randomly.
You can design more sophisticated scheduling algorithms in the same way as nova-schedule later on.
If you want to validate the behavior through the dnsmasq command, you must create a subnet for the network because the DHCP agent starts the dnsmasq service only if there is a DHCP.
To add another DHCP agent to host the network, run this command:
This command is the sibling command for the previous one.
You can see that only the DHCP agent for HostB is hosting the net2 network.
Fail the agents in turn to see if the VM can still get the desired IP.
Use the previous commands to assign the network to agents.
Log in to the myserver4 VM, and run udhcpc, dhclient or other DHCP client.
Besides stopping the neutron-dhcp-agent binary, you must stop the dnsmasq processes.
Run a DHCP client in VM to see if it can get the wanted IP.
Run udhcpc in the VM; it cannot get the wanted IP.
An administrator might want to disable an agent if a system hardware or software upgrade is planned.
Some agents that support scheduling also support disabling and enabling agents, such as L3 and DHCP agents.
After the agent is disabled, the scheduler does not schedule new resources to the agent.
After the agent is disabled, you can safely remove the agent.
Remove the resources on the agent before you delete the agent.
To run the following commands, you must stop the DHCP agent on HostA.
After deletion, if you restart the DHCP agent, it appears on the agent list again.
Use Networking You can start and stop OpenStack Networking services using the service command.
Expose the Networking API to cloud tenants, which enables them to build rich network topologies.
Have the cloud administrator, or an automated administrative tool, create network connectivity on behalf of tenants.
A tenant or cloud administrator can both perform the following procedures.
The neutron CLI is a wrapper around the Networking API.
There is also an extension to cover basic L3 forwarding and NAT, which provides capabilities similar to nova-network.
Subnet Associates a block of IP addresses and other network configuration, such as, default gateways or dnsservers, with an Networking network.
Port Represents an attachment port to a L2 Networking network.
When a port is created on the network, by default it is allocated an available fixed IP address out of one of the designated subnets for each IP version (if one exists)
When the port is destroyed, its allocated addresses return to the pool of available IPs on the subnet.
Users of the Networking API can either choose a specific IP address from the block, or let Networking choose the first available IP address.
This table summarizes the attributes available for each networking abstraction.
For information about API abstraction and operations, see the Networking API v2.0 Reference.
If specified as False (down), this network does not forward packets.
The default policy setting restricts usage of this attribute to administrative users only.
Only administrative users can set the tenant identifier; this cannot be changed using authorization policies.
List of cidr sub-ranges that are available for dynamic allocation to ports.
Only administrative users can set the tenant identifier; this cannot be changed using authorization policies.
If specified as False (down), this port does not forward packets.
Specifies IP addresses for this port; associates the port with the subnets containing the listed IP addresses.
Only administrative users can set the tenant identifier; this cannot be changed using authorization policies.
Basic Networking operations To learn about advanced capabilities that are available through the neutron command-line interface (CLI), read the networking section in the  OpenStack End User Guide.
This table shows example neutron commands that enable you to complete basic Networking operations:
Administrative operations The cloud administrator can run any neutron command on behalf of tenants by specifying an Identity tenant_id in the command, as follows:
To view all tenant IDs in Identity, run the following command as an Identity Service admin user:
This table shows example neutron commands that enable you to complete advanced Networking operations:
Creates a subnet with a specified set of host routes.
Creates a subnet with a specified set of dns name servers.
This table shows example neutron and nova commands that enable you to complete basic Compute and Networking operations:
Boots a VM with a single NIC on a selected Networking network.
Searches for ports with a device_id that matches the Compute instance UUID.
Searches for ports, but shows only the mac_address for the port.
When you boot a Compute VM, a port on the network that corresponds to the VM NIC is automatically created and associated with the default security group.
You can configure security group rules to enable users to access the VM.
When you delete a Compute VM, the underlying Networking port is automatically deleted.
This table shows example nova and neutron commands that enable you to complete advanced VM creation operations:
First, create an Networking port with a specific IP address.
Then, boot a VM specifying a portid rather than a net-id.
Boots a VM that connects to all networks that are accessible to the tenant who submits the request (without the --nic option)
Networking does not currently support the v4-fixed-ip parameter of the -nic option for the nova command.
You must configure security group rules depending on the type of plug-in you are using.
This example enables ping and ssh access to your VMs.
Does not implement Networking security groups, you can configure security group rules by using the nova secgroup-add-rule or euca-authorize command.
These nova commands enable ping and ssh access to your VMs.
If your plug-in implements Networking security groups, you can also leverage Compute security groups by setting security_group_api = neutron in the nova.conf file.
After you set this option, all Compute security group commands are proxied to Networking.
Advanced features through API extensions Several plug-ins implement API extensions that provide capabilities similar to what was available in nova-network: These plug-ins are likely to be of interest to the OpenStack community.
The provider extension allows administrators to explicitly manage the relationship between Networking virtual networks and underlying physical mechanisms such as VLANs and tunnels.
When this extension is supported, Networking client users with administrative.
The provider extension is supported by the Open vSwitch and Linux Bridge plug-ins.
Terminology A number of terms are used in the provider extension and in the configuration of plug-ins supporting the provider extension:
The Open vSwitch and Linux Bridge plug-ins each support several different mechanisms to realize virtual networks.
The provider extension and the plug-in configurations identify physical networks using simple string names.
The physical details of the network are not exposed to the tenant.
Each distinct physical network supporting VLAN networks is treated as a separate VLAN trunk, with a distinct space of VID values.
Each physical network can realize at most one flat network.
Local networks are intended mainly for single-node test scenarios, but can have other uses.
The ML2, Open vSwitch, and Linux Bridge plug-ins support VLAN networks, flat networks, and local networks.
Provider attributes The provider extension extends the Networking network resource with these attributes:
Attribute name Type Default Value Description flat, vlan, local, and gre, corresponding to flat networks, VLAN networks, local networks, and GRE networks as defined above.
All types of provider networks can be created by administrators, while tenant networks can be implemented as vlan, gre, or local network types depending on plug-in configuration.
The name of the physical network over which the virtual network is implemented for flat and VLAN networks.
The default Networking configuration authorizes both actions for users with the admin role.
An authorized client or an administrative user can view and set the provider extended attributes through Networking API calls.
See the section called “Authentication and authorization” [206] for details on policy configuration.
Provider extension API operations To use the provider extension with the default policy settings, you must have the administrative role.
This table shows example neutron commands that enable you to complete basic provider extension API operations:
When you create flat networks, <phys-net-name> must be known to the plug-in.
When you create VLAN networks, <phys-net-name> must be known to the plug-in.
When you create VLAN networks, <VID> can fall either within or outside any configured ranges of VLAN IDs from which tenant networks are allocated.
When you create GRE networks, <tunnel-id> can be either inside or outside any tunnel ID ranges from which tenant networks are allocated.
After you create provider networks, you can allocate subnets, which you can use in the same way as other virtual networks, subject to authorization policy based on the specified <tenant_id>
Networking includes an API extension that provides abstract L3 routers that API users can dynamically provision and configure.
For example, a public network for access to the Internet.
See the OpenStack Configuration Reference for details on common models of deploying Networking L3 routers.
The L3 router provides basic NAT capabilities on gateway ports that uplink the router to external networks.
This router SNATs all traffic by default, and supports floating IPs, which creates a static one-to-one mapping from a public IP on the external network to a private IP on one of the other subnets attached to the router.
This allows a tenant to selectively expose VMs on private networks to other hosts on the external network (and often to all hosts on the Internet)
You can allocate and map floating IPs from one port to another, as needed.
Only admin users can specify a tenant_id other than its own.
Null External network that this router connects to for gateway services (for example, NAT)
The external network IP address available to be mapped to an internal IP address.
Only admin users can specify a tenant_id other than its own.
However, the default policy settings enable only administrative users to create, update, and delete external networks.
This table shows example neutron commands that enable you to complete basic L3 operations:
Creates an internal-only router that connects to multiple L2 networks privately.
Connects a router to an external network, which enables that router to act as a NAT gateway for external connectivity.
Identifies the port-id that represents the VM NIC to which the floating IP should map.
Creates a floating IP address and associates it with a port.
Creates a floating IP address and associates it with a port, in a single step.
Security groups Security groups and security group rules allows administrators and tenants the ability to specify the type of traffic and direction (ingress/egress) that is allowed to pass through a port.
A security group is a container for security group rules.
When a port is created in Networking it is associated with a security group.
If a security group is not specified the port is associated with a 'default' security group.
By default, this group drops all ingress traffic and allows all egress.
Rules can be added to this group in order to change the behaviour.
To use the Compute security group APIs or use Compute to orchestrate the creation of ports for instances on specific security groups, you must complete additional configuration.
After you make this change, restart nova-api and nova-compute to pick up this change.
Then, you can use both the Compute and OpenStack Network security group APIs at the same time.
You must configure the correct firewall driver in the securitygroup section of the plug-in/agent configuration file.
Some plug-ins and agents, such as Linux Bridge Agent and Open vSwitch Agent, use the no-operation driver as the default, which results in non-working security groups.
When using the security group API through Compute, security groups are applied to all ports on an instance.
The reason for this is that Compute security group APIs are instances based and not port based as Networking.
Cannot be named default as that is automatically created for a tenant.
Only admin users can specify a tenant_id other than their own.
Only admin users can specify a tenant_id other than its own.
This table shows example neutron commands that enable you to complete basic security group operations:
Creates a security group rule to allow port 80 ingress.
The Havana release offers a reference implementation that is based on the HAProxy software load balancer.
This table shows example neutron commands that enable you to complete basic LBaaS operations:
If not used, the pool is created with default provider for LBaaS service.
If no default provider is specified for LBaaS, the --provider option is required for pool creation.
Creates a health monitor which checks to make sure our instances are still running on the specified protocol-port.
Creates a virtual IP (VIP) address that, when accessed through the load balancer, directs the requests to one of the pool members.
The FWaaS is backed by a reference implementation that works with the Networking OVS plug-in and provides perimeter firewall functionality.
It leverages the footprint of the Networking OVS L3 agent and an IPTables driver to apply the firewall rules contained in a particular firewall policy.
This reference implementation supports one firewall policy and consequently one logical firewall instance for each tenant.
This is not a constraint of the resource model, but of the current reference implementation.
If a tenant has multiple routers, the firewall is present on all the routers.
If a tenant does not have any router, the firewall is in PENDING_CREATE state until a router is created and the first interface is added to the router.
At that point the firewall policy is immediately applied to the router and the firewall changes to ACTIVE state.
Because this is the first iteration of this implementation, it should probably not be run in production environments without adequate testing.
Only admin users can specify a tenant_id other than its own.
This is a read-only attribute that gets populated with the uuid of the firewall policy when this firewall rule is associated with a firewall policy.
A firewall rule can be associated with only one firewall policy at a time.
However, the association can be changed to a different firewall policy.
It indicates the position of this rule in that firewall policy.
Facilitates selectively turning off rules without having to disassociate the rule from the firewall policy.
Only admin users can specify a tenant_id other their own.
None This is an ordered list of firewall rule uuids.
The firewall applies the rules in the order in which they appear in this list.
This attribute is meant to aid in the firewall policy audit workflows.
Each time the firewall policy or the associated firewall rules are changed, this attribute is set to False and must be explicitly set to True through an update operation.
Only admin users can specify a tenant_id other than its own.
If False (down), the firewall does not forward any packets.
This firewall implements the rules contained in the firewall policy represented by this uuid.
If the rule is protocol agnostic, the 'any' value can be used.
In addition to the protocol attribute, other attributes can be specified in the firewall rule.
A firewall policy can be created without any rules and rules can be added later either via the update operation (if adding multiple rules) or via the insert-rule operation (if adding a single rule)
Please check the CLI help for more details on these operations.
The reference implementation always adds a default deny all rule at the end of each policy.
This implies that if a firewall policy is created without any rules and is associated with a firewall, that firewall blocks all traffic.
The FWaaS features and the above workflow can also be accessed from the Horizon user interface.
The main use case for this is to enable the ability to use protocols such as VRRP which floats an ip address between two instances to enable fast data plane failover.
Plug-in specific extensions Each vendor can choose to implement additional API extensions to the core API.
The Nicira NVP QoS extension rate-limits network ports to guarantee a specific amount of bandwidth for each port.
This extension, by default, is only accessible by a tenant with an admin role but is configurable through the policy.json file.
To use this extension, create a queue and specify the min/max bandwidth rates (kbps) and optionally set the QoS Marking and DSCP value (if your network fabric uses these values to make forwarding decisions)
Once created, you can associate a queue with a network.
Then, when ports are created on that network they are automatically created and associated with the specific queue size that was associated with the network.
Because one size queue for a every port on a network might not be optimal, a scaling factor from the Nova flavor 'rxtx_factor' is passed in from Compute when creating the port to scale the queue.
Lastly, if you want to set a specific baseline QoS policy for the amount of bandwidth a single port can use (unless a network queue is specified with the network a port is created on) a default queue can be created in Networking which then causes ports created to be associated with a queue of that size times the rxtx scaling factor.
Note that after a network or default queue is specified, queues are added to ports that are subsequently created but are not added to existing ports.
This table shows example neutron commands that enable you to complete basic queue operations:
Provider networks can be implemented in different ways by the underlying NVP platform.
The FLAT and VLAN network types use bridged transport connectors.
These network types enable the attachment of large number of ports.
To handle the increased scale, the NVP plug-in can back a single Openstack Network with a chain of NVP logical switches.
The recommended value for this parameter varies with the NVP version running in the back-end, as shown in the following table.
In addition to these network types, the NVP plug-in also supports a special l3_ext network type, which maps external networks to specific NVP gateway services as discussed in the next section.
Create external network and map it to a specific NVP gateway service:
Terminate traffic on a specific VLAN from a NVP gateway service:
Starting with the Havana release, the Nicira NVP plug-in provides an asynchronous mechanism for retrieving the operational status for neutron resources from the NVP backend; this applies to network, port, and router resources.
The back-end is polled periodically, and the status for every resource is retrieved; then the status in the Networking database is updated only for the resources for which a status change occurred.
As operational status is now retrieved asynchronously, performance for GET operations is consistently improved.
Data to retrieve from the back-end are divided in chunks in order to avoid expensive API requests; this is achieved leveraging NVP APIs response paging capabilities.
The minimum chunk size can be specified using a configuration option; the actual chunk size is then determined dynamically according to: total number of resources to retrieve, interval between two synchronization task runs, minimum delay between two subsequent requests to the NVP back-end.
The operational status synchronization can be tuned or disabled using the configuration options reported in this table; it is however worth noting that the default values work fine in most cases.
Interval in seconds between two run of the synchronization task.
Setting the value for this option to 0 will disable the synchronization task.
The value of this option can be tuned according to the observed load on the NVP controllers.
Lower values will result in faster synchronization, but might increase the load on the controller cluster.
Minimum number of resources to retrieve from the backend for each synchronization chunk.
This size of a chunk might increase if the total number of resources is such that more than min_chunk_size resources must be fetched in one chunk with the current number of chunks.
When this option is enabled, the operational status will always be retrieved from the NVP back-end ad every GET request.
In this case it is advisable to disable the synchronization task.
When running multiple OpenStack Networking server instances, the status synchronization task should not run on every node; doing so sends unnecessary traffic to the NVP back-end and performs unnecessary DB operations.
Big Switch allows router rules to be added to each tenant router.
These rules can be used to enforce routing policies such as denying traffic between subnets or traffic to external networks.
By enforcing these at the router level, network segmentation policies can be enforced across many VMs that have differing security groups.
Each tenant router has a set of router rules associated with it.
Router rules and their attributes can be set using the neutron router-update command, through the Horizon interface or the Neutron API.
The network that a packet's source IP must match for the rule to be applied.
The network that a packet's destination IP must match for the rule to be applied.
Overrides the default virtual router used to handle traffic for packets that match the rule.
Overlapping rules are evaluated using longest prefix matching on the source and destination fields.
The source field is matched first so it always takes higher precedence over the destination field.
In other words, longest prefix matching is used on the destination field only if there are multiple matching rules with the same source.
Router rules are configured with a router update operation in OpenStack Networking.
The update overrides any previous rules so all rules must be provided at the same time.
The L3 metering extension is decoupled from the technology that implements the measurement.
Two abstractions have been added: One is the metering label that can contain metering rules.
Because a metering label is associated with a tenant, all virtual routers in this tenant are associated with this label.
Only administrators can manage the L3 metering labels and rules.
This table shows example neutron commands that enable you to complete basic L3 metering operations:
Logging configuration can be provided in neutron.conf or as command line options.
To configure logging for Networking components, use one of these methods:
Show more verbose log output (sets INFO log level output) if debug is False # verbose = False.
Notifications Notifications can be sent when Networking resources such as network, subnet and port are created, updated or deleted.
To set up the notification, edit notification options in neutron.conf:
Defined in rpc_notifier for rpc way, can be comma separated values.
These options configure the Networking server to send notifications through logging and RPC.
Defined in rpc_notifier for rpc way, can be comma separated values.
These options configure the Networking server to send notifications to multiple RPC topics.
Defined in rpc_notifier for rpc way, can be comma separated values.
Authentication and authorization Networking uses the Identity Service as the default authentication service.
When the Identity Service is enabled, users who submit requests to the Networking service must provide an authentication token in X-Auth-Token request header.
Users obtain this token by authenticating with the Identity Service endpoint.
When the Identity Service is enabled, it is not mandatory to specify the tenant ID for resources in create requests because the tenant ID is derived from the authentication token.
The default authorization settings only allow administrative users to create resources on behalf of a different tenant.
Networking uses information received from Identity to authorize user requests.
Operation-based policies specify access criteria for specific operations, possibly with finegrained control over specific attributes;
Resource-based policies specify whether access to specific resource is granted or not according to the permissions configured for the resource (currently available only for the network resource)
The actual authorization policies enforced in Networking might vary from deployment to deployment.
The actual location of this file might vary from distribution to distribution.
Entries can be updated while the system is running, and no service restart is required.
Every time the policy file is updated, the policies are automatically reloaded.
Currently the only way of updating such policies is to edit the policy file.
In this section, the terms policy and rule refer to objects that are specified in the same way in the policy file.
There are no syntax differences between a rule and a policy.
A policy is something that is matched directly from the Networking policy engine.
A rule is an element in a policy, which is evaluated.
Policies are triggered by the Networking policy engine whenever one of them matches an Networking API operation or a specific attribute being used in a given operation.
An authorization policy can be composed by one or more rules.
If more rules are specified, evaluation policy succeeds if any of the rules evaluates successfully; if an API operation matches multiple policies, then all the policies must evaluate successfully.
Once a rule is matched, the rule(s) can be resolved to another rule, until a terminal rule is reached.
The Networking policy engine currently defines the following kinds of terminal rules:
Role-based rules evaluate successfully if the user who submits the request has the specified role.
For instance "role:admin" is successful if the user who submits the request is an administrator.
Field-based rules evaluate successfully if a field of the resource specified in the current request matches a specific value.
Generic rules compare an attribute in the resource with an attribute extracted from the user's security credentials and evaluates successfully if the comparison is successful.
This policy evaluates successfully if either admin_or_owner, or shared evaluates successfully.
This policy restricts the ability to manipulate the shared attribute for a network to administrators only.
This policy restricts the ability to manipulate the mac_address attribute for a port only to administrators and the owner of the network where the port is attached.
In some cases, some operations are restricted to administrators only.
This example shows you how to modify a policy file to permit tenants to define networks and see their resources and permit administrative users to perform all other operations:
High availability The use of high-availability in a Networking deployment helps prevent individual node failures.
In general, you can run neutron-server and neutron-dhcp-agent in an active-active fashion.
You can run the neutron-l3-agent service as active/passive, which avoids IP conflicts with respect to gateway IP addresses.
Networking high availability with Pacemaker You can run some Networking services into a cluster (Active / Passive or Active / Active for Networking Server only) with Pacemaker.
For information about how to build a cluster, see Pacemaker documentation.
Many resources are available to help you run and use OpenStack.
Members of the OpenStack community can answer questions and help with bug suspicions.
We are constantly improving and adding to the main features of OpenStack, but if you have any problems, do not hesitate to ask.
Use the following resources to get OpenStack support and troubleshoot your existing installations.
The following books explain how to install an OpenStack cloud and its associated components:
The following books explain how to configure and run an OpenStack cloud:
The following books explain how to use the OpenStack dashboard and command-line clients:
The following documentation provides reference and guidance information for the OpenStack APIs:
The Training Guides offer software training for cloud administration and management.
Be sure to give a clear, concise summary in the title and provide as much detail as possible in the description.
Paste in your command output or stack traces, links to screen shots, and so on.
OpenStack mailing lists A great way to get answers and insights is to post your question or problematic scenario to the OpenStack mailing list.
You can learn from and help others who might have similar issues.
To subscribe or view the archives, go to http://lists.openstack.org/cgi-bin/mailman/ listinfo/openstack.
You might be interested in the other mailing lists for specific projects or development, which you can find on the wiki.
A description of all mailing lists is available at http://wiki.openstack.org/MailingLists.
The OpenStack wiki The OpenStack wiki contains a broad range of topics but some of the information can be difficult to find or is a few pages deep.
If you search for specific information, such as about networking or nova, you can find lots of relevant material.
More is being added all the time, so be sure to check back often.
You can find the search box in the upper right corner of any OpenStack wiki page.
The Launchpad Bugs area The OpenStack community values your set up and testing efforts and wants your feedback.
You can view existing bugs and report bugs in the Launchpad Bugs area.
Use the search feature to determine whether the bug has already been reported or even better, already fixed.
If it still seems like your bug is unreported, fill out a bug report.
Paste in your command output or stack traces, links to screen shots, and so on.
Any deployment specific information is helpful, such as Ubuntu 12.04 or multi-node install.
The OpenStack IRC channel The OpenStack community lives and breathes in the #openstack IRC channel on the Freenode network.
You can hang out, ask questions, or get immediate feedback for urgent and pressing issues.
You can also use Colloquy (Mac OS X, http://colloquy.info/), mIRC (Windows, http://www.mirc.com/), or XChat (Linux)
When you are in the IRC channel and want to share code or command output, the generally accepted method is to use a Paste Bin.
Just paste your longer amounts of text or logs in the web form and you get a URL you can paste into the channel.
Customize the dashboard Set up session storage for the dashboard Local memory cache Key-value stores Memcached Redis.
Remove a network from a project Multiple interfaces for your instances (multinic) Use the multinic feature.
Volumes System administration Compute service architecture Manage Compute users Manage the cloud Show usage statistics for hosts and instances Manage logs Secure with root wrappers Migration Recover from a failed compute node Evacuate instances Manual recovery.
Ring-builder Ring data structure Partition assignment list Replica counts Partition shift value Build the ring.
Failed to attach volume, systool is not installed Problem Solution.
Networking architecture Overview Place services on physical hosts Network connectivity for physical hosts.
Scalable and highly available DHCP agents Configuration Commands in agent management and scheduler extensions.
Security groups Security group API abstractions Basic security group operations.
Big Switch plug-in extensions Big Switch router rules Router rule attributes Order of rule processing Big Switch router rules operations.
Authentication and authorization High availability Networking high availability with Pacemaker.
