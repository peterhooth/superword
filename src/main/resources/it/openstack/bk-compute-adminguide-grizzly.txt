Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.
Except where otherwise noted, this document is licensed under Creative Commons Attribution ShareAlike 3.0 License.
OpenStack is a collection of open source technologies that provides massively scalable cloud computing software.
OpenStack can be used by corporations, service providers, VARS, SMBs, researchers, and global data centers looking to deploy large-scale cloud deployments for private or public clouds.
Why Cloud? In data centers today, many computers suffer the same under-utilization in computing power and networking bandwidth.
For example, projects may need a large amount of computing capacity to complete a computation, but no longer need the computing power after completing the computation.
You want cloud computing when you want a service that's available on-demand with the flexibility to bring it up or down through automation or with little intervention.
The phrase "cloud computing" is often represented with a diagram that contains a cloud-like shape indicating a layer where responsibility for service goes from user to provider.
The cloud in these types of diagrams contains the services that afford computing power harnessed to get work done.
Much like the electrical power we receive each day, cloud computing provides subscribers or users with access to a shared collection of computing resources: networks for transfer, servers for storage, and applications or services for completing tasks.
On-demand self-service: Users can provision servers and networks with little human intervention.
Network access: Any computing capabilities are available over the network.
Resource pooling: Multiple users can access clouds that serve other consumers according to demand.
Elasticity: Provisioning is rapid and scales out or in based on need.
Metered or measured service: Just like utilities that are paid for by the hour, clouds.
Cloud computing offers different service models depending on the capabilities a consumer may require.
Provides the consumer the ability to use the software in a cloud environment, such as web-based email for example.
Provides the consumer the ability to deploy applications through a programming language or tools supported by the cloud platform provider.
Provides infrastructure such as computer instances, network connections, and storage so that people can run any software or operating system.
When you hear terms such as public cloud or private cloud, these refer to the deployment model for the cloud.
A private cloud operates for a single organization, but can be managed on-premise or off-premise.
A public cloud has an infrastructure that is available to the general public or a large industry group and is likely owned by a cloud services company.
The NIST also defines community cloud as shared by several organizations supporting a specific community with shared concerns.
A hybrid cloud can be a deployment model, as a composition of both public and private clouds, or a hybrid model for cloud computing may involve both virtual and physical servers.
What have people done with cloud computing? Cloud computing can help with largescale computing needs or can lead consolidation efforts by virtualizing servers to make more use of existing hardware and potentially release old hardware from service.
People also use cloud computing for collaboration because of its high availability through networked computers.
Productivity suites for word processing, number crunching, and email communications, and more are also available through cloud computing.
Cloud computing also avails additional storage to the cloud user, avoiding the need for additional hard drives on each user's desktop and enabling access to huge data storage capacity online in the cloud.
For a more detailed discussion of cloud computing's essential characteristics and its models of service and deployment, see  http://www.nist.gov/itl/cloud/, published by the US National Institute of Standards and Technology.
What is OpenStack? OpenStack is on a mission: to provide scalable, elastic cloud computing for both public and private clouds, large and small.
At the heart of our mission is a pair of basic requirements: clouds must be simple to implement and massively scalable.
If you are new to OpenStack, you will undoubtedly have questions about installation, deployment, and usage.
But don't fear, there are places to get information to guide you and to help resolve any issues you may run into during the on-ramp process.
Because the project is so new and constantly changing, be aware of the revision time for all information.
It allows you to store or retrieve files (but not mount directories like a fileserver)
Swift is also used internally at many large companies to store their data.
Image (codenamed "Glance") provides a catalog and repository for virtual disk images.
These disk images are mostly commonly used in OpenStack Compute.
While this service is technically optional, any cloud of size will require it.
Rackspace and HP provide commercial compute services built on Nova and it is used internally at companies like Mercado Libre and NASA (where it originated)
Dashboard (codenamed "Horizon") provides a modular web-based user interface for all the OpenStack services.
With this web GUI, you can perform most operations on your cloud like launching an instance, assigning IP addresses and setting access controls.
Identity (codenamed "Keystone") provides authentication and authorization for all the OpenStack services.
It also provides a service catalog of services within a particular OpenStack cloud.
Network (codenamed "Quantum") provides "network connectivity as a service" between interface devices managed by other OpenStack services (most likely Nova)
The service works by allowing users to create their own networks and then attach interfaces to them.
OpenStack Network has a pluggable architecture to support many popular networking vendors and technologies.
Block Storage (codenamed "Cinder") provides persistent block storage to guest VMs.
In addition to these core projects, there are also a number of "incubation" projects that are being considered for future integration into the OpenStack release.
Conceptual Architecture The OpenStack project as a whole is designed to deliver a massively scalable cloud operating system.
To achieve this, each of the constituent services are designed to work together to provide a complete Infrastructure as a Service (IaaS)
This integration is facilitated through public application programming interfaces (APIs) that each service offers (and in turn can consume)
While these APIs allow each of the services to use another service, it also allows an implementer to switch out any service as long as they maintain the API.
These are (mostly) the same APIs that are available to end users of the cloud.
Conceptually, you can picture the relationships between the services as so:
Dashboard ("Horizon") provides a web front end to the other OpenStack services.
Compute ("Nova") stores and retrieves virtual disks ("images") and associated metadata in Image ("Glance")
Image ("Glance") can store the actual virtual disk files in the Object Store("Swift")
This is a stylized and simplified view of the architecture, assuming that the implementer is using all of the services together in the most common configuration.
It also only shows the "operator" side of the cloud -- it does not picture how consumers of the cloud may actually use it.
For example, many users will access object storage heavily (and directly)
Logical Architecture The following paragraphs give some details on the main modules in the OpenStack components.
These details are not meant to be exhaustive; the objective is to describe the relevant aspects that administrators need to know to better understand how to design the deployment, and install and configure the whole platform.
As you can imagine, the logical architecture is far more complicated than the conceptual architecture shown above.
As with any service-oriented architecture, diagrams quickly become "messy" trying to illustrate all the possible combinations of service communications.
The diagram below, illustrates the most common architecture of an OpenStack-based cloud.
However, as OpenStack supports a wide variety of technologies, it does not represent the only architecture possible.
This picture is consistent with the conceptual architecture above in that:
End users can interact through a common web interface (Horizon) or directly to each service through their API.
All services authenticate through a common source (facilitated through Keystone)
Individual services interact with each other through their public APIs (except where privileged administrator commands are necessary)
In the sections below, we'll delve into the architecture for each of the services.
Dashboard Horizon is a modular Django web application that provides an end user and administrator interface to OpenStack services.
As with most web applications, the architecture is fairly simple:
The code itself is separated into a reusable python module with most of the logic (interactions with various OpenStack APIs) and presentation (to make it easily customizable for different sites)
As it relies mostly on the other services for data, it stores very little data of its own.
From a network architecture point of view, this service will need to be customer accessible as well as be able to talk to each service's public APIs.
Compute Nova is the most complicated and distributed component of OpenStack.
A large number of processes cooperate to turn end user API requests into running virtual machines.
It also initiates most of the orchestration activities (such as running an instance) as well as enforces some policy.
The nova-api-metadata service is generally only used when running in multi-host mode with nova-network installations.
The process by which it does so is fairly complex but the basics are simple: accept actions from the queue and then perform a series of system commands (like launching a KVM instance) to carry them out while updating state in the database.
The nova-schedule process is conceptually the simplest piece of code in OpenStack Nova: take a virtual machine instance request from the queue and determines where it should run (specifically, which compute server host it should run on)
The nova-conductor module, introduced in the Grizzly release, works as a “mediator” between nova-compute and the database.
It is aimed at eliminating all the direct accesses to the cloud database made by nova-compute.
The nova-conductor module scales horizontally but it shouldn’t be deployed on the same node(s) where novacompute runs.
It accepts networking tasks from the queue and then performs tasks to manipulate the network (such as setting up bridging interfaces or changing iptables rules)
This functionality is being migrated to OpenStack Networking, a separate OpenStack service.
This functionality is also migrated to OpenStack Networking; a different script is provided when using OpenStack Networking (code-named Quantum)
The nova-consoleauth daemon authorizes user’s tokens that console proxies provide (see nova-novncproxy and nova-xvpnvcproxy)
This service must be running in order for console proxies to work.
Many proxies of either type can be run against a single novaconsoleauth service in a cluster configuration.
The nova-novncproxy (daemon) provides a proxy for accessing running instances through a VNC connection.
The deprecated nova-console daemon is no longer used with Grizzly, and the novaxvpnvncproxy is used instead.
The nova-xvpnvncproxy daemon is a proxy for accessing running instances through a VNC connection.
The euca2ools client is not an OpenStack module but it can be supported by OpenStack.
It’s a set of command line interpreter commands for managing cloud resources.
The nova client enables you to submit either tenant administrator’s commands or cloud user’s commands.
The queue provides a central hub for passing messages between daemons.
This is usually implemented with RabbitMQ today, but could be any AMPQ message queue (such as Apache Qpid), or Zero MQ.
The SQL database stores most of the build-time and run-time state for a cloud infrastructure.
This includes the instance types that are available for use, instances in use, networks available and projects.
Theoretically, OpenStack Nova can support any database supported by SQL-Alchemy but the only databases currently being widely used are sqlite3 (only appropriate for test and development work), MySQL and PostgreSQL.
Nova interacts with many other OpenStack services: Keystone for authentication, Glance for images and Horizon for web interface.
The API process can upload and query Glance while nova-compute will download images for use in launching images.
Object Store The swift architecture is very distributed to prevent any single point of failure as well as to scale horizontally.
It accepts files to upload, modifications to metadata or container creation.
In addition, it will also serve files or container listing to web browsers.
The proxy server may utilize an optional cache (usually deployed with memcache) to improve performance.
Account servers manage accounts defined with the object storage service.
Container servers manage a mapping of containers (i.e folders) within the object store service.
There are also a number of periodic process which run to perform housekeeping tasks on the large data store.
The most important of these is the replication services, which ensures consistency and availability through the cluster.
Authentication is handled through configurable WSGI middleware (which will usually be Keystone)
Like Nova, you can choose your database depending on your preference (but most people use MySQL or SQlite)
In the diagram above, Swift is shown as the image repository, but this is configurable.
Be aware that some of these choices are limited to read-only usage.
There are also a number of periodic process which run on Glance to support caching.
The most important of these is the replication services, which ensures consistency and availability through the cluster.
As you can see from the diagram in the Conceptual Architecture section, Glance serves a central role to the overall IaaS picture.
It accepts API requests for images (or image metadata) from end users or Nova components and can store its disk files in the object storage service, Swift.
Identity Keystone provides a single point of integration for OpenStack policy, catalog, token and authentication.
Each Keystone function has a pluggable backend which allows different ways to use the particular service.
Most support standard backends like LDAP or SQL, as well as Key Value Stores (KVS)
Most people will use this as a point of customization for their current authentication services.
Network OpenStack Networking provides "network connectivity as a service" between interface devices managed by other OpenStack services (most likely Compute)
The service works by allowing users to create their own networks and then attach interfaces to them.
Like many of the OpenStack services, OpenStack Networking is highly configurable due to its plug-in architecture.
In the above architecture, a simple Linux networking plug-in is shown.
OpenStack Networking plugins and agents perform the actual actions such as plugging and unplugging ports, creating networks or subnets and IP addressing.
These plugins and agents differ depending on the vendor and technologies used in the particular cloud.
Most OpenStack Networking installations also make use of a messaging queue to route information between the quantum-server and various agents as well as a database to store networking state for particular plugins.
OpenStack Networking interacts mainly with OpenStack Compute, where it provides networks and connectivity for its instances.
Block Storage The OpenStack Block Storage API allows for manipulation of volumes, volume types (similar to compute flavors) and volume snapshots.
It can interact with a variety of storage providers through a driver architecture.
Much like nova-scheduler, the cinder-scheduler daemon picks the optimal block storage provider node to create the volume on.
OpenStack Block Storage deployments will also make use of a messaging queue to route information between the cinder processes as well as a database to store volume state.
Storage Concepts Storage is found in many parts of the OpenStack stack, and the differing types can cause confusion to even experienced cloud engineers.
Used for adding additional persistent storage to a virtual machine (VM)
On-instance / ephemeral Volumes block storage (Cinder) Object Storage (Swift)
Persists until VM is terminated Persists until deleted Persists until deleted.
Access associated with a VM Access associated with a VM Available from anywhere.
OpenStack Object Storage is not used like a traditional hard drive.
Object storage is all about relaxing some of the constraints of a POSIX-style file system.
The access to it is APIbased (and the API uses http)
This is a good idea as if you don't have to provide atomic operations (that is, you can rely on eventual consistency), you can much more easily scale a storage system and avoid a central point of failure.
The OpenStack Image Service is used to manage the virtual machine images in an OpenStack cluster, not store them.
Instead, it provides an abstraction to different methods for storage - a bridge to the storage, not the storage itself.
The Object Storage (swift) product can be used independently of the Compute (nova) product.
OpenStack Compute gives you a tool to orchestrate a cloud, including running instances, managing networks, and controlling access to the cloud through users and projects.
The underlying open source project's name is Nova, and it provides the software that can control an Infrastructure as a Service (IaaS) cloud computing platform.
OpenStack Compute does not include any virtualization software; rather it defines drivers that interact with underlying virtualization mechanisms that run on your host operating system, and exposes functionality over a webbased API.
Hypervisors OpenStack Compute requires a hypervisor and Compute controls the hypervisors through an API server.
The process for selecting a hypervisor usually means prioritizing and making decisions based on budget and resource constraints as well as the inevitable list of supported features and required technical specifications.
The majority of development is done with the KVM and Xen-based hypervisors.
With OpenStack Compute, you can orchestrate clouds using multiple hypervisors in different zones.
The types of virtualization standards that may be used with Compute include:
Users and Tenants (Projects) The OpenStack Compute system is designed to be used by many different cloud computing consumers or customers, basically tenants on a shared system, using role-based access assignments.
Roles control the actions that a user is allowed to perform.
For example, a rule can be defined so that a user cannot allocate a public IP without the admin role.
A user's access to particular images is limited by tenant, but the username and password are assigned per user.
Key pairs granting access to an instance are enabled per user, but quotas to control resource consumption across available hardware resources are per tenant.
Earlier versions of OpenStack used the term "project" instead of "tenant"
Because of this legacy terminology, some command-line tools use -project_id when a tenant ID is expected.
While the original EC2 API supports users, OpenStack Compute adds the concept of tenants.
Tenants are isolated resource containers forming the principal organizational structure within the Compute service.
They consist of a separate VLAN, volumes, instances, images, keys, and users.
A user can specify which tenant he or she wishes to be known as by appending :project_id to his or her access key.
If no tenant is specified in the API request, Compute attempts to use a tenant with the same ID as the user.
Total size of all volumes within a project as measured in GB.
Floating IP addresses (assigned to any instance when it launches so the instance has the same publicly accessible IP addresses)
Fixed IP addresses (assigned to the same instance each time it boots, publicly or privately accessible, typically private for management purposes)
Images and Instances This introduction provides a high level overview of what images and instances are and description of the life-cycle of a typical virtual system within the cloud.
There are many ways to configure the details of an OpenStack cloud and many ways to implement a virtual system within that cloud.
These configuration details as well as the specific command line utilities and API calls to preform the actions described are presented in the Image Management and Volume Management chapters.
Images are disk images which are templates for virtual machine file systems.
The image service, Glance, is responsible for the storage and management of images within OpenStack.
Instances are the individual virtual machines running on physical compute nodes.
Each instance is run from a copy of the base image so runtime changes made by an instance do not change the image it is based on.
Snapshots of running instances may be taken which create a new image based on the current disk state of a particular instance.
When starting an instance a set of virtual resources known as a flavor must be selected.
Flavors define how many virtual CPUs an instance has and the amount of RAM and size of its ephemeral disks.
OpenStack provides a number of predefined flavors which cloud administrators may edit or add to.
Users must select from the set of available flavors defined on their cloud.
Additional resources such as persistent volume storage and public IP address may be added to and removed from running instances.
The examples below show the cinder-volume service which provide persistent block storage as opposed to the ephemeral storage provided by the instance flavor.
Here is an example of the life cycle of a typical virtual system within an OpenStack cloud to illustrate these concepts.
Initial State The following diagram shows the system state prior to launching an instance.
The image store fronted by the image service, Glance, has some number of predefined images.
In the cloud there is an available compute node with available vCPU, memory and local disk resources.
Plus there are a number of predefined volumes in the cinder-volume service.
Launching an instance To launch an instance the user selects an image, a flavor and optionally other attributes.
In this case the selected flavor provides a root volume (as all flavors do) labeled vda in the diagram and additional ephemeral storage labeled vdb in the diagram.
The user has also opted to map a volume from the cinder-volume store to the third virtual disk, vdc, on this instance.
The OpenStack system copies the base image from the image store to local disk which is used as the first disk of the instance (vda), having small images will result in faster start up of your instances as less data needs to be copied across the network.
The system also creates a new empty disk image to present as the second disk (vdb)
Be aware that the second disk is an empty disk with an emphemeral life as it is destroyed when you delete the instance.
The compute node attaches to the requested cinder-volume using iSCSI and maps this to the third disk (vdc) as requested.
The vCPU and memory resources are provisioned and the instance is booted from the first drive.
The instance runs and changes data on the disks indicated in red in the diagram.
There are many possible variations in the details of the scenario, particularly in terms of what the backing storage is and the network protocols used to attach and move storage.
One variant worth mentioning here is that the ephemeral storage used for volumes vda and vdb in this example may be backed by network storage rather than local disk.
End State Once the instance has served its purpose and is deleted all state is reclaimed, except the persistent volume.
And of course the image has remained unchanged through out.
A "cloud controller" contains many of these components, and it represents the global state and interacts with all other components.
An API Server acts as the web services front end for the cloud controller.
The compute controller provides compute server resources and typically contains the compute service, The Object Store component optionally provides storage services.
An auth manager provides authentication and authorization services when used with the Compute system, or you can use the Identity Service (keystone) as a separate authentication service.
A volume controller provides fast and permanent block-level storage for the compute servers.
A network controller provides virtual networks to enable compute servers to interact with each other and with the public network.
A scheduler selects the most suitable compute controller to host an instance.
You can run all of the major components on multiple servers including a compute controller, volume controller, network controller, and object store (or image service)
A cloud controller communicates with the internal object store via HTTP (Hyper Text Transfer Protocol), but it communicates with a scheduler, network controller, and volume controller via AMQP (Advanced Message Queue Protocol)
To avoid blocking each component while waiting for a response, OpenStack Compute uses asynchronous calls, with a call-back that gets triggered when a response is received.
To achieve the shared-nothing property with multiple copies of the same component, OpenStack Compute keeps all the cloud system state in a database.
Block Storage and OpenStack Compute OpenStack provides two classes of block storage, "ephemeral" storage and persistent "volumes"
Ephemeral storage exists only for the life of an instance, it will persist across reboots of the guest operating system but when the instance is deleted so is the associated storage.
Volumes are persistent virtualized block devices independent of any particular instance.
Volumes may be attached to a single instance at a time, but may be detached or reattached to a different instance while retaining all data, much like a USB drive.
Ephemeral Storage Ephemeral storage is associated with a single unique instance.
Its size is defined by the flavor of the instance.
Data on ephemeral storage ceases to exist when the instance it is associated with is terminated.
Rebooting the VM or restarting the host server, however, will not destroy ephemeral data.
In the typical use case an instance's root filesystem is stored on ephemeral storage.
This is often an unpleasant surprise for people unfamiliar with the cloud model of computing.
Cloud aware operating system images may discover, format, and mount this device.
For example the cloud-init package included in Ubuntu's stock cloud images will format this space as an ext3 filesystem and mount it on / mnt.
It is important to note this a feature of the guest operating system.
Volume Storage Volume storage is independent of any particular instance and is persistent.
Volumes are user created and within quota and availability limits may be of any arbitrary size.
When first created volumes are raw block devices with no partition table and no filesystem.
They must be attached to an instance to be partitioned and/or formatted.
Once this is done they may be used much like an external disk drive.
Volumes may attached to only one instance at a time, but may be detached and reattached to either the same or different instances.
It is possible to configure a volume so that it is bootable and provides a persistent virtual instance similar to traditional non-cloud based virtualization systems.
In this use case the resulting instance may still have ephemeral storage depending on the flavor selected, but the root filesystem (and possibly others) will be on the persistent volume and thus state will be maintained even if the instance it shutdown.
Details of this configuration are discussed in the Boot From Volume section of this manual.
For that you need either a traditional network filesystem like NFS or CIFS or a cluster filesystem such as GlusterFS.
These may be built within an OpenStack cluster or provisioned outside of it, but are not features provided by the OpenStack software.
The OpenStack system has several key projects that are separate installations but can work together depending on your cloud needs: OpenStack Compute, OpenStack Block Storage, OpenStack Object Storage, and the OpenStack Image Service.
You can install any of these projects separately and then configure them either as standalone or connected entities.
The installation process is documented in the OpenStack Install Guide, for either Ubuntu or RHEL/CentOS/Fedora.
Compute and Image System Requirements Hardware: OpenStack components are intended to run on standard hardware.
Recommended hardware configurations for a minimum production deployment are as follows for the cloud controller nodes and compute nodes for Compute and the Image Service, and object, account, container, and proxy servers for Object Storage.
Cloud Controller node (runs network, volume, API, scheduler and image services)
Volume storage: two disks with 2 TB (SATA) for volumes attached to the compute nodes.
A quad core server with 12 GB RAM would be more than sufficient for a cloud controller node.
Specifically for virtualization on certain hypervisors on the node or nodes running nova-compute, you need a x86 machine with an AMD processor with SVM extensions (also called AMD-V) or an Intel processor with VT (virtualization technology) extensions.
For XenServer and XCP refer to the  XenServer installation guide and the  XenServer harware compatibility list.
These packages are maintained by community members, refer to http://wiki.openstack.org/Packaging for additional links.
Database: For OpenStack Compute, you need access to either a PostgreSQL or MySQL database, or you can install it as part of the OpenStack Compute installation process.
Permissions: You can install OpenStack services either as root or as a user with sudo permissions if you configure the sudoers file to enable all the permissions.
Network Time Protocol: You must install a time synchronization program such as NTP.
For Compute, time synchronization avoids problems when scheduling VM launches on compute nodes.
For Object Storage, time synchronization ensure the object replications are accurately updating objects when needed so that the freshest content is served.
Now you can declare the repository to libzypp with zypper ar.
After declaring the repository you have to update the metadata with zypper ref.
You can list all available packages for OpenStack with zypper se openstack.
To verify that you use the correct Python interpreter simply check the version.
Now you can declare the repository to libzypp with zypper ar.
After declaring the repository you have to update the metadata with zypper ref.
You can list all available packages for OpenStack with zypper se openstack.
Installing OpenStack Compute on Debian Starting with Debian 7.0 "Wheezy", the OpenStack packages are provided as part of the distribution.
Some non-official packages for Grizzly on Debian are available here:
For the management or controller node install the following packages: (via apt-get install)
Because this manual takes active advantage of the "sudo" command, it would be easier for you to add to it your Debian system, by doing:
Otherwise you will have to replace every "sudo" call by executing from root account.
Installing on Citrix XenServer When using OpenStack Compute with Citrix XenServer or XCP hypervisor, OpenStack Compute should be installed in a virtual machine running on your hypervisor, rather than installed directly on the hypervisor, as you would do when using the Libvirt driver.
Given how you should deploy OpenStack with XenServer, the first step when setting up the compute nodes in your OpenStack cloud is to install XenServer and install the required XenServer plugins.
You can install XCP by installing Debian or Ubuntu, but generally rather than installing the operating system of your choice on your compute nodes, you should first install XenServer.
Once you have installed XenServer and the XenAPI plugins on all your compute nodes, you next need to create a virtual machine on each of those compute nodes.
This must be a Linux virtual machine running in para-virtualized mode.
It is inside each of these VMs that you will run the OpenStack components.
You can follow the previous distribution specific instructions to get the OpenStack code running in your Virtual Machine.
Once installed, you will need to configure OpenStack Compute to talk to your XenServer or XCP installation.
The OpenStack system has several key projects that are separate installations but can work together depending on your cloud needs: OpenStack Compute, OpenStack Object Storage, and OpenStack Image Store.
There are basic configuration decisions to make, and the OpenStack Install Guide covers a basic walkthrough.
Configuring your Compute installation involves many configuration files - the nova.conf file, the api-paste.ini file, and related Image and Identity management configuration files.
This section contains the basics for a simple multi-node installation, but Compute can be configured many ways.
You can find networking options and hypervisor options described in separate chapters.
Setting Configuration Options in the nova.conf File The configuration file nova.conf is installed in /etc/nova by default.
A default set of options are already configured in nova.conf when you install manually.
Starting with the default file, you must define the following required items in /etc/nova/ nova.conf.
You can place comments in the nova.conf file by entering a new line with a # sign at the beginning of the line.
To see a listing of all possible configuration options, refer to the Compute Options Reference.
Here is a simple example nova.conf file for a small private cloud, with all the cloud controller services, database server, and messaging server on the same server.
Create a nova group, so you can set permissions on the configuration file:
The nova.conf file should have its owner set to root:nova, and mode set to 0640, since the file could contain your MySQL server’s username and password.
You also want to ensure that the nova user belongs to the nova group.
These are the commands you run to ensure the database schema is current:
Creating Credentials The credentials you will use to launch instances, bundle images, and all the other assorted API functions can be sourced in a single file, such as creating one called /creds/openrc.
Lastly, here is an example openrc file that works with nova client and ec2 tools.
Next, add these credentials to your environment prior to running any nova client commands or nova commands.
Creating Certificates You can create certificates contained within pem files using these nova client commands, ensuring you have set up your environment variables for the nova client:
Creating networks You need to populate the database with the network configuration information that Compute obtains from the nova.conf file.
You can find out more about the nova network-create command with nova help network-create.
Here is an example of what this looks like with real values entered.
This example would be appropriate for FlatDHCP mode, for VLAN Manager mode you would also need to specify a VLAN.
Currently, there can only be one network, and this set up would use the max IPs available in a /24
You can choose values that let you use any valid amount that you would like.
You can alter the gateway using the -gateway flag when invoking nova network-create.
You are unlikely to need to modify the network or broadcast addresseses, but if you do, you will need to manually edit the networks table in the database.
Enabling Access to VMs on the Compute Node One of the most commonly missed configuration areas is not allowing the proper access to VMs.
Below, you will find the commands to allow ping and ssh to your VMs :
Note These commands need to be run as root only if the credentials used to interact with nova-api have been put under /root/.bashrc.
If the EC2 credentials have been put into another user's .bashrc file, then, it is necessary to run these commands as the user.
Another common issue is you cannot ping or SSH to your instances after issuing the eucaauthorize commands.
Something to look at is the amount of dnsmasq processes that are running.
If you have a running instance, check to see that TWO dnsmasq processes are running.
If you get the instance not found message while performing the restart, that means the service was not previously running.
You simply need to start it instead of restarting it:
Configuring Multiple Compute Nodes If your goal is to split your VM load across more than one server, you can connect an additional nova-compute node to a cloud controller node.
This configuring can be reproduced on multiple compute servers to start building a true multi-node OpenStack Compute cluster.
To build out and scale the Compute platform, you spread out services amongst many servers.
While there are additional ways to accomplish the build-out, this section describes adding compute nodes, and the service we are scaling out is called nova-compute.
For a multi-node install you only make changes to nova.conf and copy it to additional compute nodes.
Ensure each nova.conf file points to the correct IP addresses for the respective services.
With nova.conf updated and networking set, configuration is nearly complete.
First, bounce the relevant services to take the latest updates:
To avoid issues with KVM and permissions with Nova, run the following commands to ensure we have VM's that are running optimally:
Any server that does not have nova-api running on it needs this iptables entry so that UEC images can get metadata info.
On compute nodes, configure the iptables with this next step:
Lastly, confirm that your compute node is talking to your cloud controller.
Determining the Version of Compute You can find the version of the installation by using the nova-manage command:
Diagnose your compute nodes You can obtain extra informations about the running virtual machines: their CPU usage, the memory, the disk IO or network IO, per instance, by running the nova diagnostics command with a server ID:
The output of this command will vary depending on the hypervisor.
While the command should work with any hypervisor that is controlled through libvirt (e.g., KVM, QEMU, LXC), it has only been tested with KVM.
General Compute Configuration Overview Most configuration information is available in the nova.conf configuration option file.
Here are some general purpose configuration options that you can use to learn more about the configuration option file and the node.
You can use a particular configuration option file by using the option (nova.conf) parameter when running one of the nova-* services.
If you want to maintain the state of all the services, you can use the state_path configuration option to indicate a top-level directory for storing data related to the state of Compute including images if you are using the Compute object store.
It is not necessarily a hostname, FQDN, or IP address.
However, the node name must be valid within an AMQP key, and if using ZeroMQ, a valid hostname, FQDN, or IP address.
Valid values are False for no notifications, True for notifications on any instance changes.
Example nova.conf Configuration Files The following sections describe many of the configuration option settings that can go into the nova.conf files.
Copies of each nova.conf file need to be copied to each.
Here are some sample nova.conf files that offer examples of specific configurations.
KVM, Flat, MySQL, and Glance, OpenStack or EC2 API This example nova.conf file is from an internal Rackspace test system used for demonstrations.
This example nova.conf file is from an internal Rackspace test system.
Configuring Logging You can use nova.conf configuration options to indicate where Compute will log events, the level of logging, and customize log formats.
To customize log formats for OpenStack Compute, use these configuration option settings.
Please see the Python logging module documentation for details on logging configuration files.
If no default is set, logging will go to stdout.
Configuring Hypervisors OpenStack Compute requires a hypervisor and supports several hypervisors and virtualization standards.
Configuring and running OpenStack Compute to use a particular hypervisor takes several installation and configuration steps.
The libvirt_type configuration option indicates which hypervisor will be used.
To customize hypervisor support in OpenStack Compute, refer to these configuration settings in nova.conf.
If default is specified, then use_cow_images flag is used instead of this one.
We fall back to hard reboot if instance does not shutdown within this window.
Configuring Authentication and Authorization There are different methods of authentication for the OpenStack Compute project, including no authentication.
To customize authorization settings for Compute, see these configuration settings in nova.conf.
To customize certificate authority settings for Compute, see these configuration settings in nova.conf.
To customize Compute and the Identity service to use LDAP as a backend, refer to these configuration settings in nova.conf.
IPv4/IPv6 dual stack mode works with VlanManager and FlatDHCPManager networking modes.
In VlanManager, different 64bit global routing prefix is used for each project.
In FlatDHCPManager, one 64bit global routing prefix is used for all instances.
Each node that executes a nova-* service must have python-netaddr and radvd installed.
On all nova-network nodes install radvd and configure IPv6 networking:
Edit the nova.conf file on all nodes to set the use_ipv6 configuration option to True.
When using the command nova network-create you can add a fixed range for IPv6 addresses.
You must specify public or private after the create parameter.
When you use FlatDHCPManager, the command uses the original value of --fixed_range_v6
When you use VlanManager, the command creates prefixes of subnet by incrementing subnet id.
Guest VMs uses this prefix for generating their IPv6 global unicast address.
Compute relies on an external image service to store virtual machine images and maintain a catalog of available images.
Compute is configured by default to use the OpenStack Image service (Glance), which is the only currently supported image service.
If your installation requires the use of euca2ools for registering new images, you will need to run the nova-objectstore service.
Configuring Migrations Note This feature is for cloud administrators only.
Migration allows an administrator to move a virtual machine instance from one compute host to another.
This feature is useful when a compute host requires maintenance.
Migration can also be useful to redistribute the load when many VM instances are running on a specific physical machine.
Migration (or non-live migration): In this case the instance will be shut down (and the instance will know that it has been rebooted) for a period of time in order to be moved to another hypervisor.
Live migration (or true live migration): Almost no instance downtime, it is useful when the instances must be kept running during the migration.
Shared storage based live migration: In this case both hypervisors have access to a shared storage.
Block live migration: for this type of migration, no shared storage is required.
The following sections describe how to configure your hosts and compute nodes for migrations using the KVM and XenServer hypervisors.
This guide uses NFS but other options, including the OpenStack Gluster Connector are available.
Migrations done by the Compute service do not use libvirt's live migration functionality by default.
Because of this, guests are suspended before migration and may therefore experience several minutes of downtime.
Prepare 3 servers at least; for example, HostA, HostB and HostC.
HostA is the "Cloud Controller", and should be running: nova-api, nova-scheduler, nova-network, cinder-volume, nova-objectstore.
Ensure that, NOVA-INST-DIR (set with state_path in nova.conf) is same on all hosts.
In this example, HostA will be the NFSv4 server which exports NOVA-INST-DIR/ instances, and HostB and HostC mount it.
Configure your DNS or /etc/hosts and ensure it is consistent across all hosts.
Make sure that the three hosts can perform name resolution with each other.
As a test, use the ping command to ping each host from one another.
Ensure that the UID and GID of your nova and libvirt users are identical between each of your servers.
This ensures that the permissions on the NFS mount will work correctly.
On both compute nodes, make sure to enable the 'execute/search' bit to allow qemu to be able to use the images within the directories.
Configure NFS at HostB and HostC by adding below to /etc/fstab.
Perform the same check at HostB and HostC - paying special attention to the permissions (nova should be able to write)
After executing the command, ensure that libvirt is successfully restarted.
Configure your firewall to allow libvirt to communicate between nodes.
As this guide has disabled libvirt auth, you should take good care that these ports are only open to hosts within your installation.
In most cases, you do not need to configure any options.
By default, the Compute service does not use libvirt's live migration functionality.
To enable this functionality, add the following line to nova.conf:
The Compute service does not use libvirt's live miration by default because there is a risk that the migration process will never terminate.
This can happen if the guest operating system dirties blocks on the disk faster than they can migrated.
Shared storage: an NFS export, visible to all XenServer hosts.
In order to use shared storage live migration with XenServer hypervisors, the hosts must be joined to a XenServer pool.
In order to create that pool, a host aggregate must be created with special metadata.
This metadata will be used by the XAPI plugins to establish the pool.
Add an NFS VHD storage to your master XenServer, and set it as default SR.
For more information, please refer to the NFS VHD section of the XenServer Administrator's Guide.
Configure all the compute nodes to use the default sr for pool operations, by including:
The command will display a table which contains the id of the newly created aggregate.
Now add special metadata to the aggregate, to mark it as a hypervisor pool.
At this point, the host is part of a XenServer pool.
At this point the added compute node and the host will be shut down, in order to join the host to the XenServer pool.
The operation will fail, if any server other than the compute node is running/suspended on your host.
Please refer to the manual of your XenServer to make sure your edition has this feature.
Please note, that you need to use an extra option --block-migrate for the live migration command, in order to use block migration.
Please note, that block migration works only with EXT local storage SRs, and the server should not have any volumes attached.
Configuring Resize Resize (or Server resize) is the ability to change the flavor of a server, thus allowing it to upscale or downscale according to user needs.
In order for this feature to work properly, some underlying virt layers may need further configuration; this section describes the required configuration steps for each hypervisor layer provided by OpenStack.
XenServer To get resize to work with XenServer (and XCP), please refer to the Dom0 Modifications for Resize/Migration Support section.
In the previous section we presented a convenient way to deploy a shared storage using NFS.
MooseFS (Moose File System) is a shared file system ; it implements the same rough concepts of shared storage solutions - such as Ceph, Lustre or even GlusterFS.
A metadata server (MDS), also called master server, which manages the file repartition, their access and the namespace.
A metalogger server (MLS) which backs up the MDS logs, including, objects, chunks, sessions and object metadata.
A chunk server (CSS) which store the data as chunks and replicate them across the chunkservers.
A client, which talks with the MDS and interact with the CSS.
Two compute nodes running both MooseFS chunkserver and client services.
For that particular walkthrough, we will use the following network schema :
Installing the MooseFS metadata and metalogger servers Both components could be run anywhere , as long as the MooseFS chunkservers can reach the MooseFS master server.
In our deployment, both MooseFS master and slave run their services inside a virtual machine ; you just need to make sure to allocate enough memory to the MooseFS metadata server, all the metadata being stored in RAM when the service runs.
Install the required packages by running the following commands :
Go to the MooseFS download page and fill the download form in order to obtain your URL for the package.
For the MooseFS master server installation, we disable from the compilation the mfschunkserver and mfsmount components :
We will keep the default settings, for tuning performance, you can read the MooseFS official FAQ.
You can now start the mfsmaster and mfscgiserv deamons on the MooseFS metadataserver (The mfscgiserv is a webserver which allows you to see via a web interface the MooseFS status realtime) :
Open the following url in your browser : http://10.0.10.16:9425 to see the MooseFS status page.
Installing the MooseFS chunk and client services In the first part, we will install the last version of FUSE, and proceed to the installation of the MooseFS chunk and client in the second part.
For that setup we will retrieve the last version of fuse to make sure every function will be available :
For the MooseFS chunk server installation, we only disable from the compilation the mfsmaster component :
You only need to create on every server directories that will be used for storing the datas of your cluster.
Access to your cluster storage You can now access your cluster space from the compute node, (both acting as chunkservers) :
You can see the list of the available tools by running.
In order to make sure to have the storage mounted, you can add an entry into the /etc/ fstab on both compute nodes :
The database name is nova and entries to it are mostly written by the nova-scheduler service, although all the services need to be able to update entries in the database.
Use these settings to configure the connection string for the nova database.
Configuring the Oslo RPC Messaging System OpenStack projects use an open standard for messaging middleware known as AMQP.
This messaging middleware enables the OpenStack services which will exist across multiple servers to talk to each other.
This section discusses the configuration options that are relevant when RabbitMQ is used.
The rpc_backend option is not required as long as RabbitMQ is the default messaging system.
The following tables describe the rest of the options that can be used when RabbitMQ is used as the messaging system.
You can configure the messaging communication for different installation scenarios as well as tune RabbitMQ's retries and the size of the RPC thread pool.
Configuration for Qpid This section discusses the configuration options that are relevant if Qpid is used as the messaging system for OpenStack Oslo RPC.
Qpid is not the default messaging system, so it must be enabled by setting the rpc_backend option in nova.conf.
This next critical option points the compute nodes to the Qpid broker (server)
Set qpid_hostname in nova.conf to be the hostname where the broker is running.
The --qpid_hostname option accepts a value in the form of either a hostname or an IP address.
If you configure the Qpid broker to require authentication, you will need to add a username and password to the configuration:
If you would like to enable SSL, set the qpid_protocol option:
The following table lists the rest of the options used by the Qpid messaging driver for OpenStack Oslo RPC.
Common Configuration for Messaging This section lists options that are common between both the RabbitMQ and Qpid messaging drivers.
Various topics by message type will be appended to this.
The OpenStack Compute API allows the user to specify an admin password when creating (or rebuilding) a server instance.
If no password is specified, a randomly generated password is used.
In practice, the handling of the admin password depends on the hypervisor in use, and may require additional configuration of the instance, such as installing an agent to handle the password setting.
If the hypervisor and instance configuration do not support the setting of a password at server create time, then the password returned by the create API call will be misleading, since it was ignored.
When false, the server admin password is not included in API responses.
The rate limiting allows an administrator to configure limits on the type and number of API calls that can be made in a specific time interval.
When API rate limits are exceeded, HTTP requests will return a error with a status code of 413 "Request entity too large", and will also include a 'Retry-After' HTTP header.
The response body will include the error details, and the delay before the request should be retried.
A human readable URI that is used as a friendly description of where the limit is applied.
The limit will be applied to all URI's that match the regular expression and HTTP Method.
A limit value  that specifies the maximum count of units before the limit takes effect.
An interval that specifies time frame the limit is applied to.
Rate limits are applied in order, relative to the HTTP method, going from least to most specific.
Default Limits OpenStack compute is normally installed with the following limits enabled:
To enable limits, ensure the 'ratelimit' filter is included in the API pipeline specification.
If the 'ratelimit' filter is removed from the pipeline, limiting will be disabled.
There should also be a definition for the ratelimit filter.
To modify the limits, add a 'limits' specification to the [filter:ratelimit] section of the file.
The limits are specified in the order HTTP method, friendly URI, regex, limit, and interval.
To customize these options for OpenStack EC2 API, use these configuration option settings.
Configuring Quotas For tenants, quota controls are available to limit the (flag and default shown in parenthesis):
Total size of all volumes within a project as measured in GB (gigabytes=1000)
Publicly accessible IP addresses for persistence in DNS assignment (floating_ips=10)
Privately (or publicly) accessible IP addresses for management purposes (fixed_ips=-1 unlimited)
Amount of RAM that can be allocated in MB (ram=512000)
The defaults may be modified by setting the variable in nova.conf, then restarting the nova-api service.
To modify a value for a specific project, the nova-manage command should be used.
Alternately, quota settings are available through the OpenStack Dashboard in the "Edit Project" page.
The Compute service supports a large number of configuration options.
The configuration file is in INI file format, with options specified as key=value pairs, grouped into sections.
Almost all of the configuration options are in the DEFAULT section.
Types of configuration options Each configuration option has an associated type that indicates what values can be set.
Same as StrOpt, except that it can be declared multiple times to indicate multiple values.
Value is a list of arbitrary strings separated by commas.
If the documentation for a configuration option does not specify its section, assume that it should be placed in this one.
Options in this section describe how to connect to a remote attestation service.
Once a configuration option is set, it can be referenced in later configuration values when preceded by $
Whitespace To include whitespace in a configuration value, use a quoted string.
Specifying an alternate location for nova.conf The configuration file is loaded by all of the nova-* services, as well as the nova-manage command-line tool.
The benefit is that the instances end up with different hostnames.
To restore legacy behavior of every instance having the same name, set this option to "%(name)s"
If a node does not have a fixed PXE IP address, volumes are exported with globally opened ACL.
It is not necessarily a hostname, FQDN, or IP address.
However, the node name must be valid within an AMQP key, and if using ZeroMQ, a valid hostname, FQDN, or IP address.
Valid values are False for no notifications, True for notifications on any instance changes.
If left blank, an administrative share will be used, looking for the same "instances_path" used locally.
If default is specified, then use_cow_images flag is used instead of this one.
We fall back to hard reboot if instance does not shutdown within this window.
Please see the Python logging module documentation for details on logging configuration files.
If no default is set, logging will go to stdout.
If this is set to all then all traffic will be forwarded.
Also, if set, some rpc network calls will be sent directly to host.
Only first nic of vm will get default gateway from dhcp server.
Make sure this path can fit your biggest image in glance.
Make sure this path can fit your biggest image in glance.
Various topics by message type will be appended to this.
This property defines the subset size that a host is chosen from.
A value of 1 chooses the first host returned by the weighing functions.
Due to a bug in vSphere ESX 4.1 default wsdl.
The default value is the Local Storage in default XenServer/XCP installations.
On the other hand, to fall back on the Default SR, as displayed by XenCenter, set this flag to: default-sr:true.
If the agent is present, network configuration is not injected into the image.
Reduces the amount of time it takes nova to detect that a VM has started, when that VM does not have the agent installed.
This speeds up resizes down considerably since large runs of zeros won't have to be rsynced.
Should be a wildcard (*), an ethernet interface, or IP.
The "host" option should point or resolve to this address.
Provides a catalog of available services with their API endpoints.
To understand the Identity Service, you must understand the following concepts:
User Digital representation of a person, system, or service who uses OpenStack cloud services.
Identity authentication services will validate that incoming request are being made by the user who claims to be making the call.
Users have a login and may be assigned tokens to access resources.
Users may be directly assigned to a particular tenant and behave as if they are contained in that tenant.
Credentials Data that is known only by a user that proves who they are.
Authentication The act of confirming the identity of a user.
The Identity Service confirms an incoming request by validating a set of credentials.
These credentials are initially a username and password or a username and API key.
In response to these credentials, the Identity Service issues the user an authentication token, which the user provides in subsequent requests.
Token An arbitrary bit of text that is used to access resources.
Each token has a scope which describes which resources are accessible with it.
A token may be revoked at anytime and is valid for a finite duration.
While the Identity Service supports token-based authentication in this release, the intention is for it to support additional protocols in the future.
The intent is for it to be an integration service foremost, and not aspire to be a full-fledged identity store and management solution.
Tenant A container used to group or isolate resources and/or identity objects.
Depending on the service operator, a tenant may map to a customer, account, organization, or project.
Provides one or more endpoints through which users can access resources and perform operations.
Endpoint An network-accessible address, usually described by URL, from where you access a service.
If using an extension for templates, you can create an endpoint template, which represents the templates of all the consumable services that are available across the regions.
Role A personality that a user assumes that enables them to perform a specific set of operations.
A user assuming that role inherits those rights and privileges.
In the Identity Service, a token that is issued to a user includes the list of roles that user can assume.
Services that are being called by that user determine how they interpret the set of roles a user has and which operations or resources each role grants access to.
User management The main components of Identity user management are:
A user represents a human user, and has associated information such as username, password and email.
Whenever you make requests to OpenStack services, you must specify a tenant.
For example, if you query the Compute service for a list of running instances, you will receive a list of all of the running instances in the tenant you specified in your query.
Because the term project was used instead of tenant in earlier versions of OpenStack Compute, some command-line tools use --project_id instead of --tenant-id or --os-tenant-id to refer to a tenant ID.
A role captures what operations a user is permitted to perform in a given tenant.
It is up to individual services such as the Compute service and Image service to assign meaning to these roles.
As far as the Identity service is concerned, a role is simply a name.
The Identity service associates a user with a tenant and a role.
To continue with our previous examples, we may wish to assign the "alice" user the "compute-user" role in the "acme" tenant:
A user can be assigned different roles in different tenants: for example, Alice may also have the "admin" role in the "Cyberdyne" tenant.
A user can also be assigned multiple roles in the same tenant.
The default policy.json files in the Compute, Identity, and Image service recognize only the admin role: all operations that do not require the admin role will be accessible by any user that has any role in a tenant.
If we wished to restrict all Compute service requests to require this role, the resulting file would look like:
Service management The Identity Service provides the following service management functions:
The Identity Service also maintains a user that corresponds to each service (such as, a user named nova, for the Compute service) and a special service tenant, which is called service.
The commands for creating services and endpoints are described in a later section.
Memcached uses the host's system time in determining whether a key has expired, whereas Keystone sets key expiry in UTC.
The timezone used by Keystone and memcached must match if key expiry is to behave as expected.
The x509 certificates used by Keystone must be obtained externally and configured for use with Keystone as described in this section.
Here is the description of each of them and their purpose:
Types of certificates cacert.pem Certificate Authority chain to validate against.
Note that you may choose whatever names you want for these certificates, or combine the public/private keys in the same file if you wish.
If the private key is included in the certfile, the keyfile maybe omitted.
User CRUD Keystone provides a user CRUD filter that can be added to the public_api pipeline.
This user crud filter allows users to use a HTTP PATCH to change their own password.
Each user can then change their own password with a HTTP PATCH.
In addition to changing their password all of the users current tokens will be deleted (if the backend used is kvs or sql)
Configuration Files The Identity configuration file is an 'ini' file format with sections, extended from Paste, a common system used to configure python WSGI based applications.
In addition to the paste config entries, general configuration values are stored under [DEFAULT], [sql], [ec2] and then drivers for the various services are included under their individual sections.
When starting Identity, you can specify a different configuration file to use with --config-file.
If you do not specify a configuration file, keystone will look in the following directories for a configuration file, in order:
Logging Logging is configured externally to the rest of Identity, the file specifying the logging configuration is in the [DEFAULT] section of the keystone.conf file under log_config.
If you wish to route all your logging through syslog, set use_syslog=true option in the [DEFAULT] section.
For example in Nova, all middleware parameters can be removed from apipaste.ini like these:
Monitoring Keystone provides some basic request/response monitoring statistics out of the box.
Enable data collection by defining a stats_monitoring filter and including it at the beginning of any desired WSGI pipelines:
In order to work correctly token generation requires a public/ private key pair.
The public key must be signed in an X509 certificate, and the certificate used to sign it must be available as Certificate Authority (CA) certificate.
These files can be generated either using the keystone-manage utility, or externally generated.
The files need to be in the locations specified by the top level Keystone configuration file as specified in the above section.
Additionally, the private key should only be readable by the system user that will run Keystone.
The certificates can be world readable, but the private key cannot be.
The private key should only be readable by the account that is going to sign tokens.
When generating files with the keystone-mange pki_setup command, your best option is to run as the pki user.
If you run nova-manage as root, you can append --keystone-user and --keystone-group parameters to set the username and group keystone is going to run under.
The values that specify where to read the certificates are under the [signing] section of the configuration file.
If token_format=PKI, a typical token will be a much longer string, e.g.:
Signing Certificate Issued by External CA You may use a signing certificate issued by an external CA instead of generated by keystone-manage.
However, certificate issued by external CA must satisfy the following conditions:
The basic workflow for using a signing certificate issued by an external CA involves:
Also, make sure your trusted CA certificate chain is also in PEM format.
Make sure the certificate directory is only accessible by root.
If your certificate directory path is different from the default /etc/keystone/ssl/ certs, make sure it is reflected in the [signing] section of the configuration file.
Running Running Identity is simply starting the services by using the command:
Invoking this command starts up two wsgi.Server instances, configured by the keystone.conf file as described above.
One of these wsgi 'servers' is admin (the administration API) and the other is main (the primary/public API interface)
Initializing Keystone keystone-manage is designed to execute commands that cannot be administered through the normal REST api.
Generally, the following is the first step after a source installation:
Invoking keystone-manage by itself will give you additional usage information.
Only users with admin credentials can administer users, tenants and roles.
Token Auth Method To use keystone client using token auth, set the following flags:
The keystone client detects the version of the API from this parameter.
For example, the following parameters indicate the use of API v3:
Example usage The keystone client is set up to expect commands in the general form of keystone command argument, followed by flag-like keyword arguments to provide additional (often optional) information.
For example, the command user-list and tenantcreate can be invoked as follows:
Tenants A tenant is a group of zero or more users.
Each tenant and user pairing can have a role associated with it.
Configuring Services to work with Keystone Once Keystone is installed and running, services need to be configured to work with it.
To do this, we primarily install and configure middleware for the OpenStack service to handle authentication tasks or otherwise interact with Keystone.
Clients making calls to the service will pass in an authentication token.
The Keystone middleware will look for and validate that token, taking the appropriate action.
It will also retrieve additional information from the token such as user name, id, tenant name, id, roles, etc...
The middleware will pass those data down to the service as headers.
Setting up credentials To ensure services that you add to the catalog know about the users, tenants, and roles, you must create an admin token and create service users.
For a default installation of Keystone, before you can use the REST API, you need to define an authorization token.
This is configured in keystone.conf file under the section [DEFAULT]
In the sample file provided with the keystone project, the line defining this token is.
This configured token is a "shared secret" between keystone and other OpenStack services, and is used by the client to communicate with the API to create tenants, users, roles, etc.
You need to minimally define a tenant, user, and role to link the tenant and user as the most basic set of details to get other services authenticating and authorizing with keystone.
You will also want to create service users for Compute (nova), Image (glance), Object Storage (swift), etc.
See the configuration section for a walk through on how to create tenants, users, and roles.
To configure the OpenStack services with service users, we need to create a tenant for all the services, and then users for each of the services.
We then assign those service users an Admin role on the service tenant.
This allows them to validate tokens - and authenticate and authorize other user requests.
Create a tenant for the services, typically named 'service' (however, the name can be whatever you choose):
This returns a UUID of the tenant - keep that, you'll need it when creating the users and specifying the roles.
Create service users for nova, glance, swift, and quantum (or whatever subset is relevant to your deployment):
Email is a required field in keystone right now, but not used in relation to the service accounts.
Each of these commands will also return a UUID of the user.
For adding the Admin role to the service accounts, you'll need to know the UUID of the role you want to add.
If you don't have them handy, you can look it up quickly with:
Once you have it, assign the service users to the Admin role.
This is all assuming that you've already created the basic roles and settings as described in the configuration section:
Defining Services Keystone also acts as a service catalog to let other OpenStack systems know where relevant API endpoints exist for OpenStack Services.
The OpenStack Dashboard, in particular, uses this heavily - and this must be configured for the OpenStack Dashboard to properly function.
When keystone uses a template file backend, then changes made to the endpoints are kept in memory and don't persist if you restart the service or reboot the machine.
Use the SQL backend when deploying a system for production.
Keystone supports two means of defining the services, one is the catalog template, as described above - in which case everything is detailed in that template.
The other is a SQL backend for the catalog service, in which case after keystone is online, you need to add the services to the catalog:
The Keystone auth_token middleware is a WSGI component that can be inserted in the WSGI pipeline to handle authenticating tokens with Keystone.
Configuring Swift to use Keystone Similar to Nova, swift can be configured to use Keystone for authentication rather than its built in 'tempauth'
Reconfigure Swift's proxy server to use Keystone instead of TempAuth.
Verify that the Identity service, Keystone, is providing authentication to Object Storage (Swift)
Configuring Keystone for an LDAP backend As an alternative to the SQL Database backing store, Keystone can use a directory server to provide the Identity service.
They reflect the common standard objects according to the LDAP RFCs.
However, in a live deployment, the correct attributes can be overridden to support a preexisting, more complex schema.
For example, in the user object, the objectClass posixAccount from RFC2307 is very common.
If this is the underlying objectclass, then the uid field should probably be uidNumber and username field either uid or cn.
To change these two fields, the corresponding entries in the Keystone configuration file are:
There is a set of allowed actions per object type that you can modify depending on your specific deployment.
For example, the users are managed by another tool and you have only read access, in such case the configuration is:
There are some configuration options for filtering users, tenants and roles, if the backend is providing too much output, in such case the configuration will look like:
In case that the directory server does not have an attribute enabled of type boolean for the user, there are several configuration parameters that can be used to extract the value from an integer attribute like in Active Directory:
It also saves the value without mask to the user identity in the attribute enabled_nomask.
This is needed in order to set it back in case that we need to change it to enable/disable a user because it contains more information than the status like password expiration.
In case of Active Directory the classes and attributes could not match the specified classes in the LDAP module so you can configure them like so:
If 'admin_token' is specified it will by used only if the specified token is still valid.
It should be noted that when using this option an admin tenant/role relationship is required.
The admin user is granted access to the 'Admin' role on the 'admin' tenant.
Configuring Keystone SSL support Keystone may be configured to support 2-way SSL out-of-the-box.
The x509 certificates used by Keystone must be obtained externally and configured for use with Keystone as described in this section.
However, a set of sample certificates is provided in the.
Here is the description of each of them and their purpose:
Note that you may choose whatever names you want for these certificates, or combine the public/private keys in the same file if you wish.
If the private key is included in the certfile, the keyfile may be omitted.
When Keystone is executed in apache-httpd it is possible to use external authentication methods different from the authentication provided by the identity store backend.
For example, this makes possible to use a SQL identity backend together with X.509 authentication, Kerberos, etc.
Keystone can profit from this feature and let the authentication be done in the webserver, that will pass down the authenticated user to Keystone using the REMOTE_USER environment variable.
This user must exist in advance in the identity backend so as to get a token from the controller.
To use this method, OpenStack Identity should be running on apachehttpd.
Troubleshooting Identity (Keystone) If you see an error opening the signing key file, it's possible the person who ran keystonemanage pki_setup to generate certificates and keys isn't using the correct user.
This is problematic when trying to then run the Keystone daemon under the 'keystone' user account (nologin) when trying to run PKI.
Unless you manually chown the files keystone:keystone or run keystone-manage pki_setup with --keystone-user and --keystonegroup you'll get an error like this:
The OpenStack Image Service, code-named glance, provides functionality for discovering, registering, and retrieving virtual machine images.
The service includes a RESTful API that allows users to query VM image metadata and retrieve the actual image with HTTP requests.
You can also use the glance command-line tool, or the Python API to accomplish the same tasks.
Filesystem - The default backend that OpenStack Image Service uses to store virtual machine images is the filesystem backend.
This simple backend writes image files to the local filesystem.
Rados Block Device (RBD) - This backend stores images inside of a Ceph storage cluster using Ceph's RBD interface.
This chapter assumes you have a working installation of the Image Service, with a working endpoint and users created in the Identity Service, plus you have sourced the environment variables required by the nova client and glance client.
For some deployers, storing all images in a single place for all tenants and users to access is not ideal.
To enable access control to specific images for cloud users, you can configure the Image service with the ability to store image data in the image owner-specific locations.
Adding images with glance image-create Use the glance image-create command to add a new virtual machine image to glance, and use glance image-update to modify properties of an image that has been updated.
The image-create command takes several optional arguments, but you should specify a name for your image using the --name flag, as well as the disk format with --diskformat and container format with --container-format.
Pass in the file via standard input or using the file command.
Disk and Container Formats for Images When adding an image to the Image service (glance), you may specify what the virtual machine image’s disk format and container format are.
The disk format of a virtual machine image is the format of the underlying disk image.
Virtual appliance vendors have different formats for laying out the information contained in a virtual machine disk image.
You can set your image’s disk format to one of the following:
This is an unstructured disk image format; if you have a file without an extension it is possibly a raw format.
This is the VHD disk format, a common disk format used by virtual machine monitors from VMWare, Xen, Microsoft, VirtualBox, and others.
Another common disk format supported by many common virtual machine monitors.
A disk format supported by VirtualBox virtual machine monitor and the QEMU emulator.
A disk format supported by the QEMU emulator that can expand dynamically and supports Copy on Write.
This indicates what is stored in Glance is an Amazon kernel image.
This indicates what is stored in Glance is an Amazon ramdisk image.
This indicates what is stored in Glance is an Amazon machine image.
Container Format The container format refers to whether the virtual machine image is in a file format that also contains metadata about the actual virtual machine.
Note that the container format string is not currently used by Glance or other OpenStack components, so it is safe to simply specify bare as the container format if you are unsure.
You can set your image’s container format to one of the following:
This indicates there is no container or metadata envelope for the image.
This indicates what is stored in Glance is an Amazon kernel image.
This indicates what is stored in Glance is an Amazon ramdisk image.
This indicates what is stored in Glance is an Amazon machine image.
Image metadata You can associate metadata with an image using the --property key=value argument to glance image-create or glance image-update.For example:
Run uname -m to get the architecture of a machine.
We strongly recommend using the architecture data vocabulary defined by the libosinfo project for this purpose.
This represents the host/guest ABI (application binary interface) used for the virtual machine.
The following metadata properties are specific to the XenAPI driver:
If true, the root partition on the disk will be automatically resized before the instance boots.
This value is only taken into account by the Compute service when using a Xenbased hypervisor with the XenAPI driver.
The XenAPI driver contains logic that will take different actions depending on the value of the os_type parameter of the image.
The following metadata properties are specific to the VMware API driver:
This will be passed to the hypervisor when creating a virtual machine.
If this is not specified, it will default to otherGuest.
In order to assist end-users in utilizing images, you may wish to put additional common metadata on Glance images.
By community agreement, the following metadata keys may be used across Glance installations for the purposes described below.
For this purpose, we use the same data vocabulary as the libosinfo project.
In the interest of interoperability, please use only a recognized value for this field.
The deprecated values are listed to assist you in searching for the recognized value.
Tool support for creating images There are several open-source third-party tools available that simplify the task of creating new virtual machine images.
Customizing an image for OpenStack The OpenStack Virtual Machine Image Guide describes what customizations you should to your image to maximize compatibility with OpenStack.
Raw images are the simplest image file format and are supported by all of the hypervisors.
They take up less space than raw images (growing in size as needed), and they support snapshots.
QCOW2 images are only supported with KVM and QEMU hypervisors.
Booting a test image The following assumes you are using QEMU or KVM in your deployment.
Check that adding the image was successful (Status should be ACTIVE when the operation is complete):
Create a keypair so you can ssh to the instance:
In general, you need to use an ssh keypair to log in to a running instance, although some images have built-in accounts created with associated passwords.
However, since images are often shared by many users, it is not advised to put passwords into the images.
Nova therefore supports injecting ssh keys into instances before they are booted.
This allows a user to log in to the instances that he or she creates securely.
Generally the first thing that a user does when using the system is create a keypair.
As part of the first boot of a virtual image, the private key of your keypair is added to authorized_keys file of the login account.
Nova generates a public and private key pair, and sends the private key to the user.
The public key is stored so that it can be injected into instances.
View the list of available flavors by running nova flavor-list.
The instance will go from BUILD to ACTIVE in a short time, and you should be able to connect via ssh as 'cirros' user, using the private key you created.
The 'cirros' user is part of the sudoers group, so you can escalate to 'root' via the following command when logged in to the instance:
Tearing down (deleting) Instances When you are done with an instance, you can tear it down using the nova delete command, passing either the instance name or instance ID as the argument.
You can get a listing of the names and IDs of all running instances using the nova list.
Pausing and Suspending Instances Since the release of the API in its 1.1 version, it is possible to pause and suspend instances.
Pause/ Unpause : Stores the content of the VM in memory (RAM)
Suspend/ Resume : Stores the content of the VM on disk.
It can be interesting for an administrator to suspend instances, if a maintenance is planned; or if the instance are not frequently used.
Suspending an instance frees up memory and vCPUS, while pausing keeps the instance running, in a "frozen" state.
Select a specific host to boot instances on If you have the appropriate permissions, you can select the specific host where the instance will be launched.
Starting with the Grizzly release, you can specify which roles are permitted to boot an instance to a specific host with the create:forced_host setting within policy.json on the desired roles.
By default, only the admin role has this setting enabled.
You can view the list of valid compute hosts by using the nova hypervisor-list command, for example:
See the Manage Images Section of the OpenStack User Guide for information on how to create images from running instances.
Replicating images across multiple data centers The image service comes with a tool called glance-replicator that can be used to populate a new glance server using the images stored in an existing glance server.
The images in the replicated glance server preserve the uuids, metadata, and image data from the original.
Running the tool will output a set of commands that it supports:
If you use this option the same token is used for both the master and the slave.
Take a copy of the fromserver, and dump it onto the toserver.
Only images visible to the user running the replicator will be copied if glance is configured to use the Identity service (keystone) for authentication.
The copy is done "on-the-wire" so there are no large temporary files on the machine running the replicator to clean up.
Do the same thing as livecopy, but dump the contents of the glance server to a directory on disk.
Depending on the size of the local glance repository, the resulting dump may consume a large amount of local storage.
Therefore, we recommend you use the size comamnd first to determine the size of the resulting dump.
The dump and load are useful when replicating across two glance servers where a direct connection across the two glance hosts is impossible or too slow.
The compare command will show you the differences between the two servers, which is effectively a dry run of the livecopy command.
The size command will tell you how much disk is going to be used by image data in either a dump or a livecopy.
Note that this will provide raw number of bytes that would be written to the destination, it has no information about the redundancy costs associated with glance-registry back-ends that use replication for redundancy, such as Swift or Ceph.
The following example assumes that you have a credentials file for your primary cloud called primary.openrc and one for your secondary cloud called secondary.openrc.
Instances are the running virtual machines within an OpenStack cloud.
The Images and Instances section of the Introduction to OpenStack Compute Chapter provides a high level overview of instances and their life cycle.
This chapter deals with the details of how to manage that life cycle.
Interfaces to managing instances OpenStack provides command line, web based, and API based instance management.
Additionally a number of third party management tools are available for use with OpenStack using either the native API or the provided EC2 compatibility API.
Nova CLI The nova command provided by the OpenStack python-novaclient package is the basic command line utility for users interacting with OpenStack.
This is available as a native package for most modern Linux distributions or the latest version can be installed directly using pip python package installer:
Full details for nova and other CLI tools are provided in the OpenStack CLI Guide.
What follows is the minimal introduction required to follow the CLI example in this chapter.
In the case of a conflict the OpenStack CLI Guide should be considered authoritative (and a bug filed against this section)
In order to function the nova CLI needs to know four things:
If you are using the Horizon web dashboard, users can easily download credential files like this with the correct values for your particular implementation.
Horizon web dashboard Horizon is the highly customizable and extensible OpenStack web dashboard.
The Horizon Project home page has detailed information on deploying horizon.
The OpenStack Compute API documentation refers to instances as "servers"
The nova cli can be made to show the API calls it is making by passing it the --debug flag.
This allows legacy workflows built for EC2 to work with OpenStack.
Configuring the EC2 API lists configuration options for customizing this compatibility API on your OpenStack cloud.
Third Party Tools There are numerous third party tools and language specific SDKs for interacting with OpenStack clouds both through native and compatibility APIs.
These are not OpenStack projects so we can only provide links to some of the more popular projects and a brief description.
For detailed installation and usage info please see the individual project pages.
It can be used to access OpenStack through the EC2 compatibility API.
Unlike other projects mentioned in this section heat requires changes to your OpenStack deployment and is working toward official inclusion as an OpenStack project.
At this point heat is a development project not a production resource, but it does show what the not too distant future of instance management may be like.
Instance building blocks There are two fundamental requirements for a computing system, software and hardware.
Virtualization and cloud frameworks tend to blur these lines and some of your "hardware" may actually be "software" but conceptually you still need an operating system and something to run it on.
Images In OpenStack the base operating system is usually copied from an "image" stored in the Glance image service.
This is the most common case and results in an ephemeral instance.
It is also possible in special cases to put an operating system on a persistent "volume" in the Nova-Volume or Cinder volume system.
This gives a more traditional persistent system that accumulates state which is preserved across restarts.
To get a list of available images on your system run:
Name: a free form human readable name given to the image.
Status: shows the status of the image ACTIVE images are available for use.
Server: for images that are created as snapshots of running instance this is the UUID of the instance the snapshot derives from, for uploaded images it is blank.
To get a list of available flavors on your system run:
The nova flavor-create command allows authorized users to create new flavors.
Additional flavor manipulation commands can be shown with the command nova help |grep flavor.
This is an ephemeral disk the base image is copied into.
When booting from a persistent volume it is not used.
The "0" size is a special case which uses the native base image size as the size of the ephemeral root volume.
Ephemeral: specifies the size of a secondary ephemeral data disk.
This is an empty, unformatted disk and exists only for the life of the instance.
RXTX_Factor: optional property allows created servers to have a different bandwidth cap than that defined in the network they are attached to.
This factor is multiplied by the rxtx_base property of the network.
Default value is 1.0 (that is, the same as attached network)
Is_Public: Boolean value, whether flavor is available to all users or private to the tenant it was created in.
This is implemented as key/value pairs that must match against the corresponding key/value pairs on compute nodes.
Can be used to implement things like special resources (e.g., flavors that can only run on compute nodes with GPU hardware)
In the command, specify the server name, flavor ID, and image ID:
The status field indicates whether the server is being built or is active.
A status of BUILD indicates that your server is being built.
Copy the server ID value from the id field in the output.
You use this ID to get details for your server to determine if it built successfully.
Launch from a Volume The Compute service has support for booting an instance from a volume.
To manually create a bootable volume, mount the volume to an existing instance, and then build a volume-backed image.
This example assumes that you have a running instance with a 1GB volume mounted at /dev/vdc.
These commands will make the mounted volume bootable using a CirrOS image.
Cinder has the ability to create a bootlable volume from an image stored in Glance.
As of Grizzly, the following block storage drivers are compatible: iSCSI-based, LVM, and Ceph.
Make sure you configure Cinder with the relevant Glance options:
The output for nova help boot shows the following documentation about this flag:
In the example above, the volume was not created from a snapshot, so we will leave this field blank in our example below.
It is safe to leave this blank and have the Compute service infer the size.
Because of bug #1163566, you must specify an image when booting from a volume in Horizon, even though this image will not be used.
The following example will attempt boot from volume on the command line with ID=13, it will not delete on terminate.
Controlling where instances run The scheduler filters section provides detailed information on controlling where your instances run, including ensuring a set of instances run on different compute nodes for service resiliency or on the same node for high performance inter-instance communications.
Instance specific data For each instance, you can specify certain data including authorized_keys key injection, user-data, metadata service, and file injection.
Save the file mykey.pem to a secure location as it will allow root access to instances the mykeykey is associated with.
You will need to have the matching private key to access instances associated with this key.
To associate a key with an instance on boot add --key_name mykey to your command line for example:
Insert metadata during launch When booting a server, you can also add metadata, so that you can more easily identify it amongst your ever-growing elastic cloud.
Use the --meta option with a key=value pair, where you can make up the string for both the key and the value.
For example, you could add a description and also the creator of the server.
When viewing the server information, you can see the metadata included on the metadata line:
Providing User Data to Instances User Data is a special key in the metadata service which holds a file that cloud aware applications within the guest instance can access.
For example the cloudinit system is an open source package from Ubuntu that handles early initialization of a cloud instance that makes use of this user data.
This user-data can be put in a file on your local system and then passed in at instance creation with the flag --user-data <user-data-file> for example:
Users often want to do some configuration to their instances after booting.
For example, you may want to install some packages, start services, or manage the instance using a.
When launching instances in an OpenStack cloud, there are two technologies that work together to support automated configuration of instances at boot time: user data and cloud-init.
User data User data is the mechanism by which a user can pass information contained in a local file to an instance at launch time.
The typical use case is to pass something like a shell script or a configuration file as user data.
User data is sent using the --user-data /path/to/filename option when calling nova boot.
The following example creates a text file and then send the contents of that file as user data to the instance.
The instance can retrieve user data by querying the metadata service at using either the OpenStack metadata API or the EC2 compatibility API:
Note that the Compute service treats user data as a blob.
While the example above used a text file, user data can be in any format.
Cloud-init To do something useful with the user data, the virtual machine image must be configured to run a service on boot that retrieves the user data from the metadata service and take some action based on the contents of the data.
In particular, cloud-init is compatible with the Compute metadata service as well as the Compute config drive.
Rather, it is a package that is designed to support multiple cloud providers, so that the same virtual machine image can be used in different clouds without modification.
Cloud-init is an open source project, and the source code is available on Launchpad.
It is maintained by Canonical, the company which runs the Ubuntu project.
However, cloud-init is not designed to be Ubuntu-specific, and has been successfully ported to Fedora.
We recommend installing cloud-init on images that you create to simplify the task of configuring your instances on boot.
Even if you do not wish to use user data to configure instance behavior at boot time, cloud-init provides useful functionality such as copying the public key to an account (the ubuntu account by default on Ubuntu instances, the ec2user by default in Fedora instances)
If you do not have cloud-init installed, you will need to manually configure your image to retrieve the public key from the metadata service on boot and copy it to the appropriate account.
Cloud-init supported formats and documentation We recommend taking a look at the cloud-init doc/userdata.txt file the examples directory as well as the Ubuntu community documentation for details on how to use cloud-init.
Running a shell script on boot Assuming you have cloud-init installed, the simplest way to configure an instance on boot is to pass a shell script as user data.
The shell file must begin with #! in order for cloud-init to recognize it as a shell script.
Here's an example of a script that creates an account called clouduser.
Sending a shell script as user data has a similar effect to writing an /etc/rc.local script: it will be executed very late in the boot sequence as root.
Cloud-config format Cloud-init supports a YAML-based config format that allows the user to configure a large number of options on a system.
User data that begins with #cloud-config will be interpreted by cloud-init as cloud-config format.
Example: Setting hostname This cloud-init user data example sets the hostname and the FQDN, as well as updating / etc/hosts on the instance:
OpenStack can be configured to write metadata to a special configuration drive that will be attached to the instance when it boots.
The instance can retrieve any information that would normally be available through the metadata service by mounting this disk and reading files from it.
One use case for the config drive is to pass networking configuration (e.g., IP address, netmask, gateway) when DHCP is not being used to assign IP addresses to instances.
The instance's IP configuration can be transmitted using the config drive, which can be mounted and accessed before the instance's network settings have been configured.
The config drive can be used by any guest operating system that is capable of mounting an ISO9660 or VFAT file system.
This functionality should be available on all modern operating systems.
In addition, an image that has been built with a recent version of the cloud-init package will be able to automatically access metadata passed via config drive.
The current version of cloud-init as of this writing (0.7.1) has been confirmed to work with Ubuntu, as well as Fedora-based images such as RHEL.
If an image does not have the cloud-init package installed, the image must be customized to run a script that mounts the config drive on boot, reads the data from the drive, and takes appropriate action such as adding the public key to an account.
See below for details on how data is organized on the config drive.
To use config drive, the genisoimage program must be installed on each compute host.
In the Ubuntu packages, it is not installed by default (see bug #1165174)
Make sure you install this program on each compute host before attempting to use config drive, or an instance will not boot properly.
Here is a complex example that enables the config drive as well as passing user data, two files, and two key/value metadata pairs, all of which will be accessible from the config drive as described below.
Accessing the config drive from inside an instance The config drive will have a volume label of config-2
The cirros 0.3.0 test image does not have support for the config drive.
If your guest operating system does not use udev, then the /dev/disk/by-label directory will not be present.
The blkid command can be used to identify the block device that corresponds to the config drive.
For example, when booting the cirros image with the m1.tiny flavor, the device will be /dev/vdb:
Contents of the config drive The files that will be present in the config drive will vary depending on the arguments that were passed to nova boot.
Based on the example above, the contents of the config drive would be:
When creating images that access config drive data, if there are multiple directories under the openstack directory, always select the highest API version by date that your consumer supports.
Format of the config drive The default format of the config drive as an ISO 9660 filesystem.
It is unlikely that you would require VFAT format, since ISO 9660 is widely supported across operating systems.
If VFAT is chosen, the config drive will be 64MB in size.
Configuration Reference The following table contains all options for config drive.
A floating IP address is an IP address (typically public) that can be dynamically assigned to an instance.
Pools of floating IP addresses are created outside of python-novaclient with.
You can reserve floating IP addresses with the nova floating-ip-create command.
This command reserves the addresses for the tenant, but does not immediately associate that address with an instance.
The floating IP address has been reserved, and can now be associated with an instance with the nova add-floating-ip command.
For this example, we'll associate this IP address with an image called smallimage.
After the command is complete, you can confirm that the IP address has been associated with the nova floating-ip-list and nova-list commands.
The first table shows that the 50.56.12.232 is now associated with the smallimage instance ID, and the second table shows the IP address included under smallimage's public IP addresses.
To remove a floating IP address from an instance, use the nova remove-floating-ip command.
After the command is complete, you can confirm that the IP address has been associated with the nova floating-ip-list and nova-list commands.
You can now de-allocate the floating IP address, returning it to the pool so that it can be used by another tenant.
In this example, 50.56.12.232 was the only IP address allocated to this tenant.
Running nova floating-ip-list after the de-allocation is complete will return no results.
Manage Security Groups A security group is a named collection of network access rules that can be used to limit the types of traffic that have access to instances.
When you spawn an instance, you can assign it to one or more groups.
Any incoming traffic which is not matched by a rule is denied by default.
At any time, it is possible to add or remove rules within a security group.
Rules are automatically enforced as soon as they are created.
Before you begin, use nova secgroup-list to view the available security groups (specify -all-tenants if you are a cloud administrator wanting to view all tenants' groups)
Add or delete a security group Security groups can be added with nova secgroup-create.
The following example shows the creation of the security group secure1
After the group is created, it can be viewed in the security group list.
All the traffic originated by the instances (outbound traffic) is allowed.
All the traffic destined to instances (inbound traffic) is denied.
All the instances inside the group are allowed to talk to each other.
You can add extra rules into the default security group for handling the egress traffic.
When you view the security group list, it no longer appears.
The security group rules control the incoming traffic that is allowed to the instances in the group, while all outbound traffic is automatically allowed.
It is not possible to change the default outbound behaviour.
Every security group rule is a policy which allows you to specify inbound connections that are allowed to access the instance, by source address, destination port and IP protocol, (TCP, UDP or ICMP)
Currently, ipv6 and other protocols cannot be managed with the security rules, making them permitted by default.
To manage such, you can deploy a firewall in front of your OpenStack cloud to control other types of traffic.
The command requires the following arguments for both TCP and UDP rules :
For ICMP rules, instead of specifying a begin and end port, you specify the allowed ICMP code and ICMP type:
Entering "-1" for both code and type indicates that all ICMP codes and types should be allowed.
That notation allows you to specify a base IP address and a suffix that designates the number of significant bits in the IP address used to identify the network.
For example, in order to allow any IP address to access to a web server running on one of your instance inside the default security group:
In order to delete a rule, you need to specify the exact same arguments you used to create it:
Manage Volumes Depending on the setup of your cloud provider, they may give you an endpoint to use to manage volumes, or there may be an extension under the covers.
In either case, you can use the nova CLI to manage volumes.
Accessing running instances The most common access method for running instances is probably ssh, but this requires you have setup your instance with ssh keys and you have arranged for it to be running ssh with a public ip and opened the ssh port in your security group configuration.
If you haven't done this or you are trying to debug a problem image OpenStack can be configured to provide a VNC console, be aware that VNC is an unencrypted protocol so you should be cautious what you type across that link.
See the Getting Started With VNC Proxy section for details on how to configure and connect to this service.
Stop and Start an Instance There are two methods for stopping and starting an instance:
Pause and Unpause nova pause stores the state of the VM in RAM.
A paused instance continues to run, albeit in a "frozen" state.
Suspend and Resume nova suspend initiates a hypervisor-level suspend operation.
Suspending an instance stores the state of the VM on disk; all memory is written to disk and the virtual machine is stopped.
Suspending an instance is thus similar to placing a device in hibernation, and makes memory and vCPUs available.
Administrators may want to suspend an instance for system maintenance, or if the instance is not frequently used.
Change Server Configuration After you have created a server, you may need to increase its size, change the image used to build it, or perform other configuration changes.
Increase or Decrease Server Size Server size is changed by applying a different flavor to the server.
Before you begin, use nova flavor-list to review the flavors available to you.
While the server is rebuilding, its status will be displayed as RESIZING.
When the resize operation is completed, the status displayed is VERIFY_RESIZE.
This prompts the user to verify that the operation has been successful; to confirm:
However, if the operation has not worked as expected, you can revert it by doing:
In both cases, the server status should go back to ACTIVE.
Instance evacuation As cloud administrator, while you are managing your cloud, you may get to the point where one of the cloud compute nodes fails.
At that point you may use server evacuation in order to make managed instances available again.
Before Evacuation With the information about instance configuration, like if it is running on shared storage, you can choose the required evacuation parameters for your case.
Use the nova hostlist command to list the hosts and find new host for the evacuated instance.
In order to preserve user data on server disk, target host has to have preconfigured shared storage with down host.
As well, you have to validate that the current vm host is down.
To evacuate your server without shared storage: nova evacuate performs an instance evacuation from down host to specified host.
The instance will be booted from a new disk, but will preserve the configuration, e.g.
New instance password can be passed to the command using the -password <pwd> option.
If not given it will be generated and printed after the command finishes successfully.
Evacuate server to specified host and preserve user data In order to preserve the user disk data on the evacuated server the OpenStack Compute should be deployed with shared filesystem.
Refer to the shared storage section in the Configure migrations guide in order to configure your system.
Terminate an Instance When you no longer need an instance, use the nova delete command to terminate it.
You can use the instance name or the ID string.
You will not receive a notification indicating that the instance has been deleted, but if you run the nova list command, the instance will no longer appear in the list.
In this example, we will delete the instance tinyimage, which is experiencing an error condition.
This section assumes you have a working installation of OpenStack Compute and want to select a particular hypervisor or run with multiple hypervisors.
Before you try to get a VM running within OpenStack Compute, be sure you have installed a hypervisor and used the hypervisor's documentation to run a test VM and get it working.
Selecting a Hypervisor OpenStack Compute supports many hypervisors, an array of which must provide a bit of difficulty in selecting a hypervisor unless you are already familiar with one.
The following links provide additional information for choosing a hypervisor.
Refer to http://wiki.openstack.org/HypervisorSupportMatrix for a detailed list of features and support across the hypervisors.
Here is a list of the supported hypervisors with links to a relevant web site for configuration and use:
The virtual disk formats that it supports it inherits from QEMU since it uses a modified QEMU program to launch the virtual machine.
The supported formats include raw images, the qcow2, and VMware formats.
You must install the nova-compute service in a para-virtualized VM.
Hypervisor Configuration Basics The node where the nova-compute service is installed and running is the machine that runs all the virtual machines, referred to as the compute node in this guide.
To change to another hypervisor, change the libvirt_type option in nova.conf and restart the nova-compute service.
Here are the nova.conf options that are used to configure the compute node.
If default is specified, then use_cow_images flag is used instead of this one.
We fall back to hard reboot if instance does not shutdown within this window.
There are several sections about hypervisor selection in this document.
If you are reading this document linearly, you do not want to load the KVM module prior to installing nova-compute.
To enable KVM explicitly, add the following configuration options /etc/nova/ nova.conf:
The KVM hypervisor supports the following virtual machine image formats:
The rest of this section describes how to enable KVM on your system.
Fedora: Getting started with virtualization from the Fedora project wiki.
Checking for hardware virtualization support The processors of your compute host need to support virtualization technology (VT) (mainly Intel VT -x or AMD AMD-v technologies) to use KVM.
In order to check if your processor has VT support (which has to be enabled in the BIOS), issue as root:
If KVM is supported, the output should look something like:
If KVM is not supported, the output should look something like:
If KVM is not supported, you should get no output.
Some systems require that you enable VT support in the system BIOS.
If you believe your processor supports hardware acceleration but the above command produced no output, you may need to reboot your machine, enter the system BIOS, and enable the VT option.
In the case that KVM acceleration is not supported, Compute should be configured to use a different hypervisor, such as QEMU or Xen.
Enabling KVM KVM requires the kvm and either kvm-intel or kvm-amd modules to be loaded.
This may have been configured automatically on your distribution when KVM is installed.
You can check that they have been loaded using lsmod, as follows, with expected output for Intel-based processors:
The following sections describe how to load the kernel modules for Intel-based and AMD-based processors if they were not loaded automatically by your distribution's KVM installation process.
If your compute host is Intel-based, run the following as root to load the kernel modules:
Add the following lines to /etc/modules so that these modules will load on reboot:
If your compute host is AMD-based, run the following as root to load the kernel modules:
Add the following lines to /etc/modules so that these modules will load on reboot:
Specifying the CPU model of KVM guests The Compute service allows you to control the guest CPU model that is exposed to KVM virtual machines.
To maximize performance of virtual machines by exposing new host CPU features to the guest.
To ensure a consistent default CPU across all machines, removing reliance of variable QEMU defaults.
In libvirt, the CPU is specified by providing a base CPU model name (which is a shorthand for a set of feature flags), a set of additional feature flags, and the topology (sockets/ cores/threads)
The libvirt KVM driver provides a number of standard CPU model names.
Check this file to determine which models are supported by your local installation.
The libvirt_cpu_mode option can take one of four values: none, hostpassthrough, host-model and custom.
The difference to host-model, instead of just matching feature flags, every last detail of the host CPU is matched.
This gives absolutely best performance, and can be important to some apps which check low level CPU details, but it comes at a cost with respect to migration: the guest can only be migrated to an exactly matching host CPU.
For example, to configure the KVM guests to expose Nehalem CPUs, your nova.conf should contain:
None (default for all libvirt-driven hypervisors other than KVM & QEMU)
It will leave it up to the hypervisor to choose the default model.
This setting is equivalent to the Compute service behavior prior to the Folsom release.
This is a symptom that the KVM kernel modules have not been loaded.
If you cannot start VMs after installation without rebooting, it's possible the permissions are not correct.
To check the permissions, run ls -l /dev/kvm to see whether the group is set to kvm.
Both are controlled through libvirt, both support the same feature set, and all virtual machine images that are compatible with KVM are also compatible with QEMU.
The main difference is that QEMU does not support native virtualization.
Consequently, QEMU has worse performance than KVM and is a poor choice for a production deployment.
Running the Compute service inside of a virtual machine for development or testing purposes, where the hypervisor does not support native virtualization for guests.
If hardware support is not available (e.g., if you are running Compute inside of a VM and the hypervisor does not expose the required hardware support), you can use QEMU instead.
For some operations you may also have to install the guestmount utility:
The QEMU hypervisor supports the following virtual machine image formats:
Tips and fixes for QEMU on RHEL If you are testing OpenStack in a virtual machine, you need to configure nova to use qemu without KVM and hardware virtualization.
Note nested virtualization will be the much slower TCG variety, and you should provide lots of memory.
The above connection details are used by the OpenStack Compute service to contact your hypervisor and are the same details you use to connect XenCenter, the XenServer management console, to your XenServer or XCP box.
OpenStack with XenAPI supports the following virtual machine image formats:
This would be necessary for any Xen-based system that isn't using the XCP toolstack, such as SUSE Linux or Oracle Linux.
The rest of this section describes Xen, XCP, and XenServer, the differences between them, and how to use them with OpenStack.
Xen's architecture is different from KVM's in important ways, and we discuss those differences and when each might make sense in your OpenStack cloud.
Xen is open source (GPLv2) and is managed by Xen.org, an cross-industry organization.
Xen is a component of many different products and projects.
The hypervisor itself is very similar across all these projects, but the way that it is managed can be different, which can cause confusion if you're not clear which tool stack you are using.
Make sure you know what tool stack you want before you get started.
Xen Cloud Platform (XCP) is an open source (GPLv2) tool stack for Xen.
It is designed specifically as platform for enterprise and cloud computing, and is well integrated with OpenStack.
The current versions of XCP available in Linux distributions do not yet include all the features available in the binary distribution of XCP.
It is based on XCP, and exposes the same tool stack and management API.
As an analogy, think of XenServer being based on XCP in the way that Red Hat Enterprise Linux is based on Fedora.
XenServer has a free version (which is very similar to XCP) and paid-for versions with additional features enabled.
Citrix provides support for XenServer, but as of July 2012, they do not provide any support for XCP.
For a comparison between these products see the  XCP Feature Matrix.
Both XenServer and XCP include Xen, Linux, and the primary control daemon known as xapi.
OpenStack usually refers to XenAPI, to indicate that the integration works equally well on XCP and XenServer.
Sometimes, a careless person will refer to XenServer specifically, but you can be reasonably confident that anything that works on XenServer will also work on the latest version of XCP.
A Xen host will run a number of virtual machines, VMs, or domains (the terms are synonymous on Xen)
It is the first domain to boot after Xen, and owns the storage and networking hardware, the device drivers, and the primary control software.
Any other VM is unprivileged, and are known as a "domU" or "guest"
All customer VMs are unprivileged of course, but you should note that on Xen the OpenStack control software (nova-compute) also runs in a domU.
This gives a level of security isolation between the privileged system software and the OpenStack software (much of which is customerfacing)
There is an ongoing project to split domain 0 into multiple privileged domains known as driver domains and stub domains.
This technology is what powers Citrix XenClient RT, and is likely to be added into XCP in the next few years.
However, the current architecture just has three levels of separation: dom0, the OpenStack domU, and the completely unprivileged customer VMs.
This refers to the interaction between Xen, domain 0, and the guest VM's kernel.
However, the OpenStack domU (that's the one running nova-compute) must be running in PV mode.
XenAPI deployment architecture When you deploy OpenStack on XCP or XenServer you will get something similar to this:
Domain 0: runs xapi and some small pieces from OpenStack (some xapi plugins and network isolation rules)
The majority of this is provided by XenServer or XCP (or yourself using Kronos)
OpenStack VM: The nova-compute code runs in a paravirtualized virtual machine, running on the host under management.
It will often also be running nova-network (depending on your network mode)
In this case, nova-network is managing the addresses given to the tenant VMs through DHCP.
Nova uses the XenAPI Python library to talk to xapi, and it uses the Management Network to reach from the domU to dom0 without leaving the host.
Please note, that the VM images are downloaded by the xenapi plugins, so please make sure, that the images could be downloaded through the management network.
It usually means, binding those services to the management interface.
The parameters of this network depends on the networking model selected (Flat, Flat DHCP, VLAN)
The networks shown here need to be connected to the corresponding physical networks within the datacenter.
In the simplest case, three individual physical network cards could be used.
It is also possible to use VLANs to separate these networks.
Please note, that the selected configuration must be in line with the networking model selected for the cloud.
XenAPI pools The host-aggregates feature allows you to create pools of XenServer hosts (configuring shared storage is still an out of band activity), to enable live migration when using shared storage.
Installing XenServer and XCP When you want to run OpenStack with XCP or XenServer, you first need to install the software on  an appropriate server.
This means when your server starts the first software that runs is Xen.
This means the software you install on your compute host is XenServer or XCP, not the operating system you wish to run the OpenStack code on.
The OpenStack services will run in a VM you install on top of XenServer.
Before you can install your system you must decide if you want to install Citrix XenServer (either the free edition, or one of the paid editions) or Xen Cloud Platform from Xen.org.
When installing many servers, you may find it easier to perform  PXE boot installations of XenServer or XCP.
You can also package up any post install changes you wish to make to your XenServer by  creating your own XenServer supplemental pack.
It is also possible to get XCP by installing the xcp-xenapi package on Debian based distributions.
However, this is not as mature or feature complete as above distributions.
This will modify your boot loader to first boot Xen, then boot your existing OS on top of Xen as Dom0
It is in Dom0 that the xapi daemon will run.
You can find more details on the Xen.org wiki:  http://wiki.xen.org/wiki/Project_Kronos.
Ensure you are using the EXT type of storage repository (SR)
Features that require access to VHD files (such as copy on write, snapshot and migration) do not work when using the LVM SR.
Storage repository (SR) is a XenAPI specific term relating to the physical storage on which virtual disks are stored.
On the XenServer/XCP installation screen, this is selected by choosing "XenDesktop Optimized" option.
In case you are using an answer file, make sure you use srtype="ext" within the  installation tag of the answer file.
You are now ready to install OpenStack onto your XenServer system.
For resize and migrate functionality, please perform the changes described in the Configuring Resize section of the OpenStack Compute Administration Manual.
Install the VIF isolation rules to help prevent mac and ip address spoofing.
In order to support AMI type images, you need to set up /boot/guest symlink/ directory in Dom0
To support resize/migration, set up an ssh trust relation between your XenServer hosts, and ensure /images is properly set up.
Create a Paravirtualised virtual machine that can run the OpenStack compute code.
Install and configure the nova-compute in the above virtual machine.
For further information on these steps look at how DevStack performs the last three steps when doing developer deployments.
For more information on DevStack, take a look at the DevStack and XenServer Readme.
More information on the first step can be found in the XenServer mutli-tenancy protection doc.
More information on how to install the XenAPI plugins can be found in the  XenAPI plugins Readme.
When using Xen as the hypervisor for OpenStack Compute, you can install a Python script (usually, but it can be any executable) on the host side, and then call that through the XenAPI.
These plugins have to be copied to the hypervisor's Dom0, to the appropriate directory, where xapi can find them.
The example assumes the master branch is used, please amend the URL to match the version being used:
Follow these steps to produce a supplemental pack from the nova sources, and package it as a XenServer Supplemental Pack.
Given you have the nova sources (use one of the methods mentioned at Manual Installation):
Pack the RPM packages to a Supplemental Pack, using the XenServer DDK (the following command should be issued on the XenServer DDK virtual appliance, after the produced rpm file has been copied over):
The above command should produce an .iso file in the output directory specified.
In order to support AMI type images within your OpenStack installation, a directory / boot/guest needs to be created inside Dom0
The OpenStack VM will put the kernel and ramdisk extracted from the AKI and ARI images to this location.
This directory's content will be maintained by OpenStack, and its size should not increase during normal operation, however in case of power failures or accidental shutdowns, some files might be left over.
In order to prevent these files to fill up Dom0's disk, it is recommended to set up this directory as a symlink pointing to a subdirectory of the local SR.
Execute the following commands in Dom0 to achieve the above mentioned setup:
To get resize to work with XenServer (and XCP) you need to:
Establish a root trust between all hypervisor nodes of your deployment:
The least you can do is to symlink /images to your local storage SR.
Xen Boot from ISO XenServer, through the XenAPI integration with OpenStack provides a feature to boot instances from an ISO file.
In order to activate the "Boot From ISO" feature, the SR elements on XenServer host must be configured that way.
First, create an ISO-typed SR, such as an NFS ISO library, for instance.
You need to export an NFS volume from a remote NFS server.
Second, on the compute host, find the uuid of this ISO SR and write it down.
Now, make sure the host-uuid from "xe pbd-list" equals the uuid of the host you found earlier.
You should now be able to add images via the OpenStack Image Registry, with diskformat=iso, and boot them in OpenStack Compute.
Further reading Here are some of the resources available to learn more about Xen:
Xen Configuration Reference The following table provides a complete reference of all configuration options available for configuring Xen with OpenStack.
The default value is the Local Storage in default XenServer/XCP installations.
On the other hand, to fall back on the Default SR, as displayed by XenCenter, set this flag to: default-sr:true.
If the agent is present, network configuration is not injected into the image.
Reduces the amount of time it takes nova to detect that a VM has started, when that VM does not have the agent installed.
This speeds up resizes down considerably since large runs of zeros won't have to be rsynced.
This is different from hardware virtualization, the approach used by other hypervisors such as KVM, Xen, and VMWare.
Additional containment technologies, such as AppArmor, may be used to provide better isolation between containers, although this is not the case by default.
For all these reasons, the choice of this virtualization technology is not recommended in production.
If your compute hosts do not have hardware support for virtualization, LXC will likely provide better performance than QEMU.
In addition, if your guests need to access to specialized hardware (e.g., GPUs), this may be easier to achieve with LXC than other hypervisors.
Some OpenStack Compute features may be missing when running with LXC as the hypervisor.
On Ubuntu 12.04, enable LXC support in OpenStack by installing the nova-compute-lxc package.
This section describes the additional configuration required to launch VMWare-based virtual machine images.
There are two OpenStack Compute drivers that can be used with vSphere:
With this driver and access to shared storage, advanced vSphere features like vMotion, High Availability, and Dynamic Resource Scheduling (DRS) are availabile.
With this driver, one nova-compute service is run per vCenter cluster.
With this driver, one nova-compute service is run per ESX host.
Prerequisites You will need to install the following software installed on each nova-compute node:
If not installed, the "nova-compute" service shuts down with the message: "Unable to import suds"
Tomcat server: This is required to serve up a local version of the vSphere WSDL file (see below)
Below we will point nova.conf to fetch this WSDL file from Tomcat using a URL pointing to localhost.
Using the VMwareVCDriver This section covers details of using the VMwareVCDriver.
VMWareVCDriver configuration options When using the VMwareVCDriver (i.e., vCenter) with OpenStack Compute, nova.conf must include the following VMWare-specific config options:
Remember that you will have only one nova-compute service per cluster.
It is recommended that this host run as a VM with high-availability enabled as part of that same cluster.
Also note that many of the nova.conf options mentioned elsewhere in this document that are relevant to libvirt do not apply to using this driver.
Requirements + Limitations The VMwareVCDriver is new in Grizzly, and as a result there are some important deployment requirements and limitations to be aware of.
In many cases, these items will be addressed in future releases.
Each cluster can only be configured with a single Datastore.
If multiple Datastores are configured, the first one returned via the vSphere API will be used.
Because a single nova-compute is used per cluster, the nova-scheduler views this as a single host with resources amounting to the aggregate resources of all ESX hosts managed by the cluster.
This may result in unexpected behavior depending on your choice of scheduler.
Security Groups are only supported if the VMware driver is used in conjunction with the the OpenStack Networking Service running the Nicira NVP plugin.
Using the VMwareESXDriver This section covers details of using the VMwareESXDriver.
When using the VMwareESXDriver (i.e., no vCenter) with OpenStack Compute, configure nova.conf with the following VMWare-specific config options:
Remember that you will have one nova-compute service per ESXi host.
It is recommended that this host run as a VM on the same ESXi host it is managing.
Also note that many of the nova.conf options mentioned elsewhere in this document that are relevant to libvirt do not apply to using this driver.
The ESXDriver is unable to take advantage of many of the advanced capabilities associated with the vSphere platform, namely vMotion, High Availability, and Dynamic Resource Scheduler (DRS)
Images with VMware vSphere When using either VMware driver, images should be uploaded to the OpenStack Image Service in the VMDK format.
All VM NICs will be attached to this port group.
All VM NICs will be attached to this port group for management by the OpenStack Networking Plugin.
Volumes with VMware vSphere The VMware driver has limited support for attaching Volumes from the OpenStack Block Storage service, supporting attachments only if the volume driver type is 'iscsi'
There is no support for volumes based on vCenter Datastores in this release.
Due to a bug in vSphere ESX 4.1 default wsdl.
PowerVM compute driver connects to an Integrated Virtualization Manager (IVM) to perform PowerVM Logical Partition (LPAR) deployment and management.
Configuration To enable the PowerVM compute driver, add the following configuration options /etc/ nova/nova.conf:
Make sure this path can fit your biggest image in glance.
Make sure this path can fit your biggest image in glance.
The necessary Python components as well as the nova-compute service are installed directly onto the Windows platform.
Windows Clustering Services are not needed for functionality within the OpenStack infrastructure.
The use of the Windows Server 2012 platform is recommend for the best experience and is the platform for active development.
The following Windows platforms have been tested as compute nodes:
Both Server and Server Core with the Hyper-V role enabled (Shared Nothing Live migration is not supported using 2008r2)
Hyper-V Configuration The following sections discuss how to prepare the Windows Hyper-V node for operation as an OpenStack Compute node.
The Hyper-V compute node needs to have ample storage for storing the virtual machine images running on the compute nodes.
You may use a single volume for all, or partition it into an OS volume and VM volume.
Configure NTP Network time services must be configured to ensure proper operation of the HyperV compute node.
To set network time on your Hyper-V host you will need to run the following commands.
To quickly enable an interface to be used as a Virtual Interface the following PowerShell may be used:
Enable iSCSI Initiator Service To prepare the Hyper-V node to be able to attach to volumes provided by cinder you must first make sure the Windows iSCSI initiator service is running and started automatically.
The following outlines the steps of shared nothing live migration.
The target hosts ensures that live migration is enabled and properly configured in HyperV.
The target hosts checks if the image to be migrated requires a base VHD and pulls it from Glance if not already available on the target host.
The source hosts ensures that live migration is enabled and properly configured in HyperV.
The source hosts communicates to the manager the outcome of the operation.
The following two configuration options/flags are needed in order to support Hyper-V live migration and must be added to your nova.conf on the Hyper-V compute node:
This flag is needed to support live migration to hosts with different CPU features.
This flag is checked during instance creation in order to limit the CPU features used by the VM.
A Windows domain controller with the Hyper-V compute nodes as domain members.
The instances_path command line option/flag needs to be the same on all hosts.
The openstack-compute service deployed with the setup must run with domain credentials.
To enable shared nothing live migration run the 3 PowerShell instructions below on each Hyper-V host:
Please replace the IP_ADDRESS with the address of the interface which will provide the virtual switching for nova-network.
Here's an article that clarifies the various live migration options in Hyper-V:
Python 2.7.3 must be installed prior to installing the OpenStack Compute Driver on the Hyper-V server.
You will require pip to install the necessary python module dependencies.
Setuptools for Python 2.7 for Windows can be download from here:
The following packages need to be downloaded and manually installed onto the Compute Node.
The following python packages need to be installed via easy_install or pip.
The installer to run Git on Windows can be downloaded here:
Once the download is complete double click the installer and follow the prompts in the installation wizard.
The default should be acceptable for the needs of the document.
Once installed you may run the following to clone the Nova code.
Configuring Nova.conf The nova.conf file must be placed in C:\etc\nova for running OpenStack on Hyper-V.
The following table contains a reference of all optionsfor hyper-v.
If left blank, an administrative share will be used, looking for the same "instances_path" used locally.
Preparing Images for use with Hyper-V Hyper-V currently supports only the VHD file format for virtual machine instances.
Detailed instructions for installing virtual machines on Hyper-V can be found here:
Once you have successfully created a virtual machine, you can then upload the image to glance using the native glance-client:
Running Compute with Hyper-V To start the nova-compute service, run this command from a console in the Windows server:
I ran the nova-manage service list command from my controller; however, I'm.
Verify that you are synchronized with a network time source.
Instructions for configuring NTP on your Hyper-V compute node are located here.
The baremetal driver is a hypervisor driver for Openstack Nova Compute.
Within the Openstack framework, it has the same role as the drivers for other hypervisors (libvirt, xen, etc), and yet it is presently unique in that the hardware is not virtualized - there is no hypervisor between the tenants and the physical hardware.
It exposes hardware via Openstack's API, using pluggable sub-drivers to deliver machine imaging (PXE) and power control (IPMI)
With this, provisioning and management of physical hardware is accomplished using common cloud APIs and tools, such as Heat or salt-cloud.
However, due to this unique situation, using the baremetal driver requires some additional preparation of its environment, the details of which are beyond the scope of this guide.
Some OpenStack Compute features are not implemented by the baremetal hypervisor driver.
There are many configuration options specific to the Baremetal driver.
Also, some additional steps will be required, such as building the baremetal deploy ramdisk.
See the main wiki page for details and implementation suggestions.
If a node does not have a fixed PXE IP address, volumes are exported with globally opened ACL.
Fibre Channel support in OpenStack Compute is remote block storage attached to Compute nodes for VMs.
Fibre Channel supports the KVM hypervisor in only the grizzly release.
Requirements for KVM Hosts The KVM host must have the following system packages installed:
Installing the Required Packages Use the following commands to install the system packages.
By understanding the available networking configuration options you can design the best configuration for your OpenStack Compute instances.
Networking Options This section offers a brief overview of each concept in networking for Compute.
With the Grizzly release, you can chose either to install and configure nova-network for networking between VMs or use the Networking service (quantum) for networking.
For each VM instance, Compute assigns to it a private IP address.
Currently, Compute with nova-network only supports Linux bridge networking that allows the virtual interfaces to connect to the outside network through the physical interface.
The network controller with nova-network provides virtual networks to enable compute servers to interact with each other and with the public network.
Currently, Compute with nova-network supports three kinds of networks, implemented in three “Network Manager” types:
The three kinds of networks can co-exist in a cloud system.
However, since you can't yet select the type of network for a given project, you cannot configure more than one type of network in a given Compute installation.
All of the networking options require network connectivity to be already set up between OpenStack physical nodes.
OpenStack will automatically create all network bridges (i.e., br100) and VM virtual interfaces.
The internal network interface is used for communication with VMs, it shouldn't have an IP address attached to it before OpenStack installation (it serves merely as a fabric where the actual endpoints are VMs and dnsmasq)
Also, the internal network interface must be put in promiscuous mode, because it will have to receive packets whose target MAC address is of the guest VM, not of the host.
All the network managers configure the network using network drivers, e.g.
The driver isn't tied to any particular network manager; all network managers use the same driver.
All network managers operate in either single-host or multi-host mode.
In single-host mode, there is just 1 instance of novanetwork which is used as a default gateway for VMs and hosts a single DHCP server (dnsmasq), whereas in multi-host mode every compute node has its own nova-network.
In any case, all traffic between VMs and the outer world flows through nova-network.
There are pros and cons to both modes, read more in Existing High Availability Options.
Compute makes a distinction between fixed IPs and floating IPs for VM instances.
Fixed IPs are IP addresses that are assigned to an instance on creation and stay the same until the instance is explicitly terminated.
By contrast, floating IPs are addresses that can be dynamically associated with an instance.
A floating IP address can be disassociated and associated with another instance at any time.
A user can reserve a floating IP for their project.
The IP addresses for VM instances are grabbed from the subnet, and then injected into the image on launch.
Each instance receives a fixed IP address from the pool of available addresses.
A system administrator may create the Linux networking bridge (typically named br100, although this configurable) on the systems running the nova-network service.
All instances of the system are attached to the same bridge, configured manually by the network administrator.
In Flat DHCP Mode, OpenStack starts a DHCP server (dnsmasq) to pass out IP addresses to VM instances from the specified subnet in addition to manually configuring the networking bridge.
Like Flat Mode, all instances are attached to a single bridge on the compute node.
In addition a DHCP server is running to configure instances (depending on single-/multihost mode, alongside each nova-network)
It will also run and configure dnsmasq as a DHCP server listening on this bridge, usually on IP address 10.0.0.1 (see DHCP server: dnsmasq)
For every instance, nova will allocate a fixed IP address and configure dnsmasq with the MAC/IP pair for the VM, i.e.
These IPs are not assigned to any of the host's network interfaces, only to the VM's guest-side interface.
In any setup with flat networking, the host(-s) with nova-network on it is (are) responsible for forwarding traffic from the private network.
Compute can determine the NAT entries for each network when you have fixed_range='' in your nova.conf.
Sometimes NAT is not used, such as when fixed_range is configured with all public IPs and a hardware router is used (one of the HA options)
Such host(-s) needs to have br100 configured and physically connected to any other nodes that are hosting VMs.
Compute nodes have iptables/ebtables entries created per project and instance to protect against IP/MAC address spoofing and ARP poisoning.
For backwards compatibility, Grizzly supports the fixed_range option and if set will perform the default logic from Folsom and earlier releases.
In single-host Flat DHCP mode you will be able to ping VMs via their fixed IP from the nova-network node, but you will not be able to ping them from the compute nodes.
In this mode, Compute creates a VLAN and bridge for each project.
For multiple machine installation, the VLAN Network Mode requires a switch that supports VLAN tagging (IEEE 802.1Q)
The project gets a range of private IPs that are only accessible from inside the VLAN.
In order for a user to access the instances in their project, a special VPN instance (code named cloudpipe) needs to be created.
Compute generates a certificate and key for the user to access the VPN and starts the VPN automatically.
It provides a private network segment for each project's instances that can be accessed via a dedicated VPN connection from the Internet.
In this mode, each project gets its own VLAN, Linux networking bridge, and subnet.
The subnets are specified by the network administrator, and are assigned dynamically to a project when required.
A DHCP Server is started for each VLAN to pass out IP addresses to VM instances from the subnet assigned to the project.
All instances belonging to one project are bridged into the same VLAN for that project.
OpenStack Compute creates the Linux networking bridges and VLANs when required.
With the default Compute settings, once a virtual machine instance is destroyed, it can take some time for the IP address associated with the destroyed instance to become available for assignment to a new instance.
The result is that the IP address assigned to the instance is immediately released.
This configuration option applies to both Flat DHCP mode and VLAN Manager mode.
Verify that this program is installed on all hosts running the nova-compute service before enabling this option.
This can be checked with the which command, and will return the complete path if the program is installed.
The nova-network service is responsible for starting up dnsmasq processes.
The behavior of dnsmasq can be customized by creating a dnsmasq configuration file.
See the high availability section for an example of how to change the behavior of dnsmasq using a dnsmasq configuration file.
The dnsmasq documentation has a more comprehensive dnsmasq configuration file example.
Dnsmasq also acts as a caching DNS server for instances.
The following example would configure dnsmasq to use Google's public DNS server:
Dnsmasq logging output goes to the syslog (typically /var/log/syslog or /var/ log/messages, depending on Linux distribution)
The dnsmasq logging output can be useful for troubleshooting if VM instances boot successfully but are not reachable over the network.
This reservation only affects which IP address the VMs start at, not the fixed IP addresses that the nova-network service places on the bridges.
The Compute service uses a special metadata service to enable virtual machine instances to retrieve instance-specific data.
The metadata service supports two sets of APIs: an OpenStack metadata API and an EC2-compatible API.
To retrieve a list of supported versions for the OpenStack metadata API, make a GET request to.
To retrieve a list of supported versions for the EC2-compatible metadata API, make a GET request to.
If you write a consumer for one of these APIs, always attempt to access the most recent API version supported by your consumer first, then fall back to an earlier version if the most recent one is not available.
Here is the same content after having run through a JSON pretty-printer:
A listing of these elements can be retrieved by making a GET query to:
Instances can retrieve the public SSH key (identified by keypair name when a user requests a new instance) by making a GET request to:
Instances can retrieve user data by making a GET request to:
Running the metadata service The metadata service is implemented by either the nova-api service or the nova-apimetadata service.
If you are running the nova-api service, you must have metadata as one of the elements of the list of the enabled_apis configuration option in /etc/nova/ nova.conf.
The default enabled_apis configuration setting includes the metadata service, so you should not need to modify it.
The metadata_host configuration option must be an IP address, not a hostname.
The default Compute service settings assume that the nova-network service and the nova-api service are running on the same host.
Set the metadata_host configuration option to the IP address of the host where the nova-api service is running.
Configuring Networking on the Compute Node To configure the Compute node's networking for the VM images, the overall steps are:
You choose the networking mode for your virtual instances in the nova.conf file.
This is the Default if no network manager is defined in nova.conf.
Use the following command to create a subnet (named private in this example) that your VMs will run on :
When using the XenAPI compute driver, the OpenStack services run in a virtual machine.
This means networking is significantly different when compared to the networking with the libvirt compute driver.
Before reading how to configure networking using the XenAPI compute driver, you may find it useful to read the Citrix article on  Understanding XenServer Networking and the section of this document that describes XenAPI and OpenStack.
Configuring Flat Networking FlatNetworking uses ethernet adapters configured as bridges to allow network traffic to transit between all the various nodes.
This setup can be done with a single adapter on the physical host, or multiple.
This option does not require a switch that does VLAN tagging as VLAN networking does, and is a common development installation or proof of concept setup.
When you choose Flat networking, Nova does not manage networking at all.
Instead, IP addresses are injected into the instance via the file system (or passed in via a guest agent)
Metadata forwarding must be configured manually on the gateway if it is required within your network.
To configure flat networking, ensure that your nova.conf file contains the following line:
When configuring Flat Networking, failing to enable flat_injected can prevent guest VMs from receiving their IP information at boot time.
Compute defaults to a bridge device named ‘br100’ which is stored in the Nova database, so you can change the name of the bridge device by modifying the entry in the database.
In any set up with FlatNetworking (either Flat or FlatDHCP), the host with nova-network on it is responsible for forwarding traffic from the private network.
Compute determines the "fixed range" for IPs configured dynamically by pulling from the configured networks when you set fixed_range='' in nova.conf.
This dynamic range configuration allows for non-contiguous subnets to be configured in the fixed_ip space and only configures the NAT rules as they are needed.
This also restricts the NAT range to the smallest range required preventing the NAT from impacting subnets that might exist on the external network.
This host needs to have br100 configured and talking to any other nodes that are hosting VMs.
With either of the Flat Networking options, the default gateway for the virtual machines is set to the host which is running nova-network.
Set the compute node's external IP address to be on the bridge and add eth0 to that bridge.
To do this, edit your network interfaces configuration to look like the following example:
For an all-in-one development setup, this diagram represents the network setup.
For multiple compute nodes with a single network adapter, which you can use for smoke testing or a proof of concept, this diagram represents the network setup.
For multiple compute nodes with multiple network adapters, this diagram represents the network setup.
You may want to use this setup for separate admin and data traffic.
Otherwise, flat networking works in a very similar way with both the libvirt driver and the XenAPI driver.
Configuring Flat DHCP Networking With Flat DHCP, the host(-s) running nova-network act as the gateway to the virtual nodes.
If you're using single-host networking, you can optionally set network_host on the nova.conf stored on the nova-compute node to tell it which host the nova-network is running on so it can more efficiently communicate with nova-network.
Lastly, it sets up iptables rules to allow the VMs to communicate with the outside world and contact a special metadata server to retrieve information from the cloud.
Compute hosts in the FlatDHCP model are responsible for bringing up a matching bridge and bridging the VM tap devices into the same ethernet device that the network host is on.
The compute hosts should not have an IP address on the VM network, because the bridging puts the VMs and the network host on the same logical network.
When a VM boots, the VM sends out DHCP packets, and the DHCP server on the network host responds with their assigned IP address (remember, the address is actually assigned by nova and put into DHCP server's configuration file, the DHCP server merely tells the VM what it is)
You can read a detailed walk-through of what exactly happens in single-host Flat DHCP mode in this blogpost, parts of which are also relevant in other networking modes.
This bridge works just fine on a single host, but when there are multiple hosts, traffic needs a way to get out of the bridge onto a physical interface.
When using the libvirt driver, the setup will look like the figure below:
If you specify an interface that already has an IP it will break and if this is the interface you are connecting through with SSH, you cannot fix it unless you have ipmi/console access.
In FlatDHCP mode, the setting for -network_size should be number of IPs in the entire fixed range.
That said, it will take a very long time for you to create your initial network, as an entry for each IP will be created in the database.
The network host will automatically add the gateway ip to this bridge.
If this is the case for you, edit your nova.conf file to contain the following lines:
XenAPI Flat DHCP Networking The following figure shows a setup with Flat DHCP networking, network HA, and using multiple interfaces.
Here is an extract from a nova.conf file in a system running the above setup:
You can either specify the bridge name, such an xenbr2, or the name label, such as vmbr.
Specifying the name-label is very useful in cases where your networks are not uniform across your XenServer hosts.
When you have a limited number of network cards on your server, it is possible to use networks isolated using VLANs for the public and network traffic.
When using XenServer, it is best to use the firewall driver written specifically for XenServer.
This pushes the firewall rules down to the hypervisor, rather than running them in the VM that is running nova-network.
Outbound Traffic Flow with Any Flat Networking In any set up with FlatNetworking, the host with nova-network on it is responsible for forwarding traffic from the private network dynamically determined by Compute with the fixed_range='' directive in nova.conf.
This host needs to have a bridge interface (e.g., br100) configured and talking to any other nodes that are hosting VMs.
With either of the Flat Networking options, the default gateway for the virtual machines is set to the host which is running nova-network.
When a virtual machine sends traffic out to the public networks, it sends it first to its default gateway, which is where nova-network is configured.
Next, the host on which nova-network is configured acts as a router and forwards the traffic out to the Internet.
If you're using a single interface, then that interface (often eth0) needs to be set into promiscuous mode for the forwarding to happen correctly.
This does not appear to be needed if you're running with physical hosts that have and use two interfaces.
Configuring VLAN Networking Compute can be configured so that the virtual machine instances of different projects (tenants) are in different subnets, with each subnet having a different VLAN tag.
This can be useful in networking environments where you have a large IP space which is cut up into smaller subnets.
The purpose of this is generally to control the size of broadcast domains.
It can also be useful to provide an additional layer of isolation in a multi-tenant environment.
The terms network and subnet are often used interchangeably in discussions of VLAN mode.
Running in VLAN mode is more complex than the other network modes.
The hosts running nova-network and nova-compute must have the 8021q kernel module loaded.
Your networking switches must be configured to enable the specific VLAN tags you specify in your Compute setup.
You will need information about your networking setup from your network administrator to configure Compute properly (e.g., netmask, broadcast, gateway, ethernet device, VLAN IDs)
The network_size option refers to the default number of IP addresses in each network, although this can be overridden at network creation time.
Here is an example of how to create a network consistent with the above example configuration options, as root:
This will create a bridge interface device called br169 on the host running the nova-network service.
This device will appear in the output of an ifconfig command.
As in the example above, you may (optionally) specify this association at network creation time by using the --project_id flag which corresponds to the tenant ID.
Use the keystone tenant-list command to list the tenants and corresponding IDs that you have already created.
The nova network-create command supports many configuration options, which are displayed when called with the nova help network-create:
In particular, flags to the nova network-create command can be used to override settings from nova.conf:
To view a list of the networks that have been created, as root:
The nova command-line tool does not yet support network modifications.
To modify an existing network, you must use the nova-manage command.
The nova command-line tool does not yet support network deletions..
To delete an existing network, you must use the nova-manage command.
To delete a network, use novamanage network delete, as root:
Creating a network will automatically cause the Compute database to populate with a list of available fixed IP addresses.
You can view the list of fixed IP addresses and their associations with active virtual machines by doing, as root:
If users need to access the instances in their project across a VPN, a special VPN instance (code named cloudpipe) needs to be created as described in the section titled Cloudpipe Per Project VPNs.
To configure your nodes to support VLAN tagging, install the vlan package and load the 8021q kernel module, as root:
To have this kernel module loaded on boot, add the following line to /etc/modules:
In certain cases, the network manager may not properly tear down bridges and VLANs when it is stopped.
If you attempt to restart the network manager and it does not start, check the logs for errors indicating that a bridge device already exists.
If this is the case, you will likely need to tear down the bridge and VLAN devices manually.
These commands would stop the service, manually tear down the bridge and VLAN from the previous example, kill any remaining dnsmasq processes, and start the service up again, as root:
Here is an extract from a nova.conf file in a system running the above setup:
You should notice that vlan_interface refers to the network interface on the Hypervisor and the network interface on the VM running the OpenStack services.
As with before public_interface refers to the network interfce on the VM running the OpenStack services.
With VLAN networking and the XenAPI driver, the following things happen when you start a VM:
First the XenServer network is attached to the appropriate physical interface (PIF) and VLAN unless the network already exsists.
When the VM is created, its VIF is attached to the above network.
The DomU does this for multiple VLAN networks, so it has to be attached on a VLAN trunk.
For this reason it must have an interface on the parent bridge of the VLAN bridge where VM instances are plugged.
To help understand VLAN networking with the XenAPI further, here are some important things to note:
You won't see the bridge until a VIF is plugged in it.
For this reason it must have an interface on the parent bridge of the VLAN bridge where VM instances are plugged.
Within the Openstack domU, 'ip link' is then used to configure VLAN interfaces on the 'trunk' port.
Each of this vlan interfaces is associated with a dnsmasq instance, which will distribute IP addresses to instances.
The lease file for dnsmasq is constantly updated by nova-network, thus ensuring VMs get the IP address specified by the layer3 network driver (nova IPAM or Melange)
With this configuration, VM instances should be able to get the IP address assigned to them from the appropriate dnsmasq instance, and should be able to communicate without any problem with other VMs on the same network and with the their gateway.
With Open vSwitch, we don't really have distinct bridges for different VLANs; even if they appear as distinct bridges to linux and XenServer, they are actually the same OVS instance, which runs a distinct 'fake-bridge' for each VLAN.
The 'real' bridge is the 'parent' of the fake one.
You can easily navigate fake and real bridges with ovs-vsctl.
As you can see I am referring to Openvswitch only.
This is for a specific reason: the fakeparent mechanism automatically imply that ports which are not on a fake bridge are trunk ports.
A packet forwarded on a VLAN interfaces does not get back in the xenbrX bridge for ethX.
On XenServer 6.0 and later, Open vSwitch is the default network stack.
When using VLAN networking with XenAPI and linux bridge, the default networking stack on XenServer prior to version 6.0, you must run the network node on a VM on a XenServer that does not host any nova-compute controlled instances.
Text in this section was adapted from an email from Vish Ishaya on the OpenStack mailing list.
There is an issue with the way Compute uses dnsmasq in VLAN mode.
Compute starts up a single copy of dnsmasq for each VLAN on the network host (or on every host in multi_host mode)
The problem is in the way that dnsmasq binds to an IP address and port.
Both copies can respond to broadcast packets, but unicast packets can only be answered by one of the copies.
As a consequence, guests from only one project will get responses to their unicast DHCP renew requests.
What happens next is different depending on the guest OS.
Linux generally will send a broadcast packet out after the unicast fails, and so the only effect is a small (tens of ms) hiccup while the interface is reconfigured.
There have been observed cases where Windows just gives up and ends up with a non-configured interface.
This bug was first noticed by some users of OpenStack who rolled their own fix.
In short, on Linux, if you set the SO_BINDTODEVICE socket option, it will allow different daemons to share the port and respond to unicast packets, as long as they listen on different interfaces.
Simon Kelley, the maintainer of dnsmasq, has integrated a fix for the issue in dnsmaq version 2.61
If upgrading dnsmasq is out of the question, a possible workaround is to minimize lease renewals with something like the following combination of config options.
Cloudpipe — Per Project Vpns Cloudpipe is a method for connecting end users to their project instances in VLAN networking mode.
The support code for cloudpipe implements admin commands (via an extension) to automatically create a VM for a project that allows users to VPN into the private network of their project.
Access to this VPN is provided through a public port on the network host for the project.
This allows users to have free access to the virtual machines in their project without exposing those machines to the public internet.
The cloudpipe image is basically just a Linux instance with openvpn installed.
It needs a simple script to grab user data from the metadata server, b64 decode it into a zip file, and run the autorun.sh script from inside the zip.
The autorun script will configure and run openvpn to run using the data from nova.
It is also useful to have a cron script that will periodically redownload the metadata and copy the new Certificate Revocation List (CRL)
This list is contained within the payload file and will keeps revoked users from connecting and will disconnect any users that are connected with revoked certificates when their connection is renegotiated (every hour)
In this how-to, we are going to create our cloud-pipe image from a running Ubuntu instance which will serve as a template.
When all the components will be installed and configured, we will create an image from that instance that will be uploaded to the Glance repositories.
We start by installing the required packages on our instance :
The next step is to create both scripts that will be used when the network components will start up and shut down.
Make these two scripts executables by running the following command :
This file describes the network interfaces available on your system.
We will ask our image to retrive the payload, decrypt it, and use both key and CRL for our Openvpn service : / etc/rc.local.
In order to enable or disable this script just change the execution # bits.
The called script (autorun.sh) is a script which mainly parses the network settings of the running instances in order to set up the initial routes.
Your instance is now ready to be used as a cloudpipe image.
In the next step, we will update that instance to Glance.
Upload your instance to Glance We will make use of the nova snapshot feature in order to create an image from our running instance.
We create an image with, using the instance ID :
Make sure the instance has been upload to the Glance repository :
Make that image public (snapshot-based images are private by default):
Nova will create the necessary rules for our cloudpipe instance (icmp and OpenVPN port) :
This gives a consistent IP to the instance so that nova-network can create forwarding rules for access from the outside world.
The network for each project is given a specific high-numbered port on the public IP of the network host.
This port is automatically forwarded to 1194 on the VPN instance.
Certificates and Revocation For certificate management, it is also useful to have a cron script that will periodically download the metadata and copy the new Certificate Revocation List (CRL)
This will keep revoked users from connecting and disconnects any users that are connected with revoked certificates when their connection is re-negotiated (every hour)
You set the use_project_ca option in nova.conf for cloudpipes to work securely so that each project has its own Certificate Authority (CA)
If the use_project_ca config option is set (required to for cloudpipes to work securely), then each project has its own CA.
This CA is used to sign the certificate for the vpn, and is also passed to the user for bundling images.
When a certificate is revoked using nova-manage, a new Certificate Revocation List (crl) is generated.
As long as cloudpipe has an updated crl, it will block revoked users from connecting to the vpn.
The userdata for cloudpipe isn't currently updated when certs are revoked, so it is necessary to restart the cloudpipe instance if a user's credentials are revoked.
Restarting and Logging into the Cloudpipe VPN You can reboot a cloudpipe vpn through the api if something goes wrong (using nova reboot for example), but if you generate a new crl, you will have to terminate it and start it again using the cloudpipe extension.
The cloudpipe instance always gets the first ip in the subnet and if force_dhcp_release is not set it takes some time for the ip to be recovered.
If you try to start the new vpn instance too soon, the instance will fail to start because of a "NoMoreAddresses" error.
The keypair that was used to launch the cloudpipe instance should be in the keys/<project_id> folder.
You can use this key to log into the cloudpipe instance for debugging purposes.
If you are running multiple copies of nova-api this key will be on whichever server used the original request.
To make debugging easier, you may want to put a common administrative key into the cloudpipe image that you create.
Remote access to your cloudpipe instance from an OpenVPN client Now your cloudpipe instance is running, you can use your favorite OpenVPN client in order to access your instances within their private network cloudpipe is connected to.
Start by generating a private key and a certificate for your project:
Downgrade privileges after initialization (non-Windows only) user nobody group nogroup comp-lzo.
In order to get the public IP and port of your cloudpipe instance, you can run the following command :
Depending on the client you are using, make sure to save the configuration file under the directory it should be, so the certificate file and the private key.
In order to connect to the project's network, you will need an OpenVPN client for your computer.
In this example we will use Viscosity, but the same settings apply to any client.
Start by filling the public ip and the public port of the cloudpipe instance.
You can now save the configuration and establish the connection!
A periodic task disassociates the fixed ip address for the cloudpipe instance.
In order to resolve that issue, log into the mysql server and update the ip address status :
You can automate the image creation by download that script and running it from inside the instance : Get the script from Github.
Enabling Ping and SSH on VMs Be sure you enable access to your VMs by using the euca-authorize or nova secgroup-addrule command.
Below, you will find the commands to allow ping and ssh to your VMs:
These commands need to be run as root only if the credentials used to interact with nova-api have been put under /root/.bashrc.
If the EC2 credentials have been put into another user's .bashrc file, then, it is necessary to run these commands as the user.
If you still cannot ping or SSH your instances after issuing the nova secgroup-add-rule commands, look at the number of dnsmasq processes that are running.
If you have a running instance, check to see that TWO dnsmasq processes are running.
Every virtual instance is automatically assigned a private IP address.
OpenStack uses the term "floating IP" to refer to an IP address (typically public) that can be dynamically added to a running virtual instance.
If you plan to use this feature, you must add the following to your nova.conf file to specify which interface the nova-network service will bind public IP addresses to:
Restart the nova-network service if you change nova.conf while the service is running.
Note that due to the way floating IPs are implemented using a source NAT (SNAT rule in iptables), inconsistent behaviour of security groups can be seen if VMs use their floating IP to communicate with other virtual machines - particularly on the same physical host.
Traffic from VM to VM across the fixed network does not have this issue, and this is the recommended path.
This configuration is also necessary to make source_groups work if the vms in the source group have floating ips.
Enabling IP forwarding By default, the IP forwarding is disabled on most of Linux distributions.
The "floating IP" feature requires the IP forwarding enabled in order to work.
The IP forwarding only needs to be enabled on the nodes running the service nova-network.
If the multi_host mode is used, make sure to enable it on all the compute node, otherwise, enable it on the node running the nova-network service.
You can enable it on the fly by running the following command:
In order to make the changes permanent, edit the /etc/sysctl.conf and update the IP forwarding setting :
Save the file and run the following command in order to apply the changes :
It is also possible to update the setting by restarting the network service.
Creating a List of Available Floating IP Addresses Nova maintains a list of floating IP addresses that are available for assigning to instances.
Use the nova-manage floating create command to add entries to this list, as root.
Automatically adding floating IPs The nova-network service can be configured to automatically allocate and assign a floating IP address to virtual instances when they are launched.
Add the following line to nova.conf and restart the nova-network service.
Note that if this option is enabled and all of the floating IP addresses have already been allocated, the nova boot command will fail with an error.
Removing a Network from a Project You will find that you cannot remove a network that has already been associated to a project by simply deleting it.
To determine the project ID you must have admin rights.
You can disassociate the project from the network with a scrub command and the project ID as the final parameter:
The multi-nic feature allows you to plug more than one interface to your instances, making it possible to make several use cases available :
Each VIF is representative of a separate network with its own IP block.
Every network mode introduces it's own set of changes regarding the mulitnic usage :
Using the multinic feature In order to use the multinic feature, first create two networks, and attach them to your project :
Now every time you spawn a new instance, it gets two IP addresses from the respective DHCP servers :
Make sure to power up the second interface on the instance, otherwise that last won't be reacheable via its second IP.
If the Virtual Network Service Quantum is installed, it is possible to specify the networks to attach to the respective interfaces by using the --nic flag when invoking the nova command :
As illustrated in the Flat DHCP diagram in Section Configuring Flat DHCP Networking titled Flat DHCP network, multiple interfaces, multiple servers, traffic from the VM to the public internet has to go through the host running nova network.
The compute hosts can optionally have their own public IPs, or they can use the network host as their gateway.
This mode is pretty simple and it works in the majority of situations, but it has one major drawback: the network host is a single point of failure! If the network host goes down for any reason, it is impossible to communicate with the VMs.
Here are some options for avoiding the single point of failure.
Each compute host does NAT, DHCP, and acts as a gateway for all of its own VMs.
While there is still a single point of failure in this scenario, it is the same point of failure that applies to all virtualized systems.
This setup requires adding an IP on the VM network to each host in the system, and it implies a little more overhead on the compute hosts.
It is also possible to combine this with option 4 (HW Gateway) to remove the need for your compute hosts to gateway.
In that hybrid version they would no longer gateway for the VMs and their responsibilities would only be DHCP and NAT.
The resulting layout for the new HA networking option looks the following diagram:
In contrast with the earlier diagram, all the hosts in the system are running the novacompute, nova-network and nova-api services.
Each host does DHCP and does NAT for public traffic for the VMs running on that particular host.
In this model every compute host requires a connection to the public internet and each host is also assigned an address from the VM network where it listens for DHCP traffic.
The nova-api service is needed so that it can act as a metadata server for the instances.
To run in HA mode, each compute host must run the following services:
If the compute host is not an API endpoint, use the nova-api-metadata service.
The send_arp_for_ha option facilitates sending of gratuitous arp messages to ensure the arp caches on compute hosts are up to date.
If a compute host is also an API endpoint, use the nova-api service.
Your enabled_apis option will need to contain metadata, as well as additional options depending on the API services.
For example, if it supports compute requests, volume requests, and EC2 compatibility, the nova.conf file should contain:
The multi_host option must be in place when you create the network and nova-network must be run on every compute host.
These created multi hosts networks will send all network related commands to the host that the specific VM is on.
You need to edit the configuration option enabled_apis such that it includes metadata in the list of enabled APIs.
Other options become available when you configure multi_host nova networking please refer to  Configuration: nova.conf.
You must specify the multi_host option on the command line when creating fixed networks.
This solution is definitely an option, although it requires a second host that essentially does nothing unless there is a failure.
Also four seconds can be too long for some real-time applications.
To enable this HA option, your nova.conf file must contain the following option:
This allows us to bridge a given VM into multiple networks.
It is possible to set up two networks on separate vlans (or even separate ethernet devices on the host) and give the VMs a NIC and an IP on each network.
Each of these networks could have its own network host acting as the gateway.
In this case, the VM has two possible routes out.
If one of them fails, it has the option of using the other one.
The disadvantage of this approach is it offloads management of failure scenarios to the guest.
The guest needs to be aware of multiple networks and have a strategy for switching between them.
One would have to set up a floating IP associated with each of the IPs on private the private networks to achieve some type of redundancy.
This offloads HA to standard switching hardware and it has some strong benefits.
Unfortunately, the nova-network service is still responsible for floating IP natting and DHCP, so some failover strategy needs to be employed for those options.
If running in VLAN mode, a separate router must be specified for each network.
The networks are identified by the first argument when calling nova networkcreate to create the networks as documented in the Configuring VLAN Networking subsection.
Configure the hardware gateway to forward metadata requests to a host that's running the nova-api service with the metadata API enabled.
Make sure that the list in the enabled_apis configuration option /etc/nova/ nova.conf contains metadata in addition to the other APIs.
An example that contains the EC2 API, the OpenStack compute API, the OpenStack volume API, and the metadata service would look like:
Ensure you have set up routes properly so that the subnet that you use for virtual machines is routable.
If you aren't able to reach your instances via the floating IP address, make sure the default security group allows ICMP (ping) and SSH (port 22), so that you can reach the instances:
Ensure the NAT rules have been added to iptables on the node that nova-network is running on, as root:
Note that you cannot SSH to an instance with a public IP from within the same server as the routing configuration won't allow it.
You can use tcpdump to identify if packets are being routed to the inbound interface on the compute host.
If the packets are reaching the compute hosts but the connection is failing, the issue may be that the packet is being dropped by reverse path filtering.
For example, if the inbound interface is eth2, as root:
If this solves your issue, add the following line to /etc/sysctl.conf so that the reverse path filter will be disabled the next time the compute host reboots:
We strongly recommend you remove the above line to re-enable the firewall once your networking issues have been resolved.
If you can SSH to your instances but you find that the network interactions to your instance is slow, or if you find that running certain operations are slower than they should be (e.g., sudo), then there may be packet loss occurring on the connection to the instance.
Packet loss can be caused by Linux networking configuration settings related to bridges.
One way to check if this is the issue in your setup is to open up three terminals and run the following commands:
In the first terminal, on the host running nova-network, use tcpdump to monitor DNSrelated traffic (UDP, port 53) on the VLAN interface.
In the second terminal, also on the host running nova-network, use tcpdump to monitor DNS-related traffic on the bridge interface.
In the third terminal, SSH inside of the instance and generate DNS requests by using the nslookup command:
The symptoms may be intermittent, so try running nslookup multiple times.
If the network configuration is correct, the command should return immediately each time.
If it is not functioning properly, the command will hang for several seconds.
If the nslookup command sometimes hangs, and there are packets that appear in the first terminal but not the second, then the problem may be due to filtering done on the bridges.
If this solves your issue, add the following line to /etc/sysctl.conf so that these changes will take effect the next time the host reboots:
KVM: Network connectivity works initially, then fails Some administrators have observed an issue with the KVM hypervisor where instances running Ubuntu 12.04 will sometimes lose network connectivity after functioning properly for a period of time.
This kernel module may also improve network performance on KVM.
Note that loading the module will have no effect on instances that are already running.
The OpenStack Block Storage service provides persistent block storage resources that OpenStack Compute instances can consume.
Refer to the OpenStack Block Storage Admin Manual for information about configuring volume drivers and creating and attaching volumes to server instances.
Compute uses the nova-scheduler service to determine how to dispatch compute and volume requests.
For example, the nova-scheduler service determines which host a VM should launch on.
The term "host" in the context of filters means a physical node that has a nova-compute service running on it.
Configured to use the multi-scheduler, which allows the admin to specify different scheduling behavior for compute requests versus volume requests.
Configured as a chance scheduler, which picks a host at random that has the cinder-volume service running.
Configured as a filter scheduler, described in detail in the next section.
It supports filtering and weighting to make informed decisions on where a new instance should be created.
This Scheduler can only be used for scheduling compute requests, not volume requests, i.e.
Filters When the Filter Scheduler receives a request for a resource, it first applies filters to determine which hosts are eligible for consideration when dispatching a resource.
Filters are binary: either a host is accepted by the filter, or it is rejected.
Hosts that are accepted by the filter are then processed by a different algorithm to decide which hosts to use for that request, described in the Weights section.
The default setting specifies all of the filter that are included with the Compute service:
See the host aggregates section for documentation on how to use this filter.
If a host is in an aggregate that has the metadata key filter_tenant_id it will only create instances from that tenant (or list of tenants)
If a host does not belong to an aggregate with the metadata key, it can create instances from all tenants.
AllHostsFilter This is a no-op filter, it does not eliminate any of the available hosts.
This filter must be enabled for the scheduler to respect availability zones in requests.
If an extra specs key contains a colon ":", anything before the colon is treated as a namespace, and anything after the colon is treated as the key to be matched.
If a namespace is present and is not 'capabilities', it is ignored by this filter.
ComputeFilter Filters hosts by flavor (also known as instance type) and image properties.
The scheduler will check to ensure that a compute host has sufficient capabilities to run a virtual machine instance that corresponds to the specified flavor.
If the image has properties specified, this filter will also check that the host can support them.
CoreFilter Only schedule instances on hosts if there are sufficient CPU cores available.
If this filter is not set, the scheduler may over provision a host based on cores (i.e., the virtual cores running on an instance may exceed the physical cores)
To take advantage of this filter, the requester must pass a scheduler hint, using different_host as the key and a list of instance uuids as the value.
DiskFilter Only schedule instances on hosts if there are sufficient Disk available for ephemeral storage.
Adjusting this value to be greater than 1.0 will allow scheduling instances while over committing disk resources on the node.
This may be desirable if you use an image format that is sparse or copy on write such that each virtual instance does not require a 1:1 allocation of virtual disk to physical storage.
To take advantage of this filter, the requester must pass a scheduler hint, using group as the key and a list of instance uuids as the value.
It passes hosts that can support the specified image properties contained in the instance.
Properties include the architecture, hypervisor type, and virtual machine mode.
E.g., an instance might require a host that runs an ARM-based processor and QEMU as the hypervisor.
JsonFilter The JsonFilter allows a user to construct a custom filter by passing a scheduler hint in JSON format.
RamFilter Only schedule instances on hosts if there is sufficient RAM available.
If this filter is not set, the scheduler may over provision a host based on RAM (i.e., the RAM allocated by virtual machine instances may exceed the physical RAM)
RetryFilter Filter out hosts that have already been attempted for scheduling purposes.
If the scheduler selects a host to respond to a service request, and the host fails to respond to the request, this filter will prevent the scheduler from retrying that host for the service request.
SameHostFilter Schedule the instance on the same host as another instance in a set of instances.
To take advantage of this filter, the requester must pass a scheduler hint, using same_host as the key and a list of instance uuids as the value.
To take advantage of this filter, the requester must specify a range of valid IP address in CIDR format, by passing two scheduler hints:
Hosts are then weighed and sorted with the largest weight winning.
The default behavior is to spread instances across all hosts evenly.
Other Schedulers While an administrator is likely to only need to work with the Filter Scheduler, Compute comes with other schedulers as well, described below.
It is the default top-level scheduler as specified by the scheduler_driver configuration option.
Each node can have multiple aggregates, each aggregate can have multiple key-value pairs, and the same key-value pair can be assigned to multiple aggregate.
This information can be used in the scheduler to enable advanced scheduling, to set up Xen hypervisor resources pools or to define logical groups for migration.
Command-line interface The nova command-line tool supports the following aggregate-related commands.
Add or update metadata (key-value pairs) associated with the aggregate with id <id>
If the username and tenant you are using to access the Compute service do not have the admin role, or have not been explicitly granted the appropriate privileges, you will see one of the following errors when trying to use these commands:
Configure scheduler to support host aggregates One common use case for host aggregates is when you want to support scheduling instances to a subset of compute hosts because they have a specific capability.
For example, you may want to allow users to request compute hosts that have SSD drives if they need access to faster disk I/O, or access to compute hosts that have GPU cards to take advantage of GPU-accelerated code.
Example: specify compute hosts with SSDs In this example, we configure the Compute service to allow users to request nodes that have solid-state drives (SSDs)
Once the flavor has been created, we specify one or more key-value pair that must match the key-value pairs on the host aggregates.
Once it is set, you should see the extra_specs property of the ssd.large flavor populated with a key of ssd and a corresponding value of true.
Now, when a user requests an instance with the ssd.large flavor, the scheduler will only consider hosts with the ssd=true key-value pair.
XenServer hypervisor pools to support live migration When using the XenAPI-based hypervisor, the Compute service uses host aggregates to manage XenServer Resource pools, which are used in supporting live migration.
See Configuring Migrations for details on how to create these kinds of host aggregates to support live migration.
Cells functionality allows you to scale an OpenStack Compute cloud in a more distributed fashion without having to use complicated technologies like database and message queue clustering.
When this functionality is enabled, the hosts in an OpenStack Compute cloud are partitioned into groups called cells.
The top-level cell should have a host that runs a nova-api service, but no nova-compute services.
Each child cell should run all of the typical nova-* services in a regular Compute cloud except for novaapi.
You can think of a cells as a normal Compute deployment in that each cell has its own database server and message queue broker.
The nova-cells service handles communication between cells and selecting a cell for new instances.
Communication between cells is pluggable, with the only option currently implemented being communication via RPC.
Once a cell has been selected and the new build request has reached its nova-cells service, it will be sent over to the host scheduler in that cell and the build proceeds as it does without cells.
All cell-related configuration options go under a [cells] section in nova.conf.
These are sent to parent cells, but aren't used in scheduling until later filter/weight support is added.
Configuring the API (top-level) cell The compute API class must be changed in the API cell so that requests can be proxied via nova-cells down to the correct cell properly.
Configuring the child cells Add the following to nova.conf in the child cells, replacing cell1 with the name of each cell:
Configuring the database in each cell Before bringing the services online, the database in each cell needs to be configured with information about related cells.
In particular, the API cell needs to know about its immediate children, and the child cells need to know about their immediate agents.
The information needed is the RabbitMQ server credentials for the particular cell.
Use the nova-manage cell create command to add this information to the database in each cell:
As an example, assume we have an API cell named api and a child cell named cell1
Within the api cell, we have the following RabbitMQ server info:
And in the child cell named cell1 we have the following RabbitMQ server info:
We would run this in the API cell, as root.
In the child cell, we would run the following, as root:
By understanding how the different installed nodes interact with each other you can administer the OpenStack Compute installation.
OpenStack Compute offers many ways to install using multiple servers but the general idea is that you can have multiple compute nodes that control the virtual servers and a cloud controller node that contains the remaining Nova services.
The OpenStack Compute cloud works via the interaction of a series of daemon processes named nova-* that reside persistently on the host machine or machines.
These binaries can all run on the same machine or be spread out on multiple boxes in a large deployment.
The responsibilities of Services, Managers, and Drivers, can be a bit confusing at first.
Here is an outline the division of responsibilities to make understanding the system a little bit easier.
Currently, Services are nova-api, nova-objectstore (which can be replaced with Glance, the OpenStack Image Service), nova-compute, and nova-network.
Managers are responsible for a certain aspect of the system.
It is a logical grouping of code relating to a portion of the system.
In general other components should be using the manager to make changes to the components that it is responsible for.
It is a wsgi app that routes and authenticate requests.
There is a nova-api.conf file created when you install Compute.
It can be replaced with OpenStack Image Service and a simple image manager or use OpenStack Object Storage as the virtual machine image storage facility.
It loads a Service object which exposes the public methods on ComputeManager via Remote Procedure Call (RPC)
It loads a Service object which exposes the public methods on one of the subclasses of NetworkManager.
Different networking strategies are available to the service by changing the network_manager configuration option to FlatManager, FlatDHCPManager, or VlanManager (default is VLAN if no other is specified)
Understanding the Compute Service Architecture These basic categories describe the service architecture and what's going on within the cloud controller.
This API Server makes command and control of the hypervisor, storage, and networking programmatically available to users in realization of the definition of cloud computing.
The API endpoints are basic http web services which handle authentication, authorization, and basic command and control functions using various API interfaces under the Amazon, Rackspace, and related models.
This enables API compatibility with multiple existing tool sets created for interaction with offerings from other vendors.
Message Queue A messaging queue brokers the interaction between compute nodes (processing), the networking controllers (software which controls network infrastructure), API endpoints, the scheduler (determines which physical hardware to allocate to a virtual resource), and similar components.
Communication to and from the cloud controller is by HTTP requests through multiple API endpoints.
A typical message passing event begins with the API server receiving a request from a user.
The API server authenticates the user and ensures that the user is permitted to issue the subject command.
Availability of objects implicated in the request is evaluated and, if available, the request is routed to the queuing engine for the relevant workers.
Workers continually listen to the queue based on their role, and occasionally their type hostname.
When such listening produces a work request, the worker takes assignment of the task and begins its execution.
Upon completion, a response is dispatched to the queue which is received by the API server and relayed to the originating user.
Database entries are queried, added, or removed as necessary throughout the process.
Compute Worker Compute workers manage computing instances on host machines.
Through the API, commands are dispatched to compute workers to:
The API server dispatches commands through the message queue, which are subsequently processed by Network Controllers.
The user’s access key needs to be included in the request, and the request must be signed with the secret key.
Upon receipt of API requests, Compute will verify the signature and execute commands on behalf of the user.
In order to begin using nova, you will need to create a user with the Identity Service.
Managing the Cloud There are three main tools that a system administrator will find useful to manage their cloud; the nova client, the nova-manage command, and the Euca2ools commands.
The nova-manage command may only be run by cloud administrators.
Both novaclient and euca2ools can be used by all users, though specific commands may be restricted by Role Based Access Control in the Identity Management service.
Using the nova command-line tool Installing the python-novaclient gives you a nova shell command that enables Compute API interactions from the command line.
You install the client, and then provide your username and password, set as environment variables for convenience, and then you can have the ability to send commands to your cloud on the command-line.
Now that you have installed the python-novaclient, confirm the installation by entering:
In return, you will get a listing of all the commands and parameters for the nova command line client.
By setting up the required parameters as environment variables, you can fly through these commands on the command line.
You can add --os-username on the nova command, or set them as environment variables:
Using the Identity Service, you are supplied with an authentication endpoint, which nova recognizes as the OS_AUTH_URL.
Using the nova-manage command The nova-manage command may be used to perform many essential functions for administration and ongoing maintenance of nova, such as network creation or user manipulation.
The man page for nova-manage has a good explanation for each of its functions, and is recommended reading for those starting out.
For administrators, the standard pattern for executing a nova-manage command is:
For example, to obtain a list of all projects: nova-manage project list.
Run without arguments to see a list of available command categories: nova-manage.
You can also run with a category argument such as user to see a list of all commands in that category: nova-manage service.
Usage statistics The nova command-line tool can provide some basic statistics on resource usage for hosts and instances.
For more sophisticated monitoring, see the Ceilometer project, which is currently under development.
You may also wish to consider installing tools such as Ganglia or Graphite if you require access to more detailed data.
Host usage statistics Use the nova host-list command to list the hosts and the nova-related services that are running on them:
Use the nova host-describe command to retrieve a summary of resource usage of all of the instances running on the host.
The "cpu" column is the sum of the virtual CPUs of all of the instances running on the host, the "memory_mb" column is the sum of the memory (in MB) allocated to the instances running on the hosts, and the "disk_gb" column is the sum of the root and ephemeral disk sizes (in GB) of the instances running on the hosts.
Note that these values are computed using only information about the flavors of the instances running on the hosts.
This command does not query the CPU usage, memory usage, or hard disk usage of the physical host.
Instance usage statistics Use the nova diagnostics command to retrieve CPU, memory, I/O and network statistics from an instance:
Use the nova usage-list command to get summary statistics for each tenant:
The file must contain a section called logger_nova, for example:
This example sets the debugging level to INFO (which less verbose than the default DEBUG setting)
See the Python documentation on logging configuration file format for more details on this file, including the meaning of the handlers and quaname variables.
Syslog OpenStack Compute services can be configured to send logging information to syslog.
This is particularly useful if you want to use rsyslog, which will forward the logs to a remote machine.
You need to separately configure the Compute service (nova), the Identity service (keystone), the Image service (glance), and, if you are using it, the Block Storage service (cinder) to send log messages to syslog.
In addition to enabling syslog, these settings also turn off more verbose output and debugging output from the log.
For example, you may want to capture logging info at different severity levels for different services.
Rsyslog Rsyslog is a useful tool for setting up a centralized log server across multiple machines.
We briefly describe the configuration to set up an rsyslog server; a full treatment of rsyslog is beyond the scope of this document.
We assume rsyslog has already been installed on your hosts, which is the default on most Linux distributions.
The example below use compute-01 as an example of a compute host name:
Once you have created this file, restart your rsyslog daemon.
Error-level log messages on the compute hosts should now be sent to your log server.
The goal of the root wrapper is to allow the nova unprivileged user to run a number of actions as the root user, in the safest manner possible.
Historically, Nova used a specific sudoers file listing every command that the nova user was allowed to run, and just used sudo to run that command as root.
However this was difficult to maintain (the sudoers file was in packaging), and did not allow for complex filtering of parameters (advanced filters)
A generic sudoers entry lets the nova user run nova-rootwrap as root.
The nova-rootwrap code looks for filter definition directories in its configuration file, and loads command filters from them.
Then it checks if the command requested by Compute matches one of those filters, in which case it executes the command (as root)
Security model The escalation path is fully controlled by the root user.
A sudoers entry (owned by root) allows nova to run (as root) a specific rootwrap executable, and only with a specific.
The configuration file (also root-owned) points to root-owned filter definition directories, which contain rootowned filters definition files.
This chain ensures that the nova user itself is not in control of the configuration or modules used by the nova-rootwrap executable.
Details of rootwrap.conf The rootwrap.conf file is used to influence how nova-rootwrap works.
Since it's in the trusted security path, it needs to be owned and writeable only by the root user.
Its location is specified both in the sudoers entry and in the nova.conf configuration file with the rootwrap_config= entry.
It uses an INI file format with the following sections and parameters:
Directories defined on this line should all exist, be owned and writeable only by the root user.
Details of .filters files Filters definition files contain lists of filters that nova-rootwrap will use to allow or deny a specific command.
Since they are in the trusted security path, they need to be owned and writeable only by the root user.
It uses an INI file format with a [Filters] section and several lines, each with a unique parameter name (different for each filter you define):
Migration provides a scheme to migrate running instances from one OpenStack Compute server to another OpenStack Compute server.
First, look at the running instances, to get the ID of the instance you wish to migrate.
Second, look at information associated with that instance - our example is vm1 from above.
In this example, HostC can be picked up because nova-compute is running on it.
Finally, use the nova live-migration command to migrate the instances.
If instances are still running on HostB, check logfiles (src/dest nova-compute and nova-scheduler) to determine why.
Note While the nova command is called live-migration, under the default Compute configuration options the instances are suspended before migration.
Recovering from a failed compute node If you have deployed OpenStack Compute with a shared filesystem, you can quickly recover from a failed compute node.
Manual recovery For KVM/libvirt compute node recovery refer to section above, while the guide below may be applicable for other hypervisors.
First, you can review the status of the host using the nova database, some of the important information is highlighted below.
This example converts an EC2 API instance ID into an openstack ID - if you used the nova commands, you can substitute the ID directly.
You can find the credentials for your database in /etc/nova.conf.
Armed with the information of VMs on the failed host, determine which compute host the affected VMs should be moved to.
In this case, the VM will move to np-rcc46, which is achieved using this database command:
The important changes to make are to change the DHCPSERVER value to the host ip address of the nova compute host that is the VMs new home, and update the VNC IP if it isn't already 0.0.0.0
In theory, the above database update and nova reboot command are all that is required to recover the VMs from a failed host.
Recovering from a UID/GID mismatch When running OpenStack compute, using a shared filesystem or an automated configuration tool, you could encounter a situation where some files on your compute node are using the wrong UID or GID.
This causes a raft of errors, such as being unable to live migrate, or start virtual machines.
The following is a basic procedure run on nova-compute hosts, based on the KVM hypervisor, that could help to restore the situation:
First,make sure you don't use numbers that are already used for some other user/group.
Change all the files owned by nova or group by nova, e.g.
Repeat the steps for the libvirt-qemu owned files if those were needed to change.
Following this, you can run the find command to verify that all files using the correct identifiers.
In this section, we will review managing your cloud after a disaster, and how to easily backup the persistent storage volumes, which is another approach when you face a disaster.
A- The disaster Recovery Process presentation A disaster could happen to several components of your architecture : a disk crash, a network loss, a power cut, etc.
The example disaster will be the worst one : a power loss.
Let's see what runs and how it runs before the crash :
From the SAN to the cloud controller, we have an active iscsi session (used for the "cindervolumes" LVM's VG)
From the cloud controller to the compute node we also have active iscsi sessions (managed by cinder-volume)
From the cloud controller to the compute node, we also have iptables/ ebtables rules which allows the access from the cloud controller to the running instance.
Now, after the power loss occurs and all hardware components restart, the situation is as follows:
From the SAN to the cloud, the ISCSI session no longer exists.
From the cloud controller to the compute node, the ISCSI sessions no longer exist.
From the cloud controller to the compute node, the iptables and ebtables are recreated, since, at boot, nova-network reapply the configurations.
From the cloud controller, instances turn into a shutdown state (because they are no longer running)
Into the database, data was not updated at all, since nova could not have guessed the crash.
Before going further, and in order to prevent the admin to make fatal mistakes, the instances won't be lost, since no "destroy" or "terminate" command had been invoked, so the files for the instances remain on the compute node.
The plan is to perform the following tasks, in that exact order.
Any extra step would be dangerous at this stage :
We need to get the current relation from a volume to its instance, since we will recreate the attachment.
We need to update the database in order to clean the stalled state.
We need to restart the instances (so go from a "shutdown" to a "running" state)
After the restart, we can reattach the volumes to their respective instances.
That step, which is not a mandatory one, exists in an SSH into the instances in order to reboot them.
We need to get the current relation from a volume to its instance, since we will recreate the attachment :
This relation could be figured by running nova volume-list (note that nova client includes ability to get volume info from cinder)
Second, we need to update the database in order to clean the stalled state.
Now that we have saved the attachments we need to restore for every volume, the database can be cleaned with the following queries:
Now, when running nova volume-list all volumes should be available.
This can be done via a simple nova reboot $instance.
At that stage, depending on your image, some instances will completely reboot and become reachable, while others will stop on the "plymouth" stage.
In fact it depends on whether you added an /etc/fstab entry for that volume or not.
Images built with the cloud-init package will remain on a pending state, while others will skip the missing volume and start.
After the restart, we can reattach the volumes to their respective instances.
Now that nova has restored the right status, it is time to perform the attachments via a nova volume-attach.
Here is a simple snippet that uses the file we created :
At that stage, instances which were pending on the boot sequence (plymouth) will automatically continue their boot, and restart normally, while the ones which booted will see the volume.
If some services depend on the volume, or if a volume has an entry into fstab, it could be good to simply restart the instance.
This restart needs to be made from the instance itself, not via nova.
So, we SSH into the instance and perform a reboot :
Use the parameter errors=remount in the fstab file, which will prevent data corruption.
The system would lock any write to the disk if it detects an I/O error.
This configuration option should be added into the cinder-volume server (the one which performs the ISCSI connection to the SAN), but also into the instances' fstab file.
Do not add the entry for the SAN's disks to the cinder-volume's fstab file.
Some systems will hang on that step, which means you could lose access to your cloud-controller.
In order to re-run the session manually, you would run the following command before performing the mount:
For your instances, if you have the whole /home/ directory on the disk, instead of emptying the /home directory and map the disk on it, leave a user's directory with the user's bash files and the authorized_keys file.
This will allow you to connect to the instance, even without the volume attached, if you allow only connections via public keys.
C- Scripted DRP You can download from here a bash script which performs these five steps :
The "test mode" allows you to perform that whole sequence for only one instance.
To reproduce the power loss, connect to the compute node which runs that same instance and close the iscsi session.
Do not dettach the volume via nova volume-detach, but instead manually close the iscsi session.
In the following example, the iscsi session is number 15 for that instance :
Do not forget the flag -r; otherwise, you will close ALL sessions.
The OpenStack dashboard, a Web interface, enables you to connect to running instances through a VNC connection.
About the Dashboard The OpenStack dashboard, also known as horizon, is a Web interface that allows cloud administrators and users to manage various OpenStack resources and services.
The dashboard enables web-based interactions with the OpenStack Compute cloud controller through the OpenStack APIs.
The following instructions show you an example deployment that is configured with an Apache web server.
System Requirements for the Dashboard Because Apache does not serve content from a root user, you must use another user with sudo privileges and run as that user.
You should have a running OpenStack Compute installation with the Identity Service, Keystone, enabled for identity management.
The dashboard needs to be installed on the node that can contact the Identity Service.
You should know the URL of your Identity endpoint and the Compute endpoint.
You must know the credentials of a valid Identity service user.
It's straightforward to install it with sudo apt-get install gitcore.
Install and configure the dashboard Before you can install and configure the dashboard, meet the following system requirements:
The cloud operator must set up an OpenStack Compute installation and enable the Identity Service for user and project management.
Because Apache does not serve content from a root user, the cloud operator must set up an Identity Service user with sudo privileges.
For more information about how to deploy the dashboard, see Deploying Horizon.
Install the dashboard on the node that can contact the Identity Service as root:
If you change the memcached settings, you must restart the Apache web server for the changes to take effect.
You can use options other than memcached option for session storage.
Make sure that the web browser on your local machine supports HTML5
You can configure the dashboard for a simple HTTP deployment or a secured HTTPS deployment.
While the standard installation uses a non-encrypted HTTP channel, you can enable SSL support for the dashboard.
For multiple regions uncomment this configuration, and add (endpoint, title)
Use this setting when Horizon is running # external to the OpenStack environment.
The number of Swift containers and objects to display on a single page before # providing a paging element (a "more" link) to paginate results.
The HORIZON_CONFIG dictionary contains all the settings for the dashboard.
Whether or not a service is in the dashboard depends on the Service Catalog configuration in the Identity Service.
Point your browser to the public IP address for your instance.
After you connect to the dashboard through the URL, a login page appears.
Enter the credentials for any user that you created with the OpenStack Identity Service.
For example, enter admin for the user name and secrete for the password.
In this configuration, Apache listens on the port 443 and redirects all the hits to the HTTPS protocol for all the non-secured requests.
In the secured section, the private key, public key, and certificate to use are defined.
If you call the HTTP version of the dashboard through your browser, you are redirected to the HTTPS page.
Point your browser to the public IP address for your instance.
After you connect to the dashboard through the URL, a login page appears.
Enter the credentials for any user that you created with the OpenStack Identity Service.
For example, enter admin for the user name and secrete for the password.
The size of the window image used for VNC is hard-coded in a Django HTML template.
To alter the hard-coded values, edit the _detail_vnc.html template file.
For multiple regions uncomment this configuration, and add (endpoint, title)
Use this setting when Horizon is running # external to the OpenStack environment.
The number of Swift containers and objects to display on a single page before # providing a paging element (a "more" link) to paginate results.
The HORIZON_CONFIG dictionary contains all the settings for the Dashboard.
Whether or not a service is in the Dashboard depends on the Service Catalog configuration in the Identity service.
Refer to Horizon Settings and Configuration for the full listing.
While the standard installation uses a non-encrypted channel (HTTP), it is possible to enable the SSL support for the OpenStack Dashboard.
In the following example, we use the domain "http://openstack.example.com", make sure to use one that fits your current setup.
In this configuration, we instruct Apache to listen on the port 443 and to redirect all the hits to the HTTPs protocol for all the non-secured requests.
In the secured section, we define as well the private key, the public key, and the certificate to use.
You should now be redirected to the HTTPs page if you call the HTTP version of the dashboard via your browser.
Validating the Dashboard Install To validate the Dashboard installation, point your browser at http://192.168.206.130
Once you connect to the Dashboard with the URL, you should see a login window.
Enter the credentials for users you created with the Identity Service, Keystone.
For example, enter "admin" for the username and "secrete" as the password.
When deploying OpenStack on Ubuntu Server 12.04, you can have the openstackdashboard package installed to provide the web-based “Horizon” GUI component.
The Horizon documents briefly mention branding customization to give you a head start, but here are more specific steps.
Here’s a custom-branded Horizon dashboard with custom colors, logo, and site title:
Once you know where to make the appropriate changes, it’s super simple.
Create a new CSS stylesheet — we’ll call ours custom.css — in the directory:
Edit your CSS file using the following as a starting point for customization, which simply overrides the Ubuntu customizations made in the ubuntu.css file.
Change the colors and image file names as appropriate, though the relative directory paths should be the same.
Reload the dashboard in your browser and fine tune your CSS appropriate.
OpenStack Dashboard Session Storage Horizon uses Django’s sessions framework for handling user session data; however that’s not the end of the story.
What follows is a quick discussion of the pros and cons of each of the common options as they pertain to deploying Horizon specifically.
Local memory storage is the quickest and easiest session backend to set up, as it has no external dependencies whatsoever.
The local memory backend is enabled as the default for Horizon solely because it has no dependencies.
It is not recommended for production use, or even for serious development work.
External caching using an application such as memcached offers persistence and shared storage, and can be very useful for small-scale deployment and/or development.
However, for distributed and high-availability scenarios memcached has inherent problems which are beyond the scope of this documentation.
Memcached is an extremely fast and efficient cache backend for cases where it fits the deployment need, but it’s not appropriate for all scenarios.
Database-backed sessions are scalable (using an appropriate database strategy), persistent, and can be made high-concurrency and highly-available.
The downside to this approach is that database-backed sessions are one of the slower session storages, and incur a high overhead under heavy usage.
Proper configuration of your database deployment can also be a substantial undertaking and is far beyond the.
To enable, follow the below steps to initialise the database and configure it for use.
Create a MySQL user for the newly-created dash database that has full control of the database.
As a result, you should see the following at the end of what returns:
If you want to avoid a warning when restarting apache2, create a blackhole directory in the dashboard directory like so:
Restart Apache to pick up the default site and symbolic link settings.
Restart the nova-api service to ensure the API server can connect to the Dashboard and to avoid an error displayed in the Dashboard.
To mitigate the performance issues of database queries, you can also consider using Django’s cached_db session backend which utilizes both your database and caching infrastructure to perform write-through caching and efficient retrieval.
You can enable this hybrid setting by configuring both your database and cache as discussed above and then using:
This backend stores session data in a cookie which is stored by the user’s browser.
The backend uses a cryptographic signing technique to ensure session data is not tampered with during transport (this is not the same as encryption, session data is still readable by an attacker)
The pros of this session engine are that it doesn’t require any additional dependencies or infrastructure overhead, and it scales indefinitely as long as the quantity of session data being stored fits into a normal cookie.
The biggest downside is that it places session data into storage on the user’s machine and transports it over the wire.
It also limits the quantity of session data which can be stored.
For a thorough discussion of the security implications of this session backend, please read the Django documentation on cookie-based sessions.
This section explains the various steps to be followed to launch a instance.
Before launching a VM, first modify the Security Groups rules to allow us to ping and SSH to the instances.
This is done by editing the default security group or adding a new security group.
If you want requests from particular range of IP, provide it in CIDR field.
If you want ping requests from particular range of IP, provide it in CIDR field.
Once a Keypair is added, the public key would be downloaded.
This key can be used to SSH to the launched instance.
Once this is done, we are now all set to launch an Instance.
Click Images & Snapshots and launch a required instance from the list of images available.
Provide a Server Name, select the flavor, the keypair added above and the default security group.
Once the status is Active, the instance is ready and we can ping and SSH to the instance.
Make a secure connection to the launched instance Here are the steps to SSH into an instance using the downloaded keypair file.
The username is ubuntu for the Ubuntu cloud images on TryStack.
In a command line interface, modify the access to the .pem file:
Use the ssh-add command to ensure that the keypair is known to SSH:
Use the SSH command to make a secure connection to the instance:
You should see a prompt asking "Are you sure you want to continue connection (yes/ no)?" Type yes and you have successfully connected.
Remote Console Access OpenStack has two main methods for providing a remote console or remote desktop access to guest Virtual Machines.
They are VNC, and SPICE HTML5 and can be used either through the OpenStack dashboard and the command line.
Best practice is to select one or the other to run.
Overview of VNC Proxy The VNC Proxy is an OpenStack component that allows users of the Compute service to access their instances through VNC clients.
User connects to API and gets an access_url like http://ip:port/?token=xyz.
Proxy talks to nova-consoleauth to authorize the user's token, and then maps the token to the private host and port of an instance's VNC server.
In this way, the vnc proxy works as a bridge between the public network, and the private host network.
Proxy initiates connection to VNC server, and continues proxying until the session ends.
The proxy also performs the required function of tunneling the VNC protocol over Websockets so that the noVNC client has a way to talk VNC.
Note that in general, the VNC proxy performs multiple functions:
Bridges between public network (where clients live) and private network (where vncservers live)
Both client proxies leverage a shared service to manage token auth called novaconsoleauth.
This service must be running in order for either proxy to work.
Many proxies of either type can be run against a single nova-consoleauth service in a cluster configuration.
The nova-consoleauth shared service should not be confused with nova-console, which is a XenAPI-specific service that is not used by the most recent VNC proxy architecture.
For simple deployments, this service typically will run on the same machine as nova-api, since it proxies between the public network and the private compute host network.
This supports the special Java client discussed in this document.
For simple deployments, this service typically will run on the same machine as nova-api, since it proxies between the public network and the private compute host network.
These compute hosts must have correctly configured configuration options, as described below.
Nova provides the ability to create access_urls through the os-consoles extension.
Specify 'novnc' to retrieve a URL suitable for pasting into a web browser.
Specify 'xvpvnc' for a URL suitable for pasting into the Java client.
If you intend to support live migration, you cannot specify a specific IP address for vncserver_listen, because that IP address will not exist on the destination host.
This is the address of the compute host that nova will instruct proxies to use when connecting to instance servers.
For all-in-one XenServer domU deployments this can be set to 169.254.0.1
For multi-host XenServer domU deployments this can be set to a dom0 management ip on the same network as the proxies.
For multi-host libvirt deployments this can be set to a host management IP on the same network as the proxies.
To enable support for the OpenStack Java VNC client in Compute, we provide the novaxvpvncproxy service, which you should run to enable this feature.
As a client, you need a special Java client, which is a slightly modified version of TightVNC that supports our token auth:
To create a session, request an access URL by using python-novaclient.
You will need the novnc package installed, which contains the nova-novncproxy service.
The configuration option parameter should point to your nova.conf configuration file that includes the message queue server address and credentials.
In order to connect the service to your nova deployment, add the two following configuration options into your nova.conf file :
This configuration option allow you to specify the address for the vnc service to bind on, make sure it is assigned one of the compute node interfaces.
This address will be the one used by your domain file.
This is the address of the compute host that nova will instruct proxies to use when connecting to instance vncservers.
Retrieving an access_url for a web browser is similar to the flow for the Java client.
Additionally, you can use the OpenStack Dashboard (codenamed Horizon), to access browser-based VNC consoles for instances.
A: nova-xvpvncproxy which ships with nova, is a new proxy that supports a simple Java client.
A: You need nova-novncproxy, nova-consoleauth, and correctly configured compute hosts.
Q: When I use nova get-vnc-console or click on the VNC tab of the Dashboard, it hangs.
A: Make sure you are running nova-consoleauth (in addition to nova-novncproxy)
The proxies rely on nova-consoleauth to validate tokens, and will wait for a reply from them until a timeout is reached.
Q: My vnc proxy worked fine during my All-In-One test, but now it doesn't work on multi host.
A: The default options work for an All-In-One install, but changes must be made on your compute hosts once you start to build a cluster.
This is the address where the underlying vncserver (not the proxy) # will listen for connections.
Q: My noVNC does not work with recent versions of web browsers.
A: Make sure you have python-numpy installed, which is required to support a newer version of the WebSocket protocol (HyBi-07+)
Q: How do I adjust the dimensions of the VNC window image in horizon?
A: These values are hard-coded in a Django HTML template.
To alter them, you must edit the template file _detail_vnc.html.
The location of this file will vary based on Linux distribution.
Spice Console OpenStack Compute has long had support for VNC consoles to guests.
The VNC protocol is fairly limited, lacking support for multiple monitors, bi-directional audio, reliable cut+paste, video streaming and more.
Options for configuring SPICE as the console for OpenStack Compute can be found below.
OpenStack Compute can be integrated with various third-party technologies to increase security.
Trusted compute pools enable administrators to designate a group of compute hosts as "trusted"
These hosts use hardware-based security features, such as Intel's Trusted Execution Technology (TXT), to provide an additional level of security.
Combined with an external standalone web-based remote attestation server, cloud providers can ensure that the compute node is running software with verified measurements, thus they can establish the foundation for the secure cloud stack.
Through the Trusted Computing Pools, cloud subscribers can request services to be run on verified compute nodes.
The remote attestation server performs node verification through the following steps:
These measured data is sent to the attestation server when challenged by attestation server.
The attestation server verifies those measurements against good/known database to determine nodes' trustworthiness.
A description of how to set up an attestation service is beyond the scope of this document.
See the Open Attestation project for an open source project that can be used to implement an attestation service.
The Compute service must be configured to with the connection information for the attestation service.
The connection information is specified in the trusted_computing section of nova.conf.
Restart the nova-compute and nova-scheduler services after making these changes.
Specify trusted flavors One or more flavors must be configured as "trusted"
Users can then request trusted nodes by specifying one of these trusted flavors when booting a new instance.
A user can request that their instance runs on a trusted host by specifying a trusted flavor when invoking the nova boot command.
In a large-scale cloud deployment, automated installations are a requirement for successful, efficient, repeatable installations.
Automation for installation also helps with continuous integration and testing.
This chapter offers some tested methods for deploying OpenStack Compute with either Puppet (an infrastructure management platform) or Chef (an infrastructure management framework) paired with Vagrant (a tool for building and distributing virtualized development environments)
Provide REST API to make it possible to integrate it with other tools.
Node - The machine that is the target of installation.
Proposal - The set of the kinds of configurations which describe how to install a software.
The configurations include "Node config", "Config item", "Software config", "Component config"
Node config - A configuration that describes which component to be installed on a node.
Config item - A variable which can be used in the content of software config and component config.
Software config - A configuration that describes the content of a configuration file for all components.
Component config - A configuration that describes the content of a configuration file for only one component.
Installation The $home in the following sections is the path of the home directory of the dodai-deploy.
Execute the following commands on the dodai-deploy server and all the nodes.
Execute the following commands on dodai-deploy server to install necessary softwares and modify their settings.
Execute the following commands on all the nodes to install necessary softwares and modify their settings.
The $server in the above command is the fully qualified domain name (fqdn) of the dodai-deploy server.
After nodes were set up, the system time of nodes should be synchronized with dodaideploy server.
You must set up a storage device before swift is installed.
You should execute the commands for a physical device or for a loopback device on all nodes in which swift storage server is to be installed.
You must create a volume group before nova-volume is installed.
You should execute the commands for a physical device or for a loopback device on the node in which novavolume is to be installed.
Execute the following command on the dodai-deploy server to start the web server and job server.
You can stop the web server and job server with the following command.
Using web UI You can find step-by-step guidance at http://$dodai_deploy_server:3000/
You can get the list of REST APIs with it.
You can also execute APIs by simply filling in parameters and clicking the "Execute" button.
An instance will be started during the test of nova.
After the test, you can login the instance by executing the following commands.
Glance should be installed before using nova, because nova depends on glance in default settings.
We want OpenStack to make sense, and sometimes the best way to make sense of the cloud is to try out some basic ideas with cloud computing.
Flexible, elastic, and scalable are a few attributes of cloud computing, so these tutorials show various ways to use virtual computing or web-based storage with OpenStack components.
In this OpenStack Compute tutorial, we’ll walk through the creation of an elastic, scalable cloud running a WordPress installation on a few virtual machines.
The tutorial assumes you have obtained a TryStack account at http://trystack.org.
It has a working installation of OpenStack Compute, or you can install your own using the installation guides.
On the instances you spin up, installing Wordpress and its dependencies, the Memcached plugin, and multiple memcache servers.
Once you've joined the group, go to the TryStack dashboard and click Login using Facebook.
Enter your Facebook login information to receive your username and password that you can use with the Compute API.
Next, install the python-novaclient and set up your environment variables so you can use the client with your username and password already entered.
Next, create a file named openrc to contain your TryStack credentials, such as:
Okay, you've created the basic scaffolding for your cloud user so that you can get some images and run instances on TryStack with your starter set of StackDollars.
Part II: Starting Virtual Machines Understanding what you can do with cloud computing means you should have a grasp on the concept of virtualization.
With virtualization, you can run operating systems and applications on virtual machines instead of physical computers.
To use a virtual machine, you must have an image that contains all the information about which operating system to run, the user login and password, files stored on the system, and so on.
Now get a list of the flavors you can launch:
Create a keypair to launch the image, in a directory where you run the nova boot command later.
Create security group that enables public IP access for the webserver that will run WordPress for you.
Next, with the ID value of the server you want to launch and the ID of the flavor you want to launch, use your credentials to start up the instance with the identifier you got by looking at the image list.
Now you can look at the state of the running instances by using nova list.
The instance goes from “launching” to “running” in a short time, and you should be able to connect via SSH.
Look at the IP addresses so that you can connect to the instance once it starts running.
Diagnose your compute node You can obtain extra informations about the instance you just spawned : its CPU usage, the memory, the disk io or network io, per instance, by running the nova diagnostics command:
Basically launch a terminal window from any computer, and enter:
On this particular image, the 'ubuntu' user has been set up as part of the sudoers group, so you can escalate to 'root' via the following command:
The WordPress package will extract into a folder called wordpress in the same directory that you downloaded latest.tar.gz.
On a second VM, install MySQL Next, SSH into another virtual machine and install MySQL and use these instructions to install the WordPress database using the MySQL Client from a command line: Using the MySQL Client - Wordpress Codex.
On a third VM, install Memcache Memcache makes Wordpress database reads and writers more efficient, so your virtual servers can go to work for you in a scalable manner.
From a web browser, point to the IP address of your Wordpress server.
Running a Blog in the Cloud That's it! You're now running your blog on a cloud server in OpenStack Compute, and you've scaled it horizontally using additional virtual images to run the database and Memcache.
Now if your blog gets a big boost of comments, you'll be ready for the extra reads-and-writes to the database.
Online resources aid in supporting OpenStack and the community members are willing and able to answer questions and help with bug suspicions.
We are constantly improving and adding to the main features of OpenStack, but if you have any problems, do not hesitate to ask.
Here are some ideas for supporting OpenStack and troubleshooting your existing installations.
Community Support Here are some places you can locate others who want to help.
When visiting the Ask site at http://ask.openstack.org, it is usually good to at least scan over recently asked questions to see if your question has already been answered.
If that is not the case, then proceed to adding a new question.
Be sure you give a clear, concise summary in the title and provide as much detail as possible in the description.
Paste in your command output or stack traces, link to screenshots, and so on.
OpenStack mailing lists Posting your question or scenario to the OpenStack mailing list is a great way to get answers and insights.
You can learn from and help others who may have the same scenario as you.
You may be interested in the other mailing lists for specific projects or development - these can be found on the wiki.
A description of all the additional mailing lists is available at http://wiki.openstack.org/MailingLists.
The OpenStack Wiki search The OpenStack wiki contains content on a broad range of topics, but some of it sits a bit below the surface.
Fortunately, the wiki search feature is very powerful in that it can do both searches by title and by content.
If you are searching for specific information, say about "networking" or "api" for nova, you can find lots of content using the search feature.
More is being added all the time, so be sure to check back often.
You can find the search box in the upper right hand corner of any OpenStack wiki page.
The Launchpad Bugs area So you think you've found a bug.
The OpenStack community values your setup and testing efforts and wants your feedback.
You can view existing bugs and report your bug in the Launchpad Bugs area.
It is suggested that you first use the search facility to see if the bug you found has already been reported (or even better, already fixed)
If it still seems like your bug is new or unreported then it is time to fill out a bug report.
Paste in your command output or stack traces, link to screenshots, etc.
Be sure to include what version of the software you are using.
This is especially critical if you are using a development branch eg.
Any deployment specific info is helpful as well, such as Ubuntu 12.04, multi-node install.
The OpenStack IRC channel The OpenStack community lives and breathes in the #openstack IRC channel on the Freenode network.
You can come by to hang out, ask questions, or get immediate feedback for urgent and pressing issues.
To get into the IRC channel you need to install an IRC client or use a browser-based client by going to http://webchat.freenode.net/
You can also use Colloquy (Mac OS X, http://colloquy.info/) or mIRC (Windows, http:// www.mirc.com/) or XChat (Linux)
When you are in the IRC channel and want to share code or command output, the generally accepted method is to use a Paste Bin, the OpenStack project has one at http://paste.openstack.org.
Just paste your longer amounts of text or logs in the web form and you get a URL you can then paste into the channel.
Common problems for Compute typically involve misconfigured networking or credentials that are not sourced properly in the environment.
Also, most flat networking configurations do not enable ping or ssh from a compute node to the instances running on that node.
This section offers more information about how to troubleshoot Compute.
Log files for OpenStack Compute Log files are stored in /var/log/nova and there is a log file for each service, for example nova-compute.log.
You can format the log strings using options for the nova.log module.
You have two options for logging for OpenStack Compute based on configuration settings.
Alternatively you can set use_syslog=1, and then the nova daemon logs to syslog.
We are constantly fixing bugs, so online resources are a great way to get the most up-to-date errors and fixes.
Through current installation methods, there are basically two ways to get the novarc file.
The manual method requires getting it from within a project zipfile, and the scripted method just generates novarc out of the project zip file and sources it for you.
If you do the manual method through a zip file, then the following novarc alone, you end up losing the creds that are tied to the user you created with nova-manage in the steps before.
When you run nova-api the first time, it generates the certificate authority information, including openssl.cnf.
If it gets started out of order, you may not be able to create your zip file.
Once your CA information is available, you should be able to go back to nova-manage to create your zipfile.
You may also need to check your proxy settings to see if they are causing problems with the novarc creation.
Sometimes a particular instance shows "pending" or you cannot SSH to it.
For example, when using flat manager networking, you do not have a dhcp server, and an ami-tiny image doesn't support interface injection so you cannot connect to it.
The fix for this type of problem is to use an Ubuntu image, which should obtain an IP address correctly with FlatManager network settings.
To troubleshoot other possible problems with an instance, such as one that stays in a spawning state, first check your instances directory for i-ze0bnh1q dir to make sure it has the following files:
Check the file sizes to see if they are reasonable.
If any are missing/zero/very small then nova-compute has somehow not completed download of the images from objectstore.
Manually reset the state of an instance If an instance gets stuck in an intermediate state (e.g., "deleting"), you can manually reset the state of an instance using the nova reset-state command.
This will reset it to an error state, which you can then delete.
You can also use the --active to force the instance back into an active state instead of an error state, for example:
Problems with Injection If you are diagnosing problems with instances not booting, or booting slowly, consider investigating file injection as a cause.
This can be required if you want to make user specified files.
This document is the work of a large collection of people, from many different organisations who have put in long hours to create what you read here.
Rackspace - who created much of the original documentation, on which the current versions are based.
Anne Gentle - without her stewardship, these documents would not exist.
Installing MooseFS as shared storage for the instances directory Installing the MooseFS metadata and metalogger servers Installing the MooseFS chunk and client services Access to your cluster storage.
Users user-create user-delete user-list user-update --email user-enable user-disable user-update --password.
Getting virtual machine images Tool support for creating images Customizing an image for OpenStack Creating custom raw or QCOW2 images.
Select a specific host to boot instances on Creating images from running instances with KVM and Xen Replicating images across multiple data centers.
Manage Security Groups Add or delete a security group Modify security group rules.
Instance evacuation Before Evacuation To evacuate your server without shared storage: Evacuate server to specified host and preserve user data.
Removing a Network from a Project Using multiple interfaces for your instances (multinic) Using the multinic feature.
