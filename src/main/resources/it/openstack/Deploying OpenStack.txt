The image of a Tenrec and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
This book is aimed at developers, technologists, and system administrators eager to understand and deploy cloud computing infrastructure projects based upon OpenStack software.
It is intended to provide the reader with a solid understanding of the OpenStack project goals, details of specific OpenStack software components, general design decisions, and detailed steps to deploy OpenStack in a few controlled scenarios.
Along the way, readers would also learn common pitfalls in architecting, deploying, and implementing their cloud.
Intended Audience This book assumes that the reader is familiar with public Infrastructure as a Service (IaaS) cloud offerings such as Rackspace Cloud or Amazon Web Services.
In addition, it demands an understanding of Linux systems administration, such as installing servers, networking with iptables, and basic virtualization technologies.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
With a subscription, you can read any page and watch any video from our library online.
Access new titles before they are available for print, and get exclusive access to manuscripts in development and post feedback for the authors.
Copy and paste code samples, organize your favorites, download chapters, bookmark key sections, create notes, print out pages, and benefit from tons of other time-saving features.
To have full digital access to this book and others on similar topics from O’Reilly and other publishers, sign up for free at http://my.safaribooksonline.com.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
Find us on Facebook: http://facebook.com/oreilly Follow us on Twitter: http://twitter.com/oreillymedia Watch us on YouTube: http://www.youtube.com/oreillymedia.
Acknowledgments In general, I would like to thank the entire OpenStack community that gathers on the #openstack IRC channel, mail aliases, and forums.
The help and encouragement from hoards of people I might never meet face-to-face has been of immeasurable value.
More specifically, I would like to thank many people for their help both in the past and the present that led me to this place:
A special thanks to Josh Kearney for collaborating with me on my first Nova blueprint, as well as technically reviewing this book.
Jay Pipes, for walking me through my first halting few commits and his leadership of Glance.
Vishvananda Ishaya, for generally being a fountain of cloud knowledge and for his technical leadership of the Nova project.
Anne Gentle, for spearheading the awesome OpenStack wiki and documentation.
The people at Cloudscaling, who have been helping customers around the world deploy OpenStack clouds.
Brian Pepple, for his technical review of the book, as well as his introduction to open source development.
Diego Parrilla and the team at StackOps, for access to their distribution and for their technical review of the book.
Shlomo Swidler, for insights into cloud infrastructures at levels above where I usually contemplate.
Dan Sanderson, who unlocked the riddle of using Scrivener, DocBook, Python, and subversion in harmony for me.
All the great people that I worked with at Sun Microsystems over the years, especially Dr.
Many of them were pioneering dynamic infrastructures long ago and profoundly influenced my thinking along the way.
Finally, but certainly not least, thanks to my amazing partner, Shelley, for her love and support.
The OpenStack project has been created with the audacious goal of being the ubiquitous software choice for building cloud infrastructures.
In just over one year, it has gone from an idea to start collaborating to being the most talked-about project in open source.
In this chapter, we will examine the project’s goals, history, and how you can participate in its future.
What Is the OpenStack Project ? The OpenStack Project aims to create an open source cloud computing platform for public and private clouds aimed at scalability without complexity.
Initially focusing on Infrastructure as a Service (IaaS) offerings, the project currently encompasses three components:
OpenStack Compute: Software to orchestrate, manage, and offer virtual machines.
OpenStack Object Store: Software for the redundant storage of static objects.
OpenStack Image Service: Provides query and storage services for virtual disk images.
One of the defining core values behind the project is its embrace of openness with both open standards and open source code.
If you are unfamiliar with the license, you should review the full license or skip to the layman’s terms.
In addition, OpenStack promotes open standards through the OpenStack API.
The OpenStack project began through the work of two organizations: Rackspace Hosting (a large US hosting firm) and NASA (the US Space agency) decided to join forces and release their internal cloud object storage and cloud compute code bases (respectively) as a common open source project.
These releases were the basis for OpenStack Object Storage (“Swift”) and OpenStack Compute (“Nova”) projects.
After the first release, another project (named “Glance”) was added to handle image storage.
Currently, these are the only official components of the OpenStack project.
Release names are decided by popular vote by the community of developers from a pool of city names near the site of the next OpenStack Developers Summit.
Table 1-1 shows OpenStack releases and the corresponding software versions.
You can always see a list of all (past and future) releases at http://wiki.openstack.org/ Releases.
Community Much is made of the large community aspect of OpenStack, and with great reason: the community was created by end users (cloud service providers and large enterprise) with the active participation of large computing vendors and many other open source projects.
In less than a year, OpenStack has become arguably the largest open source cloud project.
The OpenStack community is extremely active and maintains many outlets for information about the project:
The OpenStack wiki is hosted at http://wiki.openstack.org/StartingPage and is updated almost daily with new information.
The official documentation for each of the OpenStack project releases is available at http://docs.openstack.org/
Each of the lists are targeted to different audiences and have different volumes of email.
In the future, the codebase may be moving to http://github.com/
Blog posts from OpenStack developers and prominent community members are aggregated at http://planet.openstack.org/
Many beginners assume that Swift will take the place of their file server and that they will be able to easily mount volumes on their desktops to access their files.
Object stores simply save files in logical groupings (called “containers” in Swift parlance) via a RESTful protocol.
They do not provide a true filesystem, nor are they accessible through standard file sharing protocols like NFS (Network File System, the standard for UNIX), CIFS (Common Internet File System, the standard for Windows), or AFS (Appleshare Files System, the standard for Mac OS X)
To access your files, you will need to use a the Swift API client.
Swift is configurable in terms of how many copies (called “replicas”) are written, as well as how many zones are configured.
Current best practices call for three replicas written across five zones.
As the number of replicas is less than or equal to the number of zones, Swift tries to balance the writing of objects to storage servers so that the write and read load is distributed.
Architecture The logical view of Swift can be divided into two logical parts: presentation and resource.
The major components, data stores, and interactions are illustrated in Figure 2-2
It can optionally work with a cache (memcached*) to reduce authentication, container, and account calls.
There is also an optional middleware to support the Amazon S3 protocol.
User issues a second request to Swift (directly to swift-proxy), passing the token along with the request in the HTTP headers.
Memcached is a free and open-source in-memory key-value store for caching small pieces of data.
Swift authentication can be implemented through WSGI middleware or as a separate system.
For most installations, the WSGI middleware option will be more straightforward.
However, some enterprises might find the separate system approach easier to integrate to their current authentication scheme.
Swift ships with sample authentication code called swauth, which stores the authentication database within Swift itself.
Resource Swift manages a number of information sources through three processes that fulfill requests from swift-proxy.
Each of these processes are responsible for fulfilling requests from the proxy node, as well as auditing their own mappings (database consistency) and replicating any inconsistent information to other nodes in the ring.
First debuting in the Bexar release, Glance provides a catalog service for storing and querying virtual disk images.
Glance has been designed to be a standalone service for those needing to organize large sets of virtual disk images.
However, when used along with Nova and Swift, it provides an end-toend solution for cloud disk image management.
Architecture There are three pieces to Glance architecture: glance-api, glance-registry, and the image store.
As you can probably guess, glance-api accepts API calls, much like nova-api, and the actual image blobs are placed in the image store.
The image store can be a number of different object stores, including Swift.
In the Cactus release, Glance lacks authentication and authorization, making it unsuitable for direct end user usage except in tightly controlled environments.
The best way to use this is “behind” Nova, where nova-api authenticates and authorizes requests for uploading, querying, and using virtual disk images.
The version that ships with Glance is only considered a reference implementation, as most large installations will want a customized version for their service.
The reference version uses sqlite3 to store the metadata and the Glance API for communications.
While the image representation and image metadata is stored in the database, the actual images are stored in image stores.
Image stores are the storage places for the virtual disk image and come in a number of different options.
The currently supported image stores are shown in Table 3-1
Stores, deletes, and gets images from a filesystem directory specified in the configuration file (filesys tem_store_datadir option)
This could be a filesystem on a shared drive (e.g., NFS)
Images will need to be saved to the URL via another mechanism.
Swift Stores, deletes, and gets images from a Swift installation.
Each of these options have their own strengths and weaknesses.
However, most large installations will use Swift, while smaller installations will probably gravitate to the simplicity of the filesystem option with a shared NFS server.
The S3 or HTTP image stores are probably only useful for referencing publicly available images.
With this overview of Glance, it should now be clear how Glance provides the “glue” between Swift and Nova.
Figure 3-2 shows the interactions between OpenStack projects for virtual disk image storage and retrieval.
Image Support Glance supports a wide array of virtual disk and container formats.
Virtual disks are analogous to a physical server’s boot drives, only condensed into a file.
AMI, ARI, AKI Amazon machine, ramdisk, and kernel images (respectively)
Glance also supports the concept of container formats, which describes the file format and contains additional metadata.
Glance supports two container formats as well as the absence of a container format (bare), as shown in Table 3-3
Data is returned as a JSON-encoded mapping (query) or binary (image retrieval)
Below is an example of querying Glance using curl for details on all images:
In this example, Glance shows that there is only one image in the registry.
The List Images and List Images Detail calls can return huge amounts of data for large image stores, as no record filtering exists in this version of Glance.
Store Image POST /images Stores the image and then returns the metadata created about it.
List Images Detail GET /images/detail Return list (with all metadata) of all images.
The full API documentation can be viewed at the online documentation website.
Installation If you are on Ubuntu 11.04 or later, Glance can be installed with a simple apt-get:
Once installed, it needs to be started with the glance-control utility.
Glance is now ready for uploading and querying of virtual disk images.
The glance index command shows all virtual disk images currently in Glance.
As you can see from the example above, Glance is currently empty.
The glance-upload command puts an image record into the glance-registry and stores the data file in the image store.
It requires a the disk format and container format as a minimum set of arguments.
In this example, the virtual disk is formatted as an AMI from the Ubuntu Enterprise Cloud image repository.
Now that an image has been uploaded, Glance should show it via the glance index command.
The glance show command is able to show details about the newly uploaded image.
Nova seeks to provide a framework for the large-scale provisioning and management of virtual compute instances.
Similar in functionality and scope to Amazon’s EC2 service, it allows you to create, manage, and destroy virtual servers based on your own system images through a programmable API.
Nova Architecture Nova is architected as a distributed application with many components, but the majority of these are custom-written Python daemons of two varieties:
Worker daemons to carry out orchestration tasks However, there are two essential pieces of the architecture that are neither custom written nor Python-based: the messaging queue and the database.
These two components facilitate the asynchronous orchestration of complex tasks through message passing and information sharing.
Piecing this all together we get a picture like Figure 4-1
This complicated diagram can be summed up in three sentences:
End users who want to use Nova to create compute instances call nova-api with OpenStack API or EC2 API requests.
Nova daemons exchange information through the queue (actions) and database (information) to carry out these API requests.
Glance is a completely separate service that Nova interfaces through the Glance API to provide virtual disk imaging services.
Now that we’ve seen the overview of the processes and their interactions, let’s take a closer look at each component.
Its primary purpose is to accept and fulfill incoming API requests.
To accept and fulfill API requests, nova-api provides an endpoint for all API queries (accepting requests using either the OpenStack API or the Amazon EC2 API), initiates most of the orchestration activities (such as running an instance), and also enforces some policy (mostly quota checks)
For some requests, it will fulfill the entire request itself by querying the database and then returning the answer.
Scheduler The nova-scheduler process is conceptually the simplest piece of code in Nova: it takes a virtual machine instance request from the queue and determines where it should run (specifically, which compute server host it should run on)
In practice, however, this will grow to be the most complex piece, as it needs to factor in the current state of the entire cloud infrastructure and apply complicated algorithms to ensure efficient usage.
To that end, nova-scheduler implements a pluggable architecture that lets you choose (or write) your own algorithm for scheduling.
As you can see from above code sample, the schedule method simply chooses a random host from the array of hosts that are currently known to be “up.”
Compute Worker The nova-compute process is primarily a worker daemon that creates and terminates virtual machine instances.
The process by which it does so is fairly complex, but the basics are simple: accept actions from the queue and then perform one or a series of.
An example of this would be nova-compute accepting a message from the queue to create a new instance and then using the libvirt library to start a new KVM instance.
There are a variety of ways that nova-compute manages virtual machines.
The most common is through a software package called libvirt.
This is a toolkit (API, daemon, and utilities) created by Red Hat to interact with the capabilities of a wide range of Linux virtualization technologies.
While libvirt may be the most common, novacompute also uses the Xen API, vSphere API, Windows Management Interface, and others to support other virtualization technologies.
One of strengths of Nova is its wide support for virtualization technologies.
The virtualization technologies supported in the current release version of Nova are detailed in Table 4-2
Xen Yes libvirt Most popular (along with XCP/XenServer) technology for larger scale and production deployments.
See the OpenStack VMware documentation for full information and restrictions when using this option.
Yes libvirt Generally considered a lower performance virtualization option, UML runs each guest as a regular process in user space.
Yes libvirt LXC is an operating system-level partitioning technology that allows for running multiple isolated servers (containers) in a single kernel.
Instead, it provides a virtual environment with its own process space.
While this doesn’t provide the same level of isolation (as every partition shares the common kernel), it may provide some advantages in I/O performance.
Volume Worker As you can gather by the name, nova-volume manages the creation, attaching, and detaching of persistent volumes to compute instances (similar in functionality to Amazon’s Elastic Block Storage)
It can use volumes from a variety of providers such as iSCSI or AoE.
AoE High performance layer 2 Ethernet technology that encapsulates SATA commands in Ethernet frames.
Supported on Linux through the AoE Tools package, specifically the vblade program.
This is supported by most modern operating systems, but the Nova implementation only currently supports Linux through with this implementation.
Sheepdog An open-source, distributed storage system specifically designed for QEMU/KVM installations that is developed by NTT Laboratories.
As stated on http://ceph.newdream.net/ the wiki, “Ceph is under heavy development, and is not yet suitable for any uses other than benchmarking and review.”
Unlike other providers mentioned above, this provider does not run directly on the SAN hardware.
Network Worker The nova-network worker daemon is very similar to nova-compute and nova-volume.
It accepts networking tasks from the queue and then performs system commands to manipulate the network (such as setting up bridging interfaces or changing iptables rules)
Nova defines two different types of IP addresses for an instance: Fixed IPs and Floating IPs.
These can be broadly thought of as private IPs (fixed) and public IPs (floating)
Fixed IPs are assigned on instance startup and remain the same during their entire lifetimes.
Floating IPs are dynamically allocated and associated to a domain to allow outside connectivity.
To support the assignment and connectivity of fixed IPs, Nova supports three networking managers:
Each new instance is assigned a fixed IP address and attached to a common bridge (which must be created by the administrator)
FlatDHCP builds upon the Flat manager by providing DHCP services to handle instance addressing and creation of bridges.
In this mode, nova-network creates a VLAN, a subnet, and a separate bridge for each project.
Each project also receives a range of IP only accessible within the VLAN.
Of these three network managers, VLAN is the most featured, Flat is the most barebones (but flexible), and FlatDHCP strikes a nice balance between the two.
Queue The queue provides a central hub for passing messages between daemons.
This is currently implemented with RabbitMQ today, but theoretically could be any AMPQ message queue supported by the Python ampqlib and carrot libraries.
Nova creates several types of message queues to facilitate communication between the various daemons.
Topics queues allow messages to be broadcast to the number of particular class of worker daemons.
For example, Nova uses these to pass messages to all (or any) of the compute or volume daemons.
This allows Nova to use the first available worker to process the message.
Host queues allow Nova to send messages to specific services on specific hosts.
For example, Nova often needs to send a message to a specific host’s compute worker to take action on a particular instance.
Fanout queues are only currently used for the advertising of the service capabilities to nova-scheduler workers.
Here is an example of the queues created in RabbitMQ for a simple all-in-one node installation:
As you can see from the example, topic, fanout, and host queues have been created for each service (nova-scheduler, nova-compute, nova-volume, nova-network)
Database The database stores most of the configuration and run-time state for a cloud infrastructure.
This includes the instance types that are available for use, instances in use, networks available, and projects.
Table 4-4 details all the tables in the current Nova database scheme.
The updated_at field is used to determine if a given service is considered healthy or not.
Nova supports a wide range of databases, including popular open-source stalwarts like MySQL and PostgreSQL.
Nova is distributed from several sources in many different packaging formats.
As it is a relatively new project, it lacks the installation ease and universal portability of more established open source projects like Apache Web Server.
As such, it is not easily installable on every operating system distribution without significant administrative configuration.
In this chapter, we will help you decide which version of the OpenStack code base is best suited to your deployment needs and show you how to obtain that version in your preferred packaging.
Nova Versions and Packaging As Nova is a fast-moving and relatively young project, we need to make some decisions about the codebase that we want to use.
What form of packaging do we want to use to deploy Nova?
To adequately answer these questions, you need to ask yourself two tough questions:
How proficient am I with Python development, system administration, and Linux.
How much stability am I willing to sacrifice to get the latest features?
To answer the proficiency question, you will need to honestly examine your skills across not just programming, but also system administration.
If you don’t utilize the packaged versions of Nova, you will need to understand how Python applications are built and their dependent packages.
If you want to use newer, non-production versions of the code, it is possible that you will run into bugs that you will need to troubleshoot or fix yourself.
On the systems administration side, Nova has heavy dependencies of Linux networking and virtualization support.
As stated earlier in the book, it is mostly a control framework for virtual machines, storage, and networks.
To apply its advanced configurations or options, you will need to understand the trade-offs you will be making.
Rules of thumb for classifying skill level could be as follows:
Basic Relatively unskilled in Python programming, but has basic system administration skills in Linux virtualization (specifically KVM) and networking (understanding of iptables and ifconfig)
Proficient Beginner skills in Python (can read code, perhaps written basic Python scripts)
Competent system administration skills, with advanced knowledge of key areas such as virtualization (perhaps deployed Xen or VMware at their company), storage (usage of iSCSI), and networking (understanding of VLANS and advanced switching)
Advanced system administration skills such as writing libvirt templates, defining new iptables or ebtables rules, and administering message queuing software.
As noted earlier in the book, Nova is a rapidly moving project that changes daily.
Having said that, there are still regular releases, as with any normal software project.
For the purposes of this book, we will look at three possible code releases that you might want to deploy: Release.
This is the last “released” version of the codebase and is analogous to a product release.
Released versions of the code are the most tested and polished versions of OS.
Releases are referred to by their version number or their release name.
For example, “2011.2” or “Cactus” was the third release of Nova.
These let leading-edge users familiarize themselves and test upcoming releases.
Milestones are usually fairly stable, but probably only suitable for test and development environments.
Milestones are referred to by the name of the upcoming release and the milestone number (such as “Diablo-3”)
Trunk Trunk refers to the most current version of the source code.
After every update to the official Nova codebase (called a “commit”) from any developer on the project, trunk is updated.
This is the most volatile, least-tested, but most up-to-date release of the code.
On many workdays, the trunk will get updated multiple times.
Trunk is only recommended for users who are actively developing Nova.
Trunk versions of the code are referred to by their Launchpad commit revision number (or “revno”)
Table 5-1 provides some guidance on which version and packaging format you’ll be most successful at deploying depending on your proficiency and desired environment.
Several of the cells within this table have been labelled “N/A” to indicate that these choices are not advised for the complexity of installation and skill level of the installer.
Distributions Several companies are providing Nova as the basis of their distributions.
For most people, this would be the equivalent of choosing Ubuntu or Red Hat Linux distributions instead of compiling their own Linux kernels.
Distributions can provide installers, custom documentation, tested configurations, and support.
For most people, distributions provide the quickest and easiest path from bare metal servers to working Nova deployments.
StackOps StackOps offers “a complete, ready-to-use OpenStack distribution verified, tested and designed to reach as many users as possible thanks to a new and simple installation process.” It is produced by a company of the same name.
It features a “Smart Installer” that creates deployments with default settings in three different modes: single node, dual node, and multi node.
The installer will also let you add more nodes to an existing installation.
With a focus on ease of installation and excellent documentation, this is an ideal choice for those looking to evaluate or test Nova.
You can download or learn more about this distribution at http://www.stackops.com/
In this instance “bare metal” refers to a server without an operating system already installed.
Later in this book, we use StackOps to install and configure a single-system Nova deployment.
Citrix “Project Olympus” Citrix has created “Project Olympus,” which aims to provide a “tested, certified and supported version of OpenStack” along with a “cloud-optimized version of XenServer.” While this is not yet a released project, you can learn more details and sign up for the early access program at http://deliver.citrix.com/projectolympus.
Nova Packages For most competent system administrators, package installation is the preferred method of software installation.
Like products or distributions, packages provide for easy and quick installations.
However, unlike distributions, package installations tend to require more individual pieces (and dependencies) and do not provide assistance with configuration.
On the other hand, they do provide more flexibility in that the administrator can pick and choose the pieces they would like to install.
The sections below outline the methods to obtain packages for many of today’s popular server operating systems.
Launchpad Ubuntu Packages The source of all Nova packages is the code repository at Launchpad.
If you want to closely follow the codebase, you should configure your system to obtain its packages from the PPA repositories at Launchpad.
Personal Package Archives (PPAs) is a Launchpad feature that builds Ubuntu packages from a user’s hosted source code and then distributes them to the public.
OpenStack has taken advantage of this automated feature to produce several different repositories of Nova packages, each containing different snapshots of Nova code.
If you want to run the latest release of Nova (2011.2 or “Cactus”) on Ubuntu, you can use the personal project archives (PPA) at Launchpad.
You can enable packages from the PPAs by executing the following commands:
Replace the 2011.2 part of the add-apt-repository command above with any other OpenStack release designation to use that release.
You can install slightly newer (which might provide additional functionality and bugs) by using development milestones that are released approximately every four weeks.
You can access the current development milestone schedule on the OpenStack Wiki.
For example, the current “Diablo” milestone schedule is located at http://wiki.openstack.org/DiabloReleaseSchedule.
Just as you can get the last release or milestone packages, you can also get the latest trunk code as packages.
This is the same command-line process as the release or milestone packages but specifies the trunk repository:
As stated earlier in this chapter, trunk changes very rapidly and is the least stable of all the packages.
It is prudent to review current bug reports on Launchpad before upgrading your packages on trunk, unless you’re actively trying to reproduce bugs.
These packages are not production quality and should only be used for development.
Ubuntu Distribution Packages Ubuntu recently made the decision to include OpenStack (Nova, Glance, and Swift) as part of the official Ubuntu 11.04 “Natty” release.
However, there are some differences between these packages and the Ubuntu versions.
The packages use libguestfs instead of NBD for qcow2 image support.
Network injection code (configuration of the instances’ networking) was patched.
Complete instructions and links to the development repositories can be found via the RHEL Packaging page on the OpenStack wiki or by going to their build page directly at http://yum.griddynamics.net/
Instructions for installing these RPMs and configuring them with a kickstart script are available at the Mirantis blog.
Microsoft Windows It is unlikely that you will be able to fully install Nova on Microsoft Windows.
Microsoft Windows lacks many of the Nova-supporting Python libraries and is not supported by Nova orchestration features (which are mostly Linux operating system commands)
While Microsoft Windows is not suitable for the core Nova daemons, Microsoft HyperV is supported as a compute host.
For more information on using Hyper-V as a virtualization technology on your compute hosts, be sure to consult the Hyper-V development wiki page.
Source Code Source code gives you access to the rawest, most flexible, and most up-to-date versions of Nova.
However, that flexibility requires the most proficiency on the part of the end user to install, configure, and deploy.
To gather Nova in source code format, you will need a few special tools, some knowledge of Python development, and a bit of patience.
The Nova source code can either be downloaded as a compressed tarball or via the bazaar source control system.
Unless you already happen to develop with bazaar, it is easier to just download as the tarball.
Instructions for installing from source are available on the OpenStack wiki “Install From Source” article.
As Nova supports a wide range of technologies, configurations, and designs, it will be important to make a number of architectural and design decisions before looking to deploy it.
This section guides you through the most important ones before you begin.
However, Nova does have a lot of moving parts, so it’s good to understand an overview of what is trying to be accomplished before you start installing software.
Figure 6-1 illustrates the preferred workflow for installing, configuring, and launching your first instance on Nova.
Planning Nova Deployment to decide on deployment scenario, finalize key design choices, and ensure hardware meets requirements.
Installing Nova to get the software, prerequisites, and configurations onto the servers.
Using Nova to prep the system for your initial users.
As such, it is out of scope of the book to describe everything that you will want to do with your cloud instance once you have it running.
At each phase of the installation, we will make sure to test the results of our actions.
Without these tests, it is very easy to get to the last steps and find out you need to start over again due to error in an early phase.
Virtualization Technology As you can see from the earlier discussion of Nova’s architecture, there are ample choices for virtualization products.
I will not go into all the factors about the appro31
Nova requires all compute hosts within a zone to use a single virtualization technology.
For example, you cannot mix VMware-based compute hosts with KVM-based compute hosts.
They will all need to be one or the other.
It has arguably the best support within Nova (supporting advanced Nova features like live migration) and is easy to get support on, as it is used widely in the community.
However, many people feel that it has greater overhead (especially in I/O) and doesn’t support some high-end virtualization like memory ballooning.
Xen-based solutions, on the other hand, excel at performance and have been used in some of the largest clouds in the world.
It is rumored to power Rackspace’s compute cloud, Amazon’s EC2, and GoGrid’s cloud.
However, this comes at the price of complexity, as they are much more challenging to install, configure, and maintain for people inexperienced with enterprise virtualization products.
The general rule of thumb at the current time is to configure KVM for small or nonproduction deployments but use a Xen-based technology for large-scale production installations.
By default, it will use the local configuration database (the one specified by the --sql_connection flag) for this.
However, the current version of Nova also supports LDAP for authentication.
For more information about setting up these options, consult the nova/auth/ directory in the Nova source.
However, the OpenStack API (especially the 1.1 version) will probably be the more widely implemented version in the long run, as it is an open API not controlled by a single company.
While there is no technical requirement to pick one API over the other, it will be confusing to your users if they need to use both.
Very few users tools or libraries will support both and allow them to switch on an API by API call basis.
As stated earlier, this is a conceptually simple, but vitally important part of your deployment.
The scheduler places instances (virtual servers) onto specific compute hosts.
While this may not be vitally important if you only have one or two compute nodes, it is absolutely critical once you start to.
Most installations will start with the simple scheduler and then write their own scheduler as they grow.
If writing your own scheduler is not feasible for your installation, the next version of Nova (“Diablo”) will feature more choices.
Image Service Images can be a management headache for many installations.
While using Glance along with Swift is the clear choice for larger installations, its management overhead and additional configuration complexity may be too much for smaller deployments.
Even if you choose Glance as your image catalog, you may still need the nova-objectstore on your machine.
For emulation of the Amazon EC2 AMI uploading and bundling semantics, Nova uses the nova-objectstore as a temporary holding space for various pieces of the AMI.
Only once all the pieces have arrived are they assembled and transported into Glance.
Database As stated earlier, Nova uses a Python library called SQL-Alchemy for database abstraction and access.
As such, Nova can theoretically support any database product that SQL-Alchemy supports.
However, in practice, there are only three with any level of testing and support within the Nova community: sqlite3
While sqlite3 is the default database for development and testing, it is unsuitable for production installation due to scalability, availability, and performance concerns.
MySQL MySQL is far and away the most popular database for Nova production deployments.
It is also arguably easiest to setup with Nova, and almost all of the documentation assumes you are using it.
PostgreSQL PostgreSQL is a distant third in usage within the Nova community.
However, there is a dedicated group using it and it does possess many advantageous features for use in large-scale production sites.
Users with strong experience in deploying and tuning PostgreSQL may find this an attractive option.
Assuming that you are looking for a production deployment, the decision for database product should come down to PostgreSQL or MySQL.
Unless you have significant experience with PostgreSQL, MySQL will be a better choice, as all documentation is written with MySQL in mind and there is a larger support community.
Volumes Volume storage design should be treated with particular care, as it is one of the few components of Nova that stores non-ephemeral data.
Most large installations will look at using either the SAN or iSCSI options for this, as it allows them to utilize enterpriseclass hardware and software for greater availability and redundancy.
However, specialized installations (especially high-performance computing or research) may try the less production-ready drivers (RDB or Sheepdog) if their data survivability is not paramount.
With the basics and theory behind us, it is time to get hands-on with Nova and install the code on a server.
In this chapter, we will walk through the installation and configuration of Nova on a single node with both the StackOps distro and Ubuntu packages.
You’ll get a feeling for the complexities of implementing your design choices in actual usage.
While these installations will be only single nodes, they will contain the entire array of OpenStack software and features.
Installing Nova with StackOps As we described back in “StackOps” on page 25, StackOps provides a distro for OpenStack with a bare metal installer.
The bare metal installer automates most of the installation and configuration tasks, leaving very little command line or configuration file editing for the administrator.
It will install an operating system, necessary software packages, and Nova configuration files for us.
Installing StackOps will overwrite any operating system on your server.
It is not intended to overlay Nova onto already installed servers.
Since our purpose is to get some hands-on experience with Nova, we will be installing a single node that runs all the services.
As StackOps is a full distribution, it makes many of the cloud design choices for us.
The single node installation has made the following design choices:
If you have any problems following along with the installation, you can find more detailed documentation at the StackOps Documentation Site.
Check StackOps Requirements StackOps has a very basic set of requirements for a minimal installation as you can see in Table 7-1
While these minimal requirements will get the system installed and running, you will be constrained in the number of virtual machines that you can launch.
At the base 2GB of RAM, you might only be able to launch a single small instance.
As you can see, this minimal configuration should be able to be satisfied by most desktops or servers bought within the last few years.
Of course, the minimal requirements are only useful for a proof of concept or experimental system, but it fits perfectly for our needs.
A more appropriately configured system could be used as a production system.
The baseline for this would be as shown in Table 7-2
Download StackOps The StackOps Distro is available free from their community website at http://www .stackops.org/
It comes in several versions and two formats (CD or USB stick image)
Once you have downloaded the software, burn it to a CD or transfer it to your USB stick (depending on which image you downloaded)
Install StackOps Now that we have our CD or USB stick ready, we will go ahead and install it on our server.
As we decided earlier, we will be installing a single node system (everything running on one server) with basic network and iSCSI volumes.
StackOps makes most of these configuration decisions easy for us with their predefined deployment scenarios.
Dual Node: One cloud controller node (everything but nova-compute) and one.
Multi-Node: A four node plus configuration with dedicated nova-network, novacompute, and nova-volume nodes.
This is a fairly advanced configuration that requires specialized networking.
Advanced Multi-Node: An upcoming configuration that adds monitoring and other options to create a larger scale production installation.
Your first step in installing your StackOps distro is the installation of the operation system, all the necessary prerequisite packages, OpenStack packages, and preconfigured nova components.
In addition, it installs an agent that configures OpenStack for.
When you first boot your system with the CD or USB Stick, you’ll be greeted with the StackOps splash screen (as seen in Figure 7-1) that resembles most Linux distro installations.
After choosing the “Install StackOps Controller Node,” you’ll be led through a number of standard Linux installation screens.
They will ask you about your language and keyboard layout before installing a number of basic components.
After it has completed the basic components installation, it will ask you to configure your network settings.
Enter your IP address, management network IP, netmask, and default gateway address.
It will then try and contact a public NTP (Network Time Protocol) server.
If it fails, it will ask you to specify one manually.
StackOps requires that the nodes have static IP addresses, not DHCP provided ones.
Since I am putting this server on my home network, I have chosen 192.168.1.65 as my server node IP address.
This is out of my home router’s DHCP block so that I won’t.
Once the network has been configured, you will move on to disk partitioning.
To complete this installation and use nova-volume, you need to have one extra empty partition.
This can be an extra hard drive in your machine, an external hard drive, or extra partition.
If you only have only one hard drive (like the machine in this example), you should create an unused partition in this step.
If you don’t, you’ll need an external hard drive to complete the install.
In my example, I simply plugged a 16GB flash drive into the USB port.
After the disk is partitioned and formatted, the base operating system and OpenStack packages will be installed.
When it is finished, pop out the CD or USB stick and reboot the machine.
It should boot to the command prompt, as shown in Figure 7-2
Now that the machine is up and running, let’s test to make sure everything went all right before we move on to configuring our cloud.
Login to the server as the “root” user with the password “stackops” to get a root user prompt.
Check to make sure that the StackOps agent is running by checking its log file:
With the basic distro successfully installed, it is now time to configure the Nova software.
StackOps has an agent-based “Smart Installer” that guides you through the configuration process, gives you intelligent defaults, and then applies the configuration to your newly installed server.
While we are only using it for a simple single-node install here, it will also configure and apply to multiple servers according to their role in the deployment scenario.
The first step in running the Smart Installer is to connect a web browser to the address shown in the banner of your server’s login screen.
Once you connect to that address, you will be redirected to Smart Installer login screen.
Your browser will need to have access to the Internet.
It acts as a middleman between the two, gathering config data from your server and transferring to the configuration web application.
The first screen of the Smart Installer will ask you to create an account and then login.
While creating an account is not a necessity, it will allow you to save, edit, and redeploy your configurations later.
Once registered and logged in, the Smart Installer will step you through a number of screens to configure your Nova deployment.
The first screen is the most important: choosing your deployment scenario (Figure 7-4)
The first of these screens will review your hardware configuration, as shown in Figure 7-5
Once you review the hardware configurations, you can advance to the software requirements screen.
This is also a read-only screen, and after reviewing your server network configuration, you can go on to the next screen.
The next screen shows the configuration options for network topologies.
Since we are using a single-interface test server, we don’t need to change anything (it should be preset to your eth0 interface)
In more advanced deployment scenarios, this screen lets you assign separate service, storage, and public networks.
The next screen is the most important screen of our configuration.
Of all these options, the only one that we must edit is the network section.
Misconfiguring this step will result in your instances not starting or being unable to be reached.
It is fine to use the default 10.0.0.0/8 range as long as this does not conflict with your current network settings.
For this single node installation, this is not relevant, but we will make it smaller.
Enter the range for your floating IPs (public addresses) that are available on your network.
This is the most likely place that your installation will go wrong.
Make sure that you carefully plan and review your entries here.
If you are in doubt, consult your local network administrator.
When you are satisfied with your network options, move on to the compute screen.
All the options on this screen should be fine for our test installation.
The only option that you might want to review is your libvirt type.
This pull-down menu lets you choose between QEMU and KVM virtualization.
Unless you do not have a KVM capable machine, you should leave it on KVM.
The final configuration screen for the Smart Installer is the volume options.
As the screen says, “Choose a device that you are 100% sure is not already in use!” This device will be completely erased.
I usually use a blank USB stick for this step and choose /dev/sdb1 from the pull-down menu.
With the volume configuration done, you are ready to install your configuration to your server, as shown in Figure 7-7
Test StackOps Installation Now that we have finished the installation and configuration, let’s make sure that everything is up and running.
Log in into your Nova server as root with the ’stackops’ password.
Once you’ve logged into the server, add the Nova binaries to your path.
Then check to make sure all the services are up and running with the nova-manage command.
As you can see, we’ve used the nova-manage command with the service list arguments to query the database and see which services are registered, enabled, and running.
If they hadn’t checked in with the database in a while, we would see ‘XXX’ in their listing.
Installing Nova from Packages For this installation, we will use a slightly more powerful machine.
This machine is a workstation-class machine with more RAM and a faster processor.
Please note that we don’t need any more powerful a machine for this installation—I simply would like to run more virtual instances.
We could use the exact same machine as in the StackOps installation section.
Install Base Operating System We will assume that we are starting with a default Ubuntu 10.10 server installation.
The only software packages that we have installed beyond the basics are the “virtual machine host” and “openssh server” options.
Just as with the StackOps installation, we have installed the server with fixed IP addresses and an empty partition for use with nova-volume.
Now that we are sure that virtualization works on our single machine, let’s complete a few more dependencies for Nova.
Install RabbitMQ as our message queue by adding the rabbitmq-server package.
Before installing RabbitMQ, make sure that your hostname is set to your correct IP address in your /etc/hosts file.
This will drag along a number of packages with it, mainly erlang ones.
You can check to make sure this is running with the rabbitmqctl:
Install MySQL as your database with the mysql-server package if it is not already installed on your server:
When it asks for your MySQL password, remember to write down the password, as we will need it later.
With the database server installed, let’s create the Nova database.
With the database created, we now need to make an account for the user nova.
We’ll just use some quick SQL statements to grant privileges and set the password before we login to make sure we did it correctly.
This is free software, and you are welcome to modify and redistribute it under the GPL v2 license.
The first three are necessary Python libraries and the last one will give us DHCP and other services for nova-network.
The final prerequisite is installing and configuring iSCSI for nova-volume.
This is a twostep process: creating the volume group with Linux Volume Manager (LVM) and starting iSCSI services.
To create the volume group, use the following commands (we are using /dev/sde1 for this simple example):
Your volume group needs to be called nova-volume for nova-volume to recognize it.
With these final steps complete, we are done with installing and configuring the Nova prerequisites.
Now let’s move onto installing the Nova packages, which will drag a few more dependencies with it.
In addition, the package installation scripts have added a system user “nova” and added it to the appropriate groups.
Chapter 9 explains the use of nova-manage in much greater detail.
Since we are seeing all smiley faces, it looks like Nova is installed correctly.
Now that we have Nova on the server, let’s install Glance as our image service: $ sudo apt-get install glance python-glance-doc.
This installation will use the default configuration, which uses sqlite3 for the database.
First, let’s configure it to use our MySQL database that we created earlier.
To do this, just add the following flag to the /etc/nova.conf config file:
It also optionally includes the mysql port number after the hostname or server IP address.
We will also set our installation to use KVM for virtualization.
While this is the default, it is a good practice to explicitly set it in the /etc/nova.conf file.
Now let’s set Nova to use the Glance installation by adding these two options to /etc/ nova.conf:
With that completed, we’ll move on to configuring the networking options.
First up is adding the network manager related options to /etc/nova.conf:
Finally, we will create the floating addresses using a range that will not overlap with the StackOps installation.
With all the /etc/nova.conf changes made, we can create the database schema by running the nova-manage db commands.
First, we will sync the database (create the schema), then we will version the database to make sure that the schema creation succeeded.
All that is left to do now is restart the Nova daemons to pick the new configuration options.
Now that we have a working Nova installation, we need to ready it for use by our initial users.
This requires us to do some command-line configuration on the Nova controller (or the server with the database)
First, we will add a user, then upload a virtual disk image, launch the instance, and finally configure network access for it.
Creating User and Projects The first step is using our new Nova installation is to create a user.
This is a multi-step process that uses the nova-manage utility to create a project.
The final command in the example will produce a zip-compressed file called nova.zip.
Now uncompress the credential zip file and source the resulting novarc.
This will set a number of environmental variables needed to access your Nova installation with other utilities.
If you are creating this user on behalf of another user, you will need to give him this zipfile.
While not necessary, you might want to view novarc to find out what environmental variables it is setting for you.
Remember that you will need to source this file in every session if you want to access your Nova deployment.
You might want to add it to your shell profile to have it automatically sourced on login.
Uploading Images Before we can launch instances, we need to upload a virtual disk image into Nova.
There are a number of different images that you can use with your Nova installation.
For the purposes of our test server setup, let’s use Ubuntu’s Enterprise Cloud images.
We’ll download the newest Ubuntu Server image for use on our cloud.
Once we have that image, we need to upload it into the Nova image store.
How we do this varies depending on the image store that we’ve chosen.
To use this utility, give it the compressed image and container/bucket name (I’ve used images as my bucket name for this example)
Now that we have images, let’s make sure that Nova knows the image is available and ready to use.
Make sure you’ve sourced your user credentials before executing this command.
We can also check for images through the OpenStack API with the nova commandline tool.
Launching Instances Now that we have a valid user and virtual disk image, we are ready to launch our first Nova instance.
But before we start spinning up instances, we need to make a keypair so that we will be able to log in to the new instance via ssh.
Now that we have the key, let’s launch an instance.
If we wait a few minutes and everything goes well, it should progress to the 'running' state.
We can also use the nova utility to make the same query through the OpenStack API.
With this tool, you are looking for the 'ACTIVE' status:
If your instance never makes it to 'ACTIVE' status or 'running' state after 10 minutes, something has mostly likely gone awry.
Check the your logs (in this order) for nova-api, nova-compute, nova-network, and then nova-scheduler.
Configuring Network Connectivity With the instance up and running, we now need to configure network access to it.
This is a two-step process: first, we need to permit traffic to the instance, and then we need to associate a public IP address to it.
Without configuring network access to it, you won’t be able to access it from outside of the Nova.
In this example, we will permit SSH (TCP port 22) and ICMP traffic to our instance from any IP address.
Realize that we’ve only permitted basic access to the instance.
If you want to communicate with the server beyond ping and SSH, you’ll need to authorize additional ports.
With traffic permitted to the instance, we now need to assign a public address to it.
This is also a two-step process: allocate an address and then associate it to our instance.
Public addressing operates differently on Nova than on Amazon EC2
EC2 automatically assigns each instance a public and private address (floating and fixed addresses in Nova)
Nova automatically assigns a private address but requires manual allocation of a public address.
Accessing Instances With network access completely configured, we are finally able to log in to our instance.
Our image has been set up with SSH access secured with our keypair.
To tune the system to your needs, you can choose to install one or more predefined collections of software by running the following command:
Congratulations! You have installed your cloud, configured it for use, and launched your first virtual machine.
At this point, you can deploy your applications and use it as any other server.
But before we finish, let’s do a few more things.
This is a straightforward euca-create-volume with the arguments -s 1 (for the size of the volume in gigabytes) and -z nova (which is nova as the default) for the zone.
It will return with the volume name and its status (creating)
Volumes are only currently supported with the Amazon EC2 API.
As such, you will need to use euca2ools if you want to use volumes with your instances.
Once the volume has been created, attach it to a running instance with the euca-attachvolume.
As with any raw device, you will need to make a filesystem on it and then mount it.
Volumes may only be attached to one instance at a time.
Once you are done using the volume, you can umount the volume as any other.
Now that the volume is no longer mounted on our instance, we can safely detach it with the euca-detach-volume.
Finally, you can completely destroy the detached volume with the euca-delete-volume.
This will take a while, as the volume will be completely zeroed out to prevent other users from seeing this data.
Simply shutting down or powering off an instance does not terminate it in Nova.
This is different behavior than you may be used to from Amazon EC2
Nova has a myriad of configuration options due to its wide support of differing technologies, products, and architectures.
This section gives you an overview of the most important configuration options, as well as important administrative commands to bend Nova to your will.
Configuration Files Nova daemons are given configuration options on startup through a set of flags usually set in text file.
However, these flags can also be set directly on the command line or in an alternate configuration file that is designated at run time.
To pass arbitrary flags on the command line, simply include them and they will override the values in the configuration file.
The /etc/nova.conf file is a very simple format: put each flag on a separate line, with no comments or other characters.
One of the weaknesses of Nova is that the /etc/nova.conf does not not support comments.
As such, it does not include any helpful configuration comments that you might see in other open source packages.
Please note that this last sentence said “most complete,” not “definitive.” The definitive source of all configuration flags is the Nova source code.
Configuration Tools Nova administration is accomplished through a tool called nova-manage.
Most commands take the form nova-manage command subcommand and any necessary arguments.
At any time, you can see help for nova-manage by leaving off any arguments, subcommands, or commands.
Here is an example of finding help for creating a new user:
Do not worry about the error after the nova-manage user create—it is simply telling you that you haven’t supplied the necessary arguments.
Service Services can be monitored through the nova-manage command on a service or host basis.
With the service, you can either view or actively manage services.
For example, you can query a host for the services that it currently offers, or simply list all the services that are available.
This is an essential command for testing or troubleshooting your deployment.
Below is an example that walks through the full array of of service subcommands: listing services, enabling and disabling services, and describing resources on a host.
Quotas Nova can apply quotas on number of instances, total cores, total volumes, volume size, and other items on a per-project basis.
Table 9-1 illustrates all quota options, their default values, and a brief description.
These default values for all projects are set in the source code (nova/quota.py) but can be overridden for all projects or individual projects.
To override the default value for all projects, simply add the appropriate flag with a new value to the /etc/nova.conf file.
For example, to change the total cores available to each project, append this line to the /etc/nova.conf file:
It is also possible to adjust quotas on particular projects with the nova-manage command.
To increase the total cores allotted to a mythical “payroll” project, execute the following command:
As you may have noticed, the flags for quotas (quota_cores) are different from the nova-manage command keys (cores)
Using the flag in novamanage or the nova-manage keys in /etc/nova.conf will have no effect.
As you can see from the command listing above, we specified the project (“payroll”), then the quota key (“cores”), and finally the new value.
Executing nova-manage project quota payroll without a key and value will print out a list of the current values for all quotas.
Database The nova-manage db command is rarely used except for troubleshooting and upgrades.
The sync subcommand will upgrade the database scheme for new versions of Nova and the version will report the current version.
Nova uses a database abstraction library called SQL-Alchemy to interact with its database.
A complimentary package called sqlalchemymigrate is used to manage the database schema.
Inspired by Ruby on Rails’ migrations feature, it provides a programmatic way to handle database schema changes.
For Nova administrators, this only applies when they are upgrading versions.
This should be rarely used unless you are installing from source or upgrading your installation.
If there are pending scheme migrations, it will apply those to your database.
Instance Types and Flavors Instance types (or “flavors,” as the OpenStack API calls them) are resources granted to instances in Nova.
At the current time, instance type manipulation isn’t exposed through the APIs nor the adminclient.
You can use the flavor command as a synonym for instance_types in any of these examples.
During installation, Nova creates five instance types that mirror the basic Amazon EC2 instance types.
To see all currently active instance types, use the list subcommand:
Again, and just for emphasis, you could just as easily have used the flavor subcommand to get the exact same output:
To create an instance type, use the create subcommand with the following positional arguments:
Note that the delete command only marks the instance type as inactive in the database; it does not actually remove the instance type.
This is done to preserve the instance type definition for long running instances (which may not terminate for months or years)
If you are sure that you want to delete this instance type from the database, pass the -purge flag after the name:
Be careful with deleting instance types, as you might need this information later.
This is especially true in commercial or enterprise environments where you might be creating a bill based off the instance type’s name or configuration.
Unless you truly need to prune the size of your instance_types table, you are much safer to just delete the instance type.
Virtual Machine Nova also lets you query all the current running virtual machines, similar to how the OpenStack API or EC2 API does with their tools.
There is a bug in nova-manage vm list in Cactus where it cannot properly decipher the instance type (the type field above)
This is corrected in the upcoming version of the Nova.
Live migration allows you to move virtual machine instances between hosts if the following conditions are met:
Network Nova has a trio of nova-manage networking commands: network, fixed, and floating.
It allows you to list, create, and delete networks within the Nova database.
The fixed command simply allows you to view the fixed IP address mappings to hostname, host, and MAC address.
Here are the truncated results of the command (it goes on to show the every IP address in the mapping):
The floating command is very similar to the fixed command except that it manipulates public IP addresses.
The example below creates a floating range and then shows their allocation.
Shell As purely a troubleshooting command, nova-manage shell allows you to start up a Nova environment so that you can issue ad hoc Python commands.
While I have used the basic shell in this example, you can also invoke the bpython or ipython shells.
Volumes The volume command for nova-manage should only be used when traditional methods have failed.
While both subcommands are fairly self-explanatory, the situations where they are applicable may not be.
The delete subcommand should only be used when traditional methods of removing it has failed.
As an example, we will delete a volume that has been marked in the “error” state:
This subcommand will not let you delete a volume that is marked with the status “in-use” (which would mean that it is attached to an instance)
You will need to detach the volume from the instance before trying this subcommand.
The reattach command allows you to reconnect a volume to an instance.
Most likely, this will only need to be used after a compute host has been rebooted.
About the Author Ken Pepple currently serves as the Director of Cloud Development at Internap, where he leads the engineering of their OpenStack-based cloud service.
You can contact Ken and see his current work at his blog: http://ken.pepple.info.
