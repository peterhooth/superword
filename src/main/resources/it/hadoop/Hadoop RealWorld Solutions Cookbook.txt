Realistic, simple code examples to solve problems at scale with Hadoop and related technologies.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the authors, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Owens has a background in Java and C++, and has worked in both private and public sectors as a software engineer.
Most recently, he has been working with Hadoop and related distributed processing technologies.
Currently, he works for comScore, Inc., a widely regarded digital measurement and analytics company.
At comScore, he is a member of the core processing team, which uses Hadoop and other custom distributed systems to aggregate, analyze, and manage over 40 billion transactions per day.
I would like to thank my parents James and Patricia Owens, for their support and introducing me to technology at a young age.
Jon Lentz is a Software Engineer on the core processing team at comScore, Inc., an online audience measurement and analytics company.
He prefers to do most of his coding in Pig.
Before working at comScore, he developed software to optimize supply chains and allocate fixed-income securities.
To my daughter, Emma, born during the writing of this book.
He has worked for the commercial sector in the past, but the majority of his experience comes from the government contracting space.
He currently works for Potomac Fusion in the DC/Virginia area, where they develop scalable algorithms to study and enhance some of the most advanced and complex datasets in the government space.
Within Potomac Fusion, he has taught courses and conducted training sessions to help teach Apache Hadoop and related cloud-scale technologies.
I'd like to thank my co-authors for their patience and hard work building the code you see in this book.
Also, my various colleagues at Potomac Fusion, whose talent and passion for building cutting-edge capability and promoting knowledge transfer have inspired me.
Cody is an author, speaker, and industry expert in data warehousing, Oracle Business Intelligence, and Hyperion EPM implementations.
He has consulted to both commercial and federal government clients throughout his career, and is currently managing large-scale EPM, BI, and data warehouse implementations.
I would like to commend the authors of this book for a job well done, and would like to thank Packt Publishing for the opportunity to assist in the editing of this publication.
He has worked in peace and conflict zones to showcase the hidden dynamics and anomalies in the underlying "Big Data", with clients such as ACSIM, DARPA, and various federal agencies.
His current interests include merging distributed artificial intelligence techniques with adaptive heterogeneous cloud computing.
I'd like to thank my beautiful wife Wendy, and my twin sons Christopher and Jonathan, for their love and patience while I research and review.
I owe a great deal to Brian Femiano, Bruce Miller, and Jonathan Larson for allowing me to be exposed to many great ideas, points of view, and zealous inspiration.
His non-work interests include functional programming in languages like Haskell and Lisp dialects, and their application to real-world problems.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Why Subscribe? f Fully searchable across every book published by Packt.
Preface Hadoop Real-World Solutions Cookbook helps developers become more comfortable with, and proficient at solving problems in, the Hadoop space.
Readers will become more familiar with a wide variety of Hadoop-related tools and best practices for implementation.
Each chapter contains a set of recipes that pose, and then solve, technical challenges and that can be completed in any order.
A recipe breaks a single problem down into discrete steps that are easy to follow.
This book covers unloading/loading to and from HDFS, graph analytics with Giraph, batch data analysis using Hive, Pig, and MapReduce, machine-learning approaches with Mahout, debugging and troubleshooting MapReduce jobs, and columnar storage and retrieval of structured data using Apache Accumulo.
This book will give readers the examples they need to apply the Hadoop technology to their own problems.
What this book covers Chapter 1, Hadoop Distributed File System – Importing and Exporting Data, shows several approaches for loading and unloading data from several popular databases that include MySQL, MongoDB, Greenplum, and MS SQL Server, among others, with the aid of tools such as Pig, Flume, and Sqoop.
Chapter 2, HDFS, includes recipes for reading and writing data to/from HDFS.
It shows how to use different serialization libraries, including Avro, Thrift, and Protocol Buffers.
Also covered is how to set the block size and replication, and enable LZO compression.
Chapter 3, Extracting and Transforming Data, includes recipes that show basic Hadoop ETL over several different types of data sources.
Different tools, including Hive, Pig, and the Java MapReduce API, are used to batch-process data samples and produce one or more transformed outputs.
Chapter 4, Performing Common Tasks Using Hive, Pig, and MapReduce, focuses on how to leverage certain functionality in these tools to quickly tackle many different classes of problems.
This includes string concatenation, external table mapping, simple table joins, custom functions, and dependency distribution across the cluster.
Chapter 5, Advanced Joins, contains recipes that demonstrate more complex and useful join techniques in MapReduce, Hive, and Pig.
These recipes show merged, replicated, and skewed joins in Pig as well as Hive map-side and full outer joins.
There is also a recipe that shows how to use Redis to join data from an external data store.
Chapter 6, Big Data Analysis, contains recipes designed to show how you can put Hadoop to use to answer different questions about your data.
Several of the Hive examples will demonstrate how to properly implement and use a custom function (UDF) for reuse in different analytics.
There are two Pig recipes that show different analytics with the Audioscrobbler dataset and one MapReduce Java API recipe that shows Combiners.
Chapter 7, Advanced Big Data Analysis, shows recipes in Apache Giraph and Mahout that tackle different types of graph analytics and machine-learning challenges.
Chapter 8, Debugging, includes recipes designed to aid in the troubleshooting and testing of MapReduce jobs.
There are examples that use MRUnit and local mode for ease of testing.
There are also recipes that emphasize the importance of using counters and updating task status to help monitor the MapReduce job.
Chapter 9, System Administration, focuses mainly on how to performance-tune and optimize the different settings available in Hadoop.
Several different topics are covered, including basic setup, XML configuration tuning, troubleshooting bad data nodes, handling NameNode failure, and performance monitoring using Ganglia.
Chapter 10, Persistence Using Apache Accumulo, contains recipes that show off many of the unique features and capabilities that come with using the NoSQL datastore Apache Accumulo.
The recipes leverage many of its unique features, including iterators, combiners, scan authorizations, and constraints.
There are also examples for building an efficient geospatial row key and performing batch analysis using MapReduce.
What you need for this book Readers will need access to a pseudo-distributed (single machine) or fully-distributed (multi-machine) cluster to execute the code in this book.
The various tools that the recipes leverage need to be installed and properly configured on the cluster.
Moreover, the code recipes throughout this book are written in different languages; therefore, it’s best if readers have access to a machine with development tools they are comfortable using.
Who this book is for This book uses concise code examples to highlight different types of real-world problems you can solve with Hadoop.
It is designed for developers with varying levels of comfort using Hadoop and related tools.
Hadoop beginners can use the recipes to accelerate the learning curve and see real-world examples of Hadoop application.
For more experienced Hadoop developers, many of the tools and techniques might expose them to new ways of thinking or help clarify a framework they had heard of but the value of which they had not really understood.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: “All of the Hadoop filesystem shell commands take the general form hadoop fs –COMMAND.”
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: “To build the JAR file, download the Jython java installer, run the installer, and select Standalone from the installation menu”
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Introduction In a typical installation, Hadoop is the heart of a complex flow of data.
Next, some form of processing takes place using MapReduce or one of the several languages built on top of MapReduce (Hive, Pig, Cascading, and so on)
Finally, the filtered, transformed, and aggregated results are exported to one or more external systems.
For a more concrete example, a large website may want to produce basic analytical data about its hits.
Weblog data from several servers is collected and pushed into HDFS.
A MapReduce job is started, which runs using the weblogs as its input.
The weblog data is parsed, summarized, and combined with the IP address geolocation data.
The output produced shows the URL, page views, and location data by each cookie.
Ad hoc queries can now be run against this data.
Analysts can quickly produce reports of total unique cookies present, pages with the most views, breakdowns of visitors by region, or any other rollup of this data.
The recipes in this chapter will focus on the process of importing and exporting data to and from HDFS.
The sources and destinations include the local filesystem, relational databases, NoSQL databases, distributed databases, and other Hadoop clusters.
Importing and exporting data into HDFS using Hadoop shell commands.
These commands are built on top of the HDFS FileSystem API.
Hadoop comes with a shell script that drives all interaction from the command line.
All of the Hadoop filesystem shell commands take the general form hadoop fs -COMMAND.
To get a full listing of the filesystem commands, run the hadoop shell script passing it the fs option with no commands.
These command names along with their functionality closely resemble Unix shell commands.
To get more information about a particular command, use the help option.
The shell commands and brief descriptions can also be found online in the official documentation located at http://hadoop.apache.org/common/docs/r0.20.2/hdfs_ shell.html.
In this recipe, we will be using Hadoop shell commands to import data into HDFS and export data from HDFS.
These commands are often used to load ad hoc data, download processed data, maintain the filesystem, and view the contents of folders.
Knowing these commands is a requirement for efficiently working with HDFS.
Complete the following steps to create a new folder in HDFS and copy the weblog_entries.
The result of a job run in Hadoop may be used by an external system, may require further processing in a legacy system, or the processing requirements might not fit the MapReduce paradigm.
Any one of these situations will require data to be exported from HDFS.
One of the simplest ways to download data from HDFS is to use the Hadoop shell.
When copying a file from HDFS to the local filesystem, keep in mind the space available on the local filesystem and the network connection speed.
It's not uncommon for HDFS to have file sizes in the range of terabytes or even tens of terabytes.
Downloading the example code for this book You can download the example code files for all the Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.
The Hadoop shell commands are a convenient wrapper around the HDFS FileSystem API.
The same applies to copying the data from HDFS to the local filesystem.
The copyFromLocal command takes the general form of hadoop fs –copyFromLocal LOCAL_FILE_PATH URI.
If the URI is not explicitly given, a default is used.
The copyToLocal command takes the general form of hadoop fs –copyToLocal [-ignorecrc] [-crc] URI LOCAL_FILE_PATH.
If the URI is not explicitly given, a default is used.
The copyToLocal command does a Cyclic Redundancy Check (CRC) to verify that the data copied was unchanged.
A failed copy can be forced using the optional –ignorecrc argument.
The file and its CRC can be copied using the optional –crc argument.
Although put is slightly more general, it is able to copy multiple files into HDFS, and also can read input from stdin.
The get Hadoop shell command can be used in place of the copyToLocal command.
When working with large datasets, the output of a job will be partitioned into one or more parts.
There will be one part file for each reducer task.
The number of reducers that should be used varies from job to job; therefore, this property should be set at the job and not the cluster level.
This means that the output from all map tasks will be sent to a single reducer.
Unless the cumulative output from the map tasks is relatively small, less than a gigabyte, the default value should not be used.
Setting the optimal number of reduce tasks can be more of an art than science.
In the JobConf documentation it is recommended that one of the two formulae be used:
For example, if your cluster has 10 nodes running a task tracker and the mapred.
Since the number of reduce slots must be a nonnegative integer, this value should be rounded or trimmed.
With 0.95 all of the reducers can launch immediately and start transferring map outputs as the maps finish.
With 1.75 the faster nodes will finish their first round of reduces and launch a second wave of reduces doing a much better job of load balancing.
The partitioned output can be referenced within HDFS using the folder name.
A job given the folder name will read each part file when processing.
The problem is that the get and copyToLocal commands only work on files.
It would be cumbersome and inefficient to copy each part file (there could be hundreds or even thousands of them) and merge them locally.
Fortunately, the Hadoop shell provides the getmerge command to merge all of the distributed part files into a single output file and copy that file to the local filesystem.
The Pig script can be executed from the command line by running the following command:
Since there are four reduce tasks that will be run as part of this job, we expect four part files to be created.
The getmerge command can be used to merge all four of the part files and then copy the singled merged file to the local filesystem as shown in the following command line:
See also f The Reading and writing data to HDFS recipe in Chapter 2, HDFS shows how to use.
Hadoop Distributed Copy (distcp) is a tool for efficiently copying large amounts of data within or in between clusters.
The benefits of using MapReduce include parallelism, error handling, recovery, logging, and reporting.
The Hadoop Distributed Copy command (distcp) is useful when moving data between development, research, and production cluster environments.
Getting ready The source and destination clusters must be able to reach each other.
The source cluster should have speculative execution turned off for map tasks.
This will prevent any undefined behavior from occurring in the case where a map task fails.
The source and destination cluster must use the same RPC protocol.
Typically, this means that the source and destination cluster should have the same version of Hadoop installed.
Complete the following steps to copy a folder from one cluster to another:
Synchronize the weblogs folder between cluster A and cluster B:
On the source cluster, the contents of the folder being copied are treated as a large temporary file.
A map-only MapReduce job is created, which will do the copying between clusters.
By default, each mapper will be given a 256-MB block of the temporary file.
While copying between two clusters that are running different versions of Hadoop, it is generally recommended to use HftpFileSystem as the source.
The distcp command has to be run from the destination server:
Sqoop is an Apache project that is part of the broader Hadoop ecosphere.
In many ways Sqoop is similar to distcp (See the Moving data efficiently between clusters using Distributed Copy recipe of this chapter)
Both are built on top of MapReduce and take advantage of its parallelism and fault tolerance.
Instead of moving data between clusters, Sqoop was designed to move data from and into relational databases using a JDBC driver to connect.
This recipe will show how to use Sqoop to import data from MySQL to HDFS using the weblog entries as an example.
If you are using CDH3, you already have Sqoop installed.
This recipe assumes that you have a MySQL instance up and running that can reach your Hadoop cluster.
The mysql.user table is configured to accept a user connecting from the machine where you will be running Sqoop.
The MySQL JDBC driver JAR file has been copied to $SQOOP_HOME/libs.
Complete the following steps to transfer data from a MySQL table to an HDFS file:
Create a new database in the MySQL instance: CREATE DATABASE logs;
The --username and --password options are used to authenticate the user issuing the command against the MySQL instance.
The mysql.user table must have an entry for the --username option and the host of each node in the Hadoop cluster; or else Sqoop will throw an exception indicating that the host is not allowed to connect to the MySQL Server.
In this example, we connected to the MySQL server using hdp_usr.
In our case, we are looking to import the weblogs table into HDFS.
The --target-dir argument is passed the folder path in HDFS where the imported table will be stored:
By default, the imported data will be split on the primary key.
If the table being imported does not have a primary key, the -m or --split-by arguments must be used to tell Sqoop how to split the data.
The -m argument controls the number of mappers that are used to import the data.
Since -m was set to 1, a single mapper was used to import the data.
Sqoop uses the metadata stored by the database to generate the DBWritable classes for each column.
These classes are used by DBInputFormat, a Hadoop input format with the ability to read the results of arbitrary queries run against a database.
In the preceding example, a MapReduce job is started using the DBInputFormat class to retrieve the contents from the weblogs table.
There are many useful options for configuring how Sqoop imports data.
Sqoop can import data as Avro or Sequence files using the --as-avrodatafile and --as-sequencefile arguments respectively.
The data can be compressed while being imported as well using the -z or --compress arguments.
This argument instructs Sqoop to use native import/export tools if they are supported by the configured database.
In the preceding example, if --direct was added as an argument, Sqoop would use mysqldump for fast exporting of the weblogs table.
The --direct argument is so important that in the preceding example, a warning message was logged as follows:
Sqoop is an Apache project that is part of the broader Hadoop ecosphere.
In many ways Sqoop is similar to distcp (See the Moving data efficiently between clusters using Distributed Copy recipe of this chapter)
Both are built on top of MapReduce and take advantage of its parallelism and fault tolerance.
Instead of moving data between clusters, Sqoop was designed to move data from and into relational databases using a JDBC driver to connect.
This recipe will show how to use Sqoop to export data from HDFS to MySQL using the weblog entries as an example.
If you are using CDH3, you already have Sqoop installed.
This recipe assumes that you have a MySQL instance up and running that can reach your Hadoop cluster.
The mysql.user table is configured to accept a user connecting from the machine where you will be running Sqoop.
The MySQL JDBC driver JAR file has been copied to $SQOOP_HOME/libs.
Complete the following steps to transfer data from HDFS to a MySQL table:
Create a new database in the MySQL instance: CREATE DATABASE logs;
The --username and --password options are used to authenticate the user issuing the command against the MySQL instance.
The mysql.user table must have an entry for the --username and the host of each node in the Hadoop cluster; or else Sqoop will throw an exception indicating that the host is not allowed to connect to the MySQL Server.
In this example, we connected to the MySQL server using hdp_usr.
The --table argument identifies the MySQL table that will receive the data from HDFS.
This table must be created before running the Sqoop export command.
Sqoop uses the metadata of the table, the number of columns, and their types, to validate the data coming from the HDFS folder and to create INSERT statements.
If the --update-key argument is specified, UPDATE statements will be created instead.
If the preceding example had used the argument --update-key md5, the generated code would have run like the following:
In the case where the --update-key value is not found, setting the --update-mode to allowinsert will insert the row.
The -m argument sets the number of map jobs reading the file splits from HDFS.
Each mapper will have its own connection to the MySQL Server.
It is possible that a map task failure could cause data inconsistency resulting in possible insert collisions or duplicated data.
These issues can be overcome with the use of the --staging-table argument.
This will cause the job to insert into a staging table, and then in one transaction, move the data from the staging table to the table specified by the --table argument.
The --staging-table argument must have the same format as --table.
This will allow data to be efficiently loaded from a Microsoft SQL Server database into HDFS.
If you are using CDH3, you already have Sqoop installed.
This recipe assumes that you have an instance of SQL Server up and running that can connect to your Hadoop cluster.
Complete the following steps to configure Sqoop to connect with Microsoft SQL Server:
This will result in a new folder being created, sqljdbc_3.0
Sqoop now has access to the sqljdbc4.jar file and will be able to use it to connect to a SQL Server instance.
For importing and exporting data, see the Importing data from MySQL into HDFS using Sqoop and Exporting data from HDFS into MySQL using Sqoop recipes of this chapter.
In order for SQL Server to have full compatibility with Sqoop, some configuration changes are necessary.
Exporting data from HDFS into MongoDB This recipe will use the MongoOutputFormat class to load data from an HDFS instance into a MongoDB collection.
Getting ready The easiest way to get started with the Mongo Hadoop Adaptor is to clone the Mongo-Hadoop project from GitHub and build the project configured for a specific version of Hadoop.
A Git client must be installed to clone this project.
This recipe assumes that you are using the CDH3 distribution of Hadoop.
This project needs to be built for a specific version of Hadoop.
The resulting JAR file must be installed on each node in the $HADOOP_HOME/lib folder.
The Mongo Java Driver is required to be installed on each node in the $HADOOP_HOME/ lib folder.
Complete the following steps to copy data form HDFS into MongoDB:
In the folder that mongo-hadoop was cloned to, open the build.sbt file with a text editor.
Verify that the weblogs MongoDB collection was populated from the Mongo shell:
These abstractions make working with MongoDB similar to working with any Hadoop compatible filesystem.
Importing data from MongoDB into HDFS This recipe will use the MongoInputFormat class to load data from a MongoDB collection into HDFS.
Getting ready The easiest way to get started with the Mongo Hadoop Adaptor is to clone the mongo-hadoop project from GitHub and build the project configured for a specific version of Hadoop.
A Git client must be installed to clone this project.
This recipe assumes that you are using the CDH3 distribution of Hadoop.
This project needs to be built for a specific version of Hadoop.
The resulting JAR file must be installed on each node in the $HADOOP_HOME/lib folder.
The Mongo Java Driver is required to be installed on each node in the $HADOOP_HOME/ lib folder.
Complete the following steps to copy data from MongoDB into HDFS:
In the folder that mongo-hadoop was cloned to, open the build.sbt file with a text editor.
Create a Java MapReduce program that will read the weblogs file from a MongoDB collection and write them to HDFS: import java.io.*;
This map-only job uses several classes provided by the Mongo Hadoop Adaptor.
Data that is read in from HDFS is converted to a BSONObject.
MongoDB uses these BSON objects to efficiently serialize, transfer, and store data.
The Mongo Hadoop Adaptor also provides a convenient MongoConfigUtil class to help set up the job to connect to MongoDB as if it were a filesystem.
These abstractions make working with MongoDB similar to working with any Hadoop compatible filesystem.
MongoDB is a NoSQL database that was designed for storing and retrieving large amounts of data.
This data must be cleaned and formatted before it can be made available.
Apache Pig was designed, in part, with this kind of work in mind.
The MongoStorage class makes it extremely convenient to bulk process the data in HDFS using Pig and then load this data directly into MongoDB.
This recipe will use the MongoStorage class to store data from HDFS into a MongoDB collection.
Getting ready The easiest way to get started with the Mongo Hadoop Adaptor is to clone the mongo-hadoop project from GitHub and build the project configured for a specific version of Hadoop.
A Git client must be installed to clone this project.
This recipe assumes that you are using the CDH3 distribution of Hadoop.
This project needs to be built for a specific version of Hadoop.
The resulting JAR file must be installed on each node in the $HADOOP_HOME/lib folder.
The Mongo Java Driver is required to be installed on each node in the $HADOOP_HOME/ lib folder.
Complete the following steps to copy data from HDFS to MongoDB:
In the folder that mongo-hadoop was cloned to, open the build.sbt file with a text editor.
These abstractions make working with MongoDB similar to working with any Hadoop compatible filesystem.
Using HDFS in a Greenplum external table Greenplum is a parallel database that distributes data and queries to one or more PostgreSQL instances.
It complements Hadoop by providing real-time or near real-time access to large amounts of data.
External tables are a good solution for working with data that lives outside of the Greenplum cluster.
Since data in external tables must first travel over the network, they should be infrequently used in queries with other data that lives inside of the Greenplum cluster.
This recipe will cover creating read-only and read/write external tables.
Getting ready This recipe assumes that you are using the CDH3 distribution of Hadoop.
Run an instance of Greenplum that must be able to reach the Hadoop cluster found at http://www.greenplum.com/products/greenplum-database.
Java 1.6 or above must be installed on each node in the Greenplum cluster.
Create an external table from the weblogs file in HDFS:
Greenplum has native support for loading data from HDFS in parallel.
Using Flume to load data into HDFS Apache Flume is a project in the Hadoop community, consisting of related projects designed to efficiently and reliably load streaming data from many different sources into HDFS.
A common use case for Flume is loading the weblog data from several sources into HDFS.
This recipe will cover loading the weblog entries into HDFS using Flume.
Getting ready This recipe assumes that you have Flume installed and configured.
Flume can be downloaded from its Apache page at http://incubator.apache.org/ flume/
Complete the following steps to load the weblogs data into HDFS:
Flume uses Sources and Sinks abstractions and a pipe-like data flow to link them together.
In the example, text is a source which takes a path to a file as an argument and sends the contents of that file to the configured sink.
In step 2, the Flume shell is used to configure and execute a job.
The -c argument tells Flume where to connect to the Flume Master node.
As mentioned previously, text is a source which reads all of the contents of the file it is passed.
Introduction Hadoop Distributed File System (HDFS) is a fault-tolerant distributed filesystem designed to run on "off-the-shelf" hardware.
It has been optimized for streaming reads on large files whereas I/O throughput is favored over low latency.
In addition, HDFS uses a simple model for data consistency where files can only be written to once.
For example, the default block size for HDFS is 64 MB.
Once a file has been placed into HDFS, the file is divided into one or more data blocks and is distributed to nodes in the cluster.
In addition, copies of the data blocks are made, which again are distributed to nodes in the cluster to ensure high data availability in case of a disk failure.
The number of copies HDFS makes of each data block is determined by the replication factor setting.
The default replication factor is 3, meaning three replicas of a data block will be distributed across the nodes in the cluster.
Finally, applications using HDFS can achieve high throughput because the Hadoop framework was designed to move computation to the data.
In other words, applications can run on the nodes where the data resides instead of moving the data to the application.
During the synchronizing process, the Secondary NameNode retrieves the current NameNode image and edit logs, merges them together, and then sends the merged image back to the NameNode.
The Secondary NameNode is not a "hot" backup of the NameNode.
It cannot be used in the event of a NameNode failure.
DataNode This manages the data blocks it receives from the NameNode.
It is unaware of any other DataNodes in the cluster and only communicates with the NameNode.
This chapter will use the FileSystem API, MapReduce, and advanced serialization libraries to efficiently write and store data in HDFS.
Reading and writing data to HDFS There are many ways to read data from  and write data to HDFS.
We will start by using the FileSystem API to create and write to a file in HDFS, followed by an application to read a file from HDFS and write it back to the local filesystem.
Carry out the following steps to read and write data to HDFS:
Once you have downloaded the test dataset, we can write an application to read a file from the local filesystem and write the contents to HDFS.
Next, we write an application to read the file we just created in HDFS and write its contents back to the local filesystem.
FileSystem is an abstract class that represents a generic filesystem.
Most Hadoop filesystem implementations can be accessed and manipulated through the FileSystem object.
To create an instance of the Hadoop Distributed File System, you call the method FileSystem.get()
Once an instance of the FileSystem class has been created, the HdfsWriter class calls the create() method to create a file (or overwrite if it already exists) in HDFS.
The create() method returns an OutputStream object, which can be manipulated using normal Java I/O methods.
Similarly, HdfsReader calls the method open() to open a file in HDFS, which returns an InputStream object that can be used to read the contents of the file.
To demonstrate some of the other methods available in the API, we can add some error checking to the HdfsWriter and HdfsReader classes we created.
To check whether the file exists before we call create(), use:
Compressing data using LZO Hadoop supports a number of compression algorithms, including:
However, there is a drawback to storing data in HDFS using the compression formats listed previously.
Meaning, once a file is compressed using any of the codecs that Hadoop provides, the file cannot be decompressed without the whole file being read.
To understand why this is a drawback, you must first understand how Hadoop MapReduce determines the number of mappers to launch for a given task.
The number of mappers launched is roughly equal to the input size divided by dfs.block.size (the default block size is 64 MB)
The blocks of work that each mapper will receive are called input splits.
Since files compressed using the bzip2, gzip, and DEFLATE codecs cannot be split, the whole file must be given as a single input split to the mapper.
Using the previous example, if the input to a MapReduce job was a gzip compressed file that was 128 MB, the MapReduce framework would only launch one mapper.
Now, where does LZO fit in to all of this? Well, the LZO algorithm was designed to have fast decompression speeds while having a similar compression speed as compared to DEFLATE.
In addition, thanks to the hard work of the Hadoop community, LZO compressed files are splittable.
Perform the following steps to set up LZO and then compress and index a text file:
Navigate to the directory where you extracted the hadoop-lzo source, and build the project.
If the build was successful, you should see: BUILD SUCCESSFUL.
Copy the build JAR files to the Hadoop lib folder on your cluster.
Copy the native libraries to the Hadoop native lib folder on your cluster.
You should now see two files in the /test folder.
Both HDFS and Hadoop MapReduce share this configuration file, and the value of the io.compression.
This is a MapReduce application that will read one or more LZO compressed files and index the LZO block boundaries of each file.
Once this application has been run on an LZO file, the LZO file can be split and sent to multiple mappers by using the included input format LzoTextInputFormat.
LzoIndexer launches a standalone application to index LZO files in HDFS.
Reading and writing data to SequenceFiles The SequenceFile format is a flexible format included with the Hadoop distribution.
It is capable of storing both text and binary data.
This format supports compressing the value portion of a record or an entire block of key-value pairs.
SequenceFiles are splittable even when using a compression codec that is not normally splittable, such as GzipCodec.
SequenceFiles are able to do this because individual values (or blocks) are compressed, not the entire SequenceFile.
This recipe will demonstrate how to write and read to SequenceFiles.
Now, use the MapReduce job to read a SequenceFile from HDFS and transform it back to normal text:
MapReduce is an efficient way to transform data in HDFS.
These two MapReduce jobs are very simple to code and are capable of transforming data using the distributed processing power of the cluster.
This means that Hadoop will launch only mappers to process the test data.
This is achieved by setting the number of reducers to 0, as shown in the following line of code:
Next, we want the sequence writer job to read text input and save its output as a SequenceFile.
For the next application, we wanted to read a sequence file and write a normal text file.
To do this, we reversed the input and output formats we used for the sequence writer job.
In the sequence reader job, set the input format to read SequenceFiles.
You can compress SequenceFiles using the following methods when you set up your job:
Next, set the compression option you want to use; the following code sets the record compression option:
See also In the following recipes, we will continue to explore different data serialization libraries and formats:
Using Apache Avro to serialize data The description from the Apache Avro site defines Avro as a "data serialization system"
One of the neat features of Avro is that you do not need to compile any type of interface or protocol definition files in order to use the serialization features of the framework.
In this recipe, we will use Avro to serialize and write Java objects to a file in HDFS using MapReduce.
The AvroWriter MapReduce job reads a plain text file and serializes the WeblogRecord class into an Avro file.
The first step is to set up a MapReduce job to read the text file and write the output file using the Avro file format.
Build an Avro schema based on the WeblogRecord class, and then set the output schema:
Next, we use the old Hadoop MapReduce API to write the mapper and emit the WeblogRecord object by using the AvroWrapper class.
The output of this map-only job is stored in the Avro file format.
To read the Avro file format produced by the AvroWriter job, we just need to change the input format and the mapper class.
See also The following recipes will demonstrate additional data serialization libraries that can be used with Hadoop:
Using Apache Thrift to serialize data Apache Thrift is a cross-language serialization and RPC services framework.
Thrift uses an interface definition file to generate bindings in many languages, including Java.
This recipe demonstrates the defining of a Thrift interface, the generation of the corresponding Java bindings, and the use of these bindings to serialize a Java object to HDFS using MapReduce.
To compile and install Apache Thrift, first ensure that you have all the required dependencies using Yum:
The first task required us to define and compile a Thrift interface definition.
This definition file can be used to generate bindings in any language that Thrift supports.
Next, we used Elephant Bird to build a MapReduce application to serialize the WeblogRecord object that Thrift generated.
To set up the MapReduce job, we set the input format to read a normal text file:
Then the output format was configured to use Thrift block format compression with LZO to store the output records.
In the mapper, we use the ThriftWritable class of Elephant Bird to wrap the WeblogRecord object.
The ThriftWritable class is derived from the WritableComparable class of Hadoop, which must be implemented by all the keys emitted in MapReduce.
Every time we generate any type of binding using Thrift, the ThriftWritable class helps avoid having to write a custom WritableComparable class.
In the mapper, we instantiate both ThriftWritable and WeblogRecord instances:
Then, we call the set method of the thriftRecord object with an instance of WeblogRecord.
Finally, the mapper emits the thriftRecord object, which contains an instance of WeblogRecord.
See also The following recipe will demonstrate another popular serialization framework developed by Google:
Using Protocol Buffers to serialize data Protocol Buffers is a cross-language data format.
Protocol Buffers uses an interface definition file to generate bindings in many languages, including Java.
This recipe will demonstrate how to define a Protocol Buffers message, generate the corresponding Java bindings, and use these bindings to serialize a Java object to HDFS using MapReduce.
Note that you will need to have a GNU C/C++ compiler collection installed to compile the protocol buffer source.
We will be compiling the source code for Protocol Buffers.
To install GNU C/C++ using Yum, run the following command as the root user from a bash shell:
To compile and install Protocol Buffers, type the following lines of code:
The first task is to define and compile a Protocol Buffers message definition.
This definition file can be used to generate bindings in any language the Protocol Buffers compiler supports.
There are a couple of things to note about the format of the message.
First, the package definition package example; is not related to Java packages.
It is the namespace of the message defined in the *.proto file.
Second, the option java_ package declaration is a Java package definition.
Finally, the option java_outer_ classname declaration is the output class name that will be used.
Next, we wrote a MapReduce application to serialize the WeblogRecord object generated by the Protocol Buffers compiler.
To set up the MapReduce job, we set the input format to read a normal text file.
Then, the output format was set to store the records produced from the job in the Protocol Buffers block format, compressed using LZO.
The ProtobufWritable class is derived from the WritableComparable class of Hadoop, which all keys emitted in MapReduce must implement.
Every time we generate any type of binding using protoc, the ProtobufWritable class helps avoid having to write a custom WritableComparable class.
Setting the replication factor for HDFS HDFS stores files as data blocks and distributes these blocks across the entire cluster.
As HDFS was designed to be fault-tolerant and to run on commodity hardware, blocks are replicated a number of times to ensure high data availability.
The replication factor is a property that can be set in the HDFS configuration file that will allow you to adjust the global replication factor for the entire cluster.
For each block stored in HDFS, there will be n - 1 duplicated blocks distributed across the cluster.
For example, if the replication factor was set to 3 there would be one original block and two replicas.
This file is usually found in the conf/ folder of the Hadoop installation directory.
Changing the dfs.replication property in hdfs-site.xml will change the default replication for all files placed in HDFS.
You can also change the replication factor on a per-file basis using the Hadoop FS shell.
Alternatively, you can change the replication factor of all the files under a directory.
See also f The Setting the block size for HDFS recipe in this chapter; it will explain how to set.
Setting the block size for HDFS HDFS was designed to hold and manage large amounts of data; therefore typical HDFS block sizes are significantly larger than the block sizes you would see for a traditional filesystem (for example, the filesystem on my laptop uses a block size of 4 KB)
The block size setting is used by HDFS to divide files into blocks and then distribute those blocks across the cluster.
This file is usually found in the conf/ folder of the Hadoop installation directory.
Changing the dfs.block.size property in hdfs-site.xml will change the default block size for all the files placed into HDFS.
In this case, we set the dfs.block.size to 128 MB.
Changing this setting will not affect the block size of any files currently in HDFS.
It will only affect the block size of files placed into HDFS after this setting has taken effect.
Introduction Parsing and formatting large amounts of data to meet business requirements is a challenging task.
The software and the architecture must meet strict scalability, reliability, and run-time constraints.
Hadoop is an ideal environment for extracting and transforming large-scale data.
Hadoop provides a scalable, reliable, and distributed processing environment that is ideal for large-scale data processing.
This chapter will demonstrate methods to extract and transform data using MapReduce, Apache Pig, Apache Hive, and Python.
MapReduce is an excellent tool for transforming data into tab-separated values (TSV)
Once the input data is loaded into HDFS, the entire Hadoop cluster can be utilized to transform large datasets in parallel.
This recipe will demonstrate the method to extract records from Apache access logs and store those records as tab-separated values in HDFS.
Getting ready You will need to download the apache_clf.txt dataset from the support page of the Packt website, http://www.packtpub.com/support, and place the file in HDFS.
Perform the following steps to transform Apache logs to TSV format using MapReduce:
We first created a mapper that was responsible for the extraction of the desired information we from the Apache weblogs and for emitting the extracted fields in a tab-separated format.
Next, we created a map-only job to transform the web server log data into a tab-separated format.
The key-value pairs emitted from the mapper were stored in a file in HDFS.
By default, the TextOutputFormat class uses a tab to separate the key and value pairs.
For example, to separate the IP and the timestamp by a ',', we could re-run the job using the following command:
See also The tab-separated output from this recipe will be used in the following recipes:
Using Apache Pig to filter bot traffic from web server logs.
Apache Pig is a high-level language for creating MapReduce applications.
This recipe will use Apache Pig and a Pig user-defined filter function (UDF) to remove all bot traffic from a sample web server log dataset.
Bot traffic is the non-human traffic that visits a webpage, such as spiders.
Carry out the following steps to filter bot traffic using an Apache Pig UDF:
This class will be used to filter records in the weblogs dataset by using the user agent string.
Pig script to filter the weblogs by the user agent:
To run the Pig job, put myudfjar.jar into the same folder as the Pig script and execute it.
Apache Pig is extendable through the use of user-defined functions (UDF)
One way to create a UDF is through the use of the Java abstract classes and interfaces that come with the Apache Pig distribution.
In this recipe, we wanted to remove all records that contain known bot user agent strings.
One way to do this is to create our own Pig filter.
The IsUseragentBot class extends the abstract class FilterFunc, which allows us to override the exec(Tuple t) method.
A Pig Tuple is an ordered list of fields that can be any Pig primitive, or null.
At runtime, Pig will feed the exec(Tuple t) method of the IsUseragentBot class with the user agent strings from our dataset.
The UDF will extract the user agent string by accessing the first field in the Tuple, and it will return true if we find the user agent string is a bot, otherwise the UDF returns false.
In addition, the IsUseragentBot UDF reads a file called blacklist and loads the contents into a HashSet instance.
The file named blacklist is a symbolic link to blacklist.txt, which has been distributed to the nodes in the cluster using the distributed cache mechanism.
To place a file into the distributed cache, and to create the symbolic link, set the following MapReduce properties:
It is important to note that these properties are not Pig properties.
These properties are used by the MapReduce framework, so you can use these properties to load a file to the distributed cache for any MapReduce job.
Next, we told Pig where to find the JAR file containing the IsUseragentBot UDF:
Finally, we call the UDF using the Java class name.
When the job runs, Pig will instantiate an instance of the IsUseragentBot class and feed the exec(Tuple t) method with records from the all_weblogs relation.
Most abstract Pig classes that used to create UDFs now have a method named List<String> getCacheFiles() that can be overridden to load files from HDFS into the distributed cache.
For example, the IsUseragentBot class can be modified to load the blacklist.
See also Apache Pig will be used with the following recipes in this chapter:
Using Apache Pig to sort web server log data by timestamp.
In this recipe, we will demonstrate the method of writing a simple Pig script to sort a dataset using the distributed processing power of the Hadoop cluster.
Perform the following steps to sort data using Apache Pig:
Next, order the web server log records by the timestamp field in the ascending order: ordered_weblogs = ORDER nobots BY timestamp;
The Pig relational operator ORDER BY has the capability to provide total ordering of a dataset.
The Pig ORDER BY relational operator sorts data by multiple fields, and also supports sorting data in the descending order.
For example, to sort the nobots relationship by the ip and timestamp fields, we would use the following expression:
To sort the nobots relationship by timestamp in the descending order, use the desc option:
A session represents a user's continuous interaction with a website, and the user session ends when an arbitrary activity timeout has occurred.
A new session begins once the user returns to the website after a period of inactivity.
The following are the steps to create an Apache Pig UDF to sessionize web server log data:
Copy the JAR file containing the Sessionize class to the current working directory, and run the Pig script:
We first created a UDF that extended the EvalFunc abstract class and implemented the Accumulator interface.
The EvalFunc class is used to create our own function that can be used within a Pig script.
Data will be passed to the UDF via the exec(Tuple t) method, where it is processed.
The Accumulator interface is optional for custom eval functions, and allows Pig to optimize the data flow and memory utilization of the UDF.
Instead of passing the whole dataset, similar to how the EvalFunc class works, the Accumulator interface allows for subsets of the data to be passed to the UDF.
Next, we wrote a Pig script to group all of the web server log records by IP, and sort the records by timestamp.
We need the data sorted by timestamp because the Sessionize UDF uses the sorted order of the timestamps to determine the start of each session.
Then, we generated all of the sessions associated with a specific IP by calling the Sessionize alias.
Finally, we used the FLATTEN operator to unnest the Tuples in the DataBags emitted from the UDF.
In this recipe, we will use Python to create a simple Apache Pig user-defined function (UDF) to count the number of records in a Pig DataBag.
To build the file, download the Jython java installer, run the installer, and select Standalone from the installation menu.
The following are the steps to create an Apache Pig UDF using Python:
Next, create a Pig script to group all of the web server log records by IP and page.
Then send the grouped web server log records to the Python function:
First, we created a simple Python function to calculate the length of a Pig DataBag.
In this case, we want Pig to store the data returned by this function as a Java Long in a field named hits.
Next, we wrote a Pig script that registers the Python UDF using the statement:
Finally, we called the calculate() function using the alias count, in the Pig DataBag:
In a typical MapReduce job, key-value pairs are emitted from the mappers, shuffled, and sorted, and then finally passed to the reducers.
There is no attempt by the MapReduce framework to sort the values passed to the reducers for processing.
However, there are cases when we need the values passed to the reducers to be sorted, such as in the case of counting page views.
To calculate page views, we need to calculate distinct IPs by page.
One way to calculate this is to have the mappers emit the key-value pairs: page and IP.
Then, in the reducer, we can store all of the IPs associated with a page in a set.
What happens if the weblogs contain a large number of distinct IPs visiting a single page? We might not be able to fit the entire set of distinct IPs in memory.
The MapReduce framework provides a way to work around this complication.
In this recipe, we will write a MapReduce application that allows us to sort the values going to a reducer using an approach known as the secondary sort.
Instead of holding all of the distinct IPs in memory, we can keep track of the last IP we saw while processing the values in the reducer, and we can maintain a counter to calculate distinct IPs.
The following steps show how to implement a secondary sort in MapReduce to calculate page views:
We will use this class to store the key and sort fields:
Finally, write the code to set up a normal MapReduce job, but tell the MapReduce framework to use our own partitioner and comparator classes:
This class extends the Hadoop WritableComparable interface so that we can use the CompositeKey class just like any normal Hadoop WritableComparable interface (for example, Text and IntWritable)
The first Text object is used to partition and group the key-value pairs emitted from the mapper.
The second Text object is used to perform the secondary sort.
Next, we wrote a mapper class to emit the key-value pair CompositeKey (which consists of page and IP) as the key, and IP as the value.
In addition, we wrote a reducer class that receives a CompositeKey object and a sorted list of IPs.
The distinct IP count is calculated by incrementing a counter whenever we see an IP that does not equal a previously seen IP.
After writing the mapper and reducer classes, we created three classes to partition, group, and sort the data.
In this recipe, we want all of the same pages to go to the same partition.
Therefore, we calculate the partition location based only on the first field of the CompositeKey class.
We want all of the same page keys grouped together for processing by a reducer.
Therefore, the group comparator only inspects the first member of the CompositeKey class for comparison.
This class is responsible for sorting all of the values that are sent to the reducer.
The SortComparator class compares both the first and second members of the CompositeKey class to ensure that the values a reducer receives are sorted.
See also f Creating custom Hadoop Writable and InputFormat to read geographical event data.
Using Hive and Python to clean and transform geographical event data.
This recipe uses certain operators in Hive to input/output data through a custom Python script.
The script performs a few simple pruning operations over each row, and outputs a slightly modified version of the row into a Hive table.
Follow these steps to use Python and Hive to transform data:
It is important to note that Python is sensitive to inconsistent indentation.
Be careful if you are copying and pasting Python code.
To verify that the script finished properly, run the following command using the –e option to the Hive client.
The first line is simply to force a certain JVM heap size in our execution.
You can set this to any size that may be appropriate for your cluster.
For the ACLED Nigeria dataset, a 512 MB memory is more than enough.
We can omit the fields delimited by ',' and rows delimited by '\n' since they are the default field and row delimiters assumed by ROW FORMAT, and the ACLED Nigeria data is in that format.
Once we have our receiving table defined, we need to define the SELECT statement that will transform and output the data.
The common convention is to add scripts required by SELECT before the statement.
The SELECT statement uses the Hive TRANSFORM operator to separate each column by tabs and to cast all columns as String with nulls as '\n'
The columns loc and fatalities are conditionally checked for empty strings; and if found to be empty, are set to a default value.
We specify the USING operator to provide a custom script to work with the TRANSFORM operator.
Hive requires that scripts that make a call to the USING operator for row transformation need to first invoke TRANSFORM with the appropriate columns.
If the file has been placed on the distributed cache, and each node in the cluster has Python installed, the MapReduce JVM tasks will be able to execute the script and read the rows in parallel.
The #!/usr/bin/env python statement is a hint to tell the shell how to execute the script.
Each row from the table is passed in as a line over standard input.
The call to strip() method removes any leading/trailing whitespace, and then we tokenize it into an array of named variables.
Each field from the row is put in a named variable.
The raw ACLED Nigeria data was used to create the input Hive table, and contains a header row we wish to discard.
The first condition will check for 'LOCATION' as the value of loc, which indicates the header row we want to ignore.
If the row passes this check, we look for the presence of 'ZERO_FLAG' as the value for fatalities, which we set in our Hive script.
If the script detects this value for fatalities, we set the value of fatalities to the string '0'
Finally, we output each field excluding year in the same order as it was input.
The following are a few additional explanations that will help you with Hive TRANSFORM/USING/AS operations and ETL in general.
Making every column type String This is a bit counterintuitive and certainly not found anywhere in the Hive documentation.
If your initial Hive staging table for the incoming data maps each delimited field as a string, it will aid tremendously in data validation and debugging.
You can use the Hive STRING type to successfully represent almost any input into a cleansing script or direct Hive QL statement.
Trying to perfectly map datatypes over expected values is not flexible to an erroneous input.
There may be malformed characters for fields where you expect numeric values, and other similar hang-ups that make it impossible to perform certain analytics.
Using strings over the raw data fields will allow a custom script to inspect the invalid data and decide how to respond.
Moreover, when dealing with CSV or tab-separated data, a slightly misaligned INT or FLOAT type mapping in your Hive table declaration, where the data has a STRING, could lead to NULL mappings per row.
String mappings for every field in the raw table will show you column misalignment failures such as these, very quickly.
This is just a matter of preference, and only applies to tables designed for holding the raw or dirty input for immediate validation and transformation into other Hive tables.
Type casing values using the AS keyword This recipe only outputs strings from the Python script for use over standard output.
Hive will attempt to cast them to the appropriate type in the receiving table.
The advantage to this is the time and coding space saved by not having to explicitly cast every field with the AS operator.
The disadvantage is that this will not fail should a value be cast to an incompatible type.
For instance, outputting HI THERE to a numeric field will insert NULL for the field value for that row.
This can lead to undesirable behavior for subsequent SELECT statements over the table.
It is much easier to debug your script directly on the command line than it is across MapReduce task error logs.
It likely will not prevent you from having to troubleshoot issues dealing with scale or data validity, but it will eliminate a large majority of the compile time and control flow issues.
Using Python and Hadoop Streaming to perform a time series analytic.
This recipe shows how to use Hadoop Streaming with Python to perform a basic time series analysis over the cleansed ACLED Nigeria dataset.
The program is designed to output a list of dates in sorted order for each location where the government in Nigeria regained territory.
For this recipe, we will use structured Nigerian conflict data provided by Armed Conflict Location and Event dataset collections team.
The following are the steps to use Python with Hadoop Streaming:
You should see the job start from the command line and finish successfully:
The shell script sets up the Hadoop Streaming JAR path and passes the necessary arguments to the program.
The HDFS path for MapReduce to write the job output.
Script to be run as the map function; records passed via STDIN/STDOUT.
Tells the streaming tool which field/fields should be treated as the map output key.
This parameter tells the program to treat the first two as the key.
This will leverage the secondary sort feature in MapReduce to sort our rows based on the composite of these two fields.
Guarantees that all of the map output records with the same value in the first field of the key are sent to the same reducer.
The Python script used in the map phase gets a line corresponding to each record.
We call strip() to remove any leading/trailing whitespace and then split the line on tabs.
The result is an array of variables descriptively named to the row fields they hold.
The event_date field in the raw input requires some processing.
In order for the framework to sort records in ascending order of dates, we want to take the current form, which is dd/ mm/yy and convert it to yyyy-mm-dd.
Since some of the events occurred before the year 2000, we need to expand the year variable out to four digits.
Single-digit days and months are zero-padded, so that it sorts correctly.
In the shell script, we specified the first two fields as the output key.
Specifying location as the first field groups all records with the same location on a common reducer.
The value in each key-value pair is simply of the event_type field.
Order reducer shows the records that are sorted on the composite value of location and event_date.
Our configuration specifies only one reducer, so in this recipe all of the rows will partition to the same reduce Java Virtual Machine (JVM)
If multiple reduce tasks are specified, cityA and cityB could be processed independently on separate reduce JVMs.
As the records were partitioned by location, we can assume that each partition will go to its own mapper.
Furthermore, because we specified event_date as an additional sort column, we can make the assumption that the events corresponding to a given location are sorted by date in the ascending order.
Now we are in a position to understand how the script works.
The script must keep a track of when a loc input changes from the previous location.
Such a change signifies that we are done processing the previous location, since they are all in sorted order.
We also declare an empty array govt_regains to hold the dates of events we are interested in.
If there is a change in loc and it is not the beginning of the application, we know to output the current govt_regains collection to standard out.
The change means that we are done processing the previous location, and can safely write its collection of event dates out of the reducer.
If the incoming loc value is the same as current_loc, we know that the incoming event still corresponds to the location we are currently processing.
We check to see if the event is of the type regains to show the government the regained territories in that region.
If it matches that type, we add it to the current govt_regains collection.
The net result is a single part file that is output from the reducer with a list of locations in lexicographically sorted order.
To the right-hand side of each location is a tab-separated sorted list of dates matching the occurrences of when the government regained territory in that location.
Using Hadoop Streaming with any language that can read from stdin and write to stdout You are not limited to just Python when working with Hadoop Streaming.
Java classes, shell scripts, ruby scripts, and many other languages are frequently used to transition existing code and functionality into full-fledged MapReduce programs.
Any language that can read stdin and write to stdout will work with Hadoop Streaming.
Using the –file parameter to pass additional required files for MapReduce jobs Similar to normal MapReduce programs, you can pass additional dependencies over the distributed cache to be used in your applications.
A common request among MapReduce users is to control output file names to something other than part-*
This recipe shows how you can use the MultipleOutputs class to emit different key-value pairs to the same named file that you chose.
Create a reducer that sums all of the country counts, and writes the output to separate files using MultipleOutputs:
We first set up our job using the Tool interface provided by Hadoop.
In addition, both the mapper and reducer classes are set, and we configure the InputFormat to read lines of text.
The mapper class defines a statically initialized position to read the country from each line, as well as the regex pattern to split each line.
The mapper will output the country as the key and 1 for every line it appears on.
At the reduce phase, each task JVM runs the setup() routine and initializes a MultipleOutputs instance named output.
Each call to reduce() presents a country and a tally of every occurrence of the country appearing in the dataset.
Before we emit the final count, we will use the output instance to write a header to the file.
The key contains the text for the header Output by MultipleOutputs, and we null out the value since we don't need it.
We specify key.toString() to write the header to a custom file named by the current country.
On the next line we call output.write() again, except this time with the input key as the output key, the final count as the output value, and the key.toString() method to specify the same output file as the previous output.write() method.
The end result is a named country file containing both the header and the final tallied count for that country.
By using MultipleOutputs, we don't have to configure an OutputFormat class in our job setup routine.
Also, we are not limited to just one concrete type for the reducer output key and value.
We were able to output key-value pairs for both Text/NullWritable and Text/ IntWritable to the exact same file.
Creating custom Hadoop Writable and InputFormat to read geographical event data.
When reading input, or writing output from a MapReduce application, it is sometimes easier to work with data using an abstract class instead of the primitive Hadoop Writable classes (for example, Text and IntWritable)
This recipe demonstrates how to create a custom Hadoop Writable and InputFormat that can be used by MapReduce applications.
Follow these steps to create custom InputFormat and Writable classes:
These classes represent the key-value pairs that are passed to the mapper, much as how TextInputFormat passes LongWritable and Text to the mapper.
Finally, create a simple map-only job to test the InputFormat:
The first task was to define our own Hadoop key and value representations by implementing the WritableComparable interface.
The WritableComparable interface allows us to create our own abstract types, which can be used as keys or values by the MapReduce framework.
Next, we created an InputFormat that inherits from the FileInputFormat class.
The Hadoop FileInputFormat is the base class for all file-based InputFormats.
The InputFormat takes care of managing the input files for a MapReduce job.
The isSplitable() method is used to instruct the FileInputFormat class that it is acceptable to split up the input files if there is a codec available in the Hadoop environment to read and split the file.
After the GeoInputFormat class was written, we wrote a RecordReader to process the individual input splits and create GeoKey and GeoValue for the mappers.
The GeoRecordReader class reused the Hadoop LineRecordReader class to read from the input split.
These objects are GeoKey and GeoValue, which are sent to the mapper.
Introduction When working with Apache Hive, Pig, and MapReduce, you may find yourself having to perform certain tasks frequently.
The recipes in this chapter provide solutions for executing several very common routines.
You will find that these tools let you solve the same problems in numerous different ways.
Deciding on the right implementation can be a difficult task.
The recipes presented here were designed for coding efficiency and clarity.
Hive and Pig provide a clean abstraction layer between your data flow and meaningful queries, and the complex MapReduce workflows they compile to.
You can leverage the power of MapReduce for scalable queries without having to think about the underlying MapReduce semantics.
Both tools handle the decomposition and building of your expressions into the proper MapReduce sequences.
Hive lets you build analytics and manage data using a declarative, SQL-like dialect known as HiveQL.
Pig operations are written in Pig Latin and take a more imperative form.
Using Hive to map an external table over weblog data in HDFS.
You will often want to create tables over existing data that does not live within the managed Hive warehouse in HDFS.
Creating a Hive external table is one of the easiest ways to handle this scenario.
Queries from the Hive client will execute as they normally do over internally managed tables.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster, with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Carry out the following steps to map an external table in HDFS:
Open a text editor of your choice, ideally one with SQL syntax highlighting.
I have used the Textmate text editor for this recipe.
You should see two successful commands issued to the Hive client.
The existing definition of the table weblog_entries is deleted if it already exists.
Following this, the script issues a CREATE command with the EXTERNAL keyword, which tells the Hive Metastore that the data is not managed by the Hive warehouse in HDFS.
The table is defined as having five fields per entry.
The MD5 of the URL, the URL itself, the date of the request, the exact time of the request, and the IP address that the request was associated with.
We explicitly tell the SerDe that a tab character separates each field and a newline character separates each record.
The LOCATION keyword is required by Hive when creating an external table.
It points to the HDFS directory that contains the table data using an absolute path.
There are a few handy tips that you need to know when working with external tables.
Dropping an external table does not delete the data stored in the table Unlike a managed table in Hive, the DROP command only deletes the table entry from the Metastore and not the physical data in HDFS.
Other applications that depend on data stored in the supplied HDFS directory will continue to operate normally.
You can add data to the path specified by LOCATION If new data is inserted into a directory specified in an external table's LOCATION attribute, the data will be visible to any subsequent queries performed on the table.
Using Hive to dynamically create tables from the results of a weblog query.
This recipe will outline a shorthand technique for inline table creation when the query is executed.
Having to create every table definition up front is impractical and does not scale for large ETL.
Being able to dynamically define intermediate tables is tremendously useful for complex analytics with multiple staging points.
In addition to this, we will define a new field called url_length.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster, with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Carry out the following steps to create an inline table definition using an alias:
Open a text editor of your choice, ideally one with SQL syntax highlighting.
We then define the body of this table as an alias to the result set of a nested SELECT statement.
We also defined an additional field aliased as url_length to be calculated for each selected record.
It stores an int value that represents the number of characters in the record's url field.
In one simple statement, we created a table with a subset of fields from our starting table, as well as a new derived field.
The following are a few reminders for when using external tables:
If you are scripting the CREATE ALIAS for repeated use, the next execution, especially, will fail if there are table name conflicts.
Moreover, such intermediate tables will create a warehouse namespace that will quickly become unmanageable.
Using the Hive string UDFs to concatenate fields in weblog data.
String concatenation is a very common operation in any development task.
It frequently comes up when using Hive for report generation and even simple ETL tasks.
This recipe will show a very basic and useful example using one of the Hive string concatenation UDFs.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster, with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Carry out the following steps to perform string concatenation in HiveQL:
Open a text editor of your choice, ideally one with SQL syntax highlighting.
You should see the results of the SELECT statement printed out to the console.
The following snippet is an example that contains only two sample rows.
The script relies on the Hive built-in UDF to concatenate two strings together with a supplied separator token.
The output of the function is a single string containing both the fields separated by an underscore (_)
Since the SELECT statement consists of only that function, and the function outputs just a single string, we see a single column for all 3000 rows, one printed per line.
The following are a few additional notes to help with the concat_ws() function:
If you wish to encapsulate the auto-casting of your parameters to string, use the regular concat() function.
Alias your concatenated field Like most Hive UDFs, you can alias the output of concat_ws()
This comes in handy if you are persisting the results of the concatenation and want a very descriptive column header.
However, you are not limited in the number of input string parameters that you can supply to be concatenated and separated.
The following usage of the concat_ws() function is also valid:
Using Hive date UDFs to transform and sort event dates from geographic event data.
Using Hive to build a per-month report of fatalities over geographic event data.
Using Hive to intersect weblog IPs and determine the country.
Nevertheless, it is still very common to join records on identically matching keys contained in one or more tables.
This recipe will show a very simple inner join over weblog data that links each request record in the weblog_ entries table to a country, based on the request IP.
For each record contained in the weblog_entries table, the query will print the record out with an additional trailing value showing the determined country.
Getting ready Make sure that you have access to a pseudo-distributed or fully-distributed Hadoop cluster, with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Additionally, this recipe requires that the ip-to-country dataset be loaded into a Hive table named ip_to_country with the following fields mapped to the respective datatypes.
Carry out the following steps to perform an inner join in HiveQL:
Open a text editor of your choice, ideally one with SQL syntax highlighting.
You should see the results of the SELECT statement printed out to the console.
The following snippet is a printout containing only two sample rows.
Additionally, the JOIN operator tells Hive to perform a lookup in the ip_to_country table for each record, and find the specific country that maps to that weblog record's IP address.
In other words, our join key is the IP address contained in both the tables.
The following are a few more helpful introductory tips for the Hive JOIN syntax.
Hive supports multitable joins A single SELECT statement can use multiple instances of the JOIN <table> ON syntax to match the conditions contained in multiple tables.
The ON operator for inner joins does not support inequality conditions As of Hive 0.7.1, ON conditions cannot match records based on inequality.
The same query from the recipe will fail once the conditional operator is changed to inequality.
The following is the same query from the recipe, except that we wish to match every record for which the IP does not match an IP in the JOIN table:
FAILED: Error in semantic analysis: Line 2:30 Both left and right aliases encountered in JOIN ip.
See also This recipe is designed as a quick reference for simple table joins.
More advanced Hive joins are covered in depth in the following recipes of Chapter 5, Advanced Joins:
This recipe will demonstrate how to use the Java MapReduce API to calculate n-grams over news archives.
Some of the code listed in this recipe will be useful across a variety of different MapReduce jobs.
It includes code for the ToolRunner setup, custom parameter passing via configuration, and automatic output directory removal before job submission.
Getting ready This recipe assumes you have a basic familiarity with the Hadoop 0.20 MapReduce API and the general concept of n-gram calculations.
Inside the ZIP file, you will find the rural.txt and science.txt files.
You will need access to a pseudo-distributed or fully-distributed cluster capable of running MapReduce jobs using the newer MapReduce API introduced in Hadoop 0.20
You will also need to package this code inside a JAR file that is to be executed by the Hadoop JAR launcher from the shell.
Only the core Hadoop libraries are required to compile and run this example.
Carry out the following steps to implement n-gram in MapReduce:
Create a class named NGram.java in your JAR file at whatever source package is appropriate.
The first step involves creating your concrete Tool class for job submission.
The run() method is where we set the input/output formats, mapper class configuration, and key-value class configuration:
First, we set up our imports and create a public class named NGram that implements the MapReduce Tool interface.
The static string NAME is useful, should you decide to configure this job in a Hadoop Driver implementation.
The NGram program requires three parameters in exact order, namely the input path in HDFS, the desired output location in HDFS, and the total number of grams to be calculated per token.
We pass the ToolRunner with an instance of the NGramJob class, as well as a Configuration object initialized with the aforementioned parameters.
Inside the run() method, we configure the job to accept TextInputFormat and TextOutputFormat to read the input as lines of text, and write lines of text out from the map phase.
We are also required to set the Mapper class to the public static inner class NGramMapper.
Since this is a map-only job, we set the number of reducers to zero.
Then we set the parameterized Writable types for the key-value pairs out of the mapper.
It's also very important to call the setJarByClass() method so the TaskTrackers can properly unpack and find the Mapper and Reducer classes.
The job uses the static helper methods on FileInputFormat and FileOutputFormat to set the input and output directories respectively.
Since the output directory cannot exist, the program first deletes any previously defined HDFS files or directories located at the supplied path.
With everything configured properly, the job is now ready for submission to the JobTracker.
The NGramMapper class has a few very important member variables.
The variable gram_ length is dereferenced from the job configuration, which was set before submission to the user-supplied argument.
The variable space_pattern is statically compiled to perform a regex split on space characters.
The StringBuilder instance gramBuilder is used to store the space-separated list of grams that correspond to each string token.
The mapper receives line numbers as LongWritable instances and the line content as a Text instance.
For each token, reset gramBuilder, and if that token's position on the line when summed with gram_length exceeds the total length in characters of the line, ignore it.
Otherwise, iterate over and store each following token in gramBuilder until the loop reaches gram_length; then, output the gramBuilder content and cycle the outer loop to the next token.
The net result is one or more part files stored in the directory specified by the user-supplied argument, which contains a line-separated list of n-grams in the news archives.
The following two sections discuss how to use NullWritable objects effectively, and also remind developers to use the HDFS filesystem delete functions with care.
This method is supplied this parameter by the user-supplied output directory argument, which if reversed accidentally with the input argument, would remove the input directory.
Use NullWritable to avoid unnecessary serialization overhead This program makes use of NullWritable as the output value type from the mapper.
Since the program writes a single gram per line, we can just use the key to emit all our output.
If your MapReduce job does not require both the key and the value to be emitted, using NullWritable will save the framework the trouble of having to serialize unnecessary objects out to the disk.
In many scenarios, it is often cleaner and more readable than using blank placeholder values or static singleton instances for output.
Using the distributed cache in MapReduce to find lines that contain matching keywords over news archives.
The distributed cache in MapReduce is almost always required for any complex assignment involving dependent libraries and code.
One very common operation is passing cache files for use in each map/reduce task JVM.
This recipe will use the MapReduce API and the distributed cache to mark any lines in the news archive dataset that contain one or more keywords denoted in a list.
We will use the distributed cache to make each mapper aware of the list location in HDFS.
Getting ready This recipe assumes you have a basic familiarity with the Hadoop 0.20 MapReduce API.
Inside the ZIP file, you will find rural.txt and science.txt.
Feel free to add any additional words to this file, so long as they each appear on a new line.
You will need access to a pseudo-distributed or fully-distributed cluster capable of running MapReduce jobs using the newer MapReduce API introduced in Hadoop 0.20
You will also need to package this code inside a JAR file that is to be executed by the Hadoop JAR launcher from the shell.
Only the core Hadoop libraries are required to compile and run this example.
Carry out the following steps to implement a word-matching MapReduce job:
The run() method is where we set the input/output formats, mapper class configuration, and key-value class configuration:
Inside the setup() routine, we must load and write the file to a local disk from the distributed cache:
This class implements the Hadoop Tool interface for easy submission using the ToolRunner.
Before the job is submitted, we first check for the existence of both input and output parameters.
This file contains the keywords, separated by newline characters, that we are interested in locating within the news archives corpus.
We pass the helper method a URI reference to this path and the Configuration instance.
Now we can begin configuring the rest of the job.
Since we are working with text, we will use the TextInputFormat and TextOutputFormat classes to read and write lines as strings.
We will also configure the Mapper class to use the public static inner class LineMarkerMapper.
This is a map-only job, so we set the number of reducers to zero.
We also configure the output key type to be LongWritable for the line numbers and the output value as Text for the words, as we locate them.
It's also very important to call setJarByClass() so that the TaskTrackers can properly unpack and find the Mapper and Reducer classes.
The job uses the static helper methods on FileInputFormat and FileOutputFormat to set the input and output directories respectively.
Now we are completely set up and ready to submit the job.
There is a statically compiled regex pattern used to tokenize each line by spaces, and a wordlist Set used to store each distinct word we are interested in searching for.
The setup() method in the Mapper is told to pull the complete list of cache file URIs currently in the distributed cache.
We first check that the URI array returned a non-null value and that the number of elements is greater than zero.
If the array passes these tests, grab the keywords file located in HDFS and write it to the temporary working directory for the task.
Now we are free to use the standard Java I/O classes to read/write off the local disk.
Each line contained in the file denotes a keyword that we can store in the keywords' HashSet.
Inside our map() function, we first tokenize the line by spaces, and for each token, we see if it's contained in our keyword list.
If a match is found, emit the line number it was found on as the key and the token itself as the value.
The following are a few additional tips to know when starting out with the distributed cache in MapReduce.
Use the distributed cache to pass JAR dependencies to map/ reduce task JVMs Very frequently, your map and reduce tasks will depend on third-party libraries that take the form of JAR files.
Using Pig to load a table and perform a SELECT operation with GROUP BY.
This recipe will use Pig to group the IP addresses contained in the ip_to_country dataset and count the number of IP addresses listed for each country.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster with Apache Pig 0.9.2 installed on your client machine and on the environment path for the active user account.
Carry out the following steps to perform a SELECT and GROUP BY operation in Pig:
Open a text editor of your choice, ideally one with SQL syntax highlighting.
In the directory containing the script, run the command line using the Pig client with the –f option.
The first line creates a Pig relation named ip_countries from the tab-delimited records stored in HDFS.
The relation specifies two attributes, namely ip and country, both character arrays.
The results of this iteration are persisted to a new relation named country_counts, which consists of tuples containing exactly two attributes, namely group and counts.
The output is not sorted in country in the ascending or descending order.
Using Apache Pig to filter bot traffic from web server logs.
Using Apache Pig to sort web server logs data by timestamp.
Introduction In most processing environments, there will be a need to join multiple datasets to produce some final result.
Unfortunately, joins in MapReduce are non-trivial and can be an expensive operation.
This chapter will demonstrate different approaches to joining data in Hadoop using a number of tools, including Java MapReduce, Apache Pig, and Apache Hive.
In addition, this chapter will demonstrate how to leverage external memory resources using Hadoop MapReduce.
Joining data in the Mapper using MapReduce Joining data in MapReduce is an expensive operation.
Depending on the size of the datasets, you can choose to perform a map-side join or a reduce-side join.
In a map-side join, two or more datasets are joined on a key in the map phase of a MapReduce job.
In a reduce-side join, the mapper emits the join key, and the reduce phase is responsible for joining the two datasets.
In this recipe we will demonstrate how to perform a map-side replicated join using Pig.
We will join a weblog dataset, and a dataset containing a list of distinct IPs and their associated country.
As the datasets will be joined in the map-phase, this will be a map-only job.
Carry out the following steps to join data in the map phase using MapReduce:
In step 2, we overrode the setup() method of the mapper.
The setup() method is called by the MapReduce framework only once, prior to any calls to the map() method.
The setup() method is an excellent place to perform any one-time initialization to the mapper class.
To read from the distributed cache, we used the static method DistributedCache.
The MapReduce framework also supports distributing archive files using the distributed cache.
An archive file can be a ZIP file, GZIP file, or even a JAR file.
Once the archives have been distributed to the task nodes, they will be decompressed automatically.
To add an archive to the distributed cache, simply use the addCacheArchive() static method of the DistributedCache class when configuring the MapReduce job:
See also f Joining data using Apache Pig replicated join.
The reduce-side join is the default implementation when you use Pig's JOIN operator.
Pig also supports map-side joins when you specify the replicated or merge keyword.
This recipe will demonstrate how to perform a map-side replicated join using Pig.
We will join a weblog dataset, and a dataset containing a list of distinct IPs and their associated countries.
You will also need a recent version of Apache Pig (0.9 or later) installed on the cluster.
Carry out the following steps to perform a replicated join in Apache Pig:
Next, we joined the two datasets on the ip field using Pig's replicated join.
It is important that the right-most relations be small enough to fit into a mapper's memory.
Pig will not warn you if the dataset is too large, the job will just fail with an out of memory exception.
Finally, in step 3, we formatted the joined relation into a new relation named cleaned.
There is one field that looks odd in the FOREACH statement, and that field is ip_country_ tbl::ip.
We had to use the :: operator to define which column we wanted to store in the cleaned relation, since the joined relation contains two fields named ip.
We could have easily chosen to use nobots_weblogs::ip instead; it makes no difference in this example.
The replicated join can be used on more than one relation.
For example, we can modify the previous recipe to use a replicated join to perform an inner join on three relations:
Again, the right-most datasets must fit into the main memory.
See also f Joining sorted data using Apache Pig merge join.
Like the replicated join described in the previous recipe, the Apache Pig's merge join is another map-side join technique.
However, the major difference between the two implementations is that the merge join does not place any data into main memory.
This recipe will demonstrate how to use Pig's merge join to join two datasets.
You will also need a recent version of Apache Pig (0.9 or later) installed on the cluster.
In order to use the merge join functionality in Pig, the two datasets need to be sorted on the join key.
To sort the two datasets, run the following commands using Unix sort:
Carry out the following steps to perform a merge join in Apache Pig:
In step 2, we joined the two datasets on the ip field using Pig's merge join.
Pig will launch two MapReduce jobs to perform the merge join.
It is important to place the larger of the two relations as the left-hand side input to the JOIN statement, as we did with the nobots_weblogs relation.
Once Pig has built the index, it launches a second map-only job, which reads the left-hand side relationship, and the index created in the first MapReduce job to join the two relations.
It is important to note that Pig's merge join requires the input data to be sorted in ascending order across all input files.
In addition, all of the data must be sorted in an ascending order by filename.
For example, if the nobots_weblogs relation contains three distinct IPs across two input files, the following IPs could be distributed in this fashion:
This example shows the possible total ordering of IPs across a number of files ordered by name.
Filenames need to support ascending order because this is the order in which Pig will attempt to read each file to access the sorted data.
See also f Joining skewed data using Apache Pig skewed join.
Data skew is a serious problem in a distributed processing environment, and occurs when the data is not evenly divided among the emitted key tuples from the map phase.
In the MapReduce framework, data skew can cause some mappers/reducers to take significantly more time to perform a task as compared to other mappers/reducers in the job.
Apache Pig has the skewed join to help alleviate the data skew issue with joins.
This recipe will demonstrate how to join a skewed dataset, with a small table.
You will also need a recent version of Apache Pig (0.9 or later) installed on the cluster.
Follow the steps to perform a skewed join in Apache Pig:
In step 2, we joined the two datasets on the ip field using Pig's skewed join.
Pig will launch two MapReduce jobs to perform the skewed join.
Pig will determine how the data is distributed to the reducers based on the sampling from the first map reduce job.
If there is skew present in the dataset, Pig will attempt to optimize the data distribution to the reducers.
Using a map-side join in Apache Hive to analyze geographical events.
When joining two tables in Apache Hive, one table might be significantly smaller than the other.
In such cases, Hive can push a hash table representing the smaller table over the distributed cache and join the tables entirely map-side, which can lead to better parallelism and job throughput.
In this recipe, we will use a map-side join to attach any significant holiday information that may have occurred on a particular geographic event.
Getting ready Ensure that Apache Hive 0.7.1 is installed on your client machine and on the environment path for the active user account.
Carry out the following steps to perform a map-side join in Apache Hive:
You will know the map-side join is working if you see this message in the output trace: Mapred Local Task Succeeded.
The generated MapReduce job should not have any reduce tasks.
You should see the following five rows appear first in the output console:
Battle-No change of territory  Lagos State only; in memory of failed 1993 election.
The nigeria_holidays table is very small and made the most sense to load as a hash table.
The MAPJOIN operation handles creating the hash table and distributing it to each map task.
Map-side joins can be tricky to configure and use properly.
You can configure the maximum size with the property hive.
This will tell Hive what file size (or below) constitutes a small table.
See also f Using optimized full outer joins in Apache Hive to analyze geographical events.
Using optimized full outer joins in Apache Hive to analyze geographical events.
This recipe will take a list of Nigerian VIPs and join any Nigerian ACLED events that occurred on any VIP's birthday.
We are not only interested in viewing events that did or did not occur on a famous person's birthday, but also in the people who are not linked to any event.
To accomplish this analytics in a single query, a full outer join makes the most sense.
We would also like to store the results in a table.
Getting ready Ensure that Apache Hive 0.7.1 is installed on your client machine and on the environment path for the active user account.
Follow the steps to perform a full outer join in Hive:
We use an inline CREATE TABLE AS statement to shortcut having to explicitly define the table.
The vips.description column gets the alias pers_desc to make the column label a little more meaningful.
Since we are performing a Hive common join and not a map-side join, the actual table joining will occur reduce-side.
Hive will attempt to buffer the rows from the left-most table and then stream the rows from the rightmost table.
Moreover, no two people in our list have the same birthday; therefore, the outer join does not replicate the same event into multiple rows for each VIP birthday joined.
There are other things you can do to improve the performance of the join operations in Hive.
Common join versus map-side join The Hive documentation will use the term "common join" to refer to a join operation where one or more reducers are required to physically join the table rows.
Map-side joins, as the name would suggest, perform the join across parallel map tasks and eliminate the need for the reduce phase.
Table ordering in the query matters The left-to-right ordering of join table declarations in your queries, especially in a multi-table join, is very important.
Hive will attempt to buffer the rows from the left-hand side table and stream the results of the right-hand side.
In a multi-table join, several map/reduce jobs may occur, but the same semantics apply.
The result of the first join will be buffered, while the rows of the next right-most table will be streamed.
Key-value stores are an efficient tool for storing large datasets.
In MapReduce, we can use key-value stores to house large datasets that might not fit into the memory of a mapper or mappers (remember that multiple mappers can run on the same slave node), but can fit into the main memory of the server.
In this recipe, we will demonstrate how to use Redis to perform a map-side join using MapReduce.
A quick start guide is available on the Redis website, http://redis.io/topics/quickstart.
Once you have compiled and installed the Redis server, start the server by issuing the following command:
Verify that the Redis server is working properly by using redis-cli:
Redis should respond with the message "PONG" if everything has been set up properly.
Jedis is a Redis Java client that we will use in our MapReduce application to communicate with Redis.
Follow these steps to join data in MapReduce using Redis:
This class looks very familiar to other map-only jobs we've created in past recipes, except for the loadRedis() method.
The loadRedis() method first connects to the local Redis instance using the Jedis constructor.
Next, we used the select() method to choose which Redis database we wanted to use.
A single Redis instance can contain a number of databases, which are identified using a numeric index.
Once we get connected to the desired database, we call the method flushDB(), which deletes everything currently stored in the current database.
This recipe used a very simple string data structure to store the ip/country key-value pairs.
Redis supports many other data structures, including hashes, lists, and sorted sets.
In addition, Redis has support for transactions, and a publish/subscribe mechanism.
Visit the Redis website http://redis.io/, to review all of this functionality in depth.
Introduction Learning to apply Apache Hive, Pig, and MapReduce to solve the specific problems you are faced with can be difficult.
The recipes in this chapter present a few big data problems and provide solutions that show how to tackle them.
You will notice that the questions we ask of the data are not incredibly complicated, but you will require a different approach when dealing with a large volume of data.
Even though the sample datasets in the recipes are small, you will find that the code is still very applicable to bigger problem spaces distributed over large Hadoop clusters.
The analytic questions in this chapter are designed to highlight many of the more powerful features of the various tools.
You will find many of these features and operators useful as you begin solving your own problems.
Counting distinct IPs in weblog data using MapReduce and Combiners.
This recipe will walk you through creating a MapReduce program to count distinct IPs in weblog data.
We will demonstrate the application of a combiner to optimize data transfer overhead between the map and reduce stages.
The code is implemented in a generic fashion and can be used to count distinct values in any tab-delimited dataset.
Getting ready This recipe assumes that you have a basic familiarity with the Hadoop 0.20 MapReduce API.
You will need access to the weblog_entries dataset supplied with this book and stored in an HDFS folder at the path /input/weblog.
You will need access to a pseudo-distributed or fully-distributed cluster capable of running MapReduce jobs using the newer MapReduce API introduced in Hadoop 0.20
You will also need to package this code inside a JAR file to be executed by the Hadoop JAR launcher from the shell.
Only the core Hadoop libraries are required to compile and run this example.
Perform the following steps to count distinct IPs using MapReduce:
Open a text editor/IDE of your choice, preferably one with Java syntax highlighting.
The run() method is where we set the input/output formats, mapper class configuration, combiner class, and key/value class configuration:
The following command shows the sample usage against weblog data with column position number 4, which is the IP column:
First we set up DistinctCounterJob to implement a Tool interface for remote submission.
The static constant NAME is of potential use in the Hadoop Driver class, which supports the launching of different jobs from the same JAR file.
This value is set within the job configuration, and should match the position of the column you wish to count for each distinct entry.
Supplying 4 will match the IP column for the weblog data.
Since we are reading and writing text, we can use the supplied TextInputFormat and TextOutputFormat classes.
We will set the Mapper and Reduce classes to match our DistinctMapper and DistinctReducer implemented classes respectively.
It's also very important to call setJarByClass() so that the TaskTrackers can properly unpack and find the Mapper and Reducer classes.
The job uses the static helper methods on FileInputFormat and FileOutputFormat to set the input and output directories respectively.
Now we're set up and ready to submit the job.
The Mapper class sets up a few member variables as follows:
It allows users to change which column to parse and apply the count distinct operation on.
This avoids having to create a new instance for each output that is written.
The map() function splits each incoming line's value and extracts the string located at col_ pos.
We reset the internal value for outKey to the string found on that line's position.
For our example, this will be the IP value for the row.
We emit the value of the newly reset outKey variable along with the value of outValue to mark one occurrence of that given IP address.
Without the assistance of the combiner, this would present the reducer with an iterable collection of 1s to be counted.
We can use a combiner to process the intermediate key-value pairs as they are output from each mapper and help improve the data throughput in the shuffle phase.
Since the combiner is applied against the local map output, we may see a performance improvement as the amount of data we need to transfer for an intermediate key/value can be reduced considerably.
For counting the occurrences of distinct IPs, we can use the same code in our reducer as a combiner for output in the map phase.
Without the aid of a combiner, this will be merged in the shuffle phase and presented to a single reducer as the following key-value collection:
Now let's revisit what would happen when using a Combiner against the exact same sample output:
Now the reducer will see the following for that key-value collection:
We arrived at the same total count for that IP address, but we used a combiner to limit the amount of network I/O during the MapReduce shuffle phase by pre-reducing the intermediate key-value output from each mapper.
The Combiner does not always have to be the same class as your Reducer The previous recipe and the default WordCount example show the Combiner class being initialized to the same implementation as the Reducer class.
This is not enforced by the API, but ends up being common for many types of distributed aggregate operations such as sum(), min(), and max()
One basic example might be the min() operation of the Reducer class that specifically formats output in a certain way for readability.
This will take a slightly different form from that of the min() operator of the Combiner class, which does not care about the specific output formatting.
Combiners are not guaranteed to run Whether or not the framework invokes your combiner during execution depends on the intermediate spill file size from each map output, and is not guaranteed to run for every intermediate key.
Your job should not depend on the combiner for correct results, it should be used only for optimization.
Using Hive date UDFs to transform and sort event dates from geographic event data.
This recipe will illustrate the efficient use of the Hive date UDFs to list the 20 most recent events and the number of days between the event date and the current system date.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Issue the following command to the Hive client to see the mentioned fields:
Perform the following steps to utilize Hive UDFs for sorting and transformation:
Open a text editor of your choice, ideally one with SQL syntax highlighting.
You should see the following five rows appear first in the output console:
The first argument to datediff() is the end date, with which we want to represent the current system date.
Calling unix_timestamp() with no arguments will return the current system time in milliseconds.
We only care about the date portion, so calling to_date() with the output of this function strips the HH:mm:ss.
The result is the current date in the yyyy-MM-dd form.
The second argument to datediff() is the start date, which for our query is the event_ date.
The series of function calls operate in almost the exact same manner as our previous argument, except that when we call unix_timestamp(), we must tell the function that our argument is in the SimpleDateFormat format that is yyyy-MM-dd.
We alias the output column of datediff() as days_since for each row.
The outer SELECT statement takes these three columns per row and sorts the entire output by event_date in descending order to get reverse chronological ordering.
The net result is the 20 most recent events with the number of days that have passed since that event occurred.
The date UDFs can help tremendously in performing string date comparisons.
Date format strings follow Java SimpleDateFormat guidelines Check out the Javadocs for SimpleDateFormat to learn how your custom date strings can be used with the date transform UDFs.
Default date and time formats f Many of the UDFs operate under a default format assumption.
See also f Using Hive to build a per-month report of fatalities over geographic event data.
Using Hive to build a per-month report of fatalities over geographic event data.
This recipe will show a very simple analytic that uses Hive to count fatalities for every month appearing in the dataset and print the results to the console.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Open a text editor of your choice, ideally one with SQL syntax highlighting.
You should see the following three rows appear first in the output console.
Note that the output is sorted lexicographically, and not on the order of dates.
This is also in the GROUP BY expression for totaling fatalities using sum()
The coalesce() method returns the first non-null argument passed to it.
We pass as the first argument, the value of fatalities summed for that given year-month, cast as a string.
If that value is NULL for any reason, return the constant Unknown.
Otherwise return the string representing the total fatalities counted for that year-month combination.
The following are some additional helpful tips related to the code in this recipe:
As mentioned in the Hive documentation, coalesce() supports one or more arguments.
This can be useful for evaluating several different expressions for a given column before deciding the right one to choose.
The coalesce() will return NULL if no argument is non-null.
It's not uncommon to provide a type literal to return if all other arguments are NULL.
Date reformatting code template Having to reformat dates stored in your raw data is very common.
Remember this general code template for concise date format transformation in Hive:
See also f Using Hive date UDFs to transform and sort event dates from geographic event data.
Implementing a custom UDF in Hive to help validate source reliability over geographic event data.
There are many operations you will want to repeat across various data sources and tables in Hive.
For this scenario, it makes sense to write your own user-defined function (UDF)
You can write your own subroutine in Java for use on any Writable input fields and to invoke your function from Hive scripts whenever necessary.
This recipe will walk you through the process of creating a very simple UDF that takes a source and returns yes or no for whether that source is reliable.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Additionally, you will need to place the following recipe's code into a source package for bundling within a JAR file of your choice.
In addition to the core Hadoop libraries, your project will need to have hive-exec and hive-common JAR dependencies on the classpath for this to compile.
Perform the following steps to implement a custom Hive UDF:
Open a text editor/IDE of your choice, preferably one with Java syntax highlighting.
Build the containing JAR <myUDFs.jar> and test your UDF through the Hive client.
Hive should already be on the local user environment path.
You will know that the preceding operation succeeded if you see the following messages indicating that the JAR has been added to the classpath and the distributed cache:
Create the function definition trust_source as an alias to TrustSourceUDF at whatever source package you specified in your JAR:
You should see the shell prompt you that the command executed successfully.
If you see the following error, it usually indicates your class was not found on the classpath:
You should see mostly yes printed on each line of the console, with a few no's here and there:
No methods are required for implementation; however, in order for the class to function at Hive runtime as a UDF, your subclass must override evaluate()
You can have one or more overloaded evaluate() methods with different arguments.
Ours only needs to take in a source value to check.
Within a static initialization block, we set up a few sources by their names to be flagged as unreliable.
The entries here are purely arbitrary and should not be considered reliable or unreliable outside of this recipe.
When the function is invoked, it expects a single Text instance to be checked against the sources we've flagged as unreliable.
Return yes or no depending on whether the given source appears in the set of unreliable sources or not.
We set up the private Text instance to be re-used every time the function is called.
Once the JAR file containing the class is added to the classpath, and we set up our temporary function definition, we can now use the UDF across many different queries.
The following sections list a bit more information regarding them:
Check out the existing UDFs The Hive documentation has a great explanation of the built-in UDFs bundled with the language.
To see which functions are available in your specific version of Hive, issue the following command in the Hive shell.
Once you pinpoint a function that looks interesting, learn more information about it from the Hive wiki or directly from the Hive shell by executing the following command:
User-defined table and aggregate functions Hive UDFs do not need to have a one-to-one interaction for input and output.
The API allows the generation of many outputs from one input (GenericUDTF) as well as custom aggregate functions that take a list of input rows and output a single value (UDAF)
Export HIVE_AUX_JARS_PATH in your environment Adding JAR files dynamically to the classpath is useful for testing and debugging, but can be cumbersome if you have many libraries you repeatedly wish to use.
The Hive command line interpreter will automatically look for the existence of HIVE_AUX_JARS_PATH in the executing user's environment.
Use this environment variable to set additional JAR paths that will always get loaded in the classpath of new Hive sessions for that client machine.
See also f Using Hive date UDFs to transform and sort event dates from geographic event data.
Marking the longest period of non-violence using Hive MAP/REDUCE operators and Python.
The Hive query language provides facilities to control the MapReduce dataflow and inject your own custom map, and to reduce scripts at each stage.
When used properly, this is a very powerful technique for writing concise MapReduce programs using minimal syntax.
This recipe will show a complete example of how to write custom MapReduce control flow using different operators in Hive.
The analytic will specifically look for the longest gap in events for each location to get an idea of how frequently violence occurs in that location.
Getting ready Make sure you have access to a pseudo-distributed or fully-distributed Hadoop cluster with Apache Hive 0.7.1 installed on your client machine and on the environment path for the active user account.
Your cluster will also need Python 2.7 or greater installed on each node and available on the environment path for the Hadoop user.
The script shown in this recipe assumes an installation at /usr/bin/env python.
If this does not match your installation, change the script accordingly.
Perform the following steps to mark the longest period of non-violence using Hive:
Open a text editor of your choice, ideally one with SQL and Python syntax highlighting.
Keep that in mind if you are cutting and pasting this code: #!/usr/bin/python import sys from datetime import datetime, timedelta.
You should see rows printed to the console in no particular order:
The first line is simply to force a certain JVM heap size in our execution.
You can set this to whatever is appropriate for your cluster.
For the ACLED Nigeria dataset, 512 MB is more than enough.
This will be used as our reduce script, but first we must organize the map output.
The DISTRIBUTE BY loc statement tells Hive to guarantee that all rows with matching values for loc go to the same reducer.
Now the same reducer can process every row corresponding to each location locally, and in the sorted order of event_date.
We alias the output of this SELECT statement to mapout and use the shorthand REDUCE operator to process each row from mapout.
The USING clause lets us supply a custom Python script to read each record as it comes over stdin.
The AS operator lets us map the delimited fields that are output by the script over stdout to pipe into the fields of the receiving table.
Since we have guaranteed that all records with a common loc value are at the same reducer and that those records are in the date-sorted order for each location, we are now in a position to understand how the Python script works.
We need to import sys to use the stdin and stdout operations.
We also need the datetime and timedelta class definitions from the datetime package.
The script operates very procedurally and can be a bit difficult to follow.
We then set up several different variables to hold different placeholder values to be used on a per-location basis by the for loop.
It is blank if it's the start of the app, or holds a new location value.
The for loop reads rows over stdin that have already been sorted by the combination of loc and event_date.
We parse each row into variables representing the column values by first stripping any additional newlines and splitting the line on tab characters.
The first conditional is skipped as current_loc is equal to START OF APP.
We have only begun processing the first row across all locations on that reducer, and have nothing to output yet.
Should end_date still be set to an empty string, then that indicates we only saw one event for that location.
In this scenario, we do not output anything for that location.
Finally, we reset the six placeholder variables previously explained, so that we may accurately process the records for the next location.
Following the conditional, we immediately set the value of current_loc that we are processing equal to loc, to avoid unnecessary entry of the mentioned conditional on the next iteration when we have not yet transitioned locations.
If start_time_obj is set to None, then that means we are on the first row for that location and cannot yet do a time delta comparison.
When start_time_obj is no longer set to None after the first iteration for a given location, we can begin doing diff comparisons on these two datetime objects.
In doing so, we capture the longest elapsed time period for that location between events.
The next time the loop encounters the condition where loc is not equal to current_loc, we output the currently held longest time difference between events, before we move onto the next event.
Here are a few additional notes on some of the operations touched upon in this recipe:
Here's a quick comparison so you'll know which one is appropriate for your use case:
When used alone, it does not guarantee sorted input to the reducer.
Sorted order is maintained across all of the output from every reducer.
Use this with caution as it can force all of the output records to a single reducer to perform the sorting.
You can use any one of the three and achieve the same functional results.
See also f The Using Hive and Python to clean and transform geographical event data in recipe.
Calculating the cosine similarity of artists in the Audioscrobbler dataset using Pig.
Cosine similarity is used to measure the similarity of two vectors.
In this recipe, it will be used to find the similarity of artists based on the number of times Audioscrobbler users have added each user to their playlist.
Perform the following steps to calculate cosine similarity using Pig:
The load statements tell Pig about the format and datatypes of the data being loaded.
This means that the load statements at the beginning of this script will not do any work until another statement is entered that asks for output.
This significantly reduces the processing time at the cost of accuracy.
The sample value of .01 is used, meaning that roughly one in hundred rows of data will be loaded.
A user selecting to play an artist is treated as a vote for that artist.
This ensures that each user is given the same number of votes.
Sum the number of plays for artist 1 by all users.
Sum the number of plays for artist 2 by all users.
The cosine similarly is the dot product over the total plays for artist 1 multiplied by the total plays for artist two.
Trim Outliers from the Audioscrobbler dataset using Pig and datafu.
Datafu is a Pig UDF library open sourced by the SNA team at LinkedIn.
This recipe will use play counts from the Audioscrobbler dataset and the Quantile UDF from datafu to identify and remove outliers.
Register the datafu JAR file and construct the Quantile UDF:
Group all of the data: plays_grp = group plays ALL;
This recipe takes advantage of the datafu library open sourced by LinkedIn.
Once a JAR file is registered, all of its UDFs are available to the Pig script.
The constructor of the Quantile UDF will then create an instance that will produce the ninetieth percentile of the input vector it is passed.
The define also aliases Quantile as shorthand for referencing this UDF.
The user artist data is loaded into a relation named plays.
The ALL group is a special kind of group that creates a single bag containing all of the input.
The Quantile UDF requires that the data it has passed be sorted first.
The data is sorted by play count, and the sorted play count's vector is passed to the Quantile UDF.
The sorted play count simplifies the job of the Quantile UDF.
It now picks the value at the ninetieth percentile position and returns it.
This value is then compared against each of the play counts in the user artist file.
If the play count is greater, it is trimmed down to the value returned by the Quantile UDF, otherwise the value remains as it is.
The updated user artist file with outliers trimmed is then stored back in HDFS to be used for further processing.
This UDF is similar to the Quantile UDF except that it does not require the data to be sorted before it is used.
The StreamingQuantile UDF only provides an estimation of the values.
Introduction Graph and machine learning problems are hard to solve using the MapReduce framework.
Most of these problems require iterative steps and/or knowledge of complex algorithms, which can be cumbersome to implement in MapReduce.
Luckily, there are two frameworks available to help with graph and machine learning problems in the Hadoop environment.
Apache Giraph is a graph-processing framework designed to run large-scale algorithms.
Apache Mahout is a framework that provides implementations of distributed machine learning algorithms.
This chapter will introduce readers to these two frameworks, which are capable of leveraging the distributed power of MapReduce.
PageRank with Apache Giraph This recipe is primarily aimed at building and testing the default Apache Giraph PageRank example, modeled after the Google Pregel implementation.
It will demonstrate the steps involved in submitting and executing a Giraph job to a pseudo-distributed Hadoop cluster.
Getting ready For first-time Giraph users, we recommend running this recipe using a pseudo-distributed Hadoop cluster.
For the client machine, you will need Subversion and Maven installed and configured on the user environment path.
This recipe does not require a full understanding of the Giraph API, but it does assume some familiarity with Bulk Synchronous Parallel (BSP) and the design goals of vertex-centric APIs including Apache Giraph and Google Pregel.
Carry out the following steps to build and test the default Giraph PageRank example:
Navigate to a base folder and perform an SVN checkout of the latest Giraph source, located at the official Apache site:
Change the folder into a trunk and run the build: $ mvn compile.
You should see the job execute and the MapReduce command line output show success.
The Giraph stats counter group in the printout should show the following stats:
First, we use Subversion to check out the latest source from the official Apache site.
Once we build the JAR file, the PageRankBenchmark example job is available for submission.
Before we are ready to test Giraph, we must set the following command line options:
For a more accurate testing we would want millions of vertices over a fully-distributed cluster.
This will control the number of messages that are output during each superstep to any neighboring vertices, where a neighbor is defined as a vertex connected to another vertex by one or more edges.
Since we are running a pseudo-distributed cluster (single host), it is safe to limit this to one.
In a fully-distributed cluster, we would want multiple workers spread out across different physical hosts.
The job contains no additional classpath dependencies outside of core Hadoop/ZooKeeper.
It can be directly submitted to the cluster via the hadoop jar command from the command line.
The PageRankBenchmark example does not output the results back to HDFS.
It is designed primarily to test and expose certain cluster bottlenecks that might hinder other production Giraph jobs.
Running the job against a large number of vertices with multiple edges may expose memory constraints, network I/O connectivity issues between workers, and other potential problems.
Apache Giraph is a relatively new open source batch computation framework.
Keep up with the Apache Giraph community Apache Giraph has a very active developer community.
The API is constantly being enhanced with new features, bug fixes, and occasional refactoring.
It is a good idea to update your source from trunk at least once a week.
At the time of this writing, Giraph has no public Maven artifact.
This will change in the very near future, but for now SVN is required to pull source updates.
Read and understand the Google Pregel paper Somewhere in 2009, Google published a research paper describing in high-level technical detail their proprietary software, which was made for scalable graph-centric processing based on the Bulk Synchronous Parallel (BSP) model.
Apache Giraph is an open source implementation of many of the concepts found in this research paper.
Familiarity with the Pregel design will help to explain many components found in the Giraph codebase.
In this recipe, we will implement a variant of the Google Pregel shortest-path implementation between employees connected via an acyclic directed graph.
The code will take a single source ID, and for all vertices in the graph, will mark the minimum number of hops required to reach each vertex from the source ID vertex.
The employee network is stored in HDFS as a line-separated list of RDF triples.
Resource Description Framework (RDF) is a very effective data format for representing entities and the relationships between them.
Getting ready Make sure you have a basic familiarity with Google Pregel/BSP and the Giraph API.
The code listed in this recipe uses a non-split master-worker configuration that is not ideal in fully-distributed environments.
It also assumes that you have familiarity with bash shell scripting.
You will need to load the example dataset gooftech.tsv to an HDFS folder located at /input/gooftech.
You will also need to package this code inside a JAR file to be executed by the Hadoop JAR launcher from the shell.
The shell script listed in the recipe shows a template for job submission with the correct classpath dependencies.
Carry out the following steps to implement the shortest path in Giraph:
First, we define our custom InputFormat that extends TextInputFormat to read the employee RDF triples from the text.
The job setup code, vertex class, and custom output format are all contained in a single class.
To use the alias emp_shortest_path your custom JAR file must use the Hadoop Driver class for its main class.
Under /output/gooftech should be a single part file that lists minimum number of hops required to reach each employee from source ID, or no path if the employee is not reachable.
Currently, the Giraph API requires your records to be sorted in order of the vertex ID.
This allows us to override the getRecordReader() method in our custom input format to return an instance of our own reader subclass.
The record reader delegates for an instance of the Hadoop LineReader and is responsible for creating the vertices from the text lines seen in each input split.
From here we can override getCurrentVertex() and create individual vertices from each incoming RDF triple seen by the line reader.
By extending TextVertexReader we don't have to worry about manually controlling the invocation of getCurrentVertex() for each line.
We simply need to tell the framework how to turn each line of text into a vertex with one or more edges.
From left to right, they provide the concrete type information for the vertex ID class, vertex value class, edge value class, and message class.
A quick look at the parent class shows the following generic header:
We set up several static final regex patterns to split the RDF triples properly.
The combination firstname/lastname becomes our vertex ID stored as a Text instance.
Each vertex is initialized with a vertex value of 0 stored as an IntWritable.
Each subordinate listed in the comma-delimited list is referenced as an edge ID; however, we don't need any direct value information for each edge, and thus NullWritable will suffice for the edge value.
For this particular job, our message types will be IntWritable.
This class is reused in the next recipe in this chapter titled Using Apache Giraph to perform a distributed breadth-first search.
For the sake of brevity, this input format is only explained once here.
We implement the Tool interface and define four arguments to read from the command line.
This job requires an input folder from HDFS, and an output folder to write back to HDFS, a source ID to perform single source shortest-path, and a ZooKeeper quorum to manage the job state.
Then we need to define a few other parameters as we are testing against a pseudo-distributed cluster with limited resources.
By default, this is set to true, but since we are on a pseudo-distributed single node setup, this needs to be false.
Turning off superstep counters will limit the verbosity of the MapReduce WebUI for our job.
This is can be handy when testing jobs involving hundreds or potentially thousands of supersteps.
Lastly, we turn off checkpointing to tell Giraph that we do not care about backing up the graph state at any superstep.
This works because we are only testing and are interested in rapid job execution time.
In a production job, it is recommended to checkpoint your graph state regularly at the cost of a slower overall job runtime.
We then instantiate an instance of GiraphJob and pass our configuration instance to it along with a somewhat descriptive title for the job.
The next three lines of code are critical for the Giraph job to execute properly on your cluster.
The first line tells Giraph about our custom Vertex implementation that encapsulates each vertex in the graph.
We extend the base class EdgeListVertex to leverage some pre-existing code for message handling, edge iteration, and member serialization.
Then, we set the ZooKeeper quorum and define a single worker to hold the graph partition.
If your pseudo-distributed cluster can support multiple workers (multiple concurrent map JVMs), then feel free to increase this limit.
Just remember to leave one free map slot for the master process.
Finally, we are ready to submit the job to the cluster.
After the InputFormat handles creating the vertices from the different input splits, each vertex's compute() function gets invoked.
Specifically, we are interested in the minimum number of hops required to navigate from the source vertex to every other vertex connected by one or more pathways in the graph, or no path if the target vertex is not reachable by the source.
As incoming messages are received, each vertex compares the integer contained in each message against the currently held minimum to see if it represents a lower value, therefore the business logic is made a bit easier by setting the initial minimum value to the maximum possible for the datatype.
During the first superstep, it is critical that the source vertex sends a message to its edges telling the vertex along that the edge is one hop away from the source.
To do this, we define a member instance msg just for messaging.
It is reset and reused every time the vertex needs to send a message, and helps to avoid unnecessary instantiation.
We need to compare any incoming messages with the currently held minimum hops value to see if we need to update and notify our edges.
Since we are only at S0 there are no messages, so the value remains as Integer.MAX.
Since the minimum value does not change, we avoid the last conditional branch.
At the end of each superstep for our job, always invoke voteToHalt()
The Giraph framework will automatically have reactive vertices that have incoming messages at the next superstep, but we want to render vertices inactive that are temporarily done sending/ receiving messages.
Once there are no more messages to process by any vertex in the graph, the job will stop reactivating vertices and will consider itself finished.
Second superstep (S1) After the previous superstep, every single vertex in the graph voted to halt the execution.
The only vertex that messaged its edges was the source vertex, therefore the framework will reactivate only the vertices connected by the source edges.
The source vertex told each edge that they were one hop away, which is less than Integer.MAX and immediately takes the place as the current vertex value.
Each vertex receiving the message turns around and notifies its edges that they are min + 1 hops away from the source, and the cycle continues.
Should any connected edge receive a message lower than its current vertex value, that indicates there is a path from the source ID to the current vertex that involves fewer hops, and we need to re-notify each edge connected to the current vertex.
Eventually, every vertex will know its minimum distance and no more messages will be sent at the current superstep N.
When starting superstep N + 1, there will be no vertices that need to be reactivated to process incoming messages, and the overall job will finish.
Now we need to output each vertex's current value denoting the minimum number of hops from the source vertex.
We set a Text member variable valOut to reuse while outputting the integer values as strings.
The framework automatically handles invoking writeVertex() for each vertex contained in our dataset.
If the current vertex value is still equal to Integer.MAX, we know that the graph never received any incoming messages intended for that vertex, which implies it is not traversable by the source vertex.
Otherwise, we output the minimum number of hops required to traverse to the current vertex ID from the source ID.
See also f Using Apache Giraph to perform a distributed breadth-first search.
In this recipe, we will use the Apache Giraph API to implement a distributed breadth-first search to determine if two employees are connected in the company's network via one or more pathways.
The code will rely on message passing between employee vertices to determine if a vertex is reachable.
Getting ready Make sure you have a basic familiarity with Google Pregel/BSP and the Giraph API.
The code listed in this recipe uses a split master worker configuration that is not ideal in fully-distributed environments.
You will need to load the example dataset gooftech.tsv to an HDFS folder located at / input/gooftech.
You will also need to package this code inside a JAR file to be executed by the Hadoop JAR launcher from the shell.
The shell script listed in the recipe will show a template for job submission with the correct classpath dependencies.
Carry out the following steps to perform a breadth-first search in Giraph:
The job setup code, vertex class, and custom output format are all contained in a single class.
To use the alias emp_breadth_first, your custom JAR file must use the Hadoop Driver class as its main class in the JAR file.
You should see the job submitted to the Hadoop cluster.
Upon successful completion, you should see a single part file under /output/gooftech saying Valery Dorado is not connected to Gertha Linda.
To understand how the custom InputFormat and job setup works, check out the How it works… section from the recipe titled Single-source shortest-path using Apache Giraph.
This recipe uses exactly the same input format, and the same job setup, except for the following differences:
This is explained in more detail in the following paragraph.
Starting at the first superstep, we send a message to each edge from the source ID.
If we find the supplied source IDs and destination IDs in the dataset vertices, we increment the counters to let the user know.
This will help us quickly see any incorrectly entered command-line arguments for source/destination vertex IDs.
If during any superstep a vertex receives a message, we forward that message along to each of its edges.
If the destination vertex ever receives a message, we set its value to the integer 1 contained in the message.
The framework handles this automatically by calling the writeVertex() method for each vertex in our dataset.
For this job, we are only interested in printing out whether or not the source vertex is connected by one or more paths to the destination vertex.
If the current vertex we are processing is the destination vertex, we will printout one of two strings.
If the vertex value is greater than 0, then that destination must have received one or more messages, which is only possible if there exists at least one path of edge communication between the source and destination.
Otherwise, if the value of the destination vertex is still 0, then we can safely assume that it is not reachable by the source.
For just one pair of source-destination nodes, as we have in this recipe, we could have placed this business logic directly in the job class and used counters after the execution finished, but this design is more extensible should we want to use this code to query multiple destination-source vertex pairs.
Programs designed using the Hadoop MapReduce API usually require some additional tuning once you begin testing at scale.
It is not uncommon to completely re-evaluate a chosen design pattern that simply does not scale.
Working with the Giraph API requires the same diligence and patience.
Apache Giraph jobs often require scalability tuning This is not always easy to spot initially.
You may have a relatively small graph that operates very well within a given BSP design approach.
Suddenly you hit scale and notice all sorts of errors you never planned for.
Try to keep your compute() function small to avoid complications and aid with troubleshooting.
At the time of this writing, Giraph workers will attempt to hold their assigned graph partitions directly in memory.
Moreover, many people have to tune their message passing settings using the parameters located at the top of GiraphJob.
By default, Giraph will let each worker open a communication thread to every other worker in the job.
The default value 2000 may not be adequate for your job.
Collaborative filtering with Apache Mahout Collaborative filtering is a technique that can be used to discover relationships between people and items (for example, books and music)
It works by examining the preferences of a set of users, such as the items they purchase, and then determines which users have similar preferences.
Recommender systems are used by many companies including Amazon, LinkedIn, and Facebook.
In this recipe, we are going to use Apache Mahout to generate book recommendations based on a dataset containing people's book preferences.
Getting ready You will need to download, compile, and install the following:
Once you have compiled Mahout, add the mahout binary to the system path.
In addition, you must set the HADOOP_HOME environment variable to point to the root folder of your Hadoop installation.
You can accomplish this in the bash shell by using the following commands:
Next, extract the Book-Crossing Dataset to the folder you are currently working on.
Carry out the following steps to perform Collaborative filtering in Mahout:
Run the Mahout recommender using the ratings and user information we just put into HDFS.
The first steps of this recipe required us to clean up the Book-Crossing dataset.
The Mahout recommendation engine expects the input dataset to be in the following comma-separated format:
We stripped away most of the information in the BX-Users.csv file, except for USER_ID.
Mahout will launch a series of MapReduce jobs to determine the book recommendations for a given set of users, specified with the –usersFile flag.
In this example, we wanted Mahout to generate book recommendations for all of the users in the dataset, so we provided the complete USER_ID list to Mahout.
In addition to providing an input path, output path, and user list as command-line arguments to Mahout, we also specified a fourth parameter -s SIMILARITY_ LOGLIKELIHOOD.
The -s flag is used to specify which similarity measure we want Mahout to use, to compare similar book preferences across all users.
This recipe used log likelihood because it is a simple and effective algorithm, but Mahout supports many more similarity functions.
To see for yourself, run the following command, and examine the options for the -s flag:
Clustering with Apache Mahout Clustering is a technique that can be used to divide a dataset into related partitions.
In this recipe, we are going to use a specific cluster method called k-means.
K-means clustering attempts to divide a dataset into k clusters by minimizing the distance between points located around a central point in a cluster.
In this recipe, we will use the Apache Mahout k-means implementation to cluster the words found in Shakespeare's tragedies.
Getting ready You will need to download, compile, and install the following:
Extract the contents of shakespeare.zip into a folder named shakespeare_text.
Carry out the following steps to perform clustering in Mahout:
To check the clusters identified by Mahout, use the following command:
For example, following are the top terms for the Romeo and Juliet cluster identified by the k-means algorithm:
The initial steps required us to do some pre-processing on the raw text data prior to running the k-means algorithm with Mahout.
The seqdirectory tool, simply converts the contents of a HDFS folder into SequenceFiles.
Next, the seq2sparse tool converts the newly created SequenceFiles (which still contain text), into document vectors.
The arguments to seq2sparse are described in the following list:
We set this to a high number because we only want to keep the most significant terms.
Use this setting to discard meaningless words (For example, words such as at, a, and the)
In this recipe, we used 1, which means that a term only needs to appear in one document to be processed.
The other option is TF, which would not help us identify key n-grams.
The tokens will be kept, combined, or discarded based on the other flags provided to the seq2sparse application.
Finally, we ran the k-means clustering algorithm on the Shakespeare dataset.
Mahout will launch a series of MapReduce jobs, which are configurable.
The k-means job will complete when either the k-means clusters converge, or the maximum allowed number of MapReduce jobs has been reached.
The following are definitions of the parameters we used to configure the k-means Mahout job:
We chose 6, because there were six Shakespeare documents, and we wanted to identify significant bigrams around those documents.
Sentiment classification is a classification process that tries to determine a person's propensity to like or dislike certain items.
In this recipe, we will use a naive Bayes classifier from Apache Mahout to determine if a set of terms found in a movie review mean the movie had a negative or positive reception.
Getting ready You will need to download, compile, and install the following:
Within that folder there should be two more folders named pos and neg.
The pos and neg folders hold text files containing the written reviews of movies.
Obviously, the pos folder contains positive movie reviews, and the neg folder contains negative reviews.
This application will read and write to the local filesystem, and not HDFS.
The testclassifier tool should return a similar summary and confusion matrix.
The numbers will not be exactly the same as the ones shown in the following:
The first two steps required us to prepare the data for the Mahout naive Bayes classifier.
So, instead of having 1000 positive and negative files, where each file contained a single review, we now have two files named pos.txt and neg.txt, where each contains all of the positive and negative reviews.
Mahout trains the classifier by launching a series of MapReduce jobs.
It is important to note that this statistic does not mean the classifier will be accurate 71.25 percent of the time for every movie review ever.
There are a number of ways in which classifiers can be trained and validated.
The testclassifier tool we used in step 6, did not run a MapReduce job.
If we wanted to test the classifier using MapReduce, we just need to change the -method parameter to mapreduce.
Introduction There is an adage among those working with Hadoop that everything breaks at scale.
It's an unfortunate downside of working with large amounts of unstructured data.
Within the context of Hadoop, individual tasks are isolated and given different sets of input.
This allows Hadoop to easily distribute jobs, but leads to difficulty in tracking global events and understanding the state of each individual task.
Fortunately, there are several tools and techniques available to aid in the process of debugging Hadoop jobs.
This chapter will focus on applying these tools and techniques to debug MapReduce jobs.
Using Counters in a MapReduce job to track bad records.
The MapReduce framework provides Counters as an efficient mechanism for tracking the occurrences of global events within the map and reduce phases of jobs.
For example, a typical MapReduce job will kick off several mapper instances, one for each block of the input data, all running the same code.
These instances are part of the same job, but run independent of one another.
Counters allow a developer to track aggregated events from all of those separate instances.
A more concrete use of Counters can be found in the MapReduce framework itself.
The output of these Counters can be found in the job details of the Job Tracker web UI.
The UI shows the Counter group, name, mapper totals, reducer totals, and job totals.
Counters should be limited to tracking metadata about the job.
The Map input records counter provides useful information about a particular execution of a job.
If Counters did not exist, these kinds of statistics would have to be part of the job's main output, where they don't belong; or more likely as part of a secondary output, complicating the logic of the job.
The following recipe is a simple map-only job that filters out bad records and uses a counter to log the number of records that were filtered.
The most recent jobs are at the bottom of the table.
This page has high-level statistics about the job, including the Counters.
In the CounterExample job, an Enum for tracking the count of each type of bad record was defined:
In the map function, there are two checks for valid data.
For this example, if properly formed, each record should have five columns.
As its name implies, this regular expression will match valid IP addresses.
The VALID_IP_ADDRESS regular expression is used to check every record's IP address column for a match.
For each record that does not match, the INVALID_IP_ADDRESS counter is incremented.
Each increment of a counter is first stored locally by each mapper.
The counter values are then sent to the Task Tracker for a second level of aggregation.
Finally, the values are sent to the Job Tracker where the global aggregation takes place.
In the map phase, each input record has a function applied to it, resulting in one or more key-value pairs.
The reduce phase receives a group of the key-value pairs and performs some function over that group.
Testing mappers and reducers should be as easy as testing any other function.
The complexities arise due to the distributed nature of Hadoop.
Prior to the release of MRUnit by Cloudera, even the simplest tests running in local mode would have to read from the disk and take several seconds each to set up and run.
MRUnit removes as much of the Hadoop framework as possible while developing and testing.
The focus is narrowed to the map and reduce code, their inputs, and expected outputs.
With MRUnit, developing and testing MapReduce code can be done entirely in the IDE, and these tests take fractions of a second to run.
This recipe will demonstrate how MRUnit uses the IdentityMapper provided by the MapReduce framework in the lib folder.
The IdentityMapper takes a key-value pair as input and emits the same key-value pair, unchanged.
Have the IdentityMapperTest class extend the TestCase class: public class IdentityMapperTest extends TestCase.
Create two private members of mapper and driver: private Mapper identityMapper; private MapDriver mapDriver;
MRUnit is built on top of the popular JUnit testing framework.
It uses the object-mocking library, Mockito, to mock most of the essential Hadoop objects so the user only needs to focus on the map and reduce logic.
The withInput() method is called to provide input to the Mapper class that the MapDriver class was instantiated with.
The withOutput() method is called to provide output to validate the results of the call to the Mapper class.
The call to the runTest() method actually calls the mapper, passing it the inputs and validating its outputs against the ones provided by the withOutput() method.
MRUnit also provides a ReduceDriver class that can be used in the same way as MapDriver for testing reducers.
See also f For more information on Mockito, visit http://code.google.com/p/mockito/
MRUnit provides an elegant way to test the map and reduce phases of a MapReduce job.
Initial development and testing of jobs should be done using this framework.
However, there are several key components of a MapReduce job that are not exercised when running MRUnit tests.
Running jobs in local mode will test a larger portion of a job.
When testing in local mode, it is also much easier to use a significant amount of real-world data.
This recipe will show an example of configuring Hadoop to use local mode and then debugging that job using the Eclipse debugger.
Open the Counters project in Eclipse, and set up a new remote debug configuration.
A MapReduce job that is configured to execute in local mode runs entirely in one JVM instance.
Unlike the pseudo-distributed mode, this mode makes it possible to hook up a remote debugger to debug a job.
The LocalJobRunner class, which is used when running in local mode, is responsible for implementing the MapReduce framework locally in a single process.
This has the benefit of keeping jobs that run in local mode as close as possible to the jobs that run distributed on a cluster.
One downside to using LocalJobRunner is that it carries the baggage of setting up an instance of Hadoop.
This means even the smallest jobs will require at least several seconds to run.
Apache Pig also provides a local mode for development and testing.
It uses the same LocalJobRunner class as a local mode MapReduce job.
It can be accessed by starting Pig with the following command:
See also f Developing and testing MapReduce jobs with MRUnit.
When working with the amounts of data that Hadoop was designed to process, it is only a matter of time before even the most robust job runs into unexpected or malformed data.
If not handled properly, bad data can easily cause a job to fail.
For some applications, it may be acceptable to skip a small percentage of the input data.
Even if skipping data is not acceptable for a given use case, Hadoop's skipping mechanism can be used to pinpoint the bad data and log it for review.
To enable the skipping of 100 bad record groups in a reduce job, add the following to the run() method where the job configuration is set up:
The process to skip bad records will trigger if skipping has been enabled.
Skipping is enabled by calling the static methods on the SkipBadRecords class and once a task has failed twice.
Hadoop will then perform a binary search through the input data to identify the bad records.
Keep in mind that this is an expensive task that could require multiple attempts.
A job that enables skipping will probably want to increase the number of map and reduce attempts.
By default, the process to skip bad records will be triggered after two failed attempts.
By default, skipped records will be logged to the _log/skip/ folder.
To get them into human-readable format, use the following command:
The number of Task attempts after which skip mode will be kicked off.
When skip mode is kicked off, the task reports the range of records that it will process next to the TaskTracker.
This is so that, on failures, the TaskTracker knows which ones are possibly the bad records.
This value must be set to false for applications that process the records asynchronously or buffer the input records.
In such cases, applications should increment this counter on their own.
This value must be set to false for applications that process the records asynchronously or buffer the input records.
In such cases, applications should increment this counter on their own.
User can stop writing skipped records by giving the value none.
This is the number of acceptable skip records surrounding the bad record per bad record in the mapper.
The framework tries to narrow down the skipped range by retrying until this threshold is met or all attempts get exhausted for this task.
Set the value to the value of Long.MAX_VALUE to indicate that the framework need not try to narrow down.
Whatever records (depends on the application) get skipped, are acceptable.
This is the number of acceptable skip groups surrounding the bad group per bad group in the reducer.
The framework tries to narrow down the skipped range by retrying until this threshold is met or all attempts get exhausted for this task.
Set the value to the value of Long.MAX_VALUE to indicate that the framework need not try to narrow down.
Whatever groups (depends on the application) get skipped, are acceptable.
Using Counters in a streaming job Hadoop is not limited to running MapReduce jobs written in Java or other JVM languages.
Using the streaming interface, any application that can read and write to stdin and stdout can be used in a MapReduce job.
Since streaming jobs do not have access to the Hadoop Java classes, different approaches need to be taken to get access to the framework's features.
One convenient and extremely useful feature provided by Hadoop is Counters.
This recipe will use a simple Python program to show how to increment a counter from a streaming application.
The Python code does not have direct access to the Java Reporter class used by the Hadoop framework for working with Counters.
Instead, it will write data to stderr in a format that has special meaning.
The Hadoop framework will interpret this as a request to increment the specified counter.
Complete the following steps to execute a Hadoop streaming job using the streaming_ counters.py program:
The most recent jobs are at the bottom of the table.
The Hadoop framework constantly monitors stderr for entries that fit the following format:
If it finds a string that matches this format, the Hadoop framework will check to see if that group and counter exists.
If they do exist, the current value will be incremented by that value.
If they do not exist, the group and counter will be created and set to that value.
The Python code performs two validation checks on the weblog data.
If a line has less than five columns, the program will write to stderr in the format that Hadoop expects for manipulating the Counter.
Similarly, the second validation verifies the IP address of each record and increments a counter each time an invalid IP address is found.
Streaming jobs also have access to setting the task's status message using the same basic method.
Writing to stderr in the following format will update a task's status, setting it to message.
See also f Using Counters in a MapReduce job to track bad records.
Along with maintaining counters, another role of the Reporter class in Hadoop is to capture task status information.
The task status information is periodically sent to the Job Tracker.
The Job Tracker UI is updated to reflect the current status.
When debugging a MapReduce job, it can be useful to display a custom message that gives more detailed information on how the task is running.
Getting ready f Download the source code for this chapter.
Updating a task's status message can be done using the setStatus() method of the job's Context class.
The source code for this chapter provides an example of using a custom task status message to display the number of rows being processed per second by the task.
Two private class variables are declared: rowCount for keeping track of the number of rows that are processed and startTime for keeping track of the time when processing started.
Once the map function has processed 100,000 lines, the task status is updated with the number of rows per second that are being processed.
After the message has been updated, the rowCount and startTime variables are reset and the process starts over again.
The status is stored locally in the memory of the current process.
The next time the Task Tracker pings, the Job Tracker is also sent the updated status message.
Once the Job Tracker receives the status message, this information is made available to the UI.
Using illustrate to debug Pig jobs Generating good test data for a complex distributed job that joins, filters, and aggregates gigabytes or even terabytes of data can be one of the hardest parts of the development process, or at least one of the most tedious.
Apache Pig provides an incredibly powerful tool, illustrate, that will seek out cases from the provided full input data that exercise different dataflow paths.
The following recipe shows an example of the illustrate command in use.
Getting ready Apache Pig 0.10 or a more recent version must be installed.
The following Pig code will show an example of a record with a malformed IP address:
In the preceding example, data is filtered on invalid IP addresses.
The number of records that have an invalid IP address make up a small percentage of the total.
If a traditional sampling approach was taken to create test data, chances are that the sampled data would not contain any records with an invalid IP address.
The illustrate algorithm makes four complete passes over a Pig script to generate its data.
The first pass takes a sample of data from each input and sends it through the script.
The second pass finds and removes records that followed the same path through the script.
The third pass determines if any possible paths were not taken by the sampled data from the first pass.
If there are paths that are not represented by the sampled data, the illustrate algorithm will create fake data that exercises the remaining paths.
The fourth pass is similar to the second pass; it removes any redundant data created by the third pass.
See also f To learn about generating example data for dataflow programs, visit.
Introduction This chapter will discuss how to maintain, monitor, and tune a Hadoop cluster and MapReduce jobs.
We will review the various Hadoop modes of operations, describe how to resolve problems within the Hadoop cluster, and finally review some important job tuning parameters.
Starting Hadoop in pseudo-distributed mode Hadoop supports three different operating modes:
This recipe will describe how to install and set up Hadoop to run in pseudo-distributed mode.
In pseudo-distributed mode, all of the HDFS and MapReduce processes will start on a single node.
Pseudo-distributed mode is an excellent environment to test your HDFS operations and/or your MapReduce applications on a subset of the data.
Getting ready Ensure that you have Java 1.6, ssh, and sshd installed.
In addition, the ssh daemon (sshd) should be running on the node.
You can validate the installation of these applications by using the following commands:
Carry out the following steps to start Hadoop in pseudo-distributed mode:
This is not specifically required to get Hadoop running in pseudo-distributed mode, but it is a common and good security practice.
Generate an ssh public and private key pair to allow password-less login to the node using the Hadoop user account.
When asked for a passphrase, hit the Enter key, ensuring no passphrase will be used: $ su – hadoop.
If you have more than one node, you will need to copy this key to every node in the cluster.
You should be able to ssh to localhost using your hadoop account without providing a password: $ ssh localhost.
Start all of the Hadoop HDFS and MapReduce services: $ bin/start-all.sh.
You can stop all of the Hadoop services by running the bin/stop-all.sh script.
Next, we downloaded a distribution of Hadoop and configured the distribution to run in pseudo-distributed mode.
Next, we set the replication factor of HDFS to 1 using the dfs.replication property.
Since we are running all of the Hadoop services on a single node, there is no need to replicate any information.
If we did, all of the replicated information would reside on the single node.
Finally, we formatted the NameNode and started the Hadoop services.
You need to format the NameNode after you set up a new Hadoop cluster.
Formatting a NameNode will erase all of the data in the cluster.
By default, the Hadoop distribution comes configured to run in standalone mode.
In standalone mode, there is no need to start any Hadoop service.
In addition, input and output folders will be located on the local filesystem, instead of HDFS.
To run a MapReduce job in standalone mode, use the configuration files that initially came with the distribution.
Create an input folder on the local filesystem and use the Hadoop shell script:
Starting Hadoop in distributed mode As mentioned in the previous recipe, Hadoop supports three different operating modes:
This recipe will describe how to set up Hadoop to run in fully-distributed mode.
In fullydistributed mode, HDFS and the MapReduce services will run across multiple machines.
A typical architecture is to have a dedicated node run the NameNode and the JobTracker services, another dedicated node to host the Secondary NameNode service, and the remaining nodes in the cluster running both the DataNode and TaskTracker services.
There should be a user named hadoop on every node in the cluster.
In addition, the rsa public key generated in step 2 of the previous recipe must be distributed and installed on every node in the cluster using the ssh-copy-id command.
Finally, the Hadoop distribution should be extracted and deployed on every node in the cluster.
We will now discuss the specific configurations required to get the cluster running in distributed mode.
We will assume that your cluster will use the following configuration:
Perform the following steps to start Hadoop in fully-distributed mode:
Update the masters and slaves configuration files on the head node.
The masters configuration file contains the hostname of the node which will run the Secondary NameNode.
The slaves configuration file contains a list of the hosts which will run the TaskTracker and DataNode services: $ vi conf/masters.
Format the Hadoop NameNode from the head node: $ bin/hadoop namenode –format.
From the head node as the hadoop user, start all of the Hadoop services: $ bin/start-all.sh.
Confirm that all of the correct services are running on the proper nodes:
On the master: Both the NameNode and JobTracker services should running.
On the secondary: The Secondary NameNode service should be running.
On the worker nodes: The DataNode and TaskTracker services should be running.
First we changed the Hadoop configuration files core-site.xml, hdfs-site.xml, and mapred-site.xml on every node in the cluster.
These configuration files need to be updated to tell the Hadoop services running on every node where to find the NameNode and JobTracker services.
It is not necessary to run the Secondary NameNode on a separate node.
You can run the Secondary NameNode on the same node as the NameNode and JobTracker, if you wish.
To do this, stop the cluster, modify the masters configuration file on the master node, and restart all of the services:
Another set of configuration parameters that will come in handy when your cluster grows or when you wish to perform maintenance, are the exclusion list parameters that can be added to the mapred-site.xml configuration file.
These configuration parameters will be used later when we discuss decommissioning of a node in the cluster:
See also f Adding new nodes to an existing cluster.
Adding new nodes to an existing cluster Hadoop supports adding new nodes to an existing cluster without shutting down or restarting any service.
This recipe will outline the steps required to add a new node to a pre-existing cluster.
Getting ready Ensure that you have a Hadoop cluster up and running.
In addition, ensure that you have the Hadoop distribution extracted, and the configuration files have been updated with the settings from the recipe titled Starting Hadoop in distributed mode.
We will use the following terms for our imaginary cluster:
Follow these steps to add new nodes to an existing cluster:
From the head node, update the slaves configuration file with the hostname of the new node: $ vi conf/slaves.
Log in to the new node and start the DataNode and TaskTracker services:
We updated the slaves configuration file on the head node to tell the Hadoop framework that a new node exists in the cluster.
However, this file is only read when the Hadoop services are started (for example, by executing the bin/start-all.sh script)
In order to add the new node to the cluster without having to restart all of the Hadoop services, we logged into the new node, and started the DataNode and TaskTracker services manually.
The DataNode and TaskTracker services will automatically start the next time the cluster is restarted.
When you add a new node to the cluster, the cluster is not properly balanced.
To rebalance the existing data in the cluster, you can run the following command from the head node:
Imagine, we might be moving terabytes of data around, depending on the number of nodes added to the cluster.
Job performance issues might arise when a cluster is in the process of rebalancing, and therefore regular rebalancing maintenance should be properly planned.
Safely decommissioning nodes The practice of removing nodes from a Hadoop cluster is very common.
Hardware might have failed, or machines might need to be upgraded.
In this recipe, we will show you the steps to safely remove a worker node from a Hadoop cluster.
Getting ready It is assumed that your cluster is up and running and you have configured the following properties in mapred-site.xml:
Perform the following steps to decommission a node in the Hadoop cluster:
Notify the NameNode to re-read the exclude list and disconnect the worker node which will be decommissioned: $ hadoop dfsadmin –refreshNodes.
Notify the JobTracker to re-read the exclude list and disconnect the worker node which will be decommissioned: $ hadoop mradmin –refreshNodes.
Next, we issued the hadoop dfsadmin –refreshNodes command to notify the NameNode to disconnect from all of the hosts listed in the dfs_excludes file.
Similarly, we issued the hadoop mradmin refreshNodes command to notify the JobTracker to stop using the TaskTrackers on the nodes listed in the mapred_excludes file.
Recovering from a NameNode failure The NameNode is the single most important Hadoop service.
It maintains the locations of all of the data blocks in the cluster; in addition, it maintains the state of the distributed filesystem.
When a NameNode fails, it is possible to recover from a previous checkpoint generated by the Secondary NameNode.
It is important to note that the Secondary NameNode is not a backup for the NameNode.
The data is almost certainly stale when recovering from a Secondary NameNode checkpoint.
However, recovering from a NameNode failure using an old filesystem state is better than not being able to recover at all.
Getting ready It is assumed that the system hosting the NameNode service has failed, and the Secondary NameNode is running on a separate machine.
This property tells the Secondary NameNode where to save the checkpoints on the local filesystem.
Carry out the following steps to recover from a NameNode failure:
Bring up a new machine to act as the new NameNode.
This machine should have Hadoop installed, be configured like the previous NameNode, and ssh password-less login should be configured.
In addition, it should have the same IP and hostname as the previous NameNode.
Verify that the NameNode started successfully by looking at the NameNode status page http://head:50070/
We first logged into the Secondary NameNode and stopped the service.
Next, we set up a new machine in the exact manner we set up the failed NameNode.
Next, we copied all of the checkpoint and edit files from the Secondary NameNode to the new NameNode.
This will allow us to recover the filesystem status, metadata, and edits at the time of the last checkpoint.
Recovering using the old data is unacceptable for certain processing environments.
Instead, another option would be to set up some type of offsite storage where the NameNode can write its image and edits files.
This way, if there is a hardware failure of the NameNode, you can recover the latest filesystem without resorting to restoring old data from the Secondary NameNode snapshot.
The first step in this would be to designate a new machine to hold the NameNode image and edit file backups.
Finally, modify the hdfs-site.xml file on the server running the NameNode to write to the local filesystem and the backup machine mount:
Now the NameNode will write all of the filesystem metadata to both /path/to/hadoop/ cache/hadoop/dfs and the mounted /path/to/backup folders.
Monitoring cluster health using Ganglia Ganglia is a monitoring system designed for use with clusters and grids.
Hadoop can be configured to send periodic metrics to the Ganglia monitoring daemon, which is useful for diagnosing and monitoring the health of the Hadoop cluster.
This recipe will explain how to configure Hadoop to send metrics to the Ganglia monitoring daemon.
Getting ready Ensure that you have Ganglia Version 3.1 or better installed on all of the nodes in the Hadoop cluster.
The Ganglia monitoring daemon (gmond) should be running on every worker node in the cluster.
You will also need the Ganglia meta daemon (gmetad) running on at least one node, and another node running the Ganglia web frontend.
The following is an example with modified gmond.conf file that can be used by the gmond daemon:
Also, ensure that the Ganglia meta daemon configuration file includes your cluster as a data source.
For example, modify the gmeta.conf configuration file to add the Hadoop cluster as a data source:
Perform the following steps to use Ganglia to monitor cluster metrics:
This property file will need to be updated for every node in the cluster.
Verify that Ganglia is collecting Hadoop metrics via the Ganglia web frontend.
The Ganglia monitoring daemon (gmond) is responsible for collecting metric information from the nodes where it is installed.
Finally, the Ganglia web frontend will request the aggregated metrics in the form of XML from the gmetad daemon and report that to users via the web interface.
Tuning MapReduce job parameters The Hadoop framework is very flexible and can be tuned using a number of configuration parameters.
In this recipe, we will discuss the function and purpose of different configuration parameters you can set for a MapReduce job.
Getting ready Ensure that you have a MapReduce job which has a job class that extends the Hadoop Configuration class and implements the Hadoop Tool interface, such as any MapReduce application we have written so far in this book.
Examine the following table of Hadoop job properties and values:
Boolean (true/false) Tells the Hadoop framework to speculatively launch the exact same map task on different nodes in the cluster if a task is not performing well as compared to other tasks in the job.
Boolean (true/false) Tells the Hadoop framework to speculatively launch the exact same reduce task on different nodes in the cluster if a task is not performing well as compared to other tasks in the job.
Setting this parameter might help increase the performance of small jobs because JVMs will be re-used for multiple tasks (as opposed to starting a JVM for each and every task)
These three parameters are used to compress the output of map tasks.
These three parameters are used to compress the output of a MapReduce job.
For example, we will launch a job using five reducers:
When a job class extends the Hadoop Configuration class and implements the Hadoop Tool interface, the ToolRunner class will automatically handle the following generic Hadoop arguments:
Argument/Flag Purpose -conf Takes a path to a parameter configuration file.
In the case of this recipe, the ToolRunner class will automatically place all of the parameters specified with the -D flag into the job configuration XML file.
Introduction Storage of big data is a topic of ever-increasing popularity.
Software projects facing concerns over data scalability frequently find themselves having to shell out top dollar for expensive RDBMS commercial licenses, or worse, having to rely on solutions in which scalability was an afterthought.
In the last couple of years, we have seen the introduction of many viable open source database solutions to help manage massive amounts of structured and unstructured data.
Apache Accumulo was inspired by the Google BigTable design approach, and offers scalable, distributed columnar persistence of data backed over Apache Hadoop.
The Google BigTable design is explained in detail at http://research.google.com/archive/ bigtable.html.
This chapter will show several recipes that tackle common database query/load tasks, and also shows how many of Accumulo's unique features help to streamline the implementation.
Designing a row key to store geographic events in Accumulo.
The Armed Conflict Location Event Data (ACLED) dataset is a collection of individual events that occurred across a wide range of geographic areas.
This recipe will show how we can leverage Accumulo's sorted key ordering to group ACLED event records into geographic ranges.
Furthermore, each geographic range will be subgrouped in the descending order of event occurrence.
Specifically, the code in this recipe shows the generation logic that we can turn around and use to build ACLED keys from our records.
To verify that the key generator works as expected, we will build and run unit tests with some sample row data.
Some basic familiarity with the TestNG testing API will help make sense of the unit tests.
This recipe makes use of a specific type of quadtree data structure that is useful for grouping geospatial data into indexed ranges.
Follow these steps to implement a geospatial, reverse chronological row key generator:
This code will serve as the basis for generating geospatial/reverse chronological row keys.
It exists as an independent component outside of any code that loads data to Accumulo.
It is designed specifically to build row keys that will sort in a very particular order once persisted to an Accumulo table.
All implementing classes must fulfill a simple contract for getRowID()
It takes an array of arbitrary strings and returns a single string representing the rowID.
We then start to build the Z-order structure necessary for the rowID strategy.
The getZOrderedCurve() method takes lat and lon strings as arguments.
A point is not required to contain a decimal portion, but it must at least contain an integral portion.
Moving on, we offset each point by +90 to avoid negative numbers, which would otherwise corrupt the Z-order interpretation.
If after these checks our point is still considered valid, it is time to start formatting it for proper Z-order interpretation.
This will make more sense when we examine how getZOrderCurve() uses the result.
If applicable, add the fractional portion back to the reformatted string representation without the decimal place.
Once both latitude and longitude points have been properly formatted, we are ready to start shuffling the numbers to build the quadtree.
As our loop control variable, we'll take the greater of the two lengths when comparing latitude and longitude, and use that as our loop's max variable.
As we cycle through i going from 0 to max, we start with lat and print the ith character, followed by the ith character of lon.
Should we reach the length of lat before lon, or vice versa, print 0 for an interleaved spot in the iteration.
The general idea is to interleave the points such that the most significant digits are arranged in the left-to-right order.
Accumulo will maintain the sorted order of our geospatial keys using lexicographical byte-order sorting.
This means that the points pertaining to similar geographic regions will be arranged contiguously for effective range scans.
We can quickly look up the given points for a particular lat/lon bounding region by building the offset Z-order representation of both the lower and bound bounding parameters and setting the start and end key ranges to the lower and upper bounding parameters respectively.
This is why it is critical to zero-pad the integral portion of the lat/lon points.
When storing ACLED event data, we would like to arrange events that lie in similar lat/lon regions in reverse chronological order.
The function getReverseTime() achieves this by appending a reverse timestamp for the given item to the already calculated Z-order curve, separated by an underscore token.
Events with the exact same lat/lon values will sort the records in the Accumulo table in the ascending order, but more recent events when converted to milliseconds from epoch will have larger long values.
To counter this, we subtract the long value from the maximum possible long value.
If incoming date strings do not match the simple date format yyyy-MM-dd, throw an exception.
The unit tests are designed to test the error handling of getZOrderCurve() and getReverseTime() as well as validate the expected output.
We run this suite of tests to perform a stress test on our rowID generator before using it to load new ACLED event records into our Accumulo table.
The rowID generation strategy listed in this recipe is designed to accommodate lat/lon geospatially bound queries with an optional time restriction for events.
While this sounds very open-ended, there really is no one-size-fits-all solution for rowIDs when it comes to BigTable-designed columnar datastores.
Depending on the types of queries you wish to perform over Accumulo, your rowID strategy might differ entirely.
The following are a few sections that further expand on the design choices made in this recipe.
This technique plays a key role in the previous recipe.
Z-order curve Z-order curve is a technique to generate quadtrees that represent a flattened, 2-dimensional view of geospatial data.
A more in-depth explanation can be found in Wikipedia at http://en.wikipedia.org/wiki/Z-order_curve.
Specifically, this recipe uses the technique to produce rowIDs that are flexible for range queries involving lat/lon points where the precision of the upper/lower bounding parameters can vary.
The left-to-right placement of significant digits in the rowID means that a shorter Z-order queryID will match on more rows that begin with the supplied queryID pattern than would a longer queryID pattern.
The former, less verbose start- or end-key range will return more rows than the latter, which is what you would expect.
See also f Using MapReduce to bulk import geographic event data into Accumulo.
Using MapReduce to bulk import geographic event data into Accumulo.
This recipe will use MapReduce to load tab-separated ACLED event data directly into an Accumulo table.
The shell script in this recipe assumes that Zookeeper is running on the host localhost on port 2181; you can change this to suit your environment needs.
The Accumulo installation's bin folder needs to be on your environment path.
For this recipe, you'll need to create an Accumulo instance named test with the user as root and password as password (top-notch security, I know…)
It is also highly recommended that you complete the Designing a row key to store geographic events in Accumulo recipe earlier in this chapter.
Follow these steps to bulk load events into Accumulo using MapReduce:
You will need to configure the Accumulo core and Hadoop classpath dependencies.
You should see the job execute in the MapReduce WebUI.
Upon completion, the ACLED data should be available for scan under the table acled in Accumulo.
The program takes in eight arguments, each of which is very important.
The input location is where MapReduce will find the ACLED data.
The output folder is where it will output data in Accumulo's native RFile format.
The string test is our testing Accumulo instance name stored in Zookeeper.
The string acled is our desired table name in Accumulo.
We authenticate with the Accumulo instance using the strings root:password.
For this execution, we supplied one Zookeeper host on localhost:2181
Finally, splits.txt is used to help presplit our newly created acled table.
The program clears any previous folder located in our output location.
For this job, the mapper will output the type Text for both the key and the value.
We'll create the table acled if it does not exist, and will use the assistant to presplit the table based on our locally supplied splits.txt file.
Without presplitting the table at creation time, the RangePartitioner class would force all of the intermediate key-value pairs to a single reducer.
It is much more efficient to create presplit tablets based on expected row-key distribution and to allow multiple reducers to build RFiles in parallel.
Finally, we are ready to submit the job and to examine the map and reduce phases.
See the Designing a row key to store geographic events in Accumulo recipe in this chapter for an in-depth explanation of how this class works.
Our data is tab-delimited and follows a very strict column ordering, thus we can hand-pick the column indices to read the values for lat, lon, and dtg in that respective order.
The key generator needs these fields to make a valid composite geospatial and reverse timestamp rowID.
We output the generated row key with the text value that was read for the line.
This produces a distinct intermediate key for every unique rowID we wish to insert into Accumulo.
The reducer is responsible for taking our generated rowID and reading through any other delimited lines that output an equivalent rowID.
The rowID generator in the map phase builds unique rowIDs based on the composite of lat, lon, and dtg.
By definition, an ACLED event that took place in the exact same lat/lon with the same reverse timestamp, would be grouped to the same intermediate key for the reducer.
However, having multiple ACLED events with the exact same rowID means that we have duplicate entries we wish to ignore.
Therefore, we only want to preserve the first value collected in the Iterable object.
We use a counter to keep track of duplicate occurrences, as well as invalid lines that don't split properly.
Since we are directly writing instances of Key/Value as RFiles, Accumulo requires key/value objects to be inserted in a sorted order.
The rowIDs are naturally the same for each qualifier, and the column family is a static label cf, but it's very important that we maintain lexicographical ordering while considering the write order for our qualifier labels.
Fortunately, our data is very predictable, and we hardcode the column value reads based on the alphabetic ordering of our qualifier labels.
Once the job is finished, and we have all of the RFiles for the presplit tablets, we use the assistant instance to read all of the files produced to the output directly and place them in the appropriate tablet.
The data is immediately available for querying in the acled table in Accumulo.
Here is a bit more explanation on some of the design choices you see in this recipe:
Since it requires five input strings for operation, the Builder pattern was an obvious choice to prevent accidental variable assignment constructions.
See Effective Java 2.0 by Joshua Block for more detail on the Builder design pattern.
It was more to emphasize the importance of presplitting Accumulo tables during creation.
Choosing the right split points really depends on the distribution of your rowID ranges.
Too few split points and job throughput will bottleneck at the reduce stage.
Too many, and you may start to waste resources and spin up time on underutilized reduce task JVMs.
On the other hand, if your MapReduce job is not write-intensive, it can be easier to work directly with Mutation instances instead of RFiles.
See also f Designing a row key to store geographic events in Accumulo.
Setting a custom field constraint for inputting geographic event data in Accumulo.
In this recipe, we will build a custom Constraint class to limit the types of mutations we can apply to event date values in an Accumulo table.
Specifically, we want newly entered values to conform to a particular SimpleDateFormat pattern, but these values should not be in the future according to the system time on the TabletServer.
The shell script in this recipe assumes that Zookeeper is running on the host localhost and on the port 2181; you can change this to suit your environment needs.
The Accumulo installation's bin folder needs to be on your environment path.
For this recipe you'll need to create an Accumulo instance named test with user as root and password as password.
You will need a table by the name acled to exist in the configured Accumulo instance.
It is also highly recommended that you go through the Using MapReduce to bulk import geographic event data into Accumulo recipe of this chapter.
This will give you some sample data with which you can experiment.
Follow these steps to implement and install a constraint in Accumulo:
You will need to configure the Accumulo core and Hadoop classpath dependencies.
Issue the following command to test whether the JAR file is on the Accumulo classpath: $ accumulo classpath.
You should see a constraint error printed to the console indicating Date cannot be in future.
Our Constraint class looks through every mutation and determines if the column qualifier matching dtg is involved.
If the ColumnUpdate object mutates a key-value pair containing the qualifier dtg, examine the value for errors.
If the column update contains a future date, add a constraint violation.
The main class takes all of the required parameters to connect to the Accumulo instance and adds the Constraint class to the table.
It then attempts to perform a mutation on the supplied rowID using the argument value for dtg.
If the mutation is rejected for any reason, print out the constraint violations to see if the DtgConstraint was violated.
We can modify the dtg argument in the shell script to see the different constraint violation errors our class generated.
Constraints are a powerful feature for data policy enforcement in Accumulo.
The following headings discuss a few additional things you should know.
Bundled Constraint classes The Accumulo core offers numerous constraint implementations out of the box.
They cover a variety of common checked conditions and are already on the TabletServer classpath.
Cell visibility and other core system checks in Accumulo use constraint implementations behind the scenes.
Installing a constraint on each TabletServer If after installing a custom constraint to your Accumulo instance, you'll notice every mutation being rejected; it's likely that, for whatever reason, the TabletServer server did not find your Constraint class on the classpath.
This can happen if the table configuration has the Constraint class listed but cannot find a class matching the fully qualified name.
In a fully-distributed setup, make sure to restart every TabletServer after modifying each general classpath.
See also f Using MapReduce to bulk import geographic event data into Accumulo.
This recipe will use the built-in RegExFilter class in Accumulo to return only key-value pairs, where the qualifier is of a particular source value.
The filtering will be distributed across the different TabletServers that house the table acled.
The shell script in this recipe assumes that Zookeeper is running on the host localhost and on the port 2181; you can change this to suit your environment needs.
The Accumulo installation's bin folder needs to be on your environment path.
For this recipe you'll need to create an Accumulo instance named test with the user as root and password as password.
To see the filtered results from this recipe, you will need to complete the Using MapReduce to bulk import geographic event data into Accumulo recipe listed earlier in this chapter.
This will give you some sample data to experiment with.
The script takes in the required parameters necessary to connect to the Accumulo table acled, plus an additional parameter for a source qualifier value to filter on.
We set up a Scanner instance with blank authorizations and configure an IteratorSetting of type RegExFilter to do the regex comparison on the TabletServer.
Our regex is a very simple direct match on the supplied source argument.
We then iterate over the result set and printout the rowID for any matching key-value pairs.
At the end, we print a tally of how many key-value pairs were found matching that source.
The responsibility of filtering key-value pairs based on the value is distributed across the various TabletServers that hold tablets for the acled table.
The client only sees rows that match the filter, and can immediately begin processing.
See also f Using MapReduce to bulk import geographic event data into Accumulo.
Counting fatalities for different versions of the same key using SumCombiner.
This recipe will use the built-in SumCombiner in Accumulo to treat the cell value associated with the qualifier fat as long and for each key in the acled table, to sum the total for all versions of the key.
The shell script in this recipe assumes that Zookeeper is running on the host localhost and on the port 2181; you can change this to suit your environment needs.
The Accumulo installation's bin folder needs to be on your environment path.
For this recipe, you'll need to create an Accumulo instance named test with user as root and password as password.
You will need a table by the name acled to exist in the configured Accumulo instance.
If you have an existing table by that name from an earlier recipe, delete and recreate it.
It is also highly recommended that you complete the Using MapReduce to bulk import geographic event data into Accumulo recipe earlier in this chapter.
This will give you some sample data to experiment with.
Write some sample data with the exact same rowID eventA, column family, and qualifier:
You should now see twice the count for each event:
We write two mutations for two different versions of the same key containing the rowID eventA.
We also write one mutation with the key containing the rowID eventB with a value of 7 for the qualifier fat.
We then use a Scanner instance to apply the SumCombiner at scan time over the key-value pairs in the table.
The combiner's job is to collect different long values associated with the exact same key and emit the sum of those long values.
There is only one key version associated with the rowID eventB, so the single value 7 remains the total sum for that key.
If we re-run this application, the previous mutations are still stored in the same Accumulo table.
The result is double the count from our previous execution.
This happens because at the raw key-value level, our mutations are inserting new key-value pairs to the table with different timestamps every time the application is called.
Our combiner sees all timestamped versions of every distinct key.
Combiners are on a per-key basis, not across all keys This can cause confusion with new Accumulo users.
Combiners use the Accumulo iterator pattern for key-value aggregation, but only a per-key basis across different versions of that key.
If you have a requirement to do table-wide aggregation for the values of a common qualifier, you will likely still want to use MapReduce.
See the Aggregating sources in Accumulo using MapReduce recipe in this chapter.
Combiners can be applied at scan time or applied to the table configuration for incoming mutations This recipe uses the combiner to aggregate the qualifier values at scan time.
Accumulo also supports persistent combiners stored in the table configuration that combine values during mutation writes.
See also f Using MapReduce to bulk import geographic event data into Accumulo.
Accumulo offers the ability to apply cell visibility labels for each unique key/value in a table, which is arguably its most distinguishing feature from other BigTable implementations.
This recipe will demonstrate one way to apply cell-level security.
The code in this recipe will write several mutations that can only be scanned and read with the proper authorizations.
The shell script in this recipe assumes that Zookeeper is running on the host localhost and on the port 2181; you can change this to suit your environment needs.
The Accumulo installation's bin folder needs to be on your environment path.
For this recipe you'll need to create an Accumulo instance named test with user as root and password as password.
You will need a table by the name acled to exist in the configured Accumulo instance.
If you have an existing table by that name from an earlier recipe, delete, and recreate it.
It is also highly recommended that you go through the Using MapReduce to bulk import geographic event data into Accumulo recipe earlier in this chapter.
This will give you some sample data to experiment with.
The following are the steps to read/write data to Accumulo using cell visibility controls:
You should see the following output in the console: no threat levels are visible with your current user auths:
Run the setauths command to see a list of options.
You should see the following output in the console: Scan found: eventA threat level: moderate.
The class SecurityScanMain reads the required arguments to connect to Accumulo and instantiates a BatchWriter instance to write out test data to the acled table.
The first is for rowID eventA and the column visibility expression (p1|p2|p3)
The second is for rowID eventB and the column visibility (p4|p5)
Before a scan can occur over an Accumulo table, the client must supply authorization tokens for the connected user.
Accumulo will compare the given tokens against the column visibility label on each key to determine visibility for that user over the given key/value.
By default, the root user does not have any scanning authorization tokens.
Once we set the scan tokens for the root user to p1,p4, we can view both eventA and eventB.
Cell visibility is a feature with more complexity than you might think.
Here are some things to know about cell security in Accumulo:
Writing mutations for unauthorized scanning Authorization tokens restrict what users can see during scans, but not what column visibility expressions they can write on mutations.
This is the default behavior and, for many systems, is undesirable.
Once applied to the Accumulo installation, users will be barred from writing mutations containing column visibility labels they themselves are not authorized to read.
ColumnVisibility is part of the key Different keys containing the exact same rowID, column-family, and qualifier may have different ColumnVisibility labels.
If the most recent timestamped version of a key contains a ColumnVisibility key that is not viewable by the current scan, the user will see the next oldest version of that key for which a column visibility token matches, or none if they are not authorized to see any of the versions.
The normal scanning logic for key/value presentation has the scanner returning the most recent version of a given key.
The cell visibility system adjusts that logic with one additional condition.
The scanner will return the most recently timestamped version of a given key that matches the supplied authorization tokens.
Supporting more complex Boolean expressions This recipe shows two very simple disjunction examples of the ColumnVisibilty Boolean expression.
You can apply more complicated expressions, should your application require them.
See also f Using MapReduce to bulk import geographic event data into Accumulo f Setting a custom field constraint for inputing geographic event data in Accumulo.
The shell script in this recipe assumes that Zookeeper is running on the host localhost and on the port 2181; you can change this to suit your environment needs.
The Accumulo installation's bin folder needs to be on your environment path.
For this recipe you'll need to create an Accumulo instance named test with user as root and password as password.
You will need a table by the name acled to exist in the configured Accumulo instance.
To see the filtered results from this recipe, you will need to go through the Using MapReduce to bulk import geographic event data into Accumulo recipe seen earlier in this chapter.
This will give you some sample data to experiment with.
The following are the steps to count occurrences of different sources using MapReduce:
You should see the MapReduce job start executing over your pseudo-distributed cluster.
Upon successful completion of the job, you should see the file source_count.txt in your base working folder.
Type in the cat command to see the counts for each source.
We define the SourceCountJob class to implement the Tool interface for ease of remote submission with the ToolRunner class.
For this job we're only interested in retrieving the column qualifier src from the column family cf for each key.
By default, the scanner will only return the most recent versions of each key containing the qualifier src.
If we wanted to count source occurrences across every key-value pair in the table for every version, we would have to configure maxVersions in the input format.
As our reducer class is simply adding integers together, we can re-use the sample implementation for a combiner.
We clear any existing output folders and set the number of reduce tasks to 1 as we are running a pseudo-distributed cluster.
Now we are ready to submit the job to the cluster.
The business logic operates in a very similar manner to the traditional WordCount example.
Therefore, any key/value instances that enter our ACLEDSourceMapper class's map() function are already restricted to the data we're interested in aggregating, and we can simply output 1 to represent one occurrence of that source value in our dataset.
The output key is simply the value of the incoming source.
The reduce class ACLEDSourceReducer simply tallies occurrences for each source, and outputs the results back to HDFS.
We now have one single file with newline-separated source listings and the total number of occurrences for each source.
Hive date UDFs using to sort event dates, from geographic.
MapReduce used used, for generating n-grams over news archives  115-119 map-side join.
Python AS keyword, used for type casing values  88 used, for cleaning geographical event.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licenses, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Learn how to crunch Big data to extract meaning from the data avalanche.
Shows how to build a complete infrastructure to handle your needs as your data grows.
Hands-on examples in each chapter give the big picture while also giving direct experience.
Recipes for analyzing large and complex datasets with Hadoop MapReduce.
Learn to process large and complex datasets, starting simply, then diving in deep.
Solve complex Big data problems such as classifications, finding relationships, online marketing and recommendations.
More than 50 Hadoop MapReduce recipes, presented in a simple and straightforward manner, with step-by-step instructions and real-world examples.
Move large amounts of data into HBase and learn how to manage it efficiently.
Set up HBase on the cloud, get it ready for production, and run it smoothly with high performance.
Over 150 recipes to design and optimize large-scale Apache Cassandra deployments.
Get the best out of Cassandra using this efficient recipe bank.
Chapter 3: Extracting and Transforming Data Introduction Transforming Apache logs into TSV format using MapReduce Using Apache Pig to filter bot traffic from web server logs Using Apache Pig to sort web server log data by timestamp Using Apache Pig to sessionize web server log data Using Python to extend Apache Pig functionality Using MapReduce and secondary sort to calculate page views Using Hive and Python to clean and transform geographical event data Using Python and Hadoop Streaming to perform a time series analytic Using MultipleOutputs in MapReduce to name output files Creating custom Hadoop Writable and InputFormat to read geographical event data.
Chapter 4: Performing Common Tasks Using Hive, Pig, and MapReduce Introduction Using Hive to map an external table over weblog data in HDFS Using Hive to dynamically create tables from the results of a weblog query Using the Hive string UDFs to concatenate fields in weblog data Using Hive to intersect weblog IPs and determine the country Generating n-grams over news archives using MapReduce Using the distributed cache in MapReduce to find lines that contain matching keywords over news archives Using Pig to load a table and perform a SELECT operation with GROUP BY.
Chapter 5: Advanced Joins Introduction Joining data in the Mapper using MapReduce Joining data using Apache Pig replicated join Joining sorted data using Apache Pig merge join Joining skewed data using Apache Pig skewed join Using a map-side join in Apache Hive to analyze geographical events Using optimized full outer joins in Apache Hive to analyze geographical events Joining data using an external key-value store (Redis)
Chapter 6: Big Data Analysis Introduction Counting distinct IPs in web log data using MapReduce and Combiners Using Hive date UDFs to transform and sort event dates from geographic event data Using Hive to build a per-month report of fatalities over geographic event data Implementing a custom UDF in Hive to help validate source reliability over geographic event data Marking the longest period of non-violence using Hive MAP/REDUCE operators and Python Calculating the cosine similarity of artists in the Audioscrobbler dataset using Pig Trim Outliers from the Audioscrobbler dataset using Pig and datafu.
Chapter 8: Debugging Introduction Using Counters in a MapReduce job to track bad records Developing and testing MapReduce jobs with MRUnit Developing and testing MapReduce jobs running in local mode Enabling MapReduce jobs to skip bad records Using Counters in a streaming job Updating task status messages to display debugging information Using illustrate to debug Pig jobs.
Chapter 10: Persistence Using Apache Accumulo Introduction Designing a row key to store geographic events in Accumulo Using MapReduce to bulk import geographic event data into Accumulo Setting a custom field constraint for inputting geographic event data in Accumulo Limiting query results using the regex filtering iterator Counting fatalities for different versions of the same key using SumCombiner Enforcing cell-level security on scans using Accumulo Aggregating sources in Accumulo using MapReduce.
