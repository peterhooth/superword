Pro Hadoop is a guide to using Hadoop Core, a wonderful tool that allows you to use ordinary hardware to solve extraordinary problems.
In the course of my work, I have needed to build applications that would not fit on a single affordable machine, creating custom scaling and distribution tools in the process.
With the advent of Hadoop and MapReduce, I have been able to focus on my applications instead of worrying about how to scale them.
It took some time before I had learned enough about Hadoop Core to actually be effective.
This book is a distillation of that knowledge, and a book I wish was available to me when I first started using Hadoop Core.
I begin by showing you how to get started with Hadoop and the Hadoop Core shared file system, HDFS.
Then you will see how to write and run functional and effective MapReduce jobs on your clusters, as well as how to tune your jobs and clusters for optimum performance.
I provide recipes for unit testing and details on how to debug MapReduce jobs.
I also include examples of using advanced features such as map-side joins and chain mapping.
To bring everything together, I take you through the step-by-step development of a nontrivial MapReduce application.
This will give you insight into a real-world Hadoop project.
It is my sincere hope that this book provides you an enjoyable learning experience and with the knowledge you need to be the local Hadoop Core wizard.
No part of this work may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage or retrieval system, without the prior written permission of the copyright owner and the publisher.
Rather than use a trademark symbol with every occurrence of a trademarked name, we use the names only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.
Apress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use.
For more information, reference our Special Bulk Sales–eBook Licensing web page at http://www.apress.com/info/bulksales.
The information in this book is distributed on an “as is” basis, without warranty.
Although every precaution has been taken in the preparation of this work, neither the author(s) nor Apress shall have any liability to any person or entity with respect to any loss or damage caused or alleged to be caused directly or indirectly by the information contained in this work.
The source code for this book is available to readers at http://www.apress.com.
You may need to answer questions pertaining to this book in order to successfully download the code.
JaSOn Venner is a software developer with more than 20 years of experience developing highly scaled, high-performance systems.
Earlier, he worked primarily in the financial services industry, building high-performance check-processing systems.
His more recent experience has been building the infrastructure to support highly utilized web sites.
He has an avid interest in the biological sciences and is an FAA certificated flight instructor.
Sia CYrUS’s experience in computing spans many decades and areas of software development.
During the 1980s, he specialized in database development in Europe.
In the 1990s, he moved to the United States, where he focused on client/server applications.
Since 2000, he has architected a number of middle-tier business processes.
And most recently, he has been specializing in Web 2.0, Ajax, portals, and cloud computing.
Sia is an independent software consultant who is an expert in Java and development of Java enterprise-class applications.
He has been responsible for innovative and generic software, holding a U.S.
His passion could be entitled “Enterprise Architecture in Open Source.”
When not experimenting with new technologies, Sia enjoys playing ice hockey, especially with his two boys, Jason and Brandon.
They gracefully let my mistakes pass—and there were some large-scale mistakes—and welcomed my successes.
I remember the days when I couldn’t afford to buy a compiler, and had to sneak time on the university computers, when only people who signed horrible NDAs and who worked at large organizations could read the Unix source code.
His dedication and yes, fanaticism, has changed our world substantially for the better.
Hadoop rides on the back, sweat, and love of Doug Cutting, and many people of Yahoo! Inc.
All of the Hadoop users and contributors who help each other on the mailing lists are wonderful people.
I would also like to thank the Apress staff members who have applied their expertise to make this book into something readable.
While Hadoop supplied the tools to scale applications, it lacked documentation on how to use the framework effectively.
It enables you to rapidly and painlessly get up to speed with Hadoop.
This is the book I wish was available to me when I started using Hadoop.
Who this Book is For This book has three primary audiences: developers who are relatively new to Hadoop or MapReduce and must scale their applications using Hadoop; system administrators who must deploy and manage the Hadoop clusters; and application designers looking for a detailed understanding of what Hadoop will do for them.
Hadoop experts will learn some new details and gain insights to add to their expertise.
How this Book is Structured This book provides step-by-step instructions and examples that will take you from just beginning to use Hadoop to running complex applications on large clusters of machines.
It then walks you through getting the software, installing it on your computer, and running the basic examples.
Chapter 2, The Basics of a MapReduce Job: This chapter explores what is involved in writing the actual code that performs the map and the reduce portions of a MapReduce job, and how to configure a job to use your map and reduce code.
Chapter 3, The Basics of Multimachine Clusters: This chapter walks you through the basics of creating a multimachine Hadoop cluster.
It explains what the servers are, how the servers interact, basic configuration, and how to verify that your cluster is up and running successfully.
You’ll also find out what to do if a cluster doesn’t start.
Chapter 4, HDFS Details for Multimachine Clusters: This chapter covers the details of the Hadoop Distributed File System (HDFS) and provides detailed guidance on the installation, running, troubleshooting, and recovery of your HDFS installations.
Chapter 5, MapReduce Details for Multimachine Clusters: This chapter gives you a detailed understanding of what a MapReduce job is and what the Hadoop Core framework actually does to execute your MapReduce job.
You will learn how to set your job classpath and use shared libraries.
It also covers the input and output formats used by MapReduce jobs.
Chapter 6, Tuning Your MapReduce Jobs: In this chapter, you will learn what you can tune, how to tell what needs tuning, and how to tune it.
With this knowledge, you will be able to achieve optimal performance for your clusters.
Chapter 7, Unit Testing and Debugging: When your job is run across many machines, debugging becomes quite a challenge.
Chapter 7 walks you through how to debug your jobs.
The examples and unit testing framework provided in this chapter also help you know when your job is working as designed.
Chapter 8, Advanced and Alternate MapReduce Techniques: This chapter demonstrates how to use several advanced features of Hadoop Core: map-side joins, chain mapping, streaming, pipes, and aggregators.
You will also learn how to configure your jobs to continue running when some input is bad.
Streaming is a particularly powerful tool, as it allows scripts and other external programs to be used to provide the MapReduce functionality.
Chapter 9, Solving Problems with Hadoop: This chapter describes step-by-step development of a nontrivial MapReduce job, including the whys of the design decisions.
The sample MapReduce job performs range joins, and uses custom comparator and partitioner classes.
Chapter 10, Projects Based on Hadoop and Future Directions: This chapter provides a summary of several projects that are being built on top of Hadoop Core: distributed column-oriented databases, distributed search, matrix manipulation, and machine learning.
There are also references for training and support and future directions for Hadoop Core.
Prerequisites For those of you who are new to Hadoop, I strongly urge you to try Cloudera’s open source Distribution for Hadoop (http://www.cloudera.com/hadoop)
It provides the stable base of Hadoop 0.18.3 with bug fixes and some new features back-ported in and added-in hooks to the support scribe log file aggregation service (http://scribeserver.wiki.sourceforge.net/)
The Cloudera folks have Amazon machine images (AMIs), Debian and RPM installer files, and an online configuration tool to generate configuration files.
The following are the stock Hadoop Core distributions at the time of this writing:
Separate Eclipse projects are provided for each of these releases.
You can access the source code from this book’s details page or find the source code at the following URL (search for Hadoop): http://www.apress.com/book/ sourcecode.
The sample code is designed to be imported into Eclipse as a complete project.
There are several versions of the code, each a designated version of Hadoop Core that includes that Hadoop Core version.
The src directory has the source code for the examples.
The test examples are under test/src in the corresponding package directory.
The directory src/config contains the configuration files that are loaded as Java resources.
Three directories contain JAR or zip files that have specific licenses.
The directory bsd_license contains the items that are provided under the BSD license.
The directory other_ licenses contains items that have other licenses.
A README.txt file has more details about the downloadable code.
Applications frequently require more resources than are available on an inexpensive machine.
Many organizations find themselves with business processes that no longer fit on a single cost-effective computer.
A simple but expensive solution has been to buy specialty machines that have a lot of memory and many CPUs.
This solution scales as far as what is supported by the fastest machines available, and usually the only limiting factor is your budget.
Such a cluster typically attempts to look like a single machine, and typically requires very specialized installation and administration services.
A more economical solution for acquiring the necessary computational resources is cloud computing.
A common pattern is to have bulk data that needs to be transformed, where the processing of each data item is essentially independent of other data items; that is, using a single-instruction multiple-data (SIMD) algorithm.
Hadoop Core provides an open source framework for cloud computing, as well as a distributed file system.
This book is designed to be a practical guide to developing and running software using Hadoop Core, a project hosted by the Apache Software Foundation.
This chapter introduces Hadoop Core and details how to get a basic Hadoop Core installation up and running.
Introducing the MapReduce Model Hadoop supports the MapReduce model, which was introduced by Google as a method of solving a class of petascale problems with large clusters of inexpensive machines.
The model is based on two distinct steps for an application:
The core concept of MapReduce in Hadoop is that input may be split into logical chunks, and each chunk may be initially processed independently, by a map task.
The results of these individual processing chunks can be physically partitioned into distinct sets, which are then.
A map task may run on any compute node in the cluster, and multiple map tasks may be running in parallel across the cluster.
The map task is responsible for transforming the input records into key/value pairs.
The output of all of the maps will be partitioned, and each partition will be sorted.
Each partition’s sorted keys and the values associated with the keys are then processed by the reduce task.
There may be multiple reduce tasks running in parallel on the cluster.
The application developer needs to provide only four items to the Hadoop framework: the class that will read the input records and transform them into one key/value pair per record, a map method, a reduce method, and a class that will transform the key/value pairs that the reduce method outputs into output records.
This crawler received as input large sets of media URLs that were to have their content fetched and processed.
The media items were large, and fetching them had a significant cost in time and resources.
Filter the URLs against a set of exclusion and inclusion filters.
I had 20 machines to work with on this project.
The previous incarnation of the application was very complex and used an open source queuing framework for distribution.
Hundreds of work hours were invested in writing and tuning the application, and the project was on the brink of failure.
Hadoop was suggested by a member of a different team.
After spending a day getting a cluster running on the 20 machines, and running the examples, the team spent a few hours working up a plan for nine map methods and three reduce methods.
The goal was to have each map or reduce method take less than 100 lines of code.
By the end of the first week, our Hadoop-based application was running substantially faster and more reliably than the prior implementation.
The fingerprint step used a third-party library that had a habit of crashing and occasionally taking down the entire machine.
The ease with which Hadoop distributed the application across the cluster, along with the ability to continue to run in the event of individual machine failures, made Hadoop one of my favorite tools.
Both Google and Yahoo handle applications on the petabyte scale with MapReduce clusters.
The Hadoop project provides and supports the development of open source software that supplies a framework for the development of highly scalable distributed computing applications.
The Hadoop framework handles the processing details, leaving developers free to focus on application logic.
And Hadoop happened to be the name of a stuffed yellow elephant owned by the child of the principle architect.
The introduction on the Hadoop project web page (http://hadoop.apache.org/) states:
The Apache Hadoop project develops open-source software for reliable, scalable, distributed computing, including:
Hadoop Core, our flagship sub-project, provides a distributed filesystem (HDFS) and.
HBase builds on Hadoop Core to provide a scalable, distributed database.
Pig is a high-level data-flow language and execution framework for parallel computation.
Distributed applications use ZooKeeper to store and mediate updates for critical shared state.
Hive is a data warehouse infrastructure built on Hadoop Core that provides data summarization, adhoc querying and analysis of datasets.
The Hadoop Core project provides the basic services for building a cloud computing environment with commodity hardware, and the APIs for developing software that will run on that cloud.
The two fundamental pieces of Hadoop Core are the MapReduce framework, the cloud computing environment, and he Hadoop Distributed File System (HDFS)
Note Within the Hadoop Core framework, MapReduce is often referred to as mapred, and HDFS is often referred to as dfs.
The Hadoop Core MapReduce framework requires a shared file system.
This shared file system does not need to be a system-level file system, as long as there is a distributed file system plug-in available to the framework.
In Hadoop JIRA (the issue-tracking system), item 4686 is a tracking ticket to separate HDFS into its own Hadoop project.
Users are also free to use any distributed file system that is visible as a system-mounted file system, such as Network File System (NFS), Global File System (GFS), or Lustre.
When HDFS is used as the shared file system, Hadoop is able to take advantage of knowledge about which node hosts a physical copy of input data, and will attempt to schedule the task that is to read that data, to run on that machine.
This book mainly focuses on using HDFS as the file system.
Hadoop Core MapReduce The Hadoop Distributed File System (HDFS)MapReduce environment provides the user with a sophisticated framework to manage the execution of map and reduce tasks across a cluster of machines.
The user is required to tell the framework the following:
If a job does not need a reduce function, the user does not need to specify a reducer class, and a reduce phase of the job will not be run.
The framework will partition the input, and schedule and execute map tasks across the cluster.
If requested, it will sort the results of the map task and execute the reduce task(s) with the map output.
The final output will be moved to the output directory, and the job status will be reported to the user.
The framework will convert each record of input into a key/value pair, and each pair will be input to the map function once.
The map output is a set of key/value pairs—nominally one pair that is the transformed input pair, but it is perfectly acceptable to output multiple pairs.
The map output pairs are grouped and sorted by key.
The reduce function is called one time for each key, in sort sequence, with the key and the set of values that share that key.
The reduce method may output an arbitrary number of key/value pairs, which are written to the output files in the job output directory.
If the reduce output keys are unchanged from the reduce input keys, the final output will be sorted.
The framework provides two processes that handle the management of MapReduce jobs:
Generally, there is one JobTracker process per cluster and one or more TaskTracker processes per node in the cluster.
The JobTracker is a single point of failure, and the JobTracker will work around the failure of individual TaskTracker processes.
Note One very nice feature of the Hadoop Core MapReduce environment is that you can add TaskTracker nodes to a cluster while a job is running and have the job spread out onto the new nodes.
The Hadoop Distributed File System HDFS is a file system that is designed for use for MapReduce jobs that read input in large chunks of input, process it, and write potentially large chunks of output.
For reliability, file data is simply mirrored to multiple storage nodes.
This is referred to as replication in the Hadoop community.
As long as at least one replica of a data chunk is available, the consumer of that data will not know of storage server failures.
There will be one NameNode process in an HDFS file system, and this is a single point of failure.
Hadoop Core provides recovery and automatic backup of the NameNode, but no hot failover services.
There will be multiple DataNode processes within the cluster, with typically one DataNode process per storage node in a cluster.
Note It is common for a node in a cluster to provide both TaskTracker services and DataNode services.
It is also common for one node to provide the JobTracker and NameNode services.
Installing Hadoop As with all software, you need some prerequisite pieces before you can actually use Hadoop.
It is possible to run and develop Hadoop applications under Windows, provided that Cygwin is installed.
It is strongly suggested that nodes in a production Hadoop cluster run a modern Linux distribution.
Note To use Hadoop, you’ll need a basic working knowledge of Linux and Java.
All the examples in this book are set up for the bash shell.
The Prerequisites The examples in this book were developed with the following:
The wonderful folks of the Fedora project provide torrents (downloaded with BitTorrent) for most of the Fedora versions at http://torrent.fedoraproject.org/
For those who want to bypass the update process, the people of Fedora Unity provide distributions of Fedora releases that have the updates applied, at http://spins.fedoraunity.org/spins.
The re-spins require the use of the custom download tool Jigdo.
For the novice Linux user who just wants to play around a bit, the Live CD and a USB stick for permanent storage can provide a simple and quick way to boot up a test environment.
The rpm command has options that will tell you which files were in an RPM package: -q to query, -l to list, and -p to specify that the path to package you are querying is the next argument.
Look for the string '/bin/javac$', using the egrep program, which searches for simple regular expressions in its input stream:
If you don’t use quotes, or use double quotes, the shell may try to resolve the $ character as a variable.
This assumes a working directory of ~/Downloads when running the JDK installation program, as the installer unpacks the bundled RPM files in the current working directory.
Add the following two lines to your .bashrc or .bash_profile:
This script assumes you downloaded the RPM installer for the JDK.
This script attempts to work out the installation directory of the jdk, # given the installer file.
Run the script in Listing 1-1 to work out the JDK installation directory and update your environment so that the JDK will be used by the examples:
Run a Cygwin bash shell by clicking the icon shown in Figure 1-3
This will set up your default process environment to have the java programs in your path and let programs, such as Hadoop, know where the Java installation is on your computer.
When a symbolic link is made and the JAVA_HOME set to the symbolic link location, bin/hadoop works well enough to use.
Cygwin maps Windows drive letters to the path /cygdrive/X, where X is the drive letter, and the Cygwin path element separator character is /, compared to Windows use of \
You must keep two views of your files in mind, particularly when running Java programs via the bin/hadoop script.
The bin/hadoop script and all of the Cygwin utilities see a file system that is a subtree of the Windows file system, with the Windows drives mapped to the /cygdrive directory.
In a standard Cygwin installation, the /tmp directory is also the C:\cygwin\tmp directory.
Java will parse /tmp as C:\tmp, a completely different directory.
When you receive File Not Found errors from Windows applications launched from Cygwin, the common problem is that the Windows application (Java being a Windows application) is looking in a different directory than you expect.
Note You will need to customize the Cygwin setup for your system.
The exact details change with different Sun JDK releases and with different Windows installations.
Getting Hadoop Running After you have your Linux or Cygwin under Windows environment set up, you’re ready to download and install Hadoop.
From there, find the tar.gz file of the distribution of your choice, bearing in mind what I said in the introduction, and download that file.
If you are a cautious person, go to the backup site and get the PGP checksum or the MD5 checksum of the download file.
Unpack the tar file in the directory where you would like your test installation installed.
I typically unpack this in a src directory, off my personal home directory:
Add the following two lines to your .bashrc or .bash_profile file and execute them in your.
If you chose a different directory than ~/src, adjust these export statements to reflect your chosen location.
We are now going to see if a java program and hadoop programs # are in the path, and if they are the ones we are expecting.
Double check that the java in the PATH is the expected version.
Your PATH appears to have the JAVA_HOME java program as the default java.
Your PATH appears to have the HADOOP_HOME hadoop program as the default hadoop.
Running Hadoop Examples and Tests The Hadoop installation provides JAR files with sample programs and tests that you can run.
Before you run these, you should have verified that your installation is complete and that your runtime environment is set up correctly.
Included in the JAR file are the programs listed in Table 2-1
To demonstrate using the Hadoop examples, let’s walk through running the pi program.
The number of samples is the number of points randomly set in the square.
The larger this value, the more accurate the calculation of pi.
For the sake of simplicity, we are going to make a very poor estimate of pi by using very few operations.
The pi program takes two integer arguments: the number of maps and the number of samples per map.
The total number of samples used in the calculation is the number of maps times the number of samples per map.
The command-line arguments are processed in three steps, with each step consuming some of the command-line arguments.
By default, all output by the framework will have a leading date stamp, a log level, and the name of the class that emitted the message.
In addition, the default is only to emit log messages of level INFO or higher.
For brevity, I’ve removed the data stamp and log level from the output reproduced in this book.
Now we will go through the output in Listing 2-3 chunk by chunk, so that you have an understanding of what is going on and can recognize when something is wrong.
The first section is output by the pi estimator as it is setting up the job.
The framework has taken over at this point and sets up input splits (each fragment of input is called an input split) for the map tasks.
The following line provides the job ID, which you could use to refer to this job with the job control tools:
The following lines let you know that there are two input files and two input splits:
The map output key/value pairs are partitioned, and then the partitions are sorted, which is referred to as the shuffle.
The file created for each sorted partition is called a spill.
There will be one spill file for each configured reduce task.
For each reduce task, the framework will pull its spill from the output of each map task, and merge-sort these spills.
In Listing 2-3, the next block provides detailed information on the map task and shuffle process that was run.
The framework is expecting to produce output for one reduce task (numReduceTasks: 1), which receives all of the map task output records.
Also, it expects that the map outputs have been partitioned and sorted and stored in the file system (Finished spill 0)
If there were multiple reduce tasks specified, you would see a Finished spill N for each reduce task.
The rest of the lines primarily have to do with output buffering and may be ignored.
Generated 1 samples is the output of the ending status of the map job.
Listing 2-3 has exactly two map tasks, per your command-line instructions to the task, and one reduce task, per the job design..
With a single reduce task, each map task’s output is placed into a single partition and sorted.
This results in two files, or spills, as input to the framework sort phase.
Each reduce task in a job will have its output go to the output directory and be named part-0N, where N is the ordinal number starting from zero of the reduce task.
The numeric portion of the name is traditionally five digits, with leading zeros as needed.
The next block describes the single reduce task that will be run:
The next block of output provides summary information about the completed job:
The final two lines are printed by the PiEstimator code, not the framework.
Troubleshooting The issues that can cause problems in running the examples in this book will most likely be due to environment differences.
You may also experience problems if you have space shortages on your computer.
You can set this system variable through the System Control Panel.
In the System Properties dialog box, click the Advanced tab, and then click the Environment Variables button.
In the System Variables section of the Environment Variables dialog box, select Path, click Edit, and add the following string:
While not critical, the current working directory for the shell session used for running the.
Summary Hadoop Core provides a robust framework for distributing tasks across large numbers of general-purpose computers.
Application developers just need to write the map and reduce methods for their data, and use one of the existing input and output formats.
The framework provides a rich set of input and output handlers, and you can create custom handlers, if necessary.
Getting over the installation hurdle can be difficult, but it is getting simpler as more people and organizations understand the issues and refine the processes and procedures.
Cloudera (http://www.cloudera.com) now provides a self-installing Hadoop distribution in RPM format.
The chapters to come will guide you through the trouble spots as you develop your own applications with Hadoop.
This chapter walks you through what is involved in a MapReduce job.
You will be able to write and run simple stand-alone MapReduce programs by the end of the chapter.
They should be explicitly run in a special local mode configuration for executing on a single machine, with no requirements for a running the Hadoop Core framework.
This single machine (local) configuration is also ideal for debugging and for unit tests.
The code for the examples is available from this book’s details page at the Apress web site (http://www.apress.com)
The downloadable code also includes a JAR file you can use to run the examples.
Let’s start by examining the parts that make up a MapReduce job.
The Parts of a Hadoop MapReduce Job The user configures and submits a MapReduce job (or just job for short) to the framework, which will decompose the job into a set of map tasks, shuffles, a sort, and a set of reduce tasks.
The framework will then manage the distribution and execution of the tasks, collect the output, and report the status to the user.
Start of the individual map tasks with their input split Hadoop framework.
Map function, called once for each input key/value pair User.
Shuffle, which partitions and sorts the per-map output Hadoop framework.
Sort, which merge sorts the shuffle output for each partition of all map  Hadoop framework outputs.
Start of the individual reduce tasks, with their input partition Hadoop framework.
Reduce function, which is called once for each unique input key, with all of  User the input values that share that key.
Collection of the output and storage in the configured job output directory,  Hadoop framework in N parts, where N is the number of reduce tasks.
The user is responsible for handling the job setup, specifying the input location(s), specifying the input, and ensuring the input is in the expected format and location.
The framework is responsible for distributing the job among the TaskTracker nodes of the cluster; running the map, shuffle, sort, and reduce phases; placing the output in the output directory; and informing the user of the job-completion status.
If there are no tab characters in the line, the sort will be based on the entire line.
Input Splitting For the framework to be able to distribute pieces of the job to multiple machines, it needs to fragment the input into individual pieces, which can in turn be provided as input to the individual distributed tasks.
The default rules for how input splits are constructed from the actual input files are a combination of configuration parameters and the capabilities of the class that actually reads the input records.
An input split will normally be a contiguous group of records from a single input file, and in this case, there will be at least N input splits, where N is the number of input files.
If the number of requested map tasks is larger than this number, or the individual files are larger than the suggested fragment size, there may be multiple input splits constructed of each input file.
The user has considerable control over the number of input splits.
The number and size of the input splits strongly influence overall job performance.
It is used in jobs that only need to reduce the input, and not transform the raw input.
All map functions must implement the Mapper interface, which guarantees that the map function will always be called with a key.
The key is an instance of a WritableComparable object, a value that is an instance of a Writable object, an output object, and a reporter.
Reporters are discussed in more detail in the “Creating a Custom Mapper and Reducer” section later in this chapter.
Note The code for the Mapper.java and Reducer.java interfaces is available from this book’s details page at the Apress web site (http://www.apress.com), along with the rest of the downloadable code for this book.
The framework will make one call to your map function for each record in your input.
There will be multiple instances of your map function running, potentially in multiple Java Virtual Machines (JVMs), and potentially on multiple machines.
One common mapper drops the values and passes only the keys forward:
A Simple Reduce Function: IdentityReducer The Hadoop framework calls the reduce function one time for each unique key.
The framework provides the key and the set of values that share that key.
The framework-supplied class IdentityReducer is a simple example that produces one output record for every value.
If you require the output of your job to be sorted, the reducer function must pass the key objects to the output.collect() method unchanged.
The reduce phase is, however, free to output any number of records, including zero records, with the same key and different values.
This particular constraint is also why the map tasks may be multithreaded, while the reduce tasks are explicitly only single-threaded.
A common reducer drops the values and passes only the keys forward:
Configuring a Job All Hadoop jobs have a driver program that configures the actual MapReduce job and submits it to the Hadoop framework.
The sample class MapReduceIntro provides a walk-through for using the JobConf object to configure and submit a job to the Hadoop framework for execution.
It is good practice to pass in a class that is contained in the JAR file that has your map and reduce functions.
This ensures that the framework will make the JAR available to the map and reduce tasks run for your job.
Now that you have a JobConfig object, conf, you need to set the required parameters for the job.
These include the input and output directory locations, the format of the input and output, and the mapper and reducer classes.
All jobs will have a map phase, and the map phase is responsible for handling the job input.
The configuration of the map phase requires you to specify the input locations and the class that will produce the key/value pairs from the input, the mapper class, and potentially, the suggested number of map tasks, map output types, and per-map task threading, as listed in Table 2-2
Class to read and convert the input path elements to key/ Yes value pairs.
Most Hadoop Core jobs have their input as some set of files, and these files are either a textual key/value pair per line or a Hadoop-specific binary file format that provides serialized key/value pairs.
Specifying Input Formats The Hadoop framework provides a large variety of input formats.
The major distinctions are between textual input formats and binary input formats.
The following block of code informs the framework of the type and location of the job input:
The records are further divided into key/value pairs by splitting the line at the first tab character.
If there is no tab character in the line, the entire line is the key, and the value object will contain a zero-length string.
There is no way to distinguish an input line that contains a single tab as the last character and the same line without a trailing tab character.
Suppose that an input file has the following three lines, where TAB is replaced by an US-ASCII horizontal tab character (0x09):
Your mapper would be called with the following key/value pairs:
The actual order in which the keys are passed to your map function is indeterminate.
In a real-world example, the actual machine that ran the map that got a given key would be indeterminate.
It is very likely, however, that sets of contiguous records in the input will be processed by the same map task, as each task is given one input split from which to work.
The input bytes are considered to be in the UTF-8 character set.
Now that the framework knows where to look for the input files and the class to use to generate key/value pairs from the input files, you need to inform the framework which map function to use.
Note  The simple example in this chapter does not use the optional configuration parameters.
If the map function needs to output a different key or value class than the job output, those classes may be set here.
This is ideal if the map function is not able to fully utilize the resources allocated for the map task.
A simple case of where this might be beneficial is a map task that performs DNS lookups on the IP addresses in a server log.
Setting the Output Parameters The framework requires that the output parameters be configured, even if the job will not produce any output.
The framework will collect the output from the specified tasks (either the output of the map tasks for a MapReduce job that did not include reduce tasks or the output of the job’s reduce tasks) and place them into the configured output directory.
To avoid issues with file name collisions when placing the task output into the output directory, the framework requires that the output directory not exist when you start the job.
The output parameters are actually a little more comprehensive than just the setting of the output path.
The code will also set the output format and the output key and value classes.
The Text class is the functional equivalent of a String.
Unlike String, Text is mutable, and the Text class has some explicit methods for UTF-8 byte handling.
The key feature of a Writable is that the framework knows how to serialize and deserialize a Writable object.
The WritableComparable adds the compareTo interface so the framework knows how to sort the WritableComparable objects.
The following code block provides an example of the minimum required configuration for the output of a MapReduce job:
These settings inform the framework of the types of the key/value pairs to expect for the reduce phase.
By default, these classes will also be used to set the values the framework will expect from the map output.
Configuring the Reduce Phase To configure the reduce phase, the user must supply the framework with five pieces of information:
The input and output key and value types, as well as the output file type, are the same as those covered in the previous “Setting the Output Parameters” section.
Here, we will look at setting the number of reduce tasks and the reducer class.
The configured number of reduce tasks determines the number of output files for a job that will run the reduce phase.
Tuning this value will have a significant impact on the overall performance of your job.
The time spent sorting the keys for each output file is a function of the number of keys.
In addition, the number of reduce tasks determines the maximum number of reduce tasks that can be run in parallel.
The framework generally has a default number of reduce tasks configured.
This will result in a single output file containing all of the output keys, in sorted order.
There will be one reduce task, run on a single machine that processes every key.
The number of reduce tasks is commonly set in the configuration phase of a job.
In general, unless there is a significant need for a single output file, the number of reduce tasks is set to roughly the number of simultaneous execution slots in the cluster.
A typical cluster is composed of M TaskTracker machines, with C CPUs, each of which supports T threads.
In my environment, the machines typically have eight CPUs that support one thread per CPU, and a small cluster might have ten TaskTracker machines.
If your tasks tend not to be CPU-bound, you may adjust the number of execution slots configured to optimize the CPU utilization on your TaskTracker machines.
This requires tuning on a per-job basis and is a weakness in Hadoop at present, as the maximum values are not per-job configurable and instead require a cluster restart.
The reducer class needs to be set only if the number of reduce tasks is not zero.
It is very common to not need a reducer, since frequently you do not require sorted output or value grouping by key.
The framework relies on the output parameters being set correctly.
One of the more common errors is to have each reduce task fail with an exception of the form:
This error indicates that output key class has been defaulted by the framework, or was set incorrectly during the job configuration.
Or if your map output is not the same as your job output, use this form:
This error may occur for the value class as well:
Running a Job The ultimate aim of all your MapReduce job configuration is to actually run that job.
The method runJob() submits the configuration information to the framework and waits for the framework to finish running the job.
The RunningJob class provides a number of methods for examining the response.
The first thing you will notice is that the numbers don’t seem to be in order.
The code that generates the input produces a random number for the key of each line, but the example tells the framework that the keys are Text.
Therefore, the numbers have been sorted as text rather than as numbers.
Creating a Custom Mapper and Reducer As you’ve seen, your first Hadoop job, in MapReduceIntro, produced sorted output, but the sorting was not suitable, as it sorted lexically rather than numerically, and the keys for the job were numbers.
Now, let’s work out what is required to sort numerically, using a custom mapper.
Then we’ll look at a custom reducer that outputs the values in a format that is easy to parse.
Let’s try making the output key class a LongWritable, another class supplied by the framework:
As you can see, just changing the output key class was insufficient.
If you are going to change the output key class to a LongWritable, you also need to modify the map function so that it outputs LongWritable keys.
For the job to actually produce output that is sorted numerically, you must change the job configuration and provide a custom mapper class.
This is done by two calls on the JobConf object:
This class is identical to MapReduceIntro, except for these two replacement method calls.
Note The job configuration could also provide a custom sort option.
One way to do this is to provide a custom class that implements WritableComparable and use that as the key class.
You also need to provide a mapper class that performs the transformation.
First, the class declaration is no longer generic; the types have been made concrete:
Notice that the code actually provides the types for the key/value pairs for input and for output.
The reporter object provides a mechanism for informing the framework of the current status of your job.
Each call on the reporter object or the output collector provides a heartbeat to the framework, informing it that the task is not deadlocked or otherwise unresponsive.
If your map or reduce method takes substantial time, the method must make periodic calls on the reporter.
The framework will kill tasks that have not reported in 600 seconds by default.
This block of code introduces a new object, reporter, and some best practice patterns.
The key piece of this is the transformation of the Text key to a LongWritable key.
The code in Listing 2-6 is sufficient to perform the transformation, and also includes some additional code for tracking and reporting.
The pattern of creating a new key object in the mapper for the transformation object is not the most efficient pattern.
Most key classes provide a set() method, which sets the current value of the key.
The output.collect() method uses the current value of the key, and once the collect() method is complete, the key object or the value object is free to be reused.
Extreme care must be taken in using the mapper class member variables.
A ThreadLocal LongWritable object could be used to ensure thread safety.
Object churn is a significant performance issue in a map method, and to a lesser extent, in the reduce method.
It is a good practice to wrap your map and reduce methods in a try block that catches Throwables and reports on the catches.
The JobTracker, the Hadoop Core server process that manages job execution on the cluster, accumulates the counter values and provides a final count in the job output, as well.
This interface will be discussed in more detail in Chapter 6, which covers the setup of a multimachine cluster.
You expect that some of the keys may not convert correctly into Long values, so you capture the exception.
If the counter does not already exist, it will be created.
In the sample input, there are no records that will cause a number format exception.
The only counters that are accumulated are Input.total records and Input.parsed records.
These two counters will show up in the job output as part of the Input group:
If one or more keys caused an exception during the conversion to Long, the output might look more like this:
Note The sum of the parsed records and the number formats should equal the total records.
The counters are also available via the RunningJob object, allowing for a more comprehensive check of the success status.
The totals for your job will vary from this example.
After the Job Finishes Once the job finishes, the framework will provide you with a filled-out RunningJob object.
The framework will report that the job was unsuccessful if it was unable to complete any single map task or if the job was killed.
This generally doesn’t provide enough information to make a determination on the actual success.
It may be that there was an exception in the map or method for every key or for most keys.
If the map or reduce function provides job counters for these cases, your job driver will be able to make a better determination regarding the actual success or failure of your job.
In the sample mapper, several counters were collected under different circumstances:
Now that the job driver has the counters issued by the map method, a much more accurate determination of success can be made.
In one of my production clusters, a TaskTracker node was incorrectly configured.
The result of this misconfiguration was that none of the computationally intense work could be run in the map task, and the map method would return immediately with an exception.
As far as the framework was concerned, this machine was super fast, and it scheduled almost all of the map tasks on this machine.
The job was successful as far as the framework was concerned, but totally unsuccessful per the business rules.
Was this Job really Successful? The check for success primarily involves ensuring that the number of records output is roughly the same as the number of records input.
Hadoop jobs are generally dealing with bulk realworld data, which is never 100% clean, so a small error rate is generally acceptable.
If the total number of input records is roughly the number of parsed input records, and you have no unexpected exceptions, this job is a success.
Creating a Custom Reducer The reduce method is called once for each key, and passes the key and an iterator to all of the map output values that share that key.
The reduce task is an ideal place for summarizing data and for doing basic duplicate suppression.
Note For managing duplicate suppression against a prior seen set, it is usually best to keep the prior seen set in either HBase (the Hadoop database) or in a sorted format, such as a Hadoop map file.
If this is not done, then the dataset of seen records and the dataset of input records must be merged and sorted, which can take considerable time if either dataset is large.
In the HBase case, if the input data is already sorted, the duplicate status of an input record can be rapidly determined.
With a simple sorted seen set, map-side joins may be performed.
For the sample custom reducer, let’s merge the values into a comma-separated values (CSV) form, so you have one output line per key, with all of the values in a simple-to-parse format.
After your work with the custom mapper in the preceding sections, creating a custom reducer will seem familiar.
First, the framework needs to be informed of the reducer class.
The key piece is, as usual, to inform the framework of the reducer class, so add the following single line:
The reduce method doesn’t need to know the incoming value class; it requires only the toString() method to work.
The reduce method does need to construct a new output value, and for simplicity’s sake, given this transformation, the output value is declared to be Text.
The actual method declaration also has the same type specification:
The framework will throw an error if the job is expecting a different output value type than Text.
As with the mapper example, you have a method body that employs the reporter.
As a performance optimization, to reduce object churn, two class fields are declared.
The buffer object is used to build the CSV-style line for the output, and mergedValue is the actual object that is sent to the output on each reduce() call.
It is safe to declare these as class fields, rather than as local variables, because the individual reduce tasks are run only as single threads by the framework.
Note There may be multiple reduce tasks running simultaneously, but each task is running in a separate JVM, and the JVMs are potentially running on separate physical machines.
The reduce() method is called with the key and an iterator to the values that share that key.
Recall that, ideally, a reduce task will make no changes to the key, and will use that key as the key argument to the output.collect() method calls in the reduce() method.
The design goal for this reduce() method is to output only a single row for every key, with a commaseparated list of the values that shared that key.
The core of the reduce() method has a bit of boilerplate for the object churn optimizations to reset the StringBuilder object, and a loop to process each of the values for this key:
It is rare that a reduce() method doesn’t have a loop that iterates over the values.
It is good form to report on the number of values input.
This reducer relies on the toString() method of the value object, which seems reasonable for a textual output job, as the framework would also be using the toString() method to produce the output.
The rest of the preceding code block simply builds a comma-separated list of values, with Excel-style CSV quoting.
The actual output block must build a new value for the output.
In this case, a class field mergedValue will be used.
In a larger job, there may be a billion keys passed through the reduce() method, and by using the class field, the amount of object churn is greatly reduced.
In this example, there are also counters for the output records:
This example uses Text as the output value class; it is acceptable to use any Writable as the output value class.
If the output format is a SequenceFile, there is no need for a functional toString() method on your object.
Note The framework serializes the key and value into the output stream during the collect() method, leaving the user free to change the objects values when the method returns.
This class provides basic implementations of two additional methods that are required of a mapper or a reducer by the framework.
The framework calls the configure() method upon initializing a task, and it calls the close() method when the task has finished processing its input split:
This method is where any per-task configuration and setup is done.
It is very common for the developer to have a JobConf member variable, which would be initialized in this method with the passed-in JobConf object.
I prefer to issue a logging record with detailed information about the input split.
The configure() method is also the ideal place to open additional files that need to be read or written to during the map() or reduce() method.
It is very important to close any supplemental files here to ensure that they are properly flushed to the file system.
Particularly for HDFS, if the file is not closed, data in the last block may be lost.
The following example also makes a reporter call in the close() method:
The reporter field was made a class instance field, via protected Reporter reporter, and set in the reduce() method via this.reporter = reporter.
In the reduce() method, the count of values is kept in valueCount, and if it’s larger than the instance member field, maxValueCount, maxValueCount is set to it.
This enables you to output the maximum number of values that shared a specific key.
In this case, the overall summary value is not particularly useful, as that value is the sum of all of the maximum values, but the per-task value is interesting and available via the web interface.
A more useful solution would be to maintain an additional output file and output the key/value counts into that file.
When you select a completed or running task through the web interface (which is on port 50030 on the machine running the JobTracker, by default), you are presented with the counter summary for the job and links to detailed information about the map and reduce tasks.
Each map and reduce task will have a link to the counters.
Using a Custom Partitioner By default, the framework partitions your output based on the hash value of the key, using the HashPartitioner class.
There are times when you need your output data partitioned differently.
The standard example is a single output file where multiple output files would usually.
If you need different partitioning, you have the option of setting a partitioner.
Some simple partitioner concepts could be to sort into odd/even or, if the minimum and maximum key values are known, to sort into key rangebased buckets.
When the framework is performing the shuffle, each key output by the mapper is examined, and the following operation is performed:
The value partitions is the number of reduce tasks to perform.
The key, if actually output by the reducer, will end up in the output file part partition, with an appropriate number of leading zeros so that the file names are all the same length.
The critical issues are that the number of partitions is fixed at job start time and the partition is determined in the output.collect() method of the map task.
The only information the partitioner has is the key, the value, the number of partitions, and whatever data was made available to it when it was instantiated.
The partitioner interface is very simple, as shown in Listing 2-7
The JobConfigurable interface provides an additional configure() method, as the MapReduceBase class does.
You now have a basic understanding of the JobConf object and how to use it to inform the framework of the requirements for your jobs.
You’ve seen how to write mapper and reducer classes, and how the reporter object is one of your best friends, because of the wonderful information it can provide about what is happening during the execution of your jobs.
Output partitions finally make sense, and you have a sense of when and why you configure your job to reduce, and how many reducers you will use.
As a brilliant Hadoop expert, you are totally prepared to inform people of why the files they open in mapper or reducer classes are empty or short, because you know you need to close files before the framework will flush the last file system block size worth of data to disk.
In the next chapter, you’ll learn how to set up of a multimachine cluster.
This chapter explains how to set up a multimachine cluster.
You’ll learn about the makeup of a cluster, the tools for managing clusters, and how to configure a cluster.
Here, we’ll walkthrough a simple cluster configuration, using the minimum HDFS setup necessary to bring up the cluster.
Chapter 4 will go into the details for a high-usage HDFS.
The Makeup of a Cluster A typical Hadoop Core cluster is made up of machines running a set of cooperating server processes.
The machines in the cluster are not required to be homogeneous, and commonly they are not.
The cluster machines may even have different CPU architectures and operating systems.
But if the machines have similar processing power, memory, and disk bandwidth, cluster administration is a lot easier, because in that case, only one set of configuration files and runtime environments needs to be maintained and distributed.
A cluster will have one JobTracker server, one NameNode server, and one secondary NameNode server, and DataNodes and TaskTrackers.
The JobTracker coordinates the activities of the TaskTrackers, and the NameNode manages the DataNodes.
In the context of Hadoop, a node/machine running the TaskTracker or DataNode server is considered a slave node.
It is common to have nodes that run both the TaskTracker and DataNode servers.
The Hadoop server processes on the slave nodes are controlled by their respective masters, the JobTracker and NameNode servers.
Manages metadata: (file names, file blocks, block locations, open files) and DadaNodes.
Provides a backup for the NameNode data and manages file system.
Let’s look at each of the server processes run by the machines in a cluster:
JobTracker: The JobTracker provides command and control for job management.
It supplies the primary user interface to a MapReduce cluster.
There is one instance of this server running on a cluster.
The machine running the JobTracker server is the MapReduce master.
TaskTracker: The TaskTracker provides execution services for the submitted jobs.
Each TaskTracker manages the execution of tasks on an individual compute node in the MapReduce cluster.
There is one instance of this server per compute node.
Note If your MapReduce jobs utilize external packages or services, it is very important that these external packages and services are identically configured across all of your TaskTracker machines.
It is not uncommon for external JAR files to be required for the successful running of a task.
NameNode: The NameNode provides metadata storage for the shared file system.
The NameNode supplies the primary user interface to the HDFS.
It also manages all of the metadata for the HDFS.
There is one instance of this server running on a cluster.
The metadata includes such critical information as the file directory structure and which DataNodes have copies of the data blocks that contain each file’s data.
The machine running the NameNode server process is the HDFS master.
Secondary NameNode: The secondary NameNode provides both file system metadata backup and metadata compaction.
It supplies near real-time backup of the metadata for the NameNode.
There is at least one instance of this server running on a cluster, ideally on a separate physical machine than the one running the NameNode.
The secondary NameNode also merges the metadata change history, the edit log, into the NameNode’s file system image.
Real-time backup of the NameNode data: Many installations configure the NameNode to store the file system metadata to multiple locations, where at least one of these locations resides on a separate physical machine.
Other installations use a tool such as DRBD (http://www.drbd.org/) to replicate the host file system in near real time to a separate physical machine.
DataNode: The DataNode provides data storage services for the shared file system.
The NameNode coordinates the storage and retrieval of the individual data blocks managed by a DataNode.
There is one instance of this server process per HDFS storage node.
Balancer: During normal usage, the disk utilization on the DataNode machines may become uneven.
This is particularly common if some DataNodes have less disk space available for use by HDFS.
The Balancer moves data blocks between DataNodes to even out the per-DataNode available disk space.
The Balancer will also rebalance the cluster as new DataNodes are added to an existing cluster.
It must be run by the user via the command bin/hadoop balancer [-threshold <threshold>]
The optional argument is the maximum amount of variance in disk space utilization between DataNodes for the cluster to be considered balanced.
As of Hadoop 0.19.0, this is not a configuration parameter.
These server processes are typically started once per cluster instance.
DataNodes and TaskTrackers may be dynamically added and removed from a running cluster, as described in the next section.
All of these servers are implemented in Java and require at least Java version 1.6
Cluster Administration Tools The Hadoop Core installation provides a number of scripts in the bin subdirectory of the installation that are used to stop and start the entire cluster or various pieces of the cluster.
There are also administrative scripts for the Hadoop Core servers.
The administrator has the option of starting or stopping the full set of Hadoop Core servers with the start-all.sh and stop-all.sh scripts.
These scripts start all of the server processes on the cluster machines.
The NameNode and JobTracker will be started or stopped on the machine on which the script is run, and DataNodes and TaskTracker nodes will be started on the configured slave machines.
Any requested secondary NameNodes will also be started on configured machines.
These scripts start stop-mapred.sh  or stop only the JobTracker and TaskTracker nodes.
The Balancer is expected to run on the stop-balancer.sh machine on which these scripts are executed.
The preceding start and stop scripts actually use the hadoop-daemon.sh script to start or stop the servers.
This script is used by the start and stop scripts to start the DataNodes, TaskTrackers, and secondary NameNodes.
This script will use the hadoop-daemon.sh script to start the servers on each specific machine in the set of machines on which it operates.
This feature of Hadoop Core is expected to be discontinued.
As of Hadoop 0.17, the Serialization service is the preferred method for handling external data structures.
Cluster Configuration The Hadoop Core servers load their configuration from files in the conf directory of your Hadoop Core installation.
As a general rule, identical copies of the configuration files are maintained in the conf directory of every machine in the cluster.
It is also common to have the NameNode and the JobTracker servers on the same node, especially in smaller installations.
Hadoop Configuration Files The configuration files fall into the following groups:
In this chapter, we will walk through constructing a hadoop-site.xml file for a small cluster.
Slaves and masters: Two files are used by the startup and shutdown commands discussed in the previous section to start and stop the DataNode, TaskTracker, and secondary NameNode servers.
The slaves file contains a list of hosts, one per line, that are to host DataNode and TaskTracker servers.
The masters file contains a list of hosts, one per line, that are to host secondary NameNode servers.
If the start-all.sh script is used, a DataNode and TaskTracker will be started on each host in the slaves file, and a secondary NameNode will be started on each host in the masters file.
If the start-dfs.sh script is used, DataNodes will be started on the hosts listed in slaves, and secondary NameNodes will be started on the hosts listed in masters.
Per-process runtime environment: The file hadoop-env.sh is responsible for tailoring the per-process environment.
In particular, it includes the JAVA_HOME environment variable, which provides the JVM installation location.
This file also offers a way to provide custom parameters for each of the servers.
Reporting: Hadoop Core may be configured to report detailed information about the activities on the cluster.
Hadoop Core may report to a file, via Ganglia (http://ganglia.info/), which provides a framework for displaying graphical reports summarizing the activities of large clusters of machines.
In general, the modification of the server configuration parameters by a job have no effect on the servers.
The framework will load configuration files in order, with the values defined in later files superseding those earlier definitions.
The default values for these parameters are suitable for single-machine, single-CPU temporary use only.
The following sections discuss the minimum set for cluster configuration.
It informs the framework of the directory to use for all Hadoop Core server data storage, as follows:
This may be a comma- or space-separated list of directories.
This is of critical importance and should be stored on a low-latency device with redundancy.
This directory will experience bulk I/O that has a short life.
This may be a comma- or space-separated list of directories.
By default, HDFS replicates data storage blocks to multiple DataNodes.
This may be a comma-separated list of directories, preferably on different devices.
I/O will be spread among the directories for increased performance.
This directory will also experience bulk I/O that has a short life.
This value must be unique per JobTracker if multiple MapReduce clusters share a single HDFS.
The cluster administrator must pick a location or locations for these directories that provide the required I/O performance and the required reliability.
The HDFS-level redundant block storage reduces the requirements for highly reliable block storage for individual DataNodes.
The directory specified by the dfs.data.dir parameter will experience bulk I/O transactions and should be optimized for maximum speed.
There will be a large number of files and directories created, each file being an HDFS data block.
The framework will attempt to create the hadoop.tmp.dir directory and all of the subdirectories if they do not exist.
The user that the relevant server processes are running as must have the required permissions to be able to create these directories and to add and remove files from them.
This configuration assumes that HDFS is doing redundant block storage at the HDFS level.
If it is not configured, there is no shared file system.
The URI specified here informs the Hadoop Core framework of the default file system.
The default value is file:///, which instructs the framework to use the local file system.
The file system protocol is hdfs, the host to contact for services is NamenodeHost, and the port to connect to is 8020, which is the default port for HDFS.
You can choose an arbitrary port for the hdfs NameNode.
If it is not configured, only a single machine will be used for task execution The URI specified in this parameter informs the Hadoop Core framework of the JobTracker’s location.
The default value is local, which indicates that no JobTracker server is to be run, and all tasks will be run from a single JVM.
The JobtrackerHost is the host on which the JobTracker server process will be run.
This value is read only when the TaskTracker is started.
Changes made by a job will not be honored or persist.
This parameter should be tuned to ensure that the CPU resources of the TaskTracker nodes are fully utilized.
If the machine hosts only the TaskTracker, it is common to set.
This may result in a large memory footprint, as each of the JVMs executing tasks will have a full memory allocation.
This latter choice is preferred, as the number of threads may be set on a per-job basis, allowing the job to customize its CPU consumption.
The following sample snippet demonstrates a common pattern for per-job management of map task parallelism.
The choice of 100 was made for demonstration purposes and is not suitable for a CPU-intensive map task.
Unlike in a map task, the output key ordering is critical for a reduce tasks, which precludes running multithreaded reduce tasks.
This value also determines the number of parts in which your job output is placed.
Changes made by a job will not be honored or persist.
A significant and unexpected influence on this is the heap requirements (io.sort.mb), which by default will cause 100MB of space to be used for sorting.
The node must have sufficient virtual memory to meet the memory requirements of all of the JVMs.
JVMs have non-heap memory requirements; for simplicity, 20MB is assumed.
This 5GB value does not include memory for other processes or servers that may be running on the node.
The default location of these additional options is the bottom-left corner of the page (so you usually need to scroll down the page to see them)
A Sample Cluster Configuration In this section, we will walk through a simple configuration of a six-node Hadoop cluster.
The standard machine configuration is usually an eight-CPU machine with 8GB of RAM, and a hardware RAID controller presenting a single partition to the operating system.
The single partition presentation is not ideal for Hadoop Core, as there is no opportunity to segregate the I/O for MapReduce and for HDFS.
Configuration Requirements This configuration will require the customization of several files in the conf directory and compliance with some simple network requirements.
The user that will own the Hadoop processes must also be determined.
Network requirements Hadoop Core uses Secure Shell (SSH) to launch the server processes on the slave nodes.
Hadoop Core requires that passwordless SSH work between the master machines and all of the slave and secondary machines.
For example, for OpenSSH, you can generate an unencrypted key for the user that will own the Hadoop Core server processes.
The user will need to have a directory, ~/.ssh, that only the user has access permissions for, on all machines in the cluster.
The following command generates a dsa key with an empty password in the file ~/.ssh/id_dsa:
The quotes in the command are a pair of single quote characters, side by side.
Execute the command chmod og-rwx ~/.ssh on each machine in the cluster.
You should now be able to run bin/slaves.sh uptime and receive the output of uptime.
These slave servers will need to contact their specific master (either the NameNode or the JobTracker), and this will require that several ports in the low 50000 range be unblocked and available.
Table 3-3 lists the default ports that must be unfiltered and available.
Note Hadoop Core uses a number of TCP ports in the low 50000 range.
An example of a more complex network setup is for the machines in the Amazon cloud.
Each Amazon cloud machine has two network interfaces: one for Internet access and one for intracloud traffic.
Machine Configuration requirements In the simplest case, all of the machines in the cluster will be identically configured.
They will have the same number of CPUs, the same amount of physical RAM, and the same disk capacity and configuration, with the same file system mount points.
The same version of the JVM will be installed in the same location, and the Hadoop installation directory will be the same on all of the machines.
Hadoop Core maintains process ID files in the directory /tmp, by default.
This directory must exist and be writable by the user that will own the Hadoop server processes.
Caution Remember that the parent of the directory to be used for hadoop.tmp.dir must exist and be writable by the Hadoop server user, or the hadoop.tmp.dir directory must exist and be writable by the Hadoop server user.
Configuration Files for the Sample Cluster The examples provided in this section were run using the VMware images provided by Cloudera as part of its boot camp (http://www.cloudera.com/hadoop-training-basic)
The source code for this file is provided with the rest of this book’s downloadable code.
The port of 8020 is implicit in the protocol declaration.
For setting the maximum number of map and reduce tasks per TaskTracker, several assumptions are made.
The first assumption is that the map tasks will be threaded, and the individual jobs will choose a thread count that optimizes CPU utilization.
The DataNode also will run on the same machine; therefore, you must budget for CPU and memory resources.
In the best of all worlds, the DataNode would use a different set of disks for I/O than the TaskTracker.
It is possible that the reduce tasks jobs are CPU-bound, but generally, the shuffle phase is CPU-bound and the reduce phase is I/O bound.
In a high-performance cluster, these parameters will be carefully tuned for specific jobs.
Very few MapReduce jobs will run in the 200MB default heap size specified for the JVMs.
The –server configures the JVM for the HotSpot JVM and provides other performance optimizations.
The default configuration contains a single line containing localhost, which provides no real protection from machine or disk failure.
It is a wise precaution to have a secondary NameNode on another machine.
For this simple configuration example, the masters file has a single line with slave01
Distributing the Configuration One of the reasons for requiring that all of the machines be essentially identical is that this greatly simplifies managing the cluster configuration.
All of the core configuration files can be identical, which allows the use of the Unix rsync command to distribute the configuration files.
The command I like to use assumes that HADOOP_HOME is set correctly:
Note This command assumes that the current machine is not listed in the slaves or masters file.
It’s a very good test of the passwordless SSH configuration.
This ensures that all servers are running the same configuration files and the same Hadoop JARs.
The -e ssh forces rsync to use SSH to establish the remote machine connections.
If your installation requires different configuration files on a per-machine basis, some other mechanism will be required to ensure consistency and correctness for the Hadoop installations and configuration files.
Verifying the Cluster Configuration You should take a few steps to verify that that the cluster is installed and configured properly.
At this point, you can assume that the Hadoop installation has been replicated to the same location across the cluster machines and that passwordless SSH is working.
So, you should check the location of the JVM and make sure that HADOOP_PID_DIR can be written.
To verify that the JVM is in place and that JAVA_HOME is set correctly, run the following commands from the master machine:
Every slave machine, as well as the local machine, should have an output line.
If Java is not available on a machine in the expected location, install the JVM on that machine and set the JAVA_HOME environment variable to reflect the JVM installation directory.
The next item to verify is that the various required paths exist with the proper permissions or that the proper paths can be created.
Caution All of the commands in this section must be run from the master machine and as the user that will own the cluster servers.
Failure to do this will invalidate the verification process and may cause the cluster startup to fail in complex ways.
The shell environment is also expected to be set up, as detailed in Chapter 2, such that both java and the bin directory of the Hadoop installation are the first two components of the PATH environment variable.
The user that owns the cluster processes will be referred to in some of the following text as CLUSTER_USER.
Execute the following command to verify that HADOOP_PID_DIR and hadmp.tmp.dir are writable.
Any error message about being unable to create the file dummy must be corrected before the next step is attempted.
At this point, you have checked that the cluster installation and configuration are correct, and that passwordless SSH is enabled.
It is time to format the HDFS file system and to start the HDFS and MapReduce services.
Formatting HDFS The command to format HDFS is very simple.
It may be started with the MapReduce portion of the cluster via start-all.sh.
Here, I’ll show you how to start HDFS separately to demonstrate the commands, as it is common to separate the HDFS master and configuration from the MapReduce master and configuration.
Your output should look similar to the preceding sample output.
Some common reasons for failures are listed in the next section.
After roughly one minute, allowing the DataNodes time to start and connect to the NameNode, issue the command that reports on the status of the DataNodes:
If there are not five DataNodes, or the HDFS reports that it is in safe mode, something has gone wrong.
Detailed log messages will be available in the logs directory of the Hadoop installation on the slave that is not reporting.
The final test is to attempt to copy a file into HDFS, as follows:
A zero-length file named my_first_file will be created in the /user/USERNAME directory, where USERNAME is the username of the user running the touchz command.
Correcting Errors The most common errors should not occur if the verification steps detailed in the preceding sections completed with no errors.
The most exotic error should not occur at this point, as your installation should be using the Hadoop Core default classpath.
If there is an error message in a log file that indicates that Jetty could not start its web server, there is a nonvalidating XML parser in the classpath ahead of the validating XML parser that Hadoop Core supplies.
This may be fixed by reordering the classpath or by explicitly setting the XML parser by setting a Java property.
You can modify the HADOOP_OPTS environment variable to include this string:
Caution It is highly recommended that you verify the termination of all Hadoop Core server processes across the cluster before attempting a restart.
It is also strongly recommended that the hadoop.tmp.dir have its contents wiped on all cluster machines between attempts to start a new HDFS, and then the file system be reformatted as described in this chapter.
As shown in Figure 3-3, this interface shows information about the cluster.
It also provides links to browse the file system, view the NameNode logs, and to drill down to specific node information.
Starting MapReduce The MapReduce portion of the cluster will be started by the start-mapred.sh command.
If there are any errors when starting the TaskTrackers, the detailed error message will be in the logs directory on the specific slave node of the failed TaskTracker.
The common reasons for failure are very similar to those for HDFS node startup failure.
Running a Test Job on the Cluster To test the cluster configuration, let’s run our old friend the Hadoop Core example pi (introduced in Chapter 1)
Recall that this program takes two arguments: the number of maps and the number of samples.
In this case, the cluster has five map slots, so you will set the number of maps to five.
You have a couple of machines, so you can set the number of samples to a moderately large number—say 10,000
Summary This chapter provided a simple walk-through of configuring a small Hadoop Core cluster.
It did not discuss the tuning parameters required for a larger or a high-performance cluster.
For a multimachine cluster to run, the configuration must include the following:
You now have an understanding of what a master node is, what the NameNode and JobTracker servers do, and what DataNode and TaskTracker servers are.
You may have even set up a multimachine cluster and run jobs over it.
This chapter covers HDFS installation for multimachine clusters that are not very small, as well as HDFS tuning factors, recovery procedures, and troubleshooting tips.
But first, let’s look at some of the configuration trade-offs faced by IT departments.
Configuration Trade-Offs There appears to be an ongoing conflict between the optimal machine and network configurations for Hadoop Core and the configurations required by IT departments.
These strategies reduce the risk of machine failure and provide network diagnostics, flexibility, and simplified administration.
Hadoop Core does not need highly reliable storage on the DataNode or TaskTracker nodes.
The highest performance Hadoop Core installations will have separate and possibly multiple disks or arrays for each stream of I/O.
The DataNode storage will be spread over multiple disks or arrays to allow interleaved I/O, and the TaskTracker intermediate output will also go to a separate disk or array.
This configuration reduces the contention for I/O on any given array or device, thus increasing the maximum disk I/O performance of the machine substantially.
If the switch ports are inexpensive, using bonded network interface cards (NICs) to increase per machine network bandwidth will greatly increase the I/O performance of the cluster.
Hadoop Core provides high availability of DataNode and TaskTracker services without requiring special hardware, software, or configuration.
However, there is no simple solution for high availability for the NameNode or JobTracker.
High availability for the NameNode is an active area of development within the Hadoop community.
All of these techniques require special configuration and have some performance cost.
With Hadoop 0.19.0, there is built-in recovery for JobTracker failures, and generally, high availability for the JobTracker is not considered critical.
Hadoop Core is designed to take advantage of commodity hardware, rather than more expensive specialpurpose hardware.
Most of the disk drives that are being used for Hadoop are inexpensive SATA drives that generally have a sustained sequential transfer rate of about 70Mbps.
Mixed read/write operations provide about 100 Mbps, as all I/O operations require seeks on the same set of drives.
If the six drives were provided as individual drives or as three RAID 0 pairs, the mixed read/write I/O transfer rate would be higher, as seeks for each I/O operation could occur on different drives.
The common network infrastructure is GigE network cards on a GigE switch, providing roughly 100 Mbps I/O.
I’ve used bonded pairs of GigE cards to provide 200 Mbps I/O for high-demand DataNodes to good effect.
For Hadoop nodes, providing dumb, unmanaged crossbar switches for the DataNodes is ideal.
Hdfs installation for multimachine Clusters Setting up an HDFS installation for a multimachine cluster involves the following steps:
This will configure a cluster with a NameNode on the host master, and all HDFS storage under the directory /hdfs.
Generating the conf/slaves and conf/masters Files On the machine master, create the conf/slaves file, and populate it with the names of the hosts that will be DataNodes, one per line.
In the file conf/masters, add a single host to be the secondary NameNode.
For safety, make it a separate machine, rather than localhost.
At a very minimum, this script must ensure that the correct JAVA_HOME environment variable is set.
Table 4-1 provides a list of the required, commonly set, and optional environment variables.
HADOOP_CLIENT_OPTS  Additional command-line arguments for all nonserver processes started by bin/hadoop.
This is not applied to the server processes, such as the NameNode, JobTracker, TaskTracker, and DataNode.
HADOOP_PID_DIR  The directory that server process ID (PID)  /tmp files are written to.
Used by the service start and stop scripts to determine if a prior instance of a server is running.
The default, /tmp, is not a good location, as the server PID files will be periodically removed by the system temp file cleaning service.
HADOOP_NICENESS  CPU scheduling nice factor to apply to server processes.
The suggested settings prioritize the DataNode over the TaskTracker to ensure that DataNode requests are more rapidly serviced.
They also help ensure that the NameNode or JobTracker servers have priority if a DataNode or TaskTracker is colocated on the same machine.
These suggested settings facilitate smooth cluster operation and enable easier monitoring.
HADOOP_CLASSPATH  Extra entries for the classpath for all Hadoop Java processes.
If your jobs always use specific JARs, and these JARs are available on all systems in the same location, adding the JARs here ensures that they are available to all tasks and reduces the overhead in setting up a job on the cluster.
See this book’s appendix for details on the JobConf object.
HADOOP_OPTS  Additional command-line options for all  -server processes started by bin/hadoop.
Distributing Your Installation Data Distribute your Hadoop installation to all the machines in conf/masters and conf/slaves, as well as the machine master.
Finally, ensure that passwordless SSH from master to all of the machines in conf/masters and conf/slaves works.
The user is responsible for ensuring that the JAVA_HOME environment variable is configured.
The framework will issue an ssh call to each slave machine:
Formatting Your HDFS At this point, you are ready to actually format your HDFS installation.
Run the following to format a NameNode for the first time:
If you have already formatted a NameNode with this data directory, you will see that the command will try to reformat the NameNode:
If the user doing the formatting does not have write permissions, the output will be as follows:
If you are reformatting an HDFS installation, it is recommended that you wipe the hadoop.tmp.dir directories on all of the DataNode machines.
Starting Your HDFS Installation After you’ve configured and formatted HDFS, it is time to actually start your multimachine HDFS cluster.
You can use the bin/start-dfs.sh command for this, as follows:
If you have any lines like the following in your output, then the script was unable to ssh to the slave node:
If you have a block like the following, you have not distributed your Hadoop installation correctly to all of the slave nodes:
The following output indicates that the directory specified in hadoop-env.sh for the server PID files was not writable:
When Hadoop Core is starting services on the cluster, the required directories for the server operation are created if needed.
These include the PID directory and the working and temporary storage directories for the server.
If the framework is unable to create a directory, there will be error messages to that effect logged to the error stream of the script being used to start the services.
Any messages in the startup output about files or directories that are not writable, or directories that could not be created, must be addressed.
In some cases, the server will start, and the cluster may appear to run, but there will be stability and reliability issues.
At this point, the next step is to verify that the DataNode servers started and that they were able to establish service with the NameNode.
Verifying HDFS Is Running To verify that the server processes are in fact running, wait roughly 1 minute after the finish of start-dfs.sh, and then check that the NameNode and DataNode exist.
Checking the NameNodes On the master machine, run the jps command as follows:
The first value in the output is the PID of the java process, and it will be different in your output.
Listing 4-2 indicates that the NameNode process was unable to find a valid directory for HDFS metadata.
When this occurs, the command hadoop namenode –format must be run, to determine the actual failure.
If the format command completes successfully, the next start-dfs.sh run should complete successfully.
The example in Listing 4-3 demonstrates the failed format command output.
The web interface provided by the NameNode will show information about the status of the NameNode.
Checking the DataNodes You also need to verify that there are DataNodes on each of the slave nodes.
Use jps via the bin/slaves.sh command to look for DataNode processes:
If you do not have a DataNode on each of the slaves, something has failed.
Each machine may have a different reason for failure, so you’ll need to examine the log files on each machine.
The common reason for DataNode failure is that the dfs.data.dir was not writable, as shown in Listing 4-4
The DataNode may also be unable to contact the NameNode due to network connectivity or firewall issues.
In fact, I had half of a new cluster fail to start, and it took some time to.
Listing 4-5 shows an excerpt from a DataNode log for a DataNode that failed to start due to network connectivity problems.
If the IP address is correct, verify that the NameNode is accepting connections on that port.
Hadoop also provides the dfsadmin -report command-line tool, which will provide a somewhat verbose listing of the DataNodes in a service.
You can run this useful script from your system monitoring tools, so that alerts can be generated if DataNodes go offline.
Tuning factors Here, we will look at tuning the cluster system and the HDFS parameters for performance and reliability.
Commonly, the two most important factors are network bandwidth and disk throughput.
Memory use and CPU overhead for thread handling may also be issues.
Using large blocks helps reduce the cost of a disk seek compared with the read/write time, thereby increasing the aggregate I/O rate when multiple requests are active.
The large input-split size reduces the ratio of task setup time to task run time, as there is work to be done to set up a task before the TaskTracker can start the mapper or reducer on the input split.
The various tuning factors available control the maximum number of requests in progress.
In general, the more requests in progress, the more contention there is for storage operations and network bandwidth, with a corresponding increase in memory requirements and CPU overhead for handling all of the outstanding requests.
If the number of requests allowed is too low, the cluster may not fully utilize the disk or network bandwidth, or cause requests to timeout.
Most of the tuning parameters for HDFS do not yet have exact science behind the settings.
In general, the selections are a compromise between various factors.
Finding the sweet spot requires knowledge of the local system and experience.
File Descriptors Hadoop Core uses large numbers of file descriptors for MapReduce, and the DFSClient uses a large number of file descriptors for communicating with the HDFS NameNode and DataNode server processes.
The DFSClient code also presents a misleading error message when there has been a failure to allocate a file descriptor: No live nodes contain current block.
All file system operations and file operations performed by the application will be translated into method calls on the DFSClient object, which will in turn issue the appropriate Remote Procedure Call (RPC) calls to the NameNode and the DataNodes relevant for the operations.
Prior to Hadoop 0.18, blocking operations and fixed timeouts were used for the RPC calls.
Most sites immediately bump up the number of file descriptors to 64,000, and large sites, or sites that have MapReduce jobs that open many files, might go higher.
This changes the per-user file descriptor limit to 64,000 file descriptors.
If you will run a much larger number of file descriptors, you may need to alter the per-system limits via changes to fs.file-max in /etc/sysctl.conf.
A line of the following form would set the system limit to 640,000 file descriptors:
At this point, you may alter the limits.conf file line to this:
Changes to limits.conf take effect on the next login, and changes to sysctl.conf take place on the next reboot.
You may run sysctl by hand (sysctl -p) to cause the sysctl.conf file to be reread and applied.
The web page at http://support.zeus.com/zws/faqs/2005/09/19/filedescriptors provides some instructions for several Unix operating systems.
Any user that runs processes that access HDFS should have a large limit on file descriptor access, and all applications that open files need careful checking to make sure that the files are explicitly closed.
Trusting the JVM garbage collection to close your open files is a critical mistake.
Block Service Threads Each DataNode maintains a pool of threads that handle block requests.
Currently, there does not appear to be significant research or tools on the tuning of this parameter.
The overall concept is to balance JVM overhead due to the number of threads with disk and network I/O.
The more requests active at a time, the more overlap for disk and network I/O there is.
At some point, the overlap results in contention rather than increased performance.
Medium-size clusters may use a value of 30, set as follows:
NameNode Threads Each HDFS file operation—create, open, close, read, write, stat, and unlink—requires a NameNode transaction.
The NameNode maintains the metadata of the file system in memory.
Any operation that changes the metadata, such as open, write, or unlink, results in the NameNode writing a transaction to the disks, and an asynchronous operation to the secondary NameNodes.
This adds a substantial latency to any metadata altering transaction.
Through Hadoop 0.19.0, the NameNode edit logs are forcibly flushed to disk storage, but space for the updates is preallocated before the update to reduce the overall latency.
Increasing this value will substantially increase the memory utilization of the NameNode and may result in reduced performance due to I/O contention for the metadata updates.
However, if your map and reduce tasks create or remove large numbers of files, or execute many sync operations after writes, this number will need to be higher, or you will experience file system timeouts in your tasks.
In my setup, I have a cluster of 20 DataNodes that are very active.
For large directories, this can be a considerable amount of data.
With many threads servicing requests, the amount of memory used by in transit serialized requests may be very large.
I have worked with a cluster where this transitory memory requirement exceeded 1GB.
This value is the number of outstanding, unserviced connection requests allowed by the operating system, before connections are refused.
Many operating systems have hard and small limits for this value.
If your thread counts are lower, this value will need to be higher, to prevent refused connections.
If you find connection-refused errors in your log files, you may want to increase the value of this parameter.
As a method for preventing out-of-disk conditions, Hadoop Core provides four parameters: two for HDFS and two for MapReduce.
It is common, especially in smaller clusters, for a DataNode and a TaskTracker to reside on each computer node in the cluster.
It is also common for the TaskTracker’s temporary storage to be stored in the same file system as the HDFS data blocks.
With HDFS, blocks written by an application that is running on the same machine as a DataNode will have one replica placed on that DataNode.
It is very easy for a task to fill up the disk space on a partition and cause HDFS failures, or for HDFS to fill up a partition and result in task failures.
Tuning the disk space reservation parameters will minimize these failures.
If you have more than one TaskTracker on the node, you may need to multiply this minimum by the TaskTracker count to ensure sufficient space.
This parameter has a default value of 0, disabling the check.
This parameter has a default value of 0, disabling the check.
This parameter has a default value of 0, disabling the check.
Storage Allocations The Balancer service will slowly move data blocks between DataNodes to even out the storage allocations.
This is very helpful when DataNodes have different storage capacities.
The argument –threshold VALUE, is optional, and the default threshold is 10%
The Balancer task will run until the free space percentage for block storage of each DataNode is approximately equal, with variances in free space percentages of up to the defined threshold allowed.
It is common for the Balancer task to be run automatically and periodically.
In general, the Balancer should be used if some DataNodes are close to their free space limits while other DataNodes have plenty of available space.
This usually becomes an issue only when the DataNode storage capacity varies significantly or large datasets are written into HDFS from a machine that is also a DataNode.
The Balancer may not be able to complete successfully if the cluster is under heavy load, the threshold percentage is very small, or there is insufficient free space available.
If you need to stop the Balancer at any time, you can use the command stop-balancer.sh.
Disk I/O Hadoop Core is designed for jobs that have large input datasets and large output datasets.
This I/O will be spread across multiple machines and will have different patterns depending on the purpose of the I/O, as follows:
The default for Hadoop is to place all of these storage areas under the directory defined by the parameter hadoop.tmp.dir.
In installations where multiple partitions, each on a separate physical drive or RAID assembly, are available, you may specify directories and directory sets that are stored on different physical devices to minimize seek or transfer contention.
All of the HDFS data stores are hosted on native operating system file systems.
For the NameNode data stores, RAID 1 with hot spares is preferred for the low-level storage.
For the DataNodes, no RAID is preferred, but RAID 5 is acceptable.
The file systems should be constructed with awareness of the stripe and stride of any RAID arrays and, where possible, should be mounted in such a way that access time information is not updated.
Linux supports disabling the access time updates for several file systems with the noatime and nodiratime file system mount time options.
I have found that this change alone has provided a 5% improvement in performance on DataNodes.
Journaled file systems are not needed for the NameNode, as the critical data is written synchronously.
Journaled file systems are not recommended for DataNodes due to the increase in write loading.
The downside of not having a journal is that crash recovery time becomes much larger.
Secondary NameNode Disk I/O tuning The secondary NameNode provides a replica of the file system metadata that is used to recover a failed NameNode.
It is critically important that its storage directories be on separate physical devices from the NameNode.
There is some debate about locating the secondary NameNode itself on a separate physical machine.
The merging of the edit logs into the file system image may be faster and have lower overhead when the retrieval of the edit logs and writing of the file system image are local.
However, having the secondary NameNode on a separate machine provides rapid recovery to the previous checkpoint time in the event of a catastrophic failure of the NameNode server machine.
The update operations are allowed to run behind the NameNode’s updates.
The secondary NameNode server will take a snapshot from the NameNode at defined time intervals.
At that point, the NameNode will close the current edit log and start a new edit log.
The file system image and the just closed edit log will be copied to the secondary NameNode.
The secondary NameNode will apply the edit log to the file system image, and then transfer the up-to-date file system image back to the NameNode, which replaces the prior file system image with the merged copy.
The secondary NameNode configuration is not commonly altered, except in very high utilization situations.
NameNode Disk I/O tuning The NameNode is the most critical piece of equipment in your Hadoop Core cluster.
The NameNode, like the secondary NameNode, maintains a journal and a file system image.
Some installations will place directories on remote mounted file systems in this list, to ensure that an exact copy of the data is available in the event of a NameNode disk failure.
Caution The NameNode is a critical single point of failure.
Any loss of data can wipe out your entire HDFS datastore.
DataNode Disk I/O tuning The DataNode provides two services: block storage, and retrieval of HDFS data and storage accounting information for the NameNode.
Through at least Hadoop 0.19.0, the storage accounting has significant problems if there are more than a few hundred thousand blocks per DataNode.
This is because a linear scan of the blocks is performed on a frequent basis to provide accounting information.
The following are some other DataNode parameters that may be adjusted:
The HDFS cluster can withstand the failure of one less than the value of dfs.replication before there will be service degradation, in that some files may not be served as all of the blocks are not available.
In small to medium-sized clusters, 3 is a good value.
For large clusters, it should be a number that is at least two larger than the expected number of machine failures per day.
The disk storage requirements and the write network bandwidth used are multiplied by this number.
Each node writes to the next node, to mitigate the network load on the individual machines.
The nodes are selected more or less on a random basis, with some simple rules.
If the origination machine is a DataNode for the HDFS cluster being written, one replica will go to that DataNode.
Each file data block in the file system will be in a single file in the dfs.data.dir directory.
A file in HDFS is composed of one or more data blocks.
The last data block of a file may have less than the dfs.block.size bytes of data present.
The block size may be specified on a per-file basis when the file is created.
The individual blocks of the file are replicated onto dfs.replication DataNodes.
The default input split size for a file being used as input in a MapReduce job is the dfs.block.size for the file.
The performance tuning must take place at the application layer and the hardware layer.
Hadoop Core does provide the ability to select the network to bind to for data services and the ability to specify an IP address to use for hostname resolution.
This is ideal for clusters that have an internal network for data traffic and an external network for client communications.
If your installation has switch ports to spare and the switches support it, bonding your individual network connections can greatly increase the network bandwidth available to individual machines.
It is best to isolate this traffic from your other traffic.
At the application layer, if your applications are data-intensive and your data is readily compressible, using block- or record-level compression may drastically reduce the I/O that the job requires.
Compressed input files that are not SequenceFiles, a Hadoop Core binary file format, will not be split, and a single task will handle a single file.
Here, we will look at how HDFS protects from many individual failures and how to recover from other failures.
NameNode Recovery The default Hadoop installation provides no protection from catastrophic NameNode server failures.
You can avoid data loss and unrecoverable machine failures by running the secondary NameNode on an alternate machine.
Storing the file system image and file system edit logs on multiple physical devices, or even multiple physical machines, also provides protection.
When a NameNode server fails, best practices require that all the JobTracker and TaskTrackers be restarted after the NameNode is restarted.
All incomplete HDFS blocks will be lost, but there should be no file or data loss for existing files or for completed blocks in actively written files.
The NameNode may be configured to write the metadata log to multiple locations on the host server’s file system.
In the event of data loss or corruption to one of these locations, the NameNode may be recovered by repairing or removing the failed location from the configuration, removing the data from that location, and restarting the NameNode.
For rapid recovery, you may simply remove the failed location from the configuration and restart the NameNode.
If the NameNode needs to be recovered from a secondary NameNode, the procedure is somewhat more complex.
All data written to HDFS after the last secondary NameNode checkpoint was taken will be removed and lost.
The default frequency of the checkpoints is specified by the fs.checkpoint.
At present, there are no public forensic tools that will recover data from blocks on the DataNodes.
DataNode Recovery and Addition The procedure for adding a new DataNode to a cluster and restarting a failed DataNode are identical and simple.
The server process just needs to be started, assuming the configuration is correct, and the NameNode will integrate the new server or reintegrate a restarted server into the cluster.
Tip As long as your cluster does not have underreplicated files and no file’s replication count is less than 3, it is generally safe to forcibly remove a DataNode from the cluster by killing the DataNode server process.
The next section covers how to decommission a DataNode gracefully.
The command hadoop-daemon.sh start datanode will start a DataNode server on a machine, if one is not already running.
The configuration in the conf directory associated with the script will be used to determine the NameNode address and other configuration parameters.
If more DataNodes than the dfs.replication value fail, some file blocks will be unavailable.
Your cluster will still be able to write files and access the blocks of the files that remain available.
It is advisable to stop your MapReduce jobs by invoking the stop-mapred.sh script, as most applications do not deal well with partial dataset availability.
When sufficient DataNodes have been returned to service, you may resume MapReduce job processing by invoking the start-mapred.sh script.
When you add new nodes, or return a node to service after substantial data has been written to HDFS, the added node may start up with substantially less utilization than the rest of the DataNodes in the cluster.
DataNode Decommissioning A running DataNode sometimes needs to be decommissioned.
While you may just shut down the DataNode, and the cluster will recover, there is a procedure for gracefully decommissioning a running DataNode.
This procedure becomes particularly important if your cluster has underreplicated blocks or you need to decommission more nodes than your dfs.replication value.
Caution You must not stop the NameNode during this process, or start this process while the NameNode is not running.
One is to exclude the hosts from connecting to the NameNode, which takes effect if the parameter is set when the NameNode starts.
The other starts the decommission process for the hosts, which takes place if the value is first seen after a Hadoop dfsadmin -refreshNodes.
This file should contain one hostname or IP address per line, with standard Unix line endings.
Modify the hadoop-site.xml file by adding, or updating the following block:
When the process is complete, you will see a line in the NameNode log file like the following for each entry in the file:
Deleted File Recovery It is not uncommon for a user to accidentally delete large portions of the HDFS file system due to a program error or a command-line error.
If an erroneous large delete is in progress, your best bet is to terminate the NameNode and secondary NameNodes immediately, and then shut down the DataNodes.
Use the procedures described earlier to recover from the secondary NameNode that has the edit log modification time closest to the time the deletion was started.
Files that have not had a trash checkpoint will be under the .Trash/current directory in a path that is identical to their original path.
Troubleshooting Hdfs failures The previous section dealt with the common and highly visible failure cases of a server process crashing or the machine hosting a server process failing.
This section will cover how you can determine what has happened when the failure is less visible or why a server process is crashing.
There are a number of failures that can trip up an HDFS administrator or a MapReduce programmer.
In the current state of development, it is not always clear from Hadoop’s behavior or log messages what the failure or solution is.
NameNode Failures The NameNode is the weak point in the highly available HDFS cluster.
As noted earlier, currently there are no high-availability solutions for the NameNode.
The NameNode has been designed to keep multiple copies of critical data on the local machine and close in time replicas on auxiliary machines.
Out of Memory The NameNode keeps all of the metadata for the file system in memory to speed request services.
The NameNode also serializes directory listings before sending the result to requesting applications.
The memory requirements grow with the number of files and the total number of blocks in the file system.
If your file system has directories with many entries and applications are scanning the directory, this can cause a large transient increase in the memory requirements for the name server.
I once had a cluster that was using the Filesystem in Userspace (FUSE) contrib package to export HDFS as a read-only file system on a machine, which re-exported that file system via the Common Internet File System (CIFS) to a Windows server machine.
The access patterns triggered repeated out-of-memory exceptions on the NameNode.
If the DataNodes are unreliable, and they are dropping out of service and then returning to service after a gap, the NameNode will build a large queue of blocks with invalid states.
This may consume very large amounts of memory if large numbers of blocks become transitorily unavailable.
For this problem, addressing the DataNode reliability is the only real solution, but increasing the memory size on the NameNode can help.
As a general rule, pin the full memory allocation by setting the starting heap size for the JVM to the maximum heap size.
Even so, catastrophic failures or user errors can result in data loss.
The NameNode configuration will accept a comma-separated list of directories and will maintain copies of the data in the full set of directories.
This additional level of redundancy provides a current time backup of the file system metadata.
The secondary NameNode provides a few-minutes-behind replica of the NameNode data.
If your configuration has multiple directories that contain the file system image or the edit log, and one of them is damaged, delete that directory’s content and restart the NameNode.
If the directory is unavailable, remove it from the list in the configuration and restart the NameNode.
If all of the dfs.name.dir directories are unavailable or suspect, do the following:
If there is no good copy of the NameNode data, the secondary NameNode image may be imported.
You may simply copy the file system image from the secondary NameNode to a file system image directory on the NameNode, and then restart.
As the imported data is older than the current state of the HDFS file system, the NameNode may spend significant time in safe mode as it brings the HDFS block store into consistency with restored snapshot.
No Live Node Contains Block errors Usually, if you see the no live node contains block error, it will be in the log files for your applications.
It means that the client code in your application that interfaces with HDFS was unable to find a block of a requested file.
For this error to occur, the client code received a list of DataNode and block ID pairs from the NameNode, and was unable to retrieve the block from any of the DataNodes.
This error commonly occurs when the application is unable to open a connection to any of the DataNodes.
This may be because there are no more file descriptors available, there is a DNS resolution failure, there is a network problem, or all of the DataNodes in question are actually unavailable.
This may be corrected by increasing the number of file descriptors available, as described earlier in this chapter.
An alternative is to minimize unclosed file descriptors in the applications.
I’ve seen DNS resolution failures transiently appear, and as a general rule, I now use IP addresses instead of hostnames in the configuration files.
It is very helpful if the DNS reverse lookup returns the same name as the hostname of the machine.
Write Failed If there are insufficient DataNodes available to allow full replication of a newly written block, the write will not be allowed to complete.
This may result in a zero-length or incomplete file, which will need to be manually removed.
DataNode or NameNode Pauses Through at least Hadoop 0.19.0, the DataNode has two periodic tasks that do a linear scan of all of the data blocks stored by the DataNode.
If this process starts taking longer than a small number of minutes, the DataNode will be marked as disconnected by the NameNode.
When a DataNode is marked as disconnected, the NameNode queues all of the blocks that had a replica on that DataNode for replication.
If the number of blocks is large, the NameNode may pause for a noticeable period while queuing the blocks.
The only solutions for this at present are to add enough DataNodes, so that no DataNode has more than a few hundred thousand data blocks, or to alter your application’s I/O patterns to use Hadoop archives or zip files to pack many small HDFS subblock-size files into single HDFS files.
The latter approach results in a reduction in the number of blocks stored in HDFS and the number of blocks per DataNode.
A simple way to work out how many blocks is too many is to run the following on a DataNode:
If it takes longer than a few hundred seconds, you are in the danger zone.
If it takes longer than a few minutes, you are in the pain zone.
Replace dfs.data.dir with an expanded value from your Hadoop configuration.
If your ls takes anywhere close to that, your cluster will be unstable.
With reasonable care and an understanding of HDFS’s limitations it will serve you well.
Through HDFS 0.19.0, the HDFS NameNode is a single point of failure and needs careful handling to minimize the risk of data loss.
Using a separate machine for your secondary NameNode, and having multiple devices for the file system image and edit logs, will go a long way toward providing a fail-safe, rapid recovery solution.
Monitoring your DataNode’s block totals will go a long way toward avoiding congestion collapse issues in your HDFS.
You can do this by simply running a find on the dfs.data.dir and making sure that it takes less than a couple of minutes.
Ensuring that your HDFS data traffic is segregated from your normal application traffic and crosses as few interswitch backhauls as possible will help to avoid network congestion and application misbehavior.
Ultimately, remember that HDFS is designed for a small to medium number of very large files, and not for transitory storage of large numbers of small files.
Organizations run Hadoop Core to provide MapReduce services for their processing needs.
They may have datasets that can’t fit on a single machine, have time constraints that are impossible to satisfy with a small number of machines, or need to rapidly scale the computing power applied to a problem due to varying input set sizes.
You will have your own unique reasons for running MapReduce applications.
To do your job effectively, you need to understand all of the moving parts of a MapReduce cluster and of the Hadoop Core MapReduce framework.
This chapter will raise the hood and show you some schematics of the engine.
This chapter will also provide examples that you can use as the basis for your own MapReduce applications.
Requirements for Successful MapReduce Jobs For your MapReduce jobs to be successful, the mapper must be able to ingest the input and process the input record, sending forward the records that can be passed to the reduce task or to the final output directly, if no reduce step is required.
The reducer must be able to accept the key and value groups that passed through the mapper, and generate the final output of this MapReduce step.
The job must be configured with the location and type of the input data, the mapper class to use, the number of reduce tasks required, and the reducer class and I/O types.
The TaskTracker service will actually run your map and reduce tasks, and the JobTracker service will distribute the tasks and their input split to the various trackers.
The cluster must be configured with the nodes that will run the TaskTrackers, and with the number of TaskTrackers to run per node.
The TaskTrackers need to be configured with the JVM parameters, including the classpath for both the TaskTracker and the JVMs that will execute the individual tasks.
There are three levels of configuration to address to configure MapReduce on your cluster.
From the bottom up, you need to configure the machines, the Hadoop MapReduce framework, and the jobs themselves.
We’ll get started with these requirements by exploring how to launch your MapReduce jobs.
Tip A Hadoop job is usually part of a production application, which may have many steps, some of which are MapReduce jobs.
Hadoop Core, as of version 0.19.0, provides a way of optimizing the data flows between a set of sequential MapReduce jobs.
Launching MapReduce Jobs Jobs within a MapReduce cluster can be launched by constructing a JobConf object (details on the JobConf object are provided in this book’s appendix) and passing it to a JobClient object:
You can launch the preceding example from the command line as follows:
For this to be successful requires a considerable amount of runtime environment setup.
Hadoop Core provides a shell script, bin/hadoop, which manages the setup for a job.
Using this script is the standard and recommended way to start a MapReduce job.
This script sets up the process environment correctly for the installation, including inserting the Hadoop JARs and Hadoop configuration directory into the classpath, and launches your application.
This behavior is triggered by providing the initial command-line argument jar to the bin/hadoop script.
Hadoop Core provides several mechanisms for setting the classpath for your application:
The advantage of using the DistributedCache and -libjars is that resources, such as JAR files, do not have to already exist on the TaskTracker nodes.
The disadvantages are that the resources must be unpacked on each machine and it is harder to verify which versions of the resources are used.
When launching an application, a number of command-line parameters may be provided.
These JAR files will be staged into HDFS if needed and made available as local files in a temporary job area on each of the TaskTracker nodes.
There must be whitespace between the -D and the key=value.
You can use hadoop jar to launch an application, as follows:
There are two distinct steps in the argument processing of jobs submitted by the bin/ hadoop script.
The first step is provided by the framework via the JobShell.
The arguments after jar are processed by the JobShell, per Table 5-1
The first argument not in the set recognized by the JobShell must be the path to a JAR file, which is the job JAR file.
If the job JAR file contains a main class specification in the manifest, that class will be the main class called after the first step of argument processing is complete.
If the JAR file does not have a main class in the manifest, the next argument becomes required, and is used as main class name.
Any remaining unprocessed arguments are passed to the main method of the main class as the arguments.
The second step is the processing of the remaining command-line arguments by the user-specified main class.
For example, one of my jobs required a shared library that handled job-specific image processing.
Caution If you are manually loading shared libraries, the library name passed to System.
This can result in library load failures that are hard to diagnose.
MapReduce-Specific Configuration for Each Machine in a Cluster For simplicity and ease of ongoing maintenance, this section assumes identical Hadoop Core installations will be placed on each of the machines, in the same location.
The following are the MapReduce-specific configuration requirements for each machine in the cluster:
The hadoop-env.sh script has a section for providing custom JVM parameters for the different Hadoop Core servers, including the JobTracker and TaskTrackers.
As of Hadoop 0.19.0, the classpath settings are global for all servers.
It is much better to modify the file, as the changes are persistent and stored in a single Hadoop-specific location.
Using the Distributed Cache The DistributedCache object provides a programmatic mechanism for specifying the resources needed by the mapper and reducer.
You may also invoke your MapReduce program using the bin/hadoop script and provide arguments for -libjars, -files, or -archives.
Caution The paths and URIs for DistributedCache items are stored as comma-separated lists of strings in the configuration.
Any comma characters in the paths will result in unpredictable and incorrect behavior.
Adding Resources to the Task Classpath Four methods add elements to the Java classpath for the map and reduce tasks.
The first three in the following list add archives to the classpath.
The archives are unpacked in the job local directory of the task.
You can use the following methods to add resources to the task classpath:
It is on the JobConf object, but it manipulates the same configuration keys as the DistributedCache.
The file jar will be found, and if necessary, copied into the shared file system, and the full path name on the shared file system stored under the configuration key mapred.jar.
This is a static method, and the archive (a zip or JAR file) will be made available to the running tasks via the classpath of the JVM.
The archive is also added to the list of cached archives.
The contents will be unpacked in the local job directory on each TaskTracker node.
If the path component of the URI does not exactly equal archive, archive will not be placed in the classpath of the task correctly.
Caution The archive path must be on the JobTracker shared file system, and must be an absolute path.
This is a static method that makes the file available to the running tasks via the classpath of the JVM.
If file is not exactly equal to the path portion of the constructed URI, file will not be added to the classpath of the task correctly.
Caution The file path added must be an absolute path on the JobTracker shared file system, and be only a path.
Local file system copies of these items are made on all of the TaskTracker machines, in the work area set aside for this job.
The URI must have an absolute path and be on the JobTracker shared file system.
If the URI has a fragment, a symbolic link to the archive will be placed in the task working directory as the fragment.
The archive will be unpacked into the local working directory of the task.
The URI must be on the JobTracker shared file system.
If the URI has a fragment, a symbolic link to the URI fragment will be created in the JVM working directory that points to the location on the local file system where the uri was unpacked into at task start.
The directory where DistributedCache stores the local copies of the passed items is not the current working directory of the task JVM.
This allows the items to be referenced by names that do not have any path components.
If name has a leading slash, this method will search for it in each location in the classpath, and return the URI.
The cache items will be the last items in the classpath.
The paths will be in the task local area of the local file system.
It is possible that the file name portion of your archive will be changed slightly.
DistributedCache provides the following method to help with this situation:
This takes an original archive path and returns the possibly altered file name component.
The file name portions of the paths may be different from the original file name.
Finding a File or archive in the Localized Cache The DistributedCache object may change the file name portion of the files and archives it distributes.
This is usually not a problem for classpath items, but it may be a problem for nonclasspath items.
In addition to the file name portion, the items will be stored in a location relative to the working area for the task on each TaskTracker.
Table 5-2 lists the methods provided in the downloadable code that.
These methods are designed to be used in the mapper and reducer methods.
Configuring the Hadoop Core Cluster Information The JobConf object provides two basic and critical ways for specifying the default file system: the URI to use for all shared file system paths, and the connection information for the JobTracker server.
Note The default value for the file system URI is file:///, which stores all files on the local file system.
The file system that is used must be a file system that is shared among all of the nodes in the cluster.
This will override the value specified in the hadoop-site.xml file.
Here’s a sample command line for listing files on an explicitly specified HDFS file system:
You can also use the JobConf object to set the default file system:
Through Hadoop 0.19, there is not a standard for the PORT.
Many installations use a port one higher that the HDFS port.
Note The default value for the JobTracker location is local, which will result in the job being executed by the JVM that submits it.
The value local is ideal for testing and debugging new MapReduce jobs.
It is important to ensure that any required Hadoop configuration files are in the classpath of the test jobs.
Here’s a sample command line explicitly setting the JobTracker for job control for listing jobs:
And here’s how to use the JobConf object to set the JobTracker information:
When run as a Java application, this example accepts all of the standard Hadoop arguments and may be run with custom bean context and definitions:
Mapper Methods For the mapper, the framework will call three methods:
If the class is an instance of Configurable, newInstance will call the setConf method with the supplied configuration.
If the class is an instance of JobConfiguration, newInstance will call the configure method.
Any exceptions that are thrown during the construction or initialization of the instance are rethrown as RuntimeExceptions.
The framework may attempt to retry this task on another host if the allowable number of failures for the task has not been exceeded.
It is considered good practice for any Mapper implementation to declare a member variable that the configure() method uses to store a reference to the passed-in JobConf object.
The configure() method is also used for loading any Spring application context or initializing resources that are passed via DistributedCache.
In this example, K2 is the map output key type, which defaults to the reduce output key type, which defaults to LongWritable.
V2 is the map output value key type, which defaults to the reduce output value type, which defaults to Text.
This configure() method saves a copy of the JobConf object taskId and taskName into member variables.
By using the isMap method on the taskId, you can take different actions for map and reduce tasks in the configure() and close() methods.
This becomes very useful when a single class provides both a map method and a reduce method.
No calls will be made to the map() method in an instance before the configure() method completes.
If the job is configured for running multithreaded map tasks, as follows, there may be multiple simultaneous calls to the map() method.
When running multithreaded, each map() call will have a different key and value object.
The default number of threads for a multithreaded map task is ten.
The contents of the key object and the contents of the value object are valid only during the map() method call.
The framework will reset the object contents with the next key/value pair prior to the next call to map()
You can explicitly configure the map output key and value classes, as follows:
If a map output class is set, the corresponding reduce input class is also set to the class.
If the map output key class is changed to BytesWritable, the Reducer.reduce’s key type will be BytesWritable.
This method is the place to close any open files or perform any status checking.
Unless your configure() method has saved a copy of the JobConf object, there is little interaction that can be done with the framework.
The close() method example in Listing 5-2 checks the task status based on the ratio of exceptions to input keys.
The close() method in Listing 5-2 will report success or failure status back to the framework, based on an examination of the job counters.
It assumes that the map() method reported an exception under the counter, Total Map Failure, in the counter group taskName, and the number of keys received is in the counter, Total Map Keys, in the counter group taskName.
This example also logs to counters with the values received from the Spring initialization.
I found the Spring value-based counters useful while working out how to initialize map class member variables via the Spring Framework, as described after the discussion of the mapper class declaration and member fields.
Mapper Class Declaration and Member Fields It is a best practice to capture the JobConf object passed in the configure() method into a member variable.
It is also a good practice to instantiate member variables, or thread local variables, for any key or value that would otherwise be created in the body of the map() method.
Having the TaskAttemptId available is also useful, as it is easy to determine if this is the map phase or the reduce phase of a job.
It is convenient to capture the output collector and the reporter into member fields so that they may be used in the close() method.
This has a downside in that they can be captured only in the map() method, requiring extra code in that inner loop.
Listing 5-3 shows an example that declares a number of local variables, which are initialized by the configure() method for use by the map() and close() methods.
Reporter reporter = null; /** Take this early, it is handy to have.
Initializing the Mapper with Spring Many installations use the Spring Framework to manage the services employed by their applications.
One of the more interesting issues is how to use Spring in environments where Spring does not have full control over the creation of class instances.
Spring likes to be in full control of the application and manage the creation of all of the Spring bean objects.
In the Hadoop case, the Hadoop framework is in charge and will create the object instances.
The examples in this section demonstrate how to use Spring to initialize member variables in the mapper class.
Listing 5-4 shows the bean file used in the Spring example.
Creating the Spring application Context To create an application context, you need to provide Spring with a resource set from which to load bean definitions.
Being very JobConf-oriented, I prefer to pass the names of these resources, and possibly the resources themselves, to my tasks via the JobConf and DistributedCache objects.
The example in Listing 5-5 extracts the set of resource names from the JobConf object, and if not found, will supply a default set of resource names.
This follows the Hadoop style of using comma-separated elements to store multiple elements in the configuration.
The set of resources names are unpacked and passed to the Spring Framework.
Each of these resources must be in the classpath, which includes the task working directory.
The mapper class will need to determine the name of the file passed in via the command line.
For the example, set up the Spring initialization parameters on the application command line as follows:
If it comes before the jar argument, it will become a Java system property.
Using Spring to autowire the Mapper Class Once the Spring application context has been created, the task may instantiate beans.
The first is that the bean definition to be used must specify lazy initialization, to prevent Spring from creating an instance of the bean when the application context is created.
The second is to know the bean name/ID of the mapper class.
The example in Listing 5-6 makes some assumptions about how application contexts and task beans are named, and can be easily modified for your application.
In Listing 5-6, a base name is constructed for looking up information in the configuration via the following:
If a value is found, it is treated as a comma-separated list of bean resource files to load.
A default bean file is used if no value is found.
This is the bean that will be used to configure the task.
An autowire-capable bean factory is extracted from the application context via the following:
The following line actually causes Spring to initialize the task:
The code must ensure that Spring did not return a delegator object when it was initializing the task from the bean definition:
Note This example does not handle the case where Spring returns a delegator object for the task.
To handle this case, the map() method would need to be redirected through the delegated object.
Partitioners Dissected A core part of the MapReduce concept requires that map outputs be split into multiple streams called partitions, and that each of these partitions is fed to a single reduce task.
The reduce contract specifies that each reduce task will be given as input the fully sorted set of keys.
The entire partition is the input of the reduce task.
For the framework to satisfy this contract, a number of things have to happen first.
The outputs of each map task are partitioned and sorted.
The partitioner is run in the context of the map task.
The Hadoop framework provides several partitioning classes and a mechanism to specify a class to use for partitioning.
The actual class to be used must implement the org.apache.
The piece that provides a partition number is the getPartition() method:
Note that both the key and the value are available in making the partition choice.
The key and value will be streamed into the partition number that this function returns.
Each key/value pair output by the map() method has the partition number determined and is then written to that map local partition.
For each reduce task, the framework will collect all the reduce task’s partition pieces from each of the map tasks and merge-sort those pieces.
The results of the merge-sort are then fed to the reduce() method.
The output of a reduce task will be written to the part-XXXXX file, where the XXXXX corresponds to the partition number.
Listing 5-8 shows the actual code from the default partitioner used by Hadoop.
The partition number is simply the hash value of the key modulus the number of partitions.
The resulting number has modulus the number of reduce tasks applied, % numReduceTasks, and the result returned.
This produces a positive number between 0 and one less than the number of partitions.
With this information, the partitioner is able to determine which range a key/value pair belongs in and route it to the relevant partition.
Jim Gray introduced a contest called the TeraByteSort, which was a benchmark to sort one terabyte of data and write the results to disk.
This class will sample the input to build an approximate range table.
This sampling strategy will take no more than the specified number of samples total from the input.
The user may specify a maximum number of input splits to look in as well.
The actual number of records read from each input split varies based on the number of splits and the number of records in the input split.
The Hadoop framework controls how the input is split based on the number of input files, the input format, the input file size, and the minimum split size and the HDFS block size.
Let’s look at a few examples of running InputSampler from the command line.
The input splits are examined in the order in which they are returned by InputFormat.
The input splits are sampled in a random order, and the records from each split read are sequentially.
If the 1,000 samples are not selected after processing the recommended number of splits, more splits will be sampled.
The index is set up for 15 reduce tasks, and the input comes from csvin.
The frequency parameter defines how many records will be sampled.
The input comes from csvin, and the index is written to csvout.
If the binary representation of the key is the correct sort order, a binary trie (an ordered tree structure; see http://en.wikipedia.org/wiki/Trie) will be constructed and used for searching; otherwise, a binary search based on the output key comparator will be used.
Here’s an example of how to put all this together:
In this example, csvin is the input file, and csvout is the index file.
The csvout file was set up for 15 reduce tasks, and requires the comparator rather than binary comparison.
The primary concept is that the keys may be split into pieces based on a piece separator string.
It may be set to another string, str, as follows:
Referencing individual characters within the pieces is also one-based rather than zerobased, with the index 0 being the index of the last character of the key part.
The key pieces to compare are specified by setting the key field partition option, via the following:
The str format is very similar to the key field-based comparator.
The key specification supported is of the form -k pos1[,pos2], where, pos is of.
There is also the facility to select a start and stop point within an individual key.
Note If your key specification may touch the last key piece, it is important to terminate with the last character of the key.
The Reducer Dissected The reducer has a very similar shape to the mapper.
All of the mapper good practices of saving the JobConf object and making instances of the output key and output value objects apply to the reducer as well.
Unlike the map() method, which is given a single key/value pair on each invocation, each reduce() method invocation is given a key and all of the values that share that key.
The default is to define a group as all values that share a key.
Common uses for reduce tasks are to suppress duplicates in datasets or to segregate ranges and order output of large datasets.
The identity reducer simply outputs each value in the iterator.
The configure() and close() methods have the same requirements and suggested usage as the corresponding mapper methods.
It is generally recommended that you do not make a copy of all of the value objects, as there may be very many of these objects.
Note In one of my early applications, I assumed that there would never be more than a small number of values per key.
It turned out that there were often more than 150,000 values per key!
It is possible to simulate a secondary sort/grouping of the values by setting the output value grouping.
It begins by establishing several member variables that will be used in the reduce() method to save object generation:
The working body of the reduce() method is within a try block that catches Throwables, and the input count, output count, and failure count are reported to the framework:
In the body of the example in Listing 5-10, each value that is passed in is examined and aggregated:
Finally, the output key and value are constructed with the aggregated data:
The example that runs this reducer also uses an output grouping comparator that groups the records in sets of ten.
The following command will run the SimpleReduce job (your output will vary slightly):
The actual output is the key, the average value, the number of values averaged, the minimum value, the maximum value, and the difference between the minimum and the maximum.
Now you can print the job output (key, average, count, min, max, difference), as follows:
In this example, the grouping operator groups by multiples of ten in the key space.
There is no guarantee that an entire group of keys will not be split into multiple partitions.
In the previous version, the map input keys are Text and the reduce input keys are LongWritable.
As the keys are long values, they are binary comparable.
The code in Listing 5-13 runs the InputSampler to compute and store the index in indexFile.
The assumption here is that the JobConf object conf is already correctly set up with the InputPaths and InputReader.
The sampling strategy is to randomly sample the records with a 0.1% chance that any record is chosen.
Your results will differ, as random data generation and selection are occurring.
The purpose of a combiner is to reduce the volume of data that must be passed to the reducer from a map task by summarizing output records that share the same key.
A combiner must implement the Reducer interface, and the reduce() method of the combiner will be called with each output key and all of the output values that share that key.
The output of the combiner is what will be sent over the network to the actual reduce task for the job or written to the final output directory, if there is no reduce task configured.
The combiner class reduce() method must have the same input and output key/value types as the reducer class.
When all of the map task input has been processed, these partitions are sorted, and each one is passed as input to the combiner.
The combiner’s reduce() method will be called once for each unique key in the partition, and the values will be the set of values that share that key.
The output of the combiner will replace that set of original map outputs, ideally with fewer records or smaller records.
This is suitable for jobs that are producing summary information from a large dataset.
Caution The combiner must not change the key values, as the map outputs are not re-sorted after the combiner runs.
The reduce phase requires the map outputs to be sorted by key.
Tip It not always simple to build a correct combiner.
If a job output has problems, try running the job without the combiner to see if the problem persists.
If your actual reduce() method is nontrivial, do not also use it as a combiner; instead, write a separate object to combine the map outputs.
This MapReduce job reads a set of text input files and counts the frequency of occurrence of each word in the input files.
There will be one output record for each word in the file.
The combiner will aggregate these into a set that contains one output record per unique word in the input, and the value is the number of times the word appeared in the input.
Unless the writer has such a large vocabulary that no word is used more than once, the combiner will greatly reduce the number of records to be processed by the reduce phase.
The map() method tokenizes the line and emits a record for each word of the input record, a Text and the value 1, an IntWritable.
The reduce() method simply sums the values and outputs the word as Text and the sum of values, an IntWritable.
By using the reduce() method as a combiner, there is a large reduction in the size of each map task output.
When the map task has completed and the partitions are sorted, the combiner may run over the partitions and aggregate values, reducing the total number of key/value pairs that must go over the network to the reduce task.
For example, suppose the map partition dataset originally contained the following:
After the combiner has completed, the map partition dataset would contain these keys and values:
It is fairly simply to shoot yourself in the foot with a combiner.
The combiner must not cause the loss of any information that is needed by the actual reducer.
The classic example of this is a reducer that computes the average of the values for each key.
If that reducer is also used as a combiner, the information on the number of records involved computing the average will be lost, and the reduce tasks will see only the average values for each key; the final result will be the average of the averages, instead of the actual average.
Combiners also must be idempotent, as they may be run an arbitrary number of times by the Hadoop framework over a given map task’s output.
File Types for MapReduce Jobs The Hadoop framework supports text files, binary (sequence) files, and map files, which are actually a pair of sequence files.
Let’s take a closer look at each of these file types.
Text Files The Hadoop framework supports a number of textual input files and output files.
TextInputFormat: This class reads each line of the input split and returns a record composed of the line number as a LongWritable key, and the line itself as a Text value.
The workhorse class that actually produces the key/value pairs is org.apache.
If there is no separator found, the value will be an empty string.
NLineInputFormat: This format is ideal for using the input data as control information.
It guarantees that each input split will be N lines long, with one split being the remaining lines.
There is substantial time involved in setting up and starting a task, as well as collecting the results.
If the input split is small, a substantial portion of the job runtime may be in the setup and teardown of tasks.
It basically calls the toString method on each key and value, producing a single-line key SEPARATOR value ASCII newline for each output record.
If the value is null, no SEPARATOR and no value will be emitted.
The end-of-record character is hard-coded as an ASCII newline character.
Caution It is critically important to minimize the number of HDFS files that are opened.
HDFS, through at least Hadoop 0.19.0, is designed for small numbers of very large files.
Opening many small files will bring your cluster to its knees, and may result in catastrophic failure of your job, as well as your HDFS.
Sequence Files Sequence files are a binary format for storing sets of serialized key/value pairs.
Sequence files support compression, encapsulate the key and value types, and provide validity checksums.
They are an ideal format to use for data that is expensive or complex to parse.
If the input file is a map file (described in the next section), the data file is read.
It calls the toString method on the key and value classes and returns the key/value pair as Text,Text.
The key and value types must be BytesWritable, and these raw bytes are written as the records.
Map Files Map files are a pair of sorted sequence files.
If a map file named mymap is created, there will be a directory mymap in HDFS, and two files in mymap: index and data.
The data sequence file contains the key/value pairs as records, where the records are sorted in key order.
The index sequence file is key/location information, where location is the location in data where the first record containing a key is located.
Map files provide a way to find a particular key, or region of a sorted file, without having to read the entire file.
The HBase project (http://hadoop.apache.org/hbase) provides a persistent distributed hash table stored in HDFS, using map files as the underlying storage.
When a map file is specified as a job input, the data file is used as the actual input.
The path specified is the path to the directory containing the index and data files.
Tip For best performance, it is strongly suggested that all key lookups be performed in the sort order of the underlying map file.
The default is one index entry for every 128 records.
Map files provide the following methods for looking up key/value pairs.
Compression The Hadoop framework supports several types of compression and several compression formats.
The framework supports the gzip, zip, sometimes LZO, and bzip2 compression codecs.
The framework will transparently compress and uncompress most input and output files.
Input files are uncompressed when the input file name has a suffix that maps to one of the known codecs, as shown in Table 5-3
It is incompatible with the Apache license and has been removed from some distributions.
I sincerely wish that this will be resolved and that native LZO becomes a standard part of the Hadoop distribution.
Codec Specification The Hadoop framework supports a number of codecs, with native implementations for a smaller number.
LzoCodec may not be available in some releases due to licensing issues.
The selection of a compression codec is a choice between speed and compression rate.
Sequence File Compression Sequence files are binary record-oriented files, where each record has a serialized key and serialized value.
The Hadoop framework supports compressing and decompressing sequence files transparently.
The framework will transparently compress at the record level or the block level.
In general, block-level compression is recommended, because it provides greater data reduction (at the expense of individual key access)
The compression overhead is less, and the compression ratio is much greater.
For sequence files that are being used as input to a map or reduce phase, block-level compression is ideal.
Sequence files that were written using transparent compression may be divided into multiple input splits by the framework.
Many sites will set the default to BLOCK in their hadoop-site.xml file, as follows:
Note I have spent some time running the same job with different compression codecs and RECORD or BLOCK set for compression, to determine which combination gave the overall performance for the job.
Map output block-level compression may be specified by the job or in the site configuration.
If compressed, map output, destined for a reduce task, is always BLOCK compressed.
Listing 5-19 demonstrates configuring a cluster to always use compression for final output files, and if the final output file is a sequence file, to use BLOCK compression.
The archives may be unpacked only onto the native file system, not into HDFS.
Summary The Hadoop Core framework provides a rich set of tools to support a variety of use cases.
As with most powerful tools, using them effectively requires training and experience.
This chapter has provided a solid foundation for configuring jobs to run successfully and building classes that will actual perform the work for the job.
Once you have developed your MapReduce job, you need to be able to run it at scale on your cluster.
This chapter will cover how to recognize that your job is having a problem and how to tune the scaling parameters so that your job performs optimally.
The framework provides several parameters that let you tune how your job will run on the cluster.
Most of these take effect at the job level, but a few work at the cluster level.
With large clusters of machines, it becomes important to have a simple monitoring framework that provides a visual indication of how the cluster is and has been performing.
Having alerts delivered when a problem is developing or occurs is also essential.
Finally, you’ll get some tips on what to do when your job isn’t performing as it should.
This chapter is focused on tuning jobs running on the cluster, rather than debugging the jobs themselves.
Tunable Items for Cluster and Jobs Hadoop Core is designed for running jobs that have large input data sets and medium to large outputs, running on large sets of dissimilar machines.
The framework has been heavily optimized for this use case.
Hadoop Core is optimized for clusters of heterogeneous machines that are not highly reliable.
The HDFS file system is optimized for small numbers of very large files that are accessed sequentially.
The data stored in HDFS is generally not considered valuable or irreplaceable.
The service level agreement (SLA) for jobs is long and can sustain recovery from machine failure.
Users commonly get into trouble when their jobs input large numbers of small files, output large numbers of small files, or require random access to files.
Another problem is a need for rapid access to data or for rapid turnover of jobs.
Hadoop Core does not provide high availability for HDFS or for job submission, and special care must be taken to ensure that required HDFS data can be recovered in the event of a critical failure of the NameNode.
Behind the Scenes: What the Framework Does Each job has a number of steps in its execution: the setup, the map, the shuffle/sort, and the reduce.
The framework sets up, manages, and tears down each step.
Note The following discussion assumes that no other job is running on the cluster and that on submission, the job is immediately started.
On Job Submission The framework will first store any resources that must be distributed in HDFS.
These are the resources provided via the -files, -archives, and -libjars command-line arguments, as well as the JAR file indicated as the job JAR file.
If there are a large number of resources, this may take some wall clock time.
The XML version of the JobConf data is also stored in HDFS.
The framework will then examine the input data set, using the InputFormat class to determine which input files must be passed whole to a task and which input files may be split across multiple tasks.
The framework will use the parameters listed in Table 6-1 to determine how many map tasks must be executed.
Input formats may override this; for instance, NLineInputFormat forces the splits to be made by line count.
Tip Through at least Hadoop 0.19.1, compressed files may not be split.
The ideal split size is one file system block size, as this allows the framework to attempt to provide data locally for the task that processes the split.
The end result of this process is a set of input splits that are each tagged with information about which machines have local copies of the split data.
The splits are sorted in size order so that the largest splits are executed first.
The split information and the job configuration information are passed to the JobTracker for execution via a job information file that is written to HDFS.
Map task Submission and execution The JobTracker has a set of map task execution slots, N per machine.
Each input split is sent to a task execution slot for execution.
Sending tasks to a slot that is hosted on the machine that has a local copy of the input split data minimizes network I/O.
If there are spare execution slots, and map speculative execution is enabled, multiple instances of a map task may be scheduled.
In this case, the results of the first map task to complete will be used, the other instances killed, and the output, including the counter values, removed.
When map speculative execution is not enabled, only one instance of a map task will be run at a time.
The TaskTracker on the machine will receive the task information, and if necessary, unpack all of the DistributedCache data into the task local directory and localize the paths to that data in the JobConf object that is being constructed for the task.
With speculative execution for map tasks disabled, the only time more than one instance of a map task will occur in the job will be if the task is retried after failing.
Caution The framework is able to back out only counter values and output files written to the task output directory.
Any other side effects of killed speculative execution tasks or failed tasks must be handled by the application.
A child JVM is allocated to run the mapper class, and the map task is started.
The output data of the map task is partitioned and sorted by the output partitioner class and the output comparator class, and aggregated by the combiner class, if one is present.
The result of this will be N sequence files on disk: one for each reduce task, or one file if there is no reduce task.
Each time the map method is called, an output record is emitted, or the reporter object is interacted with, a heartbeat timer is reset.
If this timeout expires, the map task is considered hung and terminated.
If a terminated task has not failed more than the allowed number of times, it is rescheduled to a different task execution slot.
The script is invoked with the additional arguments of the paths to the stdout, stderr, and syslog output files for the task.
See this book’s appendix, which covers the JobConf object, for details on how to configure a debugging script for failing tasks.
When a task finishes, the output commit class is launched on the task output directory, to decide which files are to be discarded and which files are to be committed for the next step.
If less than the required number of tasks succeed, the job is failed and the intermediate output is deleted.
The TaskTracker will inform the JobTracker of the task’s success and output locations.
Merge-Sorting The JobTracker will queue the number of reduce tasks as specified by the JobConf.
The JobTracker will queue these reduce tasks for execution among the available reduce slots.
The TaskTracker that receives a reduce task will set up the local task execution environment if needed, and then fetch each of the map outputs that are destined for this reduce task.
The number of pieces that are fetched at one time is configurable.
A number of parameters control how the merge-sorting is done, as shown in Table 6-2
This parameter often causes jobs to run out of memory on small memory machines.
The reduce output is written to the local file system.
On successful completion, the output commit class is called to select which output files are staged to the output area in HDFS.
If more than the allowed number of reduce tasks fail, the job is failed.
Once the reduce tasks have finished, the job is done.
Writing to hDFS There are two cases for an HDFS write: the write originates on a machine that hosts a DataNode of the HDFS cluster for which the write is destined, or the write originates on a machine that does not host a DataNode of the cluster.
In both cases, the framework buffers a file system block-size worth of data in memory, and when the file is closed or the block fills, an HDFS write is issued.
The write process requests a set of DataNodes that will be used to store the block.
If the local host is a DataNode in the file system, the local host will be the first DataNode in the returned set.
The set will contain as many DataNodes as the replication factor requires, up to the number of DataNodes in the cluster.
The replication factor may be set via the configuration key dfs.replication, which defaults to a factor of three, and should never be less than three.
The replication for a particular file may be set by the following:
The block is written to the first DataNode in the list, the local host if possible, with the list of DataNodes that are to be used.
On receipt of a block, each DataNode is responsible for initiating the transfer of the block to the next DataNode in the list.
This allows writes to HDFS on a machine that hosts a DataNode to be very fast for the application, as they do not require bulk network traffic.
Cluster-Level Tunable Parameters The cluster-level tunable parameters require a cluster restart to take effect.
Some of them may require a restart of the HDFS portion of the cluster; others may require a restart of the MapReduce portion of the cluster.
These parameters take effect only when the relevant server starts.
Server-Level parameters The server-level parameters, shown in Table 6-3, affect basic behavior of the servers.
In general, these affect the number of worker threads, which may improve general responsiveness of the servers with an increase in CPU and memory use.
Java application writers are notorious for not closing I/O channels, resulting in massive consumption of file descriptors by the map and reduce tasks..
The larger this value, the fewer individual blocks will be stored on the DataNodes, and the larger the input splits will be.
The DataNodes through at least Hadoop 0.19.0 have a limit to the number of blocks that can be stored.
After this size, the DataNode will start to drop in and out of the cluster.
If enough DataNodes are having this problem, the HDFS performance will tend toward full stop.
When computing the number of tasks for a job, a task is created per input split, and input splits are created one per block of each input file by default.
There is a maximum rate at which the JobTracker can start tasks, at least through Hadoop 0.19.0
The more tasks to execute, the longer it will take the JobTracker to schedule them, and the longer it will take the TaskTrackers to set up and tear down the tasks.
The other reason for increasing the block size is that on modern machines, an I/O-bound task will read 64MB of data in a small number of seconds, resulting in the ratio of task overhead to task runtime being very large.
A downside to increasing this value is that it sets the minimum amount of I/O that must be done to access a single record.
If your access patterns are not linearly reading large chunks of data from the file, having a large block size will greatly increase the disk and network loading required to service your I/O.
The DataNode and NameNode parameters are presented in Table 6-4
If specified, only the hosts in this file are permitted to connect to the NameNode.
If this value is 0, no access times are maintained.
Setting this to 0 may increase performance on busy clusters where the bottleneck is the NameNode edit log write speed.
Larger values allow more DataNodes to fail before blocks are unavailable but increase the amount of network I/O required to store data and the disk space requirements.
Large values also increase the likelihood that a map task will have a local replica of the input split.
This may  67108864 be too small or too large for your cluster, depending on your job data access patterns.
Increasing this may increase DataNode throughput, particularly if the DataNode uses multiple separate physical devices for block storage.
The  3600000 block report does a scan of every block that is stored on the DataNode and reports this information to the NameNode.
This is commonly greatly increased in busy and large clusters.
Jobtracker and tasktracker tunable parameters The JobTracker is the server that handles the management of the queued and executing jobs.
The TaskTrackers are the servers that actually execute the individual map and reduce tasks.
A  local value of local means to run the job in the current JVM with no more than 1 reduce.
If the configuration specifies local, no JobTracker server will be started.
If specified, only the hosts in this file are permitted.
This prevents tasks from failing due to lack of temp space.
The 0 value should be changed to something reasonable for your jobs.
This is used  Unlimited for processes started by the org.apache.
The framework uses this to launch external subprocesses, such as the pipes jobs and the external programs of streaming jobs.
This should either be 1 (if there is only one CPU) or roughly one less than the number of CPUs on the machine.
This parameter needs to be tuned for a particular job mix.
This value is really a function of the CPU and I/O bandwidth available to the machine.
It needs to be tuned for the machines and job mix.
If not default, this value is the name of a network interface, such as eth0
If your system has many reduce execution slots, the default may be too small.
Per-Job Tunable Parameters The framework provides rich control over the way individual jobs are executed on the cluster.
Table 6-7 shows the tunable parameters for the file system.
In general, if writes are being retried, there is a problem with the HDFS or machine configuration.
The task-tunable parameters directly control the behavior of tasks in the cluster.
Only those parameters that directly control core functions are listed in Table 6-8
Many of the parameters are detailed in this book’s appendix, which discusses the JobConf object.
This will consume more cluster resources and may offer faster job throughput.
This must be false if your map tasks have side effects that the framework cannot undo or have real costs.
This will consume more cluster resources and may offer faster job throughput.
This must be false if your reduce tasks have side effects the framework cannot undo or have real costs.
This  10 needs to be tuned on a per-job basis.
There is no automatic mechanism in the framework to clean these directories if this is set to false.
This is usually a significant win for jobs with large output.
See this  0-2 book’s appendix for how this may be set.
See this  0-2 book’s appendix for how this may be set.
The framework will attempt to narrow down the region to skip to this size.
Monitoring Hadoop Core Services To be able to detect incipient failures, or otherwise recognize that a problem is developing or has occurred, some mechanism must be available to monitor current status, and if possible provide historical status.
The Hadoop framework provides several APIs for allowing external agents to provide monitoring services to the Hadoop Core services.
This allows for the use of JMX-aware applications to collect information about the state of the servers.
The default configuration provides for only local access to the managed beans (MBeans)
The string is a JVM argument and passed to the JVM at start time on the command line.
Nagios: A Monitoring and Alert Generation Framework Nagios  (http://www.nagios.org) provides a flexible customizable framework for collecting data about the state of a complex system and triggering various levels of alerts based on the collected data.
A service of this type is essential for your cluster administration and operations team.
This example assumes that you understand how to construct the JMX password file and access control file.
To enable JMX monitoring on Hadoop, add the following lines to hadoop-env.sh:
Ganglia by itself is a highly scalable cluster monitoring tool, and provides visual information on the state of individual machines in a cluster or summary information for a cluster or sets of clusters.
Ganglia provides the ability to view different time windows into the past, normally one hour, one day, one week, one month, and so on.
Caution Due to some limitations in the Ganglia support in Hadoop through at least Hadoop 0.19.1, the configuration requirements are not as simple as Ganglia configuration normally is.
Ganglia is composed of two servers: the gmetad server, which provides historical data and collects current data, and the gmond server, which collects and serves current statistics.
The Ganglia web interface is generally installed on the host(s) running the gmetad servers, and in coordination with the host’s httpd provides a graphical view of the cluster information.
In general, each node will run gmond, but only one or a small number of nodes will also run gmetad.
Each cluster must also be allocated a unique cluster name.
The cluster name is referred to as CLUSTER in this section.
The UDP port for reporting is referred to as PORT, and for simplicity, the multicast port will be identical to PORT.
Also the multicast port unique to CLUSTER, which all gmond servers in CLUSTER will listen and transmit on.
The default of 239.2.11.71 is acceptable as long as each CLUSTER uses a unique PORT.
Note MULTICAST:PORT must be unique per CLUSTER, but generally MULTICAST is left at the default value, so PORT becomes the unique value per cluster.
The gmond on HOSTNAME will need to be configured to listen on the non-multicast UDP port of PORT.
Many enterprise-grade switches will need to have multicast enabled for each CLUSTER’s MULTICAST:PORT.
All nodes in the cluster will have the gmond server configured with the cluster name parameter set with the cluster’s unique name.
One node in the cluster, traditionally the NameNode or a JobTracker node, is configured to also accept non-multicast reporting on a port, commonly the same port as the multicast reception port.
This host will be considered the Ganglia cluster master, and its hostname is the value for HOSTNAME.
This host is also the host used in the /etc/gmetad.conf file.
The HOSTNAME and PORT must be substituted for the actual values.
This file must then be distributed to all of the Hadoop conf directories and all Hadoop servers restarted.
All of the Hadoop servers will now deliver metric data to HOSTNAME:PORT via UDP, once every 10 seconds.
The gmetad server that will collect metric information for the cluster will need to be instructed to collect metric information about CLUSTER from the master node via a TCP connection to HOSTNAME:PORT.
The following is the configuration line in the gmetad.conf file for CLUSTER:
The Ganglia web interface will provide a graphical view of the clusters, as shown in Figure 6-1
The Ganglia web view of a running set of clusters.
When tuning jobs, Ganglia provides a wonderful interface to determine when your job is fully utilizing a cluster resource.
Determining which resource is fully utilized, tuning the appropriate configuration parameters for that resource, and then rerunning the job will allow you to optimize your job’s runtime on your cluster.
Chukwa: A Monitoring Service Chukwa’s goal is to provide extract, transform, and load (ETL) services for cluster logging data, thereby providing end users with a simple and efficient way to find the logging events that are actually important.
Chukwa uses HDFS to collect data from various data providers, and MapReduce to analyze the collected data.
The instance in Hadoop 0.19.0 appears to be currently optimized for the collection of data from log files, and then run a scheduled MapReduce job over the collected data.
FailMon: A Hardware Diagnostic Tool The FailMon framework attempts to identify failures on large clusters by analyzing data collected from the Hadoop logs, the system logs, and other sources.
This is a very early technology and is expected to evolve rapidly.
The FailMon package consists primarily of data collection tools with MapReduce jobs to perform analysis of the collected data.
Tuning to Improve Job Performance The general goal for tuning is for your jobs to finish as rapidly as possible using no more resources than necessary.
This section covers best practices for achieving optimum performance of jobs.
Speeding Up the Job and Task Start If the job requires many resources to be copied into HDFS for distribution via the distributed cache, or has large datasets that need to be written to HDFS prior to job start, substantial wall clock time can be spent copying in the files.
For constant resources, it is simplest and quickest to make them available on all of the cluster machines and adjust the TaskTracker classpaths to reflect these resource locations.
The disadvantage of installing the resources on all of the machines is that it increases administrative complexity, as well as the possibility that the required resources are unavailable or an incorrect version.
The advantage of this approach is that it reduces the amount of work the framework must do when setting up each task and may decrease the overall job runtime.
Table 6-10 provides a checklist of items to look for that affect write performance and what to do when the situations occur.
Source machine CPU  The CPU is maxed out, the com- Change the compression or change utilization pression level is too high, or the  the number of threads.
Source machine network  Saturation of the outbound net- Increase the number of transfer work connection with traffic for  threads or provide a higher-speed HDFS network connection.
Per DataNode network input  If it is not saturated, more writes  Increase the number of simultacould be delivered to this DataNode  neous threads writing or reduce the.
DataNode I/O wait I/O contention on a DataNode  Add more independent locations to dfs.data.dir or add more DataNodes.
If you have a large number of files that need to be stored in HDFS prior to the task start, such as might occur if your job needs to populate the job input directory, there are several things you may try, in varying combinations:
You may change the dfs.block.size parameter to issue larger or smaller writes.
You may decrease the dfs.replication parameter to reduce the overall HDFS write load, or increase it to increase the chance of local access by later MapReduce jobs.
Figure 6-2 illustrates how HDFS operations that your application issues are actually handled by the framework.
Implicit in Figure 6-2 is that the replication count is three.
From a monitoring perspective, you will want to monitor the network utilization on the upload machine and to a lesser extent on the DataNodes.
If you are using compression, you will want to monitor CPU utilization on the machines doing the compression.
You may also wish to monitor the disk-write rate on the DataNodes, to verify that you are getting a good write rate.
Since the incoming data rate is capped by the network rate, generally this is not a significant factor.
If you see pauses in the network traffic or disk I/O, it implies that a Hadoop server may be unresponsive, and the client is timing out and will retry.
Optimizing a Job’s Map Phase The map phase involves dispatching a map task to a TaskTracker.
Once the TaskTracker has a task to execute, it will prepare the task execution environment by building or refreshing the DistributedCache data.
If you don’t have an existing child JVM that has been used for this job’s task and is within its reuse limit, start a new child JVM.
The TaskTracker will then trigger the start of the map task in the child JVM.
The child JVM will start reading key/value pairs from its input, executing the map method for each pair.
The output key/value pairs will be partitioned as needed and collected in the proper output format.
If there is a reduce phase, the output format will be on the local disk in a sequence file.
If there is not a reduce phase, the output will be in the job-specified output format and stored in HDFS.
Figure 6-3 shows a diagram of the job setup and map task execution.
The left side follows the actions of the JobTracker from job submission through executing the map tasks on the available TaskTrackers.
The right side follows the loop that a TaskTracker executes for map tasks.
The Tasktracker$Child is the class providing a main() method for the actual map task, which will be executed in a JVM launched and managed by the TaskTracker.
Tasktracker$Child Set up to read input split from HDFS and write output to local file system.
The following are some items you can tune for the map phase:
Map task run time: Each map task has some setup and teardown overhead.
If the runtime of the map task is short, this overhead becomes the major time component.
If the runtimes of tasks are too long, a single task may hold the cluster for a long period, or retrying a failed task becomes expensive.
In general, less than a minute is usually too short, What is too long for a map task is job-specific.
The primary tuning point for this is the dfs.block.size parameter.
Increasing this parameter usually increases the split size and the task run time.
TaskTracker node CPU utilization: If the map tasks are computationally intensive, a significant goal is to use all of the available CPU resources for that computation.
There are two methods for controlling CPU utilization for map tasks:
Data location: If the map tasks are not receiving their input split from a local DataNode, the I/O performance will be limited to the network speed.
This value is visible in the job counters of running and completed jobs, under the section titled “Data Local map tasks,” the Total column gives the number of map tasks that ran with the input split served from a local DataNode.
Other than increasing the replication factor and trying to ensure that the input split size is the file system block size, there is little tuning to be done.
Child garbage collection: If there is significant object churn in the Mapper.map method, and there is insufficient heap space allocated, the JVM hosting the task may spend a significant amount of wall clock time doing garbage collection.
This is readily visible only via the Ganglia reporting framework or through the JMX MBean interface.
The Ganglia reporting variable is gcTimeMillis() and is visible in the main reporting page for Ganglia, as shown in Figure 6-4
Figure 6-4 shows an example of a Ganglia report for a two-host cluster, where one host is having problems.
This would imply that the child JVM has been configured with insufficient memory.
At the current time, it is not possible to differentiate the garbage collection timing for the different server processes.
Ganglia report showing gcTime for a two-host cluster, where one host is in trouble.
In this 10-minute window, the same task was run twice.
Note how much less time was taken in garbage collection on the right side of the graph for cloud9 versus the left half of the graph.
Here are the command-line options to enable multithreaded map running with ten threads:
Tuning the Reduce Task Setup The reduce task requires the same type of setup as the map task does with respect to the DistributedCache and the child JVM working environment.
The two key differences relate to the input and the output.
The reduce task input must be fetched from each of the TaskTrackers on which a map task has run, and these individual datasets need to be sorted.
The reduce output is written to HDFS, unlike with the map task, which has output to the local file system.
As you can see from Figure 6-5, there are several steps for a reduce task, each of which has different constraints, as follows:
There are several ways to reduce the size of the map output files, which can greatly speed up this phase.
Some care needs to be used, as some of the compression options may slow down the shuffle and sort phase.
The simplest thing is to specify a combiner class, which will act as a mini-reduce phase in each map task (as described in Chapter 5)
This works very well for aggregation jobs, and not so well for jobs that need the full value space in the reduce task on which to operate.
Note There are a number of parameters that control the shuffle and merge.
Choosing the number of reduce tasks to run per machine and per cluster is the final level of tuning.
With reduce tasks, I/O, rather than CPU usage, is usually the bottleneck.
If the DataNodes are coresident with the TaskTrackers, the reduce tasks will always have a local DataNode for the output.
This will allow the initial writes to go at local speed, but the file closes will block until all the replicas are complete.
It is not uncommon for jobs to open many files in the reduce phase, which generally causes a huge slowdown, if not failure, in the HDFS cluster, so the job will take a significant amount of time to finish.
The following are some tuning points for the reduce phase:
Shuffle/sort time: The shuffle and sort cannot complete until all of the map output data is available.
If this is an issue, you can try the following:
Network saturation: The pull of the map outputs should just saturate your network.
If the network is saturated, enable compression, reduce the number of map tasks, improve the combiner class, or restructure the job to reduce the data passed to the reduce phase.
Note I once had a job where part of the value associated with each key was a large block of XML data that was unused by the reduce phase.
Modifying the map to drop the XML data provided a tenfold improvement.
Actual reduce time: You may find that the time to actually reduce the data, after the shuffle and sort are done, is too long.
If many small files must be created, write them as a zip archive.
The Ganglia gmetric value FilesCreated will give you an idea of the rate of HDFS file creation.
Write time: The write time may be too long if the volume of data or the number of files are large.
Pack multiple files into zip files or other archive formats.
Note I had one job that needed to create many tens of thousands of small files.
Writing the files as a zip archive in HDFS resulted in a hundredfold speed increase.
Overall reduce phase time: If the reduce phase is very long, you may want to tailor the number of reduce tasks per job and per machine.
The job may specify the number of reduce tasks to run, but at least through Hadoop 0.19.1, the number of reduce tasks per TaskTracker is fixed at start time.
If your cluster will run a specific set of jobs, with experimentation, you may find a reasonable number for the cluster-level parameter, and given that, identify a specific value for the number of reduce tasks for each job.
Addressing Job-Level Issues One of the more interesting things to see is a cluster going almost idle while a job is running.
This usually happens because a small number of tasks have not finished.
This can happen with the map tasks or the reduce tasks.
With multithreaded map tasks, the key or keys passed to one thread can sometimes take much longer to finish, and the task needs to wait for one or a small number of threads to complete, leaving the machine mostly idle.
In one particular job, there was a large variance in the time it took to process a key: some keys took three hours, and others three seconds.
If the long-running keys came late in an input split, the task would end up running one thread and idle six of the processors on the machine.
The only solution for this was to reorder the input keys so that the long-running keys came first in the splits, or to abandon the longrunning keys after a set elapsed run time, and reprocess all of the long-running keys in an additional job later.
Dealing with the Job tail The Hadoop standard is for very large jobs, spread over many machines, such that the time of one or two tasks is small compared to the run time of the job.
These organizations must tune their jobs so that the clusters are well utilized.
Tuning the number of reduce tasks so they fall evenly on your reduce slots may also help.
Having one reduce task start after all the rest of the reduce tasks have finished can drastically increase the job runtime.
Summary This chapter detailed how jobs are run by the Hadoop Framework and how MapReduce application writers and cluster administrators can tune both jobs and clusters for optimal performance.
The NameNode, JobTracker, DataNodes, and TaskTrackers have a number of start time parameters that directly affect how jobs are executed by the cluster and the overall run time of the jobs.
The execution of a job is performed in several steps: setup, map, shuffle/sort, and reduce.
It’s possible to do some tuning to improve performance in each step.
This chapter discussed several tools for monitoring clusters and jobs.
Ganglia is the tool I prefer for tuning and general dashboard-level awareness, and Nagios is the one I use for operational support.
Using these tools enables rapid recognition of problems with jobs and clusters and provides insight into what parameters may need to be tuned, before the CEO calls you into the front office to explain why the mission-critical jobs haven’t been running successfully.
Ganglia also provides the informative, pretty graphs that higher-level management like so much.
Two questions echo endlessly in the dark hours of small, enclosed areas lit by the dim glow of display screens: “Is it working?” and “How did that happen?” This chapter is about answering those questions.
The agile programming model suggests starting with unit tests and building the code afterwards.
I generally try to follow this model, although under pressure, I have shifted to just writing the code, usually to my later regret.
Testing the lower-level APIs in your applications is a known skill set.
This chapter will cover unit tests that actually run MapReduce jobs at a small scale.
There are several ways to determine what is happening in a running program.
The first level is through examining the log messages or other job output data.
This requires detailed understanding of the output and may not provide sufficient information to isolate the problem.
An alternative is to put custom code into the application to trigger different behavior or logging around the area of code that is in question.
The most comprehensive method is to attach to the running application with a debugger and step through the execution of the code.
This chapter will cover interactive debugging, using the Eclipse platform to provide the graphical interface.
Unit Testing MapReduce Jobs MapReduce jobs, by their very nature, don’t lend themselves to the traditional unit testing model.
The common approach is to verify that all of the lower-level APIs are working correctly through their own unit tests.
Next, you build small test datasets that have known outcomes and run them on a simulated MapReduce cluster, and then examine the output.
These tests, when run on the simulated clusters, tend to be quite slow—on the order of minutes per testdue to the cluster setup and teardown times.
If a real cluster is available for testing, the test run time will be shorter, but the tests must be coordinated among the cluster users.
The unit tests covered here are built on the Hadoop basic test case class org.apache.
This class provides a basic JUnit 3 test base that will start and stop a mini-HDFS with two DataNodes and a NameNode, and a mini-MapReduce cluster with two TaskTrackers and a JobTracker.
This is a complete cluster, with web consoles for the JobTracker and the NameNode.
All of the servers will run on the local machine, and the ports will be chosen from the free ports on the machine.
The virtual cluster will be established and torn down for each test that any derived classes execute.
In the archive that contains a Hadoop release are a number of prebuild release-specific JAR files.
These JAR files are named in the form hadoop-major release-minor release-component name.jar.
The standard Hadoop JARs are found in the root directory of the installation.
The root directory will contain JAR files for Ant, Core, examples, test, and tools.
Each component may have an associated lib directory containing JAR files on which the component depends.
By convention, the lib directory is located in the same directory as the component JAR.
For Hadoop Core unit tests, and for running standard jobs, the Hadoop Ant environment and the bin/hadoop script configure the runtime environment for the unit test or job, respectively.
The unit tests that developers write to run in their workspace, or those created for build automation tools, do not generally have this luxury and must set up the runtime environment directly.
If the cluster does not start successfully, the test case will fail, without exercising the classes being tested.
A startup failure is commonly due to a configuration issue, either with the runtime classpath or server or cluster configuration file.
In particular, the NameNode and the JobTracker use Jetty to provide web servers for their web UIs.
The error messages relating to the Jetty web server start failures do not provide sufficient information for the novice to resolve the configuration problem.
When this configuration information is absent, the virtual cluster behavior is unpredictable.
Include this JAR in the build path of your project and in the runtime classpath for the JUnit execution environment.
Include this JAR in the build path of your project and in the runtime classpath for the JUnit execution environment.
Include the needed JARs in the build path of your project and in the runtime classpath for the JUnit execution environment.
It is often simpler to just include all of the JARs in the lib directory.
It may be specified by the following argument on the JVM command line:
For unit testing, the entire classpath, including the Hadoop JARs, were folded together.
All of a sudden, the unit tests started failing with an exception thrown by Jetty.
The Saxon JAR was in the classpath before the Jetty JAR, so it was being used to deliver the XML parsers, and the parser was not validating.
A couple of system properties control the XML parser that applications will get:
This is the Sun JDK default and works well with Jetty.
Validated XML is good, and these are parsed only at job start time.
Hadoop Core servers rely on Jetty to provide web services, which are then available for internal and external use.
If the Jetty JAR is missing from the classpath, the servers will not start.
The Hadoop Core Jetty configurations also require the JARs that are in the lib/jetty-ext directory of the installation.
If they are not present in the classpath of the unit test, nondescriptive failure-to-start error messages will be generated.
In these listings, only the relevant exception lines are shown; the stack traces have been removed to aid clarity.
In this case, various required configuration files may be missing, resulting in unexpected failures.
Listing 7-8 shows the error message that results if the directories cannot be deleted.
The typical reason for the failure is that a prior instance of MiniDFSCluster is still running.
By default, this virtual cluster starts six server processes: one NameNode, one JobTracker, two DataNodes, and two TaskTrackers.
If the NameNode or the JobTracker did not start, the tests cannot be run.
If the HDFS fails to start, the MapReduce portion is not started.
The NameNode is kind enough to actually report that it is up, as in this example log line:
The DataNodes’ state must be deduced from the log messages.
As with the HDFS portion, the JobTracker informs you directly that it is running, but the TaskTracker status must be deduced from the logs.
Listing 7-10 shows the log lines that show the TaskTracker is running.
If the hadoop.log.dir system property is unset, a subprocess of the virtual cluster may crash and leave the test case in limbo.
The Eclipse framework provides a decent way to run individual or class-based Hadoop.
Occasionally, some state can get lost, particularly in the Windows environment, and the virtual cluster will fail to start.
The indication of this will be a series of messages stating that a connection attempt has failed.
This delegate class provides a JUnit 4-friendly way to build test classes, and exposes information useful to understanding what is happening in your test.
In particular, all JobConf objects used by the test cases and classes being tested must be children of the JobConf object returned by the createJobConf() method.
Any test case that needs to create files in HDFS or access files in HDFS will need to call the method getFileSystem() to get a file system object to use for the interactions.
Ensures that the required Hadoop system properties are set to sensible values if they are unset.
It will then start the virtual cluster, with NameNode, and a JobTracker.
This method will throw an exception or possibly hang if the virtual cluster does not start.
NameNode and DFS configuration objects to log at level info.
Path getTestRootDir()  To determine the root path of  Returns the path of the test case in the test in HDFS, when you  HDFS.
FileSystem getFileSystem()  When the test case need to  Returns a file system object concreate files or otherwise inter- structed for the virtual cluster’s HDFS act with HDFS.
JobConf createJobConf ()  Must be used to create the  Creates a JobConf object that is JobConf object’s used by your  correctly configured for the virtual test cases.
Configuration parameters for Interacting with Virtual Clusters Several core parameters are needed for the tester and the test cases, when interacting with the virtual cluster.
Table 7-2 details the parameter names, how to get their values, and what to do with them.
When debugging test cases, it is very useful to know where the log files are being written, and the web addresses for the NameNode and JobTracker.
For accessing files in the virtual cluster’s HDFS, the HDFS file system URL must be available.
Writing a Test Case: SimpleUnitTest The sample SimpleUnitTest test case simply starts a cluster, writes a single file to the cluster HDFS, and reads that file back, verifying the contents are correct.
Normally, the Logger would be from the class being tested to better enable control over the logging levels.
In this sample test case, there is no class being tested, so a logger is created.
The test also makes two JUnit assertNotNull checks, to verify that the cluster configuration information is available.
If there are any failures, an exception should be thrown.
The JUnit framework will catch the exception and mark the test set as failed.
It also will set a small number of required system parameters in the configuration object returned by getConf(), if those parameters are currently unset.
It is essentially the finally clause for the test class.
The test has the standard stylized framework you will see in all of the sample code.
Any code that allocates objects that hold system-level file descriptors is done in a try block.
The try block has a finally clause where the system-level file descriptors are closed.
This pattern, if rigorously applied, will greatly reduce job failures when the jobs are running at large scales.
The test grabs the FileSystem object out of the base class using the following call:
This ensures that the file operations will be on the virtual cluster’s HDFS.
As a double layer of paranoia, the following line verifies that the file system is in fact an.
The following line will create testFile, if there is not an existing file by that name.
The following line writes the string testData with a small header to testFile.
At least through Hadoop 0.19.1, files do not really become available until after they are closed; therefore, the file is closed via out.close()
Since the try block will also call the close() method on out, if out is not set to null, out is set to null to avoid a duplicate close()
This verifies that the file exists in the file system.
At this point in the test case, the file has been created and is known to exist.
The test case will now open the file and read the contents, verifying that the contents are exactly what was written.
To open the file, rather than to create it, the following line is used:
The first line in the next snippet reads back one UTF8 string, and the next line verifies that the expected data was read.
The PiEstimator class, as it stands, is not unit test-friendly, and very little information can be extracted.
The only thing that can be done to verify the result is to examine the estimated value of pi.
As is common in unit tests, this test case declares that it is in the same package as the class under test:
When the debugger breaks there, the various servers are still running and available.
The NameNode web GUI is not known to work correctly with Hadoop 0.19.0 under the virtual cluster.
The method under test is launch(), and the test method is testLaunch()
The preamble, shown in Listing 7-18, logs a couple of key pieces of information to enable the test runner to interact with the virtual cluster services.
The JobTracker URL will let you interact with the running or finished job, examine the task outputs, and look at the job counters.
The HDFS URL will let you interact directly with the file system to view the data files.
Replace the fictitious hostname with localhost, and you will be able to fetch the task logs.
As a debugging nicety, the next line tells the TaskTrackers to send all task output to the console of the process that submitted the job.
The default value is FAILED, and only failed tasks have their output printed.
The PiEstimator instance needs to be created and configured, as shown in Listing 7-20
The launch() method is invoked, and the results are tested, as shown in Listing 7-21
This requires knowledge of the proper arguments to the method.
This completes the walk-through of a unit test that invokes a MapReduce job.
Running the debugger on MapReduce Jobs There are several basic strategies for running a MapReduce job under the debugger.
Here, we’ll start with simplest and move to the more complex methods.
When you are single-stepping through a map or reduce task, it is common for more than 10 minutes to pass.
If the value has not been lengthened, the task you are debugging will be killed.
Running an Entire MapReduce Job in a Single JVM The normal process of job submission involves the JobClient class storing all of the relevant information about the job in HDFS, and then making an RPC call to the JobTracker to submit the job for execution.
This configuration key may have the value of local, in which case the entire MapReduce job will be run in the JVM that is submitting the job.
The cause of most of the restrictions is that the map and reduce tasks do not run in their own JVMs.
There is no way to change the JVM working directory, classpath, or other command-line configured options.
A significant result of this is that the DistributedCache behavior is very different.
If your job relies on the DistributedCache, this method will not be a good debugging choice.
The number of reduce tasks is limited to zero or one.
If your job requires more than one reduce task, this method will not be a good debugging choice.
The example in this section uses our old friend PiEstimator, the example that comes with Hadoop Core, as the MapReduce job.
The classpath must include the Core JAR, the example JAR (since the test case comes from this JAR), and all of the JARs from lib and lib/jetty-ext, as shown in Figure 7-1
You must explicitly specify all of the JARs required for your job here.
In this example, Eclipse is not configured to load the native compression codec libraries.
What is the point of using a visual debugger if the source code is not available? You will need to set up any source paths required for your application, as shown in Figure 7-3
You also need to set up the following arguments, as shown in Figure 7-4:
Once the directory /tmp/pidebug has been made, the Run button in the bottom-right corner of the Run Configurations window will be active.
Click this button just to verify that the job will work in this setting.
The final line, with the estimated value of pi, indicates that the environment is correctly configured.
For Figure 7-5, a breakpoint was set in the map task, and the job launched via the Debug As Java Application menu item.
The job is configured to use the local file system for all working storage, and you may examine the files using the normal file system tools.
In this case, with the PiEstimator test stopped in the first line of the first map task, you can see that the working directory in the local file system contains a number of interesting files.
Let’s examine the contents of part0 of the input to demonstrate this.
Debugging a Task Running on a Cluster Hadoop Core, as of version 0.19.0, does not provide any tools to specify which tasks of a job to enable Java debugging services on, nor does Hadoop Core provide a way to indicate on which host and port such a task might be listening for remote debugging connections.
To debug tasks running on a cluster, the JVM parameters for the task have remote debugging enabled via the command-line arguments.
The core issue is to arrange for the JVM of the server or task that is to be debugged to have the additional command-line arguments that enable the Java Platform Debugger Architecture (JPDA) servers.
Table 7-3 describes the parameters to the JPDA debugging agent, agentlib:jdwp, that must be enabled in the task JVM to allow connections from debuggers.
The program receives two arguments: the transport, dt_socket, and the allocated port.
This program may be used to provide notification that the task has engaged the debugger agent and the port it is listening on for a debugger connection.
At this point, any program defined by launch would be executed.
Program execution will be stopped, while the agent waits for a debugger connection.
The JPDA invocation arguments need to be added to the value for this parameter.
The parameters that are most relevant to the user are suspend and address.
The suspend=y setting forces the JVM to be stopped just before the main() method is invoked.
The value suspend=n may also be used, in which case the JVM will run normally.
The port that is allocated will be printed on the standard output.
This precluded specifying a fixed port as part of the address parameter.
If only a single task will run at a time, a port may be specified via address=host:port, where host is optional.
It is best to have only the option suspend set to y for all of the task JVMs, isolate this option to a single machine in the cluster, to avoid requiring extensive interaction with each task, at minimum, connecting to the task in the debugger to resume execution.
The JPDA arguments will be passed via the command line to the child JVM, and JVM reuse will be explicitly disabled to avoid complications.
Figure 7-6 shows the Eclipse setup for a remote debugging session.
The Source tab has been configured identically to the earlier source configuration shown in Figure 7-3, with the hadoop/src directory and its subfolders.
Then you click the Apply and Debug buttons, to save this session and connect to the task.
Figure 7-8 shows the PiEstimator task in the debugger, stopped at a breakpoint in the map() method.
This is configured per task and needs to be determined from task log files.
To determine the port to connect to, if no launch program has been provided, requires finding the per-task stdout log file.
At present, this step is manual and requires issuing shell commands on one of the cluster machines.
This is available via the JobTracker web interface or via the command-line tool:
These values are what you would put into the Host and Port fields of the Remote Debugging configuration dialog box.
The slaves.sh command will execute its command-line arguments as a shell command on each machine in the conf/slaves file, the output of these commands will have the generating host’s IP address prefixed to the output lines.
Let’s see an example of determining the ports and hosts of the suspended tasks:
Rerunning a Failed Task The IsolationRunner provides a way of rerunning a task out of a failed job.
Normally, the framework immediately removes all local task specific data when a task finishes.
Configuring the Job or Cluster to Save the task Local Working Directory Two configuration keys provide some control over the ability to rerun a task.
Any task that matches this pattern will not have its task files removed.
To save the results of all map tasks, a pattern *_m.* would work.
Any task that fails will not be subject to cleanup.
Running the find command on this set of directories looking for files named job.xml will present a set of candidate tasks to be run via the IsolationRunner.
It demonstrates how to run it so that the map task local file space is left in intact.
Then you find the job.xml files that can be run via the IsolationRunner and run one of them in a way that will enable the use of the debugger.
Here’s how to put it all together for the IsolationRunner:
Once the child JVM is configured and waiting, Eclipse must be configured to connect to it.
Figure 7-9 shows the Eclipse Debug Configuration window for setting up a remote debugging connection.
The Host field must be filled in with the host on which the JVM is running, and the Port field filled in with the value from the Listening for transport dt_socket at address: XXXXX output line from the JVM.
Figure 7-10 shows Eclipse connected to the running JVM of the PiExample test case.
Eclipse setup to connect to port 54990 on the specified host.
This chapter has provided you with the techniques needed to build unit tests for your MapReduce jobs.
This chapter also demonstrated three ways to run MapReduce applications under the Eclipse debugger.
This will enable you to understand problems that occur only at scale on your large clusters, as well as explore how things work directly on your local development machine.
This chapter discusses techniques for handling larger jobs with more complex requirements.
In particular, the section on map-side joins covers the case in which the input data is already sorted, and the section on chaining discusses ways of adding additional mapper classes to a job without passing all the job data through the network multiple times.
The framework will schedule the map tasks if possible so that each map task’s input is local, and provides several ways of reducing the volume of output that must pass over the network to the reduce tasks.
This is a good pattern for many (although not all) applications.
There are other options available to Hadoop Core users, either by changing the pattern of the job or by providing the ability to use other languages, such as C++ or Perl and Python, for mapping and reducing.
Streaming: Running Custom MapReduce Jobs from the Command Line The streaming API allows users to configure and submit complete MapReduce jobs using the command line.
As an added bonus, streaming provides the ability to use external programs as any of the job’s mapper, combiner, or reducer.
The job is a traditional MapReduce job, with the framework handling input splitting, scheduling map tasks, scheduling input split pairs to run, shuffling and sorting map outputs, scheduling reduce tasks to run, and then writing the reduce output to the Hadoop Distributed File System (HDFS)
In the following example, we will demonstrate how to run a simple streaming job to sort all the input records of a dataset using MapReduce.
The argument informs the bin/hadoop script that the streaming JAR is to be used.
The use of this input format requires the output key format be set to Text via -D mapred.output.
Hadoop streaming provides the user with the ability to use arbitrary programs for a job’s map and reduce methods.
The framework handles a streaming job like any other MapReduce job.
The job might specify that an executable be used as the map processor and for the reduce processor.
Each task will start an instance of the applicable executable and write an applicable representation of the input key/value pairs to the executable.
The standard output of the executable is parsed as textual key/value pairs.
The executable being run for the reduce task will given an input line for each value in the reduce value iterator, composed of the key and that value.
The following example uses /bin/cat as the mapper and a Perl program to produce line counts of distinct lines from the input set.
The argument -reducer "/usr/bin/perl -w wordCount.pl" causes the Perl program wordCount.pl to be used to perform the reduce.
I had a large dataset composed of many input files in one compression format, and the data needed to be compressed in a different format.
The author ran a streaming job, with the map executable set to /bin/cat and the minimum input split size set to Long.MAX_VALUE, and enabled map output compression of the required type.
In a few minutes the cluster had uncompressed and recompressed the data files.
The following streaming example takes input from the directory words, uses /bin/ cat as the map executable, has no reduce phase, and compresses the job output using the GzipCodec.
The IdentityMapper could have been used just as easily, but the use of /bin/cat is just plain fun.
Note The streaming API explicitly forces the output key/value classes to be text.
There are additional language constructs that allow the addition of arbitrary Java classes into the namespace of the Jython applications.
There are also additional primitive operators for interacting with native Java types, and a transparent translation between the Java String class and the Python string class.
People have good results having Python applications used by Hadoop streaming.
Streaming Command-Line Arguments The streaming command-line interface provides a rich set of command-line arguments for controlling the execution of your streaming job.
It may be given multiple times to provide multiple input paths.
The directory must not exist prior to job start, and will be created by the framework for the job.
This flag is used as the mapper for  setMapperClass() mapred.lib.
This flag is used as the combiner for  setCombinerClass() the map output.
This flag is used as the reducer for the setReducerClass() mapred.lib.
This flag is often used to pass  addCacheFile() with the executable to be used for the mapper,  symlink combiner, or reducer to the tasks.
The executable will be stored in the current working directory of the task.
The default TextInputFormat is most efficient because the individual input lines are not split into key/value pairs.
How key/value pairs are split and joined in a streaming job.
Any text in the file that is not between a beginning and an ending marker is ignored.
When run on an Ant build file, the following example produces a sorted list of the target blocks within the XML file.
This text will include any line separator sequences that are present in the original file in the block.
There is some ability to control how much read ahead is done when looking for a match end.
It is possible to control the maximum size of a key.
If the parameter slowmatch=true is provided, the framework will attempt to exclude recognizing the beginning and ending text if they are within a CDATA block.
The framework will look ahead only in lookahead bytes, which by default is equal to twice maxrec bytes, or 50,000
The value is interpreted as a regular expression if slowmatch is true.
The value is interpreted as a regular expression if slowmatch is true.
The record reader will look forward only the maximum of maxrec or lookahead bytes for the end of a CDATA block.
The record reader will look forward only the maximum of maxrec or lookahead bytes for the end of a CDATA block.
For more information, look at the excellent tutorial on using streaming at the Hadoop Core web site: http://hadoop.apache.org/core/docs/current/streaming.html.
Using Pipes Hadoop Core provides a set of APIs for use by other languages that allow a reasonably rich interaction with the Hadoop framework.
The C++ interface lends itself to usage by Simplified Wrapper and Interface Generator (SWIG) to generate other language interfaces.
The C++ APIs for interacting with MapReduce are located in the directory src/c++/pipes/api/
Using Counters in Streaming and Pipes Jobs The framework monitors the standard error stream of the mapper and reducer processes.
Any line read from the standard error stream that starts with the string reporter: is considered by the framework as an interaction command.
As of Hadoop 0.19.0, there are two commands honored: counter: and status:
We strongly recommend that you follow the same practice with counters in your streaming jobs as you do with regular jobs.
A counter record should be emitted for each input record, for each output record, for each record that is invalid, and for each crash or exception when possible.
The more detail about the job provided by the counters, the more understandable the job behavior is.
The colon character is not added by the framework as a separator.
The parameter increment must be a whole number between LONG.MIN_VALUE and Long.
The group and counter parameters must not have the comma character in them.
The library provides application writers with a set of methods for interacting with HDFS.
The methods in turn use JNI to actually interact with an embedded Java Virtual Machine (JVM) which actually interacts with HDFS.
This method is subject to underlying support for flush in HDFS (not available through Hadoop 0.19.0)
A Linux i386 version is provided in the distribution in the directory libhdfs.
If you need to build a custom version of the library.
The command to cause libhdfs to be compiled is the following:
If the libhdfs property is not set, Ant will not compile libhdfs.
If they are not preset, the embedded JVM will either fail to launch or the class loader of the embedded JVM will not be able to load the Hadoop classes required to provide HDFS file service.
It allows arbitrary programs to access data that is stored in HDFS.
The SourceForge project FUSE, http://fuse.sourceforge.net/, provides a set of APIs that allow programs written to those APIs to be mounted as host-level file systems.
There is no prebuilt version of fuse-dfs bundled into the distribution.
Note The fuse-dfs compilation environment will compile only for the i386 OS architecture.
The fuse-dfs package requires a modern Linux kernel with the FUSE module, fuse.ko, loaded.
To actually mount an HDFS file system, the environment variables listed in Table 8-4 must be set correctly.
The  set up the runtime environment for OS compilation architecture of  the fuse_dfs program.
In Hadoop 0.19.0, the FUSE mounts produced corrupted directory listings.
The core configuration requires that the LD_LIBRARY_PATH environment variable include the directories that libjvm.so and libhdfs.
Cut from bin/hadoop, to ensure classpath is the same as running installation.
The following command will mount a read-only HDFS file system with debugging on.
The mount point for the file system is /mnt/hdfs, and the arguments after the /mnt/hdfs are passed to the FUSE subsystem.
These are reasonable arguments for mounting an HDFS file system:
It is possible to set up a Linux system so that an HDFS is mounted at system start time by updating the system /etc/fstab file with a mount request for an HDFS file system.
The sorting stage requires data to be transferred across the network and also requires the computational expense of sorting.
In addition, the input data is read from and the output data is written to HDFS.
The overhead involved in passing data between HDFS and the map phase, and the overhead involved in moving the data during the sort stage, and the writing of data to HDFS at the end of the job result in application design patterns that.
Many processes require multiple steps, some of which require a reduce phase, leaving at least one input to the next job step already sorted.
Having to re-sort this data may use significant cluster resources.
The following section goes into detail about a variety of techniques that are helpful for special situations.
Chaining: Efficiently Connecting Multiple Map and/or Reduce Steps New in Hadoop 0.19.0 is the ability to connect several map tasks together in a chain.
Prior to the chaining feature, the user was forced to either construct large map methods or run multiple jobs as a pipeline, with all the additional I/O overhead.
Figure 8-2 provides a graphical depiction of the flow of key/value pairs through a job that uses chaining.
The chaining feature constructs a pipeline, internal to the task, which feeds each key/value pair from each output.collect to the map method of the next mapper in the chain.
The map task may be a chain, and the reduce task may have a chain as a post processor.
This allows for the construction of simple mapper classes that do one thing well, as well as the ability to rapidly modify a chain to support additional or different features.
Note At least through Hadoop 0.19.0, it is not possible to run the chain mapper through the streaming APIs.
Configuring for Chains There are two possible chains that can be established for a job: the map task can be a chain or the reduce task can have a chain.
The framework serializes the key/value into the output format for the particular task, and the output.collect() method returns with the contents of the key/value object unchanged.
With chaining, each key/value pair passed to the output.collect() method is the input to the next mapper in the chain.
During job configuration, when a mapper is being added to a chain, the style of key/value passage is specified, either by value or by reference.
Passing by reference eliminates a serialization and deserialization for the key/value, a potential speed increase.
If the Mapper.map() method uses the key or value method after the output.collect() call, subtle errors may occur if the key or value has been modified by a subsequent mapper.
Note If pass by reference is enabled, some level of verification needs to be in place to ensure that no use of the key/value object is made after a call to output.collect or that no mapper in the chain that receives the key or value reference modifies the contents.
Any compliance failures in this implicit contract will cause difficulties in diagnosing problems.
This is especially difficult because the configuration for pass by reference is remote from the Mapper.map() method that has to determine whether the pass by reference is safe and by the fact that the mapper class might be unaware about being part of a chain.
The RecordReaders for the input split will throw an IOException for “wrong key class” or “wrong value class”, and the OutputCollector will throw an IOException for “Type mismatch in key from map” or “Type mismatch in value from map”
At least as of Hadoop 0.19.0, the chaining code does not explicitly check the runtime types of the key/value pairs being passed between elements in the chain.
The types are checked only during the job configuration phase.
It is recommended that these custom configurations be light configurations, which have only the special parameters for that item.
For a map task, the chain will have only mapper items.
For a reduce task, the chain will have a leading reducer item and then some number of mapper items.
At task runtime, a JobConf object is made for each item.
This JobConf object is constructed by making a copy of the localized task JobConf object and then copying each key/value pair out of the per map configuration into the copy.
This modified copy is then passed to the configure method of the item.
The reducer close is called after all the mapper close() methods have completed.
If any close() method throws an exception, no further close() methods are run.
Caution This is the close() behavior, as of Hadoop 0.19.0
Configuring Mapper tasks to be a Chain A mapper task is either a normal map task or a chain.
The configuration of one excludes the configuration for the other.
The addMapper() method configures the mapper tasks to be run as chains and appends the specified mapper class to the end of the current chain mapper task chain.
Class<? extends Mapper klass false The mapper class to be run.
JobConf mapperConf true  The configuration object that provides custom configuration data for this mapper instance at mapper runtime.
The input and output classes will be stored in this object.
Any keys present will override the corresponding values in the task’s localized JobConf object.
Note For each addMapper() call, the mapperConf object should be constructed via JobConf mapperConf = new JobConf(false)
This will minimize the possibility that a configuration value will step on a task’s localized value.
Any custom parameters may then be set on the mapperConf object before the addMapper() call.
The clear() method may be used to reset a mapperConf for use in the next call to addMapper()
As of Hadoop 0.19.0, this parameter should not be passed as a null because a full JobConf object will be initialized.
Configuring the reducer tasks to Be Chains Configuring the reducer phase is very similar to the configuration of the mapper phase with one additional requirement: the job configuration step must make a call to ChainReducer.
JobConf reducerConf true  The configuration object that provides custom configuration data for this reducer instance at reducer runtime.
The input and output classes will be stored in this object.
Any keys present will override the corresponding values in the task’s localized JobConf object.
Class<? extends  klass false The mapper class to be run.
JobConf mapperConf true  The configuration object that provides custom configuration data for this mapper instance at mapper runtime.
The input and output classes will be stored in this object.
Any keys present will override the corresponding values in the task’s localized JobConf object.
The chain methods do not configure the job level chaining configuration parameters.
The maps and the reduce have a particular id to help distinguish them.
The actual ordering information has to be extracted from the job log.
The assumptions are that the hadoopprobook and commons-lang JARs are in the current working directory.
The construct 2>&1 forces the standard error output to go to the same descriptor as the standard output.
Map-side Join: Sequentially Reading Data from Multiple Sorted Inputs In a traditional MapReduce job, the framework sorts all data for a reduce task before presenting the keys sequentially to the reduce task.
If the input data is already sorted, traditional MapReduce requires that the full map shuffle and sort process take place before the reduce task receives the sorted keys.
Map-side joins provide a way for a map task to receive keys in sequential order and to receive all the values associated with each key (very similar to a reduce task)
The map task reads the data directly from HDFS and no reduce is needed, which greatly reduces cluster loading.
The map-side join provides a framework for performing operations on multiple sorted datasets.
Although the individual map tasks in a join lose much of the advantage of data locality, the overall job gains due to the potential for the elimination of the reduce phase and/or the great reduction in the amount of data required for the reduce.
Caution There are several constraints on when map-side joins may be used, and the cluster loses capability to manage data locality for the map tasks (see Table 8-11)
The author has used map-side joins extensively in large-scale web crawls to eliminate recently crawled URLs from the set of freshly harvested URLs being prepared for fetching.
As of Hadoop 0.19.0, the join package supports full inner and outer joins.
In the following section, the term dataset is used to refer to one join item in the set of elements being joined.
The dataset can be an actual dataset or the result of a join.
Note As of at least Hadoop 0.19.0, the joins handle only keys that implement WritableComparable and values that implement Writable.
The join framework has not been updated to handle arbitrary key/value classes.
A dataset is the set of input splits that an InputFormat will produce when given the name as an argument.
The goal is to force the InputFormat not to split individual data files, thereby ensuring that each returned split contains the entirety of a single reduce task output, or partition.
The directory and the partition files it contains is a dataset.
Using the join package imposes the following limitations on your application.
All datasets must be  The sort ordering of the data in each dataset must be identical for sorted using the same  datasets to be joined.
All datasets must be par- A given key has to be in the same partition in each dataset so that all titioned using the same  partitions that can hold a key are joined together.
The number of parti- A given key has to be in the same partition in each dataset so that all tions in the datasets  partitions that can hold a key are joined together.
The InputFormat must  The OutputPartitioner class returns a partition number for each return the input splits in  key, which determines the reduce task each key is assigned to.
The file name is the string part-, followed by a 0 padded five-digit number, which is the reduce output partition.
Note The map-side join has no simple way to discover what reduce partition a split was created as.
The InputFormat’s split routine is called with the minimum split size set to Long.MAX_VALUE, under the assumption that this will cause each split returned to be one complete input partition.
If this assumption of equivalent ordering is incorrect, the behavior of the map-side join will be incorrect, and this failure will be detectable only by examining the output data.
For each dataset specified in the join, the input splits of the dataset are collected.
Each map task is given a JoinRecordReader from the outermost join as the task input record reader and receives the key/value sets of the join one by one in the map method.
The default outer join behavior will receive each key in the input split set, in the sort order with all the values for that key.
The map method behaves very much like a traditional reduce.
Joins can be made on direct input datasets or on the results of joining input datasets; arbitrary deep nesting of this joining structure is supported.
The map method will be called with a key/value set only if every dataset in the join contains the key.
The TupleWritable value will contain a value for every dataset in the join.
The map method will be called for every key in the set of datasets being joined.
The TupleWritable value will contain values for only those datasets that have a value for this key.
The override join is unusual in that the there will only ever be one value passed to the map method.
In the inner and other joins there will be a set of values passed to the map method.
The override join maps a call to the map method with each key in the input split set and with that single value from the rightmost input split or join that has a value for the key.
The use of this join style requires that you order your input datasets (from least to most important)
For any given key, your map method will be given the value from the most important dataset that contains the key.
The join framework provides a mechanism for defining additional operators.
The string YOUR_OPERATOR in the key definition must be replaced with the name of the custom join operation.
YOUR_OPERATOR can then be passed as the op parameter to the compose methods that accept op, and used anywhere that the predefined operators, inner, outer, and override, are used.
Details of a Join Specification A join specification is an operator and a set of data sources.
The predefined operators are inner, outer, and override to correspond with the join types.
A data source is either a table statement or a join specification.
The comma character is used to separate data sources; parentheses, (), are used to group the data sources for an operator.
Table 8-12 provides examples of several join data source specifications.
For example, suppose that a join of two tables is made.
The map method will be called twice with key 1; once with a tuple a,c; and once with a tuple b,c.
Composing a Join Specification The framework provides three helper methods, all named compose(), which build either a full join specification or build the input specification for a particular dataset in the join.
Two of the methods construct a full join specification and are used when all the datasets within the join have the same InputFormat.
These two methods differ only in accepting String or Path objects for the dataset locations.
The third is used to construct a table statement for a dataset that includes a specified InputFormat and requires the application developer to aggregate the results into a full join specification.
This method produces a table statement from an input format class object and a path to a dataset.
The fully qualified class name of inf will be used in the returned table statement.
It is commonly used when building a join statement from input datasets that have different input formats.
This method is identical to the String variant except that Path objects instead of String objects provide the table paths.
Optionally, the mapper, reducer, reduce count, and output key/value classes may be set.
The mapper key class will be the key class of the leftmost data source, and the key classes of all data sources should be identical.
The mapper value class will be TupleWritable for inner, outer, and user-defined join operators.
For the override join operator, the mapper value class will be the value class of the data sources.
In Listing 8-7, note that the quote characters surrounding the path names are escaped.
The TupleWritable class provides a number of ways to understand the shape of the join result row.
Writable get(int i) The ordinal number  Returns the value object that the dataset has of a dataset.
The application will need to make a copy of the contents before calling get() again if the contents need to exist past the next call to get()
Iterator<Writable>   Returns an iterator through the values that iterator()   are present.
Note A dataset may provide a null value to a join result record if the dataset is composed only of keys.
Using the construct get(i)==null will not correctly indicate that dataset i did not have the join result record key present; only the call having(i) is sufficient.
The Hadoop Core framework provides a package for performing data aggregation jobs.
This package may conceptually be thought of as Hadoop streaming for statistics.
The analogy is incomplete because some code must be written to use the aggregation services.
The framework provides a set of aggregator services (see Table 8-14 for descriptions of the predefined aggregator services)
The user can define the custom aggregator (see Listing 8-15)
The aggregation framework manages the mapper, combiner, and reducer; and the aggregation service produces the correct key/value pairs to pass forward.
The user is responsible for parsing the input record and invoking the aggregate service with the record key and count; the record and count are the traditional map task output key/value pairs.
Quite often, the key has no meaning for the job and is simply a label for the end user.
The count must the textual representation of an object that the aggregator service expects: a number for DoubleValueSum, a whole number for the LongValue series, an arbitrary string for the StringValue series, and a whole number for UniqueValueCount and ValueHistogram.
DoubleValueSum  Computes the sum of input  DoubleValueSum Label The number to accumulate  - DoubleValueSum: values.
The behavior is  LabelTAB37 expected to be doubles and    identical to LongValueSum.pl, are summed.
A single out-   so the LongValueSum.pl put record per reduce.
LabelTAB37 are expected to be longs, and the max value is output.
LongValueMin  Computes the minimum in- LongValueMin Label The number to challenge the - LongValueMin: put value.
The  LabelTAB3 are expected to be longs, and    behavior is essentially identhe min value is output.
A    tical to LongValueMax, so the single output record per    LongMax.pl example is used.
LongValueSum  Computes the long sum of in- LongValueSum Label The number to add to the LongSum.pl LongValueSum: put values.
LabelTAB37 expected to be longs, and the sum is output.
LabelTABMyLong object’s toString() method      StringCandidate is invoked, and the resulting String is compared.
StringValueMin  Computes the lexically least  StringValueMin Label String to challenge the cur- - StringValueMin: input value.
UniqValueCount  Computes the set of unique  UniqValueCount Object as a Ignored; 1 is acceptable.
Any new objects encountered in a map or reduce task past this value are discarded.
ValueHistogram  Computes a histogram of  ValueHistogram The object The count of times the object LongHistogram.pl ValueHistogram: the occurrence counts of the  as a string.
The code that the user must supply can be supplied as a streaming mapper or via a Java class.
Aggregation Using Streaming The user-supplied code must take an input record and return an aggregator record.
Listing 8-9 provides a sample Perl mapper that computes the sums of input files that are sets of long values.
Each reduce task will have a single output value, the key will be the string SUM, and the value will be the sum of all of the long values routed to that reduce task.
The streaming command that was used is in Listing 8-10
Note The reducer is defined as aggregate for the streaming job in Listing 8-10
If an error shown in Listing 8-11 happens, it generally means that an unrecognized aggregator id has been output by the mapper.
The command-line arguments accepted in args are listed in Table 8-15
Required None  The input directory or file to load input records from.
As with any MapReduce job, this directory must not exist prior to job start and will be created by the framework for the job.
Optional None An XML file to load as configuration data.
The configuration data causes the class AggregatorTests to be used.
The text UserDefined tells the framework that this is a userdefined class.
The value is the two-part text string, UserDefined, and the fully qualified class name, with a comma separating the parts.
Input records are passed this order to each of the defined classes.
The framework does not have an example for defining a custom value aggregation service.
Side Effect Files: Map and Reduce Tasks Can Write Additional Output Files The Hadoop Core framework assumes that individual map and reduce tasks can be killed with impunity, which allows the use of speculative execution and retrying of failed tasks.
The framework achieves this by placing the task output in a per-task temporary directory that is deleted if the task fails or is killed, or committed to the job output if the task succeeds.
As of Hadoop 0.19.0, this directory is a function of the OutputCommitter the job is using.
Tasks that want to create additional output files directly can create them in the temporary output directory.
Tasks can create files in this directory, and the files will be part of the final job output when the tasks succeed.
The OutputCommitter actually commits the files to the actual job output directory.
This data, commonly called dirty data, is often not perfectly compliant with the data specification.
It might also be the case that some input records, while compliant, are unanticipated.
These data records can cause a map or a reduce task to hang, crash, or otherwise complete abnormally.
By default, the framework will retry the failed task, and the entire job will be terminated if the task does not complete after a number of attempts.
Operationally it is not desirable to have a long running job terminated if only a small number of records are causing problems.
New in Hadoop 0.19.0 is the ability to specify that a job can succeed even if a specified number of records cannot be processed.
The framework also allows the job to specify what percentage of the map tasks and what percentage of the reduce tasks must succeed for the job to be considered a success.
In some applications, there is a threshold for good enough that is less than 100%
In an application the author worked with, there was a piece of legacy code that would catastrophically crash every few thousand records.
Due to a variety of business reasons, it was decided not to attempt to fix the legacy application, but instead to just accept those failures.
In the real world of large-scale data processing, often the individual data records are not valuable, and the time value of the transformation result of the dataset is high.
In these situations it is acceptable to accept some failing records and or some failing tasks and then let the job complete.
Dealing with Task Failure The Hadoop framework provides four different mechanisms for dealing with task failure:
The actual value specified is the size of the record block that is acceptable to lose.
The smaller the number, the more work the framework might need to do to minimize the dropped records.
Skip processing requires that the framework keep track of what record is being processed by the task.
For streaming jobs and for jobs that consume multiple records to work on groups of records, the framework cannot track the records; the application developer has to assist the framework in this tracking.
For maps and reduces, respectively, there are two configuration parameters and two counters that the application must manage:
A binary search is used to locate the failing record group within the task.
It appears that this search is exhaustive and will continue until the number of task failures is exceeded.
For small values of these configuration parameters, increasing the number of task retries is required.
This feature provides somewhat dedicated resource pools, queuing priority, and pool-level access control.
In the public documentation, a  resource pool is referred to as a queue, so that term will be used in this document as well.
A queue has priority access to a specified percentage of the overall cluster task execution slots.
When a cluster has unused task execution slots, a job in a queue can use the idle slots, even though these slots are over the queue’s priority capacity.
If a job with priority access to these resources is started, the over-priority allocation task slots will be reclaimed as needed within a specified time interval by killing the tasks executing on them.
A queue may have an explicit list of users allowed to submit jobs to it.
The Capacity Scheduler may also have a list of users allowed to manage the queues.
Enabling the Capacity Scheduler To enable the Capacity Scheduler, the following parameter must be placed in the hadoop-site.
As of Hadoop 0.19.0, the Capacity Scheduler JAR is not part of the default runtime classpath.
Each queue that the cluster administrator defines must have a configuration block in the hadoop-site.xml file.
Listing 8-16 defines one queue, one-small-queue, with user jason and group wheel given submission and control permissions.
Figure 8-3 shows the JobTracker web interface for this queue set.
The sum of the percentage cluster capacity for all queues must not exceed 100, or the JobTracker will not start, and there will be an exception in the log file:
Summary The Hadoop framework provides a powerful set of tools to enable users to run more than standard MapReduce jobs.
This chapter covers a number (but by no means all) of the features.
Hadoop is under active development, and new features are being introduced on a regular basis.
The Hadoop streaming and aggregator features are powerful and provide the user command-line tools for performing data analysis on large datasets.
Chain mapping provides a way to maintain code simplicity and reduce overall data flow through the system by allowing multiple mapper classes to be applied to the data for a job.
Map-side joins provide databasestyle joins that can drastically speed up jobs that process bulk data that is already sorted.
There are also a number of features that have become their own Apache projects  (see Chapter 10)
On the Hadoop Core mailing list, a user was wondering about the way to handle a specific style of range query with MapReduce.
The application had a search space and incoming search requests.
In this chapter, we’ll look at a similar setup, as follows:
How do you solve this problem with a traditional MapReduce application? That’s the focus of this chapter.
There are a couple of overall design goals, and the weights of the different factors will vary by installation and by job.
In today’s environment, there is an intense pressure to get processes up quickly and evolve them.
Given agile business practices and tight budgets, rapid evolution becomes the norm.
This practice means that there will be little design time, and the application will be modified, possibly by multiple teams, over a medium to long period of time.
Design Goals Our overall goal is to have a job that runs reliably and fast.
To achieve reliability, we aim for simple code, and implement monitoring to be informed when the algorithms being used are no longer suitable for the scale or patterns of data.
Given that this application is going to evolve rapidly, and eventually be modified, perhaps by different people, each piece of code needs to be simple and clear.
This is in direct opposition to the requirement that the map and reduce methods be treated as the deeply nested inner loops that they are and carefully optimized.
The data is expected to be real-world, dirty, and to change over time.
Wherever possible, the application must handle malformed records in a graceful manner and report on the malformed rate.
To achieve good performance, the job must minimize underuse of the hardware, by managing how the data is split, partitioned, and compressed and by tuning the number of tasks run per node.
To avoid having the network speed become the limiting factor, the transform.
One dataset—the search requests—is composed of Apache log file data in common log format.
The other dataset—the search space—is composed of IP address ranges and a network name.
The output of the job, shown in Table 9-1, will be a modified common log format with the IP address, the network range, and the network name, in place of the original IP address, for those search requests for which a network was found.
Design 1: Brute-Force MapReduce The brute-force MapReduce pattern is generally the quickest to get going and the simplest to manage.
The downside is that these jobs quickly become bound by the network speed and the sorting speed for the cluster.
In a brute-force MapReduce, the only time you have ordered data is in the reduce step.
This forces all of the data to flow through to the reduce task.
There is also the additional complexity that you have multiple record types, which need to be distinguished at reduce time.
The overriding constraint here is ensuring that any given search request record finds all records that it is in range of in the search space.
A Single Reduce Task If a single reduce task is used, all search request records are guaranteed to be in the same partition as their respective search space records.
Table 9-2 defines the comparator behavior for the three cases the comparator will encounter.
The input plan for the reduce method is to receive individual records and to manage the join behavior by maintaining memory about previous records.
This adds complexity to the reduce method and increases the risk of out-of-memory conditions.
To enable the framework to do the aggregation would require having redundant data in the records; the end range would need to be in the value of the search space records.
This requirement is driven by the fact that the OutputCompartor object receives only the key.
A simplification that results from this decision is that, in the first pass, using Text is acceptable for the key and value, as the records may be distinguished lexically.
In a future step, as a performance optimization, we will implement a key class that provides a WritableComparator that handles our keys at the byte level rather than at the object level.
Using the byte-level comparator for a complex key opens the door to the key format and the comparator getting out of sync, introducing the possibility of errors.
Note Having Text objects for the key and value greatly simplifies the initial debugging of the jobs, as the data can be readily examined by eye.
Key Contents and Comparators For simplicity in this pass, we are going to use the same object, Text, for the keys for both datasets, and Text for the values.
To do this, a simple encoding must be defined that allows the origination dataset to be determined easily from the text of the key.
If there is a way to do this without needing to write a custom comparator, the job can be up and running very quickly.
For the stock comparator to work, the keys must lexically compare an order that the reduce method understands and can process with minimal complexity.
If all IP addresses are encoded as a zero-padded, fixedlength hexadecimal string, the primary lexical ordering issue is addressed.
This leaves a single issue: lexically, keys for the search requests will sort before a search space key that has a begin range value equal to the key of the search request.
In the best of all possible worlds, search request keys would appear in the sorted output, after the search space key that opens the range for the request.
The search space key may simply be the begin range and end range values, with a separator character.
There are many simple tools for splitting strings based on a separator character.
This has the advantage that if a lexically larger character is used as a suffix for the search request keys, the search request keys will sort after the search space key that defines the relevant range.
This can be quickly tested by running a small sample dataset through a streaming job to verify that the data compares the way we expect.
A test dataset will be prepared from an Apache log file, with the Perl command in Listing 9-1
A fake range is generated by printing that original value, without the semicolon, with a number ten higher, with a colon (:) separating them.
Notice that the output ordering is exactly the reverse of what our application needs.
Note Listing 9-1 is structured to run from within the Cygwin environment, in the examples directory, on a Windows installation.
Adjust the paths and file names as needed for your local installation.
In the command shown in Listing 9-1, a dataset was prepared with converted IP addresses from an Apache log file.
Listing 9-2 runs a streaming job to see how the records will actually be sorted by the default comparator.
Listing 9-2 is structured to run from the Hadoop installation directory.
A Helper Class for Keys Key management is critical for this job, and to help avoid introducing errors later in the application life cycle, a helper class for keys will be provided.
The initial version needs to be able to validate, pack, and unpack keys to and from the Text objects.
The Hadoop framework creates a runtime environment for the tasks of the job.
An instance of the configuration date is created, and the per-task information modified by adding per-task parameters and adjusting the paths of configuration parameters that have been unpacked into the job or task working areas.
The following parameters are added or modified for a task as of Hadoop 0.19.0:
In our example, four classes are associated with key handling:
These parameters have default values of semicolon (;) and colon (:), respectively.
They may be any pair of characters, as long as the range separator character sorts first.
In Listing 9-3, the key is converted to a String and examined to see if it is one of the two patterns that are accepted.
All IP addresses will be encoded as eight hexadecimal digits.
If the key is a search space item, there will be two IP addresses, with a rangeSeparator character between them only, forcing the string to be seventeen characters in length.
The String.substring method is used for extracting the actual IP address data from the raw string.
If a valid search request or search space definition is found, the helper object is marked valid, isValid = true, and beginRangeOrKey is set to the first IP address found.
If the key contained a search space request, hasEndRange is set to true and endRange is set to the second IP address.
The setToRaw method, in Listing 9-4, is used to create and store a value in a key object that correctly encodes either a search request or a search space.
If the helper object is not valid, nothing is done, and no indication of this is made.
Changing this behavior requires rearchitecting the application to provide a visible trace of this error; logging it is not likely to be sufficient.
A StringBuilder and Formatter are ThreadLocal instance variables, making this class thread-safe.
This is done as a small efficiency and a protection against the day when the helper is used in a multithreaded map task.
Note It is reasonable to assume that anything written to the log by a task will never have been seen by a human being unless something is visibly wrong with the job.
The Mapper With the plan for the comparator handled, it is time to design the mapper.
As a demonstration of using chain mapping, our mapper is going to run a chain to process the incoming values.
The first element in the chain will take action only if the incoming record does not look like a search request or search space key, but instead looks like an Apache log file record.
This mapper will transform the record into a search request.
The next map in the chain will perform validity checking on the keys.
For simplicity of debugging, this version uses text records only.
This demonstrates our standard practice of having a counter, named TOTAL INPUT.
This provides a clear indication of how the job is going.
The helper object parses a string that is either a search request or a search space, returning true if the key was recognized.
In this preamble, if the helper can parse the key, it is just passed forward.
As a general rule, we log per-key data only at level debug, as the logging volume will be very large.
In Listing 9-6, the key was not recognized as a prepared key and is assumed to be an Apache log line.
If the input separator for the TextInputFormat happens to be a single space:
In Listing 9-7, the default case of a raw log line is handled.
This code does make the assumption that the keyValueSeparator computed in Listing 9-6, is correct.
A complete line is assembled in sb, and then parsed.
The IP address is assumed to be the first text in the line and to be terminated by an ASCII space character.
This code accepts only IPv4 addresses in the format of four dot-separated octets.
Once the correct key and new value are produced, they are output.
The use of chain mapping actually reduces the efficiency of the task, but it is nice to have a demonstration.
At this point, all keys are assumed to be valid, and this map verifies that.
Several counters are kept to help with sort and long-term monitoring of the job.
The Combiner The combiner is often one of the more complex pieces of a MapReduce job, and it’s usually given the least thought.
What is the correct behavior for encountering duplicate keys in the map output? For simple aggregation jobs, this is straightforward.
In our case, we have two different types of keys, and what to do for a duplicate in either case is unclear.
The first proposal would be to use a TextArrayWritable, and just keep all of values.
This doesn’t provide much of a space saving, compared to just not running a combiner.
A combiner should provide either a significant reduction in I/O volume or a significant reduction in resource use for the reduce phase.
If a custom comparator were written, a combiner might make sense.
In the type of MapReduce application we are working on here, a combiner that suppresses duplicate key/value pairs could be helpful.
In our constructed example, we know there are no exact duplicates.
The Reducer Each reducer task will need to receive a stream of key values, where the range statements will be first in the sorting order.
This forces the reducer class to maintain state information about which ranges have been seen, and the value of those ranges.
This prior range information is bounded, and ranges may be flushed when the end range value is less than the current input.
As an added bonus, the reduce task is also run as a chain, with a postprocessing map that converts the encoded key formats back into dot-separated octet format.
At this point, any invalid key is an indication that something has gone very wrong—data corruption at some level, given the level of verification performed on the keys in earlier steps.
We keep a queue of networks, ordered by the network endof-address range.
If the current key is a search request and the current key is larger than the end of a network’s address range, the network is removed from the active queue.
If the current key is a search space key, it is added to the set of active ranges, via the following:
At this point, each network in activeRanges is a match.
A network’s end range is guaranteed to be larger than the search request key, and due to our comparator’s ordering of the keys, the network begin range must be less than or equal to our search request.
These are used to construct the actual output key and output value.
The key will be the original log record IP address, followed by the network begin and end addresses.
For ease of parsing, these will be separated by an ASCII tab character.
The value is simply the network name, ASCII tab, and the rest of the original log line.
This example relies on there being only a single reduce task, as the default partitioner will cause this job to fail.
In our next design iteration, we will write a custom partitioner.
The values for input and output are set by the use of the command-line flags --input and --output, respectively.
The setup follows the general rule for using the chain, and allocates dummyConf to use as the private configuration object for the chained map and reduce tasks.
The framework serializes the contents in each call to the ChainMapper methods, making it safe to clear dummyConf and reuse it.
The map and reduce methods used do not modify the passed-in key or value objects; therefore, the chaining framework is being formed to pass keys and values by reference.
All of the mappers and reducers expect Text objects for the input key and value, and output Text.
The Pluses and Minuses of the Brute-Force Design The biggest plus of this design is that it is simple and took about a day to put together.
The biggest disadvantages are that all of the data must pass through the mapper and be sorted, and that only a single reduce task may be used.
Given that the total number of networks is relatively bounded, if the incoming log records are batched in smaller sizes, this job will run reasonably well and reasonably fast.
Without a custom partitioner, this job cannot be made to run with multiple reduce tasks.
Design 2: Custom Partitioner for segmenting the Address space The biggest boost for the brute-force method would be to find a simple way to allow multiple reduce tasks.
The standard partitioner uses the hash value of the key, modulus the number of partitions as the partition number.
A simple strategy for this application might be to simply segment the IP address range.
There is no guarantee that the network ranges will fall cleanly on these segments.
There will need to be a mechanism to split search space keys into segmentappropriate boundaries during the job, while putting the full range in the output record.
Perhaps simply modifying the format for the search space records to allow for an original range to be part of the record will address this.
Note This partitioning method is still subject to uneven distributions of the key space resulting in a subset of reduce tasks running much longer.
To ameliorate this, the key space may be sampled and the partitioning table built using the sample data, in a manner similar to that done by the Hadoop terasort example.
The getPartition() method, shown in Listing 9-12, simply takes the IP address of a search request key or the begin range address of a search space key and returns the partition for that record.
The original design supported a configurable table to ensure that the records were partitioned approximately evenly.
This required a tool to scan the records to generate a distribution map and code to load that map into the partitioner.
During the process of actually writing the code, the decision was made that if that feature is needed, it may be implemented later.
Instead, each partition gets an approximately even number or span of addresses out of the IPv4 space.
A TreeSet is used instead of simply maintaining an array of long values.
The array of long values would be faster and would greatly reduce object churn.
The first step is to initialize the key helper and to determine if the key is actually a valid search space or search request key:
If the key is valid, the IP address of the search request record or the range begin address of the search space record is stored in begin.
Once begin is known, it may be looked up in the table, ranges, that maps addresses to reduce partitions.
The table is actually a TreeMap, and entry keys are the ending IP address of the partition.
This data structure allows the following line to provide the entry of the partition that the key/ value pair must go to:
The TreeMap method higherEntry returns the element in ranges where the entry key is closest to begin, while not being less than begin.
The value of that entry is the partition number for this key/value pair.
For debugging purposes, the entry is assigned to a local variable, partition.
The entry value could simply be returned at this point, but a little checking is done to verify that this key/ value pair is a search space record, where the end of the search space is also an address that will be in this partition.
The ranges table is constructed in the configure() method, shown in Listing 9-13, as this is the first time the number of reduce tasks is known.
The first step is to save a copy of the JobConf object into conf, our standard practice.
Each partition will span approximately rangeSpan addresses, defined as 4294967295L / numPartitions.
The application uses long values to avoid issues with sign extension, as Java does not provide an unsigned integer type.
The variable spanned contains the ending IPv4 address of the previous partition.
Each pass through the for loop adds rangeSpan to spanned defining the ending address of the next partition and increments the partition number:
These are currently added in order, which is not optimal for a TreeMap, as TreeMaps are stored as red-black trees and ordered insertion will result in an unbalanced tree.
Casting our gaze into the future, it seems unlikely that there may be more than small hundreds of reduce tasks and a rewrite might be planned to eliminate the use of TreeMap and simply use an array.
Here, I took a design expedience step that perhaps was not optimal given my later experience.
To achieve this, each search space record must be replicated so that any partition that could have matching requests each gets a copy of.
The concept is that an addition map will, for each incoming search space record, output a set of search space records such that each reduce partition that could receive a matching search request will receive one of the output search space records.
The first portion of Listing 9-14 handles the setup and validation.
The calling convention requires that the caller pass in an initialized key helper (outsideHelper) and the value to output (OutputCollector)
The Reporter object (reporter) is used to log metrics and failures.
The key helper class for these spanned keys has two additional fields: the actual begin and end of the search space request.
The begin and end fields will now be fields for the address span of the partition for which the record is output.
As a quick recap, the search space key contains an IPv4 address range, represented as a beginning and ending address.
To enable multiple reduce tasks, the search space records must be available in each reduce task that could receive search requests that would match the search space record.
This allows the search space requests to be mixed into the job input with the search requests.
Each search space key is split into a set of search space keys, such.
Implicit is that each partition starts with the address after the prior partition and there is no overlap in address space between partitions.
The block of code in Listing 9-15 is the part of the spanSpaceKeys method that produces the per-partition keys.
The variable spannedRanges is a subset of ranges that contains only partitions that have an end address larger or equal to the real begin range of the key, and equal to or less than the real end range of the key.
Put simply, spannedRanges contains the partitions that may contain addresses that would match the passed-in search space record.
The following loop examines each of the candidate partitions in ascending order of the partition end address:
The variable spanEnd contains the ending address for the current partition.
When a per-partition key is to be output, the helper is set up with the correct end address for that partition.
The end address will either be the last address of the partition, spanEnd, or.
If the end address of the output key is less than or equal to the end address of the current partition, no more keys need to be output.
The core loop is run once for each potential partition that this key may need to have a record placed.
The variable count keeps track of the number of records output, and span contains the information about the current partition, in particular the end address and the partition number.
The end of the loop actually builds the Text object with the appropriate data, helper.
Each input search space request now has a record that will be placed by the partitioner into each partition that could have search requests that match.
After working with the data for a short time, I realized that the search space was so small that it wasn’t worth the extra complexity or time to have an additional step for presorting the search space records.
I decided to simply add this mapper as part of the mapper chain, and read the search space records as input with the search request records.
A new record format needs to be designed that can carry the additional data.
The key format for the search space keys has been begin:end, where begin and end are the first and last addresses of the network, each an eight-digit hexadecimal number.
To allow partitioning, the search case keys must match keys in a particular partition.
My first idea on how to address this was to just have four values instead of two, with the same separator between each.
This leaves the old behavior intact, while allowing multiple reduce tasks.
If many types of keys are used, this method will quickly become excessively complex.
In this case, there is only one type of key, so we can defer that code cleanup to a future that may not come.
The output uses the network begin, end, and name values as secondary sort keys.
These also provide an example of how to perform a merge-sort of any reduce task output efficiently using map-side joins.
Unlike a traditional map-side join, where each path item in the input is a table and the matching partXXXXX files of each input path are joined, each individual part-XXXXX file is taken as a table, and all of the part-XXXXX files are joined together.
This causes the map-side join to perform a streaming merge-sort on all of the input data files.
If the input string has a colon (:), it is split and the parts examined:
If there are exactly two parts and the first part is a class name that implements InputFormat, that input format is used for loading the directory name in parts[1]
Basically, the input directory can be preceded by a class name and a colon, and the class will be used as the input format for loading files from that input directory.
This method examines inputPath, constructed from that passed-in path element.
The PathFilter restricts the FileStatus entries returned to those that satisfy the accept() method.
In this case, the only items accepted have file names that match the regular expression ^part-[0-9]+$, our standard reduce output file format.
Rather than try to manage the map-side join table format, the following call builds the table format for the input file:
All of the individual table entries are aggregated in the ArrayList tables.
This by itself will merge-sort all of the input data into a single output file.
The new piece, the specialty sorting of the input records before the map method, is triggered by the following line:
The mapper, shown in Listing 9-22, provides a secondary sort by network for the matched requests.
Each table is checked for a value (value.has(i)) and each table value (Writable outputValue = value.get(i)) accumulated in the values array.
Once any required sorting is completed, the records are output (output.collect( key, values[i] ))
It attempts to operate at the byte level and to minimize object allocation.
The comparator in Listing 9-24 expects input lines of the form:
It will do a primary sort using the first IP address, secondary on the second IP address, and tertiary on the network name.
If at any point there is a parse failure, the element that the parse failed on is considered greater.
The parsing is deferred as long as possible in the hopes that it.
This code tries very hard to work at the byte level and not convert items back into strings.
Listing 9-25 shows the commands used to generate the output.
Most of our later examples accept the arguments -v –deleteOutput, enabling verbose logging and causing the job output directory to be deleted if the directory exists.
The first output directory is range_join, which will be the input directory of the next command.
Design 3: Future Possibilities Two possibilities come to mind for this sample MapReduce job:
An indexed map file of search requests in the reduce task: For each search request key, the configure() method will open the relevant search space map file—either the full map file for the entire search space or a partitioned file—where the partition contains the networks that keys in this reduce task partition could match.
Map-side join of the presorted search requests and a presorted search space: This method requires presorting the search request records and the search space records, and then using the map-side join techniques discussed in Chapter 8 and the classes for working with the IP address described in this chapter.
The partitioned case reduces the data volume that must be scanned.
Even with indexes, the amount of data that needs to be fetched from disk will be smaller in the partitioned case.
The downsides are that search space needs to be repartitioned if the number of reduce tasks for the search requests is changed, and there is additional (though small) code complexity to ensure that the correct search space map file is opened in each search request reduce task.
Both techniques lose the data being local for at least the search space records, and neither seem worth the bother at present, as it is not clear that there would be any performance gain.
They also require the search request records to be sorted, and the search space is expected to be relatively small.
In the process, you have seen a number of design decisions made that become invalid as understanding arrives.
The design and development process was deliberately oriented to provide initial functionality quickly so that this understanding could arrive sooner, rather than after a large and costly development cycle.
A number of the advanced features, such as chaining and map-side joins, were used in the application, and a partitioner and several comparators were written.
The tight coupling between the custom partitioner and the comparator allowed the application to perform range-based matching very efficiently using MapReduce techniques.
The techniques that you have learned will allow you to efficiently and effectively tackle very complex problems that do not appear to fit the MapReduce framework, but in fact are ideally suited for MapReduce.
Particularly in the rapidly evolving environment of today, you will never have time to build the perfect application—just an application that works for yesterday’s goals.
Someone else will come along later and modify the application until it meets the new goals.
Be kind to that person by leaving comments, testing, and keeping it simple.
People use Hadoop to solve many types of problems, and a number of teams have built packages on top of Hadoop Core to address an even larger scope of problems.
This chapter will walk through some of the many tools being built on top of Hadoop and one tool that can be built into Hadoop.
You’ll see a section on changes later in this chapter.
This section will provide an overview of them and, when feasible, show a quick example of how to set up and use them (as well as what problems users might encounter)
I have little to no experience with most of the projects listed in this chapter, so the information in this chapter is gleaned from reading the project or company web site and/or trying the examples from a current release.
The earlier versions of HBase used the Hadoop MapFile as the underlying storage mechanism and managed updates by maintaining overlay MapFiles.
When there were sufficient updates, a merged file was reconstructed, and the overlays were discarded.
To speed access and distribute access, each individual MapFile is responsible for only a specific.
More recent versions of HBase also provide a memcached-based intermediate layer between the user and the MapFiles (http://www.danga.com/memcached/)
Prior to the addition of the memcached layer, HBase suffered terrible performance for random reads and writes, primarily because HDFS is not optimized for low latency random access.
HBase has a number of server processes, a single HBaseMaster that manages the HBase cluster and a set of HRegionServers, each of which is responsible for a set of MapFiles containing column regions.
HBase suffers terribly from the inability of applications to flush file data to storage before the file is closed, and a crash of any portion of the HBase servers or a service interrupting crash of HDFS will result in data loss.
In prior chapters there was a discussion of problems caused by applications or server processes attempting to exceed the system-imposed limit on the number of open files; HBase also has this problem.
The problem is substantially aggravated because each Hadoop MapFile is actually two files and a directory in HDFS, and each HDFS file also has a hidden checksum file.
Setting the per-process open file count very large is a necessity for the HBase servers.
A storage file format, HFile, is under development and due for Hbase version 0.20.0, and is expected to solve many of the performance and reliability issues.
HBase relies utterly on a smoothly performing HDFS for its operation; any stalls or DataNode instability will show up as HBase errors.
HBase servers, particularly the version using memcached, are memory intensive and generally require at least a gigabyte of real memory per server; any paging will drastically affect performance.
Java Virtual Machine (JVM) garbage collection thread stalls are also causing HBase failures.
HBase generally provides downloadable release bundles that track the Hadoop Core distributions.
Hive: The Data Warehouse that Facebook Built Hive provides a rich set of tools in multiple languages to perform SQL-like data analysis on data stored in HDFS.
The wonderful people at Facebook have contributed Hive to the Apache project.
As of the publication of this book, Hive is undergoing active development.
Compiled versions of Hive are part of the contrib subtree of the Hadoop Core distribution.
Cloudera, discussed later in this chapter, provides online training for Hive.
The examples listed in the wiki page http://wiki.apache.org/hadoop/Hive/ GettingStarted did not work particularly well for me (they might be updated by the time you read this chapter)
The language is named Pig Latin, and the Pig project provides a compiler that produces MapReduce jobs from a Pig Latin script.
Pig is not distributed with Hadoop Core, and is mature enough that the project has releases.
At the time of writing, Pig 0.2.0 has been released.
Pig also provides grunt, an interactive shell, for running Pig Latin commands directly.
Cloudera, listed later in this chapter, provides online training for Pig.
The setup is as simple as unpacking the distribution and setting the environment variable PIG_CLASSPATH to the directory that contains the hadoop-site.xml file that defines your cluster.
As of the time of writing, the first release, 0.1, has been made available for download.
The Taste project (a recommendation engine) has become a part of Mahout and is included in the 0.1 release.
Mahout requires Maven for operation, and it is not clear from the documentation how to run the examples, including the Taste examples, without Maven.
The project is intended to be used for large-scale numerical analyses and data mining.
The project will provide matrix-vector and matrix-matrix multiplication, linear equation solving, tools for working with graphs, data sorting, and methods of finding eigenvalues and eigenvectors.
ZooKeeper maintains a shared namespace that looks very similar to a hierarchical file system.
Each of these namespace entries may have data associated with it.
The entry data is accessed atomically, and changes are ordered.
In addition, ZooKeeper provides an ephemeral node, an entry that vanishes when the service holding the entry open disconnects.
The ephemeral nodes are used to establish service masters and sets of backup servers.
Ephemeral nodes are used to support redundant servers with hot failover.
ZooKeeper has been designed to be very reliable and very fast in environments in which data is primarily read.
The examples at http://hadoop.apache.org/zookeeper/docs/current/recipes.html provide ZooKeeper recipes for two-phase commit, leader election, barriers, queues, and locks.
Lucene: The Open Source Search Engine The Lucene project, http://lucene.apache.org/java/docs/, provides the standard open source package used for search engines.
The Lucene core provides the ability to take in documents in a variety of formats and build inverted indexes out of the terms found in the documents.
Lucene also provides a query engine that takes incoming queries, searches the indexes, and returns the documents that match.
The contrib package supports distributed indexes, shards, and unified indexes.
SOLr: a rich Set of Interfaces to Lucene The SOLR project, http://lucene.apache.org/solr/, is a stand-alone, enterprise-grade search service built on top of Lucene.
Katta: a Distributed Lucene Index Server The Katta project, http://katta.sourceforge.net/, describes itself as Lucene in the Cloud, a scalable, fault-tolerant, distributed indexing system capable of serving large replicated Lucene indexes at high loads.
Katta uses ZooKeeper to coordinate among the individual servers of the Katta cloud.
Katta supports storing shards on the local server file system, HDFS, and in Amazon’s S3
Katta also provides a distributed scoring service, allowing for the search results from multiple indexes to be merged together.
The core concept is that of defining a type in a text file and having a tool generate per-language APIs for accessing the data structure and for serializing and deserializing the data structure.
As of Hadoop 0.17.0, the framework supports using any type that provides serialization services as a key or a value.
Cascading: A Map Reduce Framework for Complex Flows Cascading, http://www.cascading.org/, describes itself as a rich API for handling complex scale-free workflows reliably on a MapReduce cluster.
The Cascading package allows the rapid wiring of components together into workflows that support flow control statements.
Cascading’s metaphor is that the incoming data flows through a series of functions and filters that allow the data to be split into multiple streams and then joined together again as needed.
An acyclic-directed graph is built by the framework, out of the functions and filters.
CloudStore: A Distributed File System CloudStore, http://kosmosfs.sourceforge.net/  (formerly known as the Kosmos file system), provides an alternative file system for use within a MapReduce cluster.
The Hypertable site is clear that the project is at a 0.9 release.
Currently, the core servers for Hypertable, the Master server and Hyperspace server, are single points of failure.
Hypertable does not provide ready-to-run distributions and must be built from source.
It provides a download link to allow you to try its software.
It also provides an in-database MapReduce that interoperates with SQL.
CloudBase: Data Warehousing The CloudBase project, http://cloudbase.sourceforge.net/, provides a high-performance, data warehousing system built on top of MapReduce, with an ANSI SQL API.
The project is developed by business.com to speed terabyte scale web log analysis.
The web site provides detailed instructions for running CloudBase instances on Amazon’s elastic compute (EC2) service.
Hadoop in the Cloud Sometimes you need additional compute resources for only a short time, you want to experiment with particular configurations, or you just don’t want to manage your own hardware.
Cloud service vendors provide the ability to spin up clusters of almost arbitrary size and capacities for short to long durations.
The best-known cloud server provider at the time of writing is Amazon, and there is direct support for running Hadoop in its cloud.
Amazon Amazon, http://aws.amazon.com, provides a large set of cloud computing services:
The one significant downside to Hadoop in the Amazon cloud is that there is no real data locality―something Hadoop works hard to achieve.
Caution Anything stored on an EC2 machine instance vanishes when the instance is shut down.
At the time of writing, the base was Hadoop 0.18.3, with important fixes and features back ported from later versions, including unreleased versions.
This is an ideal distribution for production use because it provides minimal API changes while providing bug fixes and some new features.
It provides free online basic Hadoop training at http://www.cloudera.com/hadoop-training-basic, Hive training at http://www.cloudera.com/hadoop-training-hive-introduction, and Pig training at http://www.cloudera.com/hadoop-training-pig-introduction/
There is also a session on using Eclipse with Hadoop at http://www.cloudera.com/blog/2009/04/20/configuringeclipse-for-hadoop-development-a-screencast/
Supported Distribution Cloudera provides a freely downloadable version of its distribution at http://www.cloudera.
The virtual machine has an Eclipse installation set up for use with its Hadoop distribution.
Note I used the Cloudera training virtual machine to work up some of the examples in this book.
The EC2 image has Hive and Pig installed and ready to use.
The principals are the Cascading project lead and the Katta project lead.
Our consultants’ experience does not end with Map Reduce patterns and Hadoop Distributed File System deployment models; but also spans over a wide set of related open.
Scale Unlimited also sponsors a live CD image of a Solaris installation with a three-node Hadoop cluster in zones (http://opensolaris.org/os/project/livehadoop/)
Note A live disk is a CD or DVD that boots as a running instance, not requiring any changes to the local machine’s hard disk.
An image is an .img file that most CD/DVD burner applications can burn directly to writable media.
At the time of writing, it is becoming clear that it is not ready for production use.
This section hopes to whet your appetite for these new features and help you plan for their arrival.
Vaidya: A Rule-Based Performance Diagnostic Tool for MapReduce Jobs Vaidya processes the log file data of previously run jobs and provides suggestions on how to improve performance.
Service Level Authorization (SLA) The SLA package provides the access control lists for the control APIs of the various Hadoop Core servers, providing some assurance that any client connecting to a server with SLA enabled is an authorized client.
There are plans to bring in another LZO-like codec with a license the Apache Foundations will accept.
New MapReduce Context APIs and Deprecation of the Old Parameter Passing APIs The core of this change is that a Mapper or a Reducer Context object is passed to the Mapper and Reducer classes, in place of the JobConf, to configure(), and the Reporter and OutputCollector to map() and reduce()
The Mapper and Reducer classes now have a setup(), cleanup(), and run() method in place of the configure() and close()methods.
To run it, change to a directory that will be used as the virtual cluster local storage, and run the following:
I find this particularly handy for debugging jobs when I am on the road because the HDFS data persists after the debugger has exited, and I can examine the job status via the web GUIs.
The only problem I have is that the per-task log files are not available via the web GUI, and the HDFS files are not available via the web GUI because of issues inside the Hadoop-supplied MiniMRCluster code.
The following command lists the files in the virtual HDFS:
This came into being when I was trying to work on the unit tests while on the road, using a machine with Windows XP as the host operating system.
The virtual clusters would periodically not start, and I became very frustrated.
I wrote this and after it started, it stayed running, and I could use it reliably for multiple tests.
The ability to examine the data files in HDFS and to interact with the web interfaces was a pleasant discovery.
Eclipse Project for the Example Code The example code was developed in Eclipse 3.4, and the project and class path files are part of the download, enabling you to load up, experiment with, and run the example code.
Many people and organizations are leveraging the power of Hadoop MapReduce and providing domain-specific package tools.
Distributed column-oriented databases are the current mantra of the scalable web services community; and HBase and Hypertable provide them.
Data mining, extracting, transforming, and loading without having to write custom MapReduce jobs are provided with Hive and Pig.
Machine learning and recognition are provided by Mahout and Hama, and distributed search is provided by the Katta project.
I am partial to the Cloudera Hadoop distribution because it has good support, back ported fixes, training, is free, and is responsive to community needs.
Try the various packages discussed in this chapter―explore and enjoy.
Everything in a job is controlled via the JobConf object; it is the center of the universe for a MapReduce job.
The framework will take the JobConf object and render it to XML; then all the tasks will load that XML when they start.
This section will cover all the relevant methods (as of Hadoop Core 0.19.0) and provide some basic usage examples.
Because the JobConf object is the primary interface between the programmer and the framework, I’ll detail all methods available to the user of a JobConf without distinguishing which methods come from the Configuration base class.
I suggest that you create and use only JobConf objects.
Configuration values that are loaded as resources are stored separately from the values that are set via setter calls.
When looking for a value, a value set by a setter call takes precedence over a value loaded from a resource.
Each configuration item is a name and value pair with an optional final parameter.
These parameters tell the Hadoop framework code how to contact the cluster, are defaults for various attributes, and allow for passing arbitrary values to the tasks.
Other parameters are found only by reading the source code.
You can set arbitrary names for value pairs in the configuration, and these name-value pairs are made available to MapReduce tasks.
Values that are objects are serialized and then deserialized by each MapReduce task when tasks start.
The parameters that configure the distributed file system start with dfs, and the parameters that configure the MapReduce framework start with mapred.
How configuration data is loaded into the JobConf object and resolved.
In the job driver, the JobConf object is constructed with all the parameters for the job.
At job runtime, required data, the JobConf object, JAR files, archives, and other resources are stored in the Hadoop Distributed File System (HDFS) in a job-specific directory.
Any items that must be referenced from the local file system, such as the job JAR file or other items passed via the DistributedCache, are unpacked into these local directories and the path references to items in the configuration are adjusted to be the task local path.
The classpath for the JVM that the task will run in is also set up for the task to include the location on the local file system that the classpath resources were unpacked into.
JobConf Is a Properties Table The JobConf instances maintain a table of key/value pairs for all the configuration parameters.
The values are all stored as String objects and are serialized if they are objects.
At the lowest level, operations get a value for a key or store a value for a key.
The first key examined is no.expansion; in Listing A-1, the value is defined as no.expansion Value, which is the result printed.
Constructors All code that creates and launches a MapReduce job into a Hadoop cluster creates a JobConf object.
Tip The task JVMs are run on different physical machines and do not have access to the classpath or the classpath items of the JVM that submits the job.
It is very handy when unit testing because as the unit test can construct a standard JobConf object, and each individual test can use it as a reference and change specific values.
If your driver launches multiple MapReduce jobs, each job should have its own JobConf object, and the pattern described previously for unit tests is ideal to support this.
Classes that launch jobs that may have unit tests or be called as part of a sequence of Hadoop jobs should provide a run method that accepts a Configuration object and calls this constructor to make the JobConf object for that class’s job.
This way, the unit test or calling code can preconfigure the configuration, and this class can customize its specific variables and launch the job.
Methods for Loading Additional Configuration Resources The methods described in this section load an XML configuration file resource and store it in the JobConf parameter set.
The order in which these methods are called is important because the contents specified by the most recent call will override values supplied earlier.
If a specified resource cannot be loaded or parsed as valid configuration XML, a RuntimeException will be thrown unless quiet mode is enabled via a call to setQuietMode (true)
Each call to one of these methods results in the complete destruction of the configuration data that resulted from the loading and merging of the XML resources.
There are no changes made to the configuration parameters that have been created via the set methods.
The entire set of XML resources is reparsed and merged on the next method call that reads or sets a configuration parameter.
In this case, resource objects loaded later may not change the value of the parameter.
If quietmode is false, a log message will be generated for each resource loaded.
If a resource cannot be parsed, a RuntimeException will be thrown.
It is not uncommon for these files to accidentally be bundled into a JAR file and end up overriding the cluster-specific configuration data in the conf directory.
This method actually just clears the existing configuration, and the reload will happen on the next get or set.
Basic Getters and Setters The methods in this section get and set basic types:
There is no mechanism currently to escape a comma that must be a part of an individual item in a list.
Under the covers, all data is stored as a java String object.
All items stored are serialized into a String object, and all values retrieved are deserialized from a String object.
The user is required to convert objects into String representations to store arbitrary objects in the configuration and is responsible for re-creating the object from the stored String when retrieving the object.
The configuration supports storing basic types such as various numbers, boolean values, text, and class names.
It is two integer values, in which the second integer is larger than the first.
An individual range is specified by a String containing a -, a dash character that can also have a leading and trailing integer.
Multiple ranges may be separated by ,: a comma character such as 1–5,7–9,13–50
As of Hadoop 0.19.0 it is used only to determine which MapReduce tasks to profile.
As of Hadoop 0.19.0 there is no corresponding set method, and the base set( String name, String value) is used to set a range The value has to be the valid String representation of a range or later calls to the getRange method for name will result in an exception being thrown.
The defaultValue must be passed in as a valid range.
This method looks up the value of name in the configuration, and if there is no value, the defaultValue will be used.
The resulting value will then be parsed as an IntegerRanges object and that result returned.
This method gets the value associated with name in the configuration, splits the String on commas, and returns the resulting array (see Listing A-7)
As of Java 1.5, variable argument lists are supported for method calls.
The declaration of the last parameter may have an ellipsis between the type and the name, type...name.
The caller can place an arbitrary number of objects of type in the method call, and the member method will receive an array of type with the elements from the caller’s call.
This method will get the value associated with name in the configuration and split the String on commas and return the resulting array (see Listing A-8)
If there is no value stored in the configuration for name, the array built from the defaultValue parameters will be returned.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
Getters for Localized and Load Balanced Paths The framework provides the capability for multiple local directories to be specified for task temporary files.
Multiple locations are allowed to load balance the I/O over multiple devices.
The framework will attempt to select one location set either at random or in sequential order.
The ordering used will be given in the method description.
Each of the methods described in this section is called with a trailing path component that includes a final file name.
The return value will be the full path to the file; all the intermediate directory components will be constructed if needed.
The first found complete path that can be created or exists will be returned by the method.
The intermediate directories may be constructed in locations that do not allow the complete construction of the path.
If so, those intermediate directories that have been created will not be removed.
If no path can be constructed, an IOException will be thrown.
These throwing IOException paths are explicitly constructed on the local (host) file system.
The pseudorandom method does not guarantee that all possible path candidates will be tried; only that no more than the count of path candidate elements will be tried (as of Hadoop 0.19.0)
Also as of Hadoop 0.19.0, the method does not fail if the path candidate is a file, not a directory.
This method uses the java.io.File methods to create directory paths and test for directory existence.
The paths defined by dirsProp are searched in a pseudo-random order.
The TaskTracker uses a round robin strategy to allocate a task directory for a new task.
This parameter is generally only used directly by the framework.
This is used by the framework when setting up the per-job task environment on a TaskTracker node.
Tasks can use this method to find the path to the task-specific directory on the local file system, which may be used for temporary file storage.
Methods for Accessing Classpath Resources The framework provides a way for tasks to access resources from the task-specific classpath objects.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
Caution This method does not look up the value of name in the configuration; name is the value passed to the class loader.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
Methods for Controlling the Task Classpath These methods ensure that the objects referenced are distributed to the task nodes and made available in the classpath of the tasks.
The mapred.jar key’s value is set by the setJar(String jar) method, the setJarByClass (Class cls) method, and the JobConf constructors that take a Class value as a parameter.
If the mapred.jar key has been set in the configuration, the value will be returned.
Methods for Controlling the Task Execution Environment These methods control the setup and cleanup of the individual task environment.
Caution Usernames may be overwritten with a different username by any user.
This is not a security feature, and Hadoop permissions are not a security feature.
Through at least Hadoop 0.19 any user may claim to be any other Hadoop user and act fully as if they are that user, including the removal of files or the scheduling of jobs.
There is way to prevent this; you have to trust the users who have access to your cluster because any user can override any Hadoop level permission restrictions placed on that user.
Prior to Hadoop 0.19.0, a new JVM was created for each task run by the TaskTracker.
As of Hadoop 0.19.0, the TaskTracker has the capability to reuse the task JVM for additional tasks.
Methods for Controlling the Input and Output of the Job The methods described in this section are used to configure how the jobs’ input and output will be handled.
This includes how the input is parsed and presented to the framework, the compression of intermediate and final output, and how the output is written.
The returned class will be used by the framework to read the input data set for the job.
The key/value pairs that the class extracts from the input will be passed to the map method of the mapper class in the map tasks.
There will be one instance created per map task, and that instance will receive the input split for that map task as input.
The returned class will be used by the framework to write each of the key/value pairs output by the reduce() method of the reducer class, and if not explicitly configured, each of the key/value pairs output by the map method of the mapper class.
There will be one instance of this class created for each reduce task.
By default, one instance of this class is created for each map task.
As of 0.19.0, the OutputCommitter object is used to process the files in the per-task temporary area on successful task completion, and is responsible for deciding which output files.
Prior to this, any files present were moved to the userspecified output path.
This method retrieves the value stored in the configuration under the key mapred.
If the retrieved value is not null, the method will attempt to instantiate a class using the value as the class name.
If the class name cannot be instantiated or if the instantiated class is not derived from the class OutputCommitter, a RuntimeException will be thrown.
During a Hadoop job that has a reduce phase, the map phase produces intermediate output that will be further processed by the framework.
This output will eventually become the input to the reduce phase.
This output may be compressed to reduce transitory disk space requirements and network transfer requirements.
Having the map output compressed can save substantial time because the amount of data that must traverse the network between the map and the reduce phase may be substantially reduced.
Having the job output compressed may also save substantial time because the amount of data to be stored in HDFS may be substantially reduced, greatly reducing the amount of network traffic for the replicas.
The Hadoop framework is responsible for loading the job input and converting that input into key/value pairs that are passed to the map method of the mapper, passing the key/value pairs output by the map method of the mapper to the reduce() method of the reducer, and taking the key/value pairs output by the reduce() method of the reducer and writing them to the job output.
The class that loads and transforms the input into key/value pairs is derived from InputFormat and requires that the type of the key and the type of the value be specified.
The class that handles loading the input is responsible for producing keys and values of the correct type.
A commonly used class is the KeyValueTextInput class, which parses the input as text files, with each record on a single line and the key and value separated by the first tab character.
The types of these objects will be used to define the mapper class input key and value types.
The class to receive and transform the output key/value pairs is derived from OutputFormat.
The default value for the key class is LongWritable, and the default value for the value class is org.
By default, the expected map output types are the same as the expected reduce input and output types.
The job may specify that the map output key type and or the map output value type is different from the job output key and value type.
The class specified under the key map.sort.class in the configuration will be used to sort the key objects if a reduce has been requested by the job.
Methods for Controlling Output Partitioning and Sorting for the Reduce The partitioner determines which key/value pair is sent to which reduce task.
A class used as a key object in Hadoop may define an optimized comparator class.
This avoids having to deserialize the Text object and then run String comparisons on the data in the reconstituted objects.
The keySpec String is composed of one or more space-separated groups.
There is a test class for these key fields in the examples that can be run by giving it three arguments: the key, the key spec for the combiner, and the key spec for the partitioner.
When a job is configured to have a reduce phase, the output will be split into partitions (one partition per reduce task)
The framework has a default partitioning strategy of using the hash code of the key, modulus the number of partitions, key.hashCode() % conf.
The user is free to specify a custom partitioning class.
The portion of the key selected will be hashed, and that hash modulus the number of reduces will be the partition number.
The keySpec String is composed of one or more space-separated groups.
It is often the case that there is a requirement for grouping output data.
Hadoop Core provides a way to group output values that acts very much like a secondary sort on the key data.
For this to work in the manner that the user expects, the output partitioner, the output comparator, and the output grouping comparator have to cooperate.
Because keys that must group together may not be equal in this method, the outputPartitioner has to be able to place keys that must group together into the same partition.
This will result in a call to the Reduce.reducer method for each group of keys.
The use of this method enables a grouping operator on keys and a secondary sort.
The user must set both a partitioner and a comparator that cooperate for this to be used.
It is common for the default output comparator to be used to force complete sorting of the keys output.
The output comparator must compare keys so all keys that are to be grouped together are adjacent in the sort.
The partitioner must ensure that all keys that are to be grouped together are sent to the same partition.
The Reducer.reduce() method will receive the first key in the group, and the values will be the values from all adjacent keys that the output value grouping comparator considers equal.
If keys are of the form item rank and the values are of the form data, the partitioner must use only item to partition.
The standard output comparator will sort lexically on item rank.
The output value grouping operator will use only item for comparing keys.
The Reducer.reduce() method will receive all keys that share item, and the values will be lexically sorted by rank.
The output comparator would fully sort the keys by item rank.
The output value grouping comparator would use only item for comparing keys.
Methods that Control Map and Reduce Tasks These methods actually specify the class that provides the Mapper.map() and Reducer.reduce() methods.
They specify if the map methods may be run from multiple threads or in a single thread.
They specify if the framework will attempt to run multiple instances of a task to see if one will run faster, and when to consider a task completely failed and a job completely failed.
The framework creates an instance of the mapper class in each map task.
By default, a single-threaded map runner is used, and the key/value pairs are passed to the Mapper.map() method serially.
The user may inform the framework that multiple threads are to run the Mapper.map() method.
There will be multiple simultaneous calls to the map() method of the single instance of the Mapper class, running in the JVM that hosts the map task.
The input of key/value pairs are treated as a queue, being serviced by a thread pool, which invokes the Mapper.map() method on each pair pulled from the queue.
The returned class will provide the map method that all the input data will be passed through.
The multithreaded map runner is very handy when the map method is not blocked waiting on local CPU or IO, such as when the map method is used to fetch URLs.
One instance of this class will be created in each reduce task.
Each unique key will be passed to one instance of the Reducer.reduce() method of theClass, with all the values that share that key.
A combiner class is a minireducer that is run in the context of the map task to pregroup key/value pairs that share a key.
Combiners can greatly minimize the amount of output that has to pass between the map and reduce tasks and speed up the job.
The class used for combining must implement the Reducer interface, and the class’s reduce() method will be called to combine map output values that share a key.
If the job’s reducer class is being used as a combiner, reduce() must not have side effects because there is no constraint on the number of times the reduce() method will be called in as a map output combiner.
In particular, if the same class is used for combing and reducing, unless care is taken to change the counter names, the counts displayed at job end will be the sum of the combiner and reducer counts.
In this environment, the amount of wall clock time for any given machine to execute a map or reduce task could vary widely because of differing machine capabilities.
In addition, there is no guarantee that any given InputSplit will take the same amount of wall clock time to execute.
Speculative execution informs the cluster that any unused task slots may be used to run duplicate instances of an already running task.
The first of these duplicates to complete has its results used, and the other task has its output discarded.
If your tasks do not have side effects that Hadoop cannot undo, do not consume resources with some real costs or load your machines so that other tasks run slower.
Ensure that speculative execution is disabled if your tasks have output that Hadoop cannot discard or side effects that Hadoop cannot undo.
Unlike the number of map tasks, this is exactly the number of reduce tasks that will be run.
The framework will attempt to reschedule reduce tasks that fail up to this value times before the job is considered failed.
The framework will attempt to reschedule reduce tasks that fail up to this value times before the job is considered failed.
If this value is not zero, a job may succeed if less than this value as a percentage of the map tasks cannot be successfully completed.
If this value is not zero, a job may succeed if less than this value as a percentage of the reduce tasks cannot be completed successfully.
Methods Providing Control Over Job Execution and naming These methods provide a way to specify a job name and a session identifier as well as to specify a priority for a job.
The naming is also helpful for distinguishing jobs in the reporting frameworks.
They also provide a way to enable profiling of specific tasks and of running a debugging script on failed tasks.
This is the name that the job will be identified by to the user.
Hadoop On Demand (HOD) is a package that provides virtual map/red clusters on top of a larger HDFS installation.
The use of HOD requires an understanding of torque: http://www.clusterresources.com/pages/products/torque-resource-manager.php.
The author and the team the author was working with found it too complex for the benefits provided and discontinued using it.
The author recommends avoiding HOD unless there is a local torque expert to handle the torque installation and day-to-day operation.
A job with a higher priority has first right of refusal for any map or reduce task slot available on the cluster.
If jobs have equal priority, the first requester gets the open task slots.
Caution Queuing multiple jobs into a cluster with this mechanism can result in a cluster deadlock in which no job can complete.
Hadoop 0.19 also provides a queuing mechanism that provides rich control over how task slots are allocated between multiple competing jobs.
Jobs with a higher priority have first choice of available task slots when executing in an environment in which multiple jobs are queued into a cluster.
Profiling is performed on both map tasks and reduce tasks if enabled.
It is harder to absolutely know the number of map tasks, but the same technique applies.
This String is passed to the JVM to control how the profiling is performed for the task to be profiled.
At runtime, for a profiled task a single %s will be substituted in the value with the name of the task-specific profile.out file.
No checking is performed to ensure that the individual ranges in a comma-separated set do not overlap and ordering is not required.
A linear search through the list in the order supplied is performed for each task when profiling is enabled.
No checking is performed to ensure that the individual ranges in a comma-separated set do not overlap and ordering is not required.
A linear search through the list in the order supplied is performed for each task, when profiling is enabled.
The value is the script and script arguments to be used to debug failed tasks.
The value will be split into tokens using the space character as a separator.
All the tokens are passed to the shell to be executed as a command.
The input of the command will be connected to /dev/null, and the standard and error output collected in a single stream.
The script is run with the current working directory as the task local directory.
If the script is not resident on all the TaskTracker nodes and normally executable, it must be distributed via the DistributedCache and symlinked.
The script will be invoked in the task local directory via the following shell command:
The value specified is the number of lines from the tail of the file to keep.
This information is made available via the JobTracker web interface in the task detail output.
Hadoop provides a mechanism to control the limit of virtual memory that an individual task and the task’s children use.
When the virtual memory consumption of a task and its children exceed this value, the task is killed by the framework, and marked as failed.
This is predicated on the system reporting virtual memory usage for processes in kilobytes.
The default value for this key is 536,870,912 kilobytes (roughly one-half terabyte)
Convenience Methods These methods provide convenience functions for accessing the configuration data.
The destination for this output data can be used as input to the addResource() method.
This method is used by the framework to serialize the job configuration and store it in HDFS so that the individual tasks load the job configuration at task start.
This is primarily used by the framework when preparing map and reduce tasks to include the DistributedCache classpath items in the classpath.
Methods Used to Pass Configurations Through SequenceFiles The configuration class implements the Writable interface, which allows the framework to serialize and deserialize the configuration.
It is not clear that these methods are used by the framework at the current time.
See also Eclipse framework; MapReduce framework; monitoring framework for large clusters; Spring Framework, initializing mapper with.
Spring task initialization method, 146–147 streaming command to invoke LongSum.
Tasktracker error log message due to TCP port unavailability, 93–94
Sun VM Invocation Options guide, 230 support for multihomed machines, sample.
No part may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage or retrieval system, without the prior written permission of the copyright owner and the publisher.
The purchaser may print the work in full or in part for their own noncommercial use.
The purchaser may place the eBook title on any of their personal computers for their own personal reading and reference.
You Need the Companion eBook Your purchase of this book entitles you to buy the.
Once you purchase your book, getting the $10 companion eBook is simple:
