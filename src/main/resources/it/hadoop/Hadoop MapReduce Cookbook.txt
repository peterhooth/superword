Recipes for analyzing large and complex datasets with Hadoop MapReduce.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the authors, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Srinath is also a committer of Apache open source projects Axis, Axis2, and Geronimo.
Srinath has authored many technical and peer reviewed research articles, and more detail can be found from his website.
He has worked with large-scale distributed systems for a long time.
He closely works with Big Data technologies, such as Hadoop and Cassandra daily.
He also teaches a parallel programming graduate class at University of Moratuwa, which is primarily based on Hadoop.
I would like to thank my wife Miyuru and my parents, whose never-ending support keeps me going.
I also like to thanks Sanjiva from WSO2 who encourage us to make our mark even though project like these are not in the job description.
Finally I would like to thank my colleges at WSO2 for ideas and companionship that have shaped the book in many ways.
He has extensive experience in using Apache Hadoop and related technologies for large-scale data intensive computations.
His current work focuses on developing technologies to perform scalable and efficient large-scale data intensive computations on cloud environments.
Thilina has published many articles and peer reviewed research papers in the areas of distributed and parallel computing, including several papers on extending MapReduce model to perform efficient data mining and data analytics computations on clouds.
Thilina is a regular presenter in both academic as well as industry settings.
Before starting the graduate studies, Thilina worked as a Senior Software Engineer at WSO2 Inc., focusing on open source middleware development.
This book would not have been a success without the direct and indirect help from many people.
Thanks to my wife and my son for putting up with me for all the missing family times and for providing me with love and encouragement throughout the writing period.
Thanks to my parents, without whose love, guidance and encouragement, I would not be where I am today.
Geoffrey Fox for his excellent guidance and providing me with the environment to work on Hadoop and related technologies.
Thanks to Apache Software Foundation for fostering vibrant open source communities.
Thanks to the editorial staff at Packt, for providing me the opportunity to write this book and for providing feedback and guidance throughout the process.
Thanks to the reviewers for reviewing this book, catching my mistakes, and for the many useful suggestions.
Thanks to all of my past and present mentors and teachers, including Dr.
Beth Plale, all my professors at Indiana University and University of Moratuwa for all the knowledge and guidance they gave me.
Thanks to all my past and present colleagues for many insightful discussions and the knowledge they shared with me.
He has more than seven years' experience in software and middleware (Apache, Tomcat, PostgreSQL, and Hadoop eco system) engineering.
It provides professional services from consulting, and system development to business IT outsourcing.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Why Subscribe? f Fully searchable across every book published by Packt.
Preface Hadoop MapReduce Cookbook helps readers learn to process large and complex datasets.
The book starts in a simple manner, but still provides in-depth knowledge of Hadoop.
It is a simple one-stop guide on how to get things done.
It has 90 recipes, presented in a simple and straightforward manner, with step-by-step instructions and real world examples.
What this book covers Chapter 1, Getting Hadoop Up and Running in a Cluster, explains how to install and run Hadoop both as a single node as well as a cluster.
Chapter 2, Advanced HDFS, introduces a set of advanced HDFS operations that would be useful when performing large-scale data processing with Hadoop MapReduce as well as with non-MapReduce use cases.
Chapter 3, Advanced Hadoop MapReduce Administration, explains how to change configurations and security of a Hadoop installation and how to debug.
Chapter 4, Developing Complex Hadoop MapReduce Applications, introduces you to several advanced Hadoop MapReduce features that will help you to develop highly customized, efficient MapReduce applications.
Chapter 6, Analytics, explains how to calculate basic analytics using Hadoop.
Chapter 7, Searching and Indexing, introduces you to several tools and techniques that you can use with Apache Hadoop to perform large-scale searching and indexing.
Chapter 8, Classifications, Recommendations, and Finding Relationships, explains how to implement complex algorithms such as classifications, recommendations, and finding relationships using Hadoop.
Chapter 9, Mass Text Data Processing, explains how to use Hadoop and Mahout to process large text datasets, and how to perform data preprocessing and loading operations using Hadoop.
What you need for this book All you need is access to a computer running Linux Operating system, and Internet.
Who this book is for For big data enthusiasts and would be Hadoop programmers.
The books for Java programmers who either have not worked with Hadoop at all, or who knows Hadoop and MapReduce but want to try out things and get into details.
It is also a one-stop reference for most of your Hadoop tasks.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: "From this point onward, we shall call the unpacked Hadoop directory HADOOP_HOME."
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "Create a S3 bucket to upload the input data by clicking on Create Bucket"
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.PacktPub.com.
If you purchased this book elsewhere, you can visit http://www.PacktPub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Introduction For many years, users who want to store and analyze data would store the data in a database and process it via SQL queries.
The Web has changed most of the assumptions of this era.
On the Web, the data is unstructured and large, and the databases can neither capture the data into a schema nor scale it to store and process it.
Google was one of the first organizations to face the problem, where they wanted to download the whole of the Internet and index it to support search queries.
They built a framework for large-scale data processing borrowing from the "map" and "reduce" functions of the functional programming paradigm.
Hadoop is the most widely known and widely used implementation of the MapReduce paradigm.
This chapter introduces Hadoop, describes how to install Hadoop, and shows you how to run your first MapReduce job with Hadoop.
Hadoop installation consists of four types of nodes—a NameNode, DataNodes, a JobTracker, and TaskTracker HDFS nodes (NameNode and DataNodes) provide a distributed filesystem where the JobTracker manages the jobs and TaskTrackers run tasks that perform parts of the job.
Users submit MapReduce jobs to the JobTracker, which runs each of the Map and Reduce parts of the initial job in TaskTrackers, collects results, and finally emits the results.
We will discuss the local mode in the first three recipes, and Pseudo distributed and distributed modes in the last three recipes.
Setting up Hadoop on your machine This recipe describes how to run Hadoop in the local mode.
Download the most recent Hadoop 1.0 branch distribution from http://hadoop.apache.org/
You will have to change the x.x in the filename with the actual release you have downloaded.
If you are using Windows, you should use your favorite archive program such as WinZip or WinRAR for extracting the distribution.
From this point onward, we shall call the unpacked Hadoop directory HADOOP_HOME.
You can use Hadoop local mode after unzipping the distribution.
Now, you can run Hadoop jobs through bin/hadoop command, and we will elaborate that further in the next recipe.
Hadoop local mode does not start any servers but does all the work within the same JVM.
When you submit a job to Hadoop in the local mode, that job starts a JVM to run the job, and that JVM carries out the job.
The output and the behavior of the job is the same as a distributed Hadoop job, except for the fact that the job can only use the current node for running tasks.
In the next recipe, we will discover how to run a MapReduce program using the unzipped Hadoop distribution.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.
Writing a WordCount MapReduce sample, bundling it, and running it using standalone Hadoop.
This recipe explains how to write a simple MapReduce program and how to execute it.
To run a MapReduce job, users should furnish a map function, a reduce function, input data, and an output data location.
Hadoop breaks the input data into multiple data items by new lines and runs the map function once for each data item, giving the item as the input for the function.
When executed, the map function outputs one or more key-value pairs.
Hadoop collects all the key-value pairs generated from the map function, sorts them by the key, and groups together the values with the same key.
For each distinct key, Hadoop runs the reduce function once while passing the key and list of values for that key as input.
The reduce function may output one or more key-value pairs, and Hadoop writes them to a file as the final result.
The WordCount sample uses MapReduce to count the number of word occurrences within a set of input documents.
The code has three parts—mapper, reducer, and the main program.
Ant, from the root directory of the sample code: >ant build.
If you have not done this already, you should install Apache Ant by following the instructions given at http://ant.apache.org/manual/install.html.
Alternatively, you can use the compiled JAR file included with the source code.
To be used as the input, create a directory called input under HADOOP_HOME and copy the README.txt file to the directory.
Alternatively, you can copy any text file to the input directory.
The output directory will have a file named like part-r-XXXXX, which will have the count of each word in the document.
In the preceding sample, MapReduce worked in the local mode without starting any servers and using the local filesystem as the storage system for inputs, outputs, and working data.
The following diagram shows what happened in the WordCount program under the covers:
Hadoop reads the input, breaks it by new line characters as the separator and then runs the map function passing each line as an argument.
The map function tokenizes the line, and for each token (word), emits a key value pair (word, 1)
Hadoop collects all the (word,1) pairs, sorts them by the word, groups all the values emitted against each unique key, and invokes the reduce once for each unique key passing the key and values for that key as an argument.
The reduce function counts the number of occurrences of each word using the values and emits it as a key-value pair.
As an optional step, copy the input directory to the top level of the IDE-based project (Eclipse project) that you created for samples.
Now you can run the WordCount class directly from your IDE passing input output as arguments.
Running MapReduce jobs from IDE in this manner is very useful for debugging your MapReduce jobs.
Although you ran the sample with Hadoop installed in your local machine, you can run it using distributed Hadoop cluster setup with a HDFS-distributed filesystem.
The recipes of this chapter, Setting up HDFS and Setting Hadoop in a distributed cluster environment will discuss how to run this sample in a distributed setup.
After running the map function, if there are many key-value pairs with the same key, Hadoop has to move all those values to the reduce function.
To optimize such scenarios, Hadoop supports a special function called combiner.
If provided, Hadoop will call the combiner from the same node as the map node before invoking the reducer and after running the mapper.
This can significantly reduce the amount of data transferred to the reduce step.
This recipe explains how to use the combiner with the WordCount sample introduced in the previous recipe.
Now let us run the MapReduce job adding the combiner:
Combiner must have the same interface as the reduce function.
For the WordCount sample, we will reuse the reduce function as the combiner.
Make sure to delete the old output directory before running the job.
To activate a combiner, users should provide a mapper, a reducer, and a combiner as input to the MapReduce job.
In that setting, Hadoop executes the combiner in the same node as the mapper function just after running the mapper.
With this method, the combiner can pre-process the data generated by the mapper before sending it to the reducer, thus reducing the amount of data that is getting transferred.
For example, with the WordCount, combiner receives (word,1) pairs from the map step as input and outputs a single (word, N) pair.
However, the combiner only works with commutative and associative functions.
For example, the same idea does not work when calculating mean.
As mean is not communicative and associative, a combiner in that case will yield a wrong result.
Although in the sample we reused the reduce function implementation as the combiner function, you may write your own combiner function just like we did for the map and reduce functions in the previous recipe.
However, the signature of the combiner function must be identical to that of the reduce function.
In a local setup, using a combiner will not yield significant gains.
However, in the distributed setups as described in Setting Hadoop in a distributed cluster environment recipe, combiner can give significant gains.
Setting up HDFS HDFS is the distributed filesystem that is available with Hadoop.
For the HDFS setup, we need to configure NameNodes and DataNodes, and then specify the DataNodes in the slaves file.
When we start the NameNode, startup script will start the DataNodes.
You may follow this recipe either using a single machine or multiple machines.
If you are using multiple machines, you should choose one machine as the master node where you will run the HDFS NameNode.
If you are using a single machine, use it as both the NameNode as well as the DataNode.
Install Java in all machines that will be used to set up the HDFS cluster.
If you are using Windows machines, install Cygwin and SSH server in each machine.
Now let us set up HDFS in the distributed mode.
Check that you can login to the localhost and all other nodes using SSH without a passphrase by running one of the following commands:
Then you can log in with the following command: >ssh localhost.
This command creates an SSH key pair in the .ssh/directory of the home directory, and registers the generated public key with SSH as a trusted key.
In the NameNode, change directory to the unzipped HADOOP_HOME directory.
When we start the NameNode, it will use the slaves file to start the DataNodes.
Inside each node's HADOOP_HOME/conf directory, add the following code to the core-site.xml and hdfs-site.xml files.
From the NameNode, run the following command to format a new filesystem: >bin/hadoop namenode –format.
It will then look at the HADOOP_HOME/ conf/slaves file and start the DataNodes.
It will print a message like the following to the console.
In this design, the NameNode holds the information of all the files and where the data blocks for each file are located.
The NameNode is a single point of failure, and on failure it will stop all the operations of the HDFS cluster.
To avoid this, Hadoop supports a secondary NameNode that will hold a copy of all data in NameNode.
If the NameNode fails, the secondary NameNode takes its place.
Here, replace MASTER_NODE with the IP address of the master node running the HDFS NameNode.
Finally, shut down the HDFS cluster using the following command: >bin/stop-dfs.sh.
In the HDFS basic command line file operations recipe, we will explore how to use HDFS to store and manage files.
The Setting Hadoop in a distributed cluster environment recipe describes how to set up the rest of the Hadoop.
Using HDFS monitoring UI HDFS comes with a monitoring web console to verify the installation and monitor the HDFS cluster.
It also lets users explore the content of the HDFS filesystem.
In this recipe, we will look at how we can access the HDFS monitoring UI and verify the installation.
Start the HDFS cluster as described in the previous recipe.
Access the link http://MASTER_NODE:50070/ using your browser, and verify that you can see the HDFS startup page.
Here, replace MASTER_NODE with the IP address of the master node running the HDFS NameNode.
The following screenshot shows the current status of the HDFS installation including the number of nodes, total storage, storage taken by each node.
This recipe explains how to use the HDFS basic command line to execute those commands.
It is worth noting that HDFS commands have a one-to-one correspondence with Unix commands.
The command reads the /data/foo.txt file and prints it to the screen, just like the cat command in Unix system.
Start the HDFS server by following the Setting up HDFS recipe.
Run the following command to create a new directory called /test: >bin/hadoop dfs -mkdir /test.
Run the following command to list the content of the HDFS root directory: >bin/hadoop dfs -ls /
Run the following command to copy the local readme file to /test >bin/hadoop dfs -put README.txt /test.
Run the following command to list the /test directory: >bin/hadoop dfs -ls /test.
Run the following command to copy the /test/README.txt to local directory: >bin/hadoop dfs -get /test/README.txt README-NEW.txt.
When a command is issued, the client will talk to the HDFS NameNode on the user's behalf and carry out the operation.
Generally, we refer to a file or a folder using the path starting with /; for example, /data, and the client will pick up the NameNode from configurations in the HADOOP_HOME/conf directory.
However, if needed, we can use a fully qualified path to force the client to talk to a specific NameNode.
We will use these commands throughout, in the recipes of the book.
Hadoop deployment includes a HDFS deployment, a single job tracker, and multiple TaskTrackers.
In the preceding recipe, Setting up HDFS, we discussed the HDFS deployment.
When we start the JobTracker, it will start the TaskTracker nodes.
You may follow this recipe either using a single machine or multiple machines.
If you are using multiple machines, you should choose one machine as the master node where you will run the HDFS NameNode and the JobTracker.
If you are using a single machine, use it as both the master node as well as a slave node.
Install Java in all machines that will be used to set up Hadoop.
If you are using Windows machines, first install Cygwin and SSH server in each machine.
Let us set up Hadoop by setting up the JobTracker and TaskTrackers.
Set up SSH keys to all machines so that we can log in to all from the master node.
The Setting up HDFS recipe describes the SSH setup in detail.
You can use any of the Hadoop 1.0 branch distributions.
If you are doing a single-node deployment, leave the current value, localhost, as it is.
Inside each node's HADOOP_HOME/conf directory, add the following to the core-site.xml, hdfs-site.xml and mapred-site.xml.
Map reduce local directory is the location used by Hadoop to store temporary files used.
The final property sets the maximum map tasks per node, set it the same as the amount of cores (CPU)
If you have done this as part of the HDFS installation in earlier recipe, you can skip this step.
Verify the installation by listing the processes through the ps | grep java command (if you are using Linux) or via Task Manager (if you are in Windows), in the master node and slave nodes.
Make sure HDFS setup is OK by listing the files using HDFS command line.
As described in the introduction to the chapter, Hadoop installation consists of HDFS nodes, a JobTracker and worker nodes.
When we start the NameNode, it finds the slaves through the HADOOP_HOME/slaves file and uses SSH to start the DataNodes in the remote server at the startup.
Also when we start the JobTracker, it finds the slaves through the HADOOP_HOME/ slaves file and starts the TaskTrackers.
In the next recipe, we will discuss how to run the aforementioned WordCount program using the distributed setup.
The following recipes will discuss how to use MapReduce monitoring UI to monitor the distributed Hadoop setup.
This recipe describes how to run a job in a distributed cluster.
Now let us run the WordCount sample in the distributed Hadoop setup.
To use as inputs to the WordCount MapReduce sample that we wrote in the earlier recipe, copy the README.txt file in your Hadoop distribution to the HDFS filesystem at the location /data/input1
Now, let's run the WordCount example from the HADOOP_HOME directory.
Run the following commands to list the output directory and then look at the results.
Job submission to the distributed Hadoop works in a similar way to the job submissions to local Hadoop installation, as described in the Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop recipe.
First, Hadoop stores both the inputs for the jobs and output generated by the job in HDFS filesystem.
Secondly, when job is submitted, local Hadoop installation runs the job as a local JVM execution.
However, the distributed cluster submits it to the JobTracker, and it executes the job using nodes in the distributed Hadoop cluster.
You can see the results of the WordCount application also through the HDFS monitoring UI, as described in the Using HDFS monitoring UI recipe, and also you can see the statistics about the WordCount job as explained in the next recipe, Using MapReduce Monitoring UI.
Using MapReduce monitoring UI This recipe describes how to use the Hadoop monitoring web console to verify Hadoop installation, and to monitor the allocations and uses of each part of the Hadoop cluster.
The web page shows the current status of the MapReduce installation, including running and completed jobs.
Hadoop monitoring UI lets users access the JobTracker of the Hadoop installation and find different nodes in the installation, their configurations, and usage.
For example, users can use the UI to see current running jobs and logs associated with the jobs.
Introduction Hadoop Distributed File System (HDFS) is a block-structured, distributed filesystem that is designed to run on a low-cost commodity hardware.
Hadoop supports data locality aware processing of the data stored in HDFS.
However, HDFS can be used as a general purpose distributed filesystem as well.
Setting up HDFS and other related recipes in Chapter 1, Getting Hadoop Up and Running in a Cluster, show how to deploy HDFS and give an overview of the basic operation of HDFS.
In this chapter, you will be introduced to a selected set of advanced HDFS operations that would be useful when performing large-scale data processing with Hadoop MapReduce, as well as when using HDFS as a standalone distributed filesystem for non-MapReduce use cases.
Benchmarking HDFS Running benchmarks is a good way to verify whether your HDFS cluster is set up properly and performs as expected.
This recipe shows how to use DFSIO to benchmark the read and write performance of a HDFS cluster.
You must set up and deploy HDFS and Hadoop MapReduce prior to running these benchmarks.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root directory:
The following steps show you how to run the write performance benchmark:
To run the write performance benchmark, execute the following command in the $HADOOP_HOME directory.
The –nrFiles parameter specifies the number of files and the -fileSize parameter specifies the file size in MB.
You can provide your own result file name using the –resFile parameter.
The following steps show you how to run the read performance benchmark:
Hence, the write benchmark should be executed before running the read benchmark and the files written by the write benchmark should exist in the HDFS for the read benchmark to work.
Benchmark writes the results to the console and appends the results to a logfile similarly to the write benchmark.
To clean the files generated by these benchmarks, use the following command:
Running these tests together with monitoring systems can help you identify the bottlenecks much more easily.
Adding a new DataNode This recipe shows how to add new nodes to an existing HDFS cluster without restarting the whole cluster, and how to force HDFS to rebalance after the addition of new nodes.
You can use rsync to copy the Hadoop configuration from another node.
Password-less SSH setup is optional, if you are not planning on using the bin/*.sh scripts from the master node to start/stop the cluster.
The following steps will show you how to add a new DataNode to an existing HDFS cluster:
Start the DataNode in the newly added slave node by using the following command.
This is helpful if you are adding more than one new DataNodes to the cluster.
The preceding steps apply both to adding a new node as well as re-joining a node that has been crashed and restarted.
Similarly, you can add a new node to the Hadoop MapReduce cluster as well.
Rebalancing HDFS When you add new nodes, HDFS will not rebalance automatically.
However, HDFS provides a rebalancer tool that can be invoked manually.
This tool will balance the data blocks across cluster up to an optional threshold percentage.
Rebalancing would be very helpful if you are having space issues in the other existing nodes.
The optional –threshold parameter specifies the percentage of disk capacity leeway to consider when identifying a node as under- or over-utilized.
An under-utilized data node is a node whose utilization is less than average utilization – threshold.
An over-utilized data node is a node whose utilization is greater than average utilization + threshold.
Smaller threshold values will achieve more evenly balanced nodes, but would take more time for the rebalancing.
Decommissioning DataNodes There can be multiple situations where you want to decommission one or more data nodes from an HDFS cluster.
This recipe shows how to gracefully decommission the DataNodes without incurring data loss and without having to restart the cluster.
The following steps show you how to decommission data nodes gracefully:
If your cluster doesn't have it, add an exclude file to the cluster.
Add the hostnames of the nodes that are to be decommissioned to the exclude file.
The decommissioning process can take a significant time, as it requires replication of data blocks without overwhelming the other tasks of the cluster.
The decommissioning progress can be monitored using the following command as well.
Do not shut down the nodes until the decommissioning is complete.
You can remove the nodes from the exclude file and execute the bin/hadoop dfsadmin –refreshNodes command when you want to add the nodes back in to the cluster.
The decommissioning process can be stopped by removing the node's name from the exclude file and then executing the bin/hadoop dfsadmin –refreshNodes command.
When a node is in the decommissioning process, HDFS replicates the blocks in that node to the other nodes in the cluster.
Decommissioning can be a slow process as HDFS purposely does it slowly to avoid overwhelming the cluster.
Shutting down nodes without decommissioning may result in data loss.
After the decommissioning is completed, the nodes mentioned in the exclude file are not allowed to communicate with the NameNode.
This feature allows us to utilize multiple disks/volumes to store the data blocks in DataNodes.
Hadoop will try to store equal amounts of data in each directory.
Hadoop also supports limiting the amount of disk space used by HDFS.
The following steps will show you how to add multiple disk volumes:
To limit the HDFS disk usage, add the following property to $HADOOP_HOME/conf/ hdfs-site.xml to reserve space for non-DFS usage.
The value specifies the number of bytes that HDFS cannot use per volume.
Always leave this much space free for non dfs use.
Setting HDFS block size HDFS stores files across the cluster by breaking them down in to coarser grained, fixed-size blocks.
The block size of a data product can affect the performance of the filesystem operations where larger block sizes would be more effective, if you are storing and processing very large files.
The block size of a data product can affect the performance of MapReduce computations, as the default behavior of Hadoop is to create one map task for each data block of the input files.
This change would not change the block size of the files that are already in the HDFS.
Only the files copied after the change will have the new block size.
To specify the HDFS block size for specific file paths, you can specify the block size when uploading the file from the command line as follows:
You can also specify the block size when creating files using the HDFS Java API as well.
You can use the fsck command to find the block size and block locations of a particular file path in the HDFS.
You can find this information by browsing the filesystem from the HDFS monitoring console as well.
Setting the file replication factor HDFS stores files across the cluster by breaking them down in to coarser grained fixed-size blocks.
These coarser grained data blocks are replicated in different DataNodes mainly for the fault-tolerance purposes.
Data block replication also has the ability to increase the data locality of the MapReduce computations and to increase the total data access bandwidth as well.
Reducing the replication factor helps save the storage space in HDFS.
This recipe shows how to change the default replication factor of a HDFS deployment affecting the new files that would be created afterwards, how to specify a custom replication factor at the time of file creation in HDFS, and how to change the replication factor of the existing files in HDFS.
This change would not change the replication factor of the files that are already in the HDFS.
Only the files copied after the change will have the new replication factor.
The setrep command can be used to change the replication factor of files or file paths that are already in the HDFS.
The <path> parameter of the setrep command specifies the HDFS path where the replication factor has to be changed.
The –R option recursively sets the replication factor for files and directories within a directory.
The replication factor of a file is displayed when listing the files using the ls command.
The replication factor of files is displayed when browsing files in the HDFS monitoring UI.
This API gives us the ability to utilize the data stored in HDFS from other Java programs as well as to process that data with other non-Hadoop computational frameworks.
Occasionally you may also come across a use case where you want to access HDFS directly from inside a MapReduce application.
However, if you are writing or modifying files in HDFS directly from a Map or Reduce task, be aware that you are violating the side effect free nature of MapReduce that might lead to data consistency issues based on your use case.
Set the HADOOP_HOME environment variable to point to your Hadoop installation root directory.
The following steps show you how to use the HDFS Java API to perform filesystem operations on a HDFS installation using a Java program:
The following sample program creates a new file in HDFS, writes some text to the newly created file, and reads the file back from the HDFS:
Compile and package the above program in to a JAR package.
Unzip the source package for this chapter, go to the HDFS_Java_API folder and run the Ant build.
The HDFSJavaAPI.jar file will be created in the build folder.
You can use the following Ant build file to compile the above sample program:
You can execute the above sample with Hadoop using the following command.
Running samples using the hadoop script ensures that it uses the currently configured HDFS and the necessary dependencies from the Hadoop classpath.
Use the ls command to list the newly created file: >/bin/hadoop fs -ls.
In order to interact with the HDFS programmatically, we first obtain a handle to the currently configured filesystem.
Instantiating a Configuration object and obtaining a FileSystem handle within a Hadoop environment will point it to the HDFS NameNode of that environment.
Several alternative methods to configure a FileSystem object are discussed in the Configuring the FileSystem object section.
Configuring the FileSystem object We can use the HDFS Java API from outside the Hadoop environment as well.
When doing so, we have to explicitly configure the HDFS NameNode and the port.
The following are a couple of ways to perform that configuration:
In case the above program could not find a valid HDFS configuration, it will point to the local filesystem instead of the HDFS.
You can identify the current filesystem of the FileSystem object using the getUri() function as follows.
This information would be very useful if you are planning for performing any data local operations on the file's data using a framework other than Hadoop MapReduce.
Using HDFS C API (libhdfs) libhdfs—a native shared library—provides a C API that enables non-Java programs to interact with HDFS.
You may have to download the Hadoop standard distribution and compile the libhdfs library from the source code, if your operating system is not compatible with the pre-compiled libraries.
Refer to the Mounting HDFS (Fuse-DFS) recipe for information on compiling the libhdfs library.
The following steps show you how to perform operations on a HDFS installation using a HDFS C API:
The following sample program creates a new file in HDFS, writes some text to the newly created file and reads the file back from the HDFS.
Replace NAMENODE_ HOSTNAME and PORT with the relevant values corresponding to the NameNode of your HDFS cluster.
When compiling you have to link with the libhdfs and the JVM libraries.
You also have to include the JNI header files of your Java installation.
Replace the ARCH and the architecture dependent paths with the paths relevant for your system.
Export an environment variable named CLASSPATH with the Hadoop dependencies.
Ant build script to generate the classpath Add the following Ant target to the build file given in step 2 of the HDFS Java API recipe.
The modified build.xml script is provided in the HDFS_C_ API folder of the source package for this chapter.
Copy and export this string as the CLASSPATH environmental variable.
First we connect to a HDFS cluster using the hdfsConnect command by providing the hostname (or the IP address) and port of the NameNode of the HDFS cluster.
The hdfsConnectAsUser command can be used to connect to a HDFS cluster as a specific user.
We create new file and obtain a handle to the newly created file using the hdfsOpenFile command.
The O_WRONLY|O_CREAT flags create a new file or override the existing file and open it in write only mode.
The fourth, fifth, and sixth parameters of the hdfsOpenFile command are the buffer size for read/write operations, block replication factor and block size for the newly created file.
Specify 0 if you want to use the default values for these three parameters.
The hdfsWrite command writes the provided data in to the file specified by the outFile handle.
Data size needs to be specified using the number of bytes.
The hdfsRead command reads data from the file specified by the inFile.
The size of the buffer in bytes needs to be provided as the fourth parameter.
The hdfsRead command returns the actual number of bytes read from the file that might be less than the buffer size.
If you want to ensure certain amounts of bytes that are read from the file, it is advisable to use the hdfsRead command from inside a loop until the specified number of bytes are read.
Configuring using HDFS configuration files You can also use the HDFS configuration files to point libhdfs to your HDFS NameNode, instead of specifying the NameNode hostname and the port number in the hdfsConnect command.
Add the conf directory of your HDFS installation to the CLASSPATH environmental variable.
Mounting HDFS (Fuse-DFS) The Fuse-DFS project allows us to mount HDFS on Linux (supports many other flavors of Unix as well) as a standard filesystem.
This allows any program or user to access and interact with HDFS similar to a traditional filesystem.
You must have the following software installed in your system.
Fuse development files can be found in fusedevel RPM for Redhat/Fedora and in libfuse-dev package for Debian/Ubuntu.
JAVA_HOME must be set to point to a JDK, not to a JRE.
You must have the root privileges for the node in which you are planning to mount the HDFS filesystem.
The following recipe assumes you already have pre-built libhdfs libraries.
If you are using some other platform, first follow the Building libhdfs sub section in the more info section to build the libhdfs libraries.
The following steps show you how to mount an HDFS filesystem as a standard file system on Linux:
Go to $HADOOP_HOME and create a new directory named build.
Create a symbolic link to the libhdfs libraries inside the build directory.
It would be helpful to run the following command in the debug mode to identify any error when you run it for the first time.
The rw parameter mounts the filesystem read-write (ro for read-only)
Many instances of HDFS can be mounted on to different directories using the Fuse-DFS as mentioned in the preceding sections.
Building libhdfs In order to build libhdfs, you must have the following software installed in your system:
Package the distribution together with libhdfs by executing the following command.
Provide the path to the Apache Forrest installation using the -Dforrest.home property.
Merging files in HDFS This recipe shows how to merge files in HDFS to create a single file.
This is useful when retrieving the output of a MapReduce computation with multiple reducers where each reducer produces a part of the output.
Introduction This chapter describes how to perform advanced administration steps for your Hadoop Cluster.
This chapter assumes that you have followed Chapter 1, Getting Hadoop Up and Running in a Cluster, and have installed Hadoop in a clustered or pseudo-distributed setup.
We can control Hadoop configurations through the following three configuration files:
Each configuration file has name-value pairs expressed in an XML format, and they define the workings of different aspects of Hadoop.
The following code snippet shows an example of a property in the configuration file.
The following instructions show how to change the directory to which we write Hadoop logs and configure the maximum number of map and reduce tasks:
You can verify the number of processes created using OS process monitoring tools.
If you are in Linux, run the watch ps –ef|grep hadoop command.
HADOOP_LOG_DIR redefines the location to which Hadoop writes its logs.
You can see some of them in the following tables.
This is the maximum number of parallel copies the reduce step will execute to fetch output from many parallel jobs.
Xmx200M This is for passing Java options into the map JVM.
Xmx200M This is for passing Java options into the reduce JVM.
This is the number of server threads to handle RPC calls in the NameNode.
We can use them to verify our Hadoop installation and measure Hadoop's performance.
This recipe introduces these benchmarks and explains how to run them.
You can run these benchmarks either on a cluster setup or on a pseudo-distributed setup.
First, we generate some random data using the randomwriter Hadoop job and then sort them using the sort sample.
Finally, when everything is successful, the following message will be displayed:
First, the randomwriter application runs a Hadoop job to generate random data that can be used by the second sort program.
If your computer has more capacity, you may run the initial randomwriter step with increased output sizes.
More information about these benchmarks can be found at http://www.michaelnoll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoopcluster-with-terasort-testdfsio-nnbench-mrbench/
In its default configuration, Hadoop starts a new JVM for each map or reduce task.
However, running multiple tasks from the same JVM can sometimes significantly speed up the execution.
Run the WordCount sample by passing the following option as an argument:
Monitor the number of processes created by Hadoop (through ps –ef|grephadoop command in Unix or task manager in Windows)
Hadoop starts only a single JVM per task slot and then reuses it for an unlimited number of tasks in the job.
When the value is set to -1, Hadoop runs the tasks in the same JVM.
Fault tolerance and speculative execution The primary advantage of using Hadoop is its support for fault tolerance.
When you run a job, especially a large job, parts of the execution can fail due to external causes such as network failures, disk failures, and node failures.
When a job has been started, Hadoop JobTracker monitors the TaskTrackers to which it has submitted the tasks of the job.
If any TaskTrackers are not responsive, Hadoop will resubmit the tasks handled by unresponsive TaskTracker to a new TaskTracker.
Generally, a Hadoop system may be compose of heterogeneous nodes, and as a result there can be very slow nodes as well as fast nodes.
Potentially, a few slow nodes can slow down an execution significantly.
This means if most of the map tasks have completed and Hadoop is waiting for a few more map tasks, Hadoop JobTracker will start these pending jobs also in a new node.
The tracker will use the results from the first task that finishes and stop any other identical tasks.
However, the above model is feasible only if the map tasks are side-effects free.
If such parallel executions are undesirable, Hadoop lets users turn off speculative executions.
Run the WordCount sample by passing the following option as an argument to turn off the speculative executions:
When the option is specified and set to false, Hadoop will turn off the speculative executions.
Debug scripts – analyzing task failures A Hadoop job may consist of many map tasks and reduce tasks.
Therefore, debugging a Hadoop job is often a complicated process.
It is a good practice to first test a Hadoop job using unit tests by running it with a subset of the data.
However, sometimes it is necessary to debug a Hadoop job in a distributed mode.
To support such cases, Hadoop provides a mechanism called debug scripts.
A debug script is a shell script, and Hadoop executes the script whenever a task encounters an error.
We can use the debug scripts to copy all the logfiles to a single location, e-mail them to a single e-mail account, or perform some analysis.
Compile the code base by running Ant from home directory of the source code.
The job will run the FaultyWordCount task that will always fail.
Then Hadoop will execute the debug script, and you can find the results of the debug script from HADOOP_HOME.
However, the input value is not the file location, but the command that needs to be run on the machine when an error occurs.
If you have a specific file that is present in all machines that you want to run when an error occurs, you can just add that path through the conf.
But, Hadoop runs the mappers in multiple nodes, and often in a machine different than the machine running the job's client.
Therefore, for the debug script to work, we need to get the script to all the nodes running the mapper.
As described in the Using Distributed cache to distribute resources recipe in Chapter 4, Developing Complex Hadoop MapReduce Applications, users can add files that are in the HDFS filesystem to distribute cache.
Then, Hadoop automatically copies those files to each node by running map tasks.
Then Hadoop copies the script files to each mapper node and symlinks it to the working directory of the job.
When an error occurs, Hadoop will run the ./fail-script command, which will run the script file that has been copied to the node through distributed cache.
The debug script will carry out the tasks you have programmed when an error occurs.
When processing a large amount of data, there may be cases where a small amount of map tasks will fail, but still the final results make sense without the failed map tasks.
This could happen due to a number of reasons such as:
In the first case, it is best to debug, find the cause for failures, and fix it.
However, in the second and third cases, such errors may be unavoidable.
It is possible to tell Hadoop that the job should succeed even if some small percentage of map tasks have failed.
Refer to the Setting Hadoop in a distributed cluster environment recipe from the Chapter 1, Getting Hadoop Up and Running in a Cluster.
We can turn on bad record skipping by setting the following parameters to positive values:
You can also limit the percentage of failures in map or reduce tasks by setting the JobConf.
Also, Hadoop repeats the tasks in case of a failure.
When a user submits a job to Hadoop, this job needs to be assigned a resource (a computer/host) before execution.
This process is called scheduling, and a scheduler decides when resources are assigned to a given job.
Hadoop is by default configured with a First in First out (FIFO) scheduler, which executes jobs in the same order as they arrive.
However, for a deployment that is running many MapReduce jobs and shared by many users, more complex scheduling policies are needed.
The good news is that Hadoop scheduler is pluggable, and it comes with two other schedulers.
Therefore, if required, it is possible to write your own scheduler as well.
The capacity scheduler shares computer resources allocated to a queue with other queues if those resources are not in use.
This recipe describes how to change the scheduler in Hadoop.
However, from Hadoop 1.0.0 and higher releases, this JAR file is in the right place in the Hadoop distribution.
Verify that the new scheduler has been applied by going to http://<jobtracker-host>:50030/scheduler in your installation.
If the scheduler has been properly applied, the page will have the heading "Fair Scheduler Administration"
When you follow the preceding steps, Hadoop will load the new scheduler settings when it is started.
The fair scheduler shares equal amount of resources between users unless it has been configured otherwise.
The fair scheduler supports users to configure it through two ways.
Hadoop also includes another scheduler called capacity scheduler that provides more fine-grained control than the fair scheduler.
Hadoop security – integrating with Kerberos Hadoop by default runs without security.
However, it also supports Kerberos-based setup, which provides full security.
This recipe describes how to configure Hadoop with Kerberos for security.
We will define users as principals in the Kerberos server.
Users can obtain a ticket from the Kerberos server, and use that ticket to log in to any server in Hadoop.
We will map each Kerberos principal with a Unix user.
Once logged in, the authorization is performed based on the Unix user and group permissions associated with each user.
Set up Hadoop by following Chapter 1, Getting Hadoop Up and Running in a Cluster either using pseudo-distributed or clustered setup.
We need a machine to use as the Kerberos node for which you have root access.
Kerberos will use Unix users in Hadoop machines as Kerberos principals and use local Unix-level user permissions to do authorization.
Create the following users and groups with permissions in all the machines on which you plan to run MapReduce.
We will have three users—hdfs to run HDFS server, mapred to run MapReduce server, and bob to submit jobs.
Now, we will create a key tab file that contains credentials for Kerberos principals.
We will use these credentials to avoid entering the passwords at Hadoop startup.
Deploy key tab files by moving them in to the HADOOP_HOME/conf directory.
Change the directory to HADOOP_HOME and run following commands to set the permissions for key tab files:
Configure Hadoop properties by adding following properties to the associated configuration files.
Here, Hadoop will replace the _HOST with the localhost name.
Here Hadoop will replace the _HOST with the localhost name.
However, there is a bug that stops Hadoop from reading the credentials.
We can work around this by the last command that rewrites the key in more readable format.
Start the DataNode (this must be done as the root) by running following command: >su - root.
Configure the Linux task controller, which must be used for Kerberos setup.
If all commands run successfully, you will see the WordCount output as described in Chapter 1, Getting Hadoop Up and Running in a Cluster.
By running the kinit command, the client would obtain a Kerberos ticket and store it in the filesystem.
When we run the command, the client uses the Kerberos ticket to get access to the Hadoop nodes and submit jobs.
Hadoop resolves the permission based on the user and group permissions of the Linux users that matches the Kerberos principal.
The two tools that might be useful are as follows:
Also, when you change something, make sure you restart all the processes first by killing all the running processes.
Using the Hadoop Tool interface Often Hadoop jobs are executed through a command line.
Therefore, each Hadoop job has to support reading, parsing, and processing command-line arguments.
It can be done through following commands: bin/hadoopfs -mkdir /data/output.
Try to run the WordCount without any options, and it will list the available options.
When a job extends from the Tool interface, Hadoop will intercept the command-line arguments, parse the options,  and configure the JobConf object accordingly.
Introduction This chapter introduces you to several advanced Hadoop MapReduce features that will help you to develop highly customized, efficient MapReduce applications.
In this chapter, we will explore the different data types provided by Hadoop and the steps to implement custom data types for Hadoop MapReduce computations.
We will also explore the different data input and output formats provided by Hadoop.
This chapter will provide you with the basic understanding of how to add support for new data formats in Hadoop.
We will also be discussing other advanced Hadoop features such as using DistributedCache for distribute data, using Hadoop Streaming for quick prototyping of Hadoop computations, and using Hadoop counters to report custom metrics for your computation as well as adding job dependencies to manage simple DAG-based workflows of Hadoop MapReduce computations.
Choosing appropriate Hadoop data types Hadoop uses the Writable interface based classes as the data types for the MapReduce computations.
These data types are used throughout the MapReduce computational flow, starting with reading the input data, transferring intermediate data between Map and Reduce tasks, and finally, when writing the output data.
Choosing the appropriate Writable data types for your input, intermediate, and output data can have a large effect on the performance and the programmability of your MapReduce programs.
The Writable interface defines how Hadoop should serialize and de-serialize the values when transmitting and storing the data.
In addition to the functionality of the Writable interface, the WritableComparable interface further defines how to compare the keys of this type with each other for sorting purposes.
Hadoop's Writable versus Java's Serializable Hadoop's Writable-based serialization framework provides a more efficient and customized serialization and representation of the data for MapReduce programs than using the general-purpose Java's native serialization framework.
As opposed to Java's serialization, Hadoop's Writable framework does not write the type name with each object expecting all the clients of the serialized data to be aware of the types used in the serialized data.
Omitting the type names makes the serialization process faster and results in compact, random accessible serialized data formats that can be easily interpreted by non-Java clients.
Hadoop's Writable-based serialization also has the ability to reduce the object-creation overhead by reusing the Writable objects, which is not possible with the Java's native serialization framework.
The following steps show you how to configure the input and output data types of your Hadoop MapReduce application:
Hadoop provides several primitive data types such as IntWritable, LongWritable, BooleanWritable, FloatWritable, and ByteWritable, which are the Writable versions of their respective Java primitive data types.
We can use these types as both, the key types as well as the value types.
The following are several more Hadoop built-in data types that we can use as both, the key as well as the value types:
The following Hadoop build-in collection data types can only be used as value types.
To use the TwoDArrayWritable type as the value type of a reducer's input, you need to specify the type of the stored values by creating a subclass of  the TwoDArrayWritable type similar to the ArrayWritable type.
Keys and values should be of the Writable data types.
There can be use cases where none of the built-in data types matches your requirements or a custom data type optimized for your use case may perform better than a Hadoop built-in data type.
The Writable interface-based types can be used as value types in Hadoop MapReduce computations.
In this recipe, we implement a sample Hadoop Writable data type for HTTP server log entries.
For the purpose of this sample, we consider that a log entry consists of the five fields—request host, timestamp, request URL, response size, and the http status code.
The following are the steps to implement a custom Hadoop Writable data type for the HTTP server log entries:
The Writable interface consists of the two methods, readFields() and write()
Inside the readFields() method, we de-serialize the input data and populate the fields of the Writable object.
In the preceding example, we use the Writable types as the fields of our custom Writable type and use the readFields() method of the fields for de-serializing the data from the DataInput object.
It is also possible to use java primitive data types as the fields of the Writable type and to use the corresponding read methods of the DataInput object to read the values from the underlying stream, as shown in the following code snippet:
Inside the write() method, we write the fields of the Writable object  to the underlying stream.
In case you are using Java primitive data types as the fields of the Writable object, you can use the corresponding write methods of the DataOutput object to write the values to the underlying stream as below.
Be cautious about the following issues when implementing your custom Writable data type:
In case you are using the TextOutputFormat to serialize instances of your custom Writable type, make sure to have a meaningful toString() implementation for your custom Writable data type.
You should not rely on the existing state of the object when populating it inside the readFields() method.
Implementing a custom Hadoop key type The instances of Hadoop MapReduce key types should have the ability to compare against each other for sorting purposes.
In this recipe, we modify the LogWritable data type of the Writing a custom Hadoop Writable data type recipe to implement the WritableComparable interface.
The following are the steps to implement a custom Hadoop WritableComparable data type for the HTTP server log entries, which uses the request host name and timestamp for comparison.
The WritableComparable interface introduces the comapreTo() method in addition to the readFields() and write() methods of the Writable interface.
The compareTo() method should return a negative integer, zero, or a positive integer, if this object is less than, equal to, or greater than the object being compared to respectively.
In the LogWritable implementation, we consider the objects equal if both the user's IP address and the timestamp are the same.
If the objects are not equal, we decide the sort order first based on the user IP address and then based on the timestamp.
Hadoop uses HashPartitioner as the default Partitioner implementation to calculate the distribution of the intermediate data to the reducers.
HashPartitioner requires the hashCode() method of the key objects to satisfy the following two properties:
Hence, you must implement a stable hashCode() method for your custom Hadoop key types satisfying the above mentioned two requirements.
In the LogWritable implementation, we use the hash code of the request hostname/IP address as the hash code of the LogWritable instance.
This ensures that the intermediate LogWritable data will be partitioned based on the request hostname/IP address.
Emitting data products belonging to multiple value types from a mapper is useful when performing reducer-side joins as well as when we need to avoid the complexity of having multiple MapReduce computations to summarize different types of properties in a data set.
However, Hadoop reducers do not allow multiple input value types.
In these scenarios, we can use the GenericWritable class to wrap multiple value instances belonging to different data types.
In this recipe, we reuse the HTTP server log entry analyzing sample of the Implementing a custom Hadoop Writable data type recipe.
However, instead of using a custom data type, in the current recipe we output multiple value types from the mapper.
This sample aggregates the total number of bytes served from the web server to a particular host and also outputs a tab-separated list of URLs requested by the particular host.
We use IntWritable to output the number of bytes from the mapper and Text to output the request URL.
The following steps show how to implement a Hadoop GenericWritable data type that can wrap instances of either IntWritable or Text data types.
The GenericWritable implementations serialize and de-serialize the data using the index to this array of classes.
In the mapper, you wrap each of your values with instances of the GenericWritable implementation.
The reducer implementation has to take care of the different value types manually.
The ObjectWritable class can handle Java primitive types, strings, and arrays without the need of a Writable wrapper.
However, Hadoop serializes the ObjectWritable instances by writing the class name of the instance with each serialized entry, making it inefficient compared to a GenericWritable class-based implementation.
Choosing a suitable Hadoop InputFormat for your input data format.
Hadoop supports processing of many different formats and types of data through InputFormat.
The InputFormat of a Hadoop MapReduce computation generates the key-value pair inputs for the mappers by parsing the input data.
InputFormat also performs the splitting of the input data into logical partitions, essentially determining the number of Map tasks of a MapReduce computation and indirectly deciding the execution location of the Map tasks.
Hadoop generates a map task for each logical data partition and invokes the respective mappers with the key-value pairs of the logical splits as the input.
Each line of the input data is broken into a key (text) and value (text) pair using a delimiter character.
If a line does not contain the delimiter, the whole line will be treated as the key and the value will be empty.
We can specify a custom delimiter by setting a property in the job's configuration object as follows, where we use the comma character as the delimiter between the key and value.
Hence, we specify the input path to the MapReduce computation using the setInputPaths() method of the FileInputFormat class.
We have to perform this step when using any InputFormat that is based on the FileInputFormat class.
We can provide multiple HDFS input paths to a MapReduce computation by providing a comma-separated list of paths.
You can also use the addInputPath() static method of the FileInputFormat class to add additional input paths to a computation.
Make sure that your mapper input data types match the data types generated by InputFormat used by the MapReduce computation.
The following are some of the InputFormat implementations that Hadoop provide to support several common data formats.
TextInputFormat generates a key-value record for each line of the input text files.
For each line, the key (LongWritable) is the byte offset of the line in the file and the value (Text) is the line of text.
NLineInputFormat splits the input files into logical splits of fixed number of lines.
We can use the NLineInputFormat when we want our map tasks to receive a fixed number of lines as the input.
The key (LongWritable) and value (Text) records are generated for each line in the split similar to the TextInputFormat.
By default, NLineInputFormat creates a logical split (and a Map task) per line.
The number of lines per split (or key-value records per Map task) can be specified as follows.
NLineInputFormat generates a key-value record for each line of the input text files.
Hadoop Sequence files store the data as binary key-value pairs and support data compression.
DBInputFormat uses the record number as the key (LongWritable) and the query result record as the value (DBWritable)
Using multiple input data types and multiple mapper implementations in a single MapReduce application We can use the MultipleInputs feature of Hadoop to run a MapReduce job with multiple input paths, while specifying a different InputFormat and (optionally) a mapper for each path.
Hadoop will route the outputs of the different mappers to the instances of the single reducer implementation of the MapReduce computation.
Multiple inputs with different InputFormat implementations is useful when we want to process multiple data sets with the same meaning but are in different input formats (comma-delimited data set and tab-delimited data set)
We can use the following addInputPath static method of the MutlipleInputs class to add the input paths and the respective input formats to the MapReduce computation.
The following is an example usage of the preceding method.
The multiple inputs feature with both different mappers and InputFormat is useful when performing a reduce-side join of two or more data sets.
The following is an example of using multiple inputs with different input formats and different mapper implementations.
Adding support for new input data formats implementing a custom InputFormat.
Hadoop enables us to implement and specify custom InputFormat implementations for our MapReduce computations.
In this recipe, we implement a InputFormat and a RecordReader for the HTTP log files.
This InputFormat will generate LongWritable instances as keys and LogWritable instances as the values.
The following are the steps to implement a custom InputFormat for the HTTP server log files based on the FileInputFormat.
LogFileInputFormat extends the FileInputFormat, which provides a generic splitting mechanism for HDFS-file based InputFormat.
Optionally, we can also override the isSplitable() method of the FileInputFormat to control whether the input files are split-up into logical partitions or used as whole files.
RecordReader<K,V> abstract class and uses LineRecordReader internally to perform the basic parsing of the input data.
We perform the custom parsing of the log entries of the input data in the nextKeyValue() method.
We use a regular expression to extract the fields out of the HTTP service log entry and populate an instance of the LogWritable class using those fields.
We can perform custom splitting of input data by overriding the getSplits() method of the InputFormat class.
The getSplits() method should return a list of InputSplit objects.
A InputSplit object represents a logical partition of the input data and will be assigned to a single Map task.InputSplit classes extend the InputSplit abstract class and should override the getLocations() and getLength() methods.
The getLength() method should provide the length of the split and the getLocations() method should provide a list of nodes where the data represented by this split is physically stored.
Hadoop uses a list of data local nodes for Map task scheduling.
You can write InputFormat implementations for none HDFS data as well.
DBInputFormat supports reading the input data from a SQL table.
Formatting the results of MapReduce computations – using Hadoop OutputFormats.
Often times the output of your MapReduce computation will be consumed by other applications.
Hence, it is important to store the result of a MapReduce computation in a format that can be consumed efficiently by the target application.
It is also important to store and organize the data in a location that is efficiently accessible by your target application.
We can use Hadoop OutputFormat interface to define the data storage format, data storage location and the organization of the output data of a MapReduce computation.
A OutputFormat prepares the output location and provides a RecordWriter implementation to perform the actual serialization and storage of the data.
TextOutputFormat writes the records of the output data to plain text files in HDFS using a separate line for each record.
TextOutputFormat uses the tab character to delimit between the key and the value of a record.
TextOutputFormat extends FileOutputFormat, which is the base class for all file-based output formats.
Hadoop Sequence files store the data as binary key-value pairs and supports data compression.
We can use the Sequence files to store the result of a MapReduce computation, if the output of the MapReduce computation going to be the input of another Hadoop MapReduce computation.
Hence, we specify the output path to the MapReduce computation using the setOutputPath() method of the FileOutputFormat.
We have to perform this step when using any OutputFormat that is based on the FileOutputFormat.
In case your OutputFormat implementation stores the data in a filesystem, you can extend from the FileOutputFormat class to make your life easier.
Hadoop partitions the intermediate data generated from the Map tasks across the reduce tasks of the computations.
A proper partitioning function ensuring balanced load for each reduce task is crucial to the performance of MapReduce computations.
Partitioning can also be used to group together related set of records to specific reduce tasks, where you want the certain outputs to be processed or grouped together.
Hadoop partitions the intermediate data based on the key space of the intermediate data and decides which reduce task will receive which intermediate record.
The sorted set of keys and their values of a partition would be the input for a reduce task.
In Hadoop, the total number of partitions should be equal to the number of reduce tasks for the MapReduce computation.
HashPartitioner partitions the keys based on their hashcode(), using the formula key.hashcode() mod r, where r is the number of reduce tasks.
The following diagram illustrates HashPartitioner for a computation with two reduce tasks:
There can be scenarios where our computations logic would require or can be better implemented using an application's specific data-partitioning schema.
In this recipe, we implement a custom Partitioner for our HTTP log processing application, which partitions the keys (IP addresses) based on their geographic regions.
The following steps show you how to implement a custom Partitioner that partitions the intermediate data based on the location of the request IP address or the hostname.
In the above example, we perform the partitioning of the intermediate data, such that the requests from the same geographic region will be sent to the same reducer instance.
The getGeoLocation() method returns the geographic location of the given IP address.
We omit the implementation details of the getGeoLocation() method as it's not essential for the understanding of this example.
We then obtain the hashCode() of the geographic location and perform a modulo operation to choose the reducer bucket for the request.
The set of input records to a reducer are in a sorted order ensuring proper ordering within an input partition.
However, the Hadoop default partitioning strategy (HashPartitioner) does not enforce an ordering when partitioning the intermediate data and scatters the keys among the partitions.
A key can be split into a set of fields by using a separator string.
We can specify the indexes of the set of fields to be considered when partitioning.
We can also specify the index of the characters within fields as well.
Broadcasting and distributing shared resources to tasks in a MapReduce job – Hadoop DistributedCache.
We can use the Hadoop DistributedCache to distribute read-only file based resources to the Map and Reduce tasks.
These resources can be simple data files, archives or JAR files that are needed for the computations performed by the mappers or the reducers.
The following steps show you how to add a file to the Hadoop DistributedCache and how to retrieve it from the Map and Reduce tasks.
Hadoop copies the files added to the DistributedCache to all the worker nodes before the execution of any task of the job.
Hadoop also supports creating symlinks to the DistributedCache files in the working directory of the computation by adding a fragment with the desired symlink name to the URI.
We parse and load the data from the DistributedCache in the setup() method of the mapper or the reducer.
Files with symlinks are accessible from the working directory using the provided symlink's name.
The following sections show you how to distribute the compressed archives using DistributedCache, how to add resources to the DistributedCache using the |command line and how to use the DistributedCache to add resources to the classpath of the mapper and the reducer.
Distributing archives using the DistributedCache We can use the DistributedCache to distribute archives as well.
You also can provide symlinks to the archives using the URI fragments.
The extracted directory of the archive can be accessible from the working directory of the mapper or the reducer using the above provided symlink.
You can also access the non-extracted DistributedCache archived files directly using the following method in the mapper or reducer implementation:
Files can be added to the DistributedCache using the –files command-line option, while archives can be added using the –archives command-line option.
Files or archives can be in any filesystem accessible for Hadoop, including your local filesystem.
These options support a comma-separated list of paths and the creation of symlinks using the URI fragments.
Adding resources to the classpath using DistributedCache You can use DistributedCache to distribute JAR files and other dependent libraries to the mapper or reducer.
You can use the following methods in your driver program to add the JAR files to the classpath of the JVM running the mapper or the reducer.
Similar to the –files and –archives command-line options we describe in Adding resources to the DistributedCache from the command line subsection, we can also add the JAR files to the classpath of our MapReduce computations by using the –libjars command-line option as well.
Hadoop Streaming feature allows us to use any executable or a script as the mapper or the reducer of a Hadoop MapReduce job.
Hadoop Streaming enables us to perform rapid prototyping of the MapReduce computations using Linux shell utility programs or using scripting languages.
Hadoop Streaming also allows the users with some or no Java knowledge to utilize Hadoop to process data stored in HDFS.
In this recipe, we implement a mapper for our HTTP log processing application using Python and use a Hadoop aggregate package based reducer.
The following are the steps to use a Python program as the mapper to process the HTTP server log files.
Each Map task launches the Hadoop Streaming executable as a separate process in the worker nodes.
The input records (the entries or lines of the log file, not broken in to key value pairs) to the Mapper are provided as lines to the standard input of that process.
The executable should read and process the records from the standard input until the end of the file is reached.
Hadoop Streaming collects the outputs of the executable from the standard output of the process.
Hadoop Streaming converts each line of the standard output to a key-value pair, where the text up to the first tab character is considered the key and the rest of the line as the value.
The logProcessor.py python script outputs the key-value pairs, according to this convention, as follows:
In our example, we use the Hadoop Aggregate package for the reduction part of our computation.
Hadoop aggregate package provides reducer and combiner implementations for simple aggregate operations such as sum, max, unique value count, and histogram.
When used with the Hadoop Streaming, the mapper outputs must specify the type of aggregation operation of the current computation as a prefix to the output key, which is the LongValueSum in our example.
Hadoop Streaming also supports the distribution of files to the worker nodes using the –file option.
We can use this option to distribute executable files, scripts or any other auxiliary file needed for the Streaming computation.
We can specify Java classes as the mapper and/or reducer and/or combiner programs of Hadoop Streaming computations.
We can also specify InputFormat and other options to a Hadoop Streaming computation.
Hadoop Streaming also allows us to use Linux shell utility programs as mapper and reducer as well.
The following example shows the usage of grep as the mapper of a Hadoop Streaming computation.
Hadoop streaming provides the reducer input records of the each key group line by line to the standard input of the process that is executing the executable.
However, Hadoop Streaming does not have a mechanism to distinguish when it starts to feed records of a new key to the process.
Hence, the scripts or the executables for reducer programs should keep track of the last seen key of the input records to demarcate between key groups.
Often times we require multiple MapReduce applications to be executed in a workflow-like manner to achieve our objective.
Hadoop ControlledJob and JobControl classes provide a mechanism to execute a simple workflow graph of MapReduce jobs by specifying the dependencies between them.
In this recipe, we execute the log-grep MapReduce computation followed by the log-analysis MapReduce computation on a HTTP server log data set.
The log-grep computation filters the input data based on a regular expression.
Hence, the log-analysis computation is dependent on the log-grep computation.
We use the ControlledJob to express this dependency and use the JobControl to execute the two related MapReduce computations.
The following steps show you how to add a MapReduce computation as a dependency of another MapReduce computation.
Create the Configuration and the Job objects for the first MapReduce job and populate them with the other needed configurations.
Create the Configuration and Job objects for the second MapReduce job and populate them with the necessary configurations.
The ControlledJob class encapsulates MapReduce job and provides the functionality to track the dependencies for the job.
A ControlledJob class with depending jobs becomes ready for submission only when all of its depending jobs are completed successfully.
A ControlledJob fails if any of the depending jobs fail.
The JobControl class encapsulates a set of ControlledJobs and their dependencies.
JobControl tracks the status of the encapsulated ControlledJobs and contains a thread that submits the jobs that are in the READY state.
If you want to use the output of a MapReduce job as the input of a dependent job, the input paths to the dependent job has to be set manually.
By default, Hadoop generates an output folder per reduce task name with the part prefix.
We can specify all the part prefixed subdirectories as input to the dependent job using wildcards.
We can use the JobControl class to execute and track a group of non-dependent tasks as well.
Apache Oozie is a workflow system for Hadoop MapReduce computations.
You can find more information on Oozie from the project's home page at http://oozie.apache.org/
The ChainMapper class, available in the older version of Hadoop MapReduce API, allowed us to execute a pipeline of mapper classes inside a single Map task computation in a pipeline.
Hadoop uses a set of counters to aggregate the metrics for MapReduce computations.
Hadoop counters are helpful to understand the behavior of our MapReduce programs and to track the progress of the MapReduce computations.
We can define custom counters to track the application specific metrics in MapReduce computations.
The following steps show you how to define a custom counter to count the number of bad or corrupted records in our log processing application.
You can also view the counter values in the admin console or in the command line.
The set of counters in an enum will form a group of counters.
The JobTracker aggregates the counter values reported by the mappers and the reducers.
Introduction Hadoop has a family of projects that are either built on top of Hadoop or work very closely with Hadoop.
These projects have given rise to an ecosystem that focuses on large-scale data processing, and often users can use several of these projects in combination to handle their use cases.
This chapter introduces several key projects in the Hadoop ecosystem and shows how to get started with each project.
Installing HBase HBase is a highly scalable NoSQL data store that supports columnar-style data storage.
As we will see in the next recipe, it works very closely with Hadoop.
Each table has zero or more rows where a row consists of a single row ID and multiple name-value pairs.
For an example, the first row has the row ID Foundation, and several name-value pairs such as author with value asimov.
Although the data model has some similarities with the relational data model, unlike the relational data model, different rows in HBase data model may have different columns.
For instance, the second row may contain completely different name-value pairs from the first one.
You can find more details about the data model from Google's Bigtable paper http://research.google.com/archive/ bigtable.html.
Hadoop by default loads data from flat files, and it is a responsibility of the MapReduce job to read and parse the data through data formatters.
However, often there are use cases where the data is already in a structured form.
Although it is possible to export this data into flat files, parsing and processing the use cases using conventional MapReduce jobs leads to several disadvantages:
HBase addresses these concerns by enabling users to read data directly from HBase and write results directly to HBase without having to convert them to flat files.
Create a test table and list its content using the following commands: hbase(main):001:0> create 'test', 'cf'
It prints all the data in the table: hbase(main):005:0> scan 'test'
The preceding steps configure and run the HBase in the local mode.
The server start command starts the HBase server, and HBase shell connects to the server and issues the commands.
The preceding commands show how to run HBase in the local mode.
The link http://hbase.apache.org/book/standalone_dist.html#distributed explains how to run HBase in the distributed mode.
Data random access using Java client APIs The earlier recipe introduced the command-line interface for HBase.
This recipe demonstrates how we can talk to HBase using the Java API.
Install and start HBase as described in the Installing HBase recipe.
To compile and run the sample, you would need to have Apache Ant installed in your machine.
If Apache Ant has not been installed already, install it by following the instructions given in http://ant.apache.org/manual/install.html.
The following steps explain how to connect to HBase via a Java client, store, and retrieve data from the client.
Compile the Sample by running the following command from SAMPLE5_DIR.
Run the sample by running the following command from SAMPLE5_DIR.
If all works well, this will print the content of the HBase table to the console.
When you run the commands, Ant will run the Java HBase client we had written.
It will connect to the HBase server and issue commands to store and search data in HBase storage.
This recipe explains how to run a MapReduce job that reads and writes data directly to and from an HBase storage.
HBase provides abstract mapper and reducer implementations that users can extend to read and write directly from HBase.
This recipe explains how to write a sample MapReduce application using these mappers and reducers.
We will use the World Bank's Human Development Report (HDR) data by country that shows Gross National Income (GNI) per capita, by countries.
Using MapReduce, we will calculate average value for GNI per capita, by countries.
Install and start HBase as described in the Installing HBase recipe.
To compile and run the sample, you will need to have Apache Ant installed in your machine.
If Apache Ant has not been installed already, install it by following the instructions given at http://ant.apache.org/manual/install.html.
This section demonstrates how to run a MapReduce job on data stored in HBase.
HBase provides two classes TableInputFormat and TableOutputFormat that take off most of the work of reading and writing from an HBase storage.
To be used by these classes, the mapper and reducer must extend the TableMapper and TableReducer classes.
When executed, mapper will receive each HBase row as an input.
Run the following command to compile the MapReduce job: >anthbase-build.
View the results in HBase by running the following command from the HBase shell.
You can start the HBase shell by running bin/hbaseshell from HBASE_HOME.
When we run the MapReduce job, the TableMapper and TableReducer classes receive the control.
Similarly, the TableReducer lets users emit the data directly to the HBase.
By doing that, TableMapper and TableReducer build a new programming model based on HBase APIs.
With the new programming model, users do not have to worry about parsing and formatting data like with normal MapReduce jobs.
The table mapper and reducer map the HBase data to Hadoop name-value pairs and vice versa.
Installing Pig As we described in the earlier chapters, you can use Hadoop MapReduce interface to program most of the applications.
However, if we are writing an application that includes many MapReduce steps, programming them with MapReduce is complicated.
There are several higher-level programming interfaces such as Pig and Hive to program parallel applications built on top of MapReduce.
We will discuss these two interfaces in the following recipes.
To run Pig commands, change the directory to PIG_HOME and run the pig command.
You can issue the Pig commands from the grunt shell.
The preceding instructions set up Pig in the local mode, and you can use the grunt> shell to execute the Pig commands.
The preceding commands explain how to run Pig in the local mode.
Running your first Pig command This recipe runs a basic Pig script.
As the sample dataset, we will use Human Development Report (HDR) data by country.
It shows the Gross National Income (GNI) per capita by country.
This recipe will use Pig to process the dataset and create a list of countries that have more than 2000$ of gross national income per capita (GNI) sorted by the GNI value.
This section describes how to use Pig Latin script to find countries with 2000$ GNI sorted by the same criterion from the HDR dataset.
The first line instructs Pig to load the CSV (comma-separated values) file into the variable A.
The PigStorage(',') portion tells Pig to load the data using ',' as the separator and assign them to the fields described in the AS clause.
After loading the data, you can process the data using Pig commands.
Each Pig command manipulates the data and creates a pipeline of data-processing commands.
As each step processes the data and all dependencies are defined as data dependencies, we call Pig a Dataflow language.
Finally the dump command prints the results to the screen.
When executed, the above script will print the following results.
As expressed in the script, it will print names of countries that have a GNI value greater than 2000$, sorted by GNI.
When we run the Pig script, Pig internally compiles Pig commands to MapReduce jobs in an optimized form and runs it in a MapReduce cluster.
Chaining MapReduce jobs using the MapReduce interface is cumbersome, as users will have to write code to pass the output from one job to the other and detect failures.
Pig translates such chaining to single-line command and handles the details internally.
For complex jobs, the resulting Pig script is easier to write and manage than MapReduce commands that do the same thing.
This recipe explains how to carry out join and sort operations with Pig.
The first dataset has the Gross National Income (GNI) per capita by country, and the second dataset has the exports of the country as a percentage of its gross domestic product.
This recipe will use Pig to process the dataset and create a list of countries that have more than 2000$ of gross national income per capita sorted by the GNI value, and then join them with the export dataset.
If you have not done it already, follow the earlier recipe and install Pig.
This section will describe how to use Pig to join two datasets.
The first and forth lines load the data from CSV files.
As described in the earlier recipe, PigStorage(',') asks pig to use ',' as the separator and assigns the values to the described fields in the command.
When we run the Pig script, Pig will convert the pig script to MapReduce jobs and execute them.
As described with the Pig Latin script, Pig will load the data from the CSV files, run transformation commands, and finally join the two data sets.
Installing Hive Just like with Pig, Hive also provides an alternative programming model to write data processing jobs.
It allows users to map their data into a relational model and process them through SQL-like commands.
Due to its SQL-style language, Hive is very natural for users who were doing data warehousing using relational databases.
Therefore, it is often used as a data warehousing tool.
You need a machine that has Java JDK 1.6 or later version installed.
Define the environment variables pointing to Hadoop and Hive distributions.
Delete the HADOOP_HOME/build folder to avoid a bug that will cause Hive to fail.
The preceding commands will set up Hive, and it will run using the Hadoop distribution as configured in the HADOOP_HOME.
Running a SQL-style query with Hive This recipe explains how you can use Hive to perform data processing operations using its SQL-style language.
In this recipe, we will use a data set that includes Human Development Report (HDR) by country.
If you have not done it already, please follow the previous recipe to install Hive.
This section depicts how to use Hive for filtering and sorting.
The table definition only creates the table layout; it does not put any data into the table.
Then, it uses the formats defined in the table definition to parse the data and load it to the table.
For example, the table definition in step 3 defines a table HDI that stores the data as a text file terminated with ',' (CSV format)
The input we provide for the LOAD command must follow the CSV format as per table definition.
If the command is successful, Hive will print the following information and finally print the results to the screen.
Number of reduce tasks is set to 0 since there's no reduce operator.
When we run the Hive, we first define a table and load the data from a file into the table.
It is worth noting that the table definition must match the input data file formats, and the LOAD command copies the files into the table's storage location without any change and then tries to parse the file according to the table definitions.
Once the data is loaded, we can use Hive commands to process the data using SQL-like syntax.
For example, the following command selects rows from the table that have a GNI value that is more than 2000:
Performing a join with Hive This recipe will show how to use Hive to perform joins across two datasets.
The first dataset is the Human Development Report by country.
This recipe will use Hive to process the dataset and create a list of countries that has more than 2000$ of gross national income per capita, and then join them with export dataset.
This recipe assumes that the earlier recipe has been performed.
Install Hive and follow the earlier recipe if you have not done so already.
This section demonstrates how to perform a join using Hive.
We will create a second table to join with the table we loaded in the earlier recipe.
We will load the data into the new table by running the following command with Hive.
As explained in the earlier recipe, this will move the data to the storage location for the table and parse the data according to the table definition.
Now we can join the two tables using Hive's SQL-like join command.
In order to change the average load for a reducer (in bytes):
When executed, Hive commands first define and load the second table and data.
Then it converts the join command into MapReduce job and carries out the join by running the MapReduce job.
Hive supports most SQL commands such as GROUP BY and ORDER BY, with the same semantics as SQL.
You can find more details about Hive commands from https://cwiki.
Installing Mahout Hadoop provides a framework for implementing large-scale data processing applications.
Often, the users implement their applications on MapReduce from scratch or write their applications using a higher-level programming model such as Pig or Hive.
However, implementing some of the algorithms using MapReduce can be very complex.
For example, algorithms such as collaborative filtering, clustering, and recommendations need complex code.
This is further agitated by the need to maximize parallel executions.
Mahout is an effort to implement well-known machine learning and data mining algorithms using MapReduce framework, so that the users can reuse them in their data processing without having to rewrite them from the scratch.
You can run and verify the Mahout installation by carrying out the following steps:
If all goes well, it will process and print out the clusters:
Mahout is a collection of MapReduce jobs and you can run them using the mahout command.
The preceding instructions installed and verified Mahout by running a K-means sample that comes with the Mahout distribution.
A clustering algorithm takes data points defined in an N-dimensional space, and groups them into multiple clusters considering the distance between those data points.
A cluster is a set of data points such that the distance between the data points inside the cluster is much less than the distance from data points within the cluster to data points outside the cluster.
In this recipe, we will use a data set that includes Human Development Report (HDR) by country.
This recipe will use K-means to cluster countries based on the HDR dimensions.
Follow the previous recipe if you have not already done so earlier.
This section demonstrates how to use Mahout K-means algorithm to process with a dataset.
Add the MAHOUT_HOME to the mahout.home property of build.xml file in the sample distribution.
Compile the sample by running the following command: >ant mahout-build.
The preceding sample shows how you can configure and use K-means implementation from Java.
When we run the code, it initializes the K-means MapReduce job and executes it using the MapReduce framework.
Visualizing K-means results This recipe explains how you can visualize the results of a K-means run.
This recipe assumes that you have followed the earlier recipe, have run K-means, and have access to the output of the K-means algorithm.
If you have not already done so, follow the previous recipe to run K-means.
This section demonstrates how to convert output of the K-means execution to GraphML and visualize it.
Running the following command will print the results into GraphML format, which is a standard representation of graphs.
From the layout window at the lower-left corner of the screen, use YufanHu's multilevel as the layout method, and click on Run.
Gephi will show a visualization of the graph that looks like the following:
We can use the clusterdump command of the Mahout to write them as a GraphML file, which is a standard representation of the graph.
Then, we used Gephi graph visualization software to visualize the resulting GraphML file.
Introduction This chapter discusses how we can process a dataset and understand its basic characteristics.
We will cover more complex methods like data mining, classification, and so on, in later chapters.
Given a dataset, generally there are multiple dimensions (for example, while processing HTTP access logs, names of the web page, the size of the web page, access time, and so on)
We can measure the mentioned analytics using one or more dimensions.
For example, we can group the data into multiple groups and calculate the mean value in each case.
This chapter will show how you can calculate basic analytics using a given dataset.
For recipes in this chapter, we will use two datasets:
Simple analytics using MapReduce Aggregative values (for example, Mean, Max, Min, standard deviation, and so on) provide the basic analytics about a dataset.
You may perform these calculations, either for the whole dataset or a part of the dataset.
In this recipe, we will use Hadoop to calculate the minimum, maximum, and average size of a file downloaded from the NASA servers, by processing the NASA weblog dataset.
As shown in the figure, mapper task will emit all message sizes under the key msgSize, and they are all sent to a one-reducer job.
Then the reducer will walk through all of the data and will calculate the aggregate values.
We will use HADOOP_HOME to refer to the Hadoop installation folder.
If you have not already done so, you should follow the recipe Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop from Chapter 1, Getting Hadoop Up and Running in a Cluster.
The following steps describe how to use MapReduce to calculate simple analytics about the weblog dataset:
Upload the data to HDFS by running the following commands from HADOOP_HOME.
If /data is already there, clean it up: >bin/hadoopdfs -mkdir /data.
Compile the source by running the ant build command from the CHAPTER_6_SRC folder.
Here the last token includes the size of the web page retrieved:
We will use the Java regular expressions' support to parse the log lines, and the Pattern.
Since most Hadoop jobs involve text processing, regular expressions are a very useful tool while writing Hadoop Jobs:
The map task receives each line in the log file as a different key-value pair.
It parses the lines using regular expressions and emits the file size against the key msgSize.
Then, Hadoop collects all values for the key and invokes the reducer.
Reducer walks through all the values and calculates the minimum, maximum, and mean file size of the file downloaded from the web server.
It is worth noting that by making the values available as an iterator, Hadoop gives the programmer a chance to process the data without storing them in memory.
You should therefore try to process values without storing them in memory whenever possible.
The main() method of the job looks similar to the WordCount example, except for the highlighted lines that has been changed to accommodate the input and output datatype changes:
You can learn more about Java regular expressions from the Java tutorial, http://docs.
Performing Group-By using MapReduce This recipe shows how we can use MapReduce to group data into simple groups and calculate the analytics for each group.
As shown in the figure, the mapper task groups the occurrence of each link under different keys.
Then, Hadoop sorts the keys and provides all values for a given key to a reducer, who will count the number of occurrences.
We will use the HADOOP_HOME to refer to the Hadoop installation folder.
If you have not already done so, you should follow the recipe Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop from Chapter 1, Getting Hadoop Up and Running in a Cluster.
The following steps show how we can group weblog data and calculate analytics.
Upload the data to HDFS by running the following commands from HADOOP_HOME.
If /data is already there, clean it up: >bin/hadoopdfs -mkdir /data.
Compile the source by running the ant build command from the CHAPTER_6_SRC folder.
As described in the earlier recipe, we will use regular expressions to parse HTTP logs.
Map task receives each line in the log file as a different key-value pair.
It parses the lines using regular expressions and emits the link as the key, and number one as the value.
Then, Hadoop collects all values for different keys (link) and invokes the reducer once for each link.
Then each Reducer counts the number of hits for each link.
The main() method of the job works similar to the earlier recipe.
Frequency distribution is the number of hits received by each URL sorted in the ascending order, by the number hits received by a URL.
We have already calculated the number of hits in the earlier recipe.
We will use the HADOOP_HOME to refer to Hadoop installation folder.
If you have not already done so, you should follow the recipe Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop from Chapter 1, Getting Hadoop Up and Running in a Cluster.
The following steps show how to calculate frequency distribution using MapReduce:
We will use the data from the previous recipe here.
So follow the recipe if you have not already done so.
The second recipe of this chapter calculated the number of hits received by each link, and the frequency distribution as a sorted list of those results in that recipe.
Therefore, let us sort the results of the second recipe.
MapReduce always sorts the key-value pairs emitted by the mappers by their keys before delivering them to the reducers.
Map task for the job will look like the following:
Map task receives each line in the log file as a different key-value pair.
It parses the lines using regular expressions and emits the number of hits as the key and the URL name as the value.
Hadoop sorts the key-value pairs emitted by the mapper before calling the reducers, and therefore the reducer will receive the pairs in sorted order.
Hence, it just has to emit them as they arrive.
The main() method of the job will work similar to the one in the earlier recipe.
Plotting the Hadoop results using GNU Plot Although Hadoop jobs can generate interesting analytics, making sense of those results and getting a detailed understanding about the data often require us to see the overall trends in the data.
The human eye is remarkably good at detecting patterns, and plotting the data often yields us a deeper understanding of the data.
Therefore, we often plot the results of Hadoop jobs using some plotting program.
This recipe explains how to use GNU Plot, which is a free and powerful plotting program, to plot Hadoop results.
If you have not done so, please follow the recipe.
The following steps show how to plot Hadoop job results using GNU Plot.
It will generate a file called freqdist.png, which will look like the following:
The preceding plot is plotted in log-log scale, and the first part of the distribution follows the zipf (power law) distribution, which is a common distribution seen in the web.
The last few most popular links have much higher rates than expected from a zipf distribution.
Discussion about more details on this distribution is out of scope of this book.
However, this plot demonstrates the kind of insights we can get by plotting the analytical results.
In most of the future recipes, we will use the GNU plot to plot and to analyze the results.
The following steps describe how plotting with GNU plot works:
The source for the plot will look like the following: set terminal png set output "freqdist.png"
This example uses PNG, but GNU plot supports many other terminals like SCREEN, PDF, EPS, and so on.
Next four lines define the axis labels and the title.
Next two lines define the scale of each axis, and this plot uses log scale for both.
Calculating histograms using MapReduce Another interesting view into a dataset is a histogram.
Histogram makes sense only under a continuous dimension (for example, access time and file size)
It groups the number of occurrences of some event into several groups in the dimension.
For example, in this recipe, if we take the access time from weblogs as the dimension, then we will group the access time by the hour.
Here the mapper calculates the hour of the day and emits the "hour of the day" and 1 as the key and value respectively.
Then each reducer receives all the occurrences of one hour of a day, and calculates the number of occurrences:
We will use the HADOOP_HOME variable to refer to the Hadoop installation folder.
If you have not already done so, you should follow the recipe Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop from Chapter 1, Getting Hadoop Up and Running in a Cluster.
The following steps show how to calculate and plot a Histogram:
Upload the data to HDFS by running the following commands from HADOOP_HOME.
If /data is already there, clean it up: >bin/hadoopdfs -mkdir /data.
Compile the source by running the ant build command from the CHAPTER_6_SRC folder.
It will generate a file called hitsbyHour.png, which will look like following:
As you can see from the figure, most of the access to NASA is at night, whereas there noontime.
As explained in the first recipe of this chapter, we will use regular expressions to parse the log file and extract the access time from the log files.
Map task receives each line in the log file as a different key-value pair.
It parses the lines using regular expressions and extracts the access time for each web page access.
Then, the mapper function extracts the hour of the day from the access time and emits the hour of the day and one as output of the mapper function.
Then, Hadoop collects all key-value pairs, sorts them, and then invokes the reducer once for each key.
Each reducer walks through the values and calculates the count of page accesses for each hour.
The main() method of the job looks similar to the WordCount example as described in the earlier recipe.
Calculating scatter plots using MapReduce Another useful tool while analyzing data is a Scatter plot.
We use Scatter plot to find the relationship between two measurements (dimensions)
For an example, this recipe analyzes the data to find the relationship between the size of the web pages and the number of hits received by the web page.
Here, the mapper calculates and emits the message size (rounded to 1024 bytes) as the key and one as the value.
Then the reducer calculates the number of occurrences for each message size:
We will use the HADOOP_HOME variable to refer to the Hadoop installation folder.
If you have not already done so, you should follow the recipe Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop from Chapter 1, Getting Hadoop Up and Running in a Cluster.
The following steps show how to use MapReduce to calculate the correlation between two datasets:
Upload the data to HDFS by running following commands from HADOOP_HOME.
If / data is already there, clean it up: >bin/hadoopdfs -mkdir /data.
Compile the source by running the ant build command from the CHAPTER_6_SRC folder.
Generate the plot by running the following command from HADOOP_HOME.
The plot shows a negative correlation between the number of hits and the size of the messages in the log scales, which also suggest a power law distribution.
The following code segment shows the code for the mapper.
Just like earlier recipes, we will use regular expressions to parse the log entries from log files:
Map task receives each line in the log file as a different key-value pair.
It parses the lines using regular expressions and emits the file size as 1024-byte blocks as the key and one as the value.
Then, Hadoop collects all key-value pairs, sorts them, and then invokes the reducer once for each key.
Each reducer walks through the values and calculates the count of page accesses for each file size.
The main() method of the job looks similar to the earlier recipes.
Parsing a complex dataset with Hadoop Datasets we parsed so far were simple, where each data item was contained in a single line.
Therefore, we were able to use Hadoop default parsing support to parse those datasets.
In this recipe, we will analyze Tomcat developer mailing list archives.
In the archive, each e-mail is composed of by multiple lines in the log file.
Therefore, we will write a Hadoop input formatter to process the e-mail archive.
This recipe parses the complex e-mail list archives, and finds the owner (person who started the thread) and the number of replies received by each e-mail thread.
Here the mapper emits the subject of the mail as key and the sender's e-mail address and date as the value.
Then Hadoop groups data by the e-mail subject and sends all the data related to that thread to the same reducer.
Then, the reducer calculates the owner of the thread it received, and the number of replies received by the thread.
We will use the HADOOP_HOME variable to refer to the Hadoop installation folder.
If you have not already done so, you should follow the recipe Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop from Chapter 1, Getting Hadoop Up and Running in a Cluster.
The following steps describe how to parse the Tomcat e-mail list dataset that has complex data format using Hadoop by writing an input formatter:
Upload the data to HDFS by running the following commands from HADOOP_HOME.
If /data is already there, clean it up: >bin/hadoopdfs -mkdir /data.
Compile the source by running the ant build command from the CHAPTER_6_SRC folder.
As explained before, this dataset has data items that span multiple lines.
Therefore, we have to write a custom data formatter to parse the data.
When the Hadoop job starts, it invokes the formatter to parse the input files.
We add a new formatter via the main() method as highlighted in the following code snippet:
As shown by the following code, the new formatter creates a record reader, which is used by Hadoop to read input keys and values:
Finally, the nextKeyValue() method parses the file, and gives users access to the key and values for this dataset.
Value has the from, subject, and date of each e-mail separated by a #
The following code snippet shows the map task source code:
The map task receives each line in the log file as a different key-value pair.
It parses the lines by breaking it by the #, and emits the subject as the key, and date and from as the value.
Then, Hadoop collects all key-value pairs, sorts them, and then invokes the reducer once for each key.
Since we use the e-mail subject as the key, each reducer will receive all the information about each e-mail thread.
Then, each reducer walks through all the e-mails and finds out who sent the first e-mail and how many replies have been received by each e-mail thread.
Joining two datasets using MapReduce As we have observed already, Hadoop is very good at reading through a dataset and calculating the analytics.
However, often we will have to merge two datasets for analyzing the data.
This recipe will explain how to join two datasets using Hadoop.
As an example, this recipe will use the Tomcat developer archives dataset.
A common belief among the open source community is that, the more a developer is involved with the community (for example, by replying to threads and helping others and so on), the more quickly he will receive a response to his queries.
In this recipe we will test this hypothesis using the Tomcat developer mailing list.
To test this hypothesis, we will run the MapReduce jobs as explained in the following figure:
We would start with e-mail archives in the MBOX format, and we will read the mail using the MBOX format class explained in the earlier recipe.
Then, the Hadoop job will receive the sender of the e-mail (from), e-mail subject, and the date the e-mail was sent, as inputs.
In the first job, mapper will emit the subject as key, and the sender's e-mail address and date as the value.
Then, the reducer step will receive all values with the same subject and it will output the subject as the key, and the owner and reply count as the value.
In the second job, the mapper step emits the sender's e-mail address as the key and one as the value.
Then, the reducer step will receive all the e-mails sent from the same address to the same reducer.
Using this data, each reducer will emit the e-mail address as the key and the number of e-mails sent from that e-mail address as the value.
Finally, the third job reads both the output from earlier jobs, joins the results, and emits the number of replies sent by each e-mail address and the number of replies received by each e-mail address as the output.
We will use the HADOOP_HOME variable to refer to the Hadoop installation folder.
If you have not already done so, you should follow the recipe Writing a WordCount MapReduce sample, bundling it and running it using standalone Hadoop from Chapter 1, Getting Hadoop Up and Running in a Cluster.
The following steps show how to use MapReduce to join two datasets:
If you have not already done so, run the previous recipe, which will set up the environment and run the first job as explained in the figure.
It will generate a file called sendreceive.png, which will look like following:
The graph confirms our hypothesis, and like before, the data approximately follows a power law distribution.
We have already discussed the working of the first job in the earlier recipe.
The following code snippet shows the map() function for the second job.
It receives the sender's e-mail, subject, and date separated by # as input, which parses the input and outputs the sender's e-mail as the key and the date the e-mail was sent, as the value:
The following code snippet shows the reduce() function for the second job.
Each reduce() function receives the time of all the e-mails sent by one sender.
The reducer counts the number of replies sent by each sender, and outputs the sender's name as the key and the replies sent, as the value:
The following code snippet shows the map() function for the third job.
It reads the outputs of the first and second jobs and writes them as the key-value pairs:
The following code snippet shows the reduce() function for the third job.
Since, both the output of the first and the second job has the same key, the sent replies and received replies for a given user are sent to the same reducer.
The reducer does some adjustments to remove self-replies, and outputs the sent replies and received replies as the key and value respectively of the reducer, thus joining the two datasets:
Here the final job is an example of using the MapReduce to join two datasets.
The idea is to send all the values that need to be joined under the same key to the same reducer, and join the data there.
Introduction MapReduce frameworks are well suited for large-scale search and indexing applications.
In fact, Google came up with the original MapReduce framework specifically to facilitate the various operations involved with web searching.
The Apache Hadoop project was started as a support project for the Apache Nutch search engine, before spawning off as a separate top-level project.
Given the size of the data, all these operations need to be scalable.
Typically, fetching is performed through web crawling, where the crawlers fetch a set of pages in the fetch queue, extract links from the fetched pages, add the extracted links back to the fetch queue, and repeat this process many times.
Indexing parses, organizes, and stores the fetched data in manner that is fast and efficient for querying and retrieval.
Search engines perform offline ranking of the documents based on algorithms such as PageRank and real-time ranking of the results based on the query.
In this chapter, we will introduce you to several tools that can be used with Apache Hadoop to perform large-scale searching and indexing.
Most of the text searching systems rely on inverted index to look up the set of documents that contains a given word or a term.
In this recipe, we are going to build a simple inverted index that computes a list of terms in the documents, the set of documents that contains each term, and the term frequency in each of the documents.
Retrieval of results from an inverted index can be as simple as returning the set of documents that contains the given terms or can involve much more complex operations such as returning the set of documents ordered based on a particular ranking.
You must have Apache Hadoop (preferably version 1.0.x) configured and installed to follow this recipe.
Apache Ant for the compiling and building the source code.
In the following steps, we will use a MapReduce program to build an inverted index for a text dataset.
Create a directory in HDFS and upload a text data set.
This data set should consist of one or more text files.
You can download the text versions of Project Gutenberg books by following the instructions given at the following link.
Make sure to provide the filetypes query parameter of the download request as txt.
You can use the unzipped text files as the text data set for this recipe.
Unzip the resources bundle for this chapter and change to that directory.
Compile the source by running ant build command from the unzipped directory.
Provide the HDFS directory where you uploaded the input data in step 2 as the first argument and provide a path to store the output as the second argument.
Check the output directory for the results by running the following command.
The output will consist of the term followed by a comma-separated list of filename and frequency.
We used the text outputting invert indexing MapReduce program in step 6 for the clarity of understanding the algorithm.
In the Map function, we first replace all the nonalphanumeric characters from the input text value before tokenizing it.
We use the getInputSplit() method of the MapContext to obtain a reference to InputSplit of assigned to the current Map task.
The InputSplits for this computation are instances of FileSplit due to the usage of FileInputFormat based InputFormat.
Then we use the getPath() method of FileSplit to obtain the path of the file containing the current split and extract the filename from it.
We use this extracted filename as the document ID when constructing the inverted index.
The reduce function receives IDs and frequencies of all the documents that contain the term (key) as the input.
The reduce function outputs the term and a list of document IDs and the number of occurrences of the term in each document as the output:
In the preceding model, we output a record for each word, generating a large amount of Map task to Reduce task intermediate data.
We use the following combiner to aggregate the terms emitted by the Map tasks, reducing the size and amount of Map to Reduce intermediate data transfer.
In the driver program, we set the Mapper, Reducer, and the combiner classes.
Also we specify both output value and the map output value properties as we use different value types for the Map tasks and the Reduce tasks.
MapFile is very useful when we need to random access records stored in a large SequenceFile.
We can utilize the MapFiles to store a secondary index in to our inverted index.
We can improve this indexing program by performing optimizations into such as filtering-stop words, substituting words with word stems, and storing more information about the context of the word, making the indexing a much more complex problem.
Luckily, there exist several open source indexing frameworks that we can use for the indexing purposes.
In this chapter we'll be using Apache Lucene-based Apache Solr and ElasticIndex for indexing purposes.
Web crawling is the process of visiting and downloading all or a subset of web pages on the Internet.
Although the concept of crawling and implementing a simple crawler sounds simple, building a full-fledged crawler takes great deal of work.
A full-fledged crawler that needs to be distributed has to obey the best practices such as not overloading servers, follow robots.
Apache Nutch is an open source search engine that provides a highly scalable crawler.
Apache Nutch offers features such as politeness, robustness, and scalability.
In this recipe, we are going to use Apache Nutch in the standalone mode for small-scale, intra-domain web crawling.
Nutch standalone executes these applications using the Hadoop the local mode.
Install Apache Ant and add it to the PATH environmental variable.
The following steps show you how to use Apache Nutch in standalone mode for small scale web crawling.
Apache Nutch standalone mode uses the HyperSQL database as the default data storage.
The following database uses data/nutchdb.* as the database files and uses nutchdb as the database alias name.
Go to the runtime/local directory and run the bin/nutch command to verify the Nutch installation.
A successful installation would print out the list of Nutch commands, shown as follows: > cd runtime/local.
Create a directory named urls and create a file named seed.txt inside that directory.
Seed URLs are used to start the crawling and would be pages that are crawled first.
We use http://apache.org as the seed URL in the following example: > mkdir urls.
Inject the seed URLs in to the Nutch database using the following command: > bin/nutch inject urls/
Use the following command to verify the injection of the seeds to the Nutch database.
You can use this command in the later cycles as well to get an idea about the number of web page entries in your database.
Use the following command to generate a fetch list from the injected seed URLs.
This will prepare list of web pages to be fetched in the first cycle of the crawling.
Generation will assign a batch ID to the current generated fetch list, which can be used in the subsequent commands.
This step performs the actual fetching of the web pages.
The –all parameter is used to inform Nutch to fetch all the generated batches.
Use the following command to parse and to extract the useful data from fetched web pages, such as the text content of the pages, metadata of the pages, and the set of pages linked from the fetched pages.
We call the set of pages linked from a fetched page as the out-links of that particular fetched page.
The out-links data will be used to discover new pages to fetch as well as to rank pages using link analysis algorithms such as PageRank.
Execute the following command to update the Nutch database with the data extracted in the preceding step.
This step includes updating the contents of the fetched pages as well as adding new entries of the pages discovered through the links contained in the fetched pages.
Execute the following command to generate a new fetch list using the information from the previously fetched data.
The topN parameter limits the number of URLs generated for the next fetch cycle.
Fetch the new list, parse it, and update the database.
The Whole web crawling with Apache Nutch using a Hadoop/HBase cluster and Indexing and searching web documents using Apache Solr recipes of this chapter.
Apache Solr is an open source search platform that is part of the Apache Lucene project.
It supports powerful full-text search, hit highlighting, faceted search, dynamic clustering, database integration, rich document handling (for example, Word and PDF), and geospatial search.
In this recipe, we are going to index the web pages crawled by Apache Nutch for use by Apache Solr and use Apache Solr to search through those web pages.
Crawl a set of web pages using Apache Nutch by following the Intra-domain crawling using Apache Nutch recipe.
How to do it The following steps show you how to index and search your crawled web pages dataset:
We use Apache Solr 4.0 for the examples in this chapter.
Start Solr by executing the  following command from the $SOLR_HOME/example directory.
This command pushes the data crawled by Nutch in to Solr through the Solr web service interface.
Enter a search term in the q textbox and click on Execute Query.
You can also issue your search queries directly using the HTTP GET requests.
How it works Apache Solr is built using the Apache Lucene text search library.
Apache Solr adds many features on top of Apache Lucene and provides a text search web application that works out of the box.
The preceding steps deploy Apache Solr and import the data crawled by Nutch in to the deployed Solr instance.
The metadata about the documents we plan to index and search using Solr needs to be specified through the Solr schema.xml file.
The Solr schema file should define the data fields in our documents and how these data fields should be processed by Solr.
More information about the Solr schema file can be found from http://wiki.apache.org/ solr/SchemaXml.
Apache Nutch integrates Apache Gora to add support for different backend data stores.
In this recipe, we are going to configure Apache HBase as the backend data storage for Apache Nutch.
Similarly, it is possible to plug in data stores such as RDBMS databases, Cassandra and others through Gora.
Install Apache Ant and add it to the PATH environmental variable.
How to do it The following steps show you how to configure Apache HBase local mode as the backend data store for Apache Nutch to store the crawled data.
Create two directories to store the HDFS data and Zookeeper data.
Refer to the Installing HBase recipe in Chapter 5, Hadoop Ecosystem, for more information on how to install HBase in the local mode.
In case you have not downloaded Apache Nutch for the earlier recipes in this chapter, download Nutch from the http://nutch.apache.org and extract it.
Start the Hbase shell and issue the following commands to view the fetched data.
Follow the steps in the Indexing and searching web documents using Apache Solr to index recipe and search the fetched data using Apache Solr.
The preceding steps configure and run Apache Nutch using Apache HBase as the storage backend.
When configured, Nutch stores the fetched web page data and other metadata in HBase tables.
However, as shown in the Whole web crawling with Apache Nutch using a Hadoop/HBase cluster recipe of this chapter, Nutch can be used with a distributed HBase deployment as well.
Usage of HBase as the backend data store provides more scalability and performance for Nutch crawling.
We assume you already have your Hadoop cluster (version 1.0.x) deployed.
If not, refer to the Setting Hadoop in a distributed cluster environment recipe of Chapter 1, Getting Hadoop up and running in a Cluster, to configure and deploy a Hadoop cluster.
How to do it The following steps show you how to deploy a distributed Apache HBase cluster on top of an Apache Hadoop cluster:
Open the HBase UI at http://localhost:60010 and monitor the HBase installation.
Start the HBase shell and execute the following commands to test the HBase deployment.
If the preceding command fails, check the logs in the $HBASE_HOME/ logs directory to identify the exact issue.
Hbase is very sensitive to the contents of the /etc/hosts file.
Fixing the /etc/host file would solve most of the HBase deployment errors.
The preceding steps configure and run the Apache HBase in the distributed mode.
HBase distributed mode stores the actual data of the HBase tables in the HDFS, taking advantage of the distributed and fault tolerant nature of HDFS.
In order to run HBase in the distributed mode, we have to configure the HDFS NameNode and the path to store the HBase data using the hbase.rootdir property in the hbase-site.xml.
Whole web crawling with Apache Nutch using a Hadoop/HBase cluster.
Crawling large amount of web documents can be done efficiently by utilizing the power of a MapReduce cluster.
If not, refer to the Deploying HBase on a Hadoop cluster recipe of this chapter to configure and deploy an HBase cluster on a Hadoop cluster.
How to do it The following steps show you how to use Apache Nutch with a Hadoop MapReduce cluster and a HBase data store to perform large-scale web crawling.
Add the $HADOOP_HOME/bin directory to the PATH environment variable of your machine.
In case you have not downloaded Apache Nutch for the earlier recipes in this chapter, download Nutch from http://nutch.apache.org and extract it.
Create a directory in HDFS to upload the seed urls.
Create a text file with the seed URLs for the crawl.
Upload the seed URLs file to the directory created in the above step.
JobTracker node to inject the seed URLs to the Nutch database and to generate the initial fetch list.
Repeat the commands in step 12 as many times as needed to crawl the desired number of pages or the desired depth.
Follow the Indexing and searching fetched web documents using Apache Solr recipe to index the fetched data using Apache Solr.
How it works All the Nutch operations we used in this recipe, including fetching and parsing, are implemented as MapReduce programs.
These MapReduce programs utilize the Hadoop cluster to perform the Nutch operations in a distributed manner and use the HBase to store the data across the HDFS cluster.
You can monitor these MapReduce computations through the monitoring UI (http://jobtracker_ip:50030) of your Hadoop cluster.
The bin/nutch script uses this job file to submit the MapReduce computations to Hadoop.
The Intra-domain crawling using Apache Nutch recipe of this chapter.
ElasticSearch for indexing and searching ElasticSearch (http://www.elasticsearch.org/) is an Apache 2.0 licensed open source search solution built on top of Apache Lucene.
ElasticSearch supports distributed deployments, by breaking down an index in to shards and by distributing the shards across the nodes in the cluster.
While both ElasticSearch and Apach Solr use Apache Lucene as the core search engine, ElasticSearch aims to provide a more scalable and a distributed solution that is better suited for the cloud environments than Apache Solr.
Install Apache Nutch and crawl some web pages as per the Whole web crawling with Apache Nutch using an existing Hadoop/HBase cluster recipe or the Configuring Apache HBase local mode as the backend data store for Apache Nutch recipe.
Make sure the backend Hbase (or HyperSQL) data store for Nutch is still available.
How to do it The following steps show you how to index and search the data crawled by Nutch using ElasticSearch.
Go to the extracted ElasticSearch directory and execute the following command to start the ElasticSearch server in the foreground.
Run the following command in a new console to verify your installation.
Execute the following command to index the data crawled by Nutch in to the ElasticSearch server.
How it works Similar to Apache Solr, ElasticSearch too is built using the Apache Lucene text search library.
In the preceding steps we export the data crawled by Nutch in to an instance of ElasticSearch for indexing and searching purposes.
We add the –f switch to force the ElasticSearch to run in the foreground to make the development and testing process easier.
You can also install ElasticSearch as a service as well.
We use the ElasticIndex job of Nutch to import the data crawled by Nutch into the ElasticSearch server.
The cluster name is used for auto-discovery purposes and should be unique for each ElasticSearch deployment in a single network.
The number of links to a particular web page from other pages, the number of in-links, is widely considered a good metric to measure the popularity or the importance of a web page.
In fact, the number of in-links to a web page and the importance of the sources of those links have become integral components of most of the popular link analysis algorithms such as PageRank introduced by Google.
In this recipe, we are going to extract the in-links information from a set of web pages fetched by Apache Nutch and stored in Apache HBase backend data store.
In our MapReduce program, we first retrieve the out-links information for the set of web pages stored in the Nutch HBase data store and then use that information to calculate the in-links graph for this set of web pages.
The calculated in-link graph will contain only the link information from the fetched subset of the web graph only.
Follow the Whole web crawling with Apache Nutch using an existing Hadoop/HBase cluster or the Configuring Apache HBase local mode as the backend data store for Apache Nutch recipe and crawl a set of web pages using Apache Nutch to the backend HBase data store.
This recipe requires Apache Ant for building the source code.
How to do it The following steps show you how to extract an out-links graph from the web pages stored in Nutch HBase data store and how to calculate the in-links graph using that extracted out-links graph.
Create an HBase table with the name linkdata and a column family named il.
Unzip the source package for this chapter and compile it by executing ant build from the Chapter 7 source directory.
Run the Hadoop program by issuing the following command from $HADOOP_HOME.
Start the HBase shell and scan the linkdata table using the following command to check the output of the MapReduce program: > bin/hbase shell.
How it works As we are going to use HBase to read input as well as to write the output, we will use the HBase TableMapper and TableReducer helper classes to implement our MapReduce application.
We will configure the TableMapper and the TableReducer using the utility methods given in the TableMapReduceUtil class.
The Scan object is used to specify the criteria to be used by the mapper when reading the input data from the HBase data store.
The map implementation receives the HBase rows as the input records.
In our implementation, each of the rows corresponds to a fetched web page.
The input key to the Map function consists of the web page's URL and the value consists of the web pages linked from this particular web page.
Map function emits a record for each of the linked web pages, where the key of a Map output record is the URL of the linked page and the value of a Map output record is the input key to the Map function (the URL of the current processing web page)
The reduce implementation receives a web page URL as the key and a list of web pages that contain links to that web page (provided in the key) as the values.
The reduce function stores these data in to an HBase table.
Introduction This chapter discusses how we can use Hadoop for more complex use cases such as classifying a dataset, making recommendations, or finding relationships between items.
In this recipe, we will apply these and other techniques using MapReduce.
For recipes in this chapter, we will use the Amazon product co-purchasing network metadata dataset available from http://snap.stanford.edu/data/amazon-meta.html.
Content-based recommendations Recommendations are to make suggestions to someone about things that might be of interest to him.
For example, we would recommend a good book to a friend who has similar interests.
We often find use cases for recommendations in online retail.
For example, when you browse for a product, Amazon would suggest other products that were also bought by users who bought this item.
For example, online retail sites such as Amazon have a very large collection of items.
Although books are found classified into several categories, often each category has too many to browse one after the other.
Recommendations make the user's life easy by helping them find the best products for their tastes.
As recommendations make a change of a high sale, online retailers are very much interested about recommendation algorithms.
For example, you could use categories, content similarities, and so on to identify products that are similar and recommend them to the users who have already brought one.
For example, if the same user gave a high rating to the two products, we can argue that there is some similarity between those two products.
We will look at an instance of this in the next recipe.
This recipe uses dataset collected from Amazon about products to make content-based recommendations.
In the dataset, each product has a list of similar items provided pre-calculated by Amazon.
In this recipe, we will use that data to make recommendations.
The following steps depict how to prepare for this recipe.
This recipe assumes that you have followed Chapter 1, Getting Hadoop up and running in a Cluster, and have installed Hadoop.
We will use the HADOOP_HOME to refer to the Hadoop installation directory.
Start Hadoop by following the instructions in Chapter 1, Getting Hadoop up and running in a Cluster.
This recipe assumes you are aware of how Hadoop processing works.
If you have not already done so, you should follow the Writing the WordCount MapReduce sample, bundling it and running it using standalone Hadoop recipe in Chapter 1, Getting Hadoop up and running in a Cluster.
We will use HADOOP_HOME to refer to the Hadoop installation directory.
The following steps describe how to run the content-based recommendation recipe.
Upload the data to HDFS by running following commands from HADOOP_HOME.
Compile the source by running the ant build command from the CHAPTER_8_SRC directory.
Run the first Map reduce job through the following command from HADOOP_HOME.
Run the second Map reduce job through the following command from HADOOP_HOME.
You will see that it will print the results as follows.
Each line of the result contains the customer ID and list of product recommendations for that customer.
The following listing shows an entry for one product from the dataset.
Here, each data entry includes an ID, title, categorization, similar items to this item, and information about users who has brought the item.
We have written a new Hadoop data format to read and parse the Amazon product data, and the data format works similar to the format we have written in the Simple Analytics using MapReduce recipe in Chapter 6, Analytics.
Amazon data formatter will parse the dataset and emit the data about each Amazon product as key-value pairs to the map function.
The map task of the first MapReduce job receives data about each product in the logfile as a different key-value pair.
When the map task receives the product data, it emits the customer ID as the key and the product information as the value for each customer who has bought the product.
Hadoop collects all values for the key, and invokes the reducer once for each key.
There will be a reducer task for each customer, and each reducer task will receive all products that have been brought by a customer.
The reducer emits the list of items brought by each customer, thus building a customer profile.
For limiting the size of the dataset, the reducer will not emit any customer who has brought less than five products.
The second MapReduce job uses the data generated from the first MapReduce tasks to make recommendations for each customer.
The map task receives data about each customer as the input, and the MapReduce tasks make recommendations using the following three steps:
Each product (item) data from Amazon includes similar items to that item.
Given a customer, first the map task creates a list of all similar items for each item that customer has brought.
Then the map task removes any item from the list of similar items that has already brought by the same customer.
Hierarchical clustering Many operations such as recommendations and finding relationships use clustering as an integral component.
Clustering groups a dataset into several groups using one or more measurements such that the items in the same cluster are rather similar and items in different clusters are more different.
For example, given a set of living addresses of patients, clustering can group them into several groups where members of each group are close to each other so that doctors can visit them in most optimal manner.
There are many clustering algorithms; each has different costs and accuracy.
Hierarchical clustering is one of the most basic and most accurate algorithms.
First, hierarchical clustering assigns each data point to its own cluster.
It calculates the distance between all cluster pairs and merges the two clusters that are closest to each other.
It repeats the process until only one cluster is left.
At each repetition, the algorithm records each cluster pair it has merged.
This merging history provides a tree that combines the clusters into larger and larger groups close to the root.
Users may take a cut at some place in the tree based on the number of clusters they need.
In other words, the algorithm would take about Cn2log(n) time for input of size n and a constant C.
Hence, often it is too complex to be used with a large dataset.
However, it often serves as the basis for many other clustering algorithms.
In this recipe, we will use hierarchical clustering to cluster a sample drawn from the Amazon dataset.
It is worth noting that it is not a MapReduce algorithm, but we will use its results in the MapReduce algorithm in the next recipe.
This recipe assumes that you have followed the first recipe and extracted the Amazon product data.
The following steps describe how to run the hierarchical clustering sample:
Copy the results of the first MapReduce job of the last recipe to the local machine.
The algorithm will generate a file called clusters.data that includes clusters.
You can find the information about clusters from the clusters.data file created in the local directory, which will have the centroid of each cluster.
When executed, the code would read through the input data file and load data for 1000 Amazon customers randomly selected from the input file and perform hierarchical clustering on those customers.
Hierarchical clustering starts by placing each customer in its own cluster.
Then it finds the two clusters that are close to each other and merges them into one cluster.
Then it recalculates the distance between the new cluster and the old clusters, and repeats the process until it is left with a single cluster.
With the Jaccard distance, if two customers have brought the same item and have given similar reviews to those items, then Jaccard distance will be small.
On the other hand, if they have given different reviews, the distance between them will be high.
A naive implementation of the algorithm will recalculate all distances between clusters every time two clusters are merged, and resulting algorithm will have O(n3) computational complexity.
In other words, the algorithm will take Cn3 amount of time to run with input of size n for some constant C.
However, by remembering distances between clusters and only calculating distance from new clusters, the resulting implementation will have O(n2log(n)) complexity.
The following code listing shows the implementation of the algorithm.
Finally, the program will output the centroid for each cluster.
The centroid is the point within the cluster that has the minimal value of the sum of distances to all other points in the cluster.
Among other alternative distance measures are Cosine distance, Edit distance, and Hamming distance.
Also you can learn more about hierarchical clustering from the same book.
Clustering an Amazon sales dataset As explained in the earlier recipe, although very accurate, hierarchical clustering has a time complexity of O(n2log(n)), and therefore is not applicable to large datasets.
This recipe describes how to apply the GRGPFClustering algorithm to Amazon sales dataset.
Therefore, we say that the data in a non-Euclidian space.
GRGPFClustering algorithm is a highly scalable algorithm applicable to a non-Euclidian space.
It works by first taking a small random sample of the data and clustering it using an algorithm such as hierarchical clustering, and then using those clusters to cluster a large dataset without keeping all the data in memory.
The following steps describe how to ready to cluster the Amazon dataset.
This assumes that you have followed Chapter 1, Getting Hadoop up and running in a Cluster and have installed Hadoop.
We will use the HADOOP_HOME to refer to the Hadoop installation directory.
Start Hadoop following the instructions in Chapter 1, Getting Hadoop up and running in a Cluster.
This recipe assumes you are aware of how Hadoop processing works.
If you have not already done so, you should follow the Writing the WordCount MapReduce sample, bundling it and running it using standalone Hadoop recipe in Chapter 1, Getting Hadoop up and running in a Cluster.
This assumes that you have followed the Content-based recommendations recipe of this chapter and have extracted the Amazon product data.
The following steps describe how to get cluster the Amazon dataset.
Change the hadoop.home property in the build.xml file under CHAPTER_8_SRC to point to your Hadoop installation directory.
Compile the source by running the ant build command from the CHAPTER_8_SRC directory.
Run the MapReduce job using the following command from HADOOP_HOME.
It will use the output generated by the first MapReduce task of the first recipe.
You will see that it will print the results as following.
Here the key indicates the cluster ID and the rest of the results show customer details of the customers assigned to the cluster.
As the figure depicts, this tasks has a MapReduce task.
When initialized, the MapReduce job will load the information about the clusters calculated in the earlier recipe and use those clusters to assign the rest of the dataset to clusters.
The Map task receives each line in the logfile that contains information about Amazon customer as a different key-value pair.
Then, the map task compares the information about the customer against each of the cluster's centroids, and assigns each customer to the cluster that is closest to that customer.
Then, it emits the name of the cluster as the key and information about the customer as the value.
Then, Hadoop collects all values for the different keys (clusters) and invokes the reducer once for each cluster.
The main method of the job works similar to the earlier recipes.
It is possible to improve the results by recalculating the cluster centroids in the reducer, splitting any clusters that have customers that are too far apart from each others, and rerunning the GRGPF algorithm with new clusters.
This recipe will use collaborative filtering to make recommendations for customers in the Amazon dataset.
As described in the introduction, collaborative filtering uses sales activities about a given user that is common with other users to deduce the best product recommendations for the first user.
To implement collaborative filtering, we will cluster the users based on their behavior, and use items brought by members of a cluster to find recommendations of each member of the cluster.
We will use the clusters calculated in the earlier recipe.
The following steps show how to prepare to run the collaborative filtering example:
We will use the HADOOP_HOME to refer to Hadoop installation directory.
Start Hadoop by following the instructions in the first chapter.
This recipe assumes you are aware of how Hadoop processing works.
If you have not already done so, you should follow the Writing a WordCount MapReduce Sample, Bundling it and running it using standalone Hadoop recipe from the first chapter.
This will use the results from the previous recipe of this chapter.
The following steps show how to prepare to run the collaborative filtering example:
You will see that it will print the results as following.
Here the key is the customer ID and the value is the list of recommendations for that customer.
Collaborative filtering uses the behavior of the users to decide on the best recommendations for each user.
For that process, the recipe will use the following steps:
Group customers into several groups such that similar customers are in the same group and different customers are in different groups.
For making recommendations for each customer, we have looked at the other members in the same group and used the items bought by those members assuming that similar users would like to buy similar products.
When there are many recommendations, we have used the Amazon sales rank to select the recommendations.
For grouping the customers, we can use clustering techniques used in the earlier recipes.
As a measure of the distance between customers we have used the distance measure introduced in the second recipe of this chapter that uses customer co-purchase information to decide on the similarity between customers.
We have already clustered the customers to different groups in the earlier recipe.
The map task for the job will look like the following:
The map task receives each line in the logfile as a different key-value pair.
It parses the lines using regular expressions and emits cluster ID as the key and the customer information as the value.
Hadoop will group different customer information emitted against the same customer ID and call the reducer once for each customer ID.
Then, the reducer walks through the customers assigned to this cluster and creates a list of items as potential recommendations sorted by Amazon sales rank.
Then it makes final recommendations for a given user by removing any items that he has already brought.
The main method of the job will work similar to the earlier recipes.
Classification using Naive Bayes Classifier A classifier assigns inputs into one of the N classes based on some properties (features) of inputs.
Classifiers have widespread applications such as e-mail spam filtering, finding most promising products, selecting customers for closer interactions, and taking decisions in machine learning situations, and so on.
Let us explore how to implement a classifier using a large dataset.
For instance, a spam filter will assign each e-mail to one of the two clusters—spam mail or not a spam mail.
One of the simplest, but effective algorithms is Naive Bayesian classifier that uses the Bayes theorem.
You can find more information about Bayesian classifier from http://en.wikipedia.org/wiki/Naive_Bayes_classifier and Bayes theorem from http://betterexplained.com/articles/an-intuitiveand-short-explanation-of-bayes-theorem/
For this recipe, we will also focus on the Amazon purchase dataset as before.
We will look at several features about a product such as number of reviews received, amount of positive ratings, and number of known similar items to identify a product as potential to be within the first 10,000 sales rank.
The following steps describe how to prepare to run Naive Bayesian example:
We will use the HADOOP_HOME to refer to Hadoop installation directory.
Start Hadoop by following the instructions in the first chapter.
This recipe assumes you are aware of how Hadoop processing works.
If you have not already done so, you should follow the Writing the WordCount MapReduce sample, bundling it and running it using standalone Hadoop recipe from Chapter 1, Getting Hadoop up and running in a Cluster.
The following steps describe how to run Naive Bayesian example.
Upload the data to HDFS by running the following commands from HADOOP_HOME.
If the /data directory is already there, clean it up.
Compile the source by running the ant build command from the CHAPTER_8_SRC directory.
Run the MapReduce job through the following command from HADOOP_HOME.
The goal of this recipe is to look at some properties of a product and predict whether it will fall under the first 10,000 products at Amazon by the sales rank.
We call these properties features, and for this sample we will focus on the following three properties:
The first step is to understanding the equation for Naive Bayes Classifier.
The following equation provides the probability that the product will have a sales rank less than 1000 given three independent events Ap, Bp, and Cp.
Then we will use those with above formula to classify a given product.
The mapper function walks thorugh each product and for each, it evaluates the features.
If the feature evaluates to be true, it emits the feature name as the key and notifies whether the product is within the first 10,000 products as the value.
Then each reduce job receives all values for which the feature is true, and it calculates the probability that given the feature is true, the product is within the first 10,000 products in the sales rank.
When you run the classifier testing the logic, it will load the data generated by the MapReduce job and classify 1000 randomly selected products.
Advertisements have become a major medium of revenue for the Web.
It is a billion-dollar business, and the source of the most Google revenue.
Further, it has made it possible for companies such as Google to run their main services free of cost, while collecting their revenue through advertisements.
Let us explore how we can implement a simple "Adwords" style algorithm using MapReduce.
When a user searches for a document with given keywords, the system will choose one or more advertisements among the bids for these keywords.
Advertisers will pay only if a user clicks on the advertisement.
Adwords problem is to show advertisements such that it will maximize revenue.
There are several factors in the play while designing such a solution:
Hence, we want to show advertisements that are more likely to be clicked often.
We measure this as fraction of time an advertisement was clicked as oppose to how many times it was shown.
In this recipe, we will implement a simplified version of the Adwords balance algorithm that can be used in such situations.
For simplicity, we will assume that advertisers only bid on single words.
Also, as we cannot find a real bid dataset, we will generate a sample bid dataset.
Assume that you are to support a keyword-based advertisement using the Amazon dataset.
Here, we assume that the keywords that are found in the title of the products with higher sales rank will have a better click-through rate.
We would use Adword balance algorithm, which uses the following formula.
The following formula assigns priority based on the fraction of unspent budget owned by each advertiser, bid value, and click-through rate.
The following steps describe how to prepare to run the Adwords sample:
This assumes that you have followed Chapter 1, Getting Hadoop up and running in a Cluster and have installed Hadoop.
We will use HADOOP_HOME to refer to Hadoop installation directory.
Start Hadoop by following the instructions in Chapter 1, Getting Hadoop up and running in a Cluster.
This recipe assumes you are aware of how Hadoop processing works.
If you have not already done so, you should follow the Writing the WordCount MapReduce sample, bundling it and running it using standalone Hadoop recipe from Chapter 1, Getting Hadoop up and running in a Cluster.
The following steps describe how to run the Adwords sample:
Upload the data to HDFS by running following commands from HADOOP_HOME.
If the /data directory is already there, clean it up.
This dataset is large, and might take a long time if you try to run it with a single computer.
You might want to only upload the first 50,000 lines or so of the dataset if you need the sample to run quickly.
Run the Map reduce job through the following command from HADOOP_HOME.
Generate a bid dataset by running the following command from HADOOP_HOME.
It parses the Amazon dataset using the Amazon data format, and for each word in each product title, it emits the word and the sales ranks of that product.
Then, Hadoop sorts the emitted key-value pairs by keys and invokes the reducer once for each key passing the values emitted against that key.
As shown in the following code, the reducer calculates an approximation for the click rate using sales ranks emitted against the key.
It would read the keywords generated by the earlier recipe and generate a random bid dataset.
Then we will use the second MapReduce task to merge the bid data set with click-through rate and generate a dataset that has bids' information sorted against the keyword.
The mapper function reads both the bid data set and click-through rate datasets and emits both types of data against the keyword.
Then, each reducer receives all bids and associated click-through data for each keyword.
Then the reducer merges the data and emits a list of bids against each keyword.
Finally, the Adwords assigner loads the bids data and stores them against the keywords in the memory.
Given a keyword, the Adwords assigner finds the bid that has maximum value for the following equation and selects a bid among all the bids for advertisement:
The preceding recipe assumes that Adwords assigner can load all the data in the memory to make advertisements assignment decisions.
However, if the dataset is big, we can partition the dataset among multiple computers by keywords (for example, assign keywords that start with "A-D" to the first computer and so on)
This recipe assumes that users only bid for single words.
However, to support multiple keyword bids, we would need to combine the click-through rates, and the rest of the algorithm can proceed as before.
More information about online advertisement can be found from the book, Mining of Massive Datasets, by Anand Rajaraman and Jeffrey D.
Introduction Hadoop MapReduce together with the supportive set of projects makes for a good framework choice to process large text datasets and to perform ETL-type operations.
In this chapter, we'll be exploring how to use Hadoop Streaming to perform data preprocessing operations such as data extraction, format conversion, and de-duplication.
We'll also use HBase as the data store to load the data and will explore mechanisms to perform large data loads to HBase with minimal overhead.
Towards the end of the chapter, we'll look in at performing text analytics operations using the Apache Mahout algorithms.
Data preprocessing (extract, clean, and format conversion) using Hadoop Streaming and Python.
Data preprocessing is an important and often required component in data analytics.
Data preprocessing becomes even more important when consuming unstructured text data generated from multiple sources.
Data preprocessing steps include operations such as cleaning the data, extracting important features from data, removing duplicate items from the datasets, converting data formats, and many more.
Hadoop MapReduce provides an ideal environment to perform these tasks in parallel with massive datasets.
Apart from the ability to implement Java MapReduce programs, Pig, and Hive scripts to preprocess these data, Hadoop also provides several useful tools and features that we can utilize to perform these data preprocessing operations.
One such feature is the support of different InputFormat classes, providing us with the ability to support proprietary data formats by implementing custom InputFormat classes.
Another feature is the Hadoop Streaming feature, which allows us to use our favorite scripting languages to perform the actual data cleansing and extraction, while Hadoop parallelizes the computation to hundreds of compute and storage resources.
In this recipe, we are going to use Hadoop Streaming with a Python script-based mapper to perform data extraction and format conversion.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root folder.
Install Python on your Hadoop compute nodes, if Python is not already installed.
The following steps show you how to clean and extract data from the 20news dataset and store the data as a tab-separated value (TSV) file:
Hadoop uses the default TextInputFormat class as the input specification for the preceding computation.
Usage of the TextInputFormat class generates a map task for each file in the input dataset and generates a map input record for each line.
Hadoop Streaming provides the input to the map application through the standard input.
The previous Python code reads the input lines from the standard input until the end of file is reached.
We parse the headers of the news group file till we encounter the empty line demarcating the headers from the message contents.
The message content will be read into a list line by line.
The preceding code segment merges the message content to a single string and constructs the output value of the Streaming application as a tab-delimited set of selected headers followed by the message content.
The output key value is the Message-ID header extracted from the input file.
The output is written to the standard output by using a tab to delimit the key and the value.
It is a good practice to store the data as SequenceFiles after the first pass of the input data, as SequenceFiles take less space and support compression.
However, for the preceding command to work, any writable classes that are used in the SequenceFile format should be available in the Hadoop classpath.
Data de-duplication using Hadoop Streaming Often, the datasets contain duplicate items that need to be eliminated to ensure the accuracy of the results.
In this recipe, we use Hadoop to remove the duplicate mail records in the 20news dataset.
These duplicate records are due to the user's cross-posting the same message to multiple news boards.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root folder.
Install Python on your Hadoop compute nodes, if Python is not already installed.
The following steps show how to remove duplicate mails, due to cross-posting across the lists, from the 20news dataset:
Mapper Python script outputs the message ID as the key.
We use the message ID to identify the duplicated messages that are a result of cross-posting across different newsgroups.
Hadoop Streaming provides the Reducer input records of the each key group line by line to the Streaming reducer application through the standard input.
However, Hadoop Streaming does not have a mechanism to distinguish when it starts to feed records of a new key to the process.
The Streaming reducer applications need to keep track of the input key to identify new groups.
Since we output the mapper results using the MessageID header as the key, the Reducer input gets grouped by MessageID.
Any group with more than one value (that is, message) per MessageID contains duplicates.
In the previous script, we use only the first value (message) of the record group and discard the others, which are the duplicate.
Loading large datasets to an Apache HBase data store using importtsv and bulkload tools.
Apache HBase data store is very useful when storing large-scale data in a semi-structured manner, so that they can be used for further processing using Hadoop MapReduce programs or to provide a random access data storage for client applications.
In this recipe, we are going to import a large text dataset to HBase using the importtsv and bulkload tools.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root folder.
Refer to the Deploying HBase on a Hadoop cluster recipe in this chapter for more information.
Export the HBASE_HOME environment variable to point to your HBase installation root folder.
Install Python on your Hadoop compute nodes, if Python is not already installed.
Follow the Data extract, cleaning, and format conversion using Hadoop Streaming and Python recipe to perform the preprocessing of data for this recipe.
Older versions of the importtsv (used in the next step) command can handle only a single column family.
The following are the steps to load the 20news dataset to an HBase table using the bulkload feature:
Use the following command to generate an HBase bulkload datafile:
Use the count and scan commands of the HBase Shell to verify the contents of the table: hbase(main):010:0> count 'datatsvbulk'
We import the tab-separated dataset generated by the Streaming MapReduce computations to Hbase using the importtsv tool.
The importtsv tool requires the data to have no other tab characters except for the tab characters separating the data fields.
Hence, we remove any tab characters in the input data using the following snippet of the Python script:
The importtsv tool supports loading data to HBase directly using the Put operations as well as by generating the HBase internal HFiles.
The following command loads the data to HBase directly using the Put operations.
Our generated dataset contains a key and four fields in the values.
This mapping consists of listing the respective table column names in the order of the tab-separated data fields in the input dataset:
We can use the following command to generate HBase HFiles for the dataset.
These HFiles can be directly loaded to HBase, without going through the HBase APIs, thereby reducing the amount of CPU and network resources needed.
These generated HFiles can be loaded into HBase tables by simply moving the files to the right location.
The following is an example of using a comma as the separator character to import a comma-separated dataset into a HBase table.
Look out for Bad Lines in the MapReduce job console output or in the Hadoop monitoring console.
In the preceding Python script, we remove any extra tabs in the message; here is the message displayed in the job console:
Data de-duplication using HBase HBase supports storing multiple versions of column values for each record.
When querying, HBase returns the latest version of values, unless we specify a specific time period.
This feature of HBase can be used to perform automatic de-duplication by making sure we use the same RowKey value for duplicate values.
In our 20news example, we use MessageID as the RowKey value for the records, thus ensuring that duplicate messages will appear as different versions of the same data record.
HBase allows us to configure the maximum or the minimum number of versions per column family.
Setting maximum number of versions to a low value will reduce the data usage by discarding the older versions.
Most of the text analysis data mining algorithms operate on vector data.
We can use a vector space model to represent text data as a set of vectors.
For an example, we can build a vector space model by taking the set of all terms that appear in the dataset and by assigning an index to each term in the term set.
Number of terms in the term set is the dimensionality of the resulting vectors and each dimension of the vector corresponds to a term.
For each document, the vector contains the number of occurrences of each term at the index location assigned to that particular term.
This creates vector space model using term frequencies in each document, similar to the result of the computation we perform in the Generating an inverted index using Hadoop MapReduce recipe of Chapter 7, Searching and Indexing.
However, creating vectors using the preceding term count model gives a lot of weight to the terms that occur frequently across many documents (for example, the, is, a, are, was, who, and so on), although these frequent terms have only a very minimal contribution when it comes to defining the meaning of a document.
The Term frequency-inverse document frequency (TF-IDF) model solves this issue by utilizing the inverted document frequencies (IDF) to scale the term frequencies (TF)
In this recipe, we'll create TF-IDF vectors from a text dataset using a built-in utility tool of Apache Mahout.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root folder.
Export the MAHOUT_HOME environment variable to point to your Mahout installation root folder.
Generate TF and TF-IDF sparse vector models from the text data in the sequence files:
This launches a series of MapReduce computations, as shown in the following screenshot; wait for the completion of these computations:
Optionally, you can also use the following command to dump the TF-IDF vectors as text.
How it works… Hadoop sequence files store the data as binary key-value pairs and supports data compression.
Mahout's seqdirectory command converts the text files into Hadoop SequenceFile by using the filename of the text file as the key and the contents of the text file as the value.
The seqdirectory command stores all the text contents into a single SequenceFile.
However, it's possible for us to specify a chuck size to control the actual storage of the SequenceFile data blocks in the HDFS.
Following are a selected set of options for the seqdirectory command:
The seq2sparse command is an Apache Mahout tool that supports the generation of sparse vectors from SequenceFiles containing text data.
It supports the generation of both TF as well as TF-IDF vector models.
Following are a selected set of options for the seq2sparse command:
This helps remove high frequency features such as stop words.
You can use the Mahout seqdumper command to dump the contents of a SequenceFile format that uses the Mahout Writable data types as plain text:
Clustering the text data Clustering plays an integral role in data mining computations.
Clustering groups together similar items of a dataset by using one or more features of the data items based on the use-case.
Document clustering is used in many text mining operations such as document organization, topic identification, information presentation, and so on.
Document clustering shares many of the mechanisms and algorithms with traditional data clustering mechanisms.
However, document clustering has its unique challenges when it comes to determining the features to use for clustering and when building vector space models to represent the text documents.
The Running K-Means with Mahout recipe of Chapter 5, Hadoop Ecosystem, focuses on using Mahout K-Means clustering from Java code to cluster a statistics data.
The Hierarchical clustering and Clustering an Amazon sales dataset recipes of Chapter 8, Classifications, Recommendations, and Finding Relationships, focuses on using clustering to identify customers with similar interests.
These three recipes provide a more in-depth understanding of using clustering algorithms in general.
This recipe focuses on exploring two of the several clustering algorithms available in Apache Mahout for document clustering.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root folder.
Export the MAHOUT_HOME environment variable to point to your Mahout installation root folder.
The following steps use the Apache Mahout K-Means clustering algorithm to cluster the 20news dataset:
Follow the Creating TF and TF-IDF vectors for the text data recipe in this chapter and generate TF-IDF vectors for the 20news dataset.
The following steps use the Apache Mahout MinHash clustering algorithm to cluster the 20news dataset:
The following is the usage of the Mahout K-Means algorithm:
Mahout will generate random seed clusters when an empty HDFS folder path is given to the --clusters option.
Mahout supports several different distance calculation methods such as Euclidean, Cosine, Manhattan, and so on.
Following is the usage of the Mahout MinHash clustering algorithm:
We can use Latent Dirichlet Allocation to cluster a given set of words into topics and a set of documents to combinations of topics.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root folder.
Export the MAHOUT_HOME environment variable to point to your Mahout installation root folder.
How it works… Mahout CVB version of LDA implements the Collapse Variable Bayesian inference algorithm using an iterative MapReduce approach:
The -i parameter provides the input path, while the -o parameter provides the path to store the output.
Path given in the -dt parameter stores the training topic distribution.
Path given in -mt is used as a temporary location to store the intermediate models.
All the command-line options of the cvb command can be queried by invoking the help option as follows:
Setting the number of topics to a very small value brings out very high-level topics.
Large number of topics gives more descriptive topics, but takes longer to process.
Classification assigns documents or data items to an already known set of classes with already known properties.
Document classification or categorization is used when we need to assign documents to one or more categories.
This is a frequent use-case in information retrieval as well as library science.
The Classification using Naive Bayes classifier recipe in Chapter 8, Classifications, Recommendations, and Finding Relationships, provides a more detailed description about classification use-cases and gives you an overview of using the Naive Bayes classifier algorithm.
The recipe focuses on highlighting the classification support in Apache Mahout for text documents.
Export the HADOOP_HOME environment variable to point to your Hadoop installation root folder.
Export the MAHOUT_HOME environment variable to point to your Mahout installation root folder.
The following steps use the Apache Mahout Naive Bayes algorithm to cluster the 20news dataset:
Follow the Creating TF and TF-IDF vectors for the text data recipe in this chapter and generate TF-IDF vectors for the 20news dataset.
The Mahout split command can be used to split a dataset to a training dataset and a test dataset.
The Mahout split command works with text datasets as well as with Hadoop SequenceFile datasets.
You can use the --help option with the split command to print out all the options:
The sequenceFiles option specifies that the input dataset is in Hadoop SequenceFiles format.
Following is the usage of the Mahout Naive Bayes classifier training command.
The -el option informs Mahout to extract the labels from the input dataset:
Following is the usage of the Mahout Naive Bayes classifier testing command:
Introduction Computing clouds provide on-demand, horizontal, scalable computing resources with no upfront capital investment, making them an ideal environment to perform occasional large -scale Hadoop computations.
In this chapter, we will explore several mechanisms to deploy and execute Hadoop MapReduce and Hadoop-related computations on cloud environments.
This chapter discusses how to use Amazon Elastic MapReduce (EMR), the hosted Hadoop infrastructure, to execute traditional MapReduce computations as well as Pig and Hive computations on the Amazon EC2 cloud infrastructure.
This chapter also presents how to provision an HBase cluster using Amazon EMR and how to back up and restore the data belonging to an EMR HBase cluster.
We will also use Apache Whirr, a cloud neutral library for deploying services on cloud environments, to provision Apache Hadoop and Apache HBase clusters on cloud environments.
Build the required c10-samples.jar by running the Ant build in the code samples for this chapter.
Create a S3 bucket to upload the input data by clicking on Create Bucket.
You can find more information on creating a S3 bucket in http://docs.
There exist several third-party desktop clients for the Amazon S3
You can use one of those clients to manage your data in S3 as well.
Upload your input data to the above-created bucket by selecting the bucket and clicking on Upload.
The input data for the WordCount sample should be one or more text files.
Create a S3 bucket to upload the JAR file needed for our MapReduce computation.
Create a S3 bucket to store the output data of the computation.
Create another S3 bucket to store the logs of the computation.
Hence, using the example bucket names given in this recipe might not work for you.
In such scenarios, you should give your own custom names for the buckets and substitute those names in the subsequent steps of this recipe.
Select the Custom Jar option from the drop-down menu below that.
Leave the default options and click on Continue in the next tab, Configure EC2 Instances.
Review your job flow in the Review tab and click on Create Job Flow to launch instances and to run the MapReduce computation.
Click on Refresh in the EMR console to monitor the progress of your MapReduce job.
Select your job flow entry and click on Debug to view the logs and to debug the computation.
As EMR uploads the logfiles periodically, you might have to wait and refresh to access the logfiles.
Check the output of the computation in the output data bucket using the AWS S3 console.
The prices of Spot Instances change depending on the demand.
We can submit bids for the Spot Instances and we receive the requested compute instances, if our bid exceeds the current Spot Instance price.
Amazon bills these instances based on the actual Spot Instance price, which can be lower than your bid.
Amazon will terminate your instances, if the Spot Instance price exceeds your bid.
However, Amazon do not charged for partial spot instance hours if Amazon terminated your instances.
Amazon EMR supports using Spot Instances both as master as well as worker compute instances.
Spot Instances are ideal to execute non-time critical computations such as batch jobs.
As Amazon bills the Spot Instances using the current spot price irrespective of your bid price, it is a good practice not to set the Spot Instance price too low to avoid the risk of frequent terminations.
Executing a Pig script using EMR Amazon EMR supports executing Pig scripts on the data stored in S3
In this recipe, we are going to execute the Pig script sample from the Running your first Pig commands recipe using Amazon EMR.
This sample will use the Human Development Report data (http://hdr.undp.org/en/statistics/data/) to print names of countries that have a GNI value greater than 2000 of gross national income per capita (GNI) sorted by GNI.
The following steps show you how to use a Pig script with Amazon Elastic MapReduce to process a dataset stored on Amazon S3
You can also use an existing bucket or a directory inside a bucket as well.
Modify the Pig script from the Running your first Pig commands recipe of Chapter 5, Hadoop Ecosystem, to run it using EMR.
Add a STORE command to save the result in the filesystem.
You can also use an existing bucket or a directory inside a bucket as well.
Select Pig Program option from the drop-down menu below that.
Specify the S3 location of the Pig script in the Script Location textbox of the next tab (the Specify Parameters tab)
Specify the S3 location of the uploaded input data file In the Input Location textbox.
In the Output Location textbox, specify a S3 location to store the output.
The output path should not exist and we use a directory (c10-out/out4) inside the output bucket as the output path.
Review your job flow in the Review tab and click on Create Job Flow to launch instances and to execute the Pig script.
Click on Refresh in the EMR console to monitor the progress of your MapReduce job.
Select your job flow entry and click on Debug to view the logs and to debug the computation.
As EMR uploads the logfiles periodically; you might have to wait and refresh to access the logfiles.
Check the output of the computation in the output data bucket using the AWS S3 console.
Amazon EMR allows to us to use Apache Pig in the interactive mode as well.
Starting a Pig interactive session Let's look at the steps to start a Pig interactive session:
Select the Pig Program option from the drop-down menu below that.
Review tab and click on Create Job Flow to launch instances.
Select the current job flow to view more information about the job flow.
Start the Pig interactive grunt shell in the master node and issue your Pig commands.
Executing a Hive script using EMR Amazon EMR supports executing Hive queries on the data stored in S3
In this recipe, we are going to execute the Hive queries from the Running SQL style Query with Hive recipe using Amazon EMR.
This sample will use the Human Development Report data (http://hdr.undp.org/en/statistics/data/) to print names of countries that have a GNI value greater than 2000$ of gross national income per capita (GNI) sorted by GNI.
The following steps show you how to use a Hive script with Amazon Elastic MapReduce to query a data set stored on Amazon S3
You can also use an existing bucket or a directory inside a bucket as well.
Create a Hive table to store the result of the Select query.
You can also use an existing bucket or a directory inside a bucket as well.
Select the Run your own application option under the Create a Job Flow.
Select the Hive Program option from the drop down menu below that.
Specify the S3 location of the hive script in the Script Location textbox of the next tab (Specify Parameters tab)
Specify the S3 location of the uploaded input data file In the Input Location textbox.
In the Output Location textbox, specify a S3 location to store the output.
You should specify the input and output locations using the format, s3n://bucket_name/ path.
Review your job flow in the Review tab and click on Create Job Flow to launch instances and to execute the Pig script.
Click on Refresh in the EMR console to monitor the progress of your MapReduce job.
Select your job flow entry and click Debug to view the logs and to debug the computation.
As EMR uploads the log files periodically; you might have to wait and refresh to access the logfiles.
Check the output of the computation in the output data bucket using the AWS S3 console.
Amazon EMR also allows to us to use Hive in the interactive mode as well.
Starting a Hive interactive session Let's look at the steps to start a Hive interactive session:
Select the Run your own application option under the Create a Job Flow.
Select the Hive Program option from the drop-down menu below that.
Review your job flow in the Review tab and click on Create Job Flow to launch instances.
Select the current job flow to view more information about the job flow.
Start the Hive shell in the master node and issue your Hive queries.
The EMR Command Line Interface supports creating job flows with multiple steps as well.
This recipe creates a job flow using the EMR CLI to execute the WordCount sample from the Running Hadoop MapReduce computations using Amazon ElasticMapReduce (EMR) recipe of this chapter.
The following steps show you how to create an EMR job flow using the EMR command line interface:
You can verify the version of your Ruby installation by using the following command: > ruby –v.
To create a key pair, log in to the EC2 dashboard, select a region and click on Key Pairs under the Network and Security menu.
Click on the Create Key Pair button in the Key Pairs window and provide a name for the new key pair.
Download and save the private key file (PEM format) in a safe location.
Make sure to set the appropriate file access permissions for the downloaded private key file.
Save the following JSON snippet in to a file named credentials.json in the directory of the extracted EMR CLI.
Fill the fields using the credentials of your AWS account.
A sample credentials.json file is available in the resources/emrcli folder of the resource bundle available for this chapter.
You can retrieve your AWS Access Keys from the AWS console (http://console.aws.amazon.com) by clicking on Security Credentials in the context menu that appears by clicking your AWS username in the upper-right corner of the console.
You can also retrieve the AWS Access Keys by clicking on the Security Credentials web page link in the AWS My Account portal as well.
Provide the name of your Key Pair (created in step 3) as the value of the key pair property.
Provide the path of the saved private key file as the value of the key-pair file property.
Create a S3 bucket to store the logs of the computation.
Upload your input data to the newly-created bucket by selecting the bucket and clicking on Upload.
The input data for the WordCount sample should be one or more text files.
Create a S3 bucket to upload the JAR file needed for our MapReduce computation.
Create a S3 bucket to store the output data of the computation.
Create a job flow by executing the following command inside the directory of the unzipped CLI.
The preceding commands will create a job flow and display the job flow ID.
You can use the following command to view the description of your job flow.
You can use the following command to list and to check the status of your job flows.
Once the job flow is completed, check the result of the computation in the output data location using the S3 console.
You can use EC2 spot instances with your job flows to reduce the cost of your computations.
Add a bid price to your request by adding the following commands to your job flow create command:
Refer to the Saving money by using Amazon EC2 Spot Instances to execute EMR job flows recipe in this chapter for more details on Amazon Spot Instances.
We can use Amazon Elastic MapReduce to start an Apache HBase cluster on the Amazon infrastructure to store large quantities of data in column oriented data store.
We can use the data stored on Amazon EMR HBase clusters as input and output of EMR MapReduce computations as well.
We can incrementally back up the data stored in Amazon EMR HBase clusters to Amazon S3 for data persistency.
We can also start an EMR HBase cluster by restoring the data from a previous S3 backup.
Then we start a new HBase cluster restoring the HBase data backups from the original HBase cluster.
You should have the Amazon EMR Command Line Interface (CLI) installed and configured to manually back up HBase data.
Refer to the Creating an Amazon EMR job flow using the Command Line Interface recipe in this chapter for more information on installing and configuring the EMR CLI.
Select the Run your own application option under the Create a Job Flow.
Select the HBase option from the drop-down menu below that.
Specify Backup Frequency for automatic schedules incremental data backups and provide a path inside the Blob we created in step 1 as the Backup Location.
Make sure you have the private key for the selected EC2 key pair downloaded in your computer.
To create a key pair, log in to the EC2 dashboard, select a region and click on Key Pairs under the Network and Security menu.
Click on the Create Key Pair button in the Key Pairs window and provide a name for the new key pair.
Download and save the private key file (PEM format) in to a safe location.
Note that Amazon EMR does not support the use of EC2 Small and Medium instances with HBase clusters.
Review your job flow in the Review tab and click on Create Job Flow to launch instances and to create your Apache HBase cluster.
Amazon will charge you for the compute and storage resources you use by clicking Create Job Flow in the above step.
The following steps show you how to connect to the master node of the deployed HBase cluster to start the HBase shell.
Select the job flow for the HBase cluster to view more information about the job flow.
Create the table named test in your HBase installation and insert a sample entry to the table using the put command.
Use the scan command to view the contents of the table.
The following step will back up the data stored in an Amazon EMR HBase cluster.
Execute the following command using the Amazon EMR CLI to manually backup the data stored in an EMR HBase cluster.
Retrieve the job flow name (j-FDMXCBZP9P85) from the EMR console.
Select the job flow in the EMR console and click on Terminate.
Now, we will start a new Amazon EMR HBase cluster by restoring data from a backup.
Create a new job flow by clicking on Create New Job Flow button in the EMR console.
Select the HBase option from the drop-down menu below that.
Select Yes for the Restore from Backup option and provide the backup directory path you used in step 9 in the Backup Location textbox.
Specify Backup Frequency for automatic schedules incremental data backups and provide a path inside the Blob we created in step 1 as the Backup Location.
Start the HBase shell by logging to the master node of the new HBase cluster.
Use the list command to list the set tables in HBase and the scan test command to view the contents of the test table.
Terminate your job flow using the EMR console, by selecting the job flow and clicking on the Terminate button.
The examples of Bootstrap actions include providing custom configuration for Hadoop, installing of any dependent software, distributing a common dataset, and so on.
Amazon provides a set of predefined Bootstrap actions as well as allows us to write our own custom Bootstrap actions as well.
In this recipe, we are going to use a stop words list to filter out the common words from our WordCount sample.
We download the stop words list to the workers using a custom Bootstrap action.
The following steps show you how to download a file to all the EC2 instances of an EMR computation using a Bootstrap script.
Upload the file to a Blob container in the Amazon S3
This custom Bootstrap file downloads a stop words list to each instance and copy it to a pre-designated directory inside the instance.
Review your job flow in the Review tab and click on Create Job Flow to launch instances and to run the MapReduce computation.
Click on Refresh in the EMR console to monitor the progress of your MapReduce job.
Select your job flow entry and click on Debug to view the logs and to debug the computation.
We can either upload a Hadoop configuration XML or we can specify individual configuration options as key-value pairs.
This action can be used in scenarios where we want to run a command only in the Hadoop master node.
You can also create shutdown actions by writing scripts to a designated directory in the instance.
Shutdown actions are executed after the job flow is terminated.
Using Apache Whirr to deploy an Apache Hadoop cluster in a cloud environment.
Apache Whirr provides a set of cloud vendor neutral set of libraries to provision services on the cloud resources.
Apache Whirr supports provisioning, installing, and configuring of Hadoop clusters in several cloud environments.
The following are the steps to deploy a Hadoop cluster on Amazon EC2 cloud using Apache Whirr and to execute the WordCount MapReduce sample on the deployed cluster.
Download and unzip the Apache Whirr binary distribution from http://whirr.
A sample credentials file is provide in the resources/whirr folder of the resources for this chapter.
This key pair is not the same as your AWS key pair.
You can delete that property to use EC2 traditional on-demand instances.
Execute the following command in the whirr directory to launch your Hadoop cluster on EC2
The traffic from the outside to the provisioned EC2 Hadoop cluster is routed through the master node.
Whirr generates a script that we can use to start this proxy, under a subdirectory named after your Hadoop cluster inside the ~/.whirr directory.
It will take few minutes for whirr to start the cluster and to generate this script.
You can open the Hadoop web based monitoring console in your local machine by configuring this proxy in your web browser.
You can use it to issue Hadoop commands from your local machine to your Hadoop cluster on EC2
Export the path of the generated hadoop-conf.xml file to an environmental variable named HADOOP_CONF_DIR.
Create a directory named wc-input-data in HDFS and upload a text data set to that directory.
In this step, we run the Hadoop WordCount sample in the Hadoop cluster we started in Amazon EC2
View the results of the WordCount computation by executing the following commands: >hadoop fs -ls wc-out.
Issue the following command to shut down the Hadoop cluster.
Make sure to download any important data before shutting down the cluster, as the data will be permanently lost after shutting down the cluster.
The instances of the cluster will be tagged using this name.
The preceding property specifies the number of instances to be used for each set of roles and the type of roles for the instances.
In the above example, one EC2 small instance is used with roles hadoop-jobtracker and the hadoop-namenode.
Another two EC2 small instances are used with roles hadoop-datanode and hadoop-tasktracker in each instance.
We use the Whirr Amazon EC2 provider to provision our cluster.
The preceding two properties point to the paths of the private key and the public key you provide for the cluster.
We specify a custom Hadoop version using the preceding property.
The preceding property specifies a bid price for the Amazon EC2 Spot Instances.
Specifying this property triggers Whirr to use EC2 spot instances for the cluster.
If the bid price is not met, Apache Whirr spot instance requests time out after 20 minutes.
Refer to the Saving money by using Amazon EC2 Spot Instances to execute EMR job flows recipe for more details.
Using Apache Whirr to deploy an Apache HBase cluster in a cloud environment.
Apache Whirr provides a cloud vendor neutral set of libraries to access the cloud resources.
The following are the steps to deploy a HBase cluster on Amazon EC2 cloud using Apache Whirr.
Execute the following command in the Whirr home directory to launch your HBase cluster on EC2
After provisioning the cluster, HBase prints out the commands that we can use to log in to the cluster instances.
You can log into instances using the following ssh commands:
The traffic from outside to the provisioned EC2 HBase cluster needs to be routed through the master node.
Whirr generates a script that we can use to start a proxy for this purpose.
The script can be found in a subdirectory named after your HBase cluster inside the ~/.whirr directory.
It will take few minutes for Whirr to provision the cluster and to generate this script.
Execute this script in a new terminal to start the proxy.
Hence in this recipe, we directly log in to the master node of the HBase cluster.
Issue the following command to shut down the Hadoop cluster.
Make sure to download any important data before shutting down the cluster, as the data will be permanently lost after shutting down the cluster.
Refer to the Using Apache Whirr to deploy an Apache Hadoop cluster in a cloud environment recipe for descriptions of the other properties.
This property specifies the number of instances to be used for each set of roles and the type of roles for the instances.
In the preceding example, one EC2 small instance is used with roles hbase-master, zookeeper, hadoop-jobtracker, and the hadoop-namenode.
WordCount MapReduce program combiner step, adding  12 running, in distributed cluster environment.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Learn how to crunch big data to extract meaning from the data avalanche.
Learn tools and techniques that let you approach big data with relish and not fear.
Shows how to build a complete infrastructure to handle your needs as your data grows.
Hands-on examples in each chapter give the big picture while also giving direct experience.
Realistic, simple code examples to solve problems at scale with Hadoop and related technologies.
Solutions to common problems when working in the Hadoop environment.
In depth code examples demonstrating various analytic models, analytic solutions, and common best practices.
Move large amounts of data into HBase and learn how to manage it efficiently.
Set up HBase on the cloud, get it ready for production, and run it smoothly with high performance.
Over 150 recipes to design and optimize large-scale Apache Cassandra deployments.
Get the best out of Cassandra using this efficient recipe bank.
Well illustrated, step-by-step recipes to make all tasks look easy!
Chapter 3: Advanced Hadoop MapReduce Administration Introduction Tuning Hadoop configurations for cluster deployments Running benchmarks to verify the Hadoop installation Reusing Java VMs to improve the performance Fault tolerance and speculative execution Debug scripts – analyzing task failures Setting failure percentages and skipping bad records Shared-user Hadoop clusters – using fair and other schedulers Hadoop security – integrating with Kerberos Using the Hadoop Tool interface.
Chapter 4: Developing Complex Hadoop MapReduce Applications Introduction Choosing appropriate Hadoop data types Implementing a custom Hadoop Writable data type Implementing a custom Hadoop key type Emitting data of different value types from a mapper Choosing a suitable Hadoop InputFormat for your input data format Adding support for new input data formats – implementing a custom InputFormat Formatting the results of MapReduce computations – using Hadoop OutputFormats Hadoop intermediate (map to reduce) data partitioning Broadcasting and distributing shared resources to tasks in a MapReduce job – Hadoop DistributedCache Using Hadoop with legacy applications – Hadoop Streaming Adding dependencies between MapReduce jobs Hadoop counters for reporting custom metrics.
