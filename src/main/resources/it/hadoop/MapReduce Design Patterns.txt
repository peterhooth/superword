O’Reilly books may be purchased for educational, business, or sales promotional use.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
Welcome to MapReduce Design Patterns! This book will be unique in some ways and familiar in others.
First and foremost, this book is obviously about design patterns, which are templates or general guides to solving problems.
We took a look at other design patterns books that have been written in the past as inspiration, particularly Design Patterns: Elements of Reusable Object-Oriented Software, by Gamma et al.
For each pattern, you’ll see a template that we reuse over and over that we loosely based off of their book.
Repeatedly seeing a similar template will help you get to the specific information you need.
This will be especially useful in the future when using this book as a reference.
This book is a bit more open-ended than a book in the “cookbook” series of texts as we don’t call out specific problems.
However, similarly to the cookbooks, the lessons in this book are short and categorized.
You’ll have to go a bit further than just copying and pasting our code to solve your problems, but we hope that you will find a pattern to get you at least 90% of the way for just about all of your challenges.
This book is mostly about the analytics side of Hadoop or MapReduce.
We intentionally try not to dive into too much detail on how Hadoop or MapReduce works or talk too long about the APIs that we are using.
These topics have been written about quite a few times, both online and in print, so we decided to focus on analytics.
In this preface, we’ll talk about how to read this book since its format might be a bit different than most books you’ve read.
Intended Audience The motivation for us to write this book was to fill a missing gap we saw in a lot of new MapReduce developers.
They had learned how to use the system, got comfortable with.
The intent of this book is to prevent you from having to make some of your own mistakes by educating you on how experts have figured out how to solve problems with MapReduce.
So, in some ways, this book can be viewed as an intermediate or advanced MapReduce developer resource, but we think early beginners and gurus will find use out of it.
This book is also intended for anyone wanting to learn more about the MapReduce paradigm.
The book goes deeply into the technical side of MapReduce with code examples and detailed explanations of the inner workings of a MapReduce system, which will help software engineers develop MapReduce analytics.
However, quite a bit of time is spent discussing the motivation of some patterns and the common use cases for these patterns, which could be interesting to someone who just wants to know what a system like Hadoop can do.
To get the most out of this book, we suggest you have some knowledge of Hadoop, as all of the code examples are written for Hadoop and many of the patterns are discussed in a Hadoop context.
A brief refresher will be given in the first chapter, along with some suggestions for additional reading material.
Pattern Format The patterns in this book follow a single template format so they are easier to read in succession.
Some patterns will omit some of the sections if they don’t make sense in the context of that pattern.
This section is a quick description of the problem the pattern is intended to solve.
Motivation This section explains why you would want to solve this problem or where it would appear.
Applicability This section contains a set of criteria that must be true to be able to apply this pattern to a problem.
Sometimes these are limitations in the design of the pattern and sometimes they help you make sure this pattern will work in your situation.
Structure This section explains the layout of the MapReduce job itself.
It’ll explain what the map phase does, what the reduce phase does, and also lets you know if it’ll be using any custom partitioners, combiners, or input formats.
This is the meat of the pattern and explains how to solve the problem.
Consequences This section is pretty short and just explains what the output of the pattern will be.
This is the end goal of the output this pattern produces.
Resemblances For readers that have some experience with SQL or Pig, this section will show analogies of how this problem would be solved with these other languages.
You may even find yourself reading this section first as it gets straight to the point of what this pattern does.
Sometimes, SQL, Pig, or both are omitted if what we are doing with MapReduce is truly unique.
Known Uses This section outlines some common use cases for this pattern.
Performance Analysis This section explains the performance profile of the analytic produced by the pattern.
Understanding this is important because every MapReduce analytic needs to be tweaked and configured properly to maximize performance.
Without the knowledge of what resources it is using on your cluster, it would be difficult to do this.
The Examples in This Book All of the examples in this book are written for Hadoop version 1.0.3
MapReduce is a paradigm that is seen in a number of open source and commercial systems these days, but we had to pick one to make our examples consistent and easy to follow, so we picked Hadoop.
Hadoop was a logical choice since it a widely used system, but we hope that users of MongoDB’s MapReduce and other MapReduce implementations will be able to extrapolate the examples in this text to their particular system of choice.
In general, we try to use the newer mapreduce API for all of our examples, not the deprecated mapred API.
Just be careful when mixing code from this book with other sources, as plenty of people still use mapred and their APIs are not compatible.
Our examples generally omit any sort of error handling, mostly to make the code more terse.
In real-world big data systems, you can expect your data to be malformed and you’ll want to be proactive in handling those situations in your analytics.
We use the same data set throughout this text: a dump of StackOverflow’s databases.
StackOverflow is a popular website in which software developers can go to ask and.
This data set was chosen because it is reasonable in size, yet not so big that you can’t use it on a single node.
This data set also contains human-generated natural language text as well as “structured” elements like usernames and dates.
Throughout the examples in this book, we try to break out parsing logic of this data set into helper functions to clearly distinguish what code is specific to this data set and which code is general and part of the pattern.
Since the XML is pretty simple, we usually avoid using a full-blown XML parser and just parse it with some string operations in our Java code.
The data set contains five tables, of which we only use three: comments, posts, and users.
All of the data is in well-formed XML, with one record per line.
We use the following three StackOverflow tables in this book: comments.
Comments are follow-up questions or suggestions users of the site can leave on posts (i.e., questions or answers)
A user will post a question, and then other users are free to post answers to that question.
Questions and answers can be upvoted and downvoted depending on if you think the post is constructive or not.
In order to help categorize the questions, the creator of the question can specify a number of “tags,” which say what the post is about.
In the example above, we see that this post is about asp.net, iis, and gzip.
One thing to notice is that the body of the post is escaped HTML.
This makes parsing it a bit more challenging, but it’s not too bad with all the tools available.
Most of the questions and many of the answers can get to be pretty long!
Posts are a bit more challenging because they contain both answers and questions intermixed.
Answers point to their related question via the ParentId, a field that questions do not have.
The users table contains all of the data about the account holders on StackOverflow.
Most of this information shows up in the user’s profile.
Users of StackOverflow have a reputation score, which goes up as other users upvote questions or answers that user has submitted to the website.
To learn more about the data set, refer to the documentation included with the download in README.txt.
In the examples, we parse the data set with a helper function that we wrote.
This function takes in a line of StackOverflow data and returns a HashMap.
This HashMap stores the labels as the keys and the actual data as the value.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Constant width Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Technology professionals, software developers, web designers, and business and creative professionals use Safari Books Online as their primary resource for research, problem solving, learning, and certification training.
Safari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals.
For more information about Safari Books Online, please visit us online.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
For more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.
Acknowldgements Books published by O’Reilly are always top notch and now we know why first hand.
The support staff, especially our editor Andy Oram, has been extremely helpful in guiding us through this process.
They give freedom to the authors to convey the message while supporting us in any way we need.
A special thanks goes out to those that read our book and provided useful commentary and reviews: Tom Wheeler, Patrick Angeles, Tom Kulish, and Lance Byrd.
Thanks to Jeff Gold for providing some early encouragement and comments.
We appreciate Eric Sammer’s help in finding reviewers and wish him luck with his book Hadoop Operations.
The StackOverflow data set, which is used throughout this book, is freely available under the Creative Commons license.
It’s great that people are willing to spend the time to release the data set so that projects like this can make use of the content.
Don would like to thank the support he got from coworkers at Greenplum, who provided slack in my schedule to work on this project, moral support, and technical suggestions.
Also, thanks to Andy O’Brien for contributing the chapter on Postgres.
Adam would like to thank his family, friends, and caffeine.
MapReduce is a computing paradigm for processing data that resides on hundreds of computers, which has been popularized recently by Google, Hadoop, and many others.
The paradigm is extraordinarily powerful, but it does not provide a general solution to what many are calling “big data,” so while it works particularly well on some problems, some are more challenging.
This book will teach you what problems are amenable to the MapReduce paradigm, as well as how to use it effectively.
At first glance, many people do not realize that MapReduce is more of a framework than a tool.
You have to fit your solution into the framework of map and reduce, which in some situations might be challenging.
It provides clear boundaries for what you can and cannot do, making the number of options you have to consider fewer than you may be used to.
At the same time, figuring out how to solve a problem with constraints requires cleverness and a change in thinking.
Learning MapReduce is a lot like learning recursion for the first time: it is challenging to find the recursive solution to the problem, but when it comes to you, it is clear, concise, and elegant.
In many situations you have to be conscious of system resources being used by the MapReduce job, especially inter-cluster network utilization.
The tradeoff of being confined to the MapReduce framework is the ability to process your data with distributed computing, without having to deal with concurrency, robustness, scale, and other common challenges.
But with a unique system and a unique way of problem solving, come unique design patterns.
What is a MapReduce design pattern? It is a template for solving a common and general data manipulation problem with MapReduce.
A pattern is not specific to a domain such as text processing or graph analysis, but it is a general approach to solving a problem.
Using design patterns is all about using tried and true design principles to build better software.
Designing good software is challenging for a number of reasons, and similar challenges face those who want to achieve good design in MapReduce.
Just as good programmers can produce bad software due to poor design, good programmers can produce bad MapReduce algorithms.
With MapReduce we’re not only battling with clean and maintainable code, but also with the performance of a job that will be distributed across hundreds of nodes to compute over terabytes and even petabytes of data.
In addition, this job is potentially competing with hundreds of others on a shared cluster of machines.
This makes choosing the right design to solve your problem with MapReduce extremely important and can yield performance gains of several orders of magnitude.
Before we dive into some design patterns in the chapters following this one, we’ll talk a bit about how and why design patterns and MapReduce together make sense, and a bit of a history lesson of how we got here.
Design Patterns Design patterns have been making developers’ lives easier for years.
They are tools for solving problems in a reusable and general way so that the developer can spend less time figuring out how he’s going to overcome a hurdle and move onto the next one.
They are also a way for veteran problem solvers to pass down their knowledge in a concise way to younger generations.
One of the major milestones in the field of design patterns in software engineering is the book Design Patterns: Elements of Reusable Object-Oriented Software, by Gamma et al.
None of the patterns in this very popular book were new and many had been in use for several years.
The reason why it was and still is so influential is the authors took the time to document the most important design patterns across the field of object-oriented programming.
Since the book was published in 1994, most individuals interested in good design heard about patterns from word of mouth or had to root around conferences, journals, and a barely existent World Wide Web.
Design patterns have stood the test of time and have shown the right level of abstraction: not too specific that there are too many of them to remember and too hard to tailor to a problem, yet not too general that tons of work has to be poured into a pattern to get things working.
Simply saying “abstract factory” is easier than explaining what an abstract factory is over and over.
Also, when looking at a stranger’s code that implements an abstract factory, you already have a general understanding of what the code is trying to accomplish.
MapReduce design patterns fill this same role in a smaller space of problems and solutions.
They provide a general framework for solving your data computation issues, without being specific to the problem domain.
Experienced MapReduce developers can pass on knowledge of how to solve a general problem to more novice MapReduce developers.
This is extremely important because MapReduce is a new technology with a fast adoption rate and there are new developers joining the community every day.
MapReduce design patterns also provide a common language for teams working together on MapReduce problems.
Suggesting to someone that they should use a “reduce-side join” instead of a “map-side replicated join” is more concise than explaining the lowlevel mechanics of each.
Patterns today are scattered across blogs, websites such as StackOverflow, deep inside other books, and inside very advanced technology teams at organizations across the world.
The intent of this book is not to provide some groundbreaking new ways to solve problems with MapReduce that nobody has seen before, but instead to collect patterns that have been developed by veterans in the field so that they can be shared with everyone else.
Even provided with some design patterns, genuine experience with the MapReduce paradigm is still necessary to understand when to apply them.
When you are trying to solve a new problem with a pattern you saw in this book or elsewhere, be very careful that the pattern fits the problem by paying close attention to its “Applicability” section.
For the most part, the MapReduce design patterns in this book are intended to be platform independent.
MapReduce, being a paradigm published by Google without any actual source code, has been reimplemented a number of times, both as a standalone system (e.g., Hadoop, Disco, Amazon Elastic MapReduce) and as a query language within a larger system (e.g., MongoDB, Greenplum DB, Aster Data)
Even if design patterns are intended to be general, we write this book with a Hadoop perspective.
Many of these patterns can be applied in other systems, such as MongoDB, because they conform to the same conceptual architecture.
However, some technical details may be different from implementation to implementation.
The Gang of Four’s book on design patterns was written with a C++ perspective, but developers have found the concepts conveyed in the book useful in modern languages such as Ruby and Python.
The patterns in this book should be usable with systems other than Hadoop.
You’ll just have to use the code examples as a guide to developing your own code.
MapReduce History How did we get to the point where a MapReduce design patterns book is a good idea? At a certain point, the community’s momentum and widespread use of the paradigm reaches a critical mass where it is possible to write a comprehensive list of design patterns to be shared with developers everywhere.
Several years ago, when Hadoop was still in its infancy, not enough had been done with the system to figure out what it is capable of.
But the speed at which MapReduce has been adopted is remarkable.
This paper described how Google split, processed, and aggregated their data set of mind-boggling size.
Shortly after the release of the paper, a free and open source software pioneer by the name of Doug Cutting started working on a MapReduce implementation to solve scalability in another project he was working on called Nutch, an effort to build an open source search engine.
Over time and with some investment by Yahoo!, Hadoop split out as its own project and eventually became a top-level Apache Foundation project.
Several other open source projects have been built with Hadoop at their core, and this list is continually growing.
Doug Cutting and other Hadoop experts have mentioned several times that Hadoop is becoming the kernel of a distributed operating system in which distributed applications can be built.
In this book, we’ll be explaining the examples with the least common denominator in the Hadoop ecosystem, Java MapReduce.
In the resemblance sections of each pattern in some chapters, we’ll typically outline a parallel for Pig and SQL that could be used in Hive.
MapReduce and Hadoop Refresher The point of this section is to provide a quick refresher on MapReduce in the Hadoop context, since the code examples in this book are written in Hadoop.
Some beginners might want to refer to a more in-depth resource such as Tom White’s excellent Hadoop: The Definitive Guide or the Apache Hadoop website.
These resources will help you get started in setting up a development or fully productionalized environment that will allow you to follow along the code examples in this book.
Hadoop MapReduce jobs are divided into a set of map tasks and reduce tasks that run in a distributed fashion on a cluster of computers.
The map tasks generally load, parse, transform, and filter data.
Each reduce task is responsible for handling a subset of the map task output.
Intermediate data is then copied from mapper tasks by the reducer tasks in order to group and aggregate the data.
It is incredible what a wide range of problems can be solved with such a straightforward paradigm, from simple numerical aggregations to complex join operations and Cartesian products.
The input to a MapReduce job is a set of files in the data store that are spread out over the Hadoop Distributed File System (HDFS)
In Hadoop, these files are split with an input format, which defines how to separate a file into input splits.
An input split is a byteoriented view of a chunk of the file to be loaded by a map task.
Each map task in Hadoop is broken into the following phases: record reader, mapper, combiner, and partitioner.
The output of the map tasks, called the intermediate keys and values, are sent to the reducers.
The reduce tasks are broken into the following phases: shuffle, sort, reducer, and output format.
The nodes in which the map tasks run are optimally on the nodes in which the data rests.
This way, the data typically does not have to move over the network and can be computed on the local machine.
The record reader translates an input split generated by input format into records.
The purpose of the record reader is to parse the data into records, but not parse the record itself.
It passes the data to the mapper in the form of a key/value pair.
Usually the key in this context is positional information and the value is the chunk of data that composes a record.
Customized record readers are outside the scope of this book.
We generally assume you have an appropriate record reader for your data.
The decision of what is the key and value here is not arbitrary and is very important to what the MapReduce job is accomplishing.
The key is what the data will be grouped on and the value is the information pertinent to the analysis in the reducer.
Plenty of detail will be provided in the design patterns in this book to explain what and why the particular key/value is chosen.
One major differentiator between MapReduce design patterns is the semantics of this pair.
It takes the intermediate keys from the mapper and applies a user-provided method to aggregate values in the small scope of that one mapper.
For example, because the count of an aggregation is the sum of the counts of each part, you can produce an intermediate count and then sum those intermediate counts for the final result.
In many situations, this significantly reduces the amount of data that has to move over the network.
Combiners will be covered in more depth with the patterns that use them extensively.
Many new Hadoop developers ignore combiners, but they often provide extreme performance gains with no downside.
We will point out which patterns benefit from using a combiner, and which ones cannot use a combiner.
A combiner is not guaranteed to execute, so it cannot be a part of the overall algorithm.
By default, the partitioner interrogates the object for its hash code, which is typically an md5sum.
Then, the partitioner performs a modulus operation by the number of reducers: key.hashCode() % (number of reducers)
This randomly distributes the keyspace evenly over the reducers, but still ensures that keys with the same value in different mappers end up at the same reducer.
The default behavior of the partitioner can be customized, and will be in some more advanced patterns, such as sorting.
The partitioned data is written to the local file system for each map task and waits to be pulled by its respective reducer.
This step takes the output files written by all of the partitioners and downloads them to the local machine in which the reducer is running.
These individual data pieces are then sorted by key into one larger data list.
The purpose of this sort is to group equivalent keys together so that their values can be iterated over easily in the reduce task.
This phase is not customizable and the framework handles everything automatically.
The only control a developer has is how the keys are sorted and grouped by specifying a custom Comparator object.
The function is passed the key and an iterator over all of the values associated with that key.
A wide range of processing can happen in this function, as we’ll see in many of our patterns.
The data can be aggregated, filtered, and combined in a number of ways.
Once the reduce function is done, it sends zero or more key/value pair to the final step, the output format.
Like the map function, the re duce function will change from job to job since it is a core piece of logic in the solution.
This can typically be customized to provide richer output formats, but in the end, the data is written out to HDFS, regardless of format.
Like the record reader, customizing your own output format is outside of the scope of this book, since it simply deals with I/O.
Hadoop Example: Word Count Now that you’re refreshed on the steps of the whole MapReduce process, let’s dive into a quick and simple example.
The “Word Count” program is the canonical example in MapReduce, and for good reason.
It is a straightforward application of MapReduce and MapReduce can handle it extremely efficiently.
Many people complain about the “Word Count” program being overused as an example, but hopefully the rest of the book makes up for that!
In this particular example, we’re going to be doing a word count over user-submitted comments on StackOverflow.
The content of the Text field will be pulled out and preprocessed a bit, and then we’ll count up how many times we see each word.
The first chunk of code we’ll look at is the driver.
The driver takes all of the components that we’ve built for our MapReduce job and pieces them together to be submitted to execution.
This code is usually pretty generic and considered “boiler plate.” You’ll find that in all of our patterns the driver stays the same for the most part.
The purpose of the driver is to orchestrate the jobs.
The first few lines of main are all about parsing command line arguments.
Then we start setting up the job object by telling it what classes to use for computations and what input paths and output paths to use.
That’s about it! It’s just important to make sure the class names match up with the classes you wrote and that the output key and value types match up with the output types of the mapper.
One way you’ll see this code change from pattern to pattern is the usage of job.setCom binerClass.
In some cases, the combiner simply cannot be used due to the nature of the reducer.
In other cases, the combiner class will be different from the reducer class.
The combiner is very effective in the “Word Count” program and is quite simple to activate.
Next is the mapper code that parses and prepares the text.
Once some of the punctuation and random text is cleaned up, the text string is split up into a list of words.
Then the intermediate key produced is the word and the value produced is simply “1.” This means we’ve seen this word once.
Even if we see the same word twice in one line, we’ll output the word and “1” twice and it’ll be taken care of in the end.
Eventually, all of these ones will be summed together into the global count of that word.
You’ll see it used in a number of our examples.
It basically takes a line of the StackOverflow XML (which has a very predictable format) and matches up the XML attributes with the values into a Map.
The mapper is where we’ll see most of the work done.
The first major thing to notice is the type of the parent class:
We don’t care about the key of the input in this case, so that’s why we use Object.
The data coming in is Text (Hadoop’s special String type) because we are reading the data as a line-by-line text document.
Our output key and value are Text and IntWritable because we will be using the word as the key and the count as the value.
The mapper input key and value data types are dictated by the job’s configured FileInputFormat.
The default implementation is the Tex tInputFormat, which provides the number of bytes read so far in the file as the key in a LongWritable object and the line of text as the value in a Text object.
These key/value data types are likely to change if you are using different input formats.
Up until we start using the StringTokenizer towards the bottom of the code, we’re just cleaning up the string.
We unescape the data because the string was stored in an escaped manner so that it wouldn’t mess up XML parsing.
Next, we remove any stray punctuation so that the literal string Hadoop! is considered the same word as Hadoop? and Hadoop.
Finally, for each token (i.e., word) we emit the word with the number 1, which means we saw the word once.
The framework then takes over to shuffle and sorts the key/value pairs to reduce tasks.
The reduce function gets called once per key grouping, in this case each word.
We’ll iterate through the values, which will be numbers, and take a running sum.
The final value of this running sum will be the sum of the ones.
As in the mapper, we specify the input and output types via the template parent class.
Also like the mapper, the types correspond to the same things: input key, input value, output key, and output value.
The input key and input value data types must match the output key/value types from the mapper.
The output key and output value data types must match the types that the job’s configured FileOutputFormat is expecting.
In this case, we are using the default TextOutputFormat, which can take any two Writable objects as output.
The reduce function has a different signature from map, though: it gives you an Iterator over values instead of just a single value.
This is because you are now iterating over all values that have that key, instead of just one at a time.
The key is very important in the reducer of pretty much every MapReduce job, unlike the input key in the map.
Anything we pass to context.write will get written out to a file.
Each reducer will create one file, so if you want to coalesce them together you’ll have to write a post-processing step to concatenate them.
Now that we’ve gotten a straightforward example out of the way, let’s dive into some design patterns!
Pig and Hive There is less need for MapReduce design patterns in a ecosystem with Hive and Pig.
However, we would like to take this opportunity early in the book to explain why MapReduce design patterns are still important.
They provide an interface that has nothing to do with “map” or “reduce,” but the systems interpret the higher-level language into a series of MapReduce jobs.
Much like how a query planner in an RDBMS translates SQL into actual operations on data, Hive and Pig translate their respective languages into MapReduce operations.
As will be seen throughout this book in the resemblances sections, Pig and SQL (or HiveQL) can be significantly more terse than the raw Hadoop implementations in Java.
For example, it will take several pages to explain total order sorting, while Pig is able to get the job done in a few lines.
So why should we use Java MapReduce in Hadoop at all when we have options like Pig and Hive? What was the point in the authors of this book spending time explaining how to implement something in hundreds of lines of code when the same can be accomplished in a couple lines? There are two core reasons.
First, there is conceptual value in understanding the lower-level workings of a system like MapReduce.
Using Pig or Hive without understanding MapReduce can lead to some dangerous situations.
Just because you’re benefiting from a higher-level interface doesn’t mean you can ignore the details.
Large MapReduce clusters are heavy machinery and need to be respected as such.
Second, Pig and Hive aren’t there yet in terms of full functionality and maturity (as of 2012)
It is obvious that they haven’t reached their full potential yet.
Right now, they simply can’t tackle all of the problems in the ways that Java MapReduce can.
This will surely change over time and with every major release, major features, and bux fixes are added.
With every release, more and more can be done at a higher-level of abstraction.
The funny thing about trends things like this in software engineering is that the last 10% of problems that can’t be solved with a higher-level of abstraction are also likely to be the most critical and most challenging.
This is when something like Java is going to be the best tool for the job.
Some still use assembly language when they really have to!
When you can, write your MapReduce in Pig or Hive.
Some of the major benefits of using these higher-level of abstractions include readability, maintainability, development time, and automatic optimization.
Rarely is the often-cited performance hit due to indirection a serious consideration.
These analytics are running in batch and are taking several minutes already, so what does a minute or two more really matter? In some cases, the query plan optimizer in Pig or Hive will be better at optimizing your code than you are! In a small fraction of situations, the extra few minutes added by Pig or Hive will matter, in which case you should use Java MapReduce.
Pig and Hive are likely to influence MapReduce design patterns more than anything else.
New feature requests in Pig and Hive will likely translate down into something that could be a design pattern in MapReduce.
Likewise, as more design patterns are developed for MapReduce, some of the more popular ones will become first-class operations at a higher level of abstraction.
Pig and Hive have patterns of their own and experts will start documenting more as they solve more problems.
Hive has the benefit of building off of decades of SQL patterns, but not all patterns in SQL are smart in Hive and vice versa.
Perhaps as these platforms gain more popularity, cookbook and design pattern books will be written for them.
Your data is large and vast, with more data coming into the system every day.
This chapter focuses on design patterns that produce a top-level, summarized view of your data so you can glean insights not available from looking at a localized set of records alone.
Summarization analytics are all about grouping similar data together and then performing an operation such as calculating a statistic, building an index, or just simply counting.
Calculating some sort of aggregate over groups in your data set is a great way to easily extract value right away.
For example, you might want to calculate the total amount of money your stores have made by state or the average amount of time someone spends logged into your website by demographic.
Typically, with a new data set, you’ll start with these types of analyses to help you gauge what is interesting or unique in your data and what needs a closer look.
The patterns in this chapter are numerical summarizations, inverted index, and counting with counters.
They are more straightforward applications of MapReduce than some of the other patterns in this book.
This is because grouping data together by a key is the core function of the MapReduce paradigm: all of the keys are grouped together and collected in the reducers.
If you emit the fields in the mapper you want to group on as your key, the grouping is all handled by the MapReduce framework for free.
Numerical Summarizations Pattern Description The numerical summarizations pattern is a general pattern for calculating aggregate statistical values over your data is discussed in detail.
Be careful of how deceptively simple this pattern is! It is extremely important to use the combiner properly and to understand the calculation you are performing.
Group records together by a key field and calculate a numerical aggregate per group to get a top-level view of the larger data set.
Many data sets these days are too large for a human to get any real meaning out it by reading through it manually.
For example, if your website logs each time a user logs onto the website, enters a query, or performs any other notable action, it would be extremely difficult to notice any real usage patterns just by reading through terabytes of log files with a text reader.
If you group logins by the hour of the day and perform a count of the number of records in each group, you can plot these counts on a histogram and recognize times when your website is more active.
Similarly, if you group advertisements by types, you can determine how affective your ads are for better targeting.
Maybe you want to cycle ads based on how effective they are at the time of day.
All of these types of questions can be answered through numerical summarizations to get a top-level view of your data.
Numerical summarizations should be used when both of the following are true:
Figure 2-1 shows the general structure of how a numerical summarization is executed in MapReduce.
The breakdown of each MapReduce component is described in detail:
Grouping typically involves sending a large subset of the input data down to finally be reduced.
Each input record is most likely going to be output from the map phase.
Make sure to reduce the amount of data being sent to the reducers by choosing only the fields that are necessary to the analytic and handling any bad input conditions properly.
Numerical summaries can benefit from a custom partitioner to better distribute key/value pairs across n number of reduce tasks.
The need for this is rare, but can be done if job execution time is critical, the amount of data is huge, and there is severe data skew.
A custom partitioner is often overlooked, but taking the time to understand the distribution of output keys and partitioning based on this distribution will improve performance when grouping (and everything else, for that matter)
Starting a hundred reduce tasks, only to have eighty of them complete in thirty seconds and the others in twenty-five minutes, is not efficient.
The output of the job will be a set of part files containing a single record per reducer input group.
Each record will consist of the key and all aggregate values.
The application outputs each word of a document as the key and “1” as the value, thus grouping by words.
The reduce phase then adds up the integers and outputs each unique word with the sum.
Min/Max/Count An analytic to determine the minimum, maximum, and count of a particular event, such as the first time a user posted, the last time a user posted, and the number of times they posted in between that time period.
You don’t have to collect all three of these aggregates at the same time, or any of the other use cases listed here if you are only interested in one of them.
A combiner can be used for all three, but requires a more complex approach than just reusing the reducer implementation.
The Numerical Aggregation pattern is analogous to using aggregates after a GROUP BY in SQL:
Aggregations performed by jobs using this pattern typically perform well when the combiner is properly used.
These types of operations are what MapReduce was built for.
Like most of the patterns in this book, developers need to be concerned about the appropriate number of reducers and take into account any data skew that may be present in the reduce groups.
That is, if there are going to be many more intermediate key/value pairs with a specific key than other keys, one reducer is going to have a lot more work to do than others.
Calculating the minimum, maximum, and count of a given field are all excellent applications of the numerical summarization pattern.
After a grouping operation, the reducer simply iterates through all the values associated with the group and finds the min and max, as well as counts the number of members in the key grouping.
Due to the associative and commutative properties, a combiner can be used to vastly cut down on the number of intermediate key/value pairs that need to be shuffled to the reducers.
If implemented correctly, the code used for your reducer can be identical to that of a combiner.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of user’s comments, determine the first and last time a user commented and the total number of comments from that user.
The MinMaxCountTuple is a Writable object that stores three values.
This class is used as the output value from the mapper.
While these values can be crammed into a Text object with some delimiter, it is typically a better practice to create a custom Writable.
Not only is it cleaner, but you won’t have to worry about any string parsing when it comes time to grab these values from the reduce phase.
These custom writable objects are used throughout other examples in this pattern.
Other writables used in this chapter are very similar to this and are omitted for brevity.
The mapper will preprocess our input values by extracting the XML attributes from each input record: the creation data and the user identifier.
The creation date is parsed into a Java Date object for ease of comparison in the combiner and reducer.
The output key is the user ID and the value is three columns of our future output: the minimum date, the maximum date, and the number of comments this user has created.
These three fields are stored in a custom Writable object of type MinMaxCountTuple, which stores the first two columns as Date objects and the final column as a long.
These names are accurate for the reducer but don’t really reflect how the fields are used in the mapper, but we wanted to use the same data type for both the mapper and the reducer.
In the mapper, we’ll set both min and max to the comment creation date.
The date is output twice so that we can take advantage of the combiner optimization that is described later.
The third column will be a count of 1, to indicate that we know this user posted one comment.
Eventually, all of these counts are going to be summed together and the minimum and maximum date will be determined in the reducer.
Grab the “UserID” since it is what we are grouping by.
The reducer iterates through the values to find the minimum and maximum dates, and sums the counts.
We start by initializing the output result for each input group.
For each value in this group, if the output result’s minimum is not yet set, or the value’s minimum is less than result’s current minimum, we set the result’s minimum to the input value.
The same logic applies to the maximum, except using a greater than operator.
Each value’s count is added to a running sum, similar to the word count example in the introductory chapter.
After determining the minimum and maximum dates from all input values, the final count is set to our output value.
The input key is then written to the file system along with the output value.
The reducer implementation just shown can be used as the job’s combiner.
As we are only interested in the count, minimum date, and maximum date, multiple comments from the same user do not have to be sent to the reducer.
The minimum and maximum comment dates can be calculated for each local map task without having an effect on the final minimum and maximum.
The counting operation is an associative and commutative operation and won’t be harmed by using a combiner.
Figure 2-2 shows the flow between the mapper, combiner, and reducer to help describe their interactions.
Numbers are used rather than dates for simplicity, but the concept is the same.
A combiner possibly executes over each of the highlighted output groups from a mapper, determining the minimum and maximum values in the first two columns and adding up the number of rows in the “table” (group)
The combiner then outputs the minimum and maximum along with the new count.
If a combiner does not execute over any rows, they will still be accounted for in the reduce phase.
To calculate an average, we need two values for each group: the sum of the values that we want to average and the number of values that went into the sum.
These two values can be calculated on the reduce side very trivially, by iterating through each value in the set and adding to a running sum while keeping a count.
After the iteration, simply divide the sum by the count and output the average.
However, if we do it this way we cannot use this same reducer implementation as a combiner, because calculating an average is not an associative operation.
Instead, our mapper will output two “columns” of data, count and average.
For each input record, this will simply be “1” and the value of the field.
The reducer will multiply the “count” field by the “average” field to add to a running sum, and add the “count” field to a running count.
It will then divide the running sum with the running count and output the count with the calculated average.
With this more round-about algorithm, the reducer code can be used as a combiner as associativity is preserved.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of user’s comments, determine the average comment length per hour of day.
The mapper will process each input record to calculate the average comment length based on the time of day.
The output key is the hour of day, which is parsed from the creation date XML attribute.
The output value is two columns, the comment count and the average length of the comments for that hour.
Because the mapper operates on one record at a time, the count is simply 1 and the average length is equivalent to the comment length.
These two values are output in a custom Writable, a CountAverageTuple.
This type contains two float values, a count, and an average.
The reducer code iterates through all given values for the hour and keeps two local variables: a running count and running sum.
For each value, the count is multiplied by the average and added to the running sum.
After iteration, the input key is written to the file system with the count and average, calculated by dividing the running sum by the running count.
When determining an average, the reducer code can be used as a combiner when outputting the count along with the average.
An average is not an associative operation, but if the count is output from the reducer with the count, these two values can be multiplied to preserve the sum for the final reduce phase.
Without outputting the count, a combiner cannot be used because taking an average of averages is not equivalent to the true average.
Typically, writing the count along with the average to the file system is not an issue.
However, if the count is impeding the analysis at hand, it can be omitted by making a combiner implementation nearly identical to the reducer implementation just shown.
The only differentiation between the two classes is that the reducer does not write the count with the average.
Figure 2-3 shows the flow between the mapper, combiner, and reducer to help describe their interactions.
A combiner possibly executes over each of the highlighted output groups from a mapper, determining the average and outputting it with the count, which is the number of rows corresponding to the group.
If a combiner does not execute over any rows, they will still be accounted for in the reduce phase.
Finding the median and standard deviation is a little more complex than the previous examples.
Because these operations are not associative, they cannot benefit from a combiner as easily as their counterparts.
A median is the numerical value separating the lower and higher halves of a data set.
This requires the data set to be complete, which in turn requires it to be shuffled.
The data must also be sorted, which can present a barrier because MapReduce does not sort values.
A standard deviation shows how much variation exists in the data from the average, thus requiring the average to be discovered prior to reduction.
The easiest way to perform these operations involves copying the list of values into a temporary list in order to find the median or iterating over the set again to determine the standard deviation.
With large data sets, this implementation may result in Java heap space issues, because each value is copied into memory for every input group.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of user’s comments, determine the median and standard deviation of comment lengths per hour of day.
The mapper will process each input record to calculate the median comment length within each hour of the day.
The output key is the hour of day, which is parsed from the CreationDate XML attribute.
The output value is a single value: the comment length.
The reducer code iterates through the given set of values and adds each value to an in-memory list.
After iteration, the comment lengths are sorted to find the median value.
If the list has an odd number of entries, the median value is set to the middle value.
If the number is even, the middle two values are averaged.
Next, the standard deviation is calculated by iterating through our sorted list after finding the mean from our running sum and count.
A running sum of deviations is calculated by squaring the difference between each comment length and the mean.
Finally, the median and standard deviation are output along with the input key.
The reducer requires all the values associated with a key in order to find the median and standard.
Because a combiner runs only over a map’s locally output intermediate key/ value pairs, being able to calculate the full median and standard deviation is impossible.
However, the next example describes aa more complex implementation that uses a custom combiner.
Problem: Given a list of user’s comments, determine the median and standard deviation of comment lengths per hour of day.
The mapper processes each input record to calculate the median comment length based on the hour of the day during which the comment was posted.
The output key is the hour of day, which is parsed from the creation date XML attribute.
The output value is a SortedMapWritable object that contains one element: the comment length and a count of “1”
This map is used more heavily in the combiner and reducer.
Grab the "CreationDate" field, // since it is what we are grouping by.
The reducer code iterates through the given set of SortedMapWritable to aggregate all the maps together into a single TreeMap, which is a implementation of SortedMap.
The key is the comment length and the value is the total count associated with the comment length.
Next, the standard deviation is calculated by iterating through the TreeMap again and finding the sum of squares, making sure to multiply by the count associated with the comment length.
The median and standard deviation are output with the input key, the hour during which these comments were posted.
Unlike the previous examples, the combiner for this algorithm is different from the reducer.
While the reducer actually calculates the median and standard deviation, the combiner aggregates the SortedMapWritable entries for each local map’s intermediate key/value pairs.
The code to parse through the entries and aggregate them in a local map is identical to the reducer code in the previous section.
Here, a HashMap is used instead of a TreeMap, because sorting is unnecessary and a HashMap is typically faster.
While the reducer uses this map to calculate the median and standard deviation, the combiner uses a SortedMapWritable in order to serialize it for the reduce phase.
Figure 2-4 shows the flow between the mapper, combiner, and reducer to help describe their interactions.
A combiner possibly executes over each of the highlighted output groups from a mapper.
For each group, it builds the internal map of comment length to the count of comment lengths.
The combiner then outputs the input key and the SortedMapWritable of length/count pairs, which it serializes from the map.
Inverted Index Summarizations Pattern Description The inverted index pattern is commonly used as an example for MapReduce analytics.
We’re going to discuss the general case where we want to build a map of some term to a list of identifiers.
Generate an index from a data set to allow for faster searches or data enrichment capabilities.
It is often convenient to index large data sets on keywords, so that searches can trace terms back to records that contain specific values.
While building an inverted index does require extra processing up front, taking the time to do so can greatly reduce the amount of time it takes to find something.
Imagine entering a keyword and letting the engine crawl the Internet and build a list of pages to return to you.
Such a query would take an extremely long amount of time to complete.
By building an inverted index, the search engine knows all the web pages related to a keyword ahead of time and these results are simply displayed to the user.
These indexes are often ingested into a database for fast query responses.
Building an inverted index is a fairly straightforward application of MapReduce because the framework handles a majority of the work.
Inverted indexes should be used when quick search query responses are required.
The results of such a query can be preprocessed and ingested into a database.
Figure 2-5 shows the general structure of how an inverted index is executed in MapReduce.
The breakdown of each MapReduce component is described in detail below:
The mapper outputs the desired fields for the index as the key and the unique identifier as the value.
The combiner can be omitted if you are just using the identity reducer, because under those circumstances a combiner would just create unnecessary processing.
Some implementations concatenate the values associated with a group before outputting them to the file system.
It won’t have as beneficial an impact on byte count as the combiners in other patterns, but there will be an improvement.
The partitioner is responsible for determining where values with the same key will eventually be copied by a reducer for final output.
It can be customized for more efficient load balancing if the intermediate keys are not evenly distributed.
The reducer will receive a set of unique record identifiers to map back to the input key.
The identifiers can either be concatenated by some unique delimiter, leading to the output of one key/value pair per group, or each input value can be written with the input key, known as the identity reducer.
The final output of is a set of part files that contain a mapping of field value to a set of unique IDs of records containing the associated field value.
The performance of building an inverted index depends mostly on the computational cost of parsing the content in the mapper, the cardinality of the index keys, and the number of content identifiers per key.
Parsing text or other types of content in the mapper can sometimes be the most computationally intense operation in a MapReduce job.
This is especially true for semistructured data, such as XML or JSON, since these typically require parsing arbitrary quantities of information into usable objects.
It’s important to parse the incoming records as efficiently as possible to improve your overall job performance.
If the number of unique keys and the number of identifiers is large, more data will be sent to the reducers.
If more data is going to the reducers, you should increase the number of reducers to increase parallelism during the reduce phase.
Inverted indexes are particularly susceptible to hot spots in the index keys, since the index keys are rarely evenly distributed.
For example, the reducer that handles the word “the” in a text search application is going to be particularly busy since “the” is seen in so much text.
This can slow down your entire job since a few reducers will take much longer than the others.
To avoid this problem, you might need to implement a custom partitioner, or omit common index keys that add no value to your end goal.
Building an inverted index is a straightforward MapReduce application and is often the second example newcomers to MapReduce experience after the word count application.
Much like the word count application, the bulk of the operation is a group and is therefore handled entirely by the MapReduce framework.
Suppose we want to add StackOverflow links to each Wikipedia page that is referenced in a StackOverflow comment.
The following example analyzes each comment in StackOverflow to find hyperlinks to Wikipedia.
If there is one, the link is output with the comment ID to generate the inverted index.
When it comes to the reduce phase, all the comment IDs that reference the same hyperlink will be grouped together.
These groups are then concatenated together into a white space delimited String and directly output to the file system.
From here, this data file can be used to update the Wikipedia page with all the comments that reference it.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a set of user’s comments, build an inverted index of Wikipedia URLs to a set of answer post IDs.
The mapper parses the posts from StackOverflow to output the row IDs of all answer posts that contain a particular Wikipedia URL.
First, the XML attributes for the text, post type, and row ID are extracted.
If the post type is not an answer, identified by a post type of “2”, we parse the text to find a Wikipedia URL.
This is done using the getWikipediaURL method, which takes in a String of unescaped HTML and returns a Wikipedia URL if found, or null otherwise.
If a URL is found, the URL is output as the key and the row ID is output as the value.
The reducer iterates through the set of input values and appends each row ID to a String, delimited by a space character.
The combiner can be used to do some concatenation prior to the reduce phase.
Because all row IDs are simply concatenated together, the number of bytes that need to be copied by the reducer is more than in a numerical summarization pattern.
The same code for the reducer class is used as the combiner.
Counting with Counters Pattern Description This pattern utilizes the MapReduce framework’s counters utility to calculate a global sum entirely on the map side without producing any output.
An efficient means to retrieve count summarizations of large data sets.
A count or summation can tell you a lot about particular fields of data, or your data as a whole.
Hourly ingest record counts can be post processed to generate helpful histograms.
The single reduce will sum all the input values and output the final record count with the hour.
This works very well, but it can be done more efficiently using counters.
Instead of writing any key value pairs at all, simply use the framework’s counting mechanism to keep track of the number of input records.
This requires no reduce phase and no summation! The framework handles monitoring the names of the counters and their associated values, aggregating them across all tasks, as well as taking into account any failed task attempts.
Say you want to find the number of times your employees log into your heavily used public website every day.
Assuming you have a few dozen employees, you can apply filter conditions while parsing through your web logs.
At the end of the job, simply grab the counters from the framework and save them wherever your heart desires—the log, local file system, HDFS, etc.
Some counters come built into the framework, such as number of input/output records and bytes.
Hadoop allows for programmers to create their own custom counters for whatever their needs may be.
This pattern describes how to utilize these custom counters to gather count or summation metrics from your data sets.
The major benefit of using counters is all the counting can be done during the map phase.
The caveat to using counters is they are all stored in-memory by the JobTracker.
The counters are serialized by each map task and sent with status updates.
In order to play nice and not bog down the JobTracker, the number of counters should be in the tens -- a hundred at most...
The last thing you want is to have your analytic take down the JobTracker because you created a few hundred custom counters!
You have a desire to gather counts or summations over large data sets.
The number of counters you are going to create is small—in the double digits.
Figure 2-6 shows the general structure of how this pattern works in MapReduce.
The mapper processes each input record at a time to increment counters based on certain criteria.
The counter is either incremented by one if counting a single instance, or incremented by some number if executing a summation.
These counters are then aggregated by the TaskTrackers running the tasks and incrementally reported to the JobTracker for overall aggregation upon job success.
The counters from any failed tasks are disregarded by the JobTracker in the final summation.
As this job is map only, there is no combiner, partitioner, or reducer required.
The final output is a set of counters grabbed from the job framework.
This directory will exist and contain a number of empty part files equivalent to the number of map tasks.
Simply counting the number of records over a given time period is very common.
It’s typically a counter provided by the framework, among other common things.
Count a small number of unique instances Counters can also be created on the fly by using a string variable.
You might now know what the value is, but the counters don’t have to be created ahead of time.
Simply creating a counter using the value of a field and incrementing it is enough to solve this use case.
Just be sure the number of counters you are creating is a small number!
Summations Counters can be used to sum fields of data together.
Rather than performing the sum on the reduce side, simply create a new counter and use it to sum the field values.
Using counters is very fast, as data is simply read in through the mapper and no output is written.
Performance depends largely on the number of map tasks being executed and how much time it takes to process each record.
For this example, we use a map-only job to count the number of users in each state.
The Location attribute is a user-entered value and doesn’t have any concrete inputs.
Because of this, there are a lot of null or empty fields, as well as made up locations.
We need to account for this when processing each record to ensure we don’t create a large number of counters.
We verify each location contains a state abbreviation code prior to creating a counter.
This is a manageable number of custom counters for the JobTracker, but your job should not have many more than this!
The following descriptions of each code section explain the solution to the problem.
Problem: Count the number of users from each state using Hadoop custom counters.
The mapper reads each user record and gets his or her location.
The location is split on white space and searched for something that resembles a state.
We keep a set of all the state abbreviations in-memory to prevent creating an excessive amount of counters, as the location is simply a string set by the user and nothing structured.
If a state is recognized, the counter for the state is incremented by one and the loop is broken.
Counters are identified by both a group and a name.
Here, the group is “State” (identified by a public String variable) and the counter name is the state abbreviation code.
The driver code is mostly boilerplate, with the exception of grabbing the counters after the job completes.
These counter values are also output when the job completes, so writing to stdout may be redundant if you are obtaining these values by scraping log files.
The output directory is then deleted, success or otherwise, as this job doesn’t create any tangible output.
The patterns in this chapter all have one thing in common: they don’t change the actual records.
These patterns all find a subset of data, whether it be small, like a top-ten listing, or large, like the results of a deduplication.
This differentiates filtering patterns from those in the previous chapter, which was all about summarizing and grouping data by similar fields to get a top-level view of the data.
Filtering is more about understanding a smaller piece of your data, such as all records generated from a particular user, or the top ten most used verbs in a corpus of text.
In short, filtering allows you to apply a microscope to your data.
If you are interested in finding all records that involve a particular piece of distinguishing information, you can filter out records that do not match the search criteria.
Sampling, one common application of filtering, is about pulling out a sample of the data, such as the highest values for a particular field or a few random records.
Sampling can be used to get a smaller, yet representative, data set in which more analysis can be done without having to deal with the much larger data set.
Many machine learning algorithms simply do not work efficiently over a large data set, so tools that build models need to be applied to a smaller subset.
Simply grabbing the first thousand records typically is not the best sample since the records are bound to be similar and do not give a good overall picture of the entire data set.
A well-distributed sample will hopefully provide a better view of the data set and will allow your application and analytic development to be done against more realistic data, even if it is much smaller.
Four patterns are presented in this chapter: filtering, Bloom filtering, top ten, and distinct.
There are numerous ways to find a slice of your data.
Each pattern has a slight nuance to distinguish it from the others, even if they all pretty much do the same thing.
We will see a few clever uses of MapReduce in this chapter.
Filtering, Bloom filtering, and simple random sampling allow us to use map-only jobs, which means we don’t need a reducer.
Filtering Pattern Description As the most basic pattern, filtering serves as an abstract pattern for some of the other patterns.
Filtering simply evaluates each record separately and decides, based on some condition, whether it should stay or go.
Filter out records that are not of interest and keep ones that are.
Consider an evaluation function f that takes a record and returns a Boolean value of true or false.
If this function returns true, keep the record; otherwise, toss it out.
Your data set is large and you want to take a subset of this data to focus in on it and perhaps do follow-on analysis.
The subset might be a significant portion of the data set or just a needle in the haystack.
Either way, you need to use the parallelism of MapReduce to wade through all of your data and find the keepers.
For example, you might be interested only in records that have something to do with Hadoop: Hadoop is either mentioned in the raw text or the event is tagged by a “Hadoop” tag.
Filtering can be used to keep records that meet the “something to do with Hadoop” criteria and keep them, while tossing out the rest of the records.
Big data and processing systems like Hadoop, in general, are about bringing all of your organization’s data to one location.
Filtering is the way to pull subsets back out and deliver them to analysis shops that are interested in just that subset.
Filtering is also used to zoom in on a particular set of records that match your criteria that you are more curious about.
The exploration of a subset of data may lead to more valuable and complex analytics that are based on the behavior that was observed in the small subset.
The only requirement is that the data can be parsed into “records” that can be categorized through some well-specified criterion determining whether they are to be kept.
The structure of the filter pattern is perhaps the simplest of all the patterns we’ll see in this book.
Filtering is unique in not requiring the “reduce” part of MapReduce.
Each record is looked at individually and the evaluation of whether or not to keep that record does not depend on anything else in the data set.
The mapper applies the evaluation function to each record it receives.
Typically, the mapper outputs the same key/value type as the types of the input, since the record is left unchanged.
If the evaluation function returns true, the mapper simply output the key and value verbatim.
The output of the job will be a subset of the records that pass the selection criteria.
If the format was kept the same, any job that ran over the larger data set should be able to run over this filtered data set, as well.
Prepare a particular subset of data, where the records have something in common or something of interest, for more examination.
For example, a local office in Maryland may only care about records originating in Maryland from your international dataset.
Tracking a thread of events Extract a thread of consecutive events as a case study from a larger data set.
For example, you may be interested in how a particular user interacts with your website by analyzing Apache web server logs.
The events for a particular user are interspersed with all the other events, so it’s hard to figure out what happened.
By filtering for that user’s IP address, you are able to get a good view of that particular user’s activities.
Distributed grep Grep, a very powerful tool that uses regular expressions for finding lines of text of interest, is easily parallelized by applying a regular expression match against each line of input and only outputting lines that match.
Data cleansing Data sometimes is dirty, whether it be malformed, incomplete, or in the wrong format.
The data could have missing fields, a date could be not formatted as a date, or random bytes of binary data could be present.
Filtering can be used to validate that each record is well-formed and remove any junk that does occur.
Simple random sampling If you want a simple random sampling of your data set, you can use filtering where the evaluation function randomly returns true or false.
A simple random sample is a sample of the larger data set in which each item has the same probability of being selected.
You can tweak the number of records that make it through by having the evaluation function return true a smaller percentage of the time.
For example, if your data set contains one trillion records and you want a sample size of about one million, have the evaluation function return true once in a million (because there are a million millions in a trillion)
Removing low scoring data If you can score your data with some sort of scalar value, you can filter out records that don’t meet a certain threshold.
If you know ahead of time that certain types of records are not useful for analysis, you can assign those records a small score and they will get filtered out.
This effectively has the same purpose as the top ten pattern discussed later, except that you do not know how many records you will get.
The filter pattern is synonymous to using the WHERE clause in a SELECT * statement.
The records stay the same, but some are simply filtered out.
This pattern is basically as efficient as MapReduce can get because the job is map-only.
There are a couple of reasons why map-only jobs are efficient.
Since no reducers are needed, data never has to be transmitted between the map and reduce phase.
Most of the map tasks pull data off of their locally attached disks and then write back out to that node.
Since there are no reducers, both the sort phase and the reduce phase are cut out.
This usually doesn’t take very long, but every little bit helps.
One thing to be aware of is the size and number of the output files.
Since this job is running with mappers only, you will get one output file per mapper with the prefix part-m- (note the m instead of the r)
You may find that these files will be tiny if you filter out a lot of data, which can cause problems with scalability limitations of the NameNode further down the road.
If you are worried about the number of small files and do not mind if your job runs just a little bit longer, you can use an identity reducer to collect the results without doing anything with them.
This will have the mapper send the reducer all of the data, but the reducer does nothing other than just output them to one file per reducer.
The appropriate number of reducers depends on the amount of data that will be written to the file system and just how many small files you want to deal with.
Grep is a popular text filtering utility that dates back to Unix and is available on most Unix-like systems.
It scans through a file line-by-line and only outputs lines that match a specific pattern.
We’d like to parallelize the regular expression search across a larger body of text.
In this example, we’ll show how to apply a regular expression to every line in MapReduce.
The mapper is pretty straightforward since we use the Java built-in libraries for regular expressions.
If the text line matches the pattern, we’ll output the line.
Otherwise we do nothing and the line is effectively ignored.
We use the setup function to retrieve the map regex from the job configuration.
As this is a map-only job, there is no combiner or reducer.
All output records will be written directly to the file system.
In simple random sampling (SRS), we want to grab a subset of our larger data set in which each record has an equal probability of being selected.
Typically this is useful for sizing down a data set to be able to do representative analysis on a more manageable set of data.
Implementing SRS as a filter operation is not a direct application of the filtering pattern, but the structure is the same.
Instead of some filter criteria function that bears some relationship to the content of the record, a random number generator will produce a value, and if the value is below a threshold, keep the record.
In the mapper code, the setup function is used to pull the filter_per centage configuration value so we can use it in the map function.
In the map function, a simple check against the next random number is done.
As this is a map-only job, there is no combiner or reducer.
All output records will be written directly to the file system.
When using a small percentage, you will find that the files will be tiny and plentiful.
If this is the case, set the number of reducers to 1 without specifying a reducer class, which will tell the MapReduce framework to use a single identity reducer that simply collects the output into a single file.
The other option would be to collect the files as a post-processing step using hadoop fs -cat.
Bloom Filtering Pattern Description Bloom filtering does the same thing as the previous pattern, but it has a unique evaluation function applied to each record.
Filter such that we keep records that are member of some predefined set of values.
It is not a problem if the output is a bit inaccurate, because we plan to do further checking.
The predetermined list of values will be called the set of hot values.
If that feature is a member of a set of values represented by a Bloom filter, keep it; otherwise toss it out (or the reverse)
Bloom filtering is similar to generic filtering in that it is looking at each record and deciding whether to keep or remove it.
However, there are two major differences that set it apart from generic filtering.
First, we want to filter the record based on some sort of set membership operation against the hot values.
For example: keep or throw away this record if the value in the user field is a member of a predetermined list of users.
Second, the set membership is going to be evaluated with a Bloom filter, described in the Appendix A.
In one sense, Bloom filtering is a join operation in which we don’t care about the data values of the right side of the join.
It is comparing one list to another and doing some sort of join logic, using only map tasks.
Instead of replicating the hot list everywhere with the distributed cache, as in the replicated join, we will send a Bloom filter data object to the distributed cache.
This allows a filter like operation with a Bloom filter instead of the list itself, which allows you to perform this operation across a much larger data set because the Bloom filter is much more compact.
Instead of being constrained by the size of the list in memory, you are mostly confined by the feature limitations of Bloom filters.
Using a Bloom filter to calculate set membership in this situation has the consequence that sometimes you will get a false positive.
That is, sometimes a value will return as a member of the set when it should not have.
If the Bloom filter says a value is not in the Bloom filter, we can guarantee that it is indeed not in the set of values.
For more information on why this happens, refer to Appendix A.
However, in some situations, this is not that big of a concern.
In an example we’ll show code for at the end of this chapter, we’ll gather a rather large set of “interesting” words, in which when we see a record that contains one of those words, we’ll keep the record, otherwise we’ll toss it out.
We want to do this because we want to filter down our data set significantly by removing uninteresting content.
If we are using a Bloom filter to represent the list of watch words, sometimes a word will come back as a member of that list, even if it should not have.
In this case, if we accidentally keep some records, we still achieved our goal of filtering out the majority of the garbage and keeping interesting stuff.
The following criteria are necessary for Bloom filtering to be relevant:
A feature can be extracted from each record that could be in a set of hot values.
There is a predetermined set of items for the hot values.
Figure 3-2 shows the structure of Bloom filtering and how it is split into two major components.
First, the Bloom filter needs to be trained over the list of values.
Next is the filtering MapReduce job, which has the same structure as the previous filtering pattern in this chapter, except it will make use of the distributed cache as well.
There are no reducers since the records are analyzed one-by-one and there is no aggregation done.
The first step of this job is to train the Bloom filter from the list of values.
This is done by loading the data from where it is stored and adding each item to the Bloom filter.
The trained Bloom filter is stored in HDFS at a known location.
The second step of this pattern is to do the actual filtering.
When the map task starts, it loads the Bloom filter from the distributed cache.
Then, in the map function, it iterates through the records and checks the Bloom filter for set membership in the hot values list.
Each record is either forwarded or not based on the Bloom filter membership test.
The Bloom filter needs to be re-trained only when the data changes.
Therefore, updating the Bloom filter in a lazy fashion (i.e., only updating it when it needs to be updated) is typically appropriate.
The output of the job will be a subset of the records in that passed the Bloom filter membership test.
You should expect that some records in this set may not actually be in the set of hot values, because Bloom filters have a chance of false positives.
The most straightforward use case is cleaning out values that aren’t hot.
For example, you may be interested only in data that contains a word in a list of 10,000 words that deal with Hadoop, such as “map,” “partitioning,” etc.
You take this list, train a Bloom filter on it, then check text as it is coming in to see whether you get a Bloom filter hit on any of the words.
If you do, forward the record, and if not don’t do anything.
The fact that you’ll get some false positives isn’t that big of a deal, since you still got rid of most of the data.
Prefiltering a data set for an expensive set membership check Sometimes, checking whether some value is a member of a set is going to be expensive.
For example, you might have to hit a webservice or an external database to check whether that value is in the set.
The situations in which this may be the case are far and few between, but they do crop up in larger organizations.
Instead of dumping this list periodically to your cluster, you can instead have the originating system produce a Bloom filter and ship that instead.
Once you have the Bloom filter in place and filter out most of the data, you can do a second pass on the records that make it through to double check against the authoritative source.
Later, in Chapter 5, we’ll see a pattern called “Reduce Side Join with Bloom Filtering” where a Bloom filter is used to reduce the amount of data going to reducers.
By determining whether a record will be relevant ahead of time, we can reduce network usage significantly.
Bloom filters are relatively new in the field of data analysis, likely because the properties of big data particularly benefit from such a thing in a way previous methodologies have not.
In both SQL and Pig, Bloom filters can be implemented as user-defined functions, but as of the writing of this book, there is no native functionality out of the box.
The performance for this pattern is going to be very similar to simple filtering from a performance perspective.
Loading up the Bloom filter from the distributed cache is not that expensive since the file is relatively small.
Checking a value against the Bloom filter is also a relatively cheap operation, as each test is executed in constant time.
One of the most basic applications of a Bloom filter is what it was designed for: representing a data set.
For this example, a Bloom filter is trained with a hot list of keywords.
We use this Bloom filter to test whether each word in a comment is in the hot list.
If the test returns true, the entire record is output.
Here, we are not concerned with the inevitable false positives that are output due to the Bloom filter.
The next example details how one way to verify a positive Bloom filter test using HBase.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of user’s comments, filter out a majority of the comments that do not contain a particular keyword.
To demonstrate how to use Hadoop Bloom filters, the following code segment generates a Bloom filter off a predetermined set of words.
This is a generic application that takes in an input gzip file or directory of gzip files, the number of elements in the file, a desired false positive rate, and finally the output file name.
A new BloomFilter object is constructed using the optimal vector size and optimal number of hash functions (k) based on the input parameters.
Each file returned from listStatus is read line-by-line, and each line is used to train the Bloom filter.
After all the input files are ready, the Bloom filter is serialized to the filename provided at the command line.
Because a BloomFilter is also a Writable object, serializing it is fairly trivial.
This Bloom filter can later be deserialized from HDFS just as easily as it was written.
Just open up the file using the FileSystem object and pass it to BloomFilter.read Fields.
Deserialization of this Bloom filter is demonstrated in the setup method of the following Mapper code.
The setup method is called once for each mapper by the Hadoop framework prior to the many calls to map.
Here, the Bloom filter is deserialized from the DistributedCache before being used in the map method.
The DistributedCache is a Hadoop utility that ensures that a file in HDFS is present on the local file system of each task that requires that file.
The Bloom filter was previously trained with a hot list of words.
In the map method, the comment is extracted from each input record.
The comment is tokenized into words, and each word is cleaned of any extraneous characters.
If the word is a member, the entire record is output to the file system.
A Bloom filter is trained on the bytes of the word.
The important thing of this is that the words “the” and “The” may look the same, but the bytes are different.
Unless case sensitivity matters in you algorithm, it is best to trim the string and make the string all lower case when training and testing the filter.
Because this is a map-only job, there is no combiner or reducer.
All output records will be written directly to the file system.
Bloom filters can assist expensive operations by eliminating unnecessary ones.
For the following example, a Bloom filter was previously trained with IDs of all users that have a reputation of at least 1,500
We use this Bloom filter to do an initial test before querying HBase to retrieve more information about each user.
By eliminating unnecessary queries, we can speed up processing time.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of users’ comments, filter out comments from users with a reputation of less than 1,500
The setup method is called once for each mapper by the Hadoop framework prior to the many calls to the map method.
Just like the previous example, the Bloom filter is deserialized from the DistributedCache before being used in the map method.
This Bloom filter was trained with all user IDs that have a reputation of at least 1,500
This is a little over 1.5% of all users, so we will be filtering out a lot of unnecessary queries.
In addition to the Bloom filter, a connection to the HBase table is obtained in setup.
In the map method, the user’s ID is extracted from each record and checked against the Bloom filter.
If the test is positive, HBase is queried with the user ID to get the rest of the data associated with that user.
Here, we nullify the possibilities of outputting false positives by verifying that the user’s actual reputation is at least 1,500
If it is, the record is output to the file system.
As this is a map-only job, there is no combiner or reducer.
All output records will be written directly to the file system.
Query Buffer Optimization The previous example is a fairly naive way of querying HBase.
It is meant to show how to go about executing the pattern, but can be optimized further.
HBase supports batch queries, so it would be ideal to buffer all the queries we want to execute up to some predetermined size.
This constant depends on how many records you can comfortably store in memory before querying HBase.
Then flush the queries to HBase and perform the further processing with the returned results.
If the expensive operations can be buffered, it is recommended to do so.
Just remember to flush the buffer in the mapper or the reducer’s cleanup method.
The Context object can be used to write output just like in the map or reduce methods.
Top Ten Pattern Description The top ten pattern is a bit different than previous ones in that you know how many records you want to get in the end, no matter what the input size.
In generic filtering, however, the amount of output depends on the data.
Retrieve a relatively small number of top K records, according to a ranking scheme in your data set, no matter how large the data.
Finding outliers is an important part of data analysis because these records are typically the most interesting and unique pieces of data in the set.
The point of this pattern is to find the best records for a specific criterion so that you can take a look at them and perhaps figure out what caused them to be so special.
If you can define a ranking function or comparison function between two records that determines whether one is higher than the other, you can apply this pattern to use MapReduce to find the records with the highest value across your entire data set.
The reason why this pattern is particularly interesting springs from a comparison with how you might implement the top ten pattern outside of a MapReduce context.
In SQL, you might be inclined to sort your data set by the ranking value, then take the top K records from that.
In MapReduce, as we’ll find out in the next chapter, total ordering is extremely involved and uses significant resources on your cluster.
This pattern will instead go about finding the limited number of high-values records without having to sort the data.
Plus, seeing the top ten of something is always fun! What are the highest scoring posts on Stack Overflow? Who is the oldest member of your service? What is the largest single order made on your website? Which post has the word “meow” the most number of times?
This pattern requires a comparator function ability between two records.
That is, we must be able to compare one record to another to determine which is “larger.”
The number of output records should be significantly fewer than the number of input records because at a certain point it just makes more sense to do a total ordering of the data set.
The mappers will find their local top K, then all of the individual top K sets will compete for the final top K in the reducer.
Since the number of records coming out of the mappers is at most K and K is relatively small, we’ll only need one reducer.
You can see the structure of this pattern in Figure 3-3
The mapper reads each record and keeps an array object of size K that collects the largest K values.
In the cleanup phase of the mapper (i.e., right before it exits), we’ll finally emit the K records stored in the array as the value, with a null key.
These are the lowest K for this particular map task.
We should expect K * M records coming into the reducer under one key, null, where M is the number of map tasks.
In the reduce function, we’ll do what we did in the mapper: keep an array of K values and find the top K out of the values collected under the null key.
The reason we had to select the top K from every mapper is because it is conceivable that all of the top records came from one file split and that corner case needs to be accounted for.
They may be the users that are having difficulty using your system, or power users of your website.
Outliers, like filtering and grouping, may give you another perspective from your data set.
Select interesting data If you are able to score your records by some sort of value score, you can pull the “most valuable” data.
This is particularly useful if you plan to submit data to followon processing, such as in a business intelligence tool or a SQL database, that cannot handle the scale of your original data set.
Value scoring can be as complex as you make it by applying advanced algorithms, such as scoring text based on how grammatical it is and how accurate the spelling is so that you remove most of the junk.
Catchy dashboards This isn’t a psychology book, so who knows why top ten lists are interesting to consumers, but they are.
This pattern could be used to publish some interesting top ten stats about your website and your data that will encourage users to think more about your data or even to instill some competition.
In a traditional and small SQL database, ordering may not be a big deal.
In this case, you would retrieve data ordered by the criterion for which you want the top ten, then take a limit.
You could follow this same approach in MapReduce, but as you will find out in later patterns, sorting is an expensive operation.
Pig Pig will have issues performing this query in any sort of optimal way.
The most straightforward pattern is to mirror the SQL query, but the ordering is expensive just to find a few records.
This is a situation in which you’ll find major gains in using Java MapReduce instead of Pig.
The performance of the top ten pattern is typically very good, but there are a number of important limitations and concerns to consider.
Most of these limitations spring from the use of a single reducer, regardless of the number of records it is handling.
The number we need to pay attention to when using this pattern is how many records the reducer is getting.
Each map task is going to output K records, and the job will consist of M map tasks, so the reducer is going to have to work through K * M records.
A single reducer getting a lot of data is bad for a few reasons:
The sort can become an expensive operation when it has too many records and has to do most of the sorting on local disk, instead of in memory.
The host where the reducer is running will receive a lot of data over the network, which may create a network resource hot spot for that single host.
Naturally, scanning through all the data in the reduce will take a long time if there are many records to look through.
Any sort of memory growth in the reducer has the possibility of blowing through the Java virtual machine’s memory.
For example, if you are collecting all of the values into an ArrayList to perform a median, that ArrayList can get very big.
This will not be a particular problem if you’re really looking for the top ten items, but if you want to extract a very large number you may run into memory limits.
Writing to the locally attached disk can be one of the more expensive operations in the reduce phase when we are dealing with a lot of data.
Since there is only one reducer, we are not taking advantage of the parallelism involved in writing data to several hosts, or even several disks on the same host.
Again, this is not an issue for the top ten, but becomes a factor when the data extracted is very large.
Consider the extreme case in which K is set at five million, when there are ten million records in the entire data set.
Five million exceeds the number of records in any individual input split, so every mapper will send all of its records to the reducer.
The single reducer will effectively have to handle all of the records in the entire dataset and the only thing that was parallelized was the data loading.
An optimization you could take if you have a large K and a large number of input splits is to prefilter some of the data, because you know what the top ten was last time and it hasn’t changed much.
Imagine your data has a value that can only increase with time (e.g., hits on web pages) and you want to find the top hundred records.
There is no way that a record with a value with less than 52,845 can compete with the previous top hundred that are still in the data set.
For all these reasons, this pattern is intended only for pretty small values for K, in the tens or hundreds at most, though you can likely push it a bit further.
There is a fuzzy line in which just doing a total ordering of the data set is likely more effective.
Determining the top ten records of a data set is an interesting use of MapReduce.
Each mapper determines the top ten records of its input split and outputs them to the reduce phase.
The mappers are essentially filtering their input split to the top ten records, and the reducer is responsible for the final ten.
Just remember to configure your job to only use one reducer! Multiple reducers would shard the data and would result in multiple “top ten” lists.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of user information, output the information of the top ten users based on reputation.
The mapper processes all input records and stores them in a TreeMap.
A TreeMap is a subclass of Map that sorts on key.
Then, if there are more than ten records in our TreeMap, the first element (lowest value) can be removed.
After all the records have been processed, the top ten records in the TreeMap are output to the reducers in the cleanup method.
This method gets called once after all key/value pairs have been through map, just like how setup is called once before any calls to map.
Overall, the reducer determines its top ten records in a way that’s very similar to the mapper.
The reducer iterates through all these records and stores them in a TreeMap.
If the TreeMap’s size is above ten, the first element (lowest value) is remove from the map.
After all the values have been iterated over, the values contained in the TreeMap are flushed to the file system in descending order.
This ordering is achieved by getting the descending map from the TreeMap prior to outputting the values.
This can be done directly in the reduce method, because there will be only one input group, but doing it in the cleanup method would also work.
There is no need for a combiner in this job, although the reducer code could technically be used.
The combiner would simply output the same ten records and thus cause unnecessary processing.
Also, this job is hardcoded to find the top ten records, but could easily be configured to find the top K records using a variable captured in the setup method.
Just be sure to keep in mind the limitations discussed in the Performance Analysis section as K increases.
Distinct Pattern Description This pattern filters the whole set, but it’s more challenging because you want to filter out records that look like another record in the data set.
The final output of this filter application is a set of unique records.
You have data that contains similar records and you want to find a unique set of values.
Reducing a data set to a unique set of values has several uses.
One particular use case that can use this pattern is deduplication.
In some large data sets, duplicate or extremely similar records can become a nagging problem.
The duplicate records can take up a significant amount of space or skew top-level analysis results.
For example, every time someone visits your website, you collect what web browser and device they are using for marketing analysis.
If that user visits your website more than once, you’ll log that information more than once.
If you do some analysis to calculate the percentage of your users that are using a specific web browser, the number of times users have used your website will skew the results.
Therefore, you should first deduplicate the data so that you have only one instance of each logged event with that device.
Records don’t necessarily need to be exactly the same in the raw form.
They just need to be able to be translated into a form in which they will be exactly the same.
For example, if our web browser analysis done on HTTP server logs, extract only the user name, the device, and the browser that user is using.
We don’t care about the time stamp, the resource they were accessing, or what HTTP server it came from.
The only major requirement is that you have duplicates values in your data set.
This is not a requirement, but it would be silly to use this pattern otherwise!
This pattern is pretty slick in how it uses MapReduce.
It exploits MapReduce’s ability to group keys together to remove duplicates.
This pattern uses a mapper to transform the data and doesn’t do much in the reducer.
The combiner can always be utilized in this pattern and can help considerably if there are a large number of duplicates.
Duplicate records are often located close to another in a data set, so a combiner will deduplicate them in the map phase.
The mapper takes each record and extracts the data fields for which we want unique values.
In our HTTP logs example, this means extracting the user, the web browser, and the device values.
The mapper outputs the record as the key, and null as the value.
The reducer groups the nulls together by key, so we’ll have one null per key.
We then simply output the key, since we don’t care how many nulls we have.
Because each key is grouped together, the output data set is guaranteed to be unique.
One nice feature of this pattern is that the number of reducers doesn’t matter in terms of the calculation itself.
Set the number of reducers relatively high, since the mappers will forward almost all their data to the reducers.
This is a good time to resize your data file sizes.
If you want your output files to be larger, reduce the number of reducers.
If you want them smaller, increase the number of reducers.
The files will come out to be about the same size thanks to the random hashing in the partitioner.
The output data records are guaranteed to be unique, but any order has not been preserved due to the random partitioning of the records.
If you have a system with a number of collection sources that could see the same event twice, you can remove duplicates with this pattern.
Getting distinct values This is useful when your raw records may not be duplicates, but the extracted information is duplicated across records.
Protecting from an inner join explosion If you are about to do an inner join between two data sets and your foreign keys are not unique, you risk retrieving a huge number of records.
Understanding this pattern’s performance profile is important for effective use.
The main consideration in determining how to set up the MapReduce job is the number of reducers you think you will need.
The number of reducers is highly dependent on the total number of records and bytes coming out of the mappers, which is dependent on how much data the combiner is able to eliminate.
Basically, if duplicates are very rare within an input split (and thus the combiner did almost nothing), pretty much all of the data is going to be sent to the reduce phase.
You can find the number of output bytes and records by looking at the JobTracker status of the job on a sample run.
Take the number of output bytes and divide by the number of reducers you are thinking about using.
That is about how many bytes each reducer will get, not accounting for skew.
The number that a reducer can handle varies from deployment to deployment, but usually you shouldn’t pass it more than a few hundred megabytes.
You also don’t want to pass too few records, because then your output files will be tiny and there will be unnecessary overhead in spinning up the reducers.
Since most of the data in the data set is going to be sent to the reducers, you will use a relatively large number of reducers to run this job.
Start with the theoretical estimate based on the output records, but do additional testing to find the sweet spot.
In general, with this pattern, if you want your reducers to run in half the time, double the number of reducers...
Be conscious of how many reduce slots your cluster has when selecting the number of reducers of your job.
A good start for the distinct pattern would be close to the number of reduce slots for reasonably sized data sets or twice the number of reduce slots for very large data sets.
Finding a distinct set of values is a great example of MapReduce’s power.
Because each reducer is presented with a unique key and a set of values associated with that key, in order to produce a distinct value, we simply need to set our key to whatever we are trying to gather a distinct set of.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of user’s comments, determine the distinct set of user IDs.
The Mapper will get the user ID from each input record.
This user ID will be output as the key with a null value.
The grunt work of building a distinct set of user IDs is handled by the MapReduce framework.
Each reducer is given a unique key and a set of null values.
These values are ignored and the input key is written to the file system with a null value.
A combiner can and should be used in the distinct pattern.
Duplicate keys will be removed from each local map’s output, thus reducing the amount of network I/O required.
The same code for the reducer can be used in the combiner.
In contrast to the previous chapter on filtering, this chapter is all about reorganizing data.
The value of individual records is often multipled by the way they are partitioned, sharded, or sorted.
This is especially true in distributed systems, where partitioning, sharding, and sorting can be exploited for performance.
In many organizations, Hadoop and other MapReduce solutions are only a piece in the larger data analysis platform.
Data will typically have to be transformed in order to interface nicely with the other systems.
Likewise, data might have to be transformed from its original state to a new state to make analysis in MapReduce easier.
This chapter contains several pattern subcategories as you will see in each pattern description:
The patterns in this chapter are often used together to solve data organization problems.
For example, you may want to restructure your data to be hierarchical, bin the data, and then have the bins be sorted.
Structured to Hierarchical Pattern Description The structured to hierarchical pattern creates new records from data that started in a very different structure.
Because of its importance, this pattern in many ways stands alone in the chapter.
Transform your row-based data to a hierarchical format, such as JSON or XML.
When migrating data from an RDBMS to a Hadoop system, one of the first things you should consider doing is reformatting your data into a more conducive structure.
Since Hadoop doesn’t care what format your data is in, you should take advantage of hierarchical data to avoid doing joins.
For example, our StackOverflow data contains a table about comments, a table about posts, etc.
It is pretty obvious that the data is stored in an normalized SQL database.
When you visit a post on StackOverflow, all the different pieces need to be coalesced into one view.
This gets even more complicated when you are trying to do analytics at the level of individual posts.
Imagine trying to correlate the length of the post with the length of the comments.
This requires you to first do a join, an expensive operation, then extract the data that allows you to do your real work.
If instead you group the data by post so that the comments are colocated with the posts and the edit revisions (i.e., denormalizing the tables), this type of analysis will be much easier and more intuitive.
Keeping the data in a normalized form in this case serves little purpose.
When someone posts an answer to a StackOverflow question, Hadoop can’t insert that record into the hierarchy immediately.
Therefore, creating the denormalized records for MapReduce has to be done in a batch fashion periodically.
Another way to deal with a steady stream of updates is HBase.
HBase is able to store data in a semi-structured and hierarchical fashion well.
MongoDB would also be a good candidate for storing this type of data.
The following should be true for this pattern to be appropriate:
You have data sources that are linked by some set of foreign keys.
MultipleInputs allows you to specify different input paths and different mapper classes for each input.
If you are loading data from only one source in this pattern, you don’t need this step.
The mappers load the data and parse the records into one cohesive format so that your work in the reducers is easier.
The output key should reflect how you want to identify the root of each hierarchical record.
For example, in our StackOverflow example, the root would be the post ID.
You also need to give each piece of data some context about its source.
You need to identify whether this output record is a post or a comment.
To do this, you can simply concatenate some sort of label to the output value text.
In general, a combiner isn’t going to help you too much here.
You could hypothetically group items with the same key and send them over together, but this has no major compression gains since all you would be doing is concatenating strings, so the size of the resulting string would be the same as the inputs.
The reducer receives the data from all the different sources key by key.
All of the data for a particular grouping is going to be provided for you in one iterator, so all that is left for you to do is build the hierarchical data structure from the list of data items.
With XML or JSON, you’ll build a single object and then write it out as output.
The examples in this section show XML, which provides several convenient methods for constructing data structures.
If you are using some other format, such as a custom format, you’ll just need to use the proper object building and serialization methods.
The output will be in a hierarchical form, grouped by the key that you specified.
However, be careful that many formats such as XML and JSON have some sort of toplevel root element that encompasses all of the records.
If you actually need the document to be well-formed top-to-bottom, it’s usually easier to add this header and footer text as some post-processing step.
Data arrives in disjointed structured data sets, and for analytical purposes it would be easier to bring the data together into more complex objects.
By doing this, you are setting up your data to take advantage of the NoSQL model of analysis.
Preparing data for HBase or MongoDB HBase is a natural way to store this data, so you can use this method to bring the data together in preparation for loading into HBase or MongoDB.
Creating a new table and then executing a bulk import via MapReduce is particularly effective.
The alternative is to do several rounds of inserts, which might be less efficient.
It’s rare that you would want to do something like this in a relational database, since storing data in this way is not conducive to analysis with SQL.
However, the way you would solve a similar problem in an RDBMS is to join the data and then perform analysis on the result.
You can have hierarchical bags and tuples, which make it easy to represent hierarchical structures and lists of objects in a single record.
The COGROUP method in Pig does a great job of bringing data together while preserving the original structure.
However, using the predefined keywords to do any sort of real analysis on a complex record is more challenging out of the box.
For this, a user-defined function is the right way to go.
Basically, you would use Pig to build and group the records, then a UDF to make sense of the data.
There are two performance concerns that you need to pay attention to when using this pattern.
First, you need to be aware of how much data is being sent to the reducers from the mappers, and second you need to be aware of the memory footprint of the object that the reducer builds.
Since records with the grouping key can be scattered anywhere in the data set, pretty much all of data is going to move across the network.
For this reason, you will need to pay particular attention to having an adequate number of reducers.
The same strategies apply here that are employed in other patterns that shuffle everything over the network.
The next major concern is the possibility of hot spots in the data that could result in an obscenely large record.
With large data sets, it is conceivable that a particular output record is going to have a lot of data associated with it.
Imagine that for some reason a post on StackOverflow has a million comments associated with it.
That would be extremely rare and unlikely, but not in the realm of the impossible.
If you are building some sort of XML object, all of those comments at one point might be stored in memory before writing the object out.
This can cause you to blow out the heap of the Java Virtual Machine, which obviously should be avoided.
Another problem with hot spots is a skew in how much data each reducer is handling.
This is going to be a similar problem in just about any MapReduce job.
In many cases the skew can be ignored, but if it really matters you can write a custom partitioner to split the data up more evenly.
In this example, we will take the posts and comments of the StackOverflow data and group them together.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a list of posts and comments, create a structured XML hierarchy to nest comments with their related post.
We don’t usually describe the code for the driver, but in this case we are doing something exotic with MultipleInputs.
All we do differently is create a MultipleInputs object and add the comments path and the posts path with their respective mappers.
The paths for the posts and comments data are provided via the command line, and the program retrieves them from the args array.
In this case, there are two mapper classes, one for comments and one for posts.
In both, we extract the post ID to use it as the output key.
We output the input value prepended with a character (“P” for a post or “C” for a comment) so we know which data set the record came from during the reduce phase.
All the values are iterated to get the post record and collect a list of comments.
We know which record is which by the flag we added to the value.
These flags are removed when assigning post or adding the list.
Then, if the post is not null, an XML record is constructed with the post as the parent and comments as the children.
We chose to use an XML library to build the final record, but please feel free to use whatever means you deem necessary.
The nestElements method takes the post and the list of comments to create a new string of XML to output.
It uses a DocumentBuilder and some additional helper methods to copy the Element objects into new ones, in addition to their attributes.
This copying occurs to rename the element tags from row to either post or comment.
The final Document is then transformed into an XML string.
This is a continuation of the previous example and will use the previous analytic’s output as the input to this analytic.
Now that we have the comments associated with the posts, we are going to associate the post answers with the post questions.
This needs to be done because posts consist of both answers and questions and are differentiated only by their PostTypeId.
We’ll group them together by Id in questions and ParentId in answers.
The main difference between the two applications of this pattern is that in this one we are dealing only with one data set.
Effectively, we are using a self-join here to correlate the different records from the same data set.
The following descriptions of each code section explain the solution to the problem.
Problem: Given the output of the previous example, perform a self-join operation to create a question, answer, and comment hierarchy.
The first thing the mapper code does is determine whether the record is a question or an answer, because the behavior for each will be different.
For a question, we will extract Id as the key and label it as a question.
For an answer, we will extract ParentId as the key and label it as an answer.
The reducer code is very similar to the that in the previous example.
It iterates through the input values and grabs the question and answer, being sure to remove the flag.
It then nests the answers inside the question in the same fashion as the previous example.
The difference is that tags are “question” instead of the “post” and “answer” instead of “comment.” The helper functions are omitted here for brevity.
Partitioning Pattern Description The partitioning pattern moves the records into categories (i.e., shards, partitions, or bins) but it doesn’t really care about the order of records.
The intent is to take similar records in a data set and partition them into distinct, smaller data sets.
If you want to look at a particular set of data—such as postings made on a particular date—the data items are normally spread out across the entire data set.
So looking at just one of these subsets requires an entire scan of all of the data.
Partitioning means breaking a large set of data into smaller subsets, which can be chosen by some criterion relevant to your analysis.
To improve performance, you can run a job that takes the data set and breaks the partitions out into separate files.
Then, when a particular subset for the data is to be analyzed, the job needs only to look at that data.
Partitioning by date is one of the most common schemes.
This helps when we want to analyze a certain span of time, because the data is already grouped by that criterion.
For instance, suppose you have event data that spans three years in your Hadoop cluster, but for whatever reason the records are not ordered at all by date.
It would be even better if they were partitioned by day!
Partitioning can also help out when you have several different types of records in the same data set, which is increasingly common in NoSQL.
For example, in a HTTP server logs, you’ll have GET and POST requests, internal system messages, and error messages.
Analysis may care about only one category of this data, so partitioning it into these categories will help narrow down the data the job runs over before it even runs.
In an RDBMS, a typical criterion for partitioning is what you normally filter by in the WHERE clause.
So, for example, if you are typically filtering down records by country, perhaps you should partition by country.
If you find yourself filtering out a bunch of records in the mapper due to the same criteria over and over, you should consider partitioning your data set.
There is no downside to partitioning other than having to build the partitions.
A MapReduce job can still run over all the partitions at once if necessary.
The one major requirement to apply this pattern is knowing how many partitions you are going to have ahead of time.
For example, if you know you are going to partition by day of the week, you know that you will have seven partitions.
You can get around this requirement by running an analytic that determines the number of partitions.
For example, if you have a bunch of timestamped data, but you don’t know how far back it spans, run a job that figures out the date range for you.
There is no actual partitioning logic; all you have to do is define the function that determines what partition a record is going to go to in a custom partitioner.
But this pattern can do additional processing in the reducer if needed.
Data is still going to get grouped and sorted, so data can be deduplicated, aggregated, or summarized, per partition.
The output folder of the job will have one part file for each partition.
Since each category will be written out to one large file, this is a great place to store the data in block-compressed SequenceFiles, which are arguably the most efficient and easy-to-use data format in Hadoop.
You have some sort of continuous variable, such as a date or numerical value, and at any one time you care about only a certain subset of that data.
Partitioning the data into bins will allow your jobs to load only pertinent data.
Partition pruning by category Instead of having some sort of continuous variable, the records fit into one of several clearly defined categories, such as country, phone area code, or language.
Sharding A system in your architecture has divisions of data—such as different disks—and you need to partition the data into these existing shards.
This allows “partition pruning” which allows the database to exclude large portions of irrelevant data before running the SQL.
Other patterns This pattern is similar to the binning pattern in this chapter.
In most cases, binning can perform the same partitioning behavior as this pattern.
The main performance concern with this pattern is that the resulting partitions will likely not have similar number of records.
Perhaps one partition turns out to hold 50% of the data of a very large data set.
If implemented naively, all of this data will get sent to one reducer and will slow down processing significantly.
Split very large partitions into several smaller partitions, even if just randomly.
Assign multiple reducers to one partition and then randomly assign records into each to spread it out a bit better.
For example, consider the “last access date” field for a user in StackOverflow.
If we partitioned on this property equally over months, the most recent month will very likely be much larger than any other month.
To prevent skew, it may make sense to partition the most recent month into days, or perhaps just randomly.
This method doesn’t affect processing over partitions, since you know that these set of files represent one larger partition.
In the StackOverflow data set, users are stored in the order in which they registered.
Instead, we want to organize the data into partitions based on the year of the last access date.
This is done by creating a custom partitioner to assign record to a particular partition based on that date.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a set of user information, partition the records based on the year of last access date, one partition per year.
The job needs to be configured to use the custom built partitioner, and this partitioner needs to be configured.
The reason for this is explained in the partitioner code section.
Also, the number of reducers is important to make sure the full range of partitions is accounted for.
Users can fall into these dates as well as those in between, meaning the job is configured to have exactly 4 reducers.
The mapper pulls the last access date out of each input record.
This date is output as the key, and the full input record is output as the value.
This is so the partitioner can do the work of putting each record into its appropriate partition.
This key is later ignored during output from the reduce phase.
The partitioner examines each key/value pair output by the mapper to determine which partition the key/value pair will be written.
Each numbered partition will be copied by its associated reduce task during the reduce phase.
The setConf method is called during task construction to configure the partitioner.
Here, the minimum value of the last access date is pulled from the configuration.
This date is used to subtract from each key (last access date) to determine what partition it goes to.
The reducer code is very simple since we simply want to output the values.
The work of partitioning has been done at this point.
Binning Pattern Description The binning pattern, much like the previous pattern, moves the records into categories irrespective of the order of records.
For each record in the data set, file each one into one or more categories.
Binning is very similar to partitioning and often can be used to solve the same problem.
The major difference is in how the bins or partitions are built using the MapReduce framework.
In some situations, one solution works better than the other.
Binning splits data up in the map phase instead of in the partitioner.
This has the major advantage of eliminating the need for a reduce phase, usually leading to more efficient resource allocation.
The downside is that each mapper will now have one file per possible output bin.
This means that, if you have a thousand bins and a thousand mappers, you are going to output a total of one million files.
The partitioning pattern will have one output file per category and does not have this problem.
This pattern’s driver is unique in using the MultipleOutputs class, which sets up the job’s output to write multiple distinct files.
The mapper looks at each line, then iterates through a list of criteria for each bin.
If the record meets the criteria, it is sent to that bin.
No combiner, partitioner, or reducer is used in this pattern.
Data should not be left as a bunch of tiny files.
At some point, you should run some postprocessing that collects the outputs into larger files.
This pattern has the same scalability and performance properties as other map-only jobs.
No sort, shuffle, or reduce needs to be performed, and most of the processing is going to be done on data that is local.
We want to filter data by tag into different bins so that we can run follow-on analysis without having to run over all of the data.
We care only about the Hadoop-related tags, specifically hadoop, pig, hive, and hbase.
Also, if the post mentions Hadoop anywhere in the text or title, we’ll put that into its own bin.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a set of StackOverflow posts, bin the posts into four bins based on the tags hadoop, pig, hive, and hbase.
Also, create a separate bin for posts mentioning hadoop in the text or title.
The driver is pretty much the same boiler plate code, except that we use MultipleOutputs for the different bins.
MultipleOutputs takes in a name, bins, that is used in the mapper to write different output.
The name is essentially the output directory of the job.
Output counters are disabled by default, so be sure to turn those on if you don’t expect a large number of named outputs.
We also set the number of reduce tasks to zero, as this is a map-only job.
The setup phase creates an instance of MultipleOutputs using the context.
The mapper consists of several if-else statements to check each of the tags of a post.
If the post contains the tag, it is written to the bin.
Posts with multiple interesting tags will essentially be duplicated as they are written to the appropriate bins.
Finally, we check whether the body of the post contains the word “hadoop”
If it does, we output it to a separate bin.
Be sure to close the MultipleOutputs during cleanup! Otherwise, you may not have much output at all.
The typical file names, part-mnnnnn, will be in the final output directory.
These files will be empty unless the Context object is used to write key/value pairs.
In the following example, bin_name will be, hadoop-tag, pig-tag, hive-tag, hbase-tag, or hadoop-post.
Note that setting the output format of the job to a NullOutputFormat will remove these empty output files when using the mapred package.
In the newer API, the output files are not committed from their _tem porary directory into the configured output directory in HDFS.
This may be fixed in a newer version of Hadoop.
Total Order Sorting Pattern Description The total order sorting pattern is concerned with the order of the data from record to record.
You want to sort your data in parallel on a sort key.
Sorting in MapReduce, or more generally in parallel, is not easy.
This is because the typical “divide and conquer” approach is a bit harder to apply here.
Each individual reducer will sort its data by key, but unfortunately, this sorting is not global across all data.
What we want to do here is a total order sorting where, if you concatenate the output files, the records are sorted.
If we just concatenate the output of a simple MapReduce job, segments of the data will be sorted, but the whole set will not be.
Sorted by time, it can provide a timeline view on the data.
Finding things in a sorted data set can be done with binary search instead of linear search.
In the case of MapReduce, we know the upper and lower boundaries of each file by looking at the last and first records, respectively.
This can be useful for finding records, as well, and is one of the primary characteristics of HBase.
Some databases can bulk load data faster if the data is sorted on the primary key or index column.
There are countless more reasons to have sorted data from an application standpoint or follow-on system standpoint.
However, having data sorted for use in MapReduce serves little purpose, so hopefully this expensive operation only has to be done sparingly.
The main requirement here is pretty obvious: your sort key has to be comparable so the data can be ordered.
Total order sorting may be one of the more complicated patterns you’ll see.
The reason this is that you first have to determine a set of partitions divided by ranges of values that will produce equal-sized subsets of data.
These ranges will determine which reducer will sort which range of data.
Then something similar to the partitioning pattern is run: a custom partitioner is used to partition data by the sort key.
The lowest range of data goes to the first reducer, the next range goes to the second reducer, so on and so forth.
This pattern has two phases: an analyze phase that determines the ranges, and the order phase that actually sorts the data.
You need to run it only once if the distribution of your data does not change quickly over time, because the value ranges it produces will continue to perform well.
Also, in some cases, you may be able to guess the partitions yourself, especially if the data is evenly distributed.
For example, if you are sorting comments by user ID, and you have a million users, you can assume that with a thousand reducers, each range is going to have a range of a thousand users.
This is because comments by user ID should be spread out evenly and since you know the number of total users, you can divide that number by the number of reducers you want to use.
The analyze phase is a random sampling of the data.
The principle is that partitions that evely split the random sample should evenly split the larger data set well.
When dividing records, it outputs the sort key as its output key so that the data will show up sorted at the reducer.
We don’t care at all about the actual record, so we’ll just use a null value to save on space.
Ahead of time, determine the number of records in the total data set and figure out what percentage of records you’ll need to analyze to make a reasonable sample.
For example, if you plan on running the order with a thousand reducers, sampling about a hundred thousand records should give nice, even partitions.
This will collect the sort keys together into a sorted list (they come in sorted, so that will be easy)
Then, when all of them have been collected, the list of keys will be sliced into the data range boundaries.
The order phase is a relatively straightforward application of MapReduce that uses a custom partitioner.
The mapper extracts the sort key in the same way as the analyze step.
However, this time the record itself is stored as the value instead of being ignored.
A custom partitioner is used that loads up the partition file.
It takes the data ranges from the partition file produced in the previous step and decides which reducer to send the data to.
The shuffle and sort take care of the heavy lifting.
The reduce function simply takes the values that have come in and outputs them.
Note that the number of ranges in the intermediate partition needs to be equal to the number of reducers in the order step.
If you decide to change the number of reducers and you’ve been reusing the same file, you’ll need to rebuild it.
If you want to have a primary sort key and a secondary sort key, concatenate the keys, delimited by something.
For example, if you want to sort by last name first, and city second, use a key that looks like Smith^Baltimore.
Using Text for nearly everything in Hadoop is very natural since that’s the format in which data is coming in.
Either pad the numbers with zeros or use a numerical data type.
The output files will contain sorted data, and the output file names will be sorted such that the data is in a total sorting.
In Hadoop, you’ll be able to issue hadoop fs -cat output/part-r-* and retrieve the data in a sorted manner.
Pig Ordering in Pig is syntactically pretty easy, but it’s a very expensive operation.
Behind the scenes, it will run a multi-stage MapReduce job to first find the partitions, and then perform the actual sort.
This operation is expensive because you effectively have to load and parse the data twice: first to build the partition ranges, and then to actually sort the data.
The job that builds the partitions is straightforward and efficient since it has only one reducer and sends a minimal amount of data over the network.
The output file is small, so writing it out is trivial.
Also, you may only have to run this now and then, which will amortize the cost of building it over time.
The order step of the job has performance characteristics similar to the other data organization patterns, because it has to move all of the data over the network and write all of the data back out.
Therefore, you should use a relatively large number of reducers.
The user data in our StackOverflow data set is in the order of the account’s creation.
Instead, we’d like to have the data ordered by the last time they have visited the site.
For this example, we have a special driver that runs both the analyze and order steps.
Also, there are two sets of MapReduce jobs, one for analyze and one for order.
Let’s break the driver down into two sections: building the partition list via sampling, then performing the sort.
The first section parses the input command line arguments and creates input and output variables from them.
It creates path files to the partition list and the staging directory.
The staging directory is used to store intermediate output between the two jobs.
There is nothing too special with the first job configuration.
The main thing to note is that the first job is a map-only only job that uses a SequenceFi leOutputFormat.
The second job uses the identity mapper and our reducer implementation.
The input is the output from the first job, so we’ll use the identity mapper to output the key/value pairs as they are stored from the output.
The job is configured to 10 reducers, but any reasonable number can be used.
Next, the partition file is configured, even though we have not created it yet.
This sampler writes the partition file by reading through the configured input directory of the job.
Using the RandomSam pler, it takes a configurable number of samples of the previous job’s output.
This can be an expensive operation, as the entire output is read using this constructor.
Another constructor of RandomSampler allows you to set the number of input splits that will be sampled.
This will increase execution time, but you might not get as good a distribution.
After the partition file is written, the job is executed.
The partition file and staging directory are then deleted, as they are no longer needed for this example.
If your data distribution is unlikely to change, it would be worthwhile to keep this partition file around.
It can then be used over and over again for this job in the future as new data arrives on the system.
This mapper simply pulls the last access date for each user and sets it as the sort key for the record.
This job simply uses the identity mapper to take each input key/value pair and output them.
This will produce a part file for this reducer that is sorted by last access date.
The partitioner ensures that the concatenation of all these part files (in order) produces a totally ordered data set.
Shuffling Pattern Description The total order sorting and shuffling patterns are opposites in terms of effect, but the latter is also concerned with the order of data in records.
You have a set of records that you want to completely randomize.
This whole chapter has been about applying some sort of order to your data set except for this pattern which is instead about completely destroying the order.
The use cases for doing such a thing are definitely few and far between, but two stand out.
One is shuffling the data for the purposes of anonymizing it.
Another is randomizing the data set for repeatable random sampling.
Anonymizing data has recently become important for organizations that want to maintain their users’ privacy, but still run analytics.
The order of the data can provide some information that might lead to the identity of a user.
By shuffling the entire data set, the organization is taking an extra step to anonymize the data.
Another reason for shuffling data is to be able to perform some sort of repeatable random sampling.
For example, the first hundred records will be a simple random sampling.
Every time we pull the first hundred records, we’ll get the same sample.
This allows analytics that run over a random sample to have a repeatable result.
Also, a separate job won’t have to be run to produce a simple random sampling every time you need a new sample.
All the mapper does is output the record as the value along with a random key.
The reducer sorts the random keys, further randomizing the data.
In other words, each record is sent to a random reducer.
Then, each reducer sorts on the random keys in the records, producing a random order in that reducer.
The mapper in the shuffle pattern is barely doing anything.
This would be a good time to anonymize the data further by transforming the records into an anonymized form.
The SQL equivalent to this is to order the data set by a random value, instead of some column in the table.
This makes it so each record is compared on the basis of two random numbers, which will produce a random ordering.
We don’t have to go all the way and do a total ordering in MapReduce, as in the previous pattern.
This is because sending data to a random reducer is sufficient.
Pig Shuffling in Pig can be done as we did it in SQL: performing an ORDER BY on a random column.
Instead, we can GROUP BY a random key, and then FLATTEN the grouping.
This effectively implements the shuffle pattern we proposed behind the scenes.
Since the reducer each record goes to is completely random, the data distribution across reducers will be completely balanced.
With more reducers, the data will be more spread out.
The size of the files will also be very predictable: each is the size of the data set divided by the number of reducers.
This makes it easy to get a specific desired file size as output.
Other than that, the typical performance properties for the other patterns in this chapter apply.
The pattern shuffles all of the data over the network and writes all of the data back to HDFS, so a relatively high number of reducers should be used.
To anonymize the StackOverflow comments, this example strips out the user ID and row ID, and truncates the date and time to just the date.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a large data set of StackOverflow comments, anonymize each comment by removing IDs, removing the time from the record, and then randomly shuffling the records within the data set.
The mapper transforms the data using our utility function that parses the data.
Each XML attribute is looked at, and an action is taken based on the attribute to create a new line of XML.
If it is a user ID or row ID, it is ignored.
If it is a creation date, the characters following the ‘T’ are removed to ignore the time.
A random key is generated and output along with the newly constructed record.
This reducer class just outputs the values in order to strip out the random key.
Having all your data in one giant data set is a rarity.
For example, presume you have user information stored in a SQL database because it is updated frequently.
Meanwhile, web logs arrive in a constant stream and are dumped directly into HDFS.
Also, daily analytics that make sense of these logs are stored someone where in HDFS and financial records are stored in an encrypted repository.
Data is all over the place, and while it’s very valuable on its own, we can discover interesting relationships when we start analyzing these sets together.
Joins can be used to enrich data with a smaller reference set or they can be used to filter out or select records that are in some type of special list.
In SQL, joins are accomplished using simple commands, and the database engine handles all of the grunt work.
Sadly for us, joins in MapReduce are not nearly this simple.
MapReduce operates on a single key/value pair at a time, typically from the same input.
We are now working with at least two data sets that are probably of different structures, so we need to know what data set a record came from in order to process it correctly.
Typically, no filtering is done prior to the join operation, so some join operations will require every single byte of input to be sent to the reduce phase, which is very taxing on your network.
For example, joining a terabyte of data onto another terabyte data set could require at least two terabytes of network bandwith—and that’s before any actual join logic can be done.
On top of all of the complexity so far, one has to determine the best way out of a number of different ways to accomplish the same task.
Because the framework is broken down into simple map and reduce tasks, there is a lot of hands-on work to do and a lot of things to keep in mind.
After you learn the possibilities, the question to ask is when to.
As with any MapReduce operation, network bandwith is a very important resource and joins have a tendency to use a lot of it.
Anything we can do to make the network transfer more efficient is worthwhile, and network optimizations are what differentiates these patterns.
Each of the upcoming patterns can be used to perform an inner join or at least one type of outer join.
As far as what pattern to choose, it depends largely on how large the data sets are, how your data is formatted, and what type of join you want.
On the other hand, the Cartesian product is completely different, but we can cross that bridge when we get there.
The first pattern discussed in this chapter, the reduce side join, is the most basic, along with a modified version that uses a Bloom filter.
After that, we discuss two patterns that perform a join operation on the map-side using either the distributed cache or a merging feature in the Hadoop MapReduce API.
Finally, we take a look at how to execute the crafty operation that is the Cartesian product.
Choosing the right type of join for your situation can be challenging.
Make sure to pay careful attention to the criteria in the “Applicability” section of each of the pattern descriptions.
A Refresher on Joins If you come from a strong SQL background, you can probably skip this section, but for those of us that started with Hadoop, joins may be a bit of a foreign concept.
Joins are possibly one of the most complex operations one can execute in MapReduce.
By design, MapReduce is very good at processing large data sets by looking at every record or group in isolation, so joining two very large data sets together does not fit into the paradigm gracefully.
Before we dive into the patterns themselves, let’s go over what we mean when we say join and the different types of joins that exist.
A join is an operation that combines records from two or more data sets based on a field or set of fields, known as the foreign key.
The foreign key is the field in a relational table that matches the column of another table, and is used as a means to cross-reference between tables.
Examples are the simplest way to go about explaining joins, so let’s dive right in.
To simplify explanations of the join types, two data sets will be used, A and B, with the foreign key defined as f.
With this type of join, records from both A and B that contain identical values for a given foreign key f are brought together, such that all the columns of both A and B now make a new table.
Records that contain values of f that are contained in A but not in B, and vice versa, are not represented in the result table of the join operation.
Table 5-3 shows the result of an inner join operation between A and B with User ID as f.
However, these records will be present in a type of outer join, which brings us to our next type of join!
There are three types of outer joins and each type will directly affect which unmatched records will be in the final table.
In a left outer join, the unmatched records in the “left” table will be in the final table, with null values in the columns of the right table that did not match on the foreign key.
Unmatched records present in the right table will be discarded.
A right outer join is the same as a left outer, but the difference is the right table records are kept and the left table values are null where appropriate.
A full outer join will contain all unmatched records from both tables, sort of like a combination of both a left and right outer join.
Table 5-4 shows the result of a left outer join operation between A and B on User ID.
User 8 in B does not have a match in A, so it is omitted.
Table 5-5 shows the result of a right outer join operation between A and B on User ID.
User 8 in B does not have a match in A, but is kept because B is the right table.
Table 5-6 shows the result of a full outer join operation between A and B on User ID.
That is, the resulting table contains only records that did not contain a match on f.
Table 5-7 shows the result of an antijoin operation between A and B on User ID.
Unlike the other join operations, a Cartesian product does not contain a foreign key.
As we will see in the upcoming pattern, this operation is extremely expensive to perform no matter where you implement it, and MapReduce is no exception.
Table 5-8 shows the result of a Cartesian product between A and B.
Reduce Side Join Pattern Description The reduce side join pattern can take the longest time to execute compared to the other join patterns, but it is simple to implement and supports all the different join operations discussed in the previous section.
Join large multiple data sets together by some foreign key.
A reduce side join is arguably one of the easiest implementations of a join in MapReduce, and therefore is a very attractive choice.
It can be used to execute any of the types of joins described above with relative ease and there is no limitation on the size of your data sets.
Also, it can join as many data sets together at once as you need.
All that said, a reduce side join will likely require a large amount of network bandwidth because the bulk of the data is sent to the reduce phase.
This can take some time, but if you have resources available and aren’t concerned about execution time, by all means use it! Unfortunately, if all of the data sets are large, this type of join may be your only choice.
Multiple large data sets are being joined by a foreign key.
If all but one of the data sets can be fit into memory, try using the replicated join.
You want the flexibility of being able to execute any join operation.
The mapper prepares the join operation by taking each input record from each of the data sets and extracting the foreign key from the record.
The foreign key is written as the output key, and the entire input record as the output value.
This output value is flagged by some unique identifier for the data set, such as A or B if two data sets are used.
A hash partitioner can be used, or a customized partitioner can be created to distribute the intermediate key/value pairs more evenly across the reducers.
The reducer performs the desired join operation by collecting the values of each input group into temporary lists.
These lists are then iterated over and the records from both sets are joined together.
For an inner join, a joined record is output if all the lists are not empty.
For an outer join (left, right, or full), empty lists are still joined with non empty lists.
The antijoin is done by examining that exactly one list is empty.
The records of the non-empty list are written with an empty writable.
The output is a number of part files equivalent to the number of reduce tasks.
Each of these part files together contains the portion of the joined records.
The columns of each record depend on how they were joined in the reducer.
Some column values will be null if an outer join or antijoin was performed.
Joins are very common in SQL and easy to execute.
Pig Pig has support for inner joins and left, right, and full outer joins.
A plain reduce side join puts a lot of strain on the cluster’s network.
Because the foreign key of each input record is extracted and output along with the record and no data can be filtered ahead of time, pretty much all of the data will be sent to the shuffle and sort step.
For this reason, reduce side joins will typically utilize relatively more reducers than your typical analytic.
If any of the other pattern described in this chapter can be used (other than Cartesian product), it is recommended that you do so.
Sometimes this basic join pattern is the only one that fits the circumstances.
In this example, we’ll be using the users and comments tables from the StackOverflow data set.
Storing data in this matter makes sense, as storing repetitive user data with each comment is unnecessary.
However, having disjoint data sets poses problems when it comes to associating a comment with the user who wrote it.
Through the use of a reduce side join, these two data sets can be merged together using the user ID as the foreign key.
In this example, we’ll perform an inner, outer, and antijoin.
The choice of which join to execute is set during job configuration.
Hadoop supports the ability to use multiple input data types at once, allowing you to create a mapper class and input format for each input split from different data sources.
This is extremely helpful, because you don’t have to code logic for two different data inputs in the same map implementation.
In the following example, two mapper classes are created: one for the user data and one for the comments.
Each mapper class outputs the user ID as the foreign key, and the entire record as the value along with a single character to flag which record came from what set.
The reducer then copies all values for each group in memory, keeping track of which record came from what data set.
Be advised that the output key and value types need to be identical for all of the mapper classes used.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a set of user information and a list of user’s comments, enrich each comment with the information about the user who created the comment.
The job configuration is slightly different from the standard configuration due to the user of the multiple input utility.
We also set the join type in the job configuration to args[2] so it can be used in the reducer.
The relevant piece of the driver code to use the MultipleInput follows:
This mapper parses each input line of user data XML.
It grabs the user ID associated with each record and outputs it along with the entire input value.
It prepends the letter A in front of the entire value.
This allows the reducer to know which values came from what data set.
When you output the value from the map side, the entire record doesn’t have to be sent.
This is an opportunity to optimize the join by keeping only the fields of data you want to join together.
It requires more processing on the map side, but is worth it in the long run.
Also, since the foreign key is in the map output key, you don’t need to keep that in the value, either.
Very similar to the UserJoinMapper, it too grabs the user ID associated with each record and outputs it along with the entire input value.
The only different here is that the XML attribute UserId represents the user that posted to comment, where as Id in the user data set is the user ID.
Here, this mapper prepends the letter B in front of the entire value.
The reducer code iterates through all the values of each group and looks at what each record is tagged with and then puts the record in one of two lists.
After all values are binned in either list, the actual join logic is executed using the two lists.
The join logic differs slightly based on the type of join, but always involves iterating through both lists and writing to the Context object.
The type of join is pulled from the job configuration in the setup method.
Let’s look at the main reduce method before looking at the join logic.
The input data types to the reducer are two Text objects.
The input key is the foreign join key, which in this example is the user’s ID.
Any type of data formatting you would want to perform should be done here prior to outputting.
For simplicity, the raw XML value from the left data set (users) is output as the key and the raw XML value from the right data set (comments) is output as the value.
If both the lists are not empty, simply perform two nested for loops and join each of the values together.
For a left outer join, if the right list is not empty, join A with B.
If the right list is empty, output each record of A with an empty string.
A right outer join is very similar, except switching from the check for empty elements from B to A.
If the left list is empty, write records from B with an empty output key.
A full outer join is more complex, in that we want to keep all records, ensuring that we join records where appropriate.
If list A is not empty, then for every element in A, join with B when the B list is not empty, or output A by itself.
For an antijoin, if at least one of the lists is empty, output the records from the nonempty list with an empty Text object.
Be considerate of follow on data parsing to ensure proper field delimiters.
A record that contains the proper structure but with null fields should be generated instead of outputting an empty object.
Because the join logic is performed on the reduce side, a combiner will not provide much optimization in this example.
This example is very similar to the previous one, but with the added optimization of using a Bloom filter to filter out some of mapper output.
This will help reduce the amount of data being sent to the reducers and in effect reduce the runtime of our analytic.
Say we are only interested in enriching comments with reputable users, i.e., greater than 1,500 reputation.
A standard reduce side join could be used, with the added condition to verify that a user’s reputation is greater than 1,500 prior to writing to the context object.
This requires all the data to be parsed and forwarded to the reduce phase for joining.
If we could stop outputting data from the mappers that we know are not going to be needed in the join, then we can drastically reduce network I/O.
Using a Bloom filter is particularly useful with an inner join operation, and may not be useful at all with a full outer join operation or an antijoin.
The latter two operations require all records to be sent to the reducer, so adding a Bloom filter has no value.
Filtering out users that do not meet the reputation requirement is simple enough for the UserJoinMapper class, because the user reputation is in the data.
However, there are a lot more comments than users and the user reputation is not available in each comment record.
Through the use of a Bloom filter, a small amount of memory can be used to perform the test we desire.
A preprocess stage is needed to train a Bloom filter with all users that have at least 1,500 reputation.
In the following example, both mappers are slightly different from the previous.
The UserJoinMapper adds a test prior to writing key/value pairs to the context to ensure the user has at least 1,500 reputation.
The CommentJoin Mapper deserializes a Bloom filter from the DistributedCache and then used it as a test case prior to writing any output.
The reducer remains the same as in the previous reduce side join example.
The driver code is slightly different in that we use the DistributedCache to store the Bloom filters.
This is omitted in the following code, as more information on how to use a Bloom filter with the DistributedCache can be found in the Appendix A.
The user ID is pulled from the XML record along with the reputation.
If the reputation is greater than 1,500, the record is output along with the foreign key (user ID)
The Bloom filter is initially deserialized from the DistributedCache prior to any calls to the map method.
After deserialization, the user ID is pulled from the XML record and used for the membership test of the Bloom filter.
If the test passes, the record is output along with the foreign key (user ID)
In this algorithm, we don’t need to verify the user’s reputation in the reducer prior to writing to the file system.
The main gain we received out of this Bloom filter was vastly reducing the number of comments output to the mapper phase.
Be conscious of Bloom filter false positives and how they will affect your reduce side join operation.
Replicated Join Pattern Description A replicated join is a special type of join operation between one large and many small data sets that can be performed on the map-side.
This pattern completely eliminates the need to shuffle any data to the reduce phase.
A replicated join is an extremely useful, but has a strict size limit on all but one of the data sets to be joined.
All the data sets except the very large one are essentially read into memory during the setup phase of each map task, which is limited by the JVM heap.
If you can live within this limitation, you get a drastic benefit because there is no reduce phase at all, and therefore no shuffling or sorting.
The join is done entirely in the map phase, with the very large data set being the input for the MapReduce job.
There is an additional restriction that a replicated join is really useful only for an inner or a left outer join where the large data set is the “left” data set.
The other join types require a reduce phase to group the “right” data set with the entirety of the left data set.
Although there may not be a match for the data stored in memory for a given map task, there could be match in another input split.
Because of this, we will restrict this pattern to inner and left outer joins.
The type of join to execute is an inner join or a left outer join, with the large input data set being the “left” part of the operation.
All of the data sets, except for the large one, can be fit into main memory of each map task.
The mapper is responsible for reading all files from the distributed cache during the setup phase and storing them into in-memory lookup tables.
After this setup phase completes, the mapper processes each record and joins it with all the data stored in-memory.
If the foreign key is not found in the in-memory structures, the record is either omitted or output, based on the join type.
No combiner, partitioner, or reducer is used for this pattern.
The output is a number of part files equivalent to the number of map tasks.
The part files contain the full set of joined records.
If a left outer join is used, the input to the MapReduce analytic will be output in full, with possible null values.
Pig has native support for a replicated join through a simple modification to the standard join operation syntax.
Only inner and left outer joins are supported for replicated joins, for the same reasons we couldn’t do it above.
The order of the data sets in the line of code matters because all but the first data sets listed are stored inmemory.
A replicated join can be the fastest type of join executed because there is no reducer required, but it comes at a cost.
There are limitations on the amount of data that can be stored safely inside the JVM, which is largely dependent on how much memory you are willing to give to each map and reduce task.
Experiment around with your data sets to see how much you can fit into memory prior to fully implementing this pattern.
Also, be aware that the memory footprint of your data set stored in-memory is not necessarily the number of bytes it takes to store it on disk.
The data will be inflated due to Java object overhead.
Thankfully, you can omit any data you know you will not need.
This example is closely related to the previous replicated join with Bloom filter example.
The DistributedCache is utilized to push a file around to all map tasks, but instead of a Bloom filter representation of the data, the data itself is read into memory.
Instead of filtering out data that will never be joined on the reduce side, the data is joined in the map phase.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a small set of user information and a large set of comments, enrich the comments with user information data.
During the setup phase of the mapper, the user data is read from the DistributedCache and stored in memory.
Each record is parsed and the user ID is pulled out of the record.
Then, the user ID and record are added to a HashMap for retrieval in the map method.
This is where an out of memory error could occur, as the entire contents of the file is stored, with additional overhead of the data structure itself.
If it does, you will either have to increase the JVM size or use a plain reduce side join.
After setup, consecutive calls to the map method are performed.
For each input record, the user ID is pulled from the comment.
This user ID is then used to retrieve a value from the HashMap built during the setup phase of the map.
If a value is found, the input value is output along with the retrieved value.
If a value is not found, but a left outer join is being executed, the input value is output with an empty Text object.
That’s all there is to it! The input data is enriched with the data stored in memory.
Composite Join Pattern Description A composite join is a specialized type of join operation that can be performed on the map-side with many very large formatted inputs.
Using this pattern completely eliminates the need to shuffle and sort all the data to the reduce phase.
However, it requires the data to be already organized or prepared in a very specific way.
Composite joins are particularly useful if you want to join very large data sets together.
However, the data sets must first be sorted by foreign key, partitioned by foreign key, and read in a very particular manner in order to use this type of join.
With that said, if your data can be read in such a way or you can prepare your data, a composite join has a huge leg-up over the other types.
This join utility is restricted to only inner and full outer joins.
The inputs for each mapper must be partitioned and sorted in a specific way, and each input dataset must be divided into the same number of partitions.
In addition to that, all the records for a particular foreign key must be in the same partition.
Usually, this occurs only if the output of several jobs has the same number of reducers and the same foreign key, and output files aren’t.
In many cases, one of the other patterns presented in this chapter is more applicable.
If you find yourself having to format the data prior to using a composite join, you are probably better off just using a reduce side join unless this output is used by many analytics.
All data sets can be read with the foreign key as the input key to the mapper.
Each partition is sorted by foreign key, and all the foreign keys reside in the associated partition of each data set.
That is, partition X of data sets A and B contain the same foreign keys and these foreign keys are present only in partition X.
For a visualization of this partitioning and sorting key, refer to Figure 5-3
The data sets do not change often (if they have to be prepared)
Data sets that are sorted and partitioned on the same key.
The driver code handles most of the work in the job configuration stage.
It sets up the type of input format used to parse the data sets, as well as the join type to execute.
The framework then handles executing the actual join when the data is read.
The two values are retrieved from the input tuple and simply output to the file system.
No combiner, partitioner, or reducer is used for this pattern.
The output is a number of part files equivalent to the number of map tasks.
The part files contain the full set of joined records.
If configured for an outer join, there may be null values.
A composite join can be executed relatively quickly over large data sets.
However, the MapReduce framework can only set up the job so that one of the two data sets are data local.
The respective files that are partitioned by the same key cannot be assumed to be on the same node.
Any sort of data preparation needs to taken into account in the performance of this analytic.
The data preparation job is typically a MapReduce job, but if the data sets rarely change, then the sorted and partitioned data sets can be used over and over.
Thus, the cost of producing these prepared data sets is averaged out over all of the runs.
To meet the preconditions of a composite join, both the user and comment data sets have been preprocessed by MapReduce and output using the TextOutputFormat.
The key of each data set is the user ID, and the value is either the user XML or comment XML, based on the data set.
The key will be the output key of our format job (user ID) and the value will be the output value (user or comment data)
Each data set was sorted by the foreign key, the caveat being that they are sorted as Text objects rather than LongWritable objects.
Each data set was then gzipped to prevent it from being split.
The driver code demonstrates how to configure MapReduce to handle the join, while the mapper code is trivial.
The following descriptions of each code section explain the solution to the problem.
Problem: Given two large formatted data sets of user information and comments, enrich the comments with user information data.
The driver parses the input arguments for the job: the path to the user data, the path to the comment data, the analytic output directory, and the type of join (inner or outer)
The most important piece of configuration is setting the input format and then configuring the join expression.
The input format has a static helper function to create the join expression itself.
It takes in the join type (inner or outer), the input format class used to parse all the data sets, and then as many Path or String objects as desired, which represent the data sets to join together.
That’s all there is to it! After setting the remaining required parameters, the job is run until completion and the program exits.
The input to the mapper is the foreign key and a TupleWritable.
This tuple contains a number of Text objects equivalent to the number of data sets.
As far as position is concerned, the ordering of the Text objects maps directly to how it was configured.
The first input path is the zeroth index, the second input path is the first index, and so on.
The mapper simply grabs the objects from the tuple and outputs them.
There are only two data sets to be joined in this example, so they are output as the key and value.
If more were used, the strings would need be concatenated in some manner prior to being output.
This pattern has no reducer or combiner implementation because it is map only.
Cartesian Product Pattern Description The Cartesian product pattern is an effective way to pair every record of multiple inputs with every other record.
This functionality comes at a cost though, as a job using this pattern can take an extremely long time to complete.
Pair up and compare every single record with every other record in a data set.
A Cartesian product allows relationships between every pair of records possible between one or more data sets to be analyzed.
Rather than pairing data sets together by a foreign key, a Cartesian product simply pairs every record of a data set with every record of all the other data sets.
With that in mind, a Cartesian product does not fit into the MapReduce paradigm very well because the operation is not intuitively splittable, cannot be parallelized very well, and thus requires a lot of computation time and a lot of network traffic.
Any preprocessing of that data that can be done to improve execution time and reduce the byte count should be done to improve runtimes.
It is very rare that you would need to do a Cartesian product, but sometimes there is simply no foreign key to join on and the comparison is too complex to group by ahead of time.
Most use cases for using a Cartesian product are some sort of similarity analysis on documents or media.
You want to analyze relationships between all pairs of individual records.
The cross product of the input splits is determined during job setup and configuration.
After these are calculated, each record reader is responsible for generating the cross product from both of the splits it is given.
The record reader gives a pair of records to a mapper class, which simply writes them both out to the file system.
The final data set is made up of tuples equivalent to the number of input data sets.
Every possible tuple combination from the input records is represented in the final output.
Although very rarely seen, the Cartesian product is the syntactically simplest of all joins in SQL.
Pig Pig can perform a Cartesian product using the CROSS statement.
It also comes along with a warning that it is an expensive operation and should be used sparingly.
The Cartesian product produces a massive explosion in data size, as even a self-join of a measly million records produces a trillion records.
It should be used very sparingly because it will use up many map slots for a very long time.
This will dramatically increase the run time of other analytics, as any map slots taken by a Cartesian product are unusable by other jobs until completion.
If the number of tasks is greater than or equal to the total number of map slots in the cluster, all other work won’t get done for quite some time.
Each input split is paired up with every other input split—effectively creating a data set of O(n2), n being the number of bytes.
A single record is read from the left input split, and then the entire right input split is read and reset before the second record from the left input split is read.
If a single input split contains a thousand records, this means the right input split needs to be read a thousand times before the task can finish.
This is a massive amount of processing time! If a single task fails for an odd reason, the whole thing needs to be restarted! You can see why a Cartesian product is a terrible, terrible thing to do in MapReduce.
This example demonstrates how to perform a self-join using the StackOverflow comments.
This self-join inspects a pair of comments and determines how similar they are to one another based on common words used between the two.
If they are similar enough, the pair is output to the file system.
Common words are removed from each comment along with other extra data in a preprocessing stage.
This example is different than all other examples in the book, in that it pays special attention to how the data is read.
Here, we create a custom input format to generate the Cartesian product of the input splits for the Job.
The record reader of each map task performs the actual Cartesian product and presents each pair to the mapper for processing.
It accomplishes this by reading a single record from the “left” data set, then pairing it with all the records from the “right” data set.
The next record is read from the left data set, the reader of the right data set is reset, and it is used to pair up again.
This process continues until there are no more records in the left set.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a groomed data set of StackOverflow comments, find pairs of comments that are similar based on the number of like words between each pair.
This is a homegrown Hadoop implementation of this problem for version 1.0.3 to demonstrate the idea behind how a cross product can be executed using MapReduce.
Future versions of Hadoop MapReduce will have this functionality packaged with the distribution!
It is implemented to support a Cartesian product for just two data sets for demonstration purposes in order to keep the code more simple.
A single data set can be used as both the left and right data sets, as we do for this example.
This is done by creating the underlying input format for each data set to get the splits, and then calculating the cross product.
These input splits are then assigned to map task across the cluster for processing.
The same input path is used as both data sets for the input format, as we are performing a comparison between pairs of comments.
The record reader is where the magic happens of performing the cross product.
The constructor of this class creates two separate record reader objects, one for each split.
The first call to next reads the first record from the left data set for the mapper input key, and the first record from the right data set as the mapper input value.
This key/ value pair is then given to the mapper for processing by the framework.
Subsequent calls to next then continue to read all the records from the right record reader, allowing the mapper to process them, until it says it has no more.
In this case, a flag is set and the do-while will loop backwards, reading the second record from the left data set.
The right record reader is reset, and the process continues.
This process completes until the left record reader returns false, stating there are no more key/value pairs.
At this point, the record reader has given the Cartesian product of both input splits to the map task.
Some of the more simple methods to adhere to the RecordReader interface are missing for brevity, such as close() and getPos()
There are also some optimization opportunities that could be implemented, such as forcing the record reader to the next left record if you know it is not going to be useful.
In this example, if the left record contains only one word in it and we are looking for pairs of comments that have a minimum of 3 common words, it doesn’t make much sense to read the entire right input split because no output is going to be made.
Record readers to get key value pairs private RecordReader leftRR = null, rightRR = null;
Store configuration to re-create the right record reader private FileInputFormat rightFIF; private JobConf rightConf; private InputSplit rightIS; private Reporter rightReporter;
For each Text object, it reads the word tokens into a set.
The sets are then iterated to determine how many common words there are between the two.
If there are more then ten words, the pair is output to the file system.
This chapter is different from the others in that it doesn’t contain patterns for solving a particular problem, but patterns that deal with patterns.
The term metapatterns is directly translated to “patterns about patterns.” The first method that will be discussed is job chaining, which is piecing together several patterns to solve complex, multistage problems.
The second method is job merging, which is an optimization for performing several analytics in the same MapReduce job, effectively killing multiple birds with one stone.
Job Chaining Job chaining is extremely important to understand and have an operational plan for in your environment.
Many people find that they can’t solve a problem with a single MapReduce job.
Some jobs in a chain will run in parallel, some will have their output fed into other jobs, and so on.
Once you start to understand how to start solving problems as a series of MapReduce jobs, you’ll be able to tackle a whole new class of challenges.
Job chaining is one of the more complicated processes to handle because it’s not a feature out of the box in most MapReduce frameworks.
Systems like Hadoop are designed for handling one MapReduce job very well, but handling a multistage job takes a lot of manual coding.
There are operational considerations for handling failures in the stages of the job and cleaning up intermediate output.
In this section, a few different approaches to job chaining will be discussed.
Some will seem more appealing than others for your particular environment, as each has its own pros and cons.
A couple of frameworks and tools have emerged to fill this niche.
If you do a lot of job flows and your chaining is pretty complex, you should consider using one of these.
Oozie, an open source Apache project, has functionality for building workflows and coordinating job running.
Building job chains is only one of the many features that are useful for operationally running Hadoop MapReduce.
One particular common pitfall is to use MapReduce for something that is small enough that distributing the job is not necessary.
If you think chaining two jobs together is the right choice, think about how much output there is from the first.
If there are tons of output data, then by all means use a second MapReduce job.
Many times, however, the output of the job is small and can be processed quite effectively on a single node.
The two ways of doing this is to either load the data through the file system API in the driver after the job has completed, or incorporate it in some sort of bash script wrapper.
A major problem with MapReduce chains is the size of the temporary files.
In some cases, they may be tiny, which will cause a significant amount of overhead in firing up way too many map tasks to load them.
In a nonchained job, the number of reducers typically depends more on the amount of data they are receiving than the amount of data you’d like to output.
When chaining, the size of the output files is likely more important, even if the reducers will take a bit longer.
Try to shoot for output files about the size of one block on the distributed filesystem.
Just play around with the number of reducers and see what the impact is on performance (which is good advice in general)
With the Driver Probably the simplest method for performing job chaining is to have a master driver that simply fires off multiple job-specific drivers.
There’s nothing special about a MapReduce driver in Hadoop; it’s pretty generic Java.
It doesn’t derive from some sort of special class or anything.
Take the driver for each MapReduce job and call them in the sequence they should run.
You’ll have to specifically be sure that the output path of the first job is the input path of the second.
You can be sure of this by storing the temporary path string as a variable and sharing it.
In a production scenario, the temporary directories should be cleaned up so they don’t linger past the completion of the job.
Lack of discipline here can surprisingly fill up your cluster rather quickly.
Also, be careful of how much temporary data you are actually creating because you’ll need storage in your file system to store that data.
You can pretty easily extrapolate this approach to create chains that are much longer than just two jobs.
Just be sure to keep track of all of the temporary paths and optionally clean up the data not being used anymore as the job runs.
You can also fire off multiple jobs in parallel by using Job.submit() instead of Job.wait ForCompletion()
The submit method returns immediately to the current thread and runs the job in the background.
Use Job.is Complete(), a nonblocking job completion check, to constantly poll to see whether all of the jobs are complete.
The other thing to pay attention to is job success.
It’s not good enough to just know that the job completed.
You also need to check whether it succeeded or not.
If a dependency job failed, you should break out of the entire chain instead of trying to let it continue.
It’s pretty obvious that this process is going to be rather difficult to manage and maintain from a software engineering prospective as the job chains get more complicated.
This is where something like JobControl or Oozie comes in.
The goal of this example is to output a list of users along with a couple pieces of information: their reputations and how many posts each has issued.
This could be done in a single MapReduce job, but we also want to separate users into those with an aboveaverage number of posts and those with a below-average number.
We need one job to perform the counts and another to separate the users into two bins based on the number of posts.
Four different patterns are used in this example: numerical summarization, counting, binning, and a replicated join.
The final output consists of a user ID, the number of times they posted, and their reputation.
The average number of posts per user is calculated between the two jobs using the framework’s counters.
The users data set is put in the DistributedCache in the second job to enrich the output data with the users’ reputations.
This enrichment occurs in order to feed in to the next example in this section, which calculates the average reputation of the users in the two bins.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a data set of StackOverflow posts, bin users based on if they are below or above the number of average posts per user.
Also to enrich each user with his or her reputation from a separate data set when generating the output.
Before we look at the driver, let’s get an understanding of the mapper and reducer for both jobs.
The mapper records the user ID from each record by assigning.
This value is later used in the driver to calculate the average number of posts per user.
The AVERAGE_CALC_GROUP is a public static string at the driver level.
It simply iterates through the input values (all of which we set to 1) and keeps a running sum, which is output along with the input key.
A different counter is also incremented by one for each reduce group, in order to calculate the average.
It is doing a few different things to get the desired output.
The average number of posts per user is pulled from the Context object that was set during job configuration.
This is used to write the output to different bins.
Finally, the user data set is parsed from the DistributedCache to build a map of user ID to reputation.
This map is used for the desired data enrichment during output.
Compared to the setup, the map method is much easier.
The input value is parsed to get the user ID and number of times posted.
This is done by simply splitting on tabs and getting the first two fields of data.
Then the mapper sets the output key to the user ID and the output value to the number of posts along with the user’s reputation, delimited by a tab.
The user’s number of posts is then compared to the average, and the user is binned appropriately.
A constant is used to specify the directory for users based on whether they are below or above average in their number of posts.
The filename in the folder is named through an extra /part string.
This becomes the beginning of the filename, to which the framework will append -m-nnnnn, where nnnnn is the task ID number.
With this name, a folder will be created for both bins and the folders will contain a number of part files.
This is done for easier input/output management for the next example on parallel jobs.
Now let’s take a look at this more complicated driver.
It is broken down into two sections for discussion: the first job and the second job.
The first job starts by parsing command-line arguments to create proper input and output directories.
It creates an intermediate directory that will be deleted by the driver at the end of the job chain.
A string is tacked on to the name of the output directory here to make our intermediate output directory.
This is fine for the most part, but it may be a good idea to come up with a naming convention for any intermediate directories to avoid conflicts.
If an output directory already exists during job submission, the job will never start.
The first job is checked for success before executing the second job.
This seems simple enough, but with more complex job chains it can get a little annoying.
Before the second job is configured, we grab the counter values from the first job to get the average posts per user.
We set our mapper code and disable the reduce phase, as this is a map-only job.
The other key parts to pay attention to are the configuration of MultipleOutputs and the DistributedCache.
The job is then executed and the framework takes over from there.
Lastly and most importantly, success or failure, the intermediate output directory is cleaned up.
Leaving any intermediate output will fill up a cluster quickly and require you to delete the output by hand.
If you won’t be needing the intermediate output for any other analytics, by all means delete it in the code.
The driver in parallel job chaining is similar to the previous example.
The only big enhancement is that jobs are submitted in parallel and then monitored until completion.
However, they require the previous example to have completed successfully.
This has the added benefit of utilizing cluster resources better to have them execute simultaneously.
The following descriptions of each code section explain the solution to the problem.
Problem: Given the previous example’s output of binned users, run parallel jobs over both bins to calculate the average reputation of each user.
The mapper splits the input value into a string array.
The third column of this index is the reputation of the particular user.
This key is shared across all map tasks in order to group all the reputations together for the average calculation.
NullWritable can be used to group all the records together, but we want the key to have a meaningful value.
This can be expensive for very large data sets, as one reducer is responsible for streaming all the intermediate key/value pairs over the network.
The added benefit here over serially reading the data set on one node is that the input splits are read in parallel and the reducers use a configurable number of threads to read each mapper’s output.
The reducer simply iterates through the reputation values, summing the numbers and keeping a count.
The average is then calculated and output with the input key.
The driver code parses command-line arguments to get the input and output directories for both jobs.
A helper function is then called to submit the job configuration, which we will look at next.
The Job objects for both are then returned and monitored for job completion.
So long as either job is still running, the driver goes back to sleep for five seconds.
Once both jobs are complete, they are checked for success or failure and an appropriate log message is printed.
An exit code is then returned based on job success.
This will submit the job and then immediately return, allowing the application to continue.
As we saw, the returned Job is monitored in the main method until completion.
With Shell Scripting This method of job chaining is very similar to the previous approach of implementing a complex job flow in a master driver that fires off individual job drivers, except that we do it in a shell script.
Each job in the chain is fired off separately in the way you would run it from the command line from inside of a shell script.
This has a few major benefits and a couple minor downsides.
One benefit is that changes to the job flow can be made without having to recompile the code because the master driver is a scripting language instead of Java.
This is important if the job is prone to failure and you need to easily be able to manually rerun or repair failed jobs.
Also, you’ll be able to use jobs that have already been productionalized to work through a commandline interface, but not a script.
Yet another benefit is that the shell script can interact with services, systems, and tools that are not Java centric.
For example, later in this chapter we’ll discuss post-processing of output, which may be very natural to do with sed or awk, but less natural to do in Java.
One of the downsides of this approach is it may be harder to implement more complicated job flows in which jobs are running in parallel.
You can run jobs in the background and then test for success, but it may not be as clean as in Java.
Wrapping any Hadoop MapReduce job in a script, whether it be a single Java MapReduce job, a Pig job, or whatever, has a number of benefits.
This includes post-processing, data flows, data preparation, additional logging, and more.
In general, using shell scripting is useful to chain new jobs with existing jobs quickly.
For more robust applications, it may make more sense to build a driver-based chaining mechanism that can better interface with Hadoop.
In this example, we use the Bash shell to tie together the basic job chaining and parallel jobs examples.
The script is broken into two pieces: setting variables to actually execute the jobs, and then executing them.
Input and outputs are stored in variables to create the a number of executable commands.
There are two commands to run both jobs, cat the output to the screen, and cleanup all the analytic output.
It executes the first job, and then checks the return code to see whether it failed.
If it did, output is deleted and the script exits.
If the second job completes successfully, the output of each job is written to the log file and all the output is deleted.
All the extra output is not required, and since the final output of each file consists only one line, storing it in the log file is worthwhile, instead of keeping it in HDFS.
With JobControl The JobControl and ControlledJob classes make up a system for chaining MapReduce jobs and has some nice features like being able to track the state of the chain and fire off jobs automatically when they’re ready by declaring their dependencies.
Using JobCon trol is the right way of doing job chaining, but can sometimes be too heavyweight for simpler applications.
To use JobControl, start by wrapping your jobs with ControlledJob.
Doing this is relatively simple: you create your job like you usually would, except you also create a ControlledJob that takes in your Job or Configuration as a parameter, along with a list of its dependencies (other ControlledJobs)
Then, you add them one-by-one to the JobControl object, which handles the rest.
You still have to keep track of temporary data and clean it up afterwards or in the event of a failure.
You can use any of the methods we’ve discussed so far to create iterative jobs that run the same job over and over.
Typically, each iteration takes the previous iteration’s data as input.
This is common practice for algorithms that have some sort of optimization component, such as kmeans clustering in MapReduce.
This is also common practice in many graph algorithms in MapReduce.
For an example of a driver using JobControl, let’s combine the previous two examples of basic job chaining and parallel jobs.
We are already familiar with the mapper and reducer code, so there is no need to go over them again.
The driver is the main showpiece here for job configuration.
It uses basic job chaining to launch the first job, and then uses JobControl to execute the remaining job in the chain and the two parallel jobs.
The initial job is not added via JobControl because you need to interrupt the control for the in-between step of using the counters of the first job to help assist in configuration of the second job.
All jobs must be completely configured before executing the entire job chain, which can be limiting.
Here, we parse the command line arguments and create all the paths we will need for all four jobs to execute.
We take special care when naming the variables to know our data flows.
The first job is then configured via a helper function and executed.
Upon completion of the first job, we invoke Configuration methods in helper functions to create three ControlledJob objects.
The next two jobs are dependent on the binning ControlledJob.
These two jobs will not be executed by JobControl until the binning job completes successfully.
If it doesn’t complete successfully, the other jobs won’t be executed at all.
All three ControlledJobs are added to the JobControl object, and then it is run.
The call to JobControl.run will block until the group of jobs completes.
We then check the failed job list to see if any jobs failed and set our exit code accordingly.
Following are all the helper methods used to create the actual Job or Configuration objects.
There are three separate methods, the final method being used twice to create the identical parallel jobs.
Chain Folding Chain folding is an optimization that is applied to MapReduce job chains.
Basically, it is a rule of thumb that says each record can be submitted to multiple mappers, or to a reducer and then a mapper.
Such combined processing would save a lot of time reading files and transmitting data.
The structure of the jobs often make these feasible because a map phase is completely shared-nothing: it looks at each record alone, so it doesn’t really matter what the organization of the data is or if it is grouped or not.
When building large MapReduce chains, folding the chain to combine map phases will have some drastic performance benefits.
The main benefit of chain folding is reducing the amount of data movement in the MapReduce pipeline, whether it be the I/O of loading and storing to disk, or shuffling data over the network.
In chained MapReduce jobs, temporary data is stored in HDFS, so if we can reduce the number of times we hit the disks, we’re reducing the total I/O in the chain.
There are a number of patterns in chains to look for to determine what to fold.
Take a look at the map phases in the chain.
If multiple map phases are adjacent, merge them into one phase.
This would be the case if you had a map-only job (such as a replicated join), followed by a numerical aggregation.
In this step, we are reducing the amount of times we’re hitting the disks.
Consider a two-job chain in which the first job is a map-only job, which is then followed by a traditional MapReduce job with a map phase and a reduce phase.
Without this optimization, the first map-only job will write its output out to the distributed file system, and then that data will be loaded by the second job.
Instead, if we merge the map phase of the map-only job and the traditional job, that temporary data never gets written, reducing the I/O significantly.
Also, fewer tasks are started, reducing overhead of task management.
Chaining many map tasks together is an even more drastic optimization.
In this case, there really isn’t any downside to do this other than having to possibly alter already existing code.
If the job ends with a map phase (combined or otherwise), push that phase into the reducer right before it.
This is a special case with the same performance benefits as the previous step.
It removes the I/O of writing temporary data out and then running a map-only job on it.
Note that the the first map phase of the chain cannot benefit from this next optimization.
As much as possible, split up each map phase (combined or otherwise) between operations that decrease the amount of data (e.g., filtering) and operations that increase the amount of data (e.g., enrichment)
In some cases, this is not possible because you may need some enrichment data in order to do the filtering.
In these cases, look at dependent phases as one larger phase that cumulatively increases or decreases the amount of data.
Push the processes that decrease the amount of data into the previous reducer, while keeping the processes that increase the amount of data where they are.
This step is a bit more complex and the difference is more subtle.
The gain here is that if you push minimizing map-phase processing into the previous reducer, you will reduce the amount of data written to temporary storage, as well as the amount of data loaded off disk into the next part of the chain.
This can be pretty significant if a drastic amount of filtering is done.
Be careful when merging phases that require lots of memory.
For example, merging five replicated joins together might not be a good idea because it will exceed the total memory available to the task.
In these cases, it might be better to just leave them separate.
Regardless of whether a job is a chain or not, try to filter as much data as early as possible.
The most expensive parts of a MapReduce job are typically pushing data through the pipeline: loading the data, the shuffle/sort, and storing the data.
For example, if you care only about data from item 2012, filter that out in the map phase, not after the reducer has grouped the data together.
Let’s run through a couple of examples to help explain the idea and why it is so useful.
To exemplify step one, consider the chain in Figure 6-1
The original chain (on top) is optimized so that the replicated join is folded into the mapper of the second MapReduce job (bottom)
This job performs a word count on comments from teenagers.
We do this to find out what topics are interesting to our youngest users.
The age of the user isn’t with the comment, which is why we need to do a join.
In this case, the map-only replicated join can be merged into the preprocessing of the second job.
To exemplify step two, consider the following chain in Figure 6-2
The original chain (top) is optimized so that the replicated join is folded into the reducer of the second MapReduce job (bottom)
This job enriches each user’s information with the number of comments that user has posted.
It uses a generic counting MapReduce job, then uses a replicated join to add in the user information to the count.
In this case, the map-only replicated join can be merged into the reducer.
To exemplify step three, consider the following chain in Figure 6-3
The original chain (top) is optimized so that the replicated join is folded into the reducer of the second MapReduce job (bottom)
This job is a bit more complicated than the others, as is evident from the long chain used to solve it.
The intent is to find the most popular tags per age group, which is is done by finding a count of each user, enriching their user information onto it, filtering out counts.
When we look at the map tasks (enrichment and filtering), the replicated join is adding data, while the filter is removing data.
Following step three, we are going to move the filtering to the first MapReduce job, and then move the replicated join into the map phase of the second MapReduce job.
This gives us the new chain that can be seen at the bottom of Figure 6-3
Now the first MapReduce job will write out significantly less data than before and then it follows that the second MapReduce job is loading less data.
There are two primary methods for implementing chain folding: manually cutting and pasting code together, and a more elegant approach that uses special classes called ChainMapper and ChainReducer.
If this is a one-time job and logically has multiple map phases, just implement it in one shot with the manual approach.
If several of the map phases are reused (in a software reuse sense), then you should use the ChainMapper and ChainReducer approach to follow good software engineering practice.
The ChainMapper and ChainReducer Approach ChainMapper and ChainReducer are special mapper and reducer classes that allow you to run multiple map phases in the mapper and multiple map phases after the reducer.
You are effectively expanding the traditional map and reduce paradigm into several map phases, followed by a reduce phase, followed by several map phases.
However, only one map phase and one reduce phase is ever invoked.
Each chained map phase feeds into the next in the pipeline.
The output of the first is then processed by the second, which is then processed by the third, and so on.
The map phases on the backend of the reducer take the output of the reducer and do additional computation.
Be sure that the input types and output types between each chain match up.
This example is a slight modification of the job chaining example.
Here, we use two mapper implementations for the initial map phase.
The first formats each input XML record and writes out the user ID with a count of one.
The second mapper then enriches the user ID with his or her reputation, which is read during the setup phase via the DistributedCache.
These two individual mapper classes are then chained together to feed a single reducer.
This reducer is a basic LongSumReducer that simply iterates through all the values and sums the numbers.
This sum is then output along with the input key.
Finally, a third mapper is called that will bin the records based on whether their reputation is below or above 5,000
This entire flow is executed in one MapReduce job using ChainMapper and ChainReducer.
This example uses the deprecated mapred API, because ChainMapper and ChainReducer were not available in the mapreduce package when this example was written.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a set of user posts and user information, bin users based on whether their reputation is below or above 5,000
This mapper implementation is fed the output from the previous mapper.
It reads the users data set during the setup phase to create a map of user ID to reputation.
This map is used in the calls to map to enrich the output value with the user’s reputation.
This new key is then output along with the input value.
This reducer implementation sums the values together and outputs this summation with the input key: user ID and reputation.
This mapper uses MultipleOutputs to bin users into two data sets.
The input key is parsed to pull out the reputation.
This reputation value is then compared to the value 5,000 and the record is binned appropriately.
The most interesting piece here is adding mappers and setting the reducer.
The order in which they are added affects the execution of the different mapper implementations.
ChainMapper is first used to add the two map implementations that will be called back to back before any sorting and shuffling occurs.
Then, the ChainReducer static methods are used to set the reducer implementation, and then finally a mapper on the end.
Note that you don’t use ChainMapper to add a mapper after a reducer: use ChainReducer.
The signature of each method takes in the JobConf of a mapper/reducer class, the input and output key value pair types, and another JobConf for the mapper/reducer class.
This can be used in case the mapper or reducer has overlapping configuration parameters.
No special configuration is required, so we simply pass in empty JobConf objects.
The seventh parameter in the signature is a flag as to pass values in the chain by reference or by value.
This is an added optimization you can use if the collector does not modify the keys or values in either the mapper or the reducer.
Here, we make these assumptions, so we pass objects by reference (byValue = false)
In addition to configuring the chain mappers and reducers, we also add the user data set to the DistributedCache so our second mapper can perform the enrichment.
We also set configure the MultipleOutputs and use a NullOutputFormat rather than the typical TextOutputFormat.
Use of this output format will prevent the framework from creating the default empty part files.
Job Merging Like job folding, job merging is another optimization aimed to reduce the amount of I/ O through the MapReduce pipeline.
Job merging is a process that allows two unrelated jobs that are loading the same data to share the MapReduce pipeline.
The main benefit of merging is that the data needs to be loaded and parsed only once.
For some largescale jobs, that task might be the most expensive part of the whole operation.
One of the downsides of “schema-on-load” and storing the data in its original form is having to parse it over and over again, which can really impact performance if parsing is complicated (e.g., XML)
Assume we have two jobs that need to run over the exact same massive amount of data.
These two jobs both load and parse the data, then perform their computations.
With job merging, we’ll have one MapReduce job that logically performs the two jobs at once without mixing the two applications as seen in Figure 6-4
The original chain (top) is optimized so that the two mappers run on the same data, and the two reducers run on the same data (bottom)
Nothing is stopping you from applying job merging to more than two jobs at once.
The more the merrier! The more you consolidate a shared burden across jobs, the more compute resources you’ll have available in your cluster.
Likely, this process will be relevant only for important and already existing jobs in a production cluster.
Development groups that take the time to consolidate their core analytics will see significant reductions in cluster utilization.
When the jobs are merged, they’ll have to run together and the source code will have to be kept together.
This is likely not worth it for jobs that are run in an ad hoc manner or are relatively new to the environment.
Unfortunately, you must satisfy a number of prerequisites before applying this pattern.
The most obvious one is that both jobs need to have the same intermediate keys and output formats, because they’ll be sharing the pipeline and thus need to use the same data types.
Serialization or polymorphism can be used if this is truly a problem, but adds a bit of complexity.
Some hacks will have to be done to get it to work, but definitely more work can be put into a merging solution to make it a bit cleaner.
From a software engineering perspective, this complicates the code organization, because.
At a high level, the same map function will now be performing the original duties of the old map functions, while the reduce function will perform one action or another based on a tag on the key that tells which data set it came from.
Copying and pasting the code works, but may complicate which piece of code is doing what.
The other method is to separate the code into two helper map functions that process the input for each algorithm.
In the mapper, change the writing of the key and value to “tag” the key with the map source.
Tagging the key to indicate which map it came from is critical so that the data from the different maps don’t get mixed up.
There are a few ways to do this depending on the original data type.
If it is a string, you can simply make the first character the tag, so for instance you could change “parks” to “Aparks” when it comes from the first map, and “Bparks” when it comes from the second map.
The general way to tag is to make a custom composite tuple-like key that stores the tag separately from the original data.
This is definitely the cleaner way of doing things, but takes a bit more work.
In the reducer, parse out the tag and use an if-statement to switch what reducer code actually gets executed.
As in the mapper, you can either just copy and paste the code into an if-statement or have the if-statement call out to helper functions.
The if-statement controls the path of execution based on the tag.
MultipleOutputs is a special output format helper class that allows you to write to different folders of output for the same reducer, instead of just a single folder.
Make it so the one reducer path always writes to one folder of the MultipleOutputs, while the other reducer path writes to the other folder.
One created a distinct set of users, while the other created an anonymized version of each record.
The comment portion of the StackOverflow data set is the largest we have, so merging these jobs together will definitely cut our processing time down.
This way, the data set needs to be read only once.
The following descriptions of each code section explain the solution to the problem.
Problem: Given a set of comments, generate an anonymized version of the data and a distinct set of user IDs.
A custom WritableComparable object is created to tag a Text with a string.
This is a cleaner way of splitting the logic between the two jobs, and saves us some string parsing in the reducer.
This object has two private member variables and getters and setters for each variable.
It holds a String that the mapper uses to tag each Text value that is also held by this object.
The reducer then examines the tag to find out which reduce logic to execute.
The compareTo method is what makes this object also comparable and allowed for use as a key in the MapReduce framework.
If they are equal, the text inside the object is then compared and the value immediately returned.
If they are not equal, the value of the comparison is then returned.
Items are sorted by tag first, and then by the text value.
The map method simply passes the parameters to two helper functions, each of which processes the map logic individual to write output to context.
The map methods were slightly changed from their original respective examples in order to both output Text objects as the key and value.
This is a necessary change so we can have the same type of intermediate key/value pairs we had in the separate map logic.
The anonymizeMap method generates an anonymous record from the input value, whereas the distinctMap method grabs the user ID from the record and outputs it.
Each intermediate key/value pair written out from each helper map method is tagged with either “A” for anonymize or “D” for distinct.
Each helper math method parses the input record, but this parsing should instead be done inside the actual map method, The resulting Map<String,String> can then be passed to both helper methods.
Any little optimizations like this can be very beneficial in the long run and should be implemented!
The reducer’s calls to setup and cleanup handle the creation and closing of the MultipleOutputs utility.
The reduce method checks the tag of each input key and calls a helper reducer method based on the tag.
The reduce methods are passed the Text object inside the TaggedText.
For the anonymous call, all the input values are iterated over and written to a named output of anonymize/part.
Adding the slash and the “part” creates a folder under the configured output directory that contains a number of part files equivalent to the number of reduce tasks.
For the distinct reduce call, the input key is written to MultipleOutputs with a Null Writable to a named output of distinct/part.
Again, this will create a folder called distinct underneath the job’s configured output directory.
In this example, we are outputting the same essential format—a Text object and a NullWritable object— from each of the reduce calls.
This won’t always be the case! If your jobs have conflicting output key/value types, you can utilize the Text object to normalize the outputs.
The driver code looks just like any other driver that uses MultipleOut puts.
All the logic of merging jobs is done inside the mapper and reducer implementation.
In this chapter, we’ll be focusing on what is probably the most often overlooked way to improve the value of MapReduce: customizing input and output.
You will not always want to load or store data the way Hadoop MapReduce does out of the box.
Sometimes you can skip the time-consuming step of storing data in HDFS and just accept data from some original source, or feed it directly to some process that uses it after MapReduce is finished.
Sometimes the basic Hadoop paradigm of file blocks and input splits doesn’t do what you need, so this is where a custom InputFormat or OutputFormat comes into play.
Three patterns in this chapter deal with input: generating data, external source input, and partition pruning.
All three input patterns share an interesting property: the map phase is completely unaware that tricky things are going on before it gets its input pairs.
Customizing an input format is a great way to abstract away details of the method you use to load data.
On the flip side, Hadoop will not always store data in the way you need it to.
There is one pattern in this chapter, external source output, that writes data to a system outside of Hadoop and HDFS.
Just like the custom input formats, custom output formats keep the map or reduce phase from realizing that tricky things are going on as the data is going out.
Customizing Input and Output in Hadoop Hadoop allows you to modify the way data is loaded on disk in two major ways: configuring how contiguous chunks of input are generated from blocks in HDFS (or maybe more exotic sources), and configuring how records appear in the map phase.
The two classes you’ll be playing with to do this are RecordReader and InputFormat.
These work with the Hadoop MapReduce framework in a very similar way to how mappers and reducers are plugged in.
Hadoop also allows you to modify the way data is stored in an analogous way: with an OutputFormat and a RecordWriter.
InputFormat Hadoop relies on the input format of the job to do three things:
Validate the input configuration for the job (i.e., checking that the data is there)
These pairs are sent one by one to their mapper.
The most common input formats are subclasses of FileInputFormat, with the Hadoop default being TextInputFormat.
The input format first validates the input into the job by ensuring that all of the input paths exist.
Then it logically splits each input file based on the total size of the file in bytes, using the block size as an upper bound.
Each map task will be assigned exactly one of these input splits, and then the RecordReader implementation is responsible for generate key/value pairs out of all the bytes it has been assigned.
Typically, the RecordReader has the additional responsibility of fixing boundaries, because the input split boundary is arbitrary and probably will not fall on a record boundary.
For example, the TextInputFormat reads text files using a LineRecordReader to create key/value pairs for each map task for each line of text (i.e., separated by a newline character)
The key is the number of bytes read in the file so far and the value is a string of characters up to a newline character.
Because it is very unlikely that the chunk of bytes for each input split will be lined up with a newline character, the LineRecordRead er will read past its given “end” in order to make sure a complete line is read.
This bit of data comes from a different data block and is therefore not stored on the same node, so it is streamed from a DataNode hosting the block.
This streaming is all handled by an instance of the FSDataInputStream class, and we (thankfully) don’t have to deal with any knowledge of where these blocks are.
Don’t be afraid to go past split boundaries in your own formats, just be sure to test thoroughly so you aren’t duplicating or missing any data!
As long as you can express the input as InputSplit objects and key/value pairs, custom or otherwise, you can read anything into the map phase of a MapReduce job in parallel.
Just be sure to keep in mind what an input split represents and try to take advantage of data locality.
The implementation of getSplits typically uses the given JobContext object to retrieve the configured input and return a List of InputSplit objects.
The input splits have a method to return an array of machines associated with the locations of the data in the cluster, which gives clues to the framework as to which TaskTracker should process the map task.
Typically, a new instance is created and immediately returned, because the record reader has an initialize method that is called by the framework.
RecordReader The RecordReader abstract class creates key/value pairs from a given InputSplit.
While the InputSplit represents the byte-oriented view of the split, the RecordReader makes sense out of it for processing by a mapper.
This is why Hadoop and MapReduce is considered schema on read.
It is in the RecordReader that the schema is defined, based solely on the record reader implementation, which changes based on what the expected input is for the job.
Bytes are read from the input source and turned into a Writable Comparable key and a Writable value.
Custom data types are very common when creating custom input formats, as they are a nice object-oriented way to present information to a mapper.
A RecordReader uses the data within the boundaries created by the input split to generate key/value pairs.
In the context of file-based input, the “start” is the byte position in the file where the RecordReader should start generating key/value pairs.
These are not hard boundaries as far as the API is concerned—there is nothing stopping a developer from reading the entire file for each map task.
While reading the entire file is not advised, reading outside of the boundaries it often necessary to ensure that a complete record is generated.
While using a TextInputFormat to grab each line works, XML elements are typically not on the same line and will be split by a typical MapReduce input.
By reading past the “end” input split boundary, you can complete an entire record.
After finding the bottom of the record, you just need to ensure that each record reader starts at the beginning of an XML element.
This will allow the MapReduce framework to cover the entire contents of an XML file, while not duplicating any XML records.
Any XML that is skipped by seeking forward to the start of an XML element will be read by the preceding map task.
The RecordReader abstract class has a number of methods that must be overridden.
This method takes as arguments the map task’s assigned InputSplit and TaskAt temptContext, and prepares the record reader.
For file-based input formats, this is a good place to seek to the byte position in the file to begin reading.
Be sure to reuse the objects returned by these methods if at all possible!
OutputFormat Similarly to an input format, Hadoop relies on the output format of the job for two main tasks:
Create the RecordWriter implementation that will write the output of the job.
On the flip side of the FileInputFormat, there is a FileOutputFormat to work with filebased output.
Because most output from a MapReduce job is written to HDFS, the many file-based output formats that come with the API will solve most of yours needs.
The default used by Hadoop is the TextOutputFormat, which stores key/value pairs to HDFS at a configured output directory with a tab delimiter.
Each reduce task writes an individual part file to the configured output directory.
The TextOutputFormat also validates that the output directory does not exist prior to starting the MapReduce job.
The TextOutputFormat uses a LineRecordWriter to write key/value pairs for each map task or reduce task, depending on whether there is a reduce phase or not.
This class uses the toString method to serialize each each key/value pair to a part file in HDFS, delimited by a tab.
This tab delimiter is the default and can be changed via job configuration.
Again, much like an InputFormat, you are not restricted to storing data to HDFS.
As long as you can write key/value pairs to some other source with Java (e.g., a JDBC database connection), you can use MapReduce to do a parallel bulk write.
Just make sure whatever you are writing to can handle the large number of connections from the many tasks.
The OutputFormat abstract class contains three abstract methods for implementation: checkOutputSpecs.
This method is used to validate the output specification for the job, such as making sure the directory does not already exist prior to it being submitted.
It will create temporary output directories for each map task and move the successful output to the configured output directory when necessary.
RecordWriter The RecordWriter abstract class writes key/value pairs to a file system, or another output.
Unlike its RecordReader counterpart, it does not contain an initialize phase.
However, the constructor can always be used to set up the record writer for whatever is needed.
The RecordWriter abstract class is a much simpler interface, containing only two methods: write.
This method is called by the framework for each key/value pair that needs to be written.
The implementation of this method depends very much on your use case.
The examples we’ll show will write each key/value pair to an external in-memory key/value store rather than a file system.
This can be used to release any file handles, shut down any connections to other services, or any other cleanup tasks needed.
Generating Data Pattern Description The generating data pattern is interesting because instead of loading data that comes from somewhere outside, it generates that data on the fly and in parallel.
You want to generate a lot of data from scratch.
This pattern is different from all of the others in the book in that it doesn’t load data.
With this pattern, you generate the data and store it back in the distributed file system.
Typically you’ll generate a bunch of the data at once then use it over and over again.
However, when you do need to generate data, MapReduce is an excellent system for doing it.
The most common use case for this pattern is generating random data.
Building some sort of representative data set could be useful for large scale testing for when the real data set is still too small.
It can also be useful for building “toy domains” for researching a proof of concept for an analytic at scale.
Generating random data is also used often used as part of a benchmark, such as the commonly used TeraGen/TeraSort and DFSIO.
Unfortunately, the implementation of this pattern isn’t straightforward in Hadoop because one of the foundational pieces of the framework is assigning one map task to an input split and assigning one map function call to one record.
In this case, there are no input splits and there are no records, so we have to fool the framework to think there are.
To implement this pattern in Hadoop, implement a custom InputFormat and let a RecordReader generate the random data.
For the most part, using the identity mapper is fine here, but you might want to do some post-processing in the map task, or even analyze it right away.
The RecordReader takes its fake split and generates random records from it.
In some cases, you can assign some information in the input split to tell the record reader what to generate.
For example, to generate random date/time data, have each input split account for an hour.
In most cases, the IdentityMapper is used to just write the data out as it comes in.
The lazy way of doing implementing this pattern is to seed the job with many fake input files containing a single bogus record.
Then, you can just use a generic InputFormat and RecordReader and generate the data in the map function.
The empty input files are then deleted on application exit.
There are a number of ways to create random data with SQL and Pig, but nothing that is eloquent or terse.
The major consideration here in terms of performance is how many worker map tasks are needed to generate the data.
In general, the more map tasks you have, the faster you can generate data since you are better utilizing the parallelism of the cluster.
However, it makes little sense to fire up more map tasks than you have map slots since they are all doing the same thing.
To generate random StackOverflow data, we’ll take a list of 1,000 words and just make random blurbs.
We also have to generate a random score, a random row ID (we can ignore that it likely won’t be unique), a random user ID, and a random creation date.
The following descriptions of each code section explain the solution to the problem.
The driver parses the four command line arguments to configure this job.
It sets our custom input format and calls the static methods to configure it further.
All the output is written to the given output directory.
The identity mapper is used for this job, and the reduce phase is disabled by setting the number of reduce tasks to zero.
There is no implementation for any of the overridden methods, or for methods requiring return values return basic values.
This input split is used to trick the framework into assigning a task to generate the random data.
We override the getSplits method to return a configured number of FakeInputSplit splits.
This record reader is where the data is actually generated.
It is given during our FakeInputSplit during initialization, but simply ignores it.
The number of records to create is pulled from the job configuration, and the list of random words is read from the DistributedCache.
For each call to nextKeyValue, a random record is created using a simple random number generator.
The body of the comment is generated by a helper function that randomly selects words from the list, between one and thirty words (also random)
The counter is incremented to keep track of how many records have been generated.
Once all the records are generated, the record reader returns false, signaling the framework that there is no more input for the mapper.
External Source Output Pattern Description As stated earlier in this chapter, the external source output pattern writes data to a system outside of Hadoop and HDFS.
You want to write MapReduce output to a nonnative location.
With this pattern, we are able to output data from the MapReduce framework directly to an external source.
This is extremely useful for direct loading into a system instead of staging the data to be delivered to the external source.
The pattern skips storing data in a file system entirely and sends output key/value pairs directly where they belong.
MapReduce is rarely ever hosting an applications as-is, so using MapReduce to bulk load into an external source in parallel has its uses.
In a MapReduce approach, the data is written out in parallel.
As with using an external source for input, you need to be sure the destination system can handle the parallel ingest it is bound to endure with all the open connections.
Figure 7-2 shows the external source output structure, explained below.
The OutputFormat verifies the output specification of the job configuration prior to job submission.
This is a great place to ensure that the external source is fully functional, as it won’t be good to process all the data only to find out the external source was unable when it was time to commit the data.
This method also is responsible for creating and initializing a RecordWriter implementation.
The RecordWriter writes all key/value pairs to the external source.
Much like a RecordReader, the implementation varies depending on the external data source being written to.
During construction of the object, establish any needed connections using the external source’s API.
These connections are then used to write out all the data from each map or reduce task.
The output data has been sent to the external source and that external source has loaded it successfully.
Note that task failures are bound to happen, and when they do, any key/ value pairs written in the write method can’t be reverted.
In a typical MapReduce job, temporary output is written to the file system.
In the event of a failure, this output is simply discarded.
When writing to an external source directly, it will receive the data in a stream.
If a task fails, the external source won’t automatically know about it and discard all the data it received from a task.
If this is unacceptable, consider using a custom OutputCommitter to write temporary output to the file system.
This temporary output can then be read, delivered to the external source, and deleted upon success, or deleted from the file system outright in the event of a failure.
From a MapReduce perspective, there isn’t much to worry about since the map and reduce are generic.
However, you do have to be very careful that the receiver of the data can handle the parallel connections.
Having a thousand tasks writing to a single SQL database is not going to work well.
To avoid this, you may have to have each reducer handle a bit more data than you typically would to reduce the number of parallel writes to the data sink.
This is not necessarily a problem if the destination of the data is parallel in nature and supports parallel ingestation.
For example, for writing to a sharded SQL database, you could have each reducer write to a specific database instance.
This example is a basic means for writing to a number of Redis instances in parallel from MapReduce.
It is often referred to as a data structure server, since keys can contain strings, hashes, lists, sets, and sorted sets.
Redis is written in ANSI C and works in most POSIX systems, such as Linux, without any external dependencies.
In order to work with the Hadoop framework, Jedis is used to communicate with Redis.
Jedis is an open-source “blazingly small and sane Redis java client.” A list of clients written for other languages is available on their website.
Unlike other examples in this book, there is no actual analysis in this example (along with the rest of the examples in this chapter)
It focuses on how to take a data set stored in HDFS and store it in an external data source using a custom FileOutputFormat.
In this example, the Stack Overflow users data set is written to a configurable number of Redis instances, specifically the user-to-reputation mappings.
These mappings are randomly distributed evenly among a single Redis hash.
A Redis hash is a map between string fields and string values, similar to a Java Hash Map.
Each hash is given a key to identify the hash.
Every hash can store more than four billion field-value pairs.
The sections below with its corresponding code explain the following problem.
Problem: Given a set of user information, randomly distributed user-to-reputation mappings to a configurable number of Redis instances in parallel.
Once the job has been submitted, it also creates the RecordWriter to serialize all the output key/value pairs.
However, we are not bound to using HDFS, as we will see in the RecordWriter later on.
The output format contains configuration variables that must be set by the driver to ensure it has all the information required to do its job.
Here, we have a couple public static methods to take some of the guess work out of what a developer needs to set.
This output format takes in a list of Redis instance hosts as a CSV structure and a Redis hash key to write all the output to.
In the checkOutputSpecs method, we ensure that both of these parameters are set before we even both launching the job, as it will surely fail without them.
The getRecordWriter method is used on the back end to create an instance of a Re cordWriter for the map or reduce task.
The details of this class are in the following section.
The output committer is used by the framework to manage any temporary output before committing in case the task fails and needs to be reexecuted.
For this implementation, we don’t typically care whether the task fails and needs to be re-executed.
An output committer is required by the framework, but the NullOutputFor mat contains an output committer implementation that doesn’t do anything.
Each key/value pair is randomly written to a Redis instance, providing an even distribution of all data across all Redis instances.
The constructor stores the hash key to write to and creates a new Jedis instance.
The code then connects to the Jedis instance and maps it to an integer.
This map is used in the write method to get the assigned Jedis instance.
The hash code is the key is taken modulo the number of configured Redis instances.
The key/value pair is then written to the returned Jedis instance to the configured hash.
Finally, all Jedis instances are disconnected in the close method.
The Mapper instance is very straightforward and looks like any other mapper.
The user ID and reputation are retrieved from the record and then output.
The output format does all the heavy lifting for us, allowing it to be reused multiple times to write whatever we want to a Redis hash.
The driver code parses the command lines and calls our public static methods to set up writing data to Redis.
External Source Input Pattern Description The external source input pattern doesn’t load data from HDFS, but instead from some system outside of Hadoop, such as an SQL database or a web service.
You want to load data in parallel from a source that is not part of your MapReduce framework.
The typical model for using MapReduce to analyze your data is to store it into your storage platform first (i.e., HDFS), then analyze it.
With this pattern, you can hook up the MapReduce framework into an external source, such as a database or a web service, and pull the data directly into the mappers.
There are a few reasons why you might want to analyze the data directly from the source instead of staging it first.
It may be faster to load the data from outside of Hadoop without having to stage it into files first.
For example, dumping a database to the file system is likely to be an expensive operation, and taking it from the database directly ensures that the MapReduce job has the most up-to-date data available.
A lot can happen on a busy cluster, and dumping a database prior to running an analytics can also fail, causing a stall in the entire pipeline.
In a MapReduce approach, the data is loaded in parallel rather than in a serial fashion.
The caveat to this is that the source needs to have well-defined boundaries on which data is read in parallel in order to scale.
For example, in the case of a sharded databases, each map task can be assigned a shard to load from the a table, thus allowing for very quick parallel loads of data without requiring a database scan.
The InputFormat creates all the InputSplit objects, which may be based on a custom object.
An input split is a chunk of logical input, and that largely depends on the format in which it will be reading data.
In this pattern, the input is not from a file-based input but an external source.
The input could be from a series of SQL tables or a number of distributed services spread through the cluster.
As long as the input can be read in parallel, this is a good fit for MapReduce.
The InputSplit contains all the knowledge of where the sources are and how much of each source is going to be read.
The framework uses the location information to help determine where to assign the map task.
A custom InputSplit must also implement the Writable interface, because the framework uses the methods of this interface to transmit the input split information to a TaskTracker.
The number of map tasks distributed among TaskTrackers is equivalent to the number of input splits generated by the input format.
The InputSplit is then used to initialize a RecordReader for processing.
The RecordReader uses the job configuration provided and InputSplit information to read key/value pairs.
The implementation of this class depends on the data source being read.
It sets up any connections required to read data from the external source, such as using JDBC to load from a database or creating a REST call to access a RESTful service.
Data is loaded from the external source into the MapReduce job and the map phase doesn’t know or care where that data came from.
The bottleneck for a MapReduce job implementing this pattern is going to be the source or the network.
The source may not scale well with multiple connections (e.g., a singlethreaded SQL database isn’t going to like 1,000 mappers all grabbing data at once)
Given that the source is probably not in the MapReduce cluster’s network backplane, the connections may be reaching out on a single connection on a slower public network.
This should not be a problem if the source is inside the cluster.
This example demonstrates how to read data we just wrote to Redis.
Again, we take in a CSV list of Redis instance hosts in order to connect to and read all the data from the hash.
Since we distributed the data across a number of Redis instances, this data can be read in parallel.
All we need to do is create a map task for each Redis instance, connect to Redis, and then create key/value pairs out of all the data we retrieve.
This example uses the identity mapper to simply output each key/value pair received from Redis.
The sections below with its corresponding code explain the following problem.
Problem: Given a list of Redis instances in CSV format, read all the data stored in a configured hash in parallel.
The RedisInputSplit represents the data to be processed by an individual Mapper.
In this example, we store the Redis instance hostname as the location of the input split, as well as the hash key.
The input split implements the Writable interface, so that it is serializable by the framework, and includes a default constructor in order for the framework to create a new instance via reflection.
We return the location via the getLocations method, in the hopes that the JobTracker will assign each map task to a TaskTracker that is hosting the data.
It contains configuration variables to know which Redis instances to connect to and which hash to read from.
This will create one map task for each configured Redis instance.
The createRecordReader method is called by the framework to get a new instance of a record reader.
The record reader’s initialize method is called by the framework, so we can just create a new instance and return it.
Again by convention, this class contains two nested classes for the record reader and input split implementations.
The initialize method is called by the framework and provided with an input split we created in the input format.
Here, we get the Redis instance to connect to and the hash key.
We then connect to Redis and get the number of key/value pairs we will be reading from Redis.
The hash doesn’t have a means to iterate or stream the data one at a time or in bulk, so we simply pull everything over and disconnect from Redis.
We store an iterator over the entries and log some helpful statements along the way.
In nextKeyValue, we iterate through the map of entries one at a time and set the record reader’s writable objects for the key and value.
A return value of true informs the framework that there is a key/value pair to process.
Once we have exhausted all the key/ value pairs, false is returned so the map task can complete.
The other methods of the record reader are used by the framework to get the current key and value for the mapper to process.
The getProgress method is useful for reporting gradual status to the JobTracker and should also be reused if possible.
Since we pulled all the information and disconnected from Redis in the initialize method, there is nothing to do here.
Much like the previous example’s driver, we use the public static methods provided by the input format to modify the job configuration.
Since we are just using the identity mapper, we don’t need to set any special classes.
The number of reduce tasks is set to zero to specify that this is a map-only job.
Partition Pruning Pattern Description Partition pruning configures the way the framework picks input splits and drops files from being loaded into MapReduce based on the name of the file.
You have a set of data that is partitioned by a predetermined value, which you can use to dynamically load the data based on what is requested by the application.
Typically, all the data loaded into a MapReduce job is assigned into map tasks and read in parallel.
If entire files are going to be thrown out based on the query, loading all of the files is a large waste of processing time.
By partitioning the data by a common value, you can avoid significant amounts of processing time by looking only where the data would exist.
For example, if you are commonly analyzing data based on date ranges, partitioning your data by date will make it so you only need to load the data inside of that range.
The added caveat to this pattern is this should be handled transparently, so you can run the same MapReduce job over and over again, but over different data sets.
This is done by simply changing the data you are querying for, rather than changing the implementation of the job.
A great way to do this would be to strip away how the data is stored on the file system and instead put it inside an input format.
The input format knows where to locate and get the data, allowing the number of map tasks generated to change based on the query.
This is exceptionally useful if the data storage is volatile and likely to change.
If you have dozens of analytics using some type of partitioned input format, you can change the input format implementation and simply recompile all analytics using the new input format code.
Since all your analytics get input from a query rather than a file, you don’t need to re-implement how the data is read into the analytic.
This can save a massive amount of development time, making you look really good to your boss!
Figure 7-4 shows the structure for partition pruning, explained below.
The getSplits method is where we pay special attention, because it determines the input splits that will be created, and thus the number of map tasks.
While the configuration is typically a set of files, configuration turns into more of a query than a set of file paths.
For instance, if data is stored on a file system by date, the InputFormat can accept a date range as input, then determine which folders to pull into the MapReduce job.
If data is sharded in an external service by date, say 12 shards for each month, only one shard needs to be read by the MapReduce job when looking for data in March.
The key here is that the input format determines where the data comes from based on a query, rather than passing in a set of files.
The RecordReader implementation depends on how the data is being stored.
If it is a file-based input, something like a LineRecordReader can be used to read key/ value pairs from a file.
If it is an external source, you’ll have to customize something more to your needs.
Partition pruning changes only the amount of data that is read by the MapReduce job, not the eventual outcome of the analytic.
The main reason for partition pruning is to reduce the overall processing time to read in data.
This is done by ignoring input that will not produce any output before it even gets to a map task.
When you create the table, you specify how the database should partition the data and the database will handle the rest on inserts.
Then, when you query with a specific value in the WHERE clause, the database will automatically use only the relevant partitions.
The data in this pattern is loaded into each map task is as fast as in any other pattern.
Only the number of tasks changes based on the query at hand.
Utilizing this pattern can provide massive gains by reducing the number of tasks that need to be created that would not have generated output anyways.
Outside of the I/O, the performance depends on the other pattern being applied in the map and reduce phases of the job.
This example demonstrates a smarter way to store and read data in Redis.
Rather than randomly distributing the user-to-reputation mappings, we can partition this data on particular criteria.
The user-to-reputation mappings are partitioned based on last access date and stored in six different Redis instances.
Two months of data are stored in separate hashes on each Redis instance.
By distributing the data in this manner, we can more intelligently read it based on a user query.
Whereas the previous examples took in a list of Redis instances and a hash key via the command line, this pattern hardcodes all the logic of where and how to store the data in the output format, as well as in the input format.
This completely strips away knowledge from the mapper and reducer of where the data is coming from, which has its advantages and disadvantages for a developer using our input and output formats.
It may not be the best idea to actually hardcode information into the Java code itself, but instead have a rarely-changing configuration file that can be found by your formats.
This way, things can still be changed if necessary and prevent a recompile.
Environment variables work nicely, or it can just be passed in via the command line.
The sections below with its corresponding code explain the following problem.
Problem: Given a set of user data, partition the user-to-reputation mappings by last access date across six Redis instances.
To help better store information, a custom Writable Comparable is implemented in order to allow the mapper to set information needed by the record writer.
This class contains methods to set and get the field name to be stored in Redis, as well as the last access month.
The last access month accepts a zero-based integer value for the month, but is later turned into a string representation for easier querying in the next example.
This output format is extremely basic, as all the grunt work is handled in the record writer.
The main thing to focus on is the templated arguments when extending the InputFormat class.
This output format accepts our custom class as the output key and a Text object as the output value.
Any other classes will cause errors when trying to write any output.
Since our record writer implementation is coded to a specific and known output, there is no need to verify any output specification of the job.
An output committer is still required by the framework, so we use NullOutputFormat’s output committer.
The mapper code parses each input record and sets the values for the output RedisKey and the output value.
The month of the last access data is parsed via the Calendar and SimpleDateFormat classes.
The driver looks very similar to a more basic job configuration.
All the special configuration is entirely handled by the output format class and record writer.
This example demonstrates how to query for the information we just stored in Redis.
Unlike most examples, where you provide some path to files in HDFS, we instead just pass in the months of data we want.
Figuring out where to get the data is entirely handled intelligently by the input format.
The heart of partition pruning is to avoid reading data that you don’t have to read.
By storing the user-to-reputation mappings across six different Redis servers, we need to connect only to the instances that are hosting the requested month’s data.
Even better, we need to read only from the hashes that are holding the specific month.
This is much better than having to connect to all the desired instances and read all the data, only to throw most of it away!
The sections below with its corresponding code explain the following problem.
Problem: Given a query for user to reputation mappings by months, read only the data required to satisfy the query in parallel.
The input split shown here is very similar to the input split in “External Source Input Example” (page 197)
Instead of storing one hash key, we are going to store multiple hash keys.
This is because the data is partitioned based on month, instead of all the data being randomly distributed in one hash.
Much like the output format we showed earlier in “OutputFormat code” (page 207), this output format writes RedisKey objects, this input format reads the same objects and is templated to enforce this on mapper implementations.
It initially creates a hash map of host-to-input splits in order to add the hash keys to the input split, rather than adding both months of data to the same split.
If a split has not been created for a particular month, a new one is created and the month hash key is added.
Otherwise, the hash key is added to the split that has already been created.
A List is then created out of the values stored in the map.
This will create a number of input splits equivalent to the number of Redis instances required to satisfy the query.
There are a number of helpful hash maps to help convert a month string to an integer, as well as figure out which Redis instance hosts which month of data.
The initialization of these hash maps are ommitted from the static block for brevity.
It needs to read from multiple hashes, rather than just reading everything at once in the initialize method.
In nextKeyValue, a new connection to Redis is created if the iterator through the hash is null, or if we have reached the end of all the hashes to read.
If the iterator through the hashes does not have a next value, we immediately return false, as there is no more data for the map task.
Otherwise, we connect to Redis and pull all the data from the specific hash.
The hash iterator is then used to exhaust all the field value pairs from Redis.
A do-while loop is used to ensure that once a hash iterator is complete, it will loop back around to get data from the next hash or inform the task there is no more data to be read.
The implementation of the remaining methods are identical to that of the RedisHash RecordReader and are omitted.
The driver code sets the months most recently accessed passed in via the command line.
This configuration parameter is used by the input format to determine which Redis instances to read from, rather than reading from every Redis instance.
Again, it uses the identity mapper rather than performing any analysis on the data retrieved.
At the time of this book’s writing, MapReduce is moving quickly.
New features and new systems are popping up every day and new users are out in droves.
More importantly for the subject of MapReduce Design Patterns, a growing number of users brings along a growing number of experts.
These experts are the ones that will drive the community’s documentation of design patterns not only by sharing new ones, but also by maturing the already existing ones.
In this chapter, we’ll discuss and speculate what the future holds for MapReduce design patterns.
Where will they come from? What systems will benefit from design patterns? How will today’s design patterns change with the technology? What trends in data will affect the design patterns of today?
Trends in the Nature of Data MapReduce systems such as Hadoop aren’t being used just for text analysis anymore.
Increasing number of users are deploying MapReduce jobs that analyze data once thought to be too hard for the paradigm.
New design patterns are surely to arise to deal with this to transform a solution from pushing the limits of the system to making it daily practice.
Images, Audio, and Video One of the most obvious trends in the nature of data is the rise of image, audio, and video analysis.
This form of data is a good candidate for a distributed system using MapReduce because these files are typically very large.
Retailers want to analyze their security video to detect what stores are busiest.
Unfortunately, as a text processing platform, some artifacts remain in MapReduce that make this type of analysis challenging.
Since this is a MapReduce book, we’ll acknowledge the fact that analyzing this type of data is really hard, even on a single node with not much data, but we will not go into more detail.
One place we may see a surge in design patterns is dealing with multidimensional data.
Videos have colored pixels that change over time, laid out on a two-dimensional grid.
To top it off, they also may have an audio track.
The data is in order from front to back and that is how it is analyzed.
Or, it is possible that new systems will fill this niche.
For example, SciDB, an open-source analytical database, is specifically built to deal with multi-dimensional data.
Streaming Data MapReduce is traditionally a batch analytics system, but streaming analytics feels like a natural progression.
In many production MapReduce systems, data is constantly streaming in and then gets processed in batch on an interval.
For example, data from web server logs are streaming in, but the MapReduce job is only executed every hour.
First, processing an hour’s worth of data at once can strain resources.
Because it’s coming in gradually, processing it as it arrives will spread out the computational resources of the cluster better.
Second, MapReduce systems typically depend on a relatively large block size to reduce the overhead of distributed computation.
When data is streaming in, it comes in record by record.
As in the previous section about large media files, this gap is likely to be filled by a combination of two things: new patterns and new systems.
Some new operational patterns for storing data of this nature might crop up as users take this problem more seriously in production.
New patterns for doing streaming-like analysis in the framework of batch MapReduce will mature.
Novel systems that deal with streaming data in Hadoop have cropped up, most notably the commercial product HStreaming and the open-source Storm platform, recently released by Twitter.
The authors actually considered some “streaming patterns” to be put into this book, but none of them were anywhere near mature enough or vetted enough to be officially documented.
The map task starts up and streams data into the RecordReader instead of loading already existing data from a file.
This has significant operational concerns that make it difficult to implement.
The second is splitting up the job into several one-map task jobs that get fired off every time some data comes in.
The output is partitioned into k bins for future “reducers.” Every now and then, a map-only job with k mappers starts up and plays the role of the reducer.
The Effects of YARN YARN (Yet Another Resource Negotiator) is a high-visibility advancement of Hadoop MapReduce that is currently in version 2.0.x and will eventually make it into the current stable release.
Many in the Hadoop community cannot wait for it to mature, as it fills a number of gaps.
At a high level, YARN splits the responsibilities of the JobTracker and TaskTrackers into a single ResourceManager, one NodeManager per node, and one ApplicationMaster per application or job.
The ResourceManager and NodeManagers abstract away computational resources from the current map-and-reduce slot paradigm and allow arbitrary computation.
Each ApplicationMaster handles a framework-specific model of computation that breaks down a job into resource allocation requests, which is in turn handled by the ResourceManager and the NodeManagers.
What this does is separate the computation framework from the resource management.
In this model, MapReduce is just another framework and doesn’t look any more special than a custom frameworks such as MPI, streaming, commercial products, or who knows what.
MapReduce design patterns will not change in and of themselves, because MapReduce will still exist.
However, now that users can build their own distributed application frameworks or use other frameworks with YARN, some of the more intricate solutions to problems may be more natural to solve in another framework.
We’ll see some design patterns that will still exist but just aren’t used very much anymore, since the natural solution lies in another distributed framework.
We will likely eventually see ApplicationMaster patterns for building completely new frameworks for solving a type of problem.
Patterns as a Library or Component Over time, as patterns get more and more use, someone may decide to componentize that pattern as a built-in utility class in a library.
This type of progression is seen in traditional design patterns, as well, in which the library parameterizes the pattern and you just interact with it, instead of reimplementing the pattern.
This is very natural from a standpoint of code reuse.
The patterns in this book are presented to help you start solving a problem from scratch.
By adding a layer of indirection, modules that set up the job for you and offer several parameters as points of customization can be helpful in the long run.
How You Can Help If you think you’ve developed a novel MapReduce pattern that you haven’t seen before and you are feeling generous, you should definitely go through the motions of documenting it and sharing it with the world.
There are a number of questions you should try to answer.
These were some of the questions we considered when choosing the patterns for this book.
Is the problem you are trying to solve similar to another pattern’s target problem?
Identifying this is important for preventing any sort of confusion.
What is at the root of this pattern? You probably developed the pattern to solve a very specific problem and have custom code interspersed throughout.
Developers will be smart enough to tailor a pattern to their own problem or mix patterns to solve their more complicated problems.
Tear down the code and only have the pattern left.
What is the performance profile? Understanding what kinds of resources a pattern will use is important for gauging how many reducers will be needed and in general how expensive this operation will be.
For example, some people may be surprised how resource intensive sorting is in a distributed system.
How might have you solved this problem otherwise? Finding some examples outside of a MapReduce context (such as we did with SQL and Pig) is useful as a metaphor that helps conceptually bridge to a MapReducespecific solution.
Overview Conceived by Burton Howard Bloom in 1970, a Bloom filter is a probabilistic data structure used to test whether a member is an element of a set.
Bloom filters have a strong space advantage over other data structures such as a Java Set, in that each element uses the same amount of space, no matter its actual size.
Bloom filters are introduced as part of a pattern in “Bloom Filtering” (page 49)
While the data structure itself has vast memory advantages, it is not always 100% accurate.
This means the result of each test is either a definitive “no” or “maybe.” You will never get a definitive “yes.” With a traditional Bloom filter, elements can be added to the set, but not removed.
There are a number of Bloom filter implementations that address this limitation, such as a Counting Bloom Filter, but they typically require more memory.
As more elements are added to the set, the probability of false positives increases.
Once they have been sized and trained, they cannot be reverse-engineered to achieve the original set nor resized and still maintain the same data set representation.
The following variables are used in the more detailed explanation of a Bloom filter below: m.
A Bloom filter is represented by a continuous string of m bits initialized to zero.
The bits of the Bloom filter at the resulting indices are set to one.
As elements are added to the Bloom filter, some bits may already be set to one from previous elements in the set.
When testing whether a member is an element of the set, the same hash functions are used to check the bits of the array.
If a single bit of all the hashes is set to zero, the test returns “no.” If all the bits are turned on, the test returns “maybe.” If the member was used to train the filter, the k hashs would have set all the bits to one.
The result of the test cannot be a definitive “yes” because the bits may have been turned on by a combination of other elements.
If the test returns “maybe” but should have been “no,” this is known as a false positive.
Thankfully, the false positive rate can be controlled if n is known ahead of time, or at least an approximation of n.
The following sections describe a number of common use cases for Bloom filters, the limitations of Bloom filters and a means to tweak your Bloom filter to get the lowest false positive rate.
A code example of training and using a Hadoop Bloom filter can be found in “Bloom filter training” (page 53)
Use Cases This section lists a number of common use cases for Bloom filters.
In any application that can benefit from a Boolean test prior to some sort of expensive operation, a Bloom filter can most likely be utilized to reduce a large number of unneeded operations.
Representing a Data Set One of the most basic uses of a Bloom filter is to represent very large data sets in applications.
A data set with millions of elements can take up gigabytes of memory, as well as the expensive I/O required simply to pull the data set off disk.
A Bloom filter can drastically reduce the number of bytes required to represent this data set, allowing it to fit in memory and decrease the amount of time required to read.
The obvious downside to representing a large data set with a Bloom filter is the false positives.
Whether or not this is a big deal varies from one use case to another, but there are ways to get a 100% validation of each test.
A post-process join operation on the actual data set can be executed, or querying an external database is also a good option.
Reduce Queries to External Database One very common use case of Bloom filters is to reduce the number of queries to databases that are bound to return many empty or negative results.
By doing an initial test using a Bloom filter, an application can throw away a large number of negative results before ever querying the database.
If latency is not much of a concern, the positive Bloom filter tests can be stored into a temporary buffer.
Once a certain limit is hit, the buffer can then be iterated through to perform a bulk query against the database.
This will reduce the load on the system and keep it more stable.
This method is exceptionally useful if a large number of the queries are bound to return negative results.
If most results are positive answers, then a Bloom filter may just be a waste of precious memory.
Google BigTable Google’s BigTable design uses Bloom filters to reduce the need to read a file for nonexistent data.
By keeping a Bloom filter for each block in memory, the service can do an initial check to determine whether it is worthwhile to read the file.
If the test returns a negative value, the service can return immediately.
Positive tests result in the service opening the file to validate whether the data exists or not.
By filtering out negative queries, the performance of this database increases drastically.
Downsides The false positive rate is the largest downside to using a Bloom filter.
Even with a Bloom filter large enough to have a 1% false positive rate, if you have ten million tests that should result in a negative result, then about a hundred thousand of those tests are going to return positive results.
Whether or not this is a real issue depends largely on the use case.
Traditionally, you cannot remove elements from a Bloom filter set after training the elements.
Removing an element would require bits to be set to zero, but it is extremely likely that more than one element hashed to a particular bit.
Setting it to zero would destroy any future tests of other elements.
One way around this limitation is called a Counting Bloom Filter, which keeps an integer at each index of the array.
When training a Bloom filter, instead of setting a bit to zero, the integers are increased by one.
When an element is removed, the integer is decreased by one.
This requires much more memory than using a string of bits, and also lends itself to having overflow errors with large data sets.
That is, adding one to the maximum allowed integer will result in a negative value (or zero, if using unsigned integers) and cause problems when executing tests over the filter and removing elements.
When using a Bloom filter in a distributed application like MapReduce, it is difficult to actively train a Bloom filter in the sense of a database.
However, further training of the Bloom filter would require expensive I/O operations, whether it be sending messages to every other process using the Bloom filter or implementing some sort of locking mechanism.
At this point, an external database might as well be used.
Tweaking Your Bloom Filter Before training a Bloom filter with the elements of a set, it can be very beneficial to know an approximation of the number of elements.
If you know this ahead of time, a Bloom filter can be sized appropriately to have a hand-picked false positive rate.
The lower the false positive rate, the more bits required for the Bloom filter’s array.
Figure A-1 shows how to calculate the size of a Bloom filter with an optimal-k.
The optimal-k is defined as the number of hash functions that should be used for the Bloom filter.
With a Hadoop Bloom filter implementation, the size of the Bloom filter and the number of hash functions to use are given when the object is constructed.
Using the previous formula to find the appropriate size of the Bloom filter assumes the optimalk is used.
Figure A-2 shows how the optimal-k is based solely on the size of the Bloom filter and the number of elements used to train the filter.
We’d like to hear your suggestions for improving our indexes.
About the Authors Donald Miner serves as a solutions architect at EMC Greenplum, advising and helping customers implement and use Greenplum’s big data systems.
Miner architected several large-scale and mission-critical Hadoop deployments with the U.S.
He is also involved in teaching, having previously instructed industry classes on Hadoop and a variety of artificial intelligence courses at the University of Maryland, Baltimore County (UMBC)
Adam Shook is a software engineer at ClearEdge IT Solutions, LLC, working with a number of big data technologies such as Hadoop, Accumulo, Pig, and ZooKeeper.
Shook graduated with a BS in Computer Science from the University of Maryland, Baltimore County (UMBC), and took a job building a new high-performance graphics engine for a game studio.
Seeking new challenges, he enrolled in the graduate program at UMBC with a focus on distributed computing technologies.
Shook is involved in developing and instructing training curriculum for both Hadoop and Pig.
He spends what little free time he has working on side projects and playing video games.
In China this deer is sometimes known as sibuxiang or “like none of the four” because it has characteristics of four animals and yet is none of them.
Many remark that it has the tail of a donkey, the hoofs of a cow, the neck of a camel, and the antlers of a deer.
