The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
My team was making discoveries similar to those that Doug Cutting and others at Nutch had made several years earlier regarding how to efficiently store and manage terabytes of crawled and analyzed data.
At the time, we were getting by with our home-grown distributed system, but the influx of a new data stream and requirements to join that stream with our crawl data couldn’t be supported by our existing system in the required timelines.
After some research we came across the Hadoop project, which seemed to be a perfect fit for our needs—it supported storing large volumes of data and provided a mechanism to combine them.
Within a few months we’d built and deployed a MapReduce application encompassing a number of MapReduce jobs, woven together with our own MapReduce workflow management system onto a small cluster of 18 nodes.
It was a revelation to observe our MapReduce jobs crunching through our data in minxv.
Of course we couldn’t anticipate the amount of time that we’d spend debugging and performance-tuning our MapReduce jobs, not to mention the new roles we took on as production administrators—the biggest surprise in this role was the number of disk failures we encountered during those first few months supporting production!
As our experience and comfort level with Hadoop grew, we continued to build more of our functionality using Hadoop to help with our scaling challenges.
We also started to evangelize the use of Hadoop within our organization and helped kick-start other projects that were also facing big data challenges.
The greatest challenge we faced when working with Hadoop (and specifically MapReduce) was relearning how to solve problems with it.
PREFACExvi flavor of parallel programming, which is quite different from the in-JVM programming that we were accustomed to.
The biggest hurdle was the first one—training our brains to think MapReduce, a topic which the book Hadoop in Action by Chuck Lam (Manning Publications, 2010) covers well.
After you’re used to thinking in MapReduce, the next challenge is typically related to the logistics of working with Hadoop, such as how to move data in and out of HDFS, and effective and efficient ways to work with data in Hadoop.
These areas of Hadoop haven’t received much coverage, and that’s what attracted me to the potential of this book—that of going beyond the fundamental word-count Hadoop usages and covering some of the more tricky and dirty aspects of Hadoop.
As I’m sure many authors have experienced, I went into this project confidently believing that writing this book was just a matter of transferring my experiences onto paper.
Boy, did I get a reality check, but not altogether an unpleasant one, because writing introduced me to new approaches and tools that ultimately helped better my own Hadoop abilities.
I hope that you get as much out of reading this book as I did writing it.
He also reviewed my early chapter drafts and helped mold the organization of the book.
I can’t express how much his support and encouragement has helped me throughout the process.
I’m also indebted to Cynthia Kane, my development editor at Manning, who coached me through writing this book and provided invaluable feedback on my work.
Among many notable “Aha!” moments I had while working with Cynthia, the biggest one was when she steered me into leveraging visual aids to help explain some of the complex concepts in this book.
Jonathan Seidman, the primary technical editor, did a great job reviewing the.
Many thanks to Josh Wills, the creator of Crunch, who kindly looked over the chapter that covers that topic.
And more thanks go to Josh Patterson, who reviewed my Mahout chapter.
Finally, a special thanks to my wife, Michal, who had to put up with a cranky husband working crazy hours.
She was a source of encouragement throughout the entire process.
With its distributed storage and compute capabilities, Hadoop is fundamentally an enabling technology for working with huge datasets.
Hadoop, to me, provides a bridge between structured (RDBMS) and unstructured (log files, XML, text) data, and allows these datasets to be easily joined together.
This has evolved from traditional use cases, such as combining OLTP and log files, to more sophisticated uses, such as using Hadoop for data warehousing (exemplified by Facebook) and the field of data science, which studies and makes new discoveries about data.
This book collects a number of intermediary and advanced Hadoop examples and presents them in a problem/solution format.
Each of the 85 techniques addresses a specific task you’ll face, like using Flume to move log files into Hadoop or using Mahout for predictive analysis.
Each problem is explored step by step and, as you work through them, you’ll find yourself growing more comfortable with Hadoop and at xviii.
This hands-on book targets users who have some practical experience with.
Hadoop and understand the basic concepts of MapReduce and HDFS.
Manning’s Hadoop in Action by Chuck Lam contains the necessary prerequisites to understand and apply the techniques covered in this book.
Many techniques in this book are Java-based, which means readers are expected to possess an intermediate-level knowledge of Java.
Part 1 contains a single chapter that’s the introduction to this book.
Hadoop basics and looks at how to get Hadoop up and running on a single host.
It wraps up with a walk-through on how to write and execute a MapReduce job.
Part 2, “Data logistics,” consists of two chapters that cover the techniques and tools required to deal with data fundamentals, getting data in and out of Hadoop, and how to work with various data formats.
Getting data into Hadoop is one of the first roadblocks commonly encountered when working with Hadoop, and chapter 2 is dedicated to looking at a variety of tools that work with common enterprise data sources.
Chapter 3 covers how to work with ubiquitous data formats such as XML and JSON in MapReduce, before going on to look at data formats better suited to working with big data.
Part 3 is called “Big data patterns,” and looks at techniques to help you work effectively with large volumes of data.
Chapter 6 looks at how to debug MapReduce performance issues, and also covers a number of techniques to help make your jobs run faster.
Part 4 is all about “Data science,” and delves into the tools and methods that help you make sense of your data.
Chapter 7 covers how to represent data such as graphs for use with MapReduce, and looks at several algorithms that operate on graph data.
Chapter 8 describes how R, a popular statistical and data mining platform, can be integrated with Hadoop.
Chapter 9 describes how Mahout can be used in conjunction with MapReduce for massively scalable predictive analytics.
Part 5 is titled “Taming the elephant,” and examines a number of technologies that make it easier to work with MapReduce.
The appendixes start with appendix A, which covers instructions on installing both Hadoop and all the other related technologies covered in the book.
Appendix B covers low-level Hadoop ingress/egress mechanisms that the tools covered in chapter 2 leverage.
All source code in listings or in text is in a fixed-width font like this to separate it from ordinary text.
Code annotations accompany many of the listings, highlighting important concepts.
Git is a source control management system, and GitHub provides hosted git repository services.
You can clone (download) my GitHub repository with the following command:
After the sources are downloaded you can build the code:
Running the code is equally simple with the included bin/run.sh.
If you’re running on a CDH distribution, the scripts will run configuration-free.
If you’re running on any other distribution, you’ll need to set the HADOOP_HOME environment variable to point to your Hadoop installation directory.
The bin/run.sh script takes as the first argument the fully qualified Java class name of the example, followed by any arguments expected by the example class.
As an example, to run the inverted index MapReduce code from chapter 1, you’d run the following:
The previous code won’t work if you don’t have Hadoop installed.
Please refer to chapter 1 for CDH installation instructions, or appendix A for Apache installation instructions.
I use a number of third-party libraries for the sake of convenience.
They’re included in the Maven-built JAR so there’s no extra work required to work with these libraries.
The following table contains a list of the libraries that are in prevalent use throughout the code examples.
Throughout this book you’ll work with three datasets to provide some variety for the examples.
All the datasets are small to make them easy to work with.
I also sometimes have data that’s specific to a chapter, which exists within chapter-specific subdirectories under the same GitHub location.
The data is in CSV form, and the fields are in the following order:
I created a sample log file in Apache Common Log Format (see http://mng.bz/ L4S3) with some fake Class E IP addresses and some dummy resources and response codes.
You’ll make frequent use of the IOUtils to close connections and to read the contents of files into strings.
You’ll make frequent use of the StringUtils class for tokenization.
Luckily, between the wikis and a vibrant user community your needs should be well covered.
The main wiki is located at http://wiki.apache.org/hadoop/, and contains useful presentations, setup instructions, and troubleshooting instructions.
The Hadoop Common, HDFS, and MapReduce mailing lists can all be found on http://hadoop.apache.org/mailing_lists.html.
Search Hadoop is a useful website that indexes all of Hadoop and its ecosystem projects, and it provides full-text search capabilities: http://search-hadoop.com/
You’ll find many useful blogs you should subscribe to in order to keep on top of current events in Hadoop.
The Hortonworks blog is worth reading; it discusses application and future Hadoop roadmap items: http://hortonworks.com/blog/
Michael Noll is one of the first bloggers to provide detailed setup instructions for Hadoop, and he continues to write about real-life challenges and uses of Hadoop: http://www.michael-noll.com/blog/
Purchase of Hadoop in Practice includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the author and other users.
These pages provide information on how to get on the forum after you are registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the author can take place.
It’s not a commitment to any specific amount of participation on the part of the author, whose contribution to the book’s forum remains voluntary (and unpaid)
For the last four years he has gained expertise in Hadoop solving big data problems across a number of projects.
He has presented at JavaOne and Jazoon and is currently a technical lead at VeriSign.
The book includes finely colored illustrations of figures from different regions of Croatia, accompanied by descriptions of the costumes and of everyday life.
Kistanja is a small town located in Bukovica, a geographical region in Croatia.
It is situated in northern Dalmatia, an area rich in Roman and Venetian history.
The word mamok in Croatian means a bachelor, beau, or suitor—a single young man who is of courting age—and the young man on the cover, looking dapper in a crisp, white linen shirt and a colorful, embroidered vest, is clearly dressed in his finest clothes, which would be worn to church and for festive occasions—or to go calling on a young lady.
Dress codes and lifestyles have changed over the last 200 years, and the diversity by region, so rich at the time, has faded away.
It is now hard to tell apart the inhabitants of different continents, let alone of different hamlets or towns separated by only a few miles.
Perhaps we have traded cultural diversity for a more varied personal life—certainly for a more varied and fast-paced technological life.
Manning celebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional life of two centuries ago, brought back to life by illustrations from old books and collections like this one.
The chapter then provides instructions for installing a pseudo-distributed Hadoop setup on a single host, and includes a system for you to run all of the examples in the book.
Chapter 1 also covers the basics of Hadoop configuration, and walks you through how to write and run a MapReduce job on your new setup.
Hadoop fills a gap in the market by effectively storing and providing computational capabilities over substantial amounts of data.
It’s a distributed system made up of a distributed filesystem and it offers a way to parallelize and execute programs on a cluster of machines (see figure 1.1)
You’ve most likely come across Hadoop as it’s been adopted by technology giants like Yahoo!, Facebook, and Twitter to address their big data needs, and it’s making inroads across all industrial sectors.
Because you’ve come to this book to get some practical experience with Hadoop and Java, I’ll start with a brief overview and then show you how to install Hadoop and run a MapReduce job.
By the end of this chapter you’ll have received Hadoop in a heartbeat.
We live in the age of big data, where the data volumes we need to work with on a day-to-day basis have outgrown the storage and processing capabilities of a single host.
Big data brings with it two fundamental challenges: how to store and work.
Hadoop was first conceived to fix a scalability issue that existed in Nutch,2 an open source crawler and search engine.
At the time Google had published papers that described its novel distributed filesystem, the Google File System (GFS), and MapReduce, a computational framework for parallel processing.
The successful implementation of these papers’ concepts in Nutch resulted in its split into two separate projects, the second of which became Hadoop, a first-class Apache project.
In this section we’ll look at Hadoop from an architectural perspective, examine how industry uses it, and consider some of its weaknesses.
Once we’ve covered Hadoop’s background, we’ll look at how to install Hadoop and run a MapReduce job.
Traits intrinsic to Hadoop are data partitioning and parallel computation of large datasets.
Its storage and computational capabilities scale with the addition of hosts to a Hadoop cluster, and can reach volume sizes in the petabytes on clusters with thousands of hosts.
In the first step in this section we’ll examine the HDFS and MapReduce architectures.
A model of communication where one process called the master has control over one or more other processes, called slaves.
To understand Hadoop’s architecture we’ll start by looking at the basics of HDFS.
It’s a distributed filesystem that’s modeled after the Google File System (GFS) paper.4 HDFS is optimized for high throughput and works best when reading and writing large files (gigabytes and larger)
To support this throughput HDFS leverages unusually large (for a filesystem) block sizes and data locality optimizations to reduce network input/output (I/O)
Scalability and availability are also key traits of HDFS, achieved in part due to data replication and fault tolerance.
Figure 1.3 shows a logical representation of the components in HDFS: the NameNode and the DataNode.
It also shows an application that’s using the Hadoop filesystem library to access HDFS.
Now that you have a bit of HDFS knowledge, it’s time to look at MapReduce, Hadoop’s computation engine.
MapReduce is a batch-based, distributed computing framework modeled after Google’s paper on MapReduce.5 It allows you to parallelize work over a large amount of.
The MapReduce master is responsible for organizing where computational work should be scheduled on the slave nodes.
This type of work, which could take days or longer using conventional serial programming techniques, can be reduced down to minutes using MapReduce on a Hadoop cluster.
The MapReduce model simplifies parallel processing by abstracting away the complexities involved in working with distributed systems, such as computational parallelization, work distribution, and dealing with unreliable hardware and software.
With this abstraction, MapReduce allows the programmer to focus on addressing business needs, rather than getting tangled up in distributed system complications.
MapReduce decomposes work submitted by a client into small parallelized map and reduce workers, as shown in figure 1.4
The map and reduce constructs used in MapReduce are borrowed from those found in the Lisp functional programming language, and use a shared-nothing model6 to remove any parallel execution interdependencies that could add unwanted synchronization points or state sharing.
A shared-nothing architecture is a distributed computing concept that represents the notion that each node is independent and self-sufficient.
The HDFS NameNode keeps in memory the metadata about the filesystem, such as which DataNodes manage the.
DataNodes communicate with each other for pipeline file reads and writes.
Files are made up of blocks, and each file can be replicated multiple times, meaning there are many identical copies of.
Figure 1.3 HDFS architecture shows an HDFS client communicating with the master NameNode and slave DataNodes.
What is Hadoop? The role of the programmer is to define map and reduce functions, where the map function outputs key/value tuples, which are processed by reduce functions to produce the final output.
Figure 1.5 shows a pseudo-code definition of a map function with regards to its input and output.
MapReduce decomposes the job into map and reduce tasks, and.
The map function takes as input a key/value pair, which represents a logical record from the input data source.
In the case of a file, this could be a line, or if the input source is a table in a database, it could be a row.
The map function produces zero or more output key/value pairs for that one input pair.
For example, if the map function is a filtering map function, it may only produce output if a certain condition is met.
Or it could be performing a demultiplexing operation, where a single input key/value yields multiple key/value output pairs.
The power of MapReduce occurs in between the map output and the reduce input, in the shuffle and sort phases, as shown in figure 1.6
Figure 1.7 shows a pseudo-code definition of a reduce function.
Hadoop’s MapReduce architecture is similar to the master-slave model in HDFS.
The main components of MapReduce are illustrated in its logical architecture, as shown in figure 1.8
With some MapReduce and HDFS basics tucked under your belts, let’s take a look at the Hadoop ecosystem, and specifically, the projects that are covered in this book.
The reduce function is called once per unique map output key.
The Hadoop ecosystem is diverse and grows by the day.
It’s impossible to keep track of all of the various projects that interact with Hadoop in some form.
In this book the focus is on the tools that are currently receiving the greatest adoption by users, as shown in figure 1.9
MapReduce is not for the faint of heart, which means the goal for many of these Hadoop-related projects is to increase the accessibility of Hadoop to programmers and nonprogrammers.
I cover all of the technologies listed in figure 1.9 in this book and describe them in detail within their respective chapters.
In addition, I include descriptions and installation instructions for all of these technologies in appendix A.
MapReduce clients talk to the JobTracker to launch and manage jobs.
The TaskTracker is a daemon process that spawns child processes to perform the actual map or reduce work.
Map tasks typically read their input from HDFS, and write their output to the local disk.
Reduce tasks read the map outputs over the network and write their outputs back to HDFS.
Let’s look at how to distribute these components across hosts in your environments.
The physical architecture lays out where you install and execute various components.
Figure 1.10 shows an example of a Hadoop physical architecture involving Hadoop and its ecosystem, and how they would be distributed across physical hosts.
ZooKeeper requires an odd-numbered quorum,7 so the recommended practice is to have at least three of them in any reasonably sized cluster.
For Hadoop let’s extend the discussion of physical architecture to include CPU, RAM, disk, and network, because they all have an impact on the throughput and performance of your cluster.
The term commodity hardware is often used to describe Hadoop hardware requirements.
It’s true that Hadoop can run on any old servers you can dig up, but you still want your cluster to perform well, and you don’t want to swamp your operations department with diagnosing and fixing hardware issues.
Therefore, commodity refers to mid-level rack servers with dual sockets, as much error-correcting RAM as is affordable, and SATA drives optimized for RAID storage.
Using RAID, however, is strongly discouraged on the DataNodes, because HDFS already has replication and error-checking built-in; but on the NameNode it’s strongly recommended for additional reliability.
A quorum is a High Availability (HA) concept that represents the minimum number of members required for a system to still remain online and functioning.
What is Hadoop? From a network topology perspective with regards to switches and firewalls, all of the master and slave nodes must be able to open connections to each other.
For small clusters, all the hosts would run 1 GB network cards connected to a single, good-quality switch.
Client nodes also need to be able to talk to all of the master and slave nodes, but if necessary that access can be from behind a firewall that permits connection establishment only from the client side.
In addition non-daemon software related to R (including Rhipe and RHadoop) needs to be installed.
Client hosts run application code in conjunction with the Hadoop ecosystem projects.
A reasonable question may be, why not split the Hadoop daemons onto separate hosts? If you were to do this, you would lose out on data locality (the ability to read from local disk), which is a key distributed system property of.
A single master node runs the master HDFS, MapReduce, and HBase daemons.
Running these masters on the same host is sufficient for small-to-medium Hadoop clusters, but with larger clusters it would be worth considering splitting them onto separate hosts due to the increased load they put on a single server.
After reviewing Hadoop’s physical architecture you’ve likely developed a good idea of who might benefit from using Hadoop.
Let’s take a look at companies currently using Hadoop, and in what capacity they’re using it.
Facebook uses Hadoop, Hive, and HBase for data warehousing and real-time application serving.8 Their data warehousing clusters are petabytes in size with thousands of nodes, and they use separate HBase-driven, real-time clusters for messaging and real-time analytics.
Twitter uses Hadoop, Pig, and HBase for data analysis, visualization, social graph analysis, and machine learning.
Twitter LZO-compresses all of its data, and uses Protocol Buffers for serialization purposes, all of which are geared to optimizing the use of its storage and computing resources.
Yahoo! uses Hadoop for data analytics, machine learning, search ranking, email antispam, ad optimization, ETL,9 and more.
Morgan, Groupon, LinkedIn, AOL, Last.fm, and StumbleUpon are some of the other organizations that are also heavily invested in Hadoop.
Microsoft is also starting to work with Hortonworks to ensure that Hadoop works on its platform.
Google, in its MapReduce paper, indicated that it used its version of MapReduce to create its web index from crawl data.10 Google also highlights applications of MapReduce to include activities such as a distributed grep, URL access frequency (from log data), and a term-vector algorithm, which determines popular keywords for a host.
The organizations that use Hadoop grow by the day, and if you work at a Fortune 500 company you almost certainly use a Hadoop cluster in some capacity.
It’s clear that as Hadoop continues to mature, its adoption will continue to grow.
As with all technologies, a key part to being able to work effectively with Hadoop is to understand its shortcomings and design and architect your solutions to mitigate these as much as possible.
Common areas identified as weaknesses across HDFS and MapReduce include availability and security.
All of their master processes are single points of failure, although.
In 2010 Google moved to a real-time indexing system called Caffeine: http://googleblog.blogspot.com/
What is Hadoop? you should note that there’s active work on High Availability versions in the community.
Security is another area that has its wrinkles, and again another area that’s receiving focus.
The 2.x NameNode HA design requires shared storage for NameNode metadata, which may require expensive HA storage.
It supports a single standby NameNode, preferably on a separate rack.
Hadoop does offer a security model, but by default it’s disabled.
With the security model disabled, the only security feature that exists in Hadoop is HDFS file and directory-level ownership and permissions.
But it’s easy for malicious users to subvert and assume other users’ identities.
By default, all other Hadoop services are wide open, allowing any user to perform any kind of operation, such as killing another user’s MapReduce jobs.
Hadoop can be configured to run with Kerberos, a network authentication protocol, which requires Hadoop daemons to authenticate clients, both user and other Hadoop components.
Kerberos can be integrated with an organization’s existing Active Directory, and therefore offers a single sign-on experience for users.
Finally, and most important for the government sector, there’s no storage or wire-level encryption in Hadoop.
Overall, configuring Hadoop to be secure has a high pain point due to its complexity.
Let’s examine the limitations of some of the individual systems.
The weakness of HDFS is mainly around its lack of High Availability, its inefficient handling of small files, and its lack of transparent compression.
The community is waiting for append support for files, a feature that’s nearing production readiness.
MapReduce is a batch-based architecture, which means it doesn’t lend itself to use cases that need real-time data access.
Tasks that require global synchronization or sharing of mutable data aren’t a good fit for MapReduce, because it’s a sharednothing architecture, which can pose challenges for some algorithms.
For example, HBase only works with a version of Hadoop that’s not verified as production ready, due to its HDFS sync requirements (sync is a mechanism that ensures that all writes to a stream have been written to disk across all replicas)
In reality, the HDFS single point of failure may not be terribly significant; see http://goo.gl/1iSab.
Other challenges with Hive and Hadoop also exist, where Hive may need to be recompiled to work with versions of Hadoop other than the one it was built against.
This is one of the advantages to using a Hadoop distribution other than Apache, as these compatibility problems have been fixed.
It includes all of the major Hadoop ecosystem components and runs a number of integration tests to ensure they all work in conjunction with each other.
After tackling Hadoop’s architecture and its weaknesses you’re probably ready to roll up your sleeves and get hands-on with Hadoop, so let’s take a look at how to get the Cloudera Distribution for Hadoop (CDH)12 up and running on your system, which you can use for all the examples in this book.
The goal of this section is to show you how to run a MapReduce job on your host.
To get there you’ll need to install Cloudera’s Hadoop distribution, run through some command-line and configuration steps, and write some MapReduce code.
Cloudera includes the Cloudera Manager, a full-blown service and configuration management tool that works well for provisioning Hadoop clusters with multiple nodes.
For this section we’re interested in installing Hadoop on a single host, so we’ll look at the individual packages that Cloudera offers.
You can view all of the available options at http://www.cloudera.com/hadoop/
Let’s look at the instructions for installation on a RedHat-based Linux system (in this case you’ll use CentOS)
Appendix A includes the installation instructions for both the CDH tarball and the Apache Hadoop tarball.
RedHat uses packages called RPMs for installation, and Yum as a package installer that can fetch RPMs from remote Yum repositories.
Cloudera hosts its own Yum repository containing Hadoop RPMs, which you’ll use for installation.
You’ll follow the pseudo-distributed installation instructions.13 A pseudo-distributed setup is one where all of the Hadoop components are running on a single host.
The first thing you need to do is download and install the “bootstrap” RPM, which will update your local Yum configuration to include Cloudera’s remote Yum repository:
I chose CDH for this task because of its simple installation and operation.
Next, you’ll import Cloudera’s RPM signing key so that Yum can verify the integrity of the RPMs that it downloads:
The last step is to install the pseudo-distributed RPM package, which has dependencies on all the other core Hadoop RPMs.
You’ll also install Pig, Hive, and Snappy (which is contained in the Hadoop native package), because you’ll be using them in this book:
For this book you’ll also be working with Oozie, HBase, and other projects, but you’ll find instructions for these technologies in their respective sections.
You’ve installed the basics—it’s time to learn how to configure Hadoop.
Let’s go over some basic commands so you can start and stop your cluster.
After you’ve completed the installation instructions in the previous section, your software is ready for use without editing any configuration files.
Knowing the basics of Hadoop’s configuration is useful, so let’s briefly touch upon it here.
You’ll find separate configuration files for different Hadoop components, and it’s worth providing a quick overview of them in table 1.1
Note that we had to split this command across two lines, so you use the “\” character to escape the newline.
The site XML files (those with site in their filenames) will grow as you start customizing your Hadoop cluster, and it can quickly become challenging to keep track of what changes you’ve made, and how they relate to the default configuration values.
To help with this the author has written some code14 that will compare the default and site files and indicate what properties have changed, as well as let you know about properties you may have misspelled.
Some example output of the utility is included in the following code, which shows a few of the differences between the CDH core-default.xml and the core-site.xml files:
If a current JDK isn’t in the system path you’ll want to come here to configure your JAVA_HOME.
You can also specify JVM options for various Hadoop components here.
Customizing directory locations such as the log directory and the locations of the master and slave files is also performed here, although by default you shouldn’t have to do any of what was just described in a CDH setup.
To view the default settings you can look at http://hadoop.apache.org/common/docs/r1.0.0/mapreddefault.html.
This name is misleading and should have been called secondary-masters.
When you start Hadoop it’ll launch NameNode and JobTracker on the local host from which you issued the start command, and then SSH to all the nodes in this file to launch the SecondaryNameNode.
When you start Hadoop it will SSH to each host in this file and launch the DataNode and TaskTracker daemons.
The Cloudera team has researched15 more advanced techniques using static and dynamic methods to determine what options are supported in Hadoop, as well as discrepancies between application and Hadoop configurations.
Let’s rattle through the essentials you need to get up and running.
You’ll need sudo access for your user to run this command (it launches the Hadoop services via init.d scripts):
You can test that things are up and running in a couple of ways.
First try issuing a command to list the files in the root directory in HDFS:
To make sure MapReduce is up and running you’ll need to run a quick command to see what jobs are running:
You wouldn’t think that the simple Hadoop filesystem command to list directory contents would have a quirk, but it does, and it’s one that has bitten many a user, including the author, on numerous occasions.
In bash and other shells it’s normal to affix the * wildcard to filesystem commands, and for the shell to expand that prior to running a program.
You would therefore (incorrectly) assume that the command hadoop fs -ls /tmp/* would work.
But if you run this, and /tmp exists in your filesystem, your shell will expand the path based on the contents of /tmp on your local filesystem, and pass these filenames into Hadoop.
At this point Hadoop will attempt to list files in HDFS that reside on your local system.
The workaround is to prevent path expansion from occurring by enclosing the path in double quotes—this would become hadoop fs -ls "/tmp/*"
Finally, to stop your cluster the process is similar to how you start it:
With these essentials under your belt your next step is to write a MapReduce job (don’t worry, it’s not word count) that you can run in your new cluster.
A better approach would be to tokenize each line and produce an intermediary file containing a word per line.
Each map is called once per line in the input file.
The mapper splits the line into distinct words, and outputs each word (the key) along with the word's originating filename (the value)
The reducer collects all the filenames for each key, and.
MapReduce sorts all the map output keys for a single reducer, and calls a reducer once for each unique.
MapReduce partitions the mapper output keys and ensures that the same reducer receives all output records containing.
Fo wo ID The final step would be to open all the sorted intermediary files and call a function for each unique word.
This is what MapReduce does, albeit in a distributed fashion.
Figure 1.11 walks you through an example of a simple inverted index in MapReduce.
Your reducers need to be able to generate a line for each word in your input, so your map output key should be each word in the input files so that MapReduce can join them all together.
The value for each key will be the containing filename, which is your document ID.
The goal of your reducer is to create an output line for each word, and a list of the document IDs in which the word appears.
The MapReduce framework will take care of calling your reducer once per unique key outputted by the mappers, along with a list of document IDs.
All you need to do in your reducer is combine all the document IDs together and output them once in the reducer, as you can see in the next code block.
When you extend the MapReduce mapper class you specify the key/value ty for your inputs and outputs.
You use the MapReduce default InputFormat your job, which supplies keys as byte offsets into the input file, and value.
A Text object to store the document ID (filename) for your input.
You’ll use is opportunity to store the put filename for this map.
This map method is called once per line; map tasks are run in paralle.
If th types types they will ou the k Text.
The last step is to write the driver code that will set all the necessary properties to configure your MapReduce job to run.
You need to let the framework know what classes should be used for the map and reduce functions, and also let it know where your input and output data is located.
By default MapReduce assumes you’re working with text; if you were working with more complex text structures, or altogether different data storage technologies, you would need to tell MapReduce how it should read and write from these data sources and sinks.
The reduce method is cal once per unique map out key.
Keep a set of all the document ID that you encounter for the key.
Your reduce outputs the word, an CSV-separated list of document.
Set the Reduce class that should be used for the job.
Set the Map class that should be used for the job.
Job class setJarByClass od determines the JAR contains the class that’s d-in, which beneath the s is copied by Hadoop into cluster and subsequently n the Task’s classpath so your Map/Reduce classes vailable to the Task.
In this case your map tput each word and file as ey/value pairs, and both are objects.
You’ll use a shell script to run it, supplying the two input files as arguments, along with the job output directory:
When your job completes you can examine HDFS for the job output files, and also view their contents:
You may be curious about where the map and reduce log files go.
For that you need to know the job’s ID, which will take you to the logs directory in the local filesystem.
When you run your job from the command line, part of the output is the job ID, as follows:
Hadoop expec multiple input files to be separated with comm.
With this ID in hand you can navigate to the directory on your local filesystem, which contains a directory for each map and reduce task.
These tasks can be differentiated by the m and r in the directory names:
Within each of the directories in the previous code there are three files, corresponding to standard out, standard error, and the system log (output from both the infrastructure task code, as well as any of your own log4j logging):
Remember that in the pseudo-distributed setup everything’s running on your local host, so it’s easy to see everything in one place.
Chapter summary will be local to the remote TaskTracker nodes, which can make it harder to get to them.
This is where the JobTracker and TaskTracker UI step in to provide easy access to the logs.
In CDH you can access the JobTracker UI at http://localhost:50030/jobtracker.jsp.
This completes your whirlwind tour of how to run Hadoop.
Hadoop is a distributed system designed to process, generate, and store large datasets.
Its MapReduce implementation provides you with a fault-tolerant mechanism for large-scale data analysis.
Hadoop also excels at working with heterogeneous structured and unstructured data sources at scale.
In this chapter, we examined Hadoop from functional and physical architectural standpoints.
The remainder of this book is dedicated to providing real-world techniques to solve common problems you encounter when working with Hadoop.
You’ll be introduced to a broad spectrum of subject areas, starting with HDFS and MapReduce, Pig, and Hive.
You’ll also look at data analysis techniques and explore technologies such as Mahout and Rhipe.
In chapter 2, the first stop on your journey, you’ll discover how to bring data into (and out of) Hadoop.
If you’ve been thinking about how to work with Hadoop in production settings, this part of the book covers the first two hurdles you’ll need to jump.
These chapters detail the often-overlooked yet crucial topics that deal with data management in Hadoop.
Chapter 2 looks at ways to manage moving large quantities of data into and out of Hadoop.
Examples include working with relational data in RDBMSs, structured files, and HBase.
The focus of chapter 3 is on ways to work with data stored in different formats, such as XML and JSON, which paves the way to a broader examination of data formats such as Thrift and Avro that work best with big data and Hadoop.
Moving data in and out of Hadoop, which I’ll refer to in this chapter as data ingress and egress, is the process by which data is transported from an external system into an internal system, and vice versa.
Hadoop supports ingress and egress at a low level in HDFS and MapReduce.
Files can be moved in and out of HDFS, and data can be pulled from external data sources and pushed to external data sinks using MapReduce.
Figure 2.1 shows some of Hadoop’s ingress and egress mechanisms.
The fact that your data exists in various forms and locations throughout your environments complicates the process of ingress and egress.
How do you bring in data that’s sitting in an OLTP (online transaction processing) database? Or ingress log data that’s being produced by tens of thousands of production servers? Or work with binary data sitting behind a firewall? Moving data in and out of Hadoop.
Further, how do you automate your data ingress and egress process so that your data is moved at regular intervals? Automation is a critical part of the process, along with monitoring and data integrity responsibilities to ensure correct and safe transportation of data.
In this chapter we’ll survey the tools that simplify the process of ferrying data in and out of Hadoop.
We’ll also look at how to automate the movement of log files, ubiquitous data sources for Hadoop, but which tend to be scattered throughout your environments and therefore present a collection and aggregation challenge.
In addition, we’ll cover using Flume for moving log data into Hadoop, and in the process we’ll evaluate two competing log collection and aggregation tools, Chukwa and Scribe.
We’ll also walk through how to move relational data in and out of Hadoop.
This is an emerging usage pattern where you can use Hadoop to join data sitting in your databases with data ingressed from other sources, such as log files, and subsequently push result data back out to databases.
Finally, we’ll cover how to use Sqoop for database ingress and egress activities, and we’ll look at how to ingress and egress data in HBase.
We’ll cover a lot of ground in this chapter, and it’s likely that you have specific types of data you need to ingress or egress.
If this is the case, feel free to jump directly to a particular section that provides the details you need.
In addition, if you’re looking for lower-level HDFS ingress and egress options‚ take a look at appendix B where I cover using tools such as WebHDFS and Hoop.
Let’s start things off with a look at key ingress and egress system considerations.
Figure 2.1 Hadoop data ingress and egress transports data to and from an external system to an internal one.
Moving large quantities of data in and out of Hadoop has logistical challenges that include consistency guarantees and resource impacts on data sources and destinations.
Before we dive into the techniques, however, we need to discuss the design elements to be aware of when working with data ingress and egress.
An idempotent operation produces the same result no matter how many times it’s executed.
In a relational database the inserts typically aren’t idempotent, because executing them multiple times doesn’t produce the same resulting database state.
Alternatively, updates often are idempotent, because they’ll produce the same end result.
Any time data is being written idempotence should be a consideration, and data ingress and egress in Hadoop is no different.
How well do distributed log collection frameworks deal with data retransmissions? How do you ensure idempotent behavior in a MapReduce job where multiple tasks are inserting into a database in parallel? We’ll examine and answer these questions in this chapter.
In the context of data ingress this can be useful because moving large quantities of small files into HDFS potentially translates into NameNode memory woes, as well as slow MapReduce execution times.
Having the ability to aggregate files or data together mitigates this problem, and is a feature to consider.
The data format transformation process converts one data format into another.
Often your source data isn’t in a format that’s ideal for processing in tools such as MapReduce.
If your source data is multiline XML or JSON form, for example, you may want to consider a preprocessing step.
This would convert the data into a form that can be split, such as a JSON or an XML element per line, or convert it into a format such as Avro.
Recoverability allows an ingress or egress tool to retry in the event of a failed operation.
Because it’s unlikely that any data source, sink, or Hadoop itself can be 100 percent available, it’s important that an ingress or egress action be retried in the event of failure.
In the context of data transportation, checking for correctness is how you verify that no data corruption occurred as the data was in transit.
When you work with heterogeneous systems such as Hadoop data ingress and egress tools, the fact that data is being transported across different hosts, networks, and protocols only increases the potential for problems during data transfer.
Common methods for checking correctness of raw data such as storage devices include Cyclic Redundancy Checks (CRC), which are what HDFS uses internally to maintain block-level integrity.
Resource consumption and performance are measures of system resource utilization and system efficiency, respectively.
Ingress and egress tools don’t typically incur significant load (resource consumption) on a system, unless you have appreciable data volumes.
For performance, the questions to ask include whether the tool performs ingress and egress activities in parallel, and if so, what mechanisms it provides to tune the amount of parallelism.
For example, if your data source is a production database, don’t use a large number of concurrent map tasks to import data.
Monitoring ensures that functions are performing as expected in automated systems.
For data ingress and egress, monitoring breaks down into two elements: ensuring that the process(es) involved in ingress and egress are alive, and validating that source and destination data are being produced as expected.
Let’s start with how you can leverage Hadoop’s built-in ingress and egress mechanisms.
The first step in working with data in Hadoop is to make it available to Hadoop.
As I mentioned earlier in this chapter, there are two primary methods that can be used for moving data into Hadoop: writing external data at the HDFS level (a data push), or reading external data at the MapReduce level (more like a pull)
Reading data in MapReduce has advantages in the ease with which the operation can be parallelized and fault tolerant.
Not all data is accessible from MapReduce, however, such as in the case of log files, which is where other systems need to be relied upon for transportation, including HDFS for the final data hop.
In this section we’ll look at methods to move source data into Hadoop, which I’ll refer to as data ingress.
I’ll use the data ingress design considerations in the previous section as the criteria to examine and understand the different tools as I go through the techniques.
We’ll look at Hadoop data ingress across a spectrum of data sources, starting with log files, then semistructured or binary files, then databases, and finally HBase.
We’ll start by looking at data ingress of log files.
This section will focus on high-level data ingress tools that provide easy and automated mechanisms to get data into Hadoop.
All these tools use one of a finite set of low-level mechanisms, however, which Hadoop provides to get data in and out.
An extensive evaluation of these mechanisms and tools is outside the scope of this chapter, but I provide them for reference in appendix B.
Log data has long been prevalent across all applications, but with Hadoop came the ability to process the large volumes of log data produced by production systems.
Various systems produce log data, from network devices and operating systems to web servers and applications.
These log files all offer the potential for valuable insights into how systems and applications operate as well as how they’re used.
What unifies log files is that they tend to be in text form and line-oriented, making them easy to process.
In this section we’ll look at tools that can help transport log data from source to HDFS.
We’ll also perform a deep dive into one of these tools and look at how to transport system log files into HDFS and Hive.
I’ll provide what you need to know to deploy, configure, and run an automated log collection and distribution infrastructure, and kick-start your own log data-mining activities.
Flume, Chukwa, and Scribe are log collecting and distribution frameworks that have the capability to use HDFS as a data sink for that log data.
It can be challenging to differentiate between them because they share the same features.
Apache Flume is a distributed system for collecting streaming data.
It’s an Apache project in incubator status, originally developed by Cloudera.
It offers various levels of reliability and transport delivery guarantees that can be tuned to your needs.
It’s highly customizable and supports a plugin architecture where you can add custom data sources and data sinks.
Chukwa is an Apache subproject of Hadoop that also offers a large-scale mechanism to collect and store data in HDFS.
After writing data into HDFS Chukwa runs a MapReduce job to demultiplex the data into separate streams.
Chukwa also offers a tool called Hadoop Infrastructure Care Center (HICC), which is a web interface for visualizing system performance.
Scribe is a rudimentary streaming log distribution service, developed and used heavily by Facebook.
A scribe server that collects logs runs on every node and forwards them to a central Scribe server.
Scribe supports multiple data sinks, including HDFS, regular filesystems, and NFS.
Scribe’s reliability comes from a file-based mechanism where the server persists to a local disk in the event it can’t reach the downstream server.
Unlike Flume or Chukwa, Scribe doesn’t include any convenience mechanisms to pull log data.
Instead the onus is on the user to stream the source data to the Scribe server running on the local system.
For example, if you want to push your Apache log files, you would need to write a daemon that tails and forwards the log data to the Scribe server.
Figure 2.4 Scribe architecture also pushes log data into HDFS.
Each of these three tools can fulfill the criteria of providing mechanisms to push log data into HDFS.
Table 2.1 compares the various tools based on features such as reliability and configuration.
From a high-level perspective there’s not much feature difference between these tools, other than that Scribe doesn’t offer any end-to-end delivery guarantees.
It’s also clear that the main downside for Chukwa and Scribe is their limited user documentation.
I had to pick one of these three tools for this chapter, so I picked Flume.
My reasons for selecting Flume include its centralized configuration, its flexible reliability and failover modes, and also the popularity of its mailing list.
Let’s look at how you can deploy and set up Flume to collect logs, using a problem/solution scenario.
I’ll continue to introduce techniques in this manner throughout the rest of this chapter.
You have a bunch of log files being produced by multiple applications and systems across multiple servers.
There’s no doubt there’s valuable information to be mined out of these logs, but your first challenge is a logistical one of moving these logs into your Hadoop cluster so that you can perform some analysis.
Problem You want to push all of your production server system log files into HDFS.
Solution For this technique you’ll use Flume, a data collection system, to push a Linux log file into HDFS.
We will also cover configurations required to run Flume in a distributed.
Discussion Figure 2.5 shows a full-fledged Flume deployment, and its four primary components:
Nodes—Flume data paths that ferry data from a data source to a data sink.
Agents and Collectors are simply Flume Nodes that are deployed in a way to efficiently and reliably work with a large number of data sources.
Agents—Collect streaming data from the local host and forward it to the Collectors.
Data sources are streaming data origins whose data you want to transport to a different destination.
Examples include application logs and Linux system logs, as well as nontext data that can be supported with custom data sources.
Data sinks are the destination of that data, which can be HDFS, flat files, and any data target that can be supported with custom data sinks.
You’ll run Flume in pseudo-distributed mode, which means you’ll run the Flume Collector, Agent, and Master daemons on your single host.
After you’ve installed these packages, start up the Master and Agent daemons:
Appendix A contains installation instructions and additional resources for working with Flume.
The data source in this exercise is the file /var/log/messages, the central file in Linux for system messages, and your ultimate data destination is HDFS.
Figure 2.6 shows this data flow, and the Agent and Collector configuration settings you’ll use to make it work.
By default, Flume will write data in Avro JSON format, which we’ll discuss shortly.
You’ll want to preserve the original format of your syslog file, so you’ll need to create and edit flume-site.xml and indicate the raw output format.
If you’ve set up your cluster with LZO compression, you’ll need to create a flume-env.sh file and set the directory that contains the native compression codecs:
You’ll also need to copy the LZO JAR into Flume’s lib directory:
Flume runs as the flume user, so you need to make sure that it has permissions to read any data source files (such as files under /var/log, for example)
Previously, when you launched the Flume Master and Node you were a Node short.
Let’s launch another Node, which you’ll name collector, to perform the Collector duties:
Each of the Flume daemons have embedded web user interfaces.
If you’ve followed the previous instructions, table 2.2 shows the locations where they’ll be available.
The advantage of using a Flume Master is that you can make configuration changes in a central location, and they’ll be pulled by the Flume Nodes.
There are two ways you can make configuration changes in the Flume Master: using the UI or the Flume shell.
You’ll need to configure the Agent and Collector Nodes according to the setup illustrated in figure 2.6
You’ll connect to the Flume Master UI, and select the config menu from the top, as highlighted in figure 2.7
The drop-down box contains all of the Nodes, and you’ll select the Agent node.
You should see one other Node in the drop-down called collector, which you’ll configure next.
For the Agent Node, you’ll specify that the data source is a tail of the syslog file and the data sink is the port that your Collector will run on.
Now select the Collector from the drop-down, and in a similar fashion, set the data source, which is the local port that you’ll listen on, and the data sink, which is the final destination in HDFS for your log data.
The main Flume Master screen displays all of the Nodes and their configured data sources and sinks, as shown in figure 2.9
All the actions you just performed can be executed in the Flume shell.
Here’s an example of how you can view the same information that you just saw in the UI.
To better identify the text you entered, the shell prompts are surrounded by square brackets.
After you make the configuration changes in the UI, they’ll be picked up by the Flume Nodes after a few seconds.
The Agent will then start to pipe the syslog file, and feed the output to the Collector, which in turn will periodically write output into HDFS.
We went through a whole lot of work without discussing some of the key concepts involved in setting up the data pipeline.
Let’s go back now and inspect the previous work in detail.
The first thing we’ll look at is the definition of the data source.
A data source is required for both the Agent and Collector Nodes; it determines where they collect their data.
The Agent Node’s data source is your application or system data that you want to transfer to HDFS, and the Collector Node’s data source is the Agent’s data sink.
Figure 2.10 shows a subset of data sources supported by the Agent Node.
You can configure the tail data source to emit the complete file as events, or just start from the current end of the file.
The multitail data source accepts a list of filenames, and the tailDir takes a directory name with a regular expression to filter files that should be tailed.
Flume also supports TCP/UDP data sources that can receive logs from syslog.
In addition, Flume can periodically execute a command and capture its output as an event for processing, via the execPeriodic data source.
And the exec data source gives you more control, allowing you to specify if each line of the processes output should be considered a separate message, or if the whole output should be considered the message.
Flume offers three different levels of reliability guarantees, which are summarized in table 2.3
Flume also has three levels of availability, as described in table 2.4
These reliability and availability modes are combined to form nine separate Agent sink options, as shown in figure 2.12
E2E (end to end) Guarantees that once an event enters Flume, the event will make its way to the end data sink.
Acknowledgement messages sent from downstream Nodes result in the persisted data being removed.
None Configure each Agent to write to a single Collector.
If the Collector goes down, the Agent waits until it comes back up again.
Configure each Agent with one or more Collectors in addition to the primary.
If the primary Collector fails, the events will be routed to backup Collectors.
The advantage is that Flume balances the Agent/Collector event links to ensure that individual Collectors aren’t overwhelmed.
This rebalancing also occurs in normal situations where Collectors are added and removed.
All of these Agent sinks take as arguments the Collector Node host, and the port that it’s listening on.
In the example in figure 2.7 I utilized the default agentSink option, which guarantees end-to-end delivery, but has no failover support.
The Collector is running on the same host, on port 35853:
Flume contains a single Collector data sink, called collectorSink, which you’ll configure to write to a directory on a local disk or HDFS.
The sink takes two parameters: the directory, and the filename prefix for files written in that directory.
Both of these arguments support the Flume functionality called output bucketing, which permits some macro substitutions.
Figure 2.11 Flume failover architecture shows the three levels available.
The Collection data sink supports a variety of output formats for events, some of which are shown in figure 2.13
Earlier in this chapter you created a flume-site.xml file and specified raw as your output format.
By default Flume chooses avrojson, an example of which is shown in the following code.
The body field contains the raw contents of a single line from the syslog:
This approach uses Flume to show how to capture syslog appends and write them into HDFS.
You can use this same approach for a variety of line-based text files.
Summary We’ve used Flume on a single machine, using the default configuration settings, which assumes everything runs on the local host and on standard ports.
In a fully distributed setup the Node hosts would need to specify where the Flume Master is located in flume-site.xml, as the next example demonstrates.
Figure 2.13 Flume Collector data sink supports a variety of output formats.
How do you determine the number of Flume masters that you should run? Let's say you want to be able to support the failure of two master Flume nodes.
Flume uses an embedded ZooKeeper in each of the Master daemons, but you can configure it to use an external ZooKeeper if one already exists in your environment.
If you’re capturing Apache logs, you can configure the web server to launch a Flume ad hoc Node and pipe the log output directly to Flume.
If that Node is talking to a remote Collector, then, unfortunately, the client Node can’t be configured for automatic failover or end-to-end reliability, because the Node can’t be managed by the Flume Master.
The workaround to this is to have the ad hoc Node forward to a local Flume Agent Node, which can have these reliability and failover properties.
Using Flume for log distribution has many advantages over its peers, primarily because of its high level of documentation, its ease of use, and its customizable reliability modes.
We’ve looked at an automated way to shuttle log data into HDFS.
But now imagine that the data you want to move into Hadoop isn’t log data, but instead data that these tools have a harder time working with, such as semistructured or binary files.
You’ve learned how to use log collecting tools like Flume to automate moving data into HDFS.
But these tools don’t support working with semistructured or binary data out of the box.
This section looks at techniques to help you automate moving such files into HDFS.
Production networks typically have network silos where your Hadoop clusters are segmented away from other production applications.
In such cases it’s possible that your Hadoop cluster isn’t able to pull data from other data sources, leaving you only the option to push data into Hadoop.
You need a mechanism to automate the process of copying files of any format into HDFS, similar to the Linux tool rsync.
The mechanism should be able to compress files written in HDFS and offer a way to dynamically determine the HDFS destination for data partitioning purposes.
Existing file transportation mechanisms such as Flume, Scribe, and Chukwa are geared towards supporting log files.
What if you have different file formats for your files, such as semistructured or binary? If the files were siloed in a way that the Hadoop slave nodes couldn’t directly access, then you couldn’t use Oozie to help with file ingress either.
Problem You need to automate the process by which files on remote servers are copied into HDFS.
Solution The HDFS File Slurper open source project can copy files of any format in and out of HDFS.
This technique covers how it can be configured and used to copy data into HDFS.
Discussion You can use the HDFS File Slurper project that I wrote2 to assist with your automation.
The HDFS File Slurper is a simple utility that supports copying files from a local directory into HDFS and vice versa.
Figure 2.14 provides a high-level overview of the Slurper (my nickname for the project), with an example of how you can use it to copy files.
The Slurper reads any files that exist in a source directory and optionally consults with a script to determine the file placement in the destination directory.
It then writes the file to the destination, after which there’s an optional verification step.
Slurper moves the source file to a completed folder upon successful completion of all of the previous steps.
With this technique there are a few challenges you need to make sure you address:
How do you effectively partition your writes to HDFS so that you don’t lump everything into a single directory?
How do you determine that your data is ready in HDFS for processing in order to avoid reading files that are mid-copy?
The first step is to download and build the code.
The following assumes that you have git, Java, and version 3.0 or newer of Maven installed locally:
Next you’ll need to untar the tarball that the build created under /usr/local:
The file contains the following default settings, which you can change:
This is used for the log filename when launched via the Linux init daemon management system, which we’ll cover shortly.
Files from e source directory are moved to this directory before the py to the destination starts.
After the copy has completed file is moved from the directory into the com directory.
You’ll notice that all of the directory names are HDFS URIs.
The file:/ URI denotes a path on the local filesystem, and the hdfs:/ URI denotes a path in HDFS.
In fact, the Slurper supports any Hadoop filesystem, as long as you configure Hadoop to use it.
Let’s use the default settings, create a local directory called /tmp/slurper/in, write an empty file into it, and run the utility.
If you’re running your environment on a Hadoop distribution other than CDH, the HADOOP_HOME environment variable needs to be set with the location of your Hadoop installation:
A key feature in the Slurper’s design is that it doesn’t work with partially written files.
Files must be atomically moved into the source directory (file moves in both the Linux3 and HDFS filesystems are atomic)
Another key consideration with the Slurper is the assurance that files being copied are globally unique.
If they aren’t, the Slurper will overwrite that file in HDFS, which is likely an undesirable outcome.
Moving files is atomic only if both the source and destination are on the same partition.
In other words, moving a file from a NFS mount to a local disk results in a copy, which isn’t atomic.
Files are first co into this directory in the destination system, and then the slurper perform.
The previous approach works well if you’re working with a small number of files that you’re moving into HDFS on a daily basis.
But if you’re dealing with a large volume of files you want to think about partitioning them into separate directories.
This has the benefit of giving you more finely grained control over the input data for your MapReduce jobs, as well as helping with the overall organization of your data in the filesystem (you wouldn’t want all the files on your computer to reside in a single flat directory)
How can you have more dynamic control over the destination directory and the filename that the Slurper uses? The Slurper configuration file (slurper.conf) has a SCRIPT option (which is mutually exclusive of the DEST_DIR option), where you can specify a script that can provide that dynamic mapping of the source files to destination files.
Let’s assume that the files you’re working with contain a date in the filename, and you’ve decided that you want to organize your data in HDFS by date.
Let’s write a script that can perform this mapping activity.
The following example is a Python script that does this:
What if you want to compress the output file in HDFS, and also verify that the copy is correct? You’d need to use the COMPRESSION_CODEC option, whose value, CompressionCodec classname, also needs to be used.
The Slurper also supports verification, which rereads the destination file after the copy has completed, and ensures that the checksum of the destination file matches the source file.
If you want to configure the Slurper to use Snappy for compression and verify the copy, you’d update the slurper.conf file and add the following lines:
Now that you have the basic mechanics in place, your final step is to run your tool as a daemon, so that it continuously looks for files to transfer.
This script won’t create a PID file or perform a nohup—because neither makes sense in the context of respawn since inittab is managing the process—and uses the DATASOURCE_NAME configuration value to create the log filename.
This means that multiple Slurper instances can all be launched with different config files logging to separate log files.
Summary The Slurper is a handy tool for data ingress from a local filesystem to HDFS.
It also supports data egress by copying from HDFS to the local filesystem.
Inittab is a Linux process management tool that you can configure to supervise and restart a process if it goes down.
Now let’s look at automated pulls, for situations where MapReduce has access to your data sources.
If your data is sitting on a filesystem, web server, or any other system accessible from your Hadoop cluster, you’ll need a way to periodically pull that data into Hadoop.
While tools exist to help with pushing log files and pulling from databases (which we’ll cover in this chapter), if you need to interface with some other system, it’s likely you’ll need to handle the data ingress process yourself.
There are two parts to this data ingress process: the first is how you import data from another system into Hadoop, and the second is how you regularly schedule the data transfer.
Problem You want to automate a daily task to download content from an HTTP server into HDFS.
Solution Oozie can be used to ingress data into HDFS, and can also be used to execute postingress activities such as launching a MapReduce job to process the ingressed data.
It’s a Hadoop workflow engine that manages data processing activities.
For our scenario Oozie has a coordinator engine that can start workflows based on data and time triggers.
Discussion In this technique, you perform a download of a number of URLs every 24 hours using Oozie to manage the workflow and scheduling.
The flow for this technique is shown in figure 2.15
Oozie has the notion of a coordinator job, which can launch a workflow at fixed intervals.
The first step is to look at the coordinator XML configuration file.
This file is used by Oozie’s Coordination Engine to determine when it should kick off a workflow.
Oozie uses the JSP expression language to perform parameterization, as you’ll see in the following code.
Create a file called coordinator.xml with the content shown in the next listing.5
What can be confusing about Oozie’s coordinator is that the start and end times don’t relate to the actual times that the jobs will be executed.
Rather, they refer to the dates that will be materialized for the workflow.
Determines how often the coordina scheduled to run, expressed in minutes.
In this example, you don’t want to go back in time, but instead want to schedule a job periodically every 24 hours going forward.
However, you won’t want to wait hours until the next day, so you can set the start date to be yesterday, and the end date to be some far-off date in the future.
Next you’ll need to define the actual workflow, which will be executed for every interval in the past, and going forward when the wall clock reaches an interval.
To do this, create a file called workflow.xml, with the content shown here.6
The last step is to define your properties file, which specifies how to get to HDFS, MapReduce, and the location of the two XML files previously identified in HDFS.
Create a file called job.properties, as shown in the following code:
Now you need to copy the XML files, your input file, and the JAR file containing your MapReduce code into HDFS:
You can use the job ID to get some information about the job:
The overall state is RUNNING, which means that the job is waiting for the next interval to occur.
When the overall job has completed (after the end date has been reached), the status will transition to SUCCEEDED.
Because the job ran twice you should confirm that there are two output directories in HDFS corresponding to the two materialized dates:
If you wish to stop the job, use the -suspend option:
Summary I showed you one example of the use of the Oozie coordinator, which offers cron-like capabilities to launch periodic Oozie workflows.
The Oozie coordinator can also be used to trigger a workflow based on data availability.
For example, if you had an external process, or even MapReduce generating data on a regular basis, you could use Oozie’s data-driven coordinator to trigger a workflow, which could aggregate or process that data.
In this section we covered two automated mechanisms that can be used for data ingress purposes.
The first technique covered a simple tool, the HDFS File Slurper, which automates the process of pushing data into HDFS.
The second technique looked at how Oozie could be used to periodically launch a MapReduce job to pull data into HDFS or MapReduce.
These techniques are particularly helpful in situations where the data you’re working with is in a binary or semistructured form, or is only accessible via interfaces such as HTTP or FTP.
At this point in our review of data ingress we’ve looked at pushing log files, pushing files from regular filesystems, and pulling files from web servers.
Another data source that will be of interest to most organizations is relational data sitting in OLTP databases.
Next up is a look at how you can access that data.
Most organizations’ crucial data exists across a number of OLTP databases.
The data stored in these databases contains information about users, products, and a host of other useful items.
If you want to analyze this data the traditional mechanism for doing so would be to periodically copy that data into a OLAP data warehouse.
Hadoop has emerged to play two roles in this space: as a replacement to data warehouses, and as a bridge between structured and unstructured data and data warehouses.
Figure 2.16 shows the second role in play, where Hadoop is used as a large-scale joining and aggregation mechanism prior to export to an OLAP system (a commonly used platform for business intelligence applications)
Facebook is an example of an organization that has successfully utilized Hadoop and Hive as an OLAP platform to work with petabytes of data.
Figure 2.17 shows an architecture similar to that of Facebook’s.
This architecture also includes a feedback loop into the OLTP system, which can be used to push discoveries made in Hadoop, such as recommendations for users.
In either of the usage models shown in the previous figures, you need a way to bring relational data into Hadoop, and to also export it into relational databases.
In the next techniques we’ll cover two mechanisms you can use for database ingress.
The first uses some built-in MapReduce classes, and the second provides an easy-to-use tool that removes the need for you to write your own code.
Imagine you had valuable customer data sitting in a relational database.
You used one of the previous techniques to push log data containing user activity from your web servers into HDFS.
You now want to be able to do some analytics on your log data, and tie it back with your users in your relational database.
How do you move your relational data into HDFS? Problem You want to import relational data using MapReduce.
Solution This technique covers using the DBInputFormat class to import relational data into HDFS.
It also looks at mechanisms to guard against too many concurrent connections to your relational database.
Discussion MapReduce contains DBInputFormat and DBOutputFormat classes, which can be used to read and write data from databases via JDBC.
You’ll work with stock data (more details about stock data are contained in the preface to this book)
The DBInputFormat class requires a bean representation of the table being imported, which implements both the Writable and DBWritable interfaces.
Figure 2.18 Example of MapReduce in four stages where DBInputFormat is used to pull data from a database.
Note that the array of column names in the previous code will be used later in your MapReduce configuration.
MapReduce job usingDBInputFormat to import data froma relational database intoHDFS.
Using a helper class to add the MySQL JAR to the distributed cache so that your map and reduce tasks have access to the JAR.
Database configuration step, where you specify the JDBC driver and the.
This should a low number or you may ng down your database.
Before you can continue, you’ll need access to a MySQL database and have the MySQL JDBC JAR available.10 You have access to a script (see note) that will create the necessary MySQL user and schema, and load the data for this technique.
It then loads the stocks sample data (more details in the preface) into the stocks table.
All of these steps are performed by running the following command:
MySQL installation instructions can be found in appendix A, if you don’t already have it installed.
That section also includes a link to get the JDBC JAR.
The preface has instructions to download, build, and run the examples.
Scrip thro your Here’s a quick peek at what the script did with the following MySQL client commands:
The utility run.sh will launch the DBImportMapReduce MapReduce job, as shown in the following code:
The result of this MapReduce job is a number of Avro files in the job output directory containing the results of the SQL query.
The AvroStockFileRead class, which we’ll examine in chapter 3, can be used to view the contents of the Avro files:
Add MySQL JDBC JAR to Hadoop classpath so it’s available in client code.
MySQL JDBC JAR to use to communicate with MySQL server.
There’s one file per map task and each contains a subset of the data that has been imported.
Summary There are several important considerations to bear in mind when using this technique.
First, you need to make sure that you don’t run with too many map tasks, because bombarding your database with thousands of concurrent reads will probably bring the DBA knocking on your door.
You should also ensure that your import and count queries are idempotent.
Figure 2.19 illustrates the time differences between when the initial count query is executed as part of job submission and the subsequent queries from the map tasks.
If data in the table being exported is inserted or deleted, you’ll likely get duplicate records and potentially worse side effects.
Therefore, either perform the import against an immutable table, or carefully choose your queries to guarantee that the results won’t change during the import.
As the figure 2.19 indicates, because multiple mappers will be running with differing LIMIT and OFFSET settings, the query should be written in a way that will ensure consistent and repeatable ordering in mind, which means that the query needs to leverage a unique key, such as a primary key.
Phew! Importing data from a relational database seems more involved than it should be—which makes you wonder if there’s a better way to perform the import.
The previous technique required a fair amount of work as you had to implement the Writable and DBWritable interfaces, and then write a MapReduce job to perform the import.
Surely there’s an easier way to import relational data! Problem You want to load relational data into your cluster and ensure your writes are efficient and at the same time idempotent.
The initial query is submitted by the MapReduce client to determine the number of rows to import.
The actual query to import the data is executed in each map task some.
Solution In this technique, we’ll look at how to use Sqoop as a simple mechanism to bring relational data into Hadoop clusters.
We’ll walk through the process of importing data from MySQL into Sqoop.
We’ll also cover methods for using the regular connector, as well as how to do bulk imports using the fast connector.
Discussion Sqoop is a relational database import and export system.
It was created by Cloudera, and is currently an Apache project in incubation status.
When you perform an import, Sqoop can write to HDFS, Hive, or HBase, and for exports it can do the reverse.
Importing is broken down into two activities: connecting to the data source to gather some statistics, and then firing off a MapReduce job, which performs the actual import.
Sqoop has the notion of Connectors, which contain the specialized logic to read and write to external systems.
Figure 2.21 shows these two classes of connectors and the databases that they support.
To get started, you’ll need to install Sqoop, and the instructions for this are covered in appendix A.
I recommend you read these instructions because they also contain steps for installing Sqoop dependencies, such as MySQL JDBC drivers.
The first Sqoop command will be a basic import, where you’ll specify connection information about your MySQL database, and the table you want to export:
It’s generally not a good idea to have database passwords as arguments to a script because that allows other users to see your password using commands, such as ps, when the import is occurring.
A best practice to follow is to write the password in Sqoop’s option file and ensure that only you have read permissions on it:
Sqoop also supports a -P option, which when present will result in you being prompted for the password.
Run the command again, this time specifying the options file you’ve created:
Make sure that the table name you supply in the Sqoop commands is also case sensitive.
You may wonder why you had to delete the stocks directory in HDFS before rerunning the import command.
Sqoop by default uses the table name as the destination in HDFS for the MapReduce job that it launches to perform the import.
If you run the same command again, the MapReduce job will fail because the directory already exists.
Let’s take a look at the stocks directory in HDFS:
It supports a number of other file formats, which can be activated with the arguments listed in table 2.5
If you’re importing large amounts of data you may want to use a file format such as Avro, which is a compact data format, and use it in conjunction with compression.
The following example uses the Snappy compression codec in conjunction with Avro files:
The Snappy compression codec requires you to have the Hadoop native libraries installed.
See chapter 5 for more details on compression setup and configuration.
You can introspect the structure of the Avro file to see how Sqoop has laid out the records by using an Avro dumper tool that I created.
Table 2.5 Sqoop arguments that control the file formats of import commands.
If you run your generic Avro dumper utility against the Sqoop-generated files in HDFS you’ll see the following:
In reality you’ll more likely want to periodically import a subsection of your tables based on a query.
But what if you want to import all of the Apple and Google stocks in 2007 and stick them into a custom HDFS directory? The following code shows how you would do this with Sqoop:
The --query SQL shown in the previous snippet can also be used to include only a subset of the columns in a table to be imported.
One of the things that makes SequenceFiles hard to work with is that there isn’t a generic way to access data in a SequenceFile.
You must have access to the Writable class that was used to write the data.
This introduces a major problem: if you move to a newer version of Sqoop, and that version modifies the code generator, there’s a good chance your older, code-generated class won’t work with SequenceFiles generated with the newer version of Sqoop.
You’ll either need to migrate all of your old SequenceFiles to the new version, or have code which can work with different versions of these SequenceFiles.
Due to this restriction I don’t recommend using SequenceFiles with Sqoop.
If you’re looking for more information on how SequenceFiles work, run the Sqoop import tool and look at the stocks.java file that’s generated within your working directory.
Bash by default performs globbing, meaning that it’ll expand wildcards like “*”
The NDITIONS is a Sqoop ro that must be ent in the WHERE se of the query.
It’s by Sqoop to titute LIMIT and SET options when ng mySql queries.
This argument must be supplied so t Sqoop can determine w.
It inspects the table being imported to determine the primary key and runs a query to determine the lower and upper bounds of the data in the table (shown in figure 2.22)
A somewhat even distribution of data within the minimum and maximum keys is assumed by dividing the delta by the number of mappers.
Each mapper is then fed a unique query containing a range of the primary key.
You can configure Sqoop to use a nonprimary key with the --split-by argument.
This can be useful in situations where the primary key doesn’t have an even distribution of values between the min and max values.
For large tables, however, you need to be careful that the column specified in --split-by is indexed to ensure optimal import times.
You can use the --boundary-query argument to construct an alternative query to determine the minimum and maximum values.
Sqoop supports two types, append, which works for numerical data that’s incrementing over time, such as auto-increment keys; and lastmodified, which works on timestamped data.
In both cases you need to specify the column using --check-column, the mode via the --incremental argument (the value must be either append or lastmodified), and finally, the actual value to use to determine the incremental changes, --last-value.
The number of mappers can be controlled with the --num-mappers argument.
You can see in the command output the last value that was encountered for the increment column.
How can you best automate a process that can reuse that value? Sqoop has the notion of a job, which can save this information and reuse it in subsequent executions:
This merely saves the notion of this command as a job in something called the Sqoop metastore.
By default, the metastore is contained in your home directory under .sqoop, and is only used for your own jobs.
If you want to share jobs, you would need to install a JDBC-compliant database and use the --meta-connect argument to specify its location when issuing job commands.
The job create command executed in the previous example didn’t do anything other than add the job to the metastore.
To run the job you need to explicitly execute it as shown here:
Unfortunately, the --options-file argument, which referred to your local file with your username and password, doesn’t work with jobs in Sqoop.
The password also can’t be specified when creating the job.
Sqoop will instead prompt for the password when running the job.
To make this work in an automated script you need to use Expect, a Linux automation tool, to supply the password from a local file when it detects Sqoop prompting for a password.
The source of an Expect script that works with Sqoop is on GitHub at http://goo.gl/yL4KQ.
The metadata includes the last value of your incre column.
What if you want to bypass JDBC altogether and use the fast MySQL Sqoop connector for a high-throughput load into HDFS? This approach uses the mysqldump utility shipped with MySQL to perform the load.
You must make sure that mysqldump is in the PATH of the user running the MapReduce job.
To enable use of the fast connector you must specify the --direct argument:
What are the disadvantages of fast connectors? First, only MySQL and PostgreSQL are currently supported.
Fast connectors also only work with text output files—specifying Avro or SequenceFile as the output format of the import won’t work.
The final step in this technique is to use Sqoop to import your data into a Hive table.
The only difference between an HDFS import and a Hive import is that the Hive import has a postprocessing step where the Hive table is created and loaded, as shown in figure 2.23
The HDFS directory that the Sqoop MapReduce job writes to won’t exist after the import.
Just like with the fast connector, this option isn’t compatible with the --as-avrodatafile and --as-sequencefile options.
If the Hive table already exists the data will be appended to the existing table.
If this isn’t the desired behavior, you can use the --hive-overwrite argument to indicate that the existing table should be replaced with the imported data.
The following example shows how to use the --hiveoverwrite in conjunction with enabling LZOP compression.
For this to work you’ll need to have built and installed LZOP on your cluster, since it isn’t bundled with Hadoop (or CDH) by default.
For example, if you want to partition your input by date, you would do the following:
Ideally, a single import would be able to create multiple Hive partitions.
Because you’re limited to specifying a single key and value, you’d need to run the import once per unique partition value, which is laborious.
You’d be better off importing into a nonpartitioned Hive table, and then retroactively creating partitions on the table after it had been loaded.
Also, the SQL query that you supply to Sqoop must also take care of filtering out the results, such that only those that match the partition are included.
In other words, it would have been useful if Sqoop would have updated the WHERE clause with symbol = "AAPL" rather than having to do this yourself.
Summary Obviously, for Sqoop to work your Hadoop cluster nodes need to have access to the MySQL database.
Common sources of error are either misconfiguration or lack of connectivity from the Hadoop nodes.
It’s probably wise to log on to one of the Hadoop nodes and attempt to connect to the MySQL server using the MySQL client, and/or attempt access with the mysqldump utility (if using a fast connector)
Another important note when using a fast connector is that it’s assumed that mysqldump is installed on each Hadoop node, and is in the PATH of the user running the map tasks.
Our final foray into the area of moving data into Hadoop is a look at HBase.
HBase is a real-time, column-oriented database, and is often either co-located on the same hardware that serves as your Hadoop cluster, or is in close proximity to a Hadoop cluster.
Being able to work with HBase data directly in MapReduce, or push it into HDFS, is one of the huge advantages when picking HBase as a solution.
I’ll present two techniques in this section, the first focusing on how to import HBase data into HDFS, and the second on how to use HBase as a data source for a MapReduce job.
In this first technique I’ll show you how to use a tool that HBase is bundled with to save an HBase table into HDFS.
What if you had some customer data sitting in HBase that you wanted to leverage in MapReduce in conjunction with data in HDFS? You could write a MapReduce job which takes as input the HDFS dataset and pulls data directly from HBase in your map or reduce code.
But in some cases it may be more useful to take a dump of the data in HBase into HDFS directly, especially if you plan to utilize that data in multiple MapReduce jobs and the HBase data is immutable, or changes infrequently.
Problem How do you get HBase data into HDFS? Solution HBase includes an Export class that can be used to import HBase data into HDFS in SequenceFile format.
This technique also walks through code that can be used to read the imported HBase data.
Discussion Before we get started with this technique you first need to get HBase up and running.13
To be able to import data from HBase you also need to load some data into HBase.
The loader I wrote creates an HBase table called stocks_example with a single column family, details.
Let’s run the loader and use it to load the sample stock data into HBase:
You can use the HBase shell to look at the results of the load.
The list command, without any arguments, will show you all of the tables in HBase, and the scan command, with a single argument, will dump all of the contents of a table:
Appendix A contains installation instructions and additional resources for working with HBase.
Set th this ca fit her interna and th doesn’t $ hbase shell.
With your data in place you’re ready to export it to HDFS.
An example use of the Export class is shown in the following snippet, where you’re exporting the whole HBase table:
The Export class also supports exporting only a single column family, and can also compress the following output:
What if you want to process that exported data in HDFS? The following code shows an example of how you would read the HBase SequenceFile and extract the Avro stock records.14
Snappy’s a good e since the SequenceFile lly applies the compression, e compressed content need to be split.
You can run the code against the HDFS directory that you used for the export and view the results:
Summary Exporting data from HBase into HDFS is made easier with the built-in HBase Export class.
But what if you don’t want to write HBase data into HDFS, but instead want to process it directly in a MapReduce job? Let’s look at how to use HBase as a data source for a MapReduce job.
The built-in HBase exporter writes out HBase data using SequenceFile, which isn’t supported by programming languages other than Java, and doesn’t support schema evolution.
It also only supports a Hadoop filesystem as the data sink.
Decode the byte array contents of the HBase column family/qualifier value into your Avro Stock bean.
Cre HBa con crit spec qua more control over HBase data extracts, you may have to look beyond the built-in HBase facilities.
Problem What if you want to operate on HBase directly within your MapReduce jobs without an intermediary step of copying the data into HDFS? Solution HBase has a TableInputFormat class that can be used in your MapReduce job to pull data directly from HBase.
You will use this InputFormat to write Avro files in HDFS.
Discussion The following listing shows a MapReduce job that uses TableInputFormat to read data from HBase.
This listing writes the data into HDFS in Avro form.15
Iterate through all values for the rowkey and scan criteria (defined in the main method)
Extract the Avro objec from the column valuOutput the stock symbol.
In this case you’re ifying the column family and lifier that you want to scan.
A quick peek in HDFS should tell you whether or not your MapReduce job worked as expected:
This output confirms that the MapReduce job works as expected.
Summary The TableInputFormat class examines HBase and creates an input split for each HBase table region.
If there are ten HBase regions, ten map tasks will execute.
It also includes the server that hosts the region in the input split, which means that the map tasks will be scheduled to execute on the same nodes as the HRegionServer hosting the data.
This gives you locality at the HBase level and at the HDFS level.
Data being read from the region will likely be coming from local disk, since after some time all of a region’s data will be local to it.
This all assumes that the HRegionServers are running on the same hosts as the DataNodes.
That concludes our examination of how to move data into Hadoop.
We covered a broad area of data types, including log data, binary data, and relational data.
We also looked at tools that help to automate data ingress in production environments.
With data ingress techniques covered, we’ll switch to the subject of moving data out of Hadoop.
The HBase table name that’s the data source for your job.
After data has been brought into Hadoop it will likely be joined with other datasets to produce some results.
At this point either that result data will stay in HDFS for future access, or it will be pushed out of Hadoop.
An example of this scenario would be one where you pulled some data from an OLTP database, performed some machine learning activities on that data, and then copied the results back into the OLTP database for use by your production systems.
In this section we’ll cover how to automate moving regular files from HDFS to a local filesystem.
We’ll also look at data egress to relational databases and HBase.
To start off we’ll look at how to copy data out of Hadoop using the HDFS Slurper.
In section 2.2.2, we looked at two mechanisms to move semistructured and binary data into HDFS, the HDFS File Slurper open source project, and Oozie to trigger a data ingress workflow.
The challenge to using a local filesystem for egress (and ingress for that matter) is that map and reduce tasks running on clusters won’t have access to the filesystem on a specific server.
You need to leverage one of the following three broad options for moving data from HDFS to a filesystem:
Host a proxy tier on a server, such as a web server, which you would then write to using MapReduce.
Write to the local filesystem in MapReduce and then as a postprocessing step trigger a script on the remote server to move that data.
Run a process on the remote server to pull data from HDFS directly.
The third option is the preferred approach because it’s the simplest and most efficient, and as such is the focus of this section.
We’ll look at how you can use the HDFS Slurper to automatically move files from HDFS out to a local filesystem.
Let’s say you have files being written in HDFS by MapReduce and you want to automate their extraction to a local filesystem.
This kind of feature isn’t supported by any Hadoop tools, so you have to look elsewhere for this task.
Problem How do you automate moving files from HDFS to a local filesystem? Solution The HDFS File Slurper can also be used to copy files from HDFS to a local filesystem.
This technique covers how to configure and run the HDFS File Slurper.
Discussion The goal here is to use the HDFS File Slurper project16 to assist with the automation.
We covered the HDFS File Slurper in detail in section 2.2.2—please read that section before continuing with this technique.
The HDFS Slurper also supports moving data from HDFS out to a local directory.
All you need to do is flip around the source and destination directories, as you can see from the following subsection of the Slurper’s configuration file:
You’ll notice that not only is the source directory in HDFS, but also the work, complete, and error directories are there.
The reason for this is that you need to be able to atomically move files between directories without incurring the expensive overhead of copying the files across filesystems.
Summary At this point you may wonder how you can trigger the Slurper to copy a directory that was just written with a MapReduce job.
When a MapReduce job completes successfully it creates a file called _SUCCESS in the job output directory.
This would seem like the perfect trigger to kick off an egress process to copy that content to a local filesystem.
As it turns out Oozie has a mechanism that can trigger a workflow when it detects these Hadoop “success” files, but again the challenge here is that any work performed by Oozie is performed in MapReduce, and therefore it can’t be used to perform the transfer directly.
You could write your own script, which polls HDFS for completed directories and then triggers a file copy process.
That file copy process could be the Slurper, or a simple hadoop fs -get command, if the source files need to be kept intact.
In the next topic we’ll look at writing data from Hadoop out to relational databases.
Databases are usually the target of Hadoop data egress in one of two circumstances: either when you move data back into production databases to be used by production systems, or when you move data into OLAP databases to perform business intelligence and analytics functions.
In this section we’ll use Apache Sqoop to export data from Hadoop to a MySQL database.
Sqoop is a tool that simplifies database imports and exports.
We’ll walk through the process of exporting data from HDFS to Sqoop.
We’ll also cover methods using the regular connector, as well as how to do bulk imports using the fast connector.
Hadoop excels at performing operations at scales which defeat most relational databases, so it’s common to extract OLTP data into HDFS, perform some analysis, and then export it back out to a database.
Problem How do you write data to relational databases, and at the same time ensure that writes are idempotent? Solution This technique covers how Sqoop can be used to export text files to a relational database, and also looks at how it can be configured to work with files with custom field and record delimiters.
We also cover idempotent exports to make sure that failed exports don’t leave your database in an inconsistent state.
Discussion This technique assumes you’ve already followed the instructions in technique 4 to install MySQL and create the schema.
Sqoop exports require that the database table you’re exporting into already exists.
Sqoop can support both inserts and updates of rows in the table.
Exporting data to a database shares many of the arguments that we examined in the import section.
The differences are that exports require the --export-dir argument to determine the HDFS directory to export.
You’ll also create another options file for exports to keep from insecurely supplying the password on the command line:
The first step will be to export data from MySQL to HDFS to ensure you have a good starting point, as seen in the following commands:
The result of the Sqoop import is a number of CSV files in HDFS, as you can see in the following code:
For the Sqoop export from HDFS to MySQL, you’ll specify that the target table should be stocks_export and that it should export data in the HDFS directory stocks:
By default, Sqoop exports will perform an INSERT into the target database table.
A value of updateonly means that if there’s no matching key, the updates will fail.
A value of allowinsert results in an insert if a matching key doesn’t exist.
The table column name that’s used to perform the update is supplied in the --update-key argument.
The following example indicates that only an update should be attempted, using the primary key for the update:
Several options are available that you can use to override the default Sqoop settings, which can be used to parse the input data.
The Sqoop map tasks that perform the exports use multiple transactions for their database writes.
If a Sqoop export MapReduce job fails, your table could contain partial writes.
For idempotent database writes Sqoop can be instructed to perform the MapReduce writes to a staging table.
For example, in CSV it’s common for fields to be enclosed by double quotes only when they contain commas.
You can see the sequence of events in figure 2.24
You used the fast connector in the import technique, which was an optimization that used the mysqldump utility.
Sqoop exports also support using the fast connector, which uses the mysqlimport tool.
All of the nodes in your cluster need to have mysqlimport installed and available in the PATH of the user that’s used to run MapReduce tasks.
And as with the import, the --direct argument enables utilization of the fast connectors:
Sqoop doesn’t support using fast connectors in conjunction with a staging table, which is how idempotent writes are achieved with regular connectors.
But it’s still possible to achieve idempotent writes with fast connectors with a little extra work at your end.
You need to use the fast connector to write to a staging table, and then trigger.
This breaks the earlier rule about exposing credentials on the command line, but it’s easy to write a wrapper script that can read these settings from a configuration file.
Summary Sqoop provides a more simplified usage model compared to using the DBInputFormat format classes that are provided in MapReduce.
But using the DBInputFormat classes will give you the added flexibility to transform or preprocess your data in the same MapReduce job that performs the database export.
The advantage of Sqoop is that it doesn’t require you to write any code, and has some useful notions, such as staging, to help you achieve your idempotent goals.
The final step in this section and in the chapter is to look at HBase ingress and egress.
In section 2.2.4 we looked at how to import data from HBase into HDFS, as well as how to use HBase as a data source for a MapReduce job.
In this section we’ll do the reverse, looking at how to bulk load data from HDFS into HBase, as well as use HBase as a data sink in a MapReduce job.
In this technique we’ll look at how to write a MapReduce job to pull data from HBase and write it back into HBase.
Let’s say that you serve some real-time data from your website from HBase, and use Hadoop in your back office for data analytics.
You periodically join your data in HBase with some other data sources (such as log files), perform some analytics, and then you want to push the results back into HBase.
You’ll first need to create a table for the import using the HBase shell:
You’ll import the same data that you exported from HBase in section 2.2.4
In that section you discovered that the built-in HBase export process wrote SequenceFiles into HDFS, and similarly, the HBase import tool requires that the HDFS data being imported into HBase be in the same format.
The following snippet shows the command for the HBase import:
You can use the HBase shell to verify that the write was successful:
Summary This technique is a useful way to load data without having to write any code, but you must have your data in SequenceFile form, which has disadvantages, including no support for schema evolution.
Being able to write to HBase from your MapReduce jobs means you’ll need to write code, but in doing so you aren’t tied to having your data in a specific file format, as we’ll examine next.
Imagine you’re working with HDFS data that you want to load into HBase.
That data is most likely not in the SequenceFile format required by the HBase Import class, and generating data in that format requires adding an additional step to your MapReduce workflow code.
Instead, it’s more convenient to write directly from MapReduce into HBase.
Problem How can you write to HBase directly from your MapReduce jobs? Solution MapReduce can be used to export HDFS data to HBase with the HBase TableOutputFormat class.
Discussion HBase provides a TableOutputFormat class that uses HBase as a data sink in MapReduce.
You’ll read in regular text data in the mapper and convert them into Put classes that the OutputFormat uses to write to HBase, as shown next.17
Create a new Put object and specify the rowkey in the constructor.
The key is ignored by the HBase TableOutputFormat, but it’s useful to distribute the keys across all the reduces that will perform the HBase writes.
As before, HBase should tell you whether or not your MapReduce job worked as expected:
Summary MapReduce is a great way to move data into HBase.
Unlike when exporting data into databases where you’ll need to limit the number of MapReduce tasks that write in parallel, with HBase your write throughput can be much higher, as highly parallel writes is one of HBase’s primary architectural features.
When you perform highly parallelized writes to HBase, however, you should be aware that they can lead to a phenomenon called Region Hotspotting, where all the writes are sent to a single Region server.
This can happen in cases where you’re writing row keys that are similar to each other, such as timeseries data, which will likely be routed to a single Region server.
A common best practice to avoid this includes hashing the keys so that the keys are randomly distributed across the key space.
We covered how you can use the HDFS Slurper to move data out to a filesystem, how to use Sqoop for idempotent writes to relational databases, and we wrapped up this section with a look at ways to egress Hadoop data into HBase.
Moving data in and out of Hadoop is a critical part of the Hadoop architecture.
In this chapter we covered a broad spectrum of techniques that you can use to automate data ingress and egress, and that work with a variety of data sources.
Hadoop has a number of built-in mechanisms that can facilitate ingress and egress operations, such as the embedded NameNode HTTP server, and the upcoming.
Use HBase’s helper method t set up the reduce configuratio.
But these are low-level mechanisms, and they don’t provide complete systems to manage the entire ingress and egress process.
This chapter focused on higher-level tools that you can easily integrate and automate.
We looked at how to use Oozie to periodically schedule a workflow that used MapReduce to pull data from a web server.
We compared and contrasted various log collection frameworks and showcased how Flume could be used to push data into HDFS and Hive.
We also looked at how Sqoop could be used for relational database ingress and egress, and wrapped up with a look at HBase imports and exports.
MapReduce’s model for computational parallelism offers its own unique challenges when working with various file formats, so the focus in the next chapter will be on how to work with common file formats such as XML and JSON, as well as helping you pick file formats better suited for life in Hadoop, such as SequenceFiles and Avro.
MapReduce offers straightforward, well-documented support for working with simple data formats such as log files.
But the use of MapReduce has evolved beyond log files to more sophisticated data serialization formats—such as text, XML, and JSON—to the point that its documentation and built-in support runs dry.
The goal of this chapter is to document how to work with common data serialization formats, as well as to examine more structured serialization formats and compare their fitness for use with MapReduce.
Imagine that you want to work with the ubiquitous data serialization formats XML and JSON.
These formats work in a straightforward manner in most programming languages, with several tools available to help you with marshalling, unmarshalling, and validating where applicable.
Working with XML and JSON in MapReduce, however, poses two equally important challenges.
First, though MapReduce requires classes that can support reading and writing a particular data serialization format, there’s a good chance it doesn’t have such classes to support the serialization format you’re working with.
Second, MapReduce’s power lies in its ability to parallelize reading your input data.
If your input files are large (think hundreds of megabytes or more), it’s crucial that the classes reading your serialization format be able to split your large files so multiple map tasks can read them in parallel.
We’ll start this chapter by tackling the problem of how to work with serialization formats such as XML and JSON.
Then we’ll compare and contrast data serialization formats that are better suited to working with big data.
I’ll also show how to use these serialization formats with MapReduce.
The final hurdle is when you need to work with a file format that’s proprietary, or a less common file format for which no read/write bindings exist in MapReduce.
I’ll show you how to write your own classes to read/ write your file format.
Data serialization support in MapReduce is a property of the input and output classes that read and write MapReduce data.
Let’s start with an overview of how MapReduce supports data input and output.
This chapter assumes you’re familiar with XML and JSON data formats.
Wikipedia provides some good background articles on XML and JSON, if needed.
You should also have some experience writing MapReduce programs and understand the basic concepts of HDFS and MapReduce input and output.
Chuck Lam’s book, Hadoop in Action from Manning, represents a good resource on this topic.
Your data might be XML files sitting behind a number of FTP servers, text log files sitting on a central web server, or Lucene indexes1 in HDFS.
How does MapReduce support reading and writing to these different serialization structures across the various storage mechanisms? You’ll need to know the answer in order to support a specific serialization format.
Figure 3.1 shows the high-level data flows through MapReduce and identifies the actors responsible for various parts of the flow.
On the input side you see that some work (Create split) is performed outside of the map phase, and other work is performed as part of the map phase (Read split)
All of the output work is performed in the reduce phase (Write output)
Apache Lucene is an information retrieval project that stores data in an inverted index data structure optimized for full-text search.
Figure 3.2 shows the same flow with a map-only job.
In a map-only job the MapReduce framework still uses the OutputFormat and RecordWriter classes to write the outputs directly to the data sink.
Let’s walk through the data flow and describe the responsibilities of the various actors.
As we do this, we’ll also look at the relevant code from the built-in TextInputFormat and TextOutputFormat classes to better understand the concepts.
The TextInputFormat and TextOutputFormat classes read and write line-oriented text files.
The two classes that support data input in MapReduce are InputFormat and RecordReader.
The InputFormat class is consulted to determine how the input data should be.
The InputFormat and RecordReader are responsible for determining what data to feed into the map function.
The map and reduce functions are typically written by the user to.
The RecordWriter writes the reduce output to the destination data sink, which is the final resting place of this MapReduce.
The partitioner's job is to logically funnel map outputs to the.
Every job in MapReduce must define its inputs according to contracts specified in the InputFormat abstract class.
InputFormat implementers must fulfill three contracts: first, they describe type information for map input keys and values; next, they specify how the input data should be partitioned; and finally, they indicate the RecordReader instance that should read the data from source.
Figure 3.3 shows the InputFormat class and how these three contracts are defined.
Arguably the most crucial contract is that of determining how to divide the input data.
In MapReduce nomenclature these divisions are referred to as input splits.
The input splits directly impact the map parallelism because each split is processed by a single map task.
Working with an InputFormat that is unable to create multiple input splits over a single data source (such as a file) will result in a slow map phase because the file will be processed sequentially.
The TextInputFormat class (view source at http://goo.gl/VOMcJ) provides an implementation of the InputFormat class’s createRecordReader method but delegates the calculation of input splits to its parent class, FileInputFormat.
The following code shows the relevant parts of the TextInputFormat class:
Create a RecordReader to read data from the job inputs.
Figure 3.3 The annotated InputFormat class and its three contracts.
The default record delimiter is newline, but it can be overridden with.
The code in FileInputFormat (source at http://goo.gl/mQfq1) to determine the input splits is a little more complicated.
A simplified form of the code is shown in the following example to portray the main elements of this method:
The following code is an example of how you specify the InputFormat to use for a MapReduce job:
The RecordReader class is used by MapReduce in the map tasks to read data from an input split and provide each record in the form of a key/value pair for use by mappers.
A task is commonly created for each input split, and each task has a single RecordReader that’s responsible for reading the data for that input split.
As shown in a previous section, the TextInputFormat class created a LineRecordReader to read records from the input splits.
The LineRecordReader directly extends the RecordReader class and leverages the LineReader class to read lines from the input split.
The LineRecordReader uses the byte offset in the file for the map key and the contents of the line for the map value.
I’ve included a simplified version of the LineRecordReader in the following example (source at http://goo.gl/iIS59):
The size of the splits is the same as the block size for the file.
Create a split for each file block and add it to the result.
Because the LineReader class is easy, we’ll skip that code.
The next step will be a look at how MapReduce supports data outputs.
Type definitions for map input keys and values.seeking into a file and.
Figure 3.4 The annotated RecordReader class and its abstract methods.
Open an InputStream to the input split file.Seek to the start.
If you aren’t at the start of the file, you need to figure ou where to start reading the lines.
The only way to do this is t keep reading characters until you hit a newline, at which poin.
MapReduce uses a similar process for supporting output data as it does for input data.
The OutputFormat performs some basic validation of the data sink properties, and the RecordWriter writes each reducer output to the data sink.
Much like the InputFormat class, the OutputFormat class, as shown in figure 3.5, defines the contracts that implementers must fulfill, including checking the information related to the job output, providing a RecordWriter, and specifying an output committer, which allows writes to be staged and then made “permanent” upon task and/or job success.
Please refer to chapter 5 for more details on output committing.
Just like the TextInputFormat, the TextOutputFormat also extends a base class, FileOutputFormat, which takes care of some complicated logistics such as output committing, which we’ll cover further in this chapter.
For now let’s take a look at the work the TextOutputFormat is performing (source at http://goo.gl/8ab7Z):
The following code is an example of how you specify the OutputFormat that should be used for a MapReduce job:
You’ll use the RecordWriter to write the reducer outputs to the destination data sink.
The TextOutputFormat returned a LineRecordWriter object (LineRecordWriter is an inner class of TextOutputFormat) to perform the writing to file.
A simplified version of that class (source at http://goo.gl/8ab7Z) is shown in the following example:
While on the map side it’s the InputFormat that determines how many map tasks are executed, on the reducer side the number of tasks is solely based on the value for.
Now that you know what’s involved in working with input and output data in MapReduce, it’s time to apply that knowledge to solving some common data serialization problems.
Your first step in this data serialization journey is to learn how to work with file formats such as XML.
Their ubiquity in the technology industry is evidenced by their heavy adoption in data storage and exchange.
It became a universal language for data exchange between systems.
It’s employed by many standards today such as SOAP and RSS, and used as an open data format for products such as Microsoft Office.
While MapReduce comes bundled with an InputFormat that works with text, it doesn’t come with one that supports XML.
Working on a single XML file in parallel in MapReduce is tricky because XML doesn’t contain a synchronization marker in its data format.
Problem You want to work with large XML files in MapReduce and be able to split and process them in parallel.
Solution Mahout’s XMLInputFormat can be used to work with XML files in HDFS with MapReduce.
It reads records that are delimited by a specific XML begin and end tag.
This technique also covers how XML can be emitted as output in MapReduce output.
Discussion MapReduce doesn’t contain built-in support for XML, so we’ll turn to another Apache project, Mahout, a machine learning system, to provide an XML InputFormat.
To showcase the XML InputFormat, let’s write a MapReduce job that uses Mahout’s XML InputFormat to read property names and values from Hadoop’s configuration files.
The first step will be to set up the job configuration:
Yo job is taking Hadoop config files as input, where ea.
Looking at the previous code, it quickly becomes apparent that Mahout’s XML Input-Format is rudimentary; you need to tell it an exact sequence of start and end XML tags that will be searched in the file.
Next you need to write a mapper to consume Mahout’s XML InputFormat.
The XML element in Text form has been supplied, so you’ll need to use an XML parser to extract content from the XML.
The map is given a Text instance, which contains a String representation of the data between the start and end tags.
In this code you use Java’s built-in Streaming API for XML (StAX) parser to extract the key and value for each property and output them.
If you run the MapReduce job against Cloudera’s core-site.xml and use the HDFS cat command to show the output, you’ll see the following output:
This output shows that you’ve successfully worked with XML as an input serialization format with MapReduce.
Not only that—you can support huge XML files since the InputFormat supports splitting XML.
Having successfully read XML, the next question is how do you write XML? In your reducer you have callbacks that occur before and after your main reduce method is called, which you can use to emit a start and end tag, as shown in the following example.3
This could also be embedded in an OutputFormat, but I’ll leave that as an exercise for the reader.
If you want to work with XML in Pig, the Piggybank library (a user-contributed library of useful Pig code, detailed in chapter 10) contains an XMLLoader.
It works in a way similar to this technique and captures all of the content between a start and end tag, supplying it as a single byte array field in a Pig tuple.
Currently, no means exists for working with XML in Hive.
Use the setup method to write the root element start tag.
Use the cleanup method to write the root element end tag.
Construct a child XML element for each key/value combination provided.
But it’s sensitive to an exact string match of both the start and end element names.
If the element tag can contain attributes with variable values, or the generation of the element can’t be controlled and could result in XML namespace qualifiers being used, then this approach may not work for you.
Also problematic will be situations where the element name you specify is used as a descendant child element.
If you have control over the XML laid out in the input, this exercise can be simplified by having a single XML element per line.
This will let you use the built-in MapReduce text-based InputFormats (such as TextInputFormat), which treat each line as a record and split accordingly to preserve that demarcation.
Another option worth considering is that of a preprocessing step, where you could convert the original XML into a separate line per XML element, or convert it into an altogether different data format such as a SequenceFile or Avro, both of which solve the splitting problem for you.
Now that you have a handle on how to work with XML, let’s tackle another popular serialization format, JSON.
It’s less verbose than XML, and doesn’t have the rich typing and validation features available in XML.
Imagine you have some code that’s downloading JSON data from a streaming REST service and every hour writes a file into HDFS.
The data amount that’s being downloaded is large, so each file being produced is multiple gigabytes in size.
You’ve been asked to write a MapReduce job that can take as input these large JSON files.
What you have here is a problem in two parts: first, MapReduce doesn’t come with an InputFormat that works with JSON.
Second, how does one even go about splitting JSON? Figure 3.7 shows the problem with splitting JSON.
To split files, given a random offset in a file, you’ll need to be able to determine the start of the next JSON element.
This is made more challenging when working with JSON because it’s a hierarchical data format and the same element name can be used in multiple levels, as shown in the figure.
Problem You want to work with JSON inputs in MapReduce and also ensure that input JSON files can be partitioned for concurrent reads.
Solution The Elephant Bird LzoJsonInputFormat input format is used as a basis to create an input format class to work with JSON elements.
This technique also covers another approach using my open source project that can work with multiline JSON.
Discussion Elephant Bird,5 an open source project that contains some useful utilities for working with LZOP compression, has an LzoJsonInputFormat that can read JSON, though it requires that the input file be LZOP-compressed.
You’ll use the Elephant Bird code as a template for your own JSON InputFormat, which doesn’t have the LZOP compression requirement.
We’re cheating with this solution, which assumes that each JSON record is on a separate line.
My JsonInputFormat is simple and does nothing other than construct and return a JsonRecordReader, so we’ll skip over that code.
The JsonRecordReader emits LongWritable, MapWritable key/value pairs to the mapper, where the MapWritable is a map of JSON element names and their values.
It leverages the LineRecordReader, which is a built-in MapReduce reader that emits a record for each line.
To convert the line to a MapWritable, the reader uses the following method:6
The reader uses the json-simple7 parser to parse the line into a JSON object, and then iterates over the keys and puts the keys and values into a MapWritable.
The mapper is given the JSON data in LongWritable, MapWritable pairs and can process the data accordingly.
You can view this basic code for the MapReduce job in the GitHub repository.
Because this technique assumes a JSON object per line, the following shows the JSON file you’ll work with:
Now copy the JSON file into HDFS and run your MapReduce code.
The MapReduce code writes each JSON key/value as the job output:
An approach similar to what we looked at in section 3.2.1 for writing XML could also be used to write JSON.
Elephant Bird contains a JsonLoader and an LzoJsonLoader, which you can use to work with JSON in Pig.
Each Pig tuple contains a chararray field for each JSON element in the line.
Summary This solution assumes that the JSON input is structured with a line per JSON object.
How would you work with JSON objects that are across multiple lines? An experimental project on GitHub8 works with multiple input splits over a single JSON file.
This approach searches for a specific JSON member and retrieves the containing object.
You can also review a Google Code project called hive-json-serde,9 which can support both serialization and deserialization.
As you can see, using XML and JSON in MapReduce is kludgy and has rigid requirements about how to lay out your data.
Support for these two formats in MapReduce is also complex and error prone, since neither lends itself naturally to splitting.
Clearly you need to look at alternative file formats that have built-in support for splittability.
The next step is to compare more sophisticated file formats, which are better suited to working with MapReduce, such as Avro and SequenceFiles.
Unstructured text works well when you’re working with scalar or tabular data.
Semistructured text formats such as XML and JSON can model more sophisticated data structures that include composite fields, or hierarchical data.
But when you’re working with big data volumes you’ll need serialization formats with compact serialized forms that natively support partitioning and have schema evolution features.
In this section we’ll compare the serialization formats that work best with big data in MapReduce, and follow up with how you can use them with MapReduce.
It’s important to make certain considerations when choosing a file format.
I’ve selected the following criteria based on my belief that these are the important characteristics for big data serialization:
Code generation—The ability to generate Java classes and utilities that can be used for serialization and deserialization.
Versioning—The ability for the file format to support backward or forward compatibility.
Table 3.1 compares three data serialization frameworks to see how they stack up against each other.
Additional background on these technologies is provided in the succeeding section.
Now let’s look at each of these formats in more detail.
The SequenceFile format was created to work with MapReduce, Pig, and Hive, and therefore integrates well with all of those tools.
Its shortcomings are mainly its lack of code generation and versioning support, as well as limited language support.
The Protocol Buffers format has been used heavily by Google for interoperability.
Its strengths are its versioning support and compact binary format.
Downsides include its lack of support in MapReduce (or in any third-party software) for reading files generated by Protocol Buffers serialization.
Not all is lost, however; we’ll look at how Elephant Bird uses Protocol Buffers serialization within a higher-level container file in section 3.3.3
Thrift was developed at Facebook as a data serialization and RPC framework.
It doesn’t have support in MapReduce for its native data serialization format, though it can support different wire-level data representations, including JSON and various binary encodings.
Thrift also includes an RPC layer with various types of servers, including a nonblocking implementation.
We’ll ignore the RPC capabilities for this chapter and focus on the data serialization.
The Avro format was Doug Cutting’s creation to help address the shortcomings of SequenceFiles.
Based on certain evaluation criteria, Avro seems to be the best fit as a data serialization framework in Hadoop.
SequenceFile is a close second, due to its inherent compatibility with Hadoop (it was designed for use with Hadoop)
It contains benchmarks for Avro, Protocol Buffers, and Thrift, along with a number of other frameworks.
After looking at how the various data serialization frameworks compare, we’ll dedicate the next few sections to showing you how you can work with them.
Because SequenceFiles were created for use with MapReduce, arguably they offer the highest level of integration support in conjunction with MapReduce, Pig, and Hive.
SequenceFiles are a splittable binary file format that stores data in the form of key/value pairs.
All SequenceFiles share the same header format, as shown in figure 3.8
SequenceFiles come in three types, which vary based on how you apply compression.
In addition, each type has its own corresponding Writer classes.
No advantage exists for this over the compressed formats, since compression generally reduces your storage footprint and is more efficient for reads and writes.
When a record is added to the SequenceFile, it’s immediately compressed and written to the file.
The disadvantage to this approach is that your compression ratio will suffer compared to block compression.
This file format is shown, along with uncompressed SequenceFiles, in figure 3.9
By default the block size is the same as the HDFS block size, although this can be overridden.
The advantage to this compression is that it’s more aggressive; the whole block is compressed, rather than at the record level.
Data isn’t written until it reaches the block size, at which point the whole block is compressed, resulting in good overall compression.
Hadoop comes with four serializers: Avro, Java, Tether (for binary data contained within a TetherData class), and Writable10 (the default serializer)
Writable is an interface in Hadoop used to support general-purpose data serialization, and is used for sending data across the wire between Hadoop components.
If you want your SequenceFile to contain objects that aren’t Writable or Serializable, you’ll need to implement your own Serializer and register it.
Now let’s look at how to use SequenceFiles in MapReduce.
Working with text in MapReduce can start to get tricky when you have to support complex types of data, which may include nonscalar data types such as lists or dictionaries.
Dealing directly with large text files also means that you have to manage compression yourself, which can be a burden in MapReduce.
Problem You want to work with a structured file format in MapReduce that you can use to model complex data structures, and that also supports compression and splittable inputs.
Solution This technique looks at how the SequenceFile file format can be leveraged from both standalone applications as well as MapReduce.
Discussion The SequenceFile format offers a high level of integration with computational tools such as MapReduce, and can also model complex data structures.
We’ll examine how to read and write SequenceFiles, and also use them with MapReduce, Pig, and Hive.
The most common serialization method used with SequenceFiles are Writable, so we’ll create a Writable to represent the stock data.
String symbol; String date; double open; double high; double low; double close; int volume; double adjClose;
Write out the fields of this Writable in byte form to the.
Now that you have your Writable you’ll need to write some code that will create a SequenceFile.
You’ll read your stocks file from the local disk, create the StockWritable, and write it to your SequenceFile, using the stock symbol as your key:12
Read the fields from byte form into the Writable fields.
A helper method to engineer a StockPriceWritable from a CSV line.
Create a new SequenceF writer, specifying that you wa block-level compression.
Great, now how do you go about reading the files created with your writer?
Now you need to prove that it works by writing and reading a file:
Remember earlier in this chapter when we talked about how the default SequenceFile serialization supported Writable classes for serialization? Because Writable is the native data format in MapReduce, using SequenceFiles with MapReduce is totally transparent.
Create the StockPriceWritable instance, using the fromLine helper method in the.
Create a reader that can read records from the SequenceFile.
The next method on the reader returns true until it hits the end of the file.
Linux co reset yo terminal after se binary d your scr if you agree.
Now let’s run the identity MapReduce job against the stocks SequenceFile that you created earlier in this technique:
Because all it’s doing is echoing the input to the output, you should see identical content in both files.
Let’s make sure that’s the case by reading in the job output file(s)
First of all, how do you verify that the output is a SequenceFile? Easy, just cat it—the first 3 bytes should be SEQ, followed by a fourth byte containing the SequenceFile version, which is then followed by the key and value classes:
Now try using the SequenceFile reader code you wrote earlier to dump it to standard output:
An identity function is a mathematical term to denote a function that returns the same value that was used as its argument.
In MapReduce this means the same thing—the map identity function emits all the key/value pairs that it is supplied, as does the reducer, without any transformation or filtering.
A job that doesn’t explicitly set a map or reduce class results in Hadoop using a built-in identity function.
Set the compression codec that should be used; this case you’re using the default codec, which.
Because SequenceFiles are key/value based, and the default serialization data format for SequenceFiles is Writable, the use of SequenceFiles is 100 percent transparent to your map and reduce classes.
We demonstrated this by using MapReduce’s built-in identity map and reduce classes with the SequenceFile as input.
The only work you had to do was to tell MapReduce to use the SequenceFile specific Input and Output format classes, which are built into MapReduce.
By writing your own Writable you created more work for yourself with non-MapReduce tools such as Pig.
Pig works well with Hadoop’s built-in scalar Writables such as Text and IntWritable, but doesn’t have support for custom Writables.
This will work well with MapReduce, but Pig’s SequenceFileLoader won’t work with your custom Writable, which means that you’ll need to write your own Pig loader to process your files.
Let’s try to load and dump the stock SequenceFile in Pig:
Hive contains built-in support for SequenceFiles but has two restrictions.
Second, out of the box it only works with SequenceFile values that are Writable, and the way it supports them is to perform a toString() to convert the value into a Text form.
Appendix A and chapter 11 contain details on installing and using Pig.
Writable, so you have to write a Hive SerDe, which deserializes your Writable into a form Hive can understand.
Summary SequenceFiles are useful in that they solve two problems that make using MapReduce challenging; they’re natively splittable, and they also have built-in support for compression, which makes it transparent to the user.
The thorn in the side of SequenceFiles is their lack of support outside of Java, which limits what other tools you can use with them.
If your data mostly stays in HDFS, however, and is processed with MapReduce (or Hive/ Pig), SequenceFiles may be what you need.
Next up we’ll examine how to integrate Protocol Buffers into MapReduce.
Google developers invented Protocol Buffers to help them exchange data between services written in multiple languages in a compact and efficient manner.
Protocol Buffers offer a compact and fast serialization format for semistructured data.
You define the structure of your data in a .proto file and use Protocol Buffers’ code generation capabilities to generate classes to access your data.
The challenge to using Protocol Buffers in MapReduce is that currently no mechanism exists to work directly with files that are serialized with Protocol Buffers.17
Problem You want to be able to read and write data encoded with Protocol Buffers in MapReduce.
Solution Elephant Bird has a file format that can store Protocol Buffer encoded records.
Discussion We’ll cover how you can use Protocol Buffers in your MapReduce jobs.
We’ll go through the steps to define a schema, generate Java classes from the schema, and then use your classes in a MapReduce job.
The key to this solution is Elephant Bird,18 an open source project maintained by Twitter, which contains LZOP compression utilities.
However, there’s been little activity since the end of 2008 on this ticket.
To work with Protocol Buffers and Elephant Bird, you’ll need to download and install them as well as set up LZOP compression on your system.
Elephant Bird supports two encoding mechanisms for Protocol Buffers: lineoriented and block-based.
The second encoding option, block-based, as shown in figure 3.12, is an interesting format.
It accumulates in memory the byte form of the Protocol Buffer (PB) objects being written, and then after a certain number of PB objects have accumulated (by default 100), it writes a logical block marker, followed by the block size, and then uses its own Protocol Buffer code-generated block (SerializedBlock) to use PB’s serialization to write the PB object bytes to the stream.
Table 3.2 provides an overview of the classes related to each of the Protocol Buffers’ encodings available in Elephant Bird.
Everything in Protocol Buffers starts with a .proto file, which is where you define your “messages” (which are types containing a number of uniquely numbered fields)
These numbers are how Protocol Buffers identifies that particular field, and must never change for a given field.
The next block of code is an example of a .proto file definition for your stock data:20
When your .proto file is ready, it’s time to generate a Java-specific set of classes for your message types.
The --java_out option specifies where the Java sources should be generated:
Now you have a number of generated files to start playing with Protocol Buffers.
Next you’ll write a MapReduce job that will calculate the stock averages for each company.
Your MapReduce job will both consume and produce Protocol Buffer data.
You need to load the stocks sample file, and write an LZOP-compressed Protocol Buffers file, as the following code shows.21
In this example you’ll pass the Protocol Buffers Stock object between the map and reduce functions.
To do this you need to write your own class that extends ProtobufWritable, and emits that in your map task:
Use Elephant Bird to create a Writer that will write the block-encoded Protocol Buffers object to the LZOP stream.
Create a Protocol Buffers Stock object from each input line.
Itera Stoc the The map function emits the stock symbol and the entire Stock Protocol Buffers object as output.
Notice that the Protocol Buffers Stock object is being passed to the mapper wrapped in a ProtobufWritable object:22
The reducer function sums all of the prices for a stock symbol and emits the stock average:23
The input to the Map function is the byte offset of the record in the.
The map emits the stock symbol and the original Protocol Buffers.
The reduce input is the stock symbol and all of the related.
Build the Protocol Buffers StockAvg object using the Builder that was created as part of.
Indicate used th Bird ou format block-b Protoco encodin The final step is to set up the job configuration:
If you run this job it’ll read your input text file containing stock details, create an LZOP-compressed input file, and run the MapReduce file, which will take that as input and create a final output in the form of an LZOP-compressed block-based Protocol Buffers file:
You can dump the contents of the MapReduce output file using some reader code (source on GitHub at http://goo.gl/PEyUZ):
Elephant Bird also contains a loader that can be used to read the LZOP-compressed Protocol Buffers data contained within Elephant Bird’s serialization container.
Run the following command, and then copy the output string of the script prior to running Pig:
The map output key is the stock symbol, so use the Text type.tes that you he Elephant.
If you run the next example and don’t specify your input filename to have an .lzo suffix, it’ll be ignored by the MapReduce job.
Now launch Pig and dump the contents of the LZOP-compressed Protocol Buffers file to standard out:
Summary Elephant Bird contains some handy classes to help you work with Protocol Buffers.
As I mentioned earlier, there are no InputFormat classes that will let you work with files natively serialized with Protocol Buffers.
As you’ve seen, the way Elephant Bird enables you to work with Protocol Buffers is to introduce their own file format within which Protocol Buffers objects are serialized.
Using Elephant Bird’s classes means you have to use LZOP; however, it would be possible to derive a version of their classes and remove the LZOP dependency.
No progress has been made as yet in MapReduce to provide native support for Protocol Buffers.24 This would be ideal because it would allow users to work natively with Protocol Buffers serialized files.
Thrift is another data format, which, like Protocol Buffers, doesn’t have out-of-thebox support with MapReduce.
Again, Elephant Bird comes to the rescue for Thrift, which we’ll take a look at in the next section.
You need to use a $ character to specify the.
Facebook created Thrift to help with efficient data representation and transport.
Facebook uses Thrift for a number of applications, including search, logging, and its ads platform.
As with Protocol Buffers, no InputFormats exist that can work with files generated directly from Thrift’s data serialization library.
Problem You want to use Thrift for data serialization and its code-generated beans to work with your data in MapReduce.
Solution Elephant Bird has a file format that can store Thrift encoded records.
Discussion Similar to Protocol Buffers, Elephant Bird includes a Base64-encoded, line-based mechanism to serialize Thrift, as well as a block-based mechanism, which uses a Protocol Buffers generic container to write Thrift objects.
This is the same block format shown in Protocol Buffers in figure 3.12
Thrift’s schema has a lot in common with Protocol Buffers, which isn’t surprising given the number of Google engineers who’ve joined Facebook.
Your MapReduce code looks almost identical to the Protocol Buffers code, because Elephant Bird uses the same methods for both formats.
Other than that, the only change is in how you create the Thrift Stock object and write it into HDFS, which is as follows.26
Elephant Bird requires version 0.5 of Thrift, and it also has a dependency on LZOP compression.
All of these libraries contain installation instructions you can find in appendix A.
The ThriftBlockWriter will write out the Thrift object in serialized form into a Protocol Buffers.
If you run the MapReduce code and examine the output data for the job you’ll see the following:
Summary Once again Elephant Bird has made it possible for you to work with Thrift data serialization.
As with Protocol Buffers, your restriction in using Elephant Bird is that you’re forced to use LZOP compression.
But you should find it straightforward to copy and factor out the compression code if you don’t want to use it.
Another item worth noting is that Elephant Bird doesn’t supply Hive SerDe classes to allow you to work with Thrift in Hive.
Let’s look at what’s likely the most capable data serialization format of all our options, Avro.
Doug Cutting created Avro, a data serialization and RPC library, to help improve data interchange, interoperability, and versioning in MapReduce.
Avro utilizes a compact binary data format—which you have the option to compress—that results in fast serialization times.
While it has the concept of a schema, similar to Protocol Buffers, Avro improves on Protocol Buffers because it works natively with MapReduce.
As you can see in this figure, the schema is serialized as part of the header, which makes deserialization simple, and loosens restrictions around users having to use the schema in some form when interacting with Avro for deserialization.
Each data block contains a number of Avro records and by default is 16 KB in size.
With a firm understanding of Avro’s file format under your belt, let’s dive into the next technique.
The holy grail of data serialization supports code generation, versioning, and compression, and has a high level of integration with MapReduce.
Equally important is schema evolution, and that’s the reason why Hadoop SequenceFiles aren’t appealing—they don’t support the notion of a schema or any form of data evolution.
Avro fits the bill on all fronts, but the problem you may quickly run into with Avro is that it has almost no documentation to help users understand how to use it.
Problem You want to work with a data serialization library that works well with MapReduce and supports complex data types, code generation, and schema evolution.
Solution Avro code-generation and MapReduce integration are covered in this technique.
We also cover an examination of how Avro schema evolution works so that your code can support data serialized using different versions of your Avro schema.
Discussion You can use Avro in one of two ways: either with code-generated classes or with its generic classes.
Since you’ll take the code-generated route, everything starts with a schema.
You’ll continue to model the stock prices in this section, so the first step will be to create an Avro schema to represent an entry in the stock data:27
Avro compilers come in two flavors: they can either compile a protocol file or a schema file.
A protocol file supports both RPC messages and schema information, whereas a schema file can only support a single type.
Appendix A contains instructions on how to get your hands on Avro.
For this example you would run this (note that the second command generates the code for a different schema file, stockavg.avsc):
So how do you write Avro files from outside MapReduce?28
Create a writer that can write Avro’s data file format.
Specify that Snappy should be used to compress the data.ate the a that will.
As you see, you can specify the compression codec that should be used to compress the data.
In the example you’re using Snappy, which, as shown in chapter 5, is the fastest codec for reads and writes.
You also specified a larger data block size to show an example of how this can be done.
Let’s first generate the Avro file, and then read it back:
Loop through the Stock object and use the Apache Common.
ToStringBuilder to help dump al the members to the console.
Reads the stocks.txt file from the local filesystem and writes the Avro.
The big question is, does Avro play nicely withMapReduce? Avro comes with somemapper and reducer classes that you can subclass to work with Avro.
They’re useful in situations where you want your mappers and reducers to exchange Avro objects.
But if you don’t have a requirement to pass Avro objects between your map and reduce tasks, you’re better off using the Avro Input/Output Format classes directly, as you’ll see in the following code, which produces an average of all of the opening stock values.
Your job is to consume stock data and produce stock averages, both in Avro formats.
To do this you need to set the job configuration with the schema information for both schemas.
Your map function simply extracts the necessary fields from the stock record and emits them to the reducer, with the stock symbol and the opening stock price as the key/value pairs:31
The Avro InputFormat supplies the Avro objects wrapped in an.
Finally, the reduce function sums together all of the stock prices for each stock, and outputs an average price:32
Your MapReduce job is outputting a different Avro object (StockAvg) from the job input.
You can verify that the job produced the output you expected by writing some code (not listed) to dump your Avro objects:
If you want to use Avro’s own mapper and reducer classes, take a look at their Java33 docs  for examples of how to use them via the AvroJob helper class.
In the following code you load the stock data using AvroStorage and dump it out:
Currently, Hive doesn’t support Avro, but there’s an open ticket actively being worked on.35
Now that we’ve covered how Avro works with MapReduce, let’s take a look at one of Avro’s important features, schema evolution.
Avro’s backward and forward compatibility is supported by a number of features that help with schema evolution.
Imagine that you want to update the Stocks schema to be the following:
With this second version of the Stock schema Avro can support code generated from your original version reading an Avro file serialized with the new version (forward compatibility) as well as vice versa (backward compatibility)
Summary The previous technique demonstrated how easy and straightforward it is to use Avro with MapReduce and Pig.
The main advantage of using a data serialization format such as SequenceFile over Avro is that it has Hive support.
Using Avro to store your data gives you a number of useful, free features, such as versioning support, compression, splittability, and code generation.
It plays well with MapReduce and Pig, and I hope it will have Hive support soon, too.
We’ve covered working with common file formats, and working with various data serialization tools for tighter compatibility with MapReduce.
It’s time to look at how to support file formats that may be proprietary to your organization, or even public file formats for which no input or output formats exist to work with them in MapReduce.
In any organization you’ll typically find a plethora of custom or uncommon file formats that litter its data centers.
There may be backend servers dumping out audit files in a proprietary format or old code or systems that write files using formats that aren’t in common use any longer.
If you want to work with such data in MapReduce you’ll need to write your own input and output format classes to work with your data.
At the start of this chapter we took a high-level look at the functions of Input and Output Format classes in MapReduce.
Input and Output classes are required to feed data to map functions, and to write the outputs of reduce functions.
You renamed a field, but used Avro’s aliasing capabilities to preserve the old name for.
You added a new field, providing a default that will be used when.
Imagine you have a bunch of data sitting around in CSV files and you’re writing multiple MapReduce jobs that read and write data in CSV form.
Because CSV is text, you could use the built-in TextInputFormat and TextOutputFormat, and handle parsing the CSV in your MapReduce code.
However, this can quickly become tiring, and result in the same parsing code being copy-and-pasted across all of your jobs.
If you thought MapReduce had any built-in CSV input and output formats that could take care of this parsing, you’d be out of luck—there are none.
Problem You want to write an input and output format to work with CSV.
Solution You will see how CSV input and output formats can be crafted from scratch, and along the way understand key properties of these classes.
Discussion We’ll cover all of the steps required to write your own format classes to work with CSV input and output.
Your custom InputFormat and RecordReader classes will parse CSV files and supply the data to the mapper in a user-friendly format.
Because you don’t want to reinvent the wheel, you’ll leverage the CSV parser in the open source OpenCSV36 project, which will take care of quoted fields and ignoring separator characters in quoted fields.
The function of InputFormat is to validate the set of inputs supplied to the job, identify input splits, and create a RecordReader class to read input from the sources.
The following code37 reads the separator (if supplied) from the job configuration, and constructs a CSVRecordReader:
I provided a detailed overview of InputFormat and OutputFormat and their related classes at the start of this chapter that may be worth referencing prior to looking at the code in this technique.
You extended the FileInputFormat class, which contains code that calculates input splits along HDFS block boundaries, keeping you from having to handle calculating the input splits yourself.
The FileInputFormat manages all of the input files and splits for you.
Now let’s move on to the RecordReader, which will require a little more effort.
It must first open the input source based on the input split it is supplied, and optionally seek into a specific offset in that input split.
The second function of the RecordReader is to read individual records from the input source.
In your case a logical record equates to a line in your CSV file, so you’ll leverage the existing LineRecordReader class in MapReduce to handle working with the file.
When it’s initialized with the InputSplit, it will open the input file, seek to the start of the input split, and keep reading characters until it reaches the.
If the file is compressed, it’s not splittable; oth it is.
In the previous code you’ll see that when the input is compressed, a flag is returned to indicate that it couldn’t be split.
The reason for doing this is that compression codecs aren’t splittable, apart from LZOP.
But splittable LZOP can’t work with regular InputFormat classes—it needs special-case LZOP InputFormat classes to work with them.
The LineRecordReader returns key/value pairs for each line in LongWritable/Text form.
Because you’ll want to provide some functionality in the RecordReader, you need to encapsulate the LineRecordReader within your class.
The RecordReader needs to supply a key/value pair representation of each record to the mapper, and in this case the key is the byte offset in the file, and the value is an array containing the tokenized parts of the CSV line:38
Calculate the byte offse for the end of the inpu.
Create a LineReader, which the LineRecordR uses to read each line.
If your input split didn’t start at byte 0, read a line from your LineReader and discard it.
It will open the file specified in the InputSplit and seek to.
If the L supplied process t Next, you need to provide methods to read the next record and to get at the key and value for that record.39
At this point, you’ve created an InputFormat and a RecordReader that both can work with CSV files.
Now that you’ve completed the InputFormat, it’s time to move on to the OutputFormat.
Methods for reading the next record and retrieving the key and value of the record.
Proxy the request for the key to the LineRecordReader, which in turn returns the byte offset of the line in the file.
OutputFormat classes follow a similar patter to InputFormat classes, where the OutputFormat class handles the logistics around creating the output stream, and delegates the stream writes to the RecordWriter.
The CSVOutputFormat indirectly extends the FileOutputFormat class (via the TextOutputFormat), which handles all of the logistics related to creating the output filename, creating an instance of a compression codec (if compression was enabled), and output committing, which we’ll discuss shortly.
That leaves the OutputFormat class with the tasks of supporting a custom field delimiter for your CSV output file, and also creating a compressed OutputStream if required.
It must also return your CSVRecordWriter, which will write CSV lines to the output stream:40
Your RecordWriter must write each record emitted by the reducer to the output destination.
Define a configuration constant that users can specify a custo.
Read a custom separator from configuration, and if none exists.
The constructor, which only sets the field separator and the output stream, is excluded, as seen here.41
The write method is called for each record emitted by the reducer.
Now you need to apply the new input and output format classes in a MapReduce job.
Your MapReduce jobs will take CSV as input, and also produce CSV which is separated by colons, not commas.
It’ll perform identity map and reduce functions, which means that you won’t be changing the data as it passes through MapReduce.
Your input file will be delimited with the comma character, and your output file will be colonseparated.
Your input and output format classes support the notion of custom delimiters via Hadoop configuration properties.
The map and reduce functions don’t do much other than echo their inputs to output, but you’ll include them so you can see how to work with the CSV in your MapReduce code:43
If you run this input against a comma-delimited file, you can examine the mapper output and see if the results are as expected:
You now have a functional InputFormat and OutputFormat that can consume and produce CSV output in MapReduce.
Pig’s piggybank library contains a CSVLoader that can be used to load CSVs into tuples.
It supports double-quoted fields in the CSV and provides each item as a byte array.
There’s a GitHub project called csv-serde,44 which has a Hive SerDe that can both serialize and deserialize CSV.
As with the InputFormat example, it also uses the OpenCSV project for reading and writing the CSV.
Summary This technique demonstrated how you can write your own MapReduce format classes to work with text-based data.
Arguably, it would have been simpler to use the TextInputFormat and split the line in the mapper.
But if you need to do this multiple times you’re likely suffering from the copy-paste anti-pattern, since the same code to tokenize the CSV likely exists in multiple locations.
If the code is written with code reuse in mind, you’d be covered.
We’ve covered how you can write your own IO format classes to work with a custom file format in MapReduce.
Now we need to look at a crucial aspect of working with output formats—output committing.
In the CSV OutputFormat example earlier in this chapter, you extended FileOutputFormat, which takes care of “committing” output after the task has succeeded.
Why do you need “commits” in MapReduce, and why should you care?
As a job and its tasks are executing, at some point they will start writing job output.
Tasks and jobs can fail, they can be restarted, and they can also be speculatively executed.45 To allow OutputFormats to correctly handle these scenarios, MapReduce has the notion of an OutputCommitter, which is a mechanism by which MapReduce invokes a callback when an individual task as well as the overall job have completed.
This is great if your data sink is HDFS, where you can leverage FileOutputFormat and its committing mechanism.
Things start to get trickier when you’re working with data sources other than files, such as a database.
If in such cases idempotent writes (the same operation applied multiple times without changing the result) are necessary, you’ll need to factor that into the design of your destination data store or your OutputFormat.
This topic was examined in more detail in chapter 2, which covers exporting data from Hadoop to databases.
The goal for this chapter was to show you how to work with common file formats such as XML and JSON in MapReduce.
We also looked at more sophisticated file formats such as SequenceFiles, Avro, and Protocol Buffers, which provide useful features for working with big data, such as versioning, compression, and complex data structures.
We also walked you through the process of working with your own custom file formats to ensure they’ll work in MapReduce.
At this point you’re equipped to work with any file format in MapReduce.
Now that you understand how to effectively work with files in HDFS and MapReduce, the next step is to look at patterns to help you effectively work with your data.
Speculative execution is when MapReduce executes multiple tasks for the same input data, to guard against slow or misbehaving nodes slowing down the overall job.
By default both map-side and reduce-side speculative execution is enabled.
In chapter 4 we’ll examine techniques to optimize MapReduce operations, such as joining and sorting on large datasets.
These techniques make jobs run faster and allow for more efficient use of computational resources.
Chapter 5 applies the same principles to HDFS and looks at how to work with small files, as well as how compression can save you from many storage and computational headaches.
Finally, chapter 6 looks at how to measure, collect, and profile your MapReduce jobs and identify areas in your code and hardware that could be causing jobs to run longer than they should.
With your data safely in HDFS, it’s time to learn how to work with that data in MapReduce.
Previous chapters showed you some MapReduce snippets in action when working with data serialization.
In this chapter we’ll look at how to work effectively with big data in MapReduce to solve common problems.
If you want to understand the mechanics of MapReduce and how to write basic MapReduce programs, it’s worthwhile to read Hadoop in Action by Chuck Lam.
MapReduce contains many powerful features, but in this chapter we’ll focus on joining, sorting, and sampling.
These three patterns are important because they’re natural operations you’ll want to perform on your big data, and the goal of your clusters should be to squeeze as much performance as possible from your MapReduce jobs.
The ability to join disparate and sparse data is a powerful MapReduce feature, but an awkward one in practice, so we’ll also look at advanced techniques to optimize join operations with large datasets.
Examples of joins include combining log files with reference data from a database and inbound link calculations on web graphs.
Sorting in MapReduce is also a black art, and we’ll dive into the depths of MapReduce to understand how it works by examining two techniques that everyone will encounter at some point, secondary sort and total order sorting.
We’ll wrap things up with a look at sampling in MapReduce, which provides the opportunity to quickly iterate over a large dataset by working with a small subset of that data.
Joins are relational constructs you use to combine relations together (you’re probably familiar with them in the context of databases)
In MapReduce joins are applicable in situations where you have two or more datasets you want to combine.
An example would be when you want to combine your users (which you extracted from your OLTP database) with your log files that contain user activity details.
Scenarios where it would be useful to combine these datasets together include these:
Data aggregations based on user demographics (such as differences in user habits between teenagers and users in their 30s)
To send an email to users who haven’t used the website for a prescribed number of days.
A feedback loop that examines a user’s browsing habits, allowing your system to recommend previously unexplored site features to the user.
All of these scenarios require you to join datasets together, and the two most common types of joins are inner joins and outer joins.
Inner joins compare all tuples in relations L and R, and produce a result if a join predicate is satisfied.
In contrast, outer joins don’t require both tuples to match based on a join predicate, and instead can retain a record from L or R even if no match exists.
In this section we’ll look at three joining strategies in MapReduce that support the two most common types of joins (inner and outer)
The three strategies perform the join either in the map phase or in the reduce phase by taking advantage of the MapReduce sort-merge architecture:
Repartition join—A reduce-side join for situations where you’re joining two or more large datasets together.
Semi-join—Another map-side join where one dataset is initially too large to fit into memory, but after some filtering can be reduced down to a size that can fit in memory.
After we cover these joining strategies, we’ll include a decision tree so you can see what the best join strategy is for your situation.
A repartition join is a reduce-side join that takes advantage of MapReduce’s sortmerge to group together records.
It’s implemented as a single MapReduce job, and can support an N -way join, where N is the number of datasets being joined.
The map phase is responsible for reading the data from the various datasets, determining the join value for each record, and emitting that join value as the output key.
The output value contains data that you’ll want to include when you combine datasets together in the reducer to produce the job output.
A single reducer invocation receives all of the values for a join key emitted by the map function and partitions the data into N partitions.
After the reducer has read all of the input records for the join value and partitioned them in memory, it performs a Cartesian product across all partitions and emits the results of each join.
Figure 4.2 shows the repartition join at a high level.
The contrib package does all of the heavy lifting and only requires a handful of methods to be implemented.
The contrib implementation of the repartition join is not space efficient; it requires all of the output values for a given join value to be loaded into memory before it can perform the multiway join.
It’s more efficient to load the smaller of the datasets into memory and then iterate over the larger of datasets, performing the join along the way.
Problem You want to perform a repartition join in MapReduce, but you want to do so without the overhead of caching all the records in the reducer.
Solution This technique uses an optimized repartition join framework that caches just one of the datasets being joined to reduce the amount of data cached in the reducers.
This optimized framework only caches records from the smaller of the two datasets to cut down on the memory overhead of caching all the records.
Figure 4.4 shows a class diagram broken into two parts, with a generic framework and some sample implementation classes.
With a repartition join, and with MapReduce in general, it’s a good idea to cut down on the amount of data sent between the map and reduce phases, because it’s expensive to sort and transfer data between the two phases over a network.
If reduce-side work can’t be avoided, as in the case of the repartition join, a good practice is to filter and project as much as possible in the map phase.
Filtering is the act of discarding map input records that you don’t need to include in the job output.
Projection is a relational algebra term and is used to cut down the fields sent to the reducer.
For example, if you’re working with user data and you only care about the results of the join containing the age of a user, your map task should only project (or emit) the age field, and not any of the other fields for the user.
Let’s say you want to join together some user details and logs that contain information about user activity.
The first step is to determine which of the two datasets is smaller in size.
For a reasonably sized website with these two datasets it’s likely that the user dataset will be smaller than the activity logs.
Figure 4.3 An optimized MapReduce implementation of a repartition join.
The user data in the following example consists of the user’s name, age, and state:
The user activity logs contain the user’s name, the action performed, and the source IP address.
This file would normally be a much larger file than the user’s file:
The implementation class is responsible for creating the map output key and value, as well as informing the frame work whether the current input split being worked on is the smaller of the datasets being joined:1
You hardcode the fact that th user’s file is the smaller fil.
This method needs to return a uniqu identifier for the supplied input file, s.
In this class you’re passed a map output key and two map output values from different datasets, and you need to return the reduce output tuple:2
Finally, the job driver code needs to indicate the InputFormat class and set up the secondary sort:3
Indicate if this input split is from the smaller file.
If you refer back to the original files you joined, you can see that because you implemented an inner join, the output doesn’t include entries for the users anne, alison, and others that weren’t in the log file.
Summary My join implementation improves on the Hadoop contrib join by buffering only the values of the smaller dataset.
But it still suffers from the problem of all the data being transmitted between the map and reduce phases, which is an expensive network cost to incur.
Further, while the Hadoop contrib join package can support N -way joins, my implementation only supports two-way joins.
A simple mechanism to further reduce the memory footprint of the reduce-side join is to be aggressive about projections in the map function.
Projection is the act of cutting down on the fields that the map emits.
For example, if you’re working with user data, and you only care about the result of the join containing the age of a user, then the map task should only project (or emit) the age field, and not any of the other fields for the user.
This results in less network traffic between the map and reduce tasks, and also cuts down on the reducer memory overhead when performing the join.
My repartition join implementation supports filtering and projections, like the original join contrib package.
Filtering is supported by allowing the genMapOutputValue method to return NULL, and projections are supported by allowing this same method to define the contents of the output value.
What if you want to avoid the overhead of sorting and transferring all of your data over the network to the reducer? The solution for this brings us to the two next join strategies, the replicated join and semi-join.
A replicated join is a map-side join, and gets its name from its function—the smallest of the datasets is replicated to all the map hosts.
You’ll use the distributed cache4 to copy the small dataset to the nodes running the map tasks, and use the initialization method of each map task to load the small dataset into a hashtable.
Use the key from each record fed to the map function from the large dataset to look up the small dataset hashtable, and perform a join between the large dataset record and all of the records from the small dataset that match the join value.
Figure 4.5 shows how the replicated join works in MapReduce.
The implementation of the replicated join is straightforward, and you can see a demonstration in Hadoop in Action.
Appendix D provides a generalized framework to perform replicated joins, which can work with data from any InputFormat and OutputFormat.
This join framework dynamically determines whether the contents of the distributed cache or the input split should be cached, depending on which is smaller.
Is there a way to utilize map-side joins in cases where initially neither of the datasets are small enough to fit in memory? Time to look at semi-joins.
Imagine a situation where you’re working with two large datasets that you want to join, such as user logs and user data from an OLTP database.
Neither of these datasets is small enough to cache in a map task’s memory, so it would seem you’ll have to resign.
Hadoop’s distributed cache copies files located on the MapReduce client host, or files in HDFS, to the slave nodes before any map or reduce tasks are executed on the nodes.
Tasks can read these files from their local disk to use as part of their work.
Look up and join users being fed into the map function.
Not necessarily—ask yourself this question: would one of the datasets fit into memory if you were to remove all records that didn’t match a record from the other dataset? In the example there’s a good chance that the users that appear in your logs are a small percentage of the overall set of users in your OLTP database, so by removing all the OLTP users that don’t appear in your logs, you could get the dataset down to a size that fits into memory.
If this is the case, the semijoin is the solution.
Figure 4.6 shows the three MapReduce jobs you’ll execute to perform a semi-join.
When faced with the challenge of joining two large datasets together, the obvious choice is to go with a repartition join, which leverages the full MapReduce framework to perform the join on the reduce-side.
In fact, this may be your only option if you can’t filter one of the datasets to a size that can be cached on the map side.
However, if you believe that you could pare down one dataset to a manageable size, you may not have to use a repartition join.
Problem You want to join large datasets together and at the same time avoid the overhead of the shuffle and sort phases.
Solution In this technique you will use three MapReduce jobs to join two datasets together to avoid the overhead of a reducer-side join.
This technique is useful in situations where you’re working with large datasets, but a job can be reduced down to a size that can fit into the memory of a task by filtering out records that don’t match the other dataset.
Discussion For this technique you’ll leverage the replicated join code I wrote (see appendix D) to implement the last two steps in the MapReduce job.
In this technique you’ll break down the three jobs illustrated in figure 4.6
The function of the first MapReduce job is to produce a set of unique user names that exist in the logs files.
You do this by having the map function perform a projection of the user name, and in turn use the reducers to emit the user name.
To cut down on the amount of data transferred between the map and reduce phases, have the map task cache all of the user names in a HashSet and emit the values of the HashSet in the cleanup method.
The result of the first job is a unique set of users that appears in the log files.
The second step is an elaborate filtering MapReduce job, where the goal is to remove users from the user dataset that don’t exist in the log data.
The first job operates on the large dataset—in our case, the user logs—and.
Figure 4.7 The first job in the semi-join produces a unique set of user names that exist in the log files.
The unique user’s output from job 1 will be substantially smaller than the entire user dataset, which makes it the natural selection for caching.
This is a good time to quickly look at the replicated join framework in appendix D.
As it happens, this is also how your data is laid out.
You can see the class diagram for the framework in figure 4.9
Figure 4.8 The second job in the semi-join removes users from the user dataset missing from the log data.
For this step you’re interested in the join method, which produces the output key and value for the job.
The default implementation combines the values of both datasets to produce the final output value.
You want to change this to output only the value from the user’s table, as follows:6
You also need to add the files from job 1 into the distributed cache:
The output of the second job is the filtered users that also existed in the log output.
In this final step you’ll combine the filtered users produced from job 2 with the original user logs.
Ostensibly, the filtered users are now small enough to stick into memory, allowing you to put them in the distributed cache.
The final join is also a replicated join, where we need to cache the filtered-out users.
Figure 4.10 The third job in the semi-join combines the users produced from the second job with the original user logs.
Again you’re using the replicated join to perform the join, but this time you won’t tweak the behavior of the join method because you want the data from both datasets.
Run the code and look at the output produced by each of the previous steps:
The output shows the logical progression of the jobs in the semi-join and the final join output.
Summary In this technique we looked at how to use a semi-join to combine two datasets together.
The semi-join construct involves more steps than the other joins, but it’s a powerful way to leverage a map-side join even when working with large datasets (with the caveat that one of the datasets must be reduced to a size that fits in memory)
With the three join strategies in hand, you may be wondering which one you should use in what circumstance.
Each of the join strategies we’ve covered has different strengths and weaknesses, so how do you determine which one is best suited for the data you’re working with?
The output of the first job is the unique user names in the log file.
The second job output shows the users’ file filtered by users that.
The final output has the results of the join between the user logs and.
I’ll summarize the decision tree shown in the previous figure with the following three points:
If one of your datasets is small enough to fit into a mapper’s memory, the maponly replicated join is sufficient.
If both datasets are large, and one dataset can be substantially reduced by prefiltering elements that don’t match the other, the semi-join works well.
If you can’t preprocess your data and your data sizes are too large to cache, which means you have to perform the join in the reducer, repartition joins needs to be used.
It’s possible to have reduce-side joins in MapReduce because MapReduce sorts and correlates the map output keys together.
In the next section we’ll look at common sorting techniques in MapReduce.
MapReduce sorts data for two reasons: sorting allows MapReduce to group the map keys together so that reduce tasks can be called once per unique map key.
And sorting allows users to sort job outputs when they have specific use cases that need sorting.
Examples of these use cases include data analytical jobs where you want to see the top N most popular users or web pages.
In this section you’ll look at two particular scenarios where you want to tweak the behavior of MapReduce sorting.
First we’ll look at the secondary sort, which allows you to sort values for a reducer key.
Secondary sorts are useful when you want some data to arrive at your reducer ahead of other data, as in the case of the optimized repartition join earlier in this chapter.
An example of this would be if you want to perform a primary sort of stock data by stock symbol, and then perform a secondary sort on the time of each stock quote during a day.
Secondary sorts are used in many of the techniques in this book, ranging from optimizing the repartition join to graph algorithms such as friends-of-friends.
The second scenario we’ll cover in this section looks at sorting data across all the reducer outputs.
This is useful in situations where you want to extract the top or bottom N elements from a dataset.
As you saw earlier in the joining section, you need secondary sorts to allow some records to arrive at a reducer ahead of other records.
Secondary sorts require an understanding of both data arrangement and data flows in MapReduce.
Figure 4.12 shows the three elements that impact data arrangement and flow (partitioning, sorting, and grouping) and how they’re integrated into MapReduce.
The partitioner is invoked as part of the map output collection process, and is used to determine which reducer should receive the map output.
Determines how sorted records are logically grouped together for a single reducer function call.
Finally, the grouping RawComparator is responsible for determining the group boundaries across the sorted records.
The default behavior in MapReduce is for all three functions to operate on the entire output key emitted by map functions.
Secondary sorts are useful when you want some of the values for a unique map key to arrive at a reducer ahead of other values.
Problem You want to order values sent to a single reducer invocation for a natural key.
Solution This technique covers writing your partitioner, sort comparator, and grouping comparator classes, which are required for secondary sort to work.
Discussion In this technique we’ll look at how to use secondary sort to order people’s names.
You’ll use the primary sort to order people’s last names, and secondary sort on their first names.
To support secondary sort you need to create a composite output key, which will be emitted by your map functions.
It also shows a composite value that provides reducer-side access to the secondary key.
Let’s go through the partitioning, sorting, and grouping phases and implement them for your user.
But before that, you need to write your composite key class.
The composite key contains both the first and last name.
Figure 4.14 shows the configuration names and methods to call to set the partitioning, sorting, and grouping classes, and it also shows what part of the composite key each class uses.
Let’s look at the implementation code for each of these classes.
Only use the natural key for partitioning, so that they all go to the same reducer.
The output key comparator sorts using the entire composite key.
The output value grouping compares the natural key, ignoring the secondary.
The partitioner is used to determine which reducer should receive a map output record.
The default MapReduce partitioner (HashPartitioner) calls the hashCode method of the output key and performs a modulo with the number of reducers to determine which reducer should receive the output.
The default partitioner uses the entire key, which won’t work for your composite key, because it will likely send keys with the same natural key value to different reducers.
Instead, you need to write your own Partitioner, which partitions on the natural key.
The following code shows the Partitioner interface you must implement.
The getPartition method is passed the key, value, and the number of partitions:9
Your partitioner will calculate a hash based on the last name in the Person class, and perform a modulo of that with the number of partitions (which is the number of reducers):
The map-side sorting is an optimization to help make the reducer sorting more efficient.
You want MapReduce to use your entire key for sorting purposes, which will order keys according to both the last name and the first name.
Grouping occurs when the reduce phase is streaming map output records from local disk.
Grouping is the process by which you can specify how records are combined to form one logical sequence of records for a reducer invocation.
When you’re at the grouping stage, all of the records are already in secondary-sort order, and the grouping comparator needs to bundle together records with the same last name:11
The final steps involve telling MapReduce to use the partitioner, sort comparator, and group comparator classes:12
To complete this technique you need to write the map and reduce code.
You’ll upload a small file with unordered names, and test whether the secondary sort code produces output sorted by first name:
Summary You saw how a secondary sort can be performed in MapReduce.
Next we’ll look at how to sort outputs across multiple reducers.
You’ll find a number of situations where you’ll want to have your job output in total sort order.
For example, if you want to extract the most popular URLs from a web graph you’ll have to order your graph by some measure of popularity, such as PageRank.
Or if you want to display a table in your portal of the most active users on your site, you need the ability to sort them based on some criteria such as the number of articles they wrote.
We all know that the MapReduce framework sorts map output keys prior to feeding them to reducers.
This sorting is only guaranteed within each reducer, and unless you specify a partitioner for your job, you will be using the default MapReduce partitioner, HashPartitioner, which partitions using a hash of the map output keys.
This ensures that all records with the same map output key go to the same reducer, but the HashPartitioner doesn’t perform total sorting of the map output keys across all the reducers.
Knowing this, you may be wondering how you could use MapReduce to sort keys across multiple reducers so that you can easily extract the top and bottom N records from your data.
Problem You want a total ordering of keys in your job outputs, but without the overhead of having to run a single reducer.
The partitioner ensures that output sent to the reducers is totally ordered, so as long as the reducer emits the same output key as the input key, total job output is guaranteed.
The partition file is a precomputed SequenceFile that contains N -1 keys, where N is the number of reducers.
The keys in the partition file are ordered by the map output key comparator, and as such each key represents a logical range of keys.
First you’ll use the InputSampler class, which samples the input files and creates the partition file.
Ru sam file set per suggests picks random records from the input, and the IntervalSampler class, which for every R record includes the record in the sample.
Once the samples have been extracted, they’re sorted and then N -1 keys are written to the partition file, where N is the number of reducers.
The InputSampler isn’t a MapReduce job; it reads records from the InputFormat and produces the partition within the process calling the code.
The following code shows the steps you need to execute prior to calling the InputSampler function:14
The probability that a key will be picked from the input.
Set the InputFormat for the j which the InputSampler uses.
You also need to specify the map outpu key and value classes, even if th.
This code uses all the items in the JobConf object to form this task.
For this technique you don’t want to do any processing in your MapReduce job, so you’ll not specify the map or reduce classes.
This means the identity MapReduce classes will be used, so you’re ready to run the code:
You can see from the results of the MapReduce job that the map output keys are indeed sorted across all the output files.
You could also use MapReduce to generate the partition file.
An efficient way of doing this would be to write a custom InputFormat class that performs the sampling, and then output the keys to a single reducer, which in turn can create the partition file.
This brings us to sampling, the last section of this chapter.
You ran with two reducers, so you have two part files in the output directory.
You expect to see names starting with A’s a the top of the first output fil.
The bottom of the first outpu contains names starting with L, w.
The top of the second output file continues with names that are.
Imagine you’re working with a terabyte-scale dataset and you have a MapReduce application you want to test with that dataset.
Running your MapReduce application against the dataset may take hours, and constantly iterating with code refinements and rerunning against it isn’t an optimal workflow.
To solve this problem you look to sampling, which is a statistical methodology for extracting a relevant subset of a population.
In the context of MapReduce, sampling provides an opportunity to work with large datasets without the overhead of having to wait for the entire dataset to be read and processed.
This greatly enhances your ability to quickly iterate when developing and debugging MapReduce code.
You need to iterate over the development of a MapReduce job, and have a large dataset that you want to work with to iteratively test with.
Working with the entire dataset takes a long time, and impedes your ability to rapidly work with your code.
Problem You want to work with a small subset of a large dataset during the development of a MapReduce job.
Solution You’ll write an input format that can wrap the actual input format used to read data.
The input format that you’ll write can be configured with the number of samples that should be extracted from the wrapped input format.
Discussion In this technique you’ll use reservoir sampling15 to choose samples.
Reservoir sampling is a strategy that allows a single pass through a stream to randomly produce a sample.
As such it’s a perfect fit for MapReduce because input records are streamed from an input source.
Figure 4.16 The reservoir sampling algorithm allows one pass through a stream to randomly produce a sample.
The input split determination and record reading will be delegated to wrapped InputFormat and RecordReader classes.
If you need a refresher on these classes, please review chapter 3 for more details.
At initialization time read records using the RecordReader and cache samples according to.
An upper bound on the number of records read from the data source, to avoid having to read the entire data source.
You can see the sampling InputFormat in action by running an identity job against a large file containing names:
This number is either across splits, or for each input split.
Set the maximum number of records that will be read from each input split to.
Determines whether the number of samples should be extracted per.
Summary Sampling support in MapReduce code can be a useful development and testing feature when engineers are running code against production-scale datasets.
That begs the question: what’s the best approach to integrate sampling support into an existing codebase? One approach would be to add a configurable option that would toggle the use of the sampling InputFormat, similar to the following code:
You can apply this sampling technique to any of the preceding sections as a way to work efficiently with large datasets.
Joining and sorting are cumbersome tasks in MapReduce, and we spent this chapter learning about methods to optimize and ease their use.
We looked at three different join strategies, two of which were on the map side, and one on the reduce side.
The goal was to simplify joins in MapReduce, and as such I presented two frameworks that reduce the amount of user code required for joins.
We also covered sorting in MapReduce by examining how secondary sorts work, and how you can sort all of the output across all the reducers.
We’ll cover a number of performance patterns and tuning steps in chapter 6, which will result in faster join and sorting times.
But before we get there, we’ll look at HDFS patterns to optimize storage and disk/network I/O in the next chapter.
In the previous chapter we looked at how to work effectively with MapReduce and big data.
You might tend to spend more time thinking about MapReduce because of its computational layer, but you shouldn’t deprive HDFS of attention when wrestling with big data, because improvements to HDFS will pay great dividends in terms of performance with our MapReduce jobs.
In view of this, this chapter is dedicated to looking at ways to efficiently store and access big data in HDFS.
The first subject I’ll address is how to work with a large number of small files in HDFS.
As you’ll soon discover, the NameNode loads metadata about the filesystem into memory, which can cause memory-exhaustion problems with large numbers of small files.
In this chapter we’ll look at how to use Avro to work around this limitation.
Critical in Hadoop, compression isn’t for the faint of heart.
You need compression to maximize your storage capabilities, as well as for streamlining local disk reads and data transferred across your network between map and reduce tasks.
Compression ideally would be transparent to HDFS users, but it isn’t, and it’s cumbersome and unintuitive to work with large, compressed files of common file formats.
The bulk of this chapter, therefore, contains techniques to help you effectively use compression with large files.
We’ll start things off with a look at how to work with small files in HDFS.
Hearing the term big data conjures up images of large files that are gigabytes in size or more.
But big data can also be a large number of small files.
In fact, much of the data that you’ll want to work with will be small—as an example, log files are frequently rotated when they reach megabytes in size.
In this section we’ll look at effective techniques for working with small files in HDFS.
Let’s say that you’re working on a project akin to Google Images, where you crawl the web and download image files from websites.
Your project is internet-scale, so you’re downloading millions of files and storing them individually in HDFS.
Unfortunately, in doing so you’ll be exposing some weaknesses in HDFS and MapReduce, including the following:
Hadoop’s NameNode keeps all the HDFS metadata in memory for fast metadata operations.
That’s a lot of memory for a single process, even with today’s mid-tier server RAM capacities.
This chapter assumes you have a basic understanding of HDFS concepts and you have experience working directly with HDFS.
If you need to become familiar with the topic, pick up a copy of Hadoop in Action by Chuck Lam, which offers the background information you’ll need on HDFS.
Finally, if you’re running in a controlled environment where there’s a Scheduler, you may have a cap on the number of tasks your MapReduce job can use.
Because each file (by default) results in least one map task, this could cause your job to be rejected by the Scheduler.
If at this point you’re thinking you won’t have this problem, think again.
Thinking and preparing for this eventuality is best done early in your design phase.
Problem You want to store a large number of files in HDFS, and do so without hitting the NameNode memory limits.
Solution The easiest way to work with small files in HDFS is to package them into a larger containing file.
For this technique you’ll read all of the files in a directory stored on local disk and save them in a single Avro file in HDFS.
You’ll also see how to use the Avro file in MapReduce to process the contents of the original files.
Discussion Figure 5.1 shows the first part of this technique, where you create the Avro file in HDFS.
In doing so you create fewer files in HDFS, which means less data to be stored in NameNode memory, which also means you can store more stuff.
Avro is a data serialization and RPC library invented by Doug Cutting, the creator of Hadoop.
He created Avro primarily to help improve data interchange, interoperability, and versioning in Hadoop.
Avro has strong schema evolution capabilities that give it an advantage over competitors such as SequenceFiles.
Let’s take a look at the Java code in the following listing,3 which will create the Avro file.
Check the value of dfs.block.size to see what it’s set to in your cluster.
Reads a directory containing small files and produces a single Avro file in HDFS.
Multiplexer: The multiplexer is responsible for reading the small files and packaging them in an Avro file in HDFS.
It could also have read the files from HDFS or some other filesystem.
Avro record:  Avro files consist of zero or more records, where each record conforms to an Avro schema.
In our example an Avro record represents a file, and each Avro record.
Figure 5.1 Storing small files in Avro allows you to store more.
Avro uses JSON to define t data structure schema, which.
Let’s see what happens when you run this script against Hadoop’s config directory:
For each file in the input directory, create a new Avro record, specifying y schema.
As you’re writing the file contents, you’ll also produce an.
MD5 hash, so that later you can visually compare that your.
To run the code in this chapter you’ll need to have both the Snappy and LZOP compression codecs installed on your host.
Please refer to appendix A for details on how to install and configure them.
Looks promising—let’s make sure that the output file is in HDFS:
Because Avro writes the schema into every Avro file, you don’t need to tell Avro information about the schema as part of deserialization.
Create an Avro reader object by supplying the InputStream of the file in HDFS.
Note that you don’t need to supply schema information, because Avro encodes that in.
Even though this chapter is about HDFS, the next thing you’ll likely want to do is to process the files that you wrote in MapReduce.
The next listing5 shows the code for this MapReduce job.
A MapReduce job that takes as input Avro files containing the small files.
Figure 5.2 Map job to read Avro files and write out a text file.
Avro has a convenience method to help set the appropriate job configuration settings for Avro.
If you run this MapReduce job over the Avro file you created earlier, the job log files will contain your filenames and hashes:
In this technique it was assumed that you were working with a file format (such as image files) that couldn’t be concatenated together.
If your files can be concatenated, you should consider that option.
If you go this route, try your best to make sure that the file size is at least as large as the HDFS block size to minimize the data stored in NameNode.
Summary You could have used Hadoop’s SequenceFile as a mechanism to hold your small files.
SequenceFiles are a more mature technology, having been around longer than Avro files.
But SequenceFiles are Java-specific and don’t provide the rich interoperability and versioning semantics you get with Avro.
Google’s Protocol Buffers, as well as Apache Thrift, which originated from Facebook, can also be used to store small files.
But neither has the InputFormat that works with native Thrift or Protocol Buffers files.
You extract your data from the GenericRecord using the simple get methods.
Another approach you could use is to write the files into a zip file.
The downsides to this approach are that you would have to write a custom InputFormat6 to process the zip file, and zip files aren’t splittable (as opposed to Avro and SequenceFiles)
This could be mitigated by generating multiple zip files and attempting to make them close to the HDFS block size.
In a similar vein you can also configure Hadoop to allow a single map task JVM to work on multiple tasks, reducing the expense of JVM cycling.
You also could have created a tarball file containing all the files, and then produced a separate text file which contained the locations of the tarball file in HDFS.
This text file would be supplied as the input to the MapReduce job, and the mapper would open the tarball directly.
But that approach would circumvent the locality in MapReduce, because the mappers would be scheduled to execute on the node that contained the text file, and would therefore likely need to read the tarball blocks from remote HDFS nodes incurring unnecessary network IO.
Hadoop Archive files (HARs) are Hadoop files specifically created to solve the problem of small files.
They are a virtual filesystem that sits on top of HDFS.
The disadvantages of HAR files are that they can’t be optimized for local disk access in MapReduce, and they can’t be compressed.
Hadoop version 2.x supports HDFS Federation, where HDFS is partitioned into multiple distinct namespaces, with each independently managed by a separate NameNode.
This in effect means that the overall impact of keeping block information in memory can be spread across multiple NameNodes, thereby supporting a much larger number of small files.
Hortonworks has a good blog post that contains more details about HDFS Federation at http://hortonworks.com/an-introduction-to-hdfs-federation/
Finally, MapR, which provides a Hadoop distribution, has its own distributed filesystem that supports large numbers of small files.
Using MapR for your distributed storage is a big change to your system, so it’s unlikely you’ll move to MapR to mitigate this problem with HDFS.
You may encounter times when you’ll want to work with small files in Hadoop, and using them directly would result in bloated NameNode memory use and MapReduce jobs that run slowly.
This technique helped you mitigate these issues by packaging small files into larger container files.
What if you have the opposite problem, where your files are plenty big and you want to be more efficient about how you store your data? Can you compress data in Hadoop, and how does that work with things like MapReduce? Read on to find out.
Data compression is a mechanism to reduce the size of data to a more compact form to save on storage space and to make it more efficient to transfer the data.
Compression is an important aspect of dealing with files, and becomes all the more important when dealing with the data sizes that Hadoop supports.
Your goal with Hadoop is to be as efficient as possible when working with your data, and picking a suitable compression codec7 will result in your jobs running faster and allow you to store more data in your cluster.
Using compression with HDFS isn’t as transparent as it is on filesystems such as ZFS,8
One of the advantages to working with file formats such as Avro and SequenceFiles is their built-in compression support, making compression almost completely transparent to users.
But you lose that luxury when working with most other file formats.
Problem You want to evaluate and determine the optimal compression codec for use with your data.
Solution Snappy, a compression codec from Google, offers the best combination of compressed size and read/write execution times.
However LZOP is the best codec when working with large compressed files that must support splittability.
Discussion Let’s kick things off with a quick look at the compression codecs available for use in Hadoop, shown in table 5.1
A compression codec is a programming implementation capable of reading and writing a given compression format.
ZFS, short for Z File System, is a filesystem developed by Sun Microsystems that provides innovative features to enhance data integrity.
To properly evaluate the codecs you first need to specify your evaluation criteria, which should be based in terms of functional and performance traits.
For compression your criteria is likely to include the following:
Splittability—Can a compressed file be split for use by multiple mappers? If a compressed file can’t be split, only a single mapper will be able to work on it.
If that file spans multiple blocks, you’ll lose out on data locality, because the map will likely have to read blocks from remote DataNodes incurring the overhead of network IO.
Native compression support—Is there a native library that performs compression and decompression? This will usually out-perform a compression codec written in Java with no underlying native library support.
Deflate Deflate is similar to zlib, which is the same compression algorithm that gzip uses without the gzip headers.
At one time LZO/LZOP came bundled with Hadoop, but they have since been removed due to GPL licensing restrictions.
Snappy Snappy (http://code.google.com/p/hadoop-snappy/) is a recent addition to codec options in Hadoop.
Google uses it for compressing data in both MapReduce and BigTable.a Its main drawback, which is a significant one, is that it’s not splittable.
This won’t matter if you work primarily with files that are smaller than or equal to the HDFS block size, but if you work with large files, your jobs won’t perform as fast as they should.
When you’re performing your own evaluation, I recommend you perform your tests using your data, and preferably on hosts similar to your production nodes.
This way you’ll have a good sense of the expected compression and runtimes for the codecs.
Let’s see how the compression codecs currently available compare with each other in table 5.2 (we’ll cover the space/time comparison in the next section)
Now that you understand the codecs, how do they square up when looking at their space/time trade-offs? I generated a 128 MiB (one mebibyte is 220 bytes) text file by concatenating the source code of Hadoop and various other open source libraries, and then used that to compare the codec run times and their compression sizes.
The results of these tests can be seen in table 5.3
Figure 5.3 shows the compressed sizes in bar graph form.
You need to preprocess them to create an index file, which is then used by their respective CompressionCodec implementations to determine the file splits.
We’ll cover how you achieve this in a later section on LZOP.
Deﬂate + gzip: runners-up doing well in compressed ﬁle sizes.
Figure 5.4 shows the compressed times in bar graph form.
These times will vary significantly based on hardware, and are only supplied to give a sense of how they relate to each other.
What do the space and time results tell you? If squeezing as much data into your cluster as possible is your top priority, and you can live with long compression times, then bzip2 may be the right codec for you.
If you want to compress your data but introduce the least amount of CPU overhead when it comes to reading and writing compressed files, you should look at Snappy.
Anyone looking for a balance between compression and execution times would have to eliminate bzip2 from the picture.
LZO(P) + Snappy:  decompression times are a wash, but look at how much faster they are compared to Deflate/gzip at compression.
Being able to split your compressed files is important, and here you have to choose.
The bzip2 times will likely cause most people to pause.
Its only advantage over LZOP is that its Hadoop integration is much easier to work with than LZOP’s.
While LZOP is the natural winner here, as you’ll see in the LZOP technique, it requires some effort to work with.
Summary The best codec for you will depend on your criteria.
Snappy is the most promising codec if you don’t care about splitting your files, and LZOP is what you should be looking at if you want splittable files.
Bear in mind that compressed sizes will vary based on whether your file is text or binary, and its contents, so to get accurate numbers on your data it’ll be worth your while to run similar tests against your own data.
Compressing data in HDFS has many benefits including reduced file sizes and faster MapReduce job runtimes.
A number of compression codecs are available for use in Hadoop, and we evaluated them based on features and performance.
Let’s look at how to compress files and use them with tools such as MapReduce, Pig, and Hive.
Because HDFS doesn’t provide built-in support for compression, it can be a challenge to work with compression in Hadoop.
The onus falls on you to figure out how to work with compressed files.
Also, splittable compression isn’t for the faint of heart, because it doesn’t come out of the box with Hadoop.9 If you’re dealing with medium-size files that compress down to near the HDFS block size, this technique will be the quickest and simplest way to reap the benefits of compression in Hadoop.
Problem You want to read and write compressed files in HDFS and also use them with MapReduce, Pig, and Hive.
Solution Working with compressed files in MapReduce involves updating the MapReduce configuration file mapred-site.xml and registering the compression codec you are using.
Discussion The first step is to figure out how to read and write files using any of the codecs evaluated earlier in this chapter.
All of the codecs detailed in this chapter are bundled with.
Technically, you can get out-of-the-box splittable compression with bzip2, but its performance traits, as shown earlier in this section, rule it out as a serious compression codec.
Hadoop except for LZO/LZOP and Snappy, so if you want to work with those three.
As you’ve learned, there are a number of compression codecs you can use.
To use them you need to know their class names, which are covered in table 5.4
How would you compress an existing file in HDFS using any one of the codecs mentioned in the previous table? The following code supports doing that:10
Construct an instance of the codec with the help of Hadoop’s ReflectionUtils.
Each codec has a default extensio (which was shown for each codec earlie in this section), and it’s a best practic.
You can use any standard Java OutputStream writing mechanism to write to your compressed stream; here.
Now that you can create compressed files, let’s look at how to work with them in MapReduce.
To work with compressed files in MapReduce you need to set some configuration options for your job.
The only difference between a MapReduce job that works with uncompressed versus compressed IO are the three annotated lines in the previous example.
An identity task is one that emits all of the input it receives as output, without any transformation or filtering.
One of the overheads to using compression codecs is that they can be expensive to create.
Using the Hadoop ReflectionUtils class will result in some of the reflection overhead associated with creating the instance being cached in ReflectionUtils, which should speed up subsequent creation of the codec.
Not only can a job’s input and output be compressed, but so can the intermediate map output, because it’s spilled first to disk, and then eventually over the network to the reducer.
The effectiveness of compressing the map output will ultimately depend on the type of data being emitted, but as a general rule you should see some job speed-up by making this change.
If it finds a codec that’s associated with that file extension, it automatically uses that codec to decompress the input files.
How does MapReduce know which codecs to use? You need to specify the codecs in mapred-site.xml.
The following code shows how to register all of the codecs we’ve evaluated.
Remember that other than the gzip, Deflate, and bzip2, all the other compression codecs need to be built and available on your cluster before you can register them:
Now that you’ve mastered compression with MapReduce, it’s time to look higher up the Hadoop stack.
Because compression can also be used in conjunction with Pig and Hive, let’s see how you can mirror your MapReduce compression accomplishment using Pig and Hive.
If you’re working with Pig, as with the MapReduce example, there’s no extra work required to make Pig work with compressed input files.
To have Pig work with compressed files all you need to do is specify the compression codec extension on the filename.
Let’s see how this works, starting with how to load compressed data.
The following example gzips a file and loads it into Pig:
It’s the same notion to write out a gzipped file—make sure to specify the extension for a compression codec.
The following example stores the results of Pig relation B in a file in HDFS, and then copies them to the local filesystem to examine the contents:
As with Pig, all you need to do is specify the codec extension when defining the filename:
In this situation Hive moves the file being loaded into Hive’s warehouse directory and continues to use the raw file as its storage for the table.
What if you want to create another table and also specify that it should be compressed? The following example achieves this by setting some Hive.
Run the script to generat the export command you nee.
Ending your filename with the .gz extension results in the underlying MapReduce.
OutputFormat recognizing the file as being gzipped, and using the appropriate compression.
As with the Pig example, the .g filename extension acts as a trigge.
You can verify that Hive is indeed compressing the storage for the new apachelog_backup table by looking at it in HDFS:
It should be noted that Hive recommends using SequenceFile as the output format for tables, because SequenceFile blocks can be individually compressed.
Summary This technique provides a quick and easy way to get compression running in Hadoop.
It works well for files that aren’t too large because it offers a fairly transparent way of working with compression in Hadoop.
If your compressed file sizes are much larger than the HDFS block size, read on for compression techniques that can split your files.
Imagine that you’re working with large text files which, even when compressed, are many times larger than the HDFS block size.
To avoid having one map task process an entire large compressed file, you’ll need to pick a compression codec that can support splitting that file.
Wait, you may be thinking, didn’t you state earlier that LZOP is splittable? LZOP is block-based, but you can’t perform a random seek into an LZOP file and determine the next block starting point.
This is the challenge we’ll tackle with the following technique.
Problem You want to use a compression codec that will allow MapReduce to work in parallel on a single compressed file.
Solution In MapReduce, splitting large LZOP-compressed input files requires the use of LZOPspecific input format classes such as LzoInputFormat.
The same principle applies when working with LZOP-compressed input files in both Pig and Hive.
Discussion The LZOP compression codec is one of only two codecs that allow for compressed files to be split, and therefore can be worked on in parallel by multiple reducers.
The other codec, bzip2, suffers from compression times that are so slow they arguably render the codec unusable.
Getting all the prerequisites compiled and installed on your cluster is laborious, but rest assured there are detailed instructions in appendix A.
To compile and run the code in this section you’ll need to complete the instructions in the appendix.
Previously, we covered how to read and write compressed files in section 5.2
To perform the same activity with LZOP requires you to specify the LZOP codec in your code.
Both LZO and LZOP codecs can be used with Hadoop.
Confusingly, the Hadoop codecs by default treat files ending with the .lzo extension to be LZOP-encoded, and files ending with the .lzo_deflate extension to be LZO-encoded.
Also much of the documentation seems to use LZO and LZOP interchangeably.
I’ll attempt to be consistent and refer to LZOP throughout this book.
Let’s write and read an LZOP file, and then make sure that LZOP utilities can work with the generated file:
Now make sure you can use this LZOP file with the lzop binary.
Copy the LZOP file from HDFS to local disk, uncompress it with the native lzop binary, and compare it with the original file:
The diff verified that the file compressed with the LZOP codec could be decompressed with the lzop binary.
Now that you have your LZOP file you need to index it so that it can be split.
Earlier I made the paradoxical statement that LZOP files can be split, but that they’re not natively splittable.
Let me clarify what that means—the lack of block-delimiting synchronization markers means you can’t do a random seek into an LZOP file and start reading blocks.
But because internally it does use blocks, all you need is a preprocessing step that can generate an index file containing the block offsets.
The LZOP file is read in its entirety and block offsets are written to the index file as the read is occurring.
You can create index files in one of two ways, as shown in the following two code snippets.
If you want to create an index file for a single LZOP file, then this is a simple library call that will do this for you:
The following option works well if you have a large number of LZOP files and you want a more efficient way to generate the index files.
The indexer runs a MapReduce job to create the index files.
Both files and directories (which are scanned recursively for LZOP files) are supported:
Both ways depicted in the previous options will generate an index file in the same directory as the LZOP file.
Now let’s look at how you would use the LzoIndexer in your Java code (this is copied from the main method of LzoIndexer)
Using this code will result in the index file being created synchronously:
You need the LZOP index files so that you can split LZOP files in your MapReduce, Pig, and Hive jobs.
Now that you have the aforementioned LZOP index files, you can look at how to use them with MapReduce.
After you’ve created index files for your LZOP files it’s time to start using your LZOP files with MapReduce.
Unfortunately, this brings us to the next challenge: none of the existing, built-in Hadoop-file-based Input Formats will work with splittable LZOP due to the need for specialized logic to handle input splits using the LZOP index file.
You need specific InputFormat classes to work with splittable LZOP.
The following code shows the required steps to configure the MapReduce job to work with LZOP.
You would perform these steps for a MapReduce job that had text LZP inputs and outputs:
In addition, compressing intermediary map output will also speed up the overall execution time of your MapReduce jobs:
Todd’s version is downstream of the master version maintained by Kevin Weil and other committers.
The LZOP input formats work well with LZOP files that also don’t have index files.
You can easily confgure your cluster to always compress your map output by editing hdfs-site.xml:
The number of splits per LZOP file is a function of the number of LZOP blocks that the file occupies, not the number of HDFS blocks that the file occupies.
Now that we’ve covered MapReduce, let’s look at how Pig and Hive can work with splittable LZOP.
Elephant Bird,18 a project containing utilities to work with LZOP that Twitter maintains, provides a number of useful MapReduce and Pig classes to help work with LZOP.
Elephant Bird has an LzoPigStorage class that works with text-based, LZOP-compressed data in Pig.
If you’re fortunate enough to be able to store your data in SequenceFiles or Avro, they offer the simplest way to work with files that can be easily compressed and split.
If you want to compress other file formats and need them to be split, LZOP is the only real candidate.
As I mentioned earlier, the Elephant Bird project provides some useful LZOP input formats that will work with LZOP-compressed file formats such as XML, and also plain text.
If you need to work with an LZOP-compressed file format that isn’t supported by either Todd Lipcon’s LZO project, or Elephant Bird, you’ll have to write your own Input Format.
I hope at some point Hadoop will be able to support compressed files with custom splitting logic so that end users don’t have to write their own Input Formats for compression.
Compression is likely to be a hard and fast requirement for any production environment where resources are always scarce.
Compression also allows faster execution times for your computational jobs, and so is a compelling aspect of storage.
Chapter summary section I showed you how to evaluate and pick the codec best suited for your data.
We also covered how to use compression with HDFS, MapReduce, Pig, and Hive.
Finally, we tackled the tricky subject of splittable LZOP compression.
Big data in the form of large numbers of small files brings to light a limitation in HDFS, and in this chapter we worked around this limitation by looking at how to package small files into larger Avro containers.
Compression is a key part of any large cluster, and we evaluated and compared the different compression codecs.
I recommended codecs based on various criteria, and also showed how to compress and work with these compressed files in MapReduce, Pig, and Hive.
We also looked at how to work with LZOP to achieve compression as well as blazing fast computation with multiple input splits.
This chapter and the previous chapter were dedicated to looking at techniques to work effectively with big data in MapReduce and HDFS.
The next chapter covers techniques to diagnose and tune Hadoop to squeeze as much performance as you can out of your clusters.
Imagine you wrote a new piece of MapReduce code and you’re executing it on your shiny new cluster.
You’re surprised to learn that despite having a good-size cluster, your job is running significantly longer than you expected.
You’ve obviously hit a performance issue with your job, but how do you figure out where the problem lies?
One of Hadoop’s selling points when it comes to performance is that it scales horizontally.
This means that adding nodes tends to yield a linear increase in throughput, and often in job execution times.
So isn’t the solution to your performance problem to add more nodes?
Maybe, but first let’s poke around a little to understand the root cause of your poorly performing job.
A plethora of problems may be causing your job to run Diagnosing and tuning performance problems.
Measuring MapReduce and your environment slowly, including issues with your hardware, your code, or the data with which you’re working.
To get a handle on your performance problem, you’ll need to follow a calculated and systematic approach to performance tuning, which is described in the following steps:
Ensure you have the monitoring and measurement capabilities required to gather metrics about your software and hardware.
Tweak some configuration or code in an attempt to fix the perceived problem.
To truly have a handle on the exact cause of the problem, change only one item at a time.
Re-execute the process and compare the metrics of the updated system with the previous metrics.
Many of the techniques in this chapter will show you how to isolate the cause of your performance problems in Hadoop.
Because this information is so important, we’ll spend the first half of the chapter looking at how to diagnose and measure the performance of your jobs.
This structured approach helps you discover where your performance problems lie.
The second half of this chapter focuses on techniques to improve job performance by covering topics such as skew mitigation and data serialization.
Before you can start performance tuning, you need to have the tools and processes in place to capture system metrics.
You need these tools in order to gather and examine empirical data related to your MapReduce job and determine whether or not you’re suffering from a performance problem.
In this section we’ll look at the tools and metrics that Hadoop provides, and also touch upon monitoring as an additional tool in your performance tuning toolkit.
A number of the techniques in this chapter rely on extracting job and task-level metrics from Hadoop.
You can extract these statistics in one of three ways:
Use the JobTracker UI to view job and task counters.
Use my utilities, which also extract metrics from the job history.
The last two items are useful because they allow you to view both job and task metrics, as well as some aggregated statistics.
Let’s briefly look at how job history works, and how it’s used by both the Hadoop CLI and my utilities.
For each job MapReduce generates a job statistics file, which contains task and joblevel statistics.
The easiest way to view a summary of the information in this file is to use the Hadoop CLI.
Let’s say you want to extract the metrics for a job where the output was written to an HDFS directory named output.
You would supply that directory name to the CLI, as follows:
The previous output is only a small subset of the overall output produced by the command, and it’s worth executing it yourself to see the full metrics it exposes.
This output is useful in quickly evaluating metrics such as average- and worst-task execution times.
Where does the job history file exist? Figure 6.1 shows its location.
The job history filename can be confusing because the file ends with a .jar extension, but it’s a text file, marshalled with the JobHistory class.
You can use the same class to reverse engineer a Java representation of this file.
You can use the Hadoop CLI only to extract the job statistics from the HDFS output directory, but my utilities have support for that, as well as for extracting statistics from the NameNode log directory.
It’s important to capture your CPU, memory, and network utilization as your jobs are running.
The goal at this point is to ensure that you aren’t overutilizing or underutilizing your hardware.
Measuring MapReduce and your environment spending a considerable amount of time competing for resources, be it CPU context switching or memory page swapping.
Underutilization of your cluster means you’re not leveraging all that you can from your hardware.
With MapReduce your system resource utilization will be a function of the type of jobs you’re running, and how CPU-bound or I/O-bound they are.
Being able to automatically track system utilization is crucial—without it you won’t have a historical perspective on the normal performance traits of your hardware, nor will you be able to do useful things like provide alerts when you’ve reach undesired utilization levels.
There are many fine tools that you can use for this, ranging from sar,1 the built-in Linux utility that collects and reports on system activity, to more sophisticated tools such as Nagios and Ganglia.
Ganglia has the added advantage of being able to pull statistics from Hadoop.3
The Hadoop wiki has basic instructions on Ganglia and Hadoop integration http://wiki.apache.org/
Let’s say a job is taking a ridiculously long period of time to execute and you suspect something isn’t quite right.
In this section we’ll cover how to isolate where that job is spending time.
To do this we’ll look at how to use built-in tools that Hadoop provides as well as some utilities I wrote.
Before we delve into the techniques let’s take a high-level look at what can adversely impact the performance of your job.
You can break down the items that can impact your jobs into the following broad categories:
Conversely, an incorrectly tuned configuration can also lead to performance issues such as swapping and CPU saturation.
Figure 6.2 Ganglia screenshots showing CPU utilization for multiple hosts.
Hadoop 0.20.x doesn’t have a built-in way to extract CPU and memory utilization for individual MapReduce tasks.
Map tasks—The performance of map tasks is sensitive to unusually large or small data inputs.
Reduce tasks—Items such as data skew, the number of reducers, and unoptimized user code all can contribute to slow tasks.
Hardware—Badly behaving nodes and network issues will have a big impact on your cluster, particularly if it’s not a large cluster.
We’ll go through a number of common scenarios that cause your job to perform poorly.
Each scenario will be a separate technique that shows how to identify the problem.
The scenarios will be grouped together by the categories covered in an earlier section.
The decision tree in figure 6.3 describes the techniques to consider depending on different performance properties.
Figure 6.4 shows the main units of work performed in the map task, and highlights areas that can have an impact on the performance of a job.
One cause of slow jobs may be that the data the job is working on has grown larger than expected.
For example, if you’re running a web log processing job, and on a particular day your website is overwhelmed with page hits, this will result in much largerthan-normal log files, which in turn slows down your MapReduce job.
Problem You want to quickly evaluate if there are spikes in your map and reduce input sizes.
Solution Use the JobTracker UI to compare the map input and output data sizes of the slow job with the input and output sizes of historical jobs.
Discussion Go to the JobTracker UI and select the slow-running job.
In the job summary screen you’ll see job statistics similar to those shown in figure 6.5
Compare these numbers with previous runs of the same job—if the previous runs have smaller input sizes, they could well be the reason why your job runs slowly.
Summary Depending on what input data you’re processing, it’s possible there’s a spike in the data volumes that has caused your job to work with larger-than-normal data sizes.
Wait for scheduler to schedule task User map function Spill.
Reading inputs and calling the user's map function are performed in the same.
Input size too small - Nonsplittable input - Data skew.
On the map side, data skew is typically the result of a handful of large unsplittable files or a large number of smaller files.
Problem You want to determine if a job runs slowly due to skewed data.
Solution Use the JobTracker UI to compare the input sizes across map tasks for the same job.
Discussion Data skew manifests itself as a small number of tasks taking disproportionately longer to execute than most of the others.
If you combine this with comparing the input sizes of the lingering tasks with the completed task input sizes, you’ll have an almost foolproof way of identifying data skew.
Figure 6.6 shows the steps to take to use the JobTracker UI to identify data skew.
Summary Imagine that you’ve successfully used this technique to identify that your job, which is still executing, exhibits significant data skew.
Your next step should be to try and mitigate the effects of skew in your job.
This is the number of input bytes that mappers read.
This is the number of output bytes that mappers produced.
The total number of input bytes for all the reducers.
Determine if the lingering task is working on a much larger input than other tasks.
The task list view shows the input file, the start byte offset, and the number of bytes to read.
The task counters can provide the same information, which can help when working with.
Click on the link to get to the views shown in the next step.
In this technique we’ll try to determine if your map tasks have low throughput and we’ll talk about likely issues that can cause low throughput.
Problem You want to determine if a job runs slowly due to low task throughput.
Solution Calculate the map task throughput using metrics from the JobTracker UI, or the job history metadata.
Discussion You can calculate the map throughput for individual tasks by using the JobTracker to get at the task execution times.
Figure 6.7 shows how to calculate throughput for map tasks.
The easiest way to get at the throughput statistics for all map tasks is to execute the TaskThroughput class (source: http://goo.gl/QQvvQ) against the job history file for your job, as shown in figure 6.8
Summary You’ve calculated the map task throughput, but how do you know if that number is low or high? If your tasks are reading all their data from their local nodes (as they ideally should be when working with HDFS and files that can be split), you would expect the throughput numbers to be close to the local disk read throughput.
For jobs that read their inputs from a data source other than HDFS, this determination is harder, and requires knowledge about the read rate of the data source as well as network latencies that exist between the Hadoop nodes and the data source.
The source files are significantly smaller than the HDFS block size, which means that you’re spending more time starting and stopping tasks, and not enough time reading and processing inputs.
The files being worked on aren’t splittable, so you’re having to incur network I/O to read file blocks from other nodes.
A node’s local disk or disk controller is running in a degraded mode that results in poor read and write performance.
This is more likely to affect individual nodes as opposed to all the nodes.
If the input data to your MapReduce isn’t files in HDFS, you’ll want to look into latency issues between your Hadoop nodes and your input data source.
The map task reads its input from a different DataNode.
This can be checked by looking at the map task details in the JobTracker, which in conjunction with all the task attempts also shows the input split locations.
If the machine that the task is scheduled on isn’t one of the input split locations, you’re losing out on data locality.
Separate techniques will be discussed for all of the items listed that will help determine the exact cause of low map task throughput.
Running a job over thousands of small files is inefficient because this results in thousands of Java task processes being spawned to process small input datasets.
Problem You want to determine if a job runs slowly due to small input files.
Solution Use the JobTracker UI or the job history metadata to inspect the size of your input splits.
Look at the average map task throughput, represented in Bps (bytes per second)
This is calculated by taking the input bytes and dividing by the task execution time in seconds.
Alternatively, the job statistics summarization tool, shown in figure 6.10, will show minimum, maximum, average, and median values for the input map bytes.
Summary If the input files to a job are significantly smaller than the HDFS block size, it’s likely that your cluster spends more effort starting and stopping Java processes than it spends performing work.
If you’re suffering from this problem you should consult chapter 5, where I explain various approaches you can take to work efficiently with small files.
Look at the input files and specifically the data range for.
Figure 6.9 The status column from the Hadoop map task list.
Look at the median and mean (average) input sizes for the map tasks.
If they are substantially smaller than the HDFS block size (as they are in this example because we're running with the default of 64MB), you will have to tune your inputs to extract better performance from your job.
Note: for small jobs the input and output byte values can be -1, which means no metrics were.
If your jobs are working with input files that can’t be split (such as files compressed with certain codecs, or binary files that by nature aren’t block-based), you’ll lose out on HDFS locality and optimal parallelization of your data.
In this technique we’ll look at how to determine if this is impacting your jobs.
Problem You want to determine if a job runs slowly due to unsplittable files.
Solution Use the JobTracker UI or the job history metadata to determine if the size of your input splits are too large.
Discussion You can eyeball the input files in the JobTracker’s map task list for your job.
Figure 6.11 shows an example of the map task list for a job with large input files that aren’t being split.
Alternatively, the job statistics summarization tool, shown in figure 6.12, will show minimum, maximum, average, and median values for the input map bytes.
Use the JobTracker to view the list of map tasks, and calculate the average number of input bytes for each map task.
Look at the median and mean (average) input sizes for the map tasks.
Summary If the input files to a job are significantly larger than the HDFS block size, it’s likely that you have a handful of slots in the cluster processing these large files, and a bunch of available slots that aren’t being utilized to work in the inputs because the input files aren’t split.
If this is happening to you, the first thing to consider is whether this is the expected behavior.
This can be the case when you’re working with a binary file that’s not block-based and can’t be split.
This would also be the case when working with compressed files that can’t be split, which is the case with nearly every compression codec apart from LZOP and bzip2
The other thing to watch out for is cases where this happens with LZOP files.
There’s no associated index file for the LZOP file, so the LZOP input format classes don’t know how to split the file.
If this is the case, refer to chapter 5 for instructions on how to create an index file.
You’re not using an LZOP InputFormat to process the file.
If you use a regular input format (such as the default TextInputFormat) for your job, it doesn’t know how to split an LZOP file and as such will treat the whole file as a single split.
You must use an input format whose name starts with Lzo, such as LzoTextInputFormat.
Next we’ll look at Reducer task performance, and we’ll follow that with some general task performance issues.
Much like map tasks, reduce tasks have their own unique problems that can affect performance.
Figure 6.13 shows the reduce task timeline with the units of work and potential areas impacting performance.
This timeline is conceptual - each time the reduce function emits data this results in a synchronous invocation of the RecordWriter.
In this section we’ll look at how common problems can affect the performance of reducer tasks.
For the most part, parallelism on the map side is automatically set, and is a function of your input files and the InputFormat you’re using.
But on the reduce side you have total control over the number of reducers for your job, and if that number is too small or too big, you’re potentially not getting the most value out of your cluster.
Problem You want to determine if a job runs slowly due to the number of reducers.
Solution The JobTracker UI can be used to inspect the number of reducers running for your job.
Discussion Use the JobTracker UI to look at the number of reducers for your job, as shown in figure 6.14
Use the JobTracker UI to look at the number of reducers for your job.
This number should ideally be set to a number smaller than the number of reduce slots in your cluster.
You can view the number of slots by going to the JobTracker UI, as shown in figure 6.15
Summary There are circumstances where you can’t avoid running with a small number of reducers, such as when you’re writing to an external resource (such as a database) that you don’t want to overwhelm.
A common anti-pattern in MapReduce is using a single reducer when you want job output to have total order, and not ordered within the scope of a reducer’s output.
Figure 6.15 The available slots in your cluster using the JobTracker UI.
If your job is writing to HDFS, you should make use of the available reduce slots in your cluster.
You can easily determine this number by looking at the JobTracker, as I showed in this technique.
The ideal number of reducers is equal to the number of reduce slots in your cluster—minus a few slots—so that if a certain percentage of nodes goes down you’ll still be able to run all the reducers in parallel.
If the number is significantly less than the number of available slots, you’re likely not maximizing the parallelization of your work.
If the number is greater than the number of reducers, your job execution time takes longer, because the additional number of reducers will need to execute after the first batch has completed.
Bear in mind this is a general and simple rule that works well when your cluster is running only a single job at a time.
In reality there are likely to be multiple jobs running concurrently, which makes it harder to come up with the ideal number of reducers for your job.
But the number of reduce slots works well as a general rule.
On the reduce side, data skew exists when there’s a disproportionately large number of map output values for a key, or when some values are significantly large in size compared to others.
Problem You want to determine if a job runs slowly due to skewed data.
Solution Use the JobTracker UI to compare the shuffled bytes across the reducers in your job to determine whether some reducers are receiving the bulk of the mapper outputs.
This technique also covers visualizing the map and reduce task runtimes to help understand potential data skew issues.
Discussion As with map-side, reduce-side data skew manifests itself as a small number of tasks taking disproportionately longer to execute than most of the others.
Figure 6.16 shows how you can use the JobTracker UI to identify data skew.
This approach works well for a quick sanity check on a potential data skew issue.
Visually examining task execution times can get you there much faster, which prompted me to write a simple utility that provides task-level statistics, including the input/output record counts and the sizes of inputs and outputs in bytes.
The output is broken into map and reduce sections, and each section contains three subsections, where results are ordered by execution time, number of input records, and the input size in bytes:
I also have a utility that will dump out a tab-separated file of task execution times (and input sizes), which you can use for plotting purposes to help eyeball problems.
In this illustration you’ll see that some map tasks are taking significantly longer than other tasks, but the reduce tasks all seem to be taking around the same amount of time.
The task counters provide the metrics including the reduce shuffle bytes, which are the.
If you click on the counters link for the running task, as well as one of the completed tasks, you can compare the counters.
Summary When you’ve identified that you have reducer skew, your next step should be to try and mitigate the effects of skew in your job.
Reduce tasks running slowly will contribute to the overall slowness of a MapReduce job.
There are a variety of issues that could cause reduce tasks to run slowly, some related to the user’s reduce code, and others related to hardware issues.
The challenge is in identifying whether your reduce tasks are running slower than you should expect.
Problem You want to determine if a job runs slowly due to low task throughput.
Solution Use the JobTracker UI or the job history metadata to calculate the throughput of your reduce tasks.
Discussion The reduce throughput can be calculated for individual tasks by using the JobTracker to get at the task execution times.
Figure 6.18 shows how to calculate throughput for reduce tasks.
The easiest way to get at the throughput statistics for all the reduce tasks is to execute my script against the job history file for your job, as shown in figure 6.19
Summary You have four throughput metrics to examine from the task throughput script, which will help you isolate if one aspect of the reduce task is significantly slower than the other.
In this technique you’ll focus on the reduce throughput; in the next technique we’ll look at the shuffle and sort phases.
Look at the average reduce task throughput, represented in Bps (bytes per second)
The reduce phase accounts for time taken to read the spilled map outputs on local disk, for the reduce code to operate, and for the reduce outputs to be written to the data sink.
Items that could contribute to low reduce throughput include the following:
Local disk issues can cause issues as the MapReduce framework reads the spilled local inputs and feeds them to the reduce code.
Separate techniques will be discussed for all of the items listed (apart from the last one) to help determine the exact cause of low reduce throughput.
The shuffle phase involves fetching the map output data from the TaskTrackers and merging them in the background.
The sort phase, which is another merge, will merge the files together into a smaller number of files.
Problem You want to determine if a job runs slowly due to the shuffle and sort phases.
Solution Use the job history metadata to extract statistics around the shuffle and sort execution times.
Discussion Figure 6.20 uses the job summary code to examine statistics around a job’s shuffle and sort times, and looks at some areas for potential improvements in their times.
Summary The simplest ways to cut down on shuffle and sort times is to use a combiner, and to compress your map outputs.
Both approaches reduce the amount of data flowing between the map and reduce tasks and lessen the network and CPU/disk burden related to the shuffle and sort phases.
You can also tune a variety of configuration settings to increase the sort buffer size, as well as the number of threads used on the reduce side and map side to transfer map outputs.
In this section we’ll look at problems that can affect both map and reduce tasks.
Look at the median and mean (average) shuﬀle and sort times for the reduce tasks…
The number of threads in the reducer is controlled with.
Imagine that you’ve tuned your job such that the map tasks are all working on large files that can be split, and your number of reducers is set to the number of reduce slots configured in your cluster.
Your job could still be running slowly due to other environmental issues, as you’ll see in this next technique.
Problem You want to determine if a job runs slowly due to other jobs running in the cluster.
Solution Compare the number of executing reduce tasks with your Hadoop cluster’s reduce task capacity.
Discussion If, based on previous techniques, you believe you’ve configured your job correctly, and the per-task throughput numbers look good, then your job slowness could be due to competition for resources on the cluster.
How can you make this determination? I have a couple of approaches to show you.
If the job is still running, go to the JobTracker and look at the number of concurrently executing map and reduce tasks for your job, and compare these numbers to the cluster capacity, as shown in figure 6.21
The figure shows how you’re clearly running with far fewer reducers than what’s configured for the cluster.
Summary The amount of parallelism available to you is a function of your cluster capacity, the other jobs running at the same time as your job, and any schedulers configured for your environment.
You can use the JobTracker UI to determine if other jobs are running at the same time as yours.
If this is the case‚ there’s no simple solution to your job’s throughput issues.
By default MapReduce uses a FIFO scheduler to determine how tasks should be scheduled if there are multiple jobs running at the same time.
If someone else submitted a job ahead of your job‚ that job will have its tasks scheduled ahead of your tasks.
Depending on the importance of your job you may want to use the Fair or Capacity Schedulers so that resources are more evenly distributed.
These schedulers can also be set up in a way such that they will give more cluster resources to some jobs over other jobs.
For example, many of the Java String tokenization techniques you’re accustomed to using are inefficient and can substantially increase the runtime of a job.
Problem You want to determine if a job runs slowly due to inefficiencies in your code.
Solution Determine the host and process ID of currently executing tasks, and take a number of stack dumps, which are subsequently examined, to narrow-down bottlenecks in your code.
Discussion The challenge with this technique is that MapReduce versions 1.0.0 and earlier don’t have metrics around how much time is spent in user code in the map and reduce phases.
The best approach to understanding time that’s being taken in your code is to update your code to time how long you spend in each task.
But in this technique you want to get a rough sense of whether this is an issue without having to change code.
Find the task attempt and the host it's running on.
In an earlier technique we looked at how to calculate the throughput of map tasks.
This calculation was a rough one based on the map execution times and the map input sizes.
You can take the same approach to estimate the throughput of reduce tasks.
If you have user code that’s not performing optimally, these throughput numbers will be low.
But they can be low for a variety of reasons (which were covered in that technique), and not as a result of your code.
Use the prior techniques to eliminate other potential issues in the cluster.
If you’re doing something inefficient in your code, chances are that you’ll be able to discover what it is by taking some stack dumps of the task process.
Figure 6.22 shows how to identify the job and task details so that you can take the stack dumps.
Now that you know the job name, task name, and the host it’s executing on, you can take your stack dumps and follow the next steps, as shown in figure 6.23
Summary Stack dumps are a primitive yet often effective means to discover where a Java process is spending its time, particularly if that process is CPU-bound.
Clearly they’re not as effective as using a profiler, which will more accurately pinpoint where time is being spent, but the advantage of stack dumps is that they can be performed on any running Java process, whereas if you were to use a profiler, you’d need to re-execute the process with the required profiling JVM settings—a nuisance in MapReduce.
Wait a few seconds between each execution of the kill.
View the contents of the task's output file and determine if time is being spent in a particular method call.
View the contents of standard out, which will contain the 3 stack dumps.
When taking stack dumps, it’s useful to take multiple dumps with some pauses in between successive dumps.
This allows you to visually determine if the code execution stacks across multiple dumps are roughly in the same point.
If this is the case, there’s a good chance the code in the stack is what’s causing the slowness.
If your code isn’t in the same location across the different stack dumps, this doesn’t necessarily indicate that there aren’t inefficiencies.
In this case the best approach is to profile your code or add some measurements in your code and rerun your job to get a more accurate breakdown of where time is being spent.
Modern hardware is generally reliable with mean time to failure (MTTF) values that typically span multiple years.
But when you’re working with clusters, the overall MTTF drops significantly; clusters with hundreds of nodes should expect one failure or more per week.
In this section we’ll look at how to determine if your CPU, memory, disks, and networks are overutilized, and what you can do to bring them down to reasonable operating levels.
Nodes can fail due to issues such as disk controller failures, disk space issues, and other hardware issues.
It’s also possible that a bug in Hadoop causes tasks to fail, although this is less likely.
When this happens it can have an adverse effect on job execution times, particularly in smaller clusters.
We’ll look at how to determine if nodes are failing in your cluster.
Problem You want to determine if a job runs slowly due to hardware problems, and you want to see if nodes are blacklisted or graylisted.
Solution Use the JobTracker UI to check for nodes that are graylisted or blacklisted.
Discussion Go to the JobTracker UI and check to see if there are any blacklisted or graylisted nodes.
Figure 6.24 shows where you should look to keep tabs on nodes.
Nodes are typically scheduled on blacklisted due to a persistent hardware issue.
The JobTracker still schedules work on graylisted nodes, but will eventually blacklist them if tasks continue to fail.
If nodes are blacklisted or graylisted the cause of the problem can sometimes be determined by looking at task and TaskTracker or DataNode log files.
In any case, Operations should be contacted to investigate the failing nodes.
It’s good to set up monitoring for blacklisted and graylisted nodes so that you can quickly react to node failures.
An example of a Nagios monitor that does this can be seen at http://goo.gl/R6deM.
It uses curl to download the blacklisted web page from the JobTracker using http://127.0.0.1:50030/machines.jsp?type=blacklisted (this URL is intended to be fetched on the same host as the JobTracker) and checks that no blacklisted nodes exist.
If your nodes are overutilized from a CPU perspective, your overall computational throughput suffers because the OS spends more time context switching than it does performing work.
Problem You want to determine if a job runs slowly due to CPU overutilization.
Solution Use the Linux tool vmstat to observe the CPU context switches.
Discussion vmstat is an excellent tool to quickly gauge how busy a host is in terms of CPU utilization, I/O wait, and context switching.
Figure 6.25 shows how it can be executed and explains some options.
The first row contains averages since the last time the host was rebooted.
Figure 6.26 depicts some of the items you should watch.
Summary The longer the Linux run queue, which is the number of processes waiting for CPU time, the more time the Linux kernel spends context switching from one process to another.
Each context switch is expensive because it requires the CPU to save the state of the existing process and load the state of the next process.
Therefore, excessive context switching means that you’re possibly running too many tasks on your host.
Sometimes the TaskTracker can become CPU pegged, which can cause high CPU loads on your server.
Use top to determine if the TaskTracker is CPU bound, and if it is consider bouncing the TaskTracker process.
In addition, procinfo is a handy tool to use in situations where you want to figure out what devices are causing high interrupt numbers, and sar is useful for collecting and saving system data into files for later use.
Finally, mpstat is useful because it gives statistics broken down by CPU, and not aggregated across all the CPUs, as other commands will do.
Swapping happens when you start exceeding the physical limits of memory on your hosts, which makes the OS start to use disk as an overflow mechanism for memory.
A high number of interrupts can indicate a problem with hardware.
If this is consistently larger than the number of processes and the CPU idle time is low, then the host is CPU-bound and is considered to have a high load.
If this number is high, then the CPU is busy spending a lot of time saving and restoring process.
Figure 6.26 Annotated vmstat output helps you watch your host.
Figure 6.28 looks at some of the items to watch.
Rate at which memory has been swapped in from disk since the last sample.
System buffers that contain data not yet flushed to disk.
Rate at which memory has been swapped out to disk since the last sample (MiB/s)
Swapping is bad, and if these numbers are greater than 0 it means data is being transferred between.
Having a large cache size is fine as it means the OS is caching data that.
If the swpd value is greater than 0, and the free/buff/cache values are also small, then the OS doesn't have enough physical memory for the applications that are executing, and is swapping memory to disk.
A Hadoop cluster under load puts a lot of stress on disks and disk controllers.
In this section we’ll look at how to evaluate whether your drives are healthy, and also investigate if your drives have been remounted as read-only due to I/O errors.
Problem You want to understand if a drive runs in degraded or read-only mode.
Solution The Linux tool iostat can be used to look at drive request queues and IO wait times.
Other Linux tools such as dmesg can help determine if a drive has gone into a readonly mode.
Discussion Out of all the tools in Linux‚ iostat is the most comprehensive one for disk and device-level statistics.
Figure 6.29 shows how it can be executed and explains some options.
Figure 6.30 looks at the output of iostat and highlights some of the items to watch for.
Another important thing to watch for is a filesystem going into read-only mode.
This can happen due to bugs in disk or RAID controllers in Linux, and will debilitate the node.
If this happens your TaskTracker and DataNode logs will contain errors complaining that they can’t write to disk.
The first report generated by the iostat command provides statistics concerning the time since the system was booted.
Each subsequent report contains statistics collected during the interval since the previous report.
Summary Too much load on disks can cause read and write requests to get queued, increasing I/O wait times, which has the effect of slowing down a node.
Further, the Linux kernel will mount drives as read-only if I/O errors related to the drive are encountered, which, depending on your drive setup, can cripple your node.
You can use hdparm to measure the sequential read rate of your hard disk.
Replace /dev/md1 with the name of your disk device (consult /etc/fstab for device names):
Percentage of CPU time during which I/O requests were issued.
Device saturation occurs when this  value is close to 100%
The average time it took to service a request after it was popped.
If the delta between await and svctm becomes significant, this signals that the disk isn't keeping up with requests.
The number of megabytes read (r) and written (w) from the device per second.
If the DataNodes are exhibiting I/O wait issues, you may want to install additional hard drives on each node and configure dfs.data.dir to use multiple disks, which will spread the read and write load across them.
A networking problem will have a big impact on the performance of MapReduce and HDFS, because both are heavily reliant on data transfer between nodes.
In this technique we’ll look at ways to ensure you’ve configured your nodes correctly, and also look at how you can test the network throughput between nodes.
Problem You want to determine if a job runs slowly due to network issues.
Solution Examining the output of the Linux tools ethtool and sar can help diagnose network mis-configurations.
Compare this value with the advertised speed of your ethernet card and switch.
Figure 6.31 shows how to use ethtool to get that information.
The ethtool command will give you the totals for items such as transmitted bytes.
To view differences in network card metrics as they happen you can use sar, as shown in figure 6.32
Figure 6.33 looks at the output of sar and highlights metrics that could indicate network issues.
Total number of bytes received (rx) and transmitted (tx) per second.
Number of collisions that happened per second while transmitting packets.
Summary In this technique we looked at ways to verify that you configured your network cards correctly, and also looked at tools to capture network metrics.
I also explained how to measure network bandwidth to test your network card and switches.
In MapReduce it’s useful to measure the execution time of the various MapReduce stages, as shown in figure 6.35
Unfortunately, as of the 0.20.x release Hadoop doesn’t measure and expose all of these stages.
You c direct the lo your g script TECHNIQUE 44 Extracting and visualizing task execution times.
We’ll look at how to use the job history metrics to retrieve a timeline of the number of tasks running during the course of the job execution.
Visually examining your job can be useful in identify issues such as straggling tasks and low task concurrency numbers.
Problem You want to visualize the performance characteristics of a slow job rather than comb through a bunch of numbers.
Solution Metrics can be extracted from the job history metadata and plotted with graphing software to visually inspect map, reduce, sort, and shuffle execution times.
Discussion To execute your code you must determine the location of the history file for a job you want to analyze, or the job output directory in HDFS must still exist (see section 6.1.1 for more details):
You’re using the Linux graphing utility gnuplot4 to plot your data in a stacked histogram form.
The tab-separated data file is written to standard error, and here you pipe it.
Run gnuplot, which will generate a PNG image file called mr.png.
With the histogram displayed, you now need to use it to narrow down where your performance problem may exist.
There are two main things you should be looking out for.
First, because the x-axis is the timeline for your job, you should use it to determine if there’s a particular stage in MapReduce that’s taking up a large part of the job execution time.
This isn’t an exact science, but if, for example, you have a CPU-bound process, you may see large periods of time spent executing your MapReduce code.
Second, the ideal MapReduce histogram is one where both the map and reduce phases have consistently high numbers of concurrent tasks.
If there’s a long tail in the map and reduce phases, this can indicate some data skew issues, where map and reduce tasks are receiving a disproportionate number of records.
Figure 6.38 shows three examples of task execution graphs, two of which have lessthan-ideal patterns, and the third of which is the ideal graph.
Gnuplot version 4.4 was used to generate the graphs in this section.
If you encounter syntax errors or appearance differences this may be a result of running a different version of gnuplot.
Summary Visualizing MapReduce job and task metrics is a useful way to examine and make observations about your data.
I’ve compiled a number of other visualizations to be made from the history files, which can help you better understand your jobs, including the following:
Plotting the start and end execution times for tasks, to give a sense of how long they’re taking to run.
Correlating the input and output data sizes with the amount of time it takes for a task to run.
Plotting the number of tasks for each slave node executing over time to help identify hosts that aren’t having many tasks allocated to them.
One of the challenges to working with the history file format is that it’s unstructured.
If your histogram indicates that your map and reduce code is taking a long time, your next step is to try and determine what’s causing the slowdown in your application code.
This section and the previous section focused on helping identify potential performance problems while running a MapReduce job.
The next section looks at solutions to common performance problems.
In the previous sections we looked at how you could narrow down where your performance problems lie.
This section is all about profiling and tuning your jobs to extract as much performance out of them as you can.
We’ll look at how to improve the performance of the shuffle and sort phases, and several other tips that will help cut down on your job execution times.
The Map phase is disproportionately longer than the reduce phase.
This may be OK, but it may also indicate that there may be some user map code needs to be optimized.
You don profile tasks, b your ex uses th reducer.
When you suspect that there’s something awry in your map or reduce code that’s causing slow execution times, it’s time to break out the profiler.
Profiling standalone Java applications is straightforward and well supported by a large number of tools.
In MapReduce you’re working in a distributed environment running multiple map and reduce tasks, so it’s less clear how you would go about profiling your code.
Problem You suspect that there are inefficiencies in your map and reduce code, and need to identify where they exist.
Solution Use HPROF in combination with a number of MapReduce job methods such as setProfileEnabled to profile your tasks.
Discussion Hadoop has built-in support for the HPROF profiler, Oracle’s Java profiler built into the JVM.
This will generate object allocation stacks sizes that are too small to be useful, so instead you’ll programmatically set custom HPROF parameters:
It parses a file containing IP addresses and extracts the first octet from the IP and emits it as the output value:
He you’re running with a stack depth o eight, to help you tie the bottom o.
This method sets the range of tasks that will be profiled.
Th is useful because it allows you to profile a small subsection o.
As you can see, the grammar is flexible an allows individual task numbers as well as ranges to be defined.
The first flag is a Boolean that indicates whether the range being specified for the map or reduce task.
You’ll upload a large(ish) file of IP addresses and run your job against it, with the previous profiling options set:
The HPROF option you used will create a text file that can be easily parsed.
The file contains a number of stack traces, and at the bottom contains memory and CPU time accumulations, with references to stack traces that accounted for the accumulations.
In the example you ran you’ll look at the top two items, which accounted for the most CPU times, and correlate them with the code:
The two issues identified at the top of the CPU times, the String.split method and the Text constructor, are both addressed later in this section.
The job copies the profile files from the remote TaskTrackers to the local.
The stack trace that had the mo accumulated time has a trace ID o.
You use this ID to sear for the stack in the fi.
The item that is the second-most utilized in the task is in the constructor of the Text object, and the overhead of creating the.
Summary Running HPROF adds significant overhead to the execution of Java; it instruments Java classes to collect the profiling information as your code is executing.
This isn’t something you’ll want to regularly run in production.
You can tune a number of settings in Hadoop depending on your hardware resources, and the types of MapReduce jobs you are running, as you’ll see in table 6.1
Table 6.1 Configurations that can be tuned for improved performance.
Map io.sort.mb 100 The amount of memory reserved to store and sort map outputs, in megabytes.
This should be approximately 70% of the map task’s heap size.
Map, Reduce io.sort.factor 10 The number of files to merge together in a single pass.
This should be approximately 70% of the reducer’s heap size.
On sites with heterogeneous hardware this should be tuned for each node stereotype in the cluster.
Table 6.1 Configurations that can be tuned for improved performance (continued)
The shuffle stage in MapReduce is an expensive one, which can incur high network utilization when transferring data between map and reduce tasks.
The overhead of sorting and merging can also be significant.
The goal of this section is to provide a number of techniques to help mitigate the overhead of the shuffle phase.
Reducers are useful mechanisms when you want to join data together, but they come at the expense of transferring data over the network.
Problem You want to consider not using the reduce phase in MapReduce.
Solution Set the MapReduce configuration parameter setNumReduceTasks to 0 to run a maponly job.
Discussion The shuffle and sort phases are typically used to join data together.
But as you saw in chapter 4, some joins can be performed on the map side if certain conditions hold true about your data.
In such situations you can run a map-only job, with no reducers.
This is easily accomplished by setting the number of reducers to 0:
Summary A map-only job uses the same OutputFormat that the reducer uses to write the job outputs, as shown in figure 6.39
Map outputs are fetched by each reducer over the network, and merged together.
Figure 6.39 Comparing a MapReduce job with a map-only job.
If you can’t run without a reducer in your job, the next step is to minimize the impact it has on your job execution time.
Much of the data emitted from the map to the reduce tasks is transferred over the network, which is expensive.
Problem You want to cut down on data being shuffled.
Solution Reduce the size of the map output records, and aggressively omit records from being outputted by the mapper.
Discussion Filtering and projecting are relational concepts to cut down on data being processed.
These concepts also apply in MapReduce as a way to minimize the data that map tasks emit.
The following code is an example of both concepts in action:5
Summary The use of filtering and projection are two of the easiest mechanisms that you can employ to dramatically reduce the runtime of your MapReduce jobs.
If you’ve ensured that you’re performing as much filtering and projection as possible in your map task, the next step to the shuffle and sort optimization is to look at using the combiner.
The combiner is another mechanism you can use to cut down on data traveling between map and reduce tasks.
Projection of the first two octets of the IP address.
The combiner is a powerful mechanism to aggregate data in the map phase to cut down on data sent to the reducer.
It’s a map-side optimization where your user code is invoked with a number of map output values for the same output key.
Problem You’re filtering and projecting your data, but your shuffle and sort are still taking longer than you want.
Solution Define a combiner and use the setCombinerClass method to set it for your job.
Discussion The combiner is invoked on the map side as part of writing map output data to disk in both the spill and merge phases, as shown in figure 6.40
To help with grouping values together to maximize the effectiveness of a combiner, use a precursory sorting step in both phases prior to calling the combiner function.
Calling the setCombinerClass sets the combiner for a job, similar to how the map and reduce classes are set:
In this technique you’ll build upon the IP filter and projection code that you wrote in the previous technique, and combine records together where the second octet is identical:6
It’s important that if you have a combiner, the function is distributive.
In figure 6.40 you saw that the combiner will be called multiple times for the same input key, and there are no guarantees about how the output values will be organized when they’re sent to the combiner (other than that they were paired with the combiner key)
A distributive function is one where the end result is identical regardless of how inputs were combined.
Summary The combiner is a powerful tool in your MapReduce toolkit, as it helps cut-down on the amount of data transmitted over the network between the mappers and reducers.
Another tool to improve the execution times of your MapReduce jobs are binary comparators, which we’ll examine next.
When MapReduce is sorting or merging it leverages the RawComparator for the map output key to compare keys.
Built-in Writable classes (such as Text and IntWritable) have byte-level implementations that are fast because they don’t require the byte form of the object to be unmarshalled to Object form for the comparison.
When writing your own Writable, it may be tempting to implement the WritableComparable interface, but this can lead to longer shuffle and sort phases because it requires Object unmarshalling from byte form for comparisons.
Problem You have custom Writable implementations and you want to reduce the sort times for your jobs.
Solution Write a byte-level comparator to ensure optimal comparisons during sorting.
Much like a reducer, the combiner will be called with multiple values.
You only output a key/value pair if you detect a new value.
The MapReduce framework reuses iterator value objects supplied to combine.
The argum the s being Discussion In MapReduce there are multiple stages where output keys are compared to each other when data is being sorted.
To facilitate key sorting, all map output keys must implement the WritableComparable interface:
The trouble with this Comparator is that MapReduce stores your intermediary map output data in byte form, and every time it needs to sort your data it has to unmarshall it into Writable form to perform the comparison.
This unmarshalling is expensive because it recreates your objects for comparison purposes.
If you look at the built-in Writables in Hadoop, not only do they extend the WritableComparable interface, but they also provide their own custom Comparator that extends the WritableComparator class.
The following code presents a subsection of the WritableComparator class:
The b1 field contains a byte array, part which contains the WritableComparable in b form.
To write a byte-level Comparator the compare method needs to be overridden.
Let’s look at how the IntWritable class implements this method:
The built-in Writable classes all provide WritableComparator implementations, which means you don’t need to worry about optimizing the Comparators as long as your MapReduce job output keys use these built-in Writables.
But if, as in the earlier example, you have a custom Writable that you use as an output key, you’ll ideally provide a WritableComparator.
We’ll now revisit your Person class and look at how to do this.
Return of the on the f In your Person class you had two fields, the first and last names.
Your implementation stored them as strings, and used the DataOutput’s writeUTF method to write them out:
The first thing you need to understand is how your Person object is represented in byte form, based on the previous code.
The writeUTF method writes two bytes containing the length of the string, followed by the byte form of the string.
Figure 6.41 shows how this information is laid out in byte form.
Assume that you want to perform a lexicographical comparison that includes both the last and first names, but you can’t do this with the entire byte array because the string lengths are also encoded in the array.
Instead, the Comparator needs to be smart enough to skip over the string lengths.
Read the size of the last name from the first byte array.
Read the size of the last name from the second byte array.
Summary Using the writeUTF is limited because it can only support strings that contain less than 65,536 characters.
This is probably fine for the scenario where you’re working with people’s names.
If you need to work with a larger string, you should look at using Hadoop’s Text class, which can support much larger strings.
If you look at the Comparator inner class in the Text class you’ll see that its binary string comparator works in a similar fashion to yours.
Your approach could be easily extended to work with names represented with Text objects rather than Java String objects.
The next section in performance tuning is a look at how you can guard against the impact that data skews can have on your MapReduce jobs.
There are always outliers that will cause your data to be skewed, and these outliers can significantly impede the progress of your MapReduce jobs.
Skewed data frequencies—Where there are a disproportionately large number of records in a dataset.
Skewed record sizes—Where some records are significantly larger than the average record size.
Data skews can exist on both the map and reduce sides.
On the map side, skews can exacerbate matters when working with heterogeneous datasets.
On the reduce side, the default MapReduce partitioner can create unintended data skew.
Skews are bad because the map and reduce tasks that operate on datasets that contain skews can take much longer to complete than other tasks.
A secondary problem that arises with skewed data is that the memory consumption of tasks can be adversely affected.
This is of concern when you’re performing actions such as joins, which require data to be cached.
In this section we’ll examine a number of techniques to help you identify what data is skewed, and how you can mitigate the effect.
When using some of the earlier techniques, you may have determined that you have map or reduce data skew.
The next step is to find out what keys are skewed.
We’ll focus on data skew issues on the reduce side resulting from a large number of values for specific map output keys.
Problem You want to identify the map output keys that are causing data skews.
Solution Add intelligence to your reduce method to log details about a larger number of map outputs for a given map output key.
Discussion When you realize you have skewed data, it can be helpful to understand what keys are causing the skews.
An easy way to do this is to keep track of the keys with the most values in your code.
You can drop a configurable threshold into your code, and after you receive more values than your threshold, you dump out the key to the logs:9
After you run your job you can look at the logs to determine what keys are skewed, and by how much.
Read the configurable number of max values for a key.
Dump the key name and the number of values to the log file.
Summary Keeping track of your skewed data is an important step to better understanding your data, which in turn will help you when it comes to designing your MapReduce jobs.
Next up is a look at how to guard against reducer skew.
Reducer skew is typically a result of a number of map output keys with disproportionately large numbers of associated map output values.
Problem You want to look at ways to mitigate reduce-side data skew problems.
Solution Examine a number of techniques to cut down on the risk of data skew, such as using custom partitioners and using map-side joins.
Discussion In this solution we’ll cover a variety of approaches to guard against reducer data skew.
The default partitioner in Hadoop partitions based on a hash of the map output key.
This works great if your output keys have a somewhat even frequency distribution, but doesn’t work so well if your data has skewed key frequencies.
What you need in such situations is a partitioner that operates based on some knowledge of your data.
This same technique will work well at counteracting skewed key frequencies in your data.
An alternative approach to sampling and range partitioning would be to build a custom partitioner based on explicit knowledge of the output keys.
For example, if you have a MapReduce job whose output keys are the words in a book, you would expect to see high frequency stopwords.10 Your custom partitioner could send such output to a fixed set of reducers, and partition all other words to the remaining reducers.
Using a combiner can go a long way towards reducing both skewed key frequencies and skewed record sizes.
The goal of the combiner is to aggregate and condense data if possible.
Stop words are common words frequently filtered out as part of natural language processing; see http://en.wikipedia.org/wiki/Stop_words.
Skewed record sizes in combination with any reduce- or map-side caching can be particularly problematic as they frequently result in the dreaded OutOfMemoryError exception in Java.
You won’t find any easy way to handle these situations.
Summary Next up we’ll take a look at user space MapReduce code and constructs in Java that can have a detrimental impact on performance.
We’ll also look at some patterns to optimize your code.
MapReduce’s execution patterns exercise code in different ways than what you may be accustomed to with other Java applications.
This is largely due to the fact that the MapReduce framework is efficient at processing vast amounts of data, which results in map and reduce functions being called millions of times over short periods of times.
The end result is that you frequently uncover undesired performance traits in common libraries, including Java’s JDK.
In technique 45 we uncovered how to profile MapReduce code to understand where your code is spending time.
Using this technique we identified issues related to the following code:
Joshua Bloch’s excellent book Effective Java contains many useful tips to help write efficient Java code.
This was the line that was consuming most of the CPU for your job, as a.
In this section we’ll discuss the two issues discovered in the previous code (regular expressions in Java, and lack of code reuse), as well as some other general Java gotchas in MapReduce.
Regular expressions in Java have a rich and flexible feature set.
But the flexibility comes at the price of execution speed.
The slowness of regular expressions in Java were exposed in technique 45, where the profile showed that most of the time in your map was spent in the Pattern.split code.
As a general guideline, it’s best to avoid regular expressions where possible in MapReduce applications.
If they can’t be avoided outright, it’s still worth factoring out as many uses of them as possible.
Java’s documentation recommends the use of the String.split and Scanner classes for string tokenization.
You may be surprised to know that both classes use regular expressions behind the scenes, which as you’ve found out are slow.
Ironically, the next step may be to fall back on the StringTokenizer class, use of which is discouraged in the class Javadocs comments in favor of the newer regular expression-based tokenizers.
But the implementation of StringTokenizer isn’t optimal, and benchmarks comparing all of the previous methods with the Apache commons StringUtils class in figure 6.42 show that using StringUtils is the way to go.
The second item taking up CPU times in your HPROF output was the following line in your code, which you were executing for each key/value pair fed into the map method:
The issue with this code is again a consequence of it being called many times.
Object allocation in Java is expensive, incurring CPU overhead at construction time as well as during garbage collection.
The following shows how this code should be written to maximize reuse:
This line was the second-biggest CPU hog as a result of constructing the Text object for.
Hadoop also reuses objects in its own code, which results in a gotcha to watch out for in the reducer.
When the reducer feeds data into the values iterator it reuses the objects.
If you’re caching any of the value data in the reducer you’ll need to clone the object, as shown in the following code:
WritableUtils is a Hadoop helper class that contains numerous methods to make working with Writable.
The clone method uses Hadoop’s serialization to make a copy of the Writable, so that your copy doesn’t get overwritten by the.
One of the oldest rules in Java has always been that string concatenation using the plus operator should be avoided at all cost, which the following code example shows:
The reason it was discouraged is that older compilers would generate bytecode using the StringBuffer class, whose synchronization slowed down its execution.
In recent years, however, the generated bytecode replaced StringBuffer with its nonsynchronized cousin StringBuilder.
Does that mean that the plus operator is now safe to use? The answer is complicated, and is a yes in situations where the resulting String length is less than 16 characters.
A detailed analysis of this topic is available at http://goo.gl/9NGe8, but the bottom line is that the safest bet is to use the StringBuilder class, and ideally use the constructor to preallocate enough space for the resulting String to avoid reallocation of the internal byte array in StringBuilder.
In Java caching data is expensive, so let’s briefly get a sense of how much space is taken up when working with common Strings and arrays.
To start things off, what do you think is the memory usage of the following code?
Let’s go through the exercise of calculating the memory usage of the previous ArrayList and its contents:
Therefore the ArrayList starts off with 8 bytes in size.
The ArrayList is occupying 24 bytes of memory and we’ve not yet stored anything in it.
Let’s look at the memory usage for the Object array within the ArrayList:
Each element in the array requires 4 bytes for the object reference.
Finally, you need to understand the memory usage of Strings in Java, which are calculated with the following formula:
The grand total of memory usage for storing the two strings in the ArrayList is 128 bytes!
The reason I went through this exercise is to build awareness of the overhead incurred in Java to cache data.
This is also relevant in MapReduce because you often want to cache data, and it’s useful to be able to accurately calculate the memory usage when caching data.
More details for memory usage in Java can be seen at http:// goo.gl/V8sZi.
How you store and transfer your data has an impact on performance.
In this section we’ll briefly look at data serialization best practices to squeeze the most performance out of Hadoop.
You can lessen your data storage footprint when job outputs are compressed and also accelerate the speed at which the data is ingested in a downstream MapReduce job.
It’s also important to compress the map outputs to reduce the network I/O between map and reduce tasks.
Much like compression, using binary file formats such as Avro and SequenceFiles results in a more compact representation of your data, and yields improved marshalling and unmarshalling times compared to storing data as text.
I dedicated a large part of chapter 3 to working with these file formats.
Even when the end result of your work in MapReduce is a nonbinary file format, it’s good practice to store your intermediate data in binary form so that you can reap the benefits of your data being in binary form.
For example, if you have a MapReduce pipeline where you have a sequence of MapReduce jobs executing, you should consider using Avro or SequenceFile to store your individual job outputs.
The last job that’s producing the final results can use whatever output format is required for your use case, but intermediate jobs use a binary output format to speed up the writing and reading parts of MapReduce.
In this chapter we covered the three essential ingredients necessary to tune a job in MapReduce:
Fixing common performance problems by looking at MapReduce/HDFS configurations, optimizing MapReduce shuffle/sort phases, and also some user space MapReduce performance techniques.
In part 4, we’ll look at Hadoop in the context of data science, and how you can model complex data structures and perform data-mining activities over data.
The ultimate challenge to working with Hadoop and big data is how to mine useful information about your data.
The objective of this part of the book is to present techniques to address nontrivial questions asked about your data, and to create new insights into your data.
Data modeling and algorithms are the pillars on which data science is built, and chapter 7 examines how graphs can be represented and utilized in MapReduce to implement algorithms such as Friends-of-Friends and PageRank.
Chapter 8 explores how R and MapReduce can work in concert to quickly bring data scientists to the Hadoop table.
Chapter 9 covers the three C’s of Mahout: clustering, classification, and collaboration.
These three topics offer different data mining approaches that work with MapReduce.
In this chapter we’ll look at how you can implement algorithms in MapReduce to work with internet-scale data.
We’ll focus on nontrivial data, which is commonly represented using graphs.
We’ll also look at how you can use graphs to model connections between entities, such as relationships in a social network.
We’ll run through a number of useful algorithms that can be performed over graphs, such as the shortest path algorithm, friends-of-friends (FoF) to help expand the interconnectedness of a network, and PageRank, which looks at how to determine the popularity of web pages.
You’ll learn how to use Bloom filters, whose unique space-saving properties make them handy to solve distributed system problems in P2P (peer-to-peer) and distributed databases.
We’ll also create Bloom filters in MapReduce, and then use them to optimize joins in MapReduce.
A chapter on scalable algorithms wouldn’t be complete without mention of statistics and machine learning.
Let’s kick things off with a look at how to model graphs in MapReduce.
Graphs are mathematical constructs that represent an interconnected set of objects.
They’re used to represent data such as the hyperlink structure of the internet, social networks (where they represent relationships between users), and in internet routing to determine optimal paths for forwarding packets.
A graph consists of a number of nodes (formally called vertices) and links (informally called edges) that connect nodes together.
The edges can be directed (implying a one-way relationship), or undirected.
For example, you would use a directed graph to model relationships between users in a social network because relationships are not always bidirectional.
In cyclic graphs it’s possible for a vertex to reach itself by traversing a sequence of edges.
In an acyclic graph it’s not possible for a vertex to traverse a path to reach itself.
Modeling data and solving problems with graphs you’ll need to be able to represent them in your code.
So what are the common methods used to represent these graph structures?
Two common ways of representing graphs are with adjacency matrices and with adjacency lists.
With an adjacency matrix, you represent a graph as an N x N square matrix M, where N is the number of nodes, and Mij represents an edge between nodes i and j.
Figure 7.4 shows a directed graph representing connections in a social graph.
The adjacency matrix shows how this graph would be represented.
The disadvantage of adjacency matrices are that they model both the existence and lack of a relationship, which makes them a dense data structure.
Adjacency lists are similar to adjacency matrices, other than the fact that they don’t model the lack of relationship.
Figure 7.5 shows how you’d represent a graph using an adjacency list.
The advantage of the adjacency list is that it offers a sparse representation of the data, which is good because it requires less space.
It also fits well when representing graphs in MapReduce because the key can represent a vertex, and the values are a list of vertices that denote a directed or undirected relationship node.
Next up we’ll cover three graph algorithms starting off with the shortest path algorithm.
The shortest path algorithm is a common problem in graph theory, where the goal is to find the shortest route between two nodes.
Figure 7.6 shows an example of this algorithm on a graph where the edges don’t have a weight, in which case the shortest path is the.
Applications of this algorithm include traffic mapping software to determine the shortest route between two addresses, routers that compute the shortest path tree for each route, and social networks to determine connections between users.
Dijkstra’s algorithm is a shortest path algorithm commonly taught in undergraduate computer science courses.
A basic implementation uses a sequential iterative process to traverse the entire graph from the starting node, as seen in the algorithm presented in figure 7.7
The basic algorithm doesn’t scale to graphs that exceed your memory sizes, and it’s also sequential and not optimized for parallel processing.
Problem You need to use MapReduce to find the shortest path between two people in a social graph.
Solution Use an adjacency list to model a graph, and for each node store the distance from the original node, as well as a backpointer to the original node.
Use the mappers to propagate the distance to the original node, and the reducer to restore the state of the graph.
Iterative process where all the unvisited nodes are iterated, and the distance from the start node is.
All nodes other than the starting node start with a distance of infinity, denoting.
Discussion Figure 7.8 shows a small social network, which you’ll use for this technique.
Your goal is to find the shortest path between Dee and Joe.
There are four paths that you can take from Dee to Joe, but only one of them results in the fewest number of hops.
You’ll implement a parallel breadth-first search algorithm to find the shortest path between two users.
Because you’re operating on a social network, you don’t need to care about weights on your edges.
The pseudo-code for the algorithm can be seen in figure 7.9
Figure 7.10 shows the algorithm iterations in play with your social graph.
Just like Dijkstra’s algorithm, you’ll start with all the node distances set to infinite, and set the distance for the starting node, Dee, at zero.
With each MapReduce pass, you’ll determine nodes that don’t have an infinite distance and propagate their distance values to their adjacent nodes.
This is done by reading in the social network (which is stored as an adjacency list) from file and setting the initial distance values.
Figure 7.11 shows the two file formats, the second being the format that’s used iteratively in your MapReduce code.
Figure 7.9 Pseudo-code for breadth-first parallel search on graph using MapReduce.
Your first step is to create the MapReduce form from the original file.
The following listing shows the original input file, and the MapReduce-ready form of the input file generated by the transformation code:
First iteration where all the unvisited nodes are iterated, and.
Figure 7.11 Original social network file format and MapReduce form optimized for algorithm.
The code that generates the previous output is shown here:1
The structure of the MapReduce data isn’t changed across iterations of the algorithms; each job produces the same structure, which makes it easy to iterate, because the input format is the same as the output format.
First, it outputs all the node data to preserve the original structure of the graph.
If you didn’t do this, you couldn’t make this an interactive process because the reducer wouldn’t be able to reproduce the original graph structure for the next map phase.
The second function of the map is, for all adjacent nodes, to output each adjacent node with its distance and a backpointer, if the node has a non-infinite distance number.
The backpointer carries information about the nodes visited from the starting node, so that when you reach the end node you know the exact path that was taken to get there:2
The MapReduce-ready form of the input data, with the addition of a numeric that indicates the number of hops.
When outputting the original input node, as well as the adjacent nodes and the distances to them, the format (not contents) of the map output value are identical, to make it easier for your reducer to read the data.
To do this you use a Node class to model the notion of a node, its adjacent nodes, and the distance from the starting node.
Its toString method generates a string form of this data, which is used as the map output key, as shown in the following listing.3
The Node class, which will help with serialization in the MapReduce code.
Calculate the backpointer, which is simply existing node’s backpointer with the node’s n.
Your reducer’s job is to calculate the minimum distance for each node, and to output the minimum distance, the backpointer, and the original adjacent nodes.
The counter enum you’ll use to set the number of hops when you’ve reached the target node.
You need to copy the input file into HDFS, and then kick off your MapReduce job, specifying the start node name (dee) and target node name (joe):
If the node represents the original node (with adjacent nodes), preserve it.
If the distance to this node from an adjacent node is less than the minimum.
Store the minimum distance and backpointer from the adjacent node.
If the current node is the target node, and you have a valid distance value, you’re done and you.
The output of your job shows that the minimum hops between Dee and Joe is 2, and that Ali was the connecting node.
Summary This exercise showed how a shortest path algorithm could be used to determine the minimum number of hops between two people in a social network.
An algorithm related to the shortest path algorithm, called graph diameter estimation,5 attempts to determine the average number of hops between nodes.
This has been used to support6 the notion of six degrees of separation in large social network graphs with millions of nodes.
The shortest path algorithm has multiple applications, but an arguably more useful and utilized algorithm in social networks is that of friends-of-friends (FoF)
Social network sites such as LinkedIn and Facebook use the FoF algorithm to help users broaden their networks.
The Friends-of-friends (FoF) algorithm suggests friends that a user may know that aren’t part of their immediate network.
The key ingredient to success with this approach is to order the FoFs by the number of common friends, which increases the chances that the user knows the FoF.
Problem You want to implement the FoF algorithm in MapReduce.
Solution Two MapReduce jobs are required to calculate the FoFs for each user in a social network.
The first job calculates the common friends for each user, and the second job sorts the common friends by the number of connections to your friends.
Discussion You should first look at an example graph and understand what results you’re looking for.
Figure 7.13 shows a network of people with Jim, one of the users, highlighted.
Next to Jim’s FoFs are the number of friends that the FoF and Jim have in common.
Your goal here is to determine all the FoFs and order them by the number of fiends in common.
Therefore, your expected results would have Joe as the first FoF recommendation, followed by Dee, and then Jon.
The text file to represent the social graph for this technique is shown here:
The first job, the pseudocode for which is shown in figure 7.14, calculates the FoFs and counts the number of friends in common.
The result of the job is a line for each FoF relationship excluding people who are already friends.
The output for executing this job against the graph in figure 7.13 is shown below:
Your second job needs to produce output such that for each user you should have a list of FoFs in order of common friends.
You’re using secondary sort to order a user’s FoFs in order of common friends.
The output for executing this job against the output of the previous job can be seen here:
Let’s dive into the code in the following listing and look at the first MapReduce job, which calculates the FoFs for each user.7
Figure 7.15 The second MapReduce job, which sorts the common friends by the number of friends in common.
For each friend, emit the fact that friends so that this relationship.
For each friend, go through the remaining friends and emit the fact that they’re a FoF.
The job of the second MapReduce job in the following listing is to sort the FoFs such that you see FoFs with a higher number of mutual friends ahead of those that have a smaller number of mutual friends.8
Mapper and reducer implementations that sorts FoFs by the number of shared common friends.
Output the fact that they’re FoFs, including a count of common friends.
I won’t show the whole driver code, but to enable secondary sort, I had to write a few extra classes as well as inform the job to use the classes for partitioning and sorting purposes (for more details on how secondary sort works, look at chapter 4):
You’ll copy an input file containing the friend relationships into HDFS and then run your driver code to run your two MapReduce jobs.
The last two arguments are the output directories for the two MapReduce jobs:
After running your code you can look at the output in HDFS:
All the people in your list are sorted in order of common friends.
This output indeed verifies what you saw with your own eyes in figure 7.13, that Jim has three FoFs, and that they’re ordered by the number of common friends.
Summary This approach can be used not only as a recommendation engine to help users grow their networks, but also for informational purposes as the user is browsing the social network’s website.
For example, when you view people in LinkedIn, you’ll be shown the degrees of separation between you and the person being viewed.
Your approach can be used to precompute that information for two hops.
To reproduce this for three hops (for example, to show friends-of-friends of a friend) you would need to introduce a third MapReduce job to compute the third hop from the output of the first job.
To simplify your approach, you used a undirected graph, which implies that user relationships are bidirectional.
Most social networks don’t have such a notion, and the algorithm would need some minor tweaks to model this behavior.
This brings us to the final graph technique, which is how to use PageRank to calculate the popularity of web pages.
The paper discusses their overall approach to crawling and indexing the web, and includes as part of that a calculation, which they titled PageRank, which gives a score to each web page indicating the page’s importance.
This wasn’t the first paper to introduce a scoring mechanism for web pages,10 but it was the first to weigh scores propagated to each outbound link based on the total number of outbound links.
Fundamentally, PageRank gives pages that have a large number of inbound links a higher score than pages that have a smaller number of inbound links.
When evaluating the score for a page, PageRank uses the scores for all the inbound links to calculate a page’s PageRank.
But it penalizes individual inbound links from sources that have a high number of outbound links by dividing that outbound link PageRank by the number of outbound links.
Figure 7.16 presents a simple example of a web graph with three pages and their respective PageRank values.
Problem You want to implement an the iterative PageRank graph algorithm in MapReduce.
Solution PageRank can be implemented by iterating a MapReduce job until the graph has converged.
The mappers are responsible for propagating node PageRank values to their adjacent nodes, and the reducers are responsible for calculating new PageRank values for each node, and for re-creating the original graph with the updated PageRank values.
Discussion One of the advantages of PageRank is that it can be computed iteratively and applied locally.
Every vertex starts with a seed value, with is 1 divided by the number of nodes, and with each iteration each node propagates its value to all pages it links to.
Each vertex in turn sums up the value of all the inbound vertex values to compute a new seed value.
This iterative process is repeated until such a time as convergence is reached.
Convergence is a measure of how much the seed values have changed since the last iteration.
If the convergence value is below a certain threshold, it means that there’s been minimal change and you can stop the iteration.
It’s also common to limit the number of iterations in cases.
All nodes push their PageRank values (PR / # outbound nodes) to the.
Figure 7.18 shows two iterations of the PageRank against the simple graph you saw at the start of this technique.
Figure 7.19 shows the PageRank algorithm expressed as map and reduce parts.
The map phase is responsible for preserving the graph as well as emitting the PageRank value to all the outbound nodes.
The reducer is responsible for recalculating the new PageRank value for each node and including it in the output of the original graph.
In this technique you’ll operate on the graph shown in figure 7.20
In this graph all the nodes have both inbound and output edges.
The map task in the following listing has two primary functions: to preserve the graph structure and to propagate the PageRank number to each outbound node.11
The map task outputs two pieces of information: the original graph and the outbound PageRank values.
Your reducer’s job, as shown in the next listing, is to reconstruct the original graph and to update each node with a new PageRank value based on all the sum of inbound PageRank values.12
The PageRank mapper inverts the graph structure so the reducer can calculate PageRanks.
For each outbound node, emit that node name along with the PageRank.
The final piece is a driver class that will keep launching the MapReduce job and read the counter from each job until you have convergence.
This is as simple as dividing all the deltas between the node’s old and new PageRank values by the number of nodes, and checking that they’re below the desired threshold:13
Read the number of nodes in the graph from configuration.
The node represents the outbound node and PageRank, so add the PageRank to the total.
Write out the original graph information with the updated PageRank information.
Update the counter with the delta between the old and new PageRank values.
If you push the web graph into HDFS and run your job, it will run for five iterations until the graph converges:
You can look at the output of the last MapReduce job to see the PageRank values for each graph node:
According to your output, node C has the highest PageRank, followed by node B.
Initially, this observation may be surprising given that B has three inbound links and C has just two.
But if you look at who’s linking to C, you can see that node B, who also has a high PageRank value, only has one outbound link to C, so node C gets B’s entire PageRank score in addition to its other inbound PageRank score from node D.
Therefore, node C’s PageRank will always be higher than B’s.
Summary Although you did implement the PageRank formula, it was made simple by the fact that your graph was well connected, and that every node had outbound links.
Dangling pages pose a problem with the PageRank algorithm because they become PageRank sinks, because their PageRank values can’t be further propagated through the graph.
This in turn causes convergence problems because graphs that aren’t strongly connected aren’t guaranteed to converge.
You could remove the dangling nodes before your PageRank iterations, and then add them back for a final PageRank iteration after the graph has converged.
Or you could sum together the PageRank totals for all dangling pages and redistribute them across all the nodes in the graph.
For a detailed examination of dealing with dangling pages, as well as advanced PageRank practices, see Google’s PageRank and Beyond by Amy N.
As you learned, graphs are useful mechanisms to represent people in a social network and pages in a web graph.
You used these models to find some useful information about your data, such as finding the shortest path between two points, and what web pages are more popular than others.
You crafted your own MapReduce code in the techniques—if you wanted to leverage a framework you could have used Giraph (http://incubator.apache.org/giraph/), which provides a vertex message-passing mechanism.
This brings us to the subject of the next section, Bloom filters.
Bloom filters are a different kind of data structure from graphs.
While graphs are used to represent entities and their relationships, Bloom filters are a mechanism to model sets and offer membership queries on their data, as you’ll discover next.
A Bloom filter is a data structure that offers a membership query mechanism where the answer to a lookup is one of two values: a definitive no, meaning that the item being looked up doesn’t exist in the Bloom filter, or a maybe, meaning that there’s a probability that the item exists.
The amount of false positives in a Bloom filter can be tuned, which we’ll discuss shortly.
Bloom filters are used in BigTable and in HBase to remove the need to read blocks from disk to determine if they contain a key.
They’re also used in distributed network applications such as Squid to share cache details between its multiple instances without having to replicate the whole cache or incur a network I/O hit in the case of cache misses.
They use a bit array of size m bits, where initially each bit is set to 0 (zero)
They also contain k hash functions, which are used to map elements to k locations in the bit array.
To add an element into a Bloom filter, it’s hashed k times, and a modulo of the hashed value and the size of the bit array is used to map the hashed value to a specific bit array location.
That bit in the bit array is then toggled to 1 (one)
Figure 7.21 shows three elements being added to a Bloom filter and their locations in the bit array.
To check the membership of an element in the Bloom filter, just like with the add operation, the element is hashed k times and each hash key is used to index into the bit array.
This is an example of a true positive membership result because the element had been previously added to the Bloom filter.
The next example shows how you can get a false positive result for a membership query.
The element being queried is d, which hadn’t been added to the Bloom filter.
As it happens, all k hashes for d are mapped to locations that are set to 1 from other elements.
This is an example of collision in the Bloom filter where the result is a false positive.
The probability of false positives can be tuned based on two factors: m, the number of bits in the bit array, and k, the number of hash functions.
Or expressed another way, if you have a desired false positive rate in mind, and you know how many elements will be added to the Bloom filter, you can calculate the number of bits needed in the bit array with the equation in figure 7.24
Figure 7.22 An example of a Bloom filter membership query that yields a true positive result.
Figure 7.23 An example of a Bloom filter membership query that yields a false positive result.
Table 7.1 shows the calculated number of bits per element depending on various false positive rates.
With all that theory in your heads, you now need to turn your attention to the subject of how Bloom filters can be utilized in MapReduce.
MapReduce is good for processing large amounts of data in parallel, and therefore is a good fit if you want to create a Bloom filter based on a large set of input data.
For example, let’s say you’re a large internet social media organization with hundreds of millions of users, and you want to create a Bloom filter for a subset of users that are within a certain age demographic.
Problem You want to create a Bloom filter in MapReduce.
Solution Write a MapReduce job to create and output a Bloom filter using the Hadoop built-in BloomFilter class.
The mappers are responsible for creating intermediary Bloom filters, and the single reducer combines them together to output a combined Bloom filter.
You’ll write a mapper, which will process user data and create a Bloom filter containing users in a certain age bracket.
The mappers will emit their Bloom filters, and a single reducer will combine them together.
The final result is a single Bloom filter stored in HDFS in Avro form.
Luckily, it’s a Writable, which makes it easy to ship around in MapReduce.
The Key class is used to represent an element, and it’s simply a Writable container for a byte array.
Figure 7.25 Equation to calculate the optimal number of hashes.
The constructor requires that you tell it what hashing function to use.
There are two implementations you can choose from: Jenkins and Murmur.
They’re both faster than cryptographic hashers such as SHA-1 and produce good distributions.
Benchmarks indicate that Murmur has faster hashing times than Jenkins, so that’s what you’ll use.
Your map function will operate on your user information, which is a simple key/value pair, where the key is the user name, and the value is their age:14
The built-in Hadoop BloomFilter class is Writable, making it easy to transport between map and reduce tasks.
Figure 7.26 A MapReduce job to create a Bloom filter.
Create the BloomFilter with a certain size, number of hashes and hash type.
Both structures must have identical sizes, number of bases, and hash types.
Add an element, which will be hashed nbHash times and the respective bit.
Checks to see if a BloomFilter may contain an element.
Why do you output the Bloom filter in the close method, and not output it for every record you process in the map method? You do this to cut down on the amount of traffic between the map and reduce phases; there’s no reason to output a whole lot of data if you can combine them yourself on the map side and just emit a single BloomFilter per map.
Your reducer’s job is to combine all the Bloom filters outputted by the mappers into a single Bloom filter.
The unions are performed with the bitwise or method exposed by the BloomFilter class.
When performing a union, all the BloomFilter attributes such as bit array size and number of hashes must be identical:15
If the user’s age is over 30, add the user name to your BloomFilter.
When the map function has executed over all the input data, output the BloomFilter.
You’ll upload your sample user file and kick off your job.
When the job is complete, you’ll dump the contents of the Avro file to view the contents of your BloomFilter:
Summary You used Avro as a serialization format for the Bloom filter.
You could have just as easily emitted the BloomFilter object itself in your reducer because it’s a Writable.
You used a single reducer in this technique, which will scale well to jobs that use thousands of map tasks and BloomFilters whose bit array sizes are in the millions.
If the time taken to execute the single reducer becomes long, you can run with multiple reducers to parallelize the Bloom filter unions and have a postprocessing step to combine them further into a single Bloom filter.
Another distributed method to create a Bloom filter would be to view the set of reducers as the overall bit array, and in the map phase perform the hashing and output the hashes.
The partitioner would then partition the output to the relevant reducer that manages that section of the bit array.
For code comprehensibility you hardcoded the BloomFilter parameters; in reality you’ll want to either calculate them dynamically or move them into a configuration file.
This BloomFilter could be pulled out of HDFS and used in another system or it could be used directly in Hadoop, as you’ll see in the next section.
In chapter 4 we looked at an implementation of a join, which used a semi-join as one of the steps to try and avoid performing a reduce-side join.
The semi-join was performed using a HashMap to cache the smaller of the datasets.
Bloom filters can replace a HashMap in the semi-join if you don’t care about storing the contents of the dataset being cached, and you only care about whether the element on which you’re performing the join exists or doesn’t exist in the other dataset.
Problem You want to perform an efficient semi-join in MapReduce.
Solution Perform a semi-join by having the mappers load a Bloom filter from the Distributed Cache, and then filter results from the actual MapReduce data source by performing membership queries against the Bloom filter to determine which data source records should be emitted to the reducers.
Discussion The key to this technique is to create a Bloom filter on a reasonably sized dataset.
You’ll extend the work you performed in the previous technique, where you created a Bloom filter on all users of a certain age.
Imagine you wanted to join that set of users with all their tweets, which is a much larger dataset than the set of users.
Figure 7.29 shows how you would filter data in a mapper such that the join in MapReduce operates on a smaller set of the overall data.
The code for the mapper is shown in the following listing:16
Figure 7.29 A MapReduce job using a map to join and filter.
Retrieve a list of the files in the distributed cache.
Check if the user (the key) is a member of your Bloom filter and, if it is, output it to.
The only additional item you need to perform in your MapReduce driver code is to add the Bloom filter file you created in the previous technique into the distributed cache, as follows:
Your MapReduce job doesn’t have a reducer, so the map output will be written to HDFS.
How about you launch your job and verify the output?
A peek at the output of the job in HDFS verifies that it only contains users joe, alison, and marie, who were present in the Bloom filter.
Summary This technique only works in situations where one of the datasets being joined is small enough to be expressed in a Bloom filter.
In the example, you were joining tweets with users, in which case the users could easily be represented with a Bloom filter.
Compare this with the best-case use of a HashSet containing integers, which require 8 bytes.
Or if you were to have a HashSet that only reflected the presence of an element that ignores collision, you end up with a Bloom filter with a single hash, yielding higher false positives.
Version 0.10 of Pig will include support for Bloom filters in a similar mechanism to what I’ve presented here.
In this section you learned that Bloom filters offer good space-constrained set membership capabilities.
We looked at how you could create Bloom filters in MapReduce, and you also applied that code to a subsequent technique, which helped you optimize a MapReduce semi-join.
Copy the user tweets into HDFS (the user age file was copied in the previous technique)
The two datasets being joined, the user ages, and the user tweets.
Most of the algorithms laid out in this chapter are straightforward.
What makes things interesting is how they’re applied in MapReduce in ways that enable you to efficiently work with large datasets.
The two main data structures presented were graphs—good for modeling relationships—and Bloom filters, which excel at compact set membership.
With graphs we looked at how you would use them to model social networks and web graphs, and went through some algorithms such as FoFs and PageRank to mine out interesting facts about your data.
With Bloom filters we looked at how to use MapReduce to create a Bloom filter in parallel, and then apply that Bloom filter to optimize a semi-join operation in MapReduce.
We’ve only scratched the surface in this chapter with regard to how data can be modeled and processed.
The capabilities of R1 let you perform statistical and predictive analytics, data mining, and visualization functions on your data.
Its breadth of coverage and applicability across a wide range of sectors (such as finance, life sciences, manufacturing, retail, and more) make it a popular tool.
A data scientist who’s working with Hadoop likely has an existing arsenal of homegrown and external R packages that they leverage.
What you need is a way to use R in conjunction with Hadoop and bridge the gap between Hadoop and the huge database of information that exists in R.
Much of the data you work with exists in text form, such as tweets from Twitter, logs, and stock records.
In this chapter we’ll look at how you can use R to calculate some simple average-based calculations on some text-based stock data.
In doing so we’ll highlight how you can use R with three different integration approaches: R with Streaming, Rhipe, and RHadoop.
By the end of the chapter you’ll understand the various ways that R can be integrated with Hadoop and pick the best approach for your application.
In this section we’ll evaluate the three different methods you’ll use in this chapter to integrate R with MapReduce.
I picked these three approaches due to their popularity and differing approaches to solving the same problem, that of combining R and Hadoop together.
Rhipe—Rhipe is an open source project which allows MapReduce to be closely integrated with R on the client side.
RHadoop—Like Rhipe, RHadoop also provides an R wrapper around MapReduce so that they can be seamlessly integrated on the client side.
The focus of this chapter is on integrating R with Hadoop.
To get up to speed (or as a refresher) on statistics, I recommend the book Statistics: A Gentle Introduction, http://www.sagepub.com/books/Book235514
So which tool should you pick? As we go through the techniques, you may find that one approach lends itself more to your situation than others.
Table 8.2 represents the author’s opinion on which tool performs best.
Before we start diving into these technologies, let’s take a few minutes to look at some R basics so that your techniques don’t look too alien.
The R package needs to be installed on each DataNode, but packages are available on publicly available Yum repositories for easy installation.
To do so requires building Protocol Buffers, and the Rhipe installation isn’t seamless and can require some hacking to get it to work.
But these packages can be installed with CRAN, and the RHadoop installation, while not via CRAN, is straight-forward.
You need to use the Hadoop command-line to launch a Streaming job, and specify as arguments the map-side and reduce-side R scripts.
Rhipe is an R library which handles running a MapReduce job when the appropriate function is called.
Users simply write native R map and reduce functions in R, and Rhipe takes care of the logistics of transporting them and invoking them from the map and reduce tasks.
RHadoop is also an R library, where users define their map and reduce functions in R.
Rather than using Streaming, Rhipe instead uses its own map and reduce Java functions, which stream the map and reduce inputs in Protocol Buffers encoded form to a Rhipe C executable, which uses embedded R to invoke the user’s map and reduce R functions.
RHadoop is a simple, thin wrapper on top of Hadoop and Streaming.
Therefore, it has no proprietary MapReduce code, and has a simple wrapper R script which is called from Streaming and in turn calls the user’s map and reduce R functions.
Table 8.2 Areas where R and MapReduce approaches work well, and where they don’t.
Approach Works well in these situations Things to bear in mind.
Hard to invoke directly from existing R scripts, as opposed to the other approaches.
You also want to work with existing MapReduce Input and Output Format classes.
There needs to be sufficient memory to store all the reducer values for a unique key in memory; values aren’t streamed to the reducer function.
In this section we’ll quickly look at some R basics to get R installed on your system, and a reference guide to help understand the basic R language constructs and types.
Care should be taken to install R in the same directory on all the nodes.
Also make sure that all nodes are running the same version of R.
We’ll look at a handful of R basics to help you understand the techniques in this chapter, as detailed in figure 8.1
In R, vectors are the most useful data structure because most of the numerical functions support them.
If the result of a function or operation in R isn’t assigned to a variable, by default it’s printed on the console.
We’ve looked at some simple examples of how to get up and running with R.
It’s time to introduce Hadoop into the equation and look at how you can use R in combination with Hadoop Streaming.
Convert the string vector into a numerical vector and remove non-numerics.
Because we're mixing types R coerces all the data into strings.
This message means that elements which couldn't be converted into numerics now.
The is.na returns a vector of TRUE or FALSE elements which we use to remove NAs from x.
Not all numerical functions in R support lists, so it's generally best to keep your data in vectors.
Generate a new vector with all the values incremented by one.
The same as lapply, but conveniently returns a vector rather than a list.
The result is a list of the same length as x.
With Hadoop Streaming, you can write map and reduce functions in any programming or scripting language that supports reading data from standard input and writing to standard output.
In this section we’ll look at how you can get Streaming to work directly with R in two steps, first in a map-only job, and then in a full MapReduce job.
You’ll be working with stock data and performing simple calculations.
The goal is to show how the integration of R and Hadoop can be made with Streaming.
Just like with regular MapReduce, you can have a map-only job in Streaming and R.
Map-only jobs make sense in situations where you don’t care to join or group your data together in the reducer.
In this technique you’ll look at how Hadoop Streaming and R can be used on your stock data to calculate the daily means for each stock symbol.
Problem You want to integrate R and MapReduce, and you don’t need to join your data, or sort your outputs.
Solution Use R and Hadoop Streaming to process data in a map-only job.
Discussion In this technique you’ll work on the stock CSV file, which contains the following elements for each stock:
A subset of the contents of the file can be viewed here:
In your job, you’ll calculate the daily mean for each line using the open and close prices.
The R script to perform that task is shown here:2
This sink function controls the destination of outp Because your code is being used in Hadoop Streaming, y.
Summary Figure 8.3 shows how Streaming and R work together in a map-only job.
Any MapReduce code can be challenging to test, but the great thing about using Hadoop Streaming code is that it’s very easy to test on the command line without having to involve MapReduce at all.
Read a line from standard inpu is the number of lines that sh.
Concatenate the symbol, date, and mean for the day and write.
Each record is pushed as a single line to the R child process.
The child process is started when the Streaming map is initialized, and terminated after all the input records have been fed to the child process.
Streaming reads each line of output from the R process and forwards it to the output collector (which in a map-only job uses the OutputFormat's RecordWriter to write to the data sink)
That output looks good, so you’re ready to run this in a Hadoop job:
You can perform a simple cat that shows you that the output is identical to what you produced when calling the R script directly:
You may have noticed that you used TextInputFormat, which emits a key/value tuple where the key is the byte offset in the file, and the value contains the contents of a line.
But in your R script you were only supplied the value part of the tuple.
This is an optimization in Hadoop Streaming, where if it detects you’re using TextInputFormat it ignores the key from the TextInputFormat.
Figure 8.4 shows some Streaming configuration settings, which can be used to customize map inputs and outputs.
Remove the output directory in HDF the directory doesn’t exist thi.
Tell Streaming the location of the executable that should be executed in the.
Now that we’ve covered how to use R and Streaming for a map-only job, let’s see how to get R working with a full map and reduce job.
We’ll now look at how you can integrate R with a full-blown MapReduce job.
You’ll build upon what you’ve learned about using Streaming and a map-side R function, and introduce a reduce-side function.
In doing so you’ll learn how Hadoop Streaming supplies map output keys and the list of map output value tuples to the standard input of the R function, and how the R function outputs are collected.
The previous technique calculated the daily mean for each stock symbol, and you’ll use the MapReduce framework to group together all of the daily means for each stock symbol across multiple days, and then calculate a cumulative moving average (CMA) over that data.
Problem You want to integrate R and Streaming in both the map and reduce sides.
Solution Use R and Hadoop Streaming to process data in mappers and reducers.
To extract the key/value pair from a line of output from a script, Streaming will split using the tab character.
If instead you wanted to split on the third instance of the separator character you would specify the above setting in your job:
By default the input keys and values are separated by the tab character.
In this example you're telling Streaming to use the comma as the separator string:
Read the i Discussion If you recall the map-side technique, the map R script emitted tab-separated output with the following fields:
MapReduce will sort and group together the output keys of your map script, which is the stock symbol.
For each unique stock symbol MapReduce will feed your reduce R script with all the map output values for that stock symbol.
Your script will sum the means together and emit a single output containing the CMA.3
A simple R function that takes as input the stock symbol and a vector of means.
It calculates the CMA and writes the symbol and CMA to standard output.
When you find a new key it means you’ve hit a new map output key.
Summary Figure 8.5 shows how Streaming and your R script work together on the reduce side.
The beauty of Streaming is that you can easily test it with streaming Linux commands:
That output looks good, so you’re ready to run this in a Hadoop job:
These are two separate scripts, one for the map side, and the other for the reduce side.
The reduce script is supplied each map output record on a separate line.
The map output key and value are separated by tab by default.
Map output keys are grouped together, so our code needs to read the input line by line, and when we see a change in the input key we can process all the values for that key.
Just like on the map side, the R script should write its output as.
By default the tab character separates the output key from.
The only difference is that the output isn't written to the OutputFormat, and instead is collected and spilled to disk, awaiting fetch commands from reducer tasks.
You can perform a simple cat that shows you that the output is identical to what you produced when calling the R script directly:
Figure 8.6 shows some Streaming configuration settings, which can be used to customize reduce inputs and outputs.
For additional Streaming features such as more control over sorts, please look at the Hadoop streaming documentation.4
We’ve looked at how you can use R in combination with Streaming to calculate the means over your stock data.
One of the disadvantages of this approach is that this can’t be easily integrated into client-side R scripts.
Specify that MapReduce should part output based on the first token in the.
Rhipe, short for R and Hadoop Integrated Processing Environment, is an open source project that, as the name suggests, provides a closer integration of R and Hadoop than what you saw with R and Streaming.
In R and Streaming you used the command line to launch a Hadoop job, whereas with Rhipe you can actually work with MapReduce directly in R.
Before we get started, you’ll need to follow the instructions in appendix A to install Rhipe and its dependencies on the Hadoop nodes in your cluster.
In this technique you’ll again implement the CMA of each stock symbol, just like you did with R and Streaming.
But in this technique you’ll use Rhipe and see how you can achieve tight integration of R and Hadoop.
Problem You want to seamlessly leverage Hadoop from your R code.
Solution This technique shows how Rhipe allows you to write client-side R code that can launch a MapReduce job.
It also takes a look at how Rhipe R callbacks are used in the scope of Rhipe MapReduce jobs.
Discussion The Rhipe script to calculate the stock CMA is shown next.
Notice that the MapReduce integration is fully baked into Rhipe, which makes it easy to integrate MapReduce into your existing R scripts and processes:5
To set a custom output key/value separator string, use the following conﬁguration key.
To set a custom input key/value separator string, use the following conﬁguration key.
Summary As opposed to your R with Streaming technique, with Rhipe you can execute the R script directly, which in turn will launch the MapReduce job:
To try and understand how Rhipe works, and how your R code is integrated with Rhipe, we’ll walk through the series of steps in a Rhipe workflow, starting with a look at the main parts of your R script, and how the MapReduce job is triggered, as shown in figure 8.7
Define the map expression that’s executed in the map task.
The Rhipe function rhcollect is called emit key/value tuples from the map ph.
The pre block is called for each uniq map output key, prior to the values for the key being supplied to the reduce block.
The reduce block is called containing a vector of values in reduce.values.
This is called multiple times if the number of values for a key is greater than 10,000
Just like in the map expression, the rhcollect function is called to emit the.
The rhmr function is used to set up the job.
Next we’ll look at how Rhipe works in the context of MapReduce tasks, starting with the map task (as shown in figure 8.8)
Now we’ll see how Rhipe works on the reduce side (as shown in figure 8.9)
Rhipe also contains a number of functions that can be used to read and write to.
One area to watch out for in Rhipe is that it doesn’t use Streaming and instead uses its own map and reduce functions and its own Input/Output Format classes.
As a result, it can’t use other Input/Output Format classes, which you may already have in place to work with your data formats.
Executing the R script results in Rhipe kicking-oﬀ a MapReduce job.
The reduce expression, with three callbacks called at the start, during and end of each unique map output key.
That concludes our look at Rhipe, which offers an integration of R and Hadoop into client-side R scripts.
The final section takes a look at RHadoop, which also offers a client-side integration of R and Hadoop, albeit in a more lightweight way.
Once the buffer exceeds a threshold, it is written into an R vector called map.values and R is told to execute the user function.
When the user function calls rhcollect, this results in a callback into Rhipe's C code.
The callback converts from R types into the Protocol Buffers form, and pushes it onto standard output.
RHadoop is an open source project created by Revolution Analytics, which provides another approach to integrating R and Hadoop.
Just like Rhipe, RHadoop allows MapReduce interactions directly from within your R code.
Once the buffer exceeds a threshold, it is written into an R vector called reduce.values and R is told to execute the user function.
When the user function calls rhcollect, this results in a callback into Rhipe's C code.
The callback converts from R types into the Protocol Buffers form, and pushes it onto standard output.
Called once all the values have been supplied to (2)
We’ll focus on using rmr in this chapter because we’re mostly interested in R and MapReduce integration, but rdfs and rhbase are worth a look for a completely integrated R and Hadoop experience.
To set up RHadoop and its dependencies, follow the instructions in appendix A.
In this technique we’ll look at how you can use RHadoop to calculate the CMA using your stock data.
Problem You want a simpler R and Hadoop client-side integrated solution.
Solution This technique looks at how RHadoop can be used with R to launch a MapReduce job inside R, and it also looks at how RHadoop works with Hadoop Streaming.
Discussion Conceptually, RHadoop works in a way similar to Rhipe, where you define your map and reduce operations, which RHadoop invokes as part of the MapReduce job:6
Define a map function, which takes a key/value pair as input.
The keyval function is called for each key/value output tuple that the map emits.
The reduce function, which is called once for each unique map key, where k is the.
Summary To execute the code in this technique, you’d run the following commands:
Figure 8.10 shows you how your code correlates to the MapReduce job execution.
One of the interesting features of rmr is that it makes the R client-side environment available to the map and reduce R functions executed in MapReduce.
What this means is that the map and reduce functions can reference variables outside of the scope of their respective functions, which is a huge boon for R developers.
The reduce function takes a key and a list of values.
The rmr mapreduce function is a trigger to launch a MapReduce job.
In your technique, the input to your job was already in HDFS, and you didn’t interact with the output of your job in R.
But rmr has support for writing R variables directly to HDFS, using them as inputs to the MapReduce job and, after the job has completed, loading them back into an R data structure.
This isn’t the approach you’ll want to take when working with large volumes of data, but nonetheless is great for prototyping and testing with smaller datasets:
The fusion of R and Hadoop allows for large-scale statistical computation, which becomes all the more compelling as both your data sizes and analysis needs grow.
In this chapter we focused on three approaches you can use to combine R and Hadoop together.
After reading this chapter you should have enough information to choose the right level of R and Hadoop integration appropriate for your project.
The next chapter continues the data science theme by looking at how you can use Mahout for predictive analytics.
The result from the MapReduce job is a closure that can be used to read the results back out of HDFS.
It’s one of the main tools in a data scientist’s tool belt, whose job is to examine large datasets (often called big data these days) and derive meaningful insights from that data, optimally in the form of new products.
Predictive analytics can be broken down into three broad categories:
These items can be other users in a social network, or products and services in retail websites.
Examples of classification include email spam filtering and detection of fraudulent credit card transactions.
Predictive analytics is the field of deriving information from current and historical.
Mahout is a machine learning library which includes implementations of these three classes of predictive analytics techniques.
Many of its algorithms have MapReduce implementations, which is the focus of this chapter, and this is where Mahout comes into its own—its ability to work with huge datasets that other predictive analytics tools can’t support.
In fact Mahout only starts to make sense if you’re working with datasets that number in the millions or more.
In this chapter we’ll look at the Mahout MapReduce implementations of recommenders, classifiers, and clusterers.
You’ll use recommenders to recommend movies similar to movies that users have already rated; you’ll write a classifier that can filter out spam emails; and, finally, we’ll look at how you can use clustering to discover structure in your data.
Recommender systems, which are also known as collaborative filtering (CF) systems, are the computer equivalent of you asking your friends for a restaurant recommendation.
The more recommendations you receive from your friends, the higher the probability that you’ll go there.
In the online world you see recommender engines in play every day—most social and retail websites recommend new people for you to interact with, or a new product for you to buy.
There are two types of collaborative recommenders: user-based recommenders and item-based recommenders:
User-based recommenders look at users similar to a target user, and use their collaborative ratings to make predictions to the target user.
Item-based recommenders look at similar items, and use this information to recommend items that are related to items previously used by a target user.
You can see the results of recommenders in action in figure 9.1
Both types of recommender systems need to be able to determine the degree of similarity between users or items, so we first need to look at how similarity metrics work.
In both user- and item-based recommenders, the system needs to find similar users or items.
They do this by comparing users or items with each other to arrive at a similarity score.
Popular measures that can calculate these scores include Euclidean distance and Pearson’s correlation.
These algorithms operate on numerical data, where the data points are vector-like (points in space)
The Euclidean distance is the most commonly used distance measure, and can be seen in figure 9.2, which shows how the Euclidean distance calculation works.
The Euclidean distance is in a family of related distance measures, which also includes the Manhattan distance (the distance between two points measured along axes at right angles)
These measures are all similar in how they calculate distances, and as such switching from one to the other is not likely to significantly change results.
Correlation-based measures, however, are less concerned with the distance between points in a dataset and care more about similarity, which is the degree of linear relationship between two variables.
Pearson’s correlation is widely used in science to measure the dependencies between two variables.
Figure 9.3 shows an example of a highly correlated relationship.
The math behind Pearson’s correlation is more complex than the Euclidean distance, and is outside of the scope of this chapter, but if you’re interested you can read more details at http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient.
Mahout can support the Euclidean and Pearson’s similarity measures for both user-based and item-based recommenders.
Mahout supports additional similarity measures for item-based recommenders such as the Tanimoto coefficient.1 All the user-based and item-based similarity implementations can be seen by searching for implementations of the UserSimilarity and ItemSimilarity interfaces.
Next we’ll look at the dataset you’ll use in this section on recommenders.
GroupLens is a research lab in the Department of Computer Science and Engineering at the University of Minnesota.
Here you’ll download this data and prepare it in a format that can be used by the Mahout recommender engines.
Figure 9.3 A plot showing two users’ preferences for different movies, and Pearson’s correlation.
You can see an example from the top of the file:
This isn’t quite in the format that you need for Mahout, which expects CSV-delimited format as you see next:
You can write a simple awk script to convert the GroupLens data into CSV form:
Your data is now in a form that you can use in the techniques in this section.
Follow the instructions in appendix A to install Mahout on your system before executing the instructions in this section.
Mahout doesn’t have a way to run user-based recommenders in MapReduce because the user-based recommender is only designed to work within a single JVM.
Because user-based recommenders aren’t relevant for this book, we’ll skip them and move on to look at item-based recommenders.
If you’re curious about user-based recommenders, the book Mahout in Action, by Owen et al., covers them in detail.
Item-based recommenders calculate recommendations based on items, and not users.
But rather than look at similarities between users, item-based recommenders look at similarities between items.
Because the item rating is the only data point to go by, this combined with what examining all items that all users rate are the data points used to make the item predictions.
Because item similarities tend to be less volatile than user similarities, they lend themselves more to precomputation, which can speed up the recommender operation.
The item-based recommender in Mahout supports a distributed execution model, so that it can be computed using MapReduce.
The distributed recommender will be the focus of this section.
Let’s use figure 9.4 to walk through a simple example of item recommendation.
Let’s now look at how you can get the distributed item-based recommender working.
How would you use the GroupLens movie rating dataset to recommend movies for three users in the dataset using MapReduce? Problem You have a large number of user item preferences and you want to recommend additional items to users in a scalable fashion.
Solution Use Mahout’s MapReduce item-based recommender in combination with the GroupLens data to make movie recommendations.
Mahout’s item recommender uses 10 MapReduce jobs to perform this recommendation, which perform functions such as data preparation, similarity co-occurrence calculations, and calculating the final recommendations.
Discussion The distributed item-based recommender requires two inputs: the recommendations file and a file of user IDs for which item recommendations will be generated.
Let’s go ahead and create the file of user IDs (you’ll select the first three IDs from the ratings file), and then push both files into HDFS:
When the job completes you can view the output in HDFS.
The output format consists of the user ID, followed by a comma-separated list of item IDs and their related scores:
Summary Running the distributed item-based recommender resulted in ten MapReduce jobs being executed.
The jobs and a quick description of them can be seen in figure 9.5
The distributed implementation of an item-based recommender creates a cooccurrence matrix to associate similar items together.
It does this by combining items with similar ratings from each user, and then counting the number of times that each pair of items was rated by all the users.
It then predicts the ratings for unknown items by multiplying the users’ ratings for an item with all the item’s co-occurrences, and then sorts all these item predictions and retains the top K as recommendations.
So far you’ve been focused on the item-based recommender where items are entities such as movies or products.
You could also have used this approach to recommend users.
Existing user relationships could be modeled by replacing item IDs with user IDs.
The rating value itself could then become a constant, or if desired you could also use it to model the level of friendship (for example, if two users commonly interact their rating could be higher than those that don’t)
Mahout also comes with another distributed item-based recommender called the slope-one recommender, which is a simpler recommender that just requires two MapReduce jobs.
It doesn’t use a similarity algorithm to measure the similarity of items, and instead performs a simple average of the difference between rating values.
In this section you learned how user-based and item-based recommenders work, and you looked in detail at how a MapReduce item-based recommender could be.
You must export the HADOOP_HOME environment variable to refer to the location of your local install of Hadoop (/usr/lib/hadoop for CDH-packaged installations, and /usr/local/hadoop for installations that followed the tarball instructions in appendix A)
Mahout uses this variable to discover your Hadoop cluster settings.
If this step is omitted, Mahout will use the local filesystem for storage, and run MapReduce jobs on your client host.
You’ll now move into the second section in a three-part sections on predictive analytics, and look at how you can use classification to make movie predictions.
Invert the data such that on each line an item contains a list of users and their rating for that item.
Creates a co-occurrence matrix for each item and other items rated by the same user, only retaining item pairs that are similar.
Nullify similarity numbers of co-occurrences containing an item the user has already reviewed.
Inner join between users to be recommended with the ratings data.
Inner join co-occurrence matrix items with items reviewed by user.
Multiple user rating for each co-occurring item and rank to determine best predictions.
Classification, also known as supervised learning, is a fancy term for a system that makes predictions on data based on some previously known data.
As humans we do this all the time—when you see an email titled “REQUEST FOR URGENT BUSINESS RELATIONSHIP,” do you eagerly open it? The answer is, no.
Prior experience has told you that the combination of words, and the fact that they’re uppercase, means that this email is most likely spam.
This is an example of human supervised learning, where your current behavior is a result of previous observations you made on similar data.
You may not have seen an email subject with the exact same sequence of words, but you’ve seen enough examples of similar email subjects which were spam to make you immediately suspicious.
In the case of email spam detection, you train a system using data which has already been labeled (or marked) as being either spam or ham (legitimate email) to build a model, and then use that model to make predictions about emails that the system hasn’t seen before.
There are multiple steps involved in building such a system, which are described in figure 9.6
To build a model you need some data to train.
A higher-quality training dataset will result in a better model and resulting classifier.
If the data is email, it could be a word; if the data is weather data, it could be temperature, or pressure.
Once we know the features, we need to extract the features from our training data into a form that works with the algorithms we're working with.
Build a classifier using our prepared training data and some machine learning algorithms.
It's common to try multiple algorithms to see if some classifiers perform better than others.
Once we have built a classifier we want to see how it performs on some test data.
Some of the terms you just saw will be used throughout this section, and are defined in figure 9.7 using email spam classification as an example.
Training—The process by which categorized documents are used to build a model, which can be used by a classifier to categorized unseen documents.
Classifier—A classifier uses a model extracted from the training data to make predictions about unseen documents or datasets.
Before we dive into the Hadoop side of supervised learning, we’ll walk through building a handmade classifier using a simple training dataset to get an idea of the overall process.
The examples here show the categories in a spam classification system.
Training data is a representative real-world sample of data with corresponding categories, used to train a classifier.
Test data is a representative real-world sample of data which a trained classifier uses to validate the.
This is distinct from the data used to train the classifier.
A feature is an attribute of your data that you want to include when training and classifying.
Bayes theorem comes into play once each word has had a spam and ham probability calculated, and combines them together to form the overall email probability of ham or spam.
Your first task is to find some data to train your classifier.
The two hardest parts of building a classifier happen to be the first two steps.
When it comes to training data, your classifier is only as good as the data used to train it.
Lucky for you there are some high-quality datasets you can use when working with spam.
For now we’ll just conjure up a few example subject lines which you’ll use for training data, which you can see in figure 9.8
After you have a set of training data that you’re happy with, you can start examining that data for attributes that you want to use for training.
This is called feature selection, and there’s a whole science around this one topic.
The goal of feature selection is to pick features that you believe will increase the ability of your classifier to separate data between the different categories.
When building an email classifier, there are several attributes within an email that can be used as features, such as the contents of the subject line, other email header data points, and the body of the email.
As an example, the date the email was sent doesn’t lend itself to being a feature because there’s nothing about the date that can be used to separate ham from spam.
But the email subject and body can definitely contain text that can be used to help identify spam.
The text in the email subject and body are useful, but should all the words be used as features? The short answer is, no.
You don’t want words that commonly appear in the English language (referred to in text mining circles as stopwords) to be used as features because they’ll appear in every single email.
Similarly, you don’t want words that are rare and only appear in a handful of emails because they won’t help with classification, either.
For now let’s assume that your data only consists of the email subject line.
Next you need to model the notion of a category, and keep track of the words in the category, the number of times you saw the word in the category, and the number of documents that were used to train the category:
The rest of the tokens are words from the email subject line.
Keep a count of the number of documents used to train this category.
Keep a count of the number of times you’ve seen a word in this category.
After you’ve trained your classifier with the four documents, your classifier contains all the data it needs for classification.
Figure 9.9 shows the data that you have in your classifier after you’ve finished training.
When you see a new email subject, you need to calculate the probability of each word being in a category.
To calculate this number, you take the number of times a word appears in the category and divide it by the total number of documents for the category.
The following code shows this probability being calculated for a word in a specific category:
The probability that a randomly selected document is in this.
We can ignore P(D) since it acts as a scaling factor and doesn't change the ranking.
These values will be used as the value of P(W|C) in the Bayes formula, which you’ll see shortly.
So you have the probability that each individual word is in a category, but how do you calculate the probability of the overall document being in a category? Bayes comes to the rescue with a theorem into which you can plug your word probabilities, as you see in figure 9.10
Probability that for a given category C ,i the words in D appear in that category.
We ignore P(D) to arrive at a simpliﬁed Bayes’ theorem.
Calculated as the number of times W1  appears in Cidivided by the total number of documents in Ci.
Calculated as the number of documents in the category Ci divided by the total number of documents.
If you run your code, you’ll see the same values that you calculated by hand earlier:
In real life your classifier wouldn’t work because of the small amount of training documents.
You’d ideally want a large and equal number of spam and ham documents for training purposes.
Let’s move on to look at how you’d do that with Mahout.
You now need to separate the corpus into a testing set and training set.
The training set will be used to build the classifier model, and the testing set will be classified with.
Each email is contained in a separate file—list the number of.
Conver into a work w classif the model to gauge the accuracy of the model.
In creating the training corpus, you’re careful to use the same number of spam and ham emails, which will help with the performance of your classifier:
Now that you’ve separated your training set from your test set, you need to convert it into a form that the training code can work with.
The data as it stands right now consists of a file for each email, and the format you want to convert it into contains an email (or document) per line, with the category name as the first token in the line.
Mahout has a built-in tool that can perform that conversion for you, which you’ll run on both the training and test set.
Now that your data is in a form that can be used by the training tool, you can run that tool to generate the model:
Copy the first 400 spam emails into a training directory.
Copy the last 100 spam emails into a testing directory.
Copy the last 100 ham emails into a testing directory.
Copy the first 400 ham emails into a training directory.
You must export HADOOP_HOME so that the prepped data is stored in HDFS.
Copy the prepared data into HDFS in preparation for training and testing.
The size of the n-grams (the number of contiguous words used to create features)
You can now run the classifier on your test data by specifying the location of your model:
Your classifier also was able to be 100 percent accurate at classifying ham emails.
Summary In this technique we looked at how we could use Mahout to train and test a spam classifier—let's look at them in more detail.
The scalability that can be achieved with training in Mahout comes at a high cost, as highlighted in figure 9.12, which shows all the MapReduce jobs that must execute to train the model.
The size of the n-grams (the number of contiguous words used to create features)
You want to run this in MapReduce, but if you specify sequential, the test will be performed in the client-side JVM.
When training the model, you can play with the -ng argument, which specifies the size of the n-grams extracted from the training data.
You ran with it set to 1, which means every word was an independent feature, but the overall accuracy of the classifier can improve with larger values.
When you completed testing your classifier, you were presented with something called a confusion matrix.
A confusion matrix is used to visualize the performance of your classifier, and tells you how well your model works.
Let’s look at the confusion matrix and try and make head or tail of the output (see figure 9.13)
There are two options available to use when using your classifier to classify documents—online and offline:
In online mode you’re responding to a classification request in real time, such as a backend REST API.
Offline mode would be suited to cases where you want to classify millions of documents.
In this situation you’d write a MapReduce job that performed a function similar to Mahout’s testing code.
Mahout contains other classification algorithms (in various stages of completion), which are outlined in table 9.1
Your final predictive analytics technique is a look at clustering, a series of algorithms whose goal is to group data together and help you make new discoveries about your data.
In the previous section we looked at supervised learning, where a classifier is built with some data which was previously categorized.
Clustering is an example of an unsupervised learning technique, where no such categorized data is used to build the model.
Clustering is different from classification and recommenders: the goal of classification and recommenders is to make predictions about some data, such as whether or not a web page is in a sports category, or whether a user is likely to enjoy a book they’ve not read before.
The goal of clustering is to partition data into a number of clusters so that data in each cluster is more similar to each other than to data in other clusters.
To help you understand clustering, figure 9.14 shows an example of data which you can see is tightly grouped into three distinct clusters.
The goal of clustering is to identify these clusters so that you can make new discoveries about your data.
In this figure it’s obvious that these clusters exist, but in reality your data isn’t likely to be so well separated, and this is where clustering can help you find groupings of data leading to previously unknown observations.
Clustering has many applications in fields such as marketing, to find groups of customers with similar behaviors, and biology, to classify plants and animals, and so on.
Mahout has a number of clustering algorithms, but we’ll look at K-means, one of the simpler algorithms.
Logistic regression is a model used for prediction of the probability of occurrence of an event.
It makes use of several predictor variables that may be either numerical or categories.
Neural Network Neural Networks are a means for classifying multidimensional objects.
Hidden Markov Models are used in multiple areas of machine learning, such as speech recognition, handwritten letter recognition, or natural language processing.
With K-means you tell  the K-means algorithm ahead of time how many clusters you’re looking for (the K in K-means)
The K-means process starts with the initial placement of the K cluster centroids.
The initial K centroids can either be randomly located or specifically placed.
Randomly located centroids will likely give different results, and therefore the recommendation is that centroids are located as far away from each other as possible.
Once the initial centroids have been placed, K-means follows an iterative algorithm whereby each data point is associated to the nearest cluster centroid, and then the cluster centroids are repositioned relative to all the data points.
This process repeats until such a time as the cluster centroids don’t move, at which time the clusters are considered to have converged.
To determine the distances between data points and the cluster centroids, clustering supports most of the similarity metrics that you saw in the recommenders section, such as Euclidean distance, Tanimoto, and Manhattan.
Model the objects that you want to cluster into N dimensions.
Place the K centroids into the space represented by the objects.
Using a distance metric, assign each object to the centroid that is closest to it.
Figure 9.15 shows this algorithm in play with a two-dimensional space.
Now that you understand the basics of K-means, let’s look at how you can use it in.
As with many algorithms in Mahout, K-means also has both sequential (in-memory) and parallel (MapReduce) implementations.
In this section we’ll look at the parallel implementation of K-means.
Solution Use Mahout’s MapReduce K-means algorithm to cluster data together and observe the importance of the placement of the initial centroids.
Discussion The synthetic data is a series of two-dimensional data points that represent twenty synthetic clusters:
At this point you're done, because the centroids won't move.
If you produce a scatter plot of this data you’ll see output similar to figure 9.16.3
You need to convert the synthetic 2D data into the SequenceFile format required by Mahout for clustering:
Next, you use your utility to create the input data in HDFS:
This will start an iterative sequence of MapReduce jobs until such a time as the clusters converge, or you hit the maximum number of iterations.
When the clustering has completed there should be a number of directories in HDFS that contain output for each of the MapReduce iterations.
Read through each line of input and create a vector with a 2D data.
Generate a SequenceFile in HDFS with the input data represented in vector form.
The number of centroids to sample from the input vectors (K)
Summary When the clustering has completed, you can use the clusterdump Mahout utility to dump out the cluster details of the last job:
If you superimpose these clusters on top of the original scatterplot, you can see how well the algorithm worked.
Note that the clusters will vary based on the initial centroids, so your results will differ from those presented here (see figure 9.17)
The radius of the cluster, expressed as a standard deviation of the distance from the centroid to all 65 data points in the cluster.
Clustering is sensitive to the initial locations of the centroids; if they had been more randomly distributed, the clustering algorithm would have identified more of the clusters.
In this simplified example, you only worked with data in two dimensions, which is easy to visualize and vectorize.
Working in two dimensions means that you can only work with two features, because each dimension represents a single feature.
If you want to work with n features, you need n dimensions, which the Mahout Vector classes all support.
An example would be working with text, where each unique word is considered a separate feature, and therefore a dimension.
Because K-means works with vectorized data, if you want to use K-means with text data (such as clustering on the Reuters news collection) you need to vectorize that data prior to using it with Mahout.
How you do this is outside of the scope of this book, but.
Figure 9.17 The initial and ending cluster centroids plotted against the input data.
Mahout didn't do a great job of picking initial random cluster locations (denoted with gray crosses)
Because of the initial poor cluster locations, only a handful of them account for the majority of the actual clusters.
The black crosses are the cluster centroids after the clustering has completed.
The solid gray circle represents the cluster radius standard deviation from the centroid to all points in the cluster.
Mahout in Action (Owen et al., Manning, 2011) also contains a section that goes through this exercise in detail.
Mahout contains other clustering algorithms, some of which can be seen in table 9.2
This brings to a close part 4, the series of chapters related to data science.
In part 5, we’ll look at tools and approaches that make life that much easier when working with Hadoop, which I’ve dubbed, “Taming the elephant.”
Hierarchical clustering is the process or finding bigger clusters, and also the smaller clusters inside the bigger clusters.
It tries to find bigger clusters first and then does finegrained clustering on these clusters—hence the name Top Down.
Canopy clustering Canopy clustering is a simple, fast, and surprisingly accurate method for grouping objects into clusters.
Canopy clustering is often used as an initial step in more rigorous clustering techniques, such as K-means clustering.
By starting with an initial clustering, the number of more expensive distance measurements can be significantly reduced by ignoring points outside of the initial canopies.
Fuzzy K-means Fuzzy K-means (also called Fuzzy C-means) is an extension of K-means, the popular simple clustering technique.
While K-means discovers hard clusters (a point belong to only one cluster), Fuzzy K-means is a more statistically formalized method and discovers soft clusters where a particular point can belong to more than one cluster with certain probability.
Part 5 is called “Taming the elephant,” and it’s dedicated to examining languages, tools, and processes that make it easier to work with MapReduce.
Why is elephant in the title? Hadoop was created by Doug Cutting, who named the framework after his son’s yellow stuffed elephant.
Chapter 10 dives into Hive, a SQL-like domain-specific language that’s one of the most accessible interfaces when you work with MapReduce.
Pig, covered in chapter 11, offers a good compromise as an abstracted MapReduce language that can bump down into Java code when you need a lower level of access to Hadoop.
Chapter 12 targets programmers who want to integrate MapReduce with existing Java applications.
You’ll learn about Crunch and Cascading, two technologies offering abstractions that let you hide some of the boilerplate code you just can’t avoid with raw MapReduce.
Chapter 13, the final chapter, examines how you can tame MapReduce code using different approaches to unit testing.
It also looks at how you can debug any MapReduce job, and offers some anti-patterns you’d best avoid.
Working with MapReduce is nontrivial and has a steep learning curve, even for Java programmers.
Over the course of the next three chapters, we’ll look at technologies that lower the barrier of entry to MapReduce.
Let’s say that it’s nine o’clock in the morning and you’ve been asked to generate a report on the top ten countries that generated visitor traffic over the last month.
Your log data is sitting in HDFS ready to be used.
Are you going to break out your IDE and start writing Java MapReduce code? Not likely.
This is where languages such as Hive come into play.
Hive, with its SQL-like syntax, allows you to write and start executing MapReduce jobs in the same time that it would take you to write your main method in Java.
Hive is one of the easiest to use of the high-level MapReduce frameworks.
Hive owes much of its popularity to the fact that its language is essentially SQL, and as such is accessible to those who’ve had some exposure to SQL in the past.
Hive was originally an internal Facebook project that eventually tenured into a fullblown Apache project.
It was created to simplify access to MapReduce by exposing a SQL-based language for data manipulation.
In this chapter we’ll look at practical examples of how you can use Hive to work with Apache web server logs.
We’ll look at different ways you can load and arrange data in Hive to optimize how you access that data.
We’ll also look at some advanced join mechanisms and other relational operations such as grouping and sorting.
We’ll kick things off with a brief introduction to Hive.
Hive maintains metadata in a metastore, which is stored in a relational database.
This metadata contains information about what tables exist, their columns, privileges, and more.
By default Hive uses Derby to store the metastore, which is an embedded Java relational database.
Because it’s embedded, Derby can’t be shared between users, and as such it can’t be used in a multiuser environment where the metastore needs to be shared.
Appendix A should be consulted on how to use MySQL as a metastore in multiuser shared environments.
Hive can support multiple databases, which can be used to avoid table name collisions (two teams or users that have the same table name) and to allow separate databases for different users or products.
A Hive table is a logical concept that’s physically comprised of a number of files in HDFS.
Tables can either be internal—where Hive organizes them inside a warehouse.
Tables can be partitioned, which is a physical arrangement of data associated with each partition into distinct subdirectories for each unique partitioned key.
Partitions can be static and dynamic, and we’ll look at both cases later in this section.
Hive’s query language supports much of the SQL specification, along with Hive-specific extensions, some of which are covered in this section.
Hive in noninteractive mode lets you execute scripts containing Hive commands.
Note that you used the -S option so that only the output of the Hive command was written to the console:
Another noninteractive feature is the -e option, which lets you supply a Hive command as an argument:
If you’re debugging something in Hive and you want to see more detailed output on the console output, you can use the following command to run Hive:
Next we’ll look at how you can use Hive to mine interesting data from your log files.
The goal with this section is to walk through a practical application of Hive and use it to showcase several intermediary Hive features.
For an example, imagine that you’re working at a company that provides online movie streaming services, and you want to perform some basic data analytics (mapping out the top movie categories for each country) on your user log data.
Serialization and deserialization, or SerDe as it’s known in Hive speak, is what allows Hive to read data from a table (deserialization), and also to write it into HDFS (serialization)
In addition to a number of built-in SerDe classes, Hive supports custom SerDe implementations.
Imagine that you had a number of Apache logs files that you wanted to work with in Hive.
The first step is to create a table into which they can be loaded.
Problem You have log files that you want to load into a Hive table, and using the default Hive SerDe class won’t tokenize it correctly.
Solution Use the RegexSerDe bundled with Hive and define a regular expression that can be used to parse the contents of Apache log files.
This technique also looks at how serialization and deserialization works in Hive, and how to write your own SerDe to work with log files.
Discussion Let’s assume that you have logs being written into HDFS under the directory /data/ logs/YYYMMDD, and you want to calculate daily analytics on the data.
Assume that you don’t want Hive to manage the table storage for you, so it will be an external table.
Hive comes with a contrib RegexSerDeclass, which can tokenize your logs:
A quick test will tell you if the data’s being correctly handled by the SerDe.
Since the RegexSerDe class is part of the Hive contrib, you’ll need to register the JAR so that it’s copied into the distributed cache and can be loaded by the MapReduce tasks:
If you’re seeing nothing but NULL values in the output, it’s probably because you have a missing space in your regular expression.
Ensure that the regex in the CREATE statement looks like figure 10.2
When a table is being read in Hive, as in the case of the SELECT you just executed, Hive uses information you specified when creating the table to materialize rows and.
Also no that there’s a single space separator where t.
You should substitute $HIVE_HOM with the location of your Hive installation becau.
Hive doesn’t restrict how rows and fields in Hive are represented, and only requires that an ObjectInspector exists that knows how to extract fields from the objects returned by the deserializer.
Hive has built-in ObjectInspectors to support Java primitives as well as collections of Java primitives, which are commonly used in deserializers, as in the case of the RegexSerDe class.
The SerDe and ObjectInspector concepts means that it’s really easy to work natively with serialized data forms such as Avro and Protocol Buffers, since they don’t need to be converted to a Hive-specific format.
You can examine how the RegexSerDe works to learn about how you would write your own SerDe.
RegexSerDe implements the SerDe interface, which in turn implements Deserializer and Serializer (not pictured)
Each record returned by the RecordReader is a row in Hive.
Ensure type is Let’s look at the RegexSerDe implementation of the three methods in the Deserializer interface, starting with the initialize and getObjectInspector methods.
Convert the object created by the InputFormat into a Hive form.
Validate that the user-specified fields conform to the specifications of the implementation.
Construct the Java Pattern object, w will be used in the deserialize meth.
The deserialize method is called for each record produced by the RecordReader, and it needs to convert the Writable into a Hive object which the ObjectInspector class you created previously can access:2
Create a structure ObjectInspector for the row that works with List and Array-based representations.
If the regular expression didn’t match the record‚ return null.
For each group in the regular expression, set the appropriate column in the array—the row is a reusable ArrayList that was created in.
The following code shows the implementations of the Serialized interface in the RegexSerDe class, with some unimportant code removed to help with readability:
Given an Object and related ObjectInspector create a Writable which is used by the OutputFormat for serialization.
Get the type of the objects that are returned by the serialize method.
Validate that the user-specified fields conform to the specifications of the implementation.
Summary Hive’s SerDe is a flexible mechanism that can be used to extend Hive to work with any file format, as long as an InputFormat exists which can work with that file format.
We’ve looked at how Hive reads and writes tables, so it’s time to start doing something useful with your data.
Since we’re looking to cover more advanced techniques, we’ll look at how you can write a custom Hive UDF to geolocate your logs.
In doing so, you’ll write the results using partitioning and bucketing, which gives you the ability to store the data in a form optimized for lookup by certain columns.
We’ll also look at compression so that you can efficiently store your data, as well as optimize your read/write IO.
Your next step is to geolocate the IP address from the logs and write a subset of the log details to a new internal Hive table.
This table will use partitions to determine how the data is laid out in HDFS.
The partitions should be organized in a way to optimize common queries such that the queries don’t need to process data in the entire table.
In your case, you want to perform lookups by day and country; therefore, you’ll use a static partition to organize the log data by day, and a secondary dynamic partition to store data by country.
Problem How do you write a custom function in Hive, and work with compressed partitioned tables? Solution Learn how to write a UDF in Hive, and how static and dynamic partitions and buckets work.
Discussion The following HiveQL creates a table which you’ll partition by date as well as by country.
To allow you to sample data from this table, it’s defined that it should be bucketed into 256 buckets (more on that shortly)
This table will become large over time because every day you’ll be adding additional data into the table, so you’re storing it using SequenceFile as the storage format, and you’ll also compress the data within the SequenceFile (which you’ll see shortly):
You have two partitions: dt is the date, and country is the two-digit country code.
Bucketing is a mechanism that helps you with samplin data from your table.
Use th compre You’ll geolocate the IP addresses from the logs table using the free geolocation database from MaxMind.
Download the free country geolocation database,4 unzip it, and copy the GeoIP.dat file under /tmp/
Next you’ll use a UDF to geolocate the IP address from the log table that you created in the previous technique, and use the country code as a dynamic partition.
The difference between static and dynamic partitions is that with a static partition, the name of the partition is hardcoded in the insert statement, whereas with a dynamic partition, Hive will automatically determine the partition based on the value of the partition field:
Whenever you write to a bucketed table, you need to make sure that you.
Add the JAR containing your UDF so that it can be used in MapReduce.he geolocation ile into the uted cache.
Define country_udf as the alias for your geolocation UDF and specify the class name.
Define an alias for your movie UDF, which extracts the movie name from the URL path.
Call your UDF specifying the field on which it should operate (the host column from the logs table), and the filename of the.
An example of both a static (the dt column) and dynamic (the country column) partitions in action.
If you examine the output of running the previous commands, you’ll see the partitions that were used as part of the insert.
The SHOW PARTITIONS command can also show you all the partitions for a table:
Using partitions and buckets also changes the physical storage characteristics of your table.
A separate directory is used to store each partition, and is nested according to how many partitions you have.
Figure 10.6 shows how your table is laid out in HDFS.
Since we had two partitions we have two levels of directories.
At the leaf directory we have a file for each.
This is called bucketing, and the reason you want to do this is that it will help optimize sample operations, as you see in the following example:
If you didn’t have buckets, your TABLESAMPLE would have to look like the following HiveQL, which would incur a full table scan:
But this approach has the upside in that it allows you to dynamically size how large you want your sample to be.
If you used buckets when creating the table, you would be stuck with whatever bucket size you specified at that time.
When writing a UDF there are two implementation options, either extend the UDF class, or implement the GenericUDF class, as seen in figure 10.7
The main differences between them are that the GenericUDF class can work with arguments that are complex types, and they are more efficient because the UDF class requires Hive to use reflection for discovery and invocation.
Generic UDFs are more efficient as they don't require reflection to call the evaluate.
Th loo We’ll first look at the simple UDF that extracted the movie name from the URL.
You’ll extend the UDF class to see how it works:5
Looking at the UDF class, there aren’t actually any metho override to implement the function.
Hive actually uses reflect find methods whose names are evaluate and matches the argu.
Hive can work with bot Hadoop Writables and the Java primitives, but it’s recommend.
This UDF works on the request field, which you split into three parts: the HTTP method, the resource.
Ignore URLs that don’t pertain to the movie part of your w.
Extract the text after the leading movie path, which contains your movie title.
Create a converter that you can use in the eval method to convert all the argum.
Converters, which you’ll use to convert the input types to the types you want.
The Description annotation, which we used to annotate the GeolocUDF class in the previous listing, can be viewed in the Hive shell with the DESCRIBE FUNCTION command:
Summary Although the UDF we looked at operates on scalar data, Hive also has something called a UDAF, which allows more complex processing capabilities over aggregated data.
Specify that the return type for the UDF (in other words the evaluate function) will be a.
After retrieving the IP address and geolocation filename from the arguments, call a function to.
Creates a string that’s used in situations such as exceptions to provide some context.
Hive also has UDTFs, which operate on scalar data but can emit more than one output for each input.
Much like SQL, a join in Hive combines records from two tables by using values common to each.
As you saw in chapter 4, joins in MapReduce were fairly complex, especially when working with joins such as map-side joins.
But when working with big data, you need to understand what Hive is doing behind the scenes and what optimizations are available to streamline your joins.
The overall goal of your data analytics is to provide the top movie categories for each country.
So far you have a Hive table containing geolocated IP addresses and movie titles.
You now need to combine your movie titles with another Hive table that also contains movie titles in addition to the category for the movie.
Problem Your Hive joins are running slower than expected, and you want to learn what options you have to speed them up.
Solution Look at how you can optimize Hive joins with repartition joins, replication joins, and semi-joins.
Discussion We’ll cover three type of joins in Hive: the repartition join, which is the standard reduce-side join; the replication join which is the map-side join; and the semi-join which only cares about retaining data from one table.
Repartition joins are essentially equi-joins (inner or outer joins) executed in the reduce side.
You can see how they work in MapReduce in figure 10.8
For this technique let’s suppose that you have another table which contains category information for your movies:
This is an example of an inner join, which only returns results for rows where both tables have the same join value:
To provide results even when there’s no match, you need to use an outer join.
There are three types of outer joins: left, right, and full.
The result of a left outer join always contains all the rows from the “left” table, even if the join condition doesn’t find any matching row in the “right” table:
A right outer join is essentially the same as the left outer join, with the roles reversed.
Rows from the right table are always returned, and only matching rows from the left table are included:
In this join rows from both tables are always included, even if there’s no match:
A left outer join includes all rows from the viewed_movies table regardless of.
You have some movies that don’t have categories, in which case with a left outer join they’ll have a.
A right outer join includes all rows from the movie_categories tables, and only matching.
A full outer join includes all rows from both tables regardless of whether there is a match.
Just like with the left and right outer joins, any rows that fail to match will contain NULL values for the.
A replicated join is a map-side join where a small table is cached in memory and the big table is streamed.
You can see how it works in MapReduce in figure 10.9
To perform a replicated join, there’s a hint you can give to Hive to indicate which table is small and should be cached:
In joins, it’s important that the largest table is the last table in your Hive statement.
The hint which triggers the map join and also tells Hive which table (movie_categories) to cache.
The MAPJOIN is merely a hint, and Hive will only perform a map-side join if the following conditions are met:
Isn’t Hive smart enough to automatically convert a repartition join into a replicated join if one of the tables is small enough? As it turns out, it is, but this behavior isn’t enabled by default.
Let’s revisit your original inner join and see how you can enable Hive’s automatic replicated join capabilities:
If any of these steps don’t succeed, Hive will default back to the regular reduce-side repartition join.
A semi-join is one where you only need the results to contain data from one of the tables:
Skew can lead to lengthy MapReduce execution times since a small number of reducers may receive a disproportionately large number of records for some join values.
Hive, by default, doesn’t attempt to do anything about this, but it can be configured to detect skew and optimize joins on skewed keys:
Enable Hive’s automatic join optimization to convert repartition joins to replicated joins.
This output tells you that Hive decided to go ahead and use a.
In a semi-join you can’t have the result contain any fields from the right-hand side of the join, which.
So what happens when Hive detects skew? You can see the additional step that Hive adds in figure 10.10, where skewed keys are written to HDFS and processed in a separate MapReduce job.
It should be noted that this skew optimization only works with reduce-side repartition joins, not map-side replication joins.
Summary This technique covered a wide variety of joining options in Hive.
Your goal was to join movie titles and their categories together, and assuming that your movie category table can fit into memory, the best join option would have been an inner replicated join.
Grouping is a relational operation used to group a set of rows together and perform an aggregate function on them, and you’ll use it to group movie categories together.
You’ll then use sorting to order them by the number of IPs that viewed that movie category, thus giving you the most popular categories first.
When you’re done, you’ll look at Hive’s EXPLAIN keyword, which can be used to figure out the query plan for your query.
Tell Hive to optimize joins where it sees skewed data.
Sets the threshold beyond which a key is considered to be skewed.
The original goal of this section was to produce the top movie categories for a country.
Let’s go ahead and write the query which will produce this result:
Why didn’t you use SORT BY for sorting the results? SORT BY only sorts results within a reducer, so if your job is running with multiple reducers, you won’t get total ordering.
But ORDER BY does give you total ordering across all the reducers.
Three MapReduce jobs are executed to satisfy the above query.
Why so many? Much like with regular databases, grouping and sorting are expensive operations in Hive.
So how can you figure out what each of the MapReduce jobs are doing? That’s where EXPLAIN comes into play.
You can prefix the SELECT with EXPLAIN and see how Hive is distributing work across the various MapReduce jobs.
The EXPLAIN output first contains the abstract syntax tree, which is simply a tokenized tree representation of the query:
It’s not particularly interesting for the intent and purpose of understanding how Hive is distributing the work.
The next output is a list of stages, where each stage describes a unit of work, which can be MapReduce jobs or some HDFS activity.
This output also indicates if any stages require other stages to complete prior to their execution:
The last chunk of data is the stage plans, which detail the steps involved in each stage.
For a MapReduce stage it indicates information about the inputs, operators, and outputs for the job.
Let’s study each of these concepts before we dive into the explain output:
MapReduce inputs can be either a single or multiple Hive tables, or the output from a previous MapReduce job.
Search for the top categories in Russia Your table is partitioned by country.
Order the results by the number of movies in each category, so the most popular category is the first result.
MapReduce outputs will include information about what fields are included in the output, as well as the OutputFormat class used to write the output.
Let’s look at the output of the first stage, which is a MapReduce job.
Let’s hope the reducer output is as simple to understand:
This tells you that the movie_categories table is one of the input tables for the job.
This tells you that the map is emitting an output key/value tuple where the key is the movie title.
The title, which is the output key, is being used for partitioning.
The output key/value tuples are the movie title and country, respectively.
The first operation on the reduce side is a join, and by looking down a couple of lines you can see that it’s an inner join.
That reducer output was a lot more complex to understand.
There were five operators chained together which joined, filtered, grouped, and emitted the key/value output tuple for the job, where the key is the movie category, and the value is a localized count of movies that were encountered as part of the join.
What do the other two MapReduce jobs do? The first MapReduce job groups the movie categories in the reducer and creates an aggregated count for each category.
The second job simply sorts all the results by the number of movies for each category.
So the group and sort in your query each incurred the expense of an additional MapReduce job.
Hive is a great tool for nonprogrammers and programmers alike who are versed in using SQL.
In this chapter I showed you a number of useful techniques related to loading and working with log data in Hive.
We focused on understanding how Hive manages data in internal and external tables, and also looked at advanced topics such as join customization and user-defined functions.
The next high-level language we’ll look at is Pig, which also offers a simple way to interact with Hadoop.
This operator is filtering results based on your criteria that only results from Russia be included.
The select operator does nothing other than pass through data to the next operator.
This group is a localized group within all the rows for a given join key.
You grouping on the movie category, so the output of this group is the catego.
The last step indicates that the output is being written to file and the output format used to perform the write.
Pig is a platform that offers a high-level language with rich data analysis capabilities, making it easy to harness the power of MapReduce in a simplified manner.
Pig started life in Yahoo! as a research project to aid in working rapidly with MapReduce for prototyping purposes, and a year later was externalized into an Apache project.
It uses its own language called PigLatin to model and operate on data.
It’s extensible with its user-defined functions (UDFs), which allow users to bump down to Java when needed for fine-grained control over loading, manipulating, and storing data.
External programs such as shell scripts, binaries, and other programming languages can interact with Pig.
We’ll work with web server log data in this chapter, and use this data to follow through the typical Pig process flow.
The techniques will cover different methods to load data, effective practices to operate on your data, such as some data analysis functions, and finally how to store results in HDFS.
After you have these techniques down, we’ll identify some tricks for optimizing Pig workflows during development and testing phases, and also examine some performance techniques to ensure speedy job execution.
This chapter isn’t meant to serve as an introduction to Pig.
It’s instead aimed at users who’ve already experienced working with Pig.
We’ll start with some very brief Pig basics and then jump into the thick of things with some log parsing.
In this section we’ll quickly skim through some Pig basics.
Pig consists of a shell, which allows for interactive command execution, PigLatin, its data flow language, and its execution engine, which translates the PigLatin commands into MapReduce execution plans and executes them.
We’ll quickly step through PigLatin, Pig’s programming language, and specifically its type system, functions, and operators.
After that we’ll be ready to move on to some techniques.
Pig’s language is called PigLatin, and is used to model data flows in Pig.
PigLatin has functions and operators that support many of the relational SQL constructs you saw in Hive, such as joining, grouping, and sorting.
PigLatin contains a small number of scalar, array, and composite data types.
Composite types are limited to tuples, ordered list fields, bags, unordered sets of tuples, and maps, sets of key/value pairs.
You’ll hear the terms relation and bag used interchangeably in Pig.
Relation is simply a synonym for bag, or a set of tuples.
Figure 11.2 shows the Pig dump command, which writes the contents of a relation to standard output, and identifies some of these concepts.
Tuples in Pig consist of an ordered list of fields.
By default Pig will treat all fields as byte arrays, unless a function, or the user, specifies a type for each field.
PigLatin provides you with a mechanism to view the type information for a given relation with the describe keyword.
Operators in PigLatin are language constructs used to load, operate on, and store data.
They can be broadly broken into different categories, such as relational, arithmetic, and others.
Relational operators are used in PigLatin to perform activities that are similar to relational database SQL constructs, such as joining, grouping, and filtering your data.
Functions in Pig are callable units of work that can be referenced in the context of specific operators.
Pig has a number of built-in functions and also allows users to define their own functions, called user-defined functions (UDFs) in Pig terminology.
Functions are grouped into one of four types, each related to a specific operator in whose context they can be used:
Load functions—These are used in conjunction with the LOAD operator, and are responsible for unmarshalling data from either HDFS or some external source into Pig’s Tuple form.
Evaluation functions—These are used in conjunction with the FOREACH operator, and manipulate data to produce modified output.
Filter functions—These, used in conjunction with the FILTER operator, filter out input data and produce an output relation, which is a subset of the input.
There’s only one built-in filter function called IsEmpty, which filters out any bag or map tuples that don’t contain any fields.
Store functions—These, used in conjunction with the STORE operator, store Pig tuple data in some storage format, either in HDFS or an external source.
Pig can be used in noninteractive mode to execute scripts containing PigLatin commands:
Another noninteractive feature is the -e option, which lets you supply a Pig command as an argument:
The -d ERROR is an option that turns off the verbose logging and only shows error output.
With this whirlwind tour of Pig completed, it’s time to move on to look at some advanced Pig techniques!
We’ll highlight some advanced Pig techniques by walking through an real-life application of Pig.
We’ll work with Apache web server logs, which contain details about how your website is being used, and attempt to mine some interesting data that could suggest misuse of your website.
Along the way we’ll look at how to write user-defined functions to load and operate on data, and also see how streaming and the distributed cache can be utilized.
Figure 11.3 shows the pipeline that we’ll go through, where each pipe will be a separate technique.
To work with data in Pig, the first thing you need to do is load data from a source, and Pig has a LOAD function that does just that.
In this section you’ll skip straight into writing your own load function.
Writing custom Pig loaders is useful in situations where you’re working with data formats for which Pig loader doesn’t exist, or if you want to provide rich capabilities to a loader such as schema details, and custom parsing features.
Adding schema details to loaders is useful because it means that users of your loaders won’t have to define the schema every time they work with your loader.
Tuning a loader to work with a specific file format removes the parsing onus from downstream users, freeing up their time to solve problems.
This chapter focuses on web server log data, so we’ll look at writing a loader for them.
If you look at the Apache Pig mailing lists, you’ll see that working with web server log files is a common use of Pig.
You’ll use this technique both to show how a custom loader in Pig works, and at the same time provide a fully featured log loader that will make working with log data a whole lot simpler.
But it doesn’t provide rich parsing capabilities, doesn’t define schema information, and is slow as a result of regular expressions.
Solution Learn how Pig loads data with LoadFuncs, and write your own, which loads Apache log files.
Discover how to use the REGISTER and DEFINE statements to register and define an alias for your LoadFunc, and how to use your LoadFunc in combination with the LOAD statement.
Discussion Your goal here is to create a more useful log file loader.
Your loader needs to be able to perform the following:
In Pig terminology, code that handles reading input and converting it into Pig tuple form is called a LoadFunc, short for load function.
Pig has an abstract class called LoadFunc that has a number of methods that must be provided by concrete subclasses, as seen in figure 11.4
The LoadFunc is tightly integrated with Hadoop’s InputFormat class; reading data from source is delegated to the InputFormat, and the concrete LoadFunc is responsible for converting it from the InputFormat’s key/value form to Pig’s tuple form.
Pig provides a FileInputLoadFunc class that extends LoadFunc, and is the basis for most Pig LoadFuncs that are file-based, including yours.
Let’s take a look at how you tell Pig what InputFormat it should use.
In your case, you’ve defined your own InputFormat class, which will parse the log file (available on GitHub at http://goo.gl/1qT7M):1
Lets the loader know about the location of the input data.
Retrieves the MapReduce InputFormat that should be used to read the input data.
Implementors should retrieve the next record from the RecordReader, map it to.
The getNext method in the following listing pops records from the input data using the RecordReader, which in your case is each line of your log file.
Because your InputFormat already deals with parsing the line, all you need to do here is grab your data from your RecordReader, and set the equivalent Pig fields.2
The advantage of creating an InputFormat that performs the parsing for you (as opposed to using a TextInputFormat and parsing the lines in your LoadFunc directly) is that you can also leverage the InputFormat directly in MapReduce, if you decide to optimize your code later.
You also want your loader to specify type information about your fields.
To do this you need to implement the LoadMetadata interface and specify an ordered list of your.
Here you’ creating it and presetting the size to the number of field.
If the RecordReader has reached the end of the input split, you return NULL to indicate to Pig that you’ve.
Call your setTuple method with the value from your RecordRead Your RecordReader gives you a Java Bean representing the log l.
TupleFactory is a factory that can engineer Tup from various representations, including an ArrayL.
When your LoadFunc has been created, you need to register the JAR that contains the class with Pig, specify an alias (to prevent finger strain from repetitively typing in package names), and then load using the LOAD operator:
You’re done! Now your data is loaded with field names and types defined by your LoadFunc.
Let’s perform a simple operation and count the number of requests for each HTTP status code, which can be used to validate that the majority of requests are successful and, if not, serve as a starting point for further diagnosis into potential misuses of your websites:
Consider the Pig code you would write to use this LoadFunc:
Using the CommonLogLoader, you would need to define your own schema, handle the Apache no-data character (-), and, if you wanted to work with the date in epoch form, perform that conversion.
Your LoadFunc contained all these features, which makes it easy to work with the tuples it produced.
In your LoadFunc, you could have left the schema definition (using the AS construct) to the user.
But this would have meant that the schema would have to be defined each time the data’s loaded, so it’s best to let the LoadFunc specify the schema.
There’s no requirement in Pig that all tuples in a bag contain the same fields; in this sense Pig supports sparse data.
I just showed how to load data into Pig, which was an important prerequisite to working with that data.
Now that the data is loaded, the next step is to perform a number of Pig operations on your data.
Filters provide a mechanism to reduce a relation into a subset of tuples, and projection is a way to reduce the number of fields being worked on.
There’s a reason that we’ll cover them in the first manipulation technique; they’re one of the most effective techniques to reduce the amount of data you work with.
Less data in the Pig pipeline means faster execution times, which is a win for you!
We’ll look at some filtering methods you can apply to your log data.
Problem How do you remove tuples from a relation? Solution Use the FOREACH operator for projection to cut down on the data you are working with.
This technique also covers the FILTER operator to remove unwanted records and writing your own UDF to perform custom filtering.
Projection gives you the ability to reduce the fields being operated on with the FOREACH operator.
In this example, you’re looking for malicious user activity, and so you’re interested in agents that may be looking for certain URLs on your web server that don’t exist.
So you only really care about retaining the IP address, HTTP response code, and the resource, which you can achieve with the following statement:
Your goal with filtering is to remove log entries that originated from inside your organization, and to only retain log entries that resulted in a 404 HTTP response code.
Assume that all internal IP addresses are private.4 Luckily, Pig supports regular expression matching, which you can use for this technique.
The FILTER operator selects tuples based on the condition that opera.
The MATCHES keyword is used to denote that a regular expression match should be performed.
If you need a more sophisticated filtering capability for your data, the filter function can work in conjunction with a UDF to reduce the dataset.
Let’s suppose that you want to optimize the filter you just wrote to not use regular expressions, and to package it so that it can be easily reused.
The UDF will be supplied each tuple in the relation, and you need to return a Boolean indicating if the tuple should exist in the result relation, as you see here:5
Your UDF constructor creates and populates a list of private IP ranges.
This method is used to specify the expected input type for your UDF.
To call your filter function you need to register your JAR, define an alias for your class, and then use it with the FILTER operator:
Outside of the FILTER operator, other mechanisms that can be employed for the same effect are Pig streaming, and MapReduce in conjunction with Pig.
Summary A popular use of filters is to remove tuples that contain a NULL value for a field.
The following example removes all log tuples that don’t have a user ID:
This completes the first data manipulation step in your pipeline, which you used to remove private IP addresses.
Grouping allows you to collect data together for downstream processing.
Grouping can occur based on individual or multiple fields in your tuples.
The downstream processing can be in the form of aggregate functions, or any other data-related analysis that you want to perform over your grouped data.
Aggregate functions are functions that let you perform operations over a collection of items.
They’re typically used for summarization operations, such as determining the number of requests that resulted in unsuccessful HTTP status codes, or finding the most popular URL.
After the filter operation, you now want to group log entries together by IP address, and then provide counts.
Problem How do you group tuples together in Pig and apply aggregate functions on them? Solution Use the GROUP and COGROUP operators, and learn how they relate to JOIN and FLATTEN.
Discussion The filter technique produced a filtered_logs relation, which you’ll now use to group logs with identical IP address and status code together:
The group command generates a tuple per unique field being grouped, with the resulting tuple containing two fields.
The first field contains all the fields that were used in the group operator, and its name is always group.
The second field is an inner bag6 containing all the tuples from the original relation which matched the group field.
The field name of the inner bag is the same as that of the original relation, in your case filtered_logs.
You can use the DESCRIBE command to show this information:
With your data grouped together, you now want to count the number of occurrences of the combination of IP address and status code.
To operate on grouped data you need to use the FOREACH operator, which lets you perform functions over each row of the group relation:
In the preceding example you used the COUNT aggregate function to count the number of log entries for each unique combination of IP address and HTTP status code.
Summary The power of groups in Pig is that you can group on multiple fields, and you’re not restricted with what you do in your data pipeline with your grouped tuple.
The GROUP keyword can only operate on a single relation.
If you want to group multiple relations together, the COGROUP keyword provides that functionality.
This may prompt the question: what’s the difference between a cogroup and a join? With a join, the results won’t be grouped together by the join key, but with a cogroup they will be.
Let’s look at a simple example to better understand this difference:
An inner bag is simply a bag that’s contained within a tuple.
An outer bag, which is referred to as bag generally in this text, is the overall container for tuples (outer bags are also referred to as relations)
Therefore, a JOIN is analogous to a COGROUP combined with a FLATTEN.
Pig performs a group or cogroup in the reduce phase of MapReduce because it.
As a result, group operations can be parallelized, which we’ll look at in section 11.4
Each unique grouped field value will be fed to a single reduce function, along with all the tuples that contain the same field value.
This can lead to data skew if some grouped fields have a significantly larger number of matching tuples than others.
This can be exacerbated in situations where aggregation functions being performed on the grouped data aren’t algebraic, because this means that the entire input relation tuples are fed to the reducer.
All the built-in PigLatin aggregate functions such as COUNT are algebraic.
An algebraic function is a special kind of function in Pig, which can be decomposed into subfunctions corresponding to, and invoked from, each of the MapReduce phases (map, combine, and reduce)
Algebraic functions must implement the Algebraic interface, as seen in figure 11.5, and provide UDF EvalFuncs corresponding to each MapReduce phase.
Functions such as COUNT, SUM, and MAX can be algebraic because they can be broken down into multiple steps.
They don’t need to have all the tuples in a group at one time to perform their operations, and they can work on subsets of the data and produce intermediate values that can be aggregated in the reducer.
How do you determine how efficient a function is and whether it has algebraic properties that can optimize.
Let’s compare the output of explain against a relation generated with an algebraic built-in function (COUNT), and contrast it with a contrived UDF I created that isn’t algebraic.
Notice that your UDF is only invoked in the reduce phase, not in the map phase.
Contrast this with the explain result you see when applied to the algebraic count function:
Here you see the function being applied in the map, combine, and reduce phases.
That’s an overall win for the efficiency and execution time of your MapReduce job!
Not all functions can be algebraic, such as cases where the ordering of the tuples in the group is significant.
If a function can’t be algebraic but can operate on subsets of the data, it’s recommended to implement the Accumulator interface, which can be seen in figure 11.6
The accumulator is a reduce-side optimization, where rather than feed the UDF a single bag containing all the group tuples, the group tuples are fed to the accumulator in chunks, and at the end the accumulator is asked for the final result.
At this point in your pipeline, you’ve loaded some log files, you filtered out entries that contained private IP addresses and had good HTTP response codes, and you grouped them by IP address and HTTP response code.
Your next step is to geolocate the IP addresses that will be useful to identify if attacks on your website are originating from specific countries.
Geolocation isn’t a built-in function with Pig, so you’ll need to write your own using an evaluation UDF.
Evaluation UDFs, or EvalFuncs, are a mechanism that allow custom functions to be performed on a bag of tuples.
They can only be used in conjunction with the FOREACH ...
Generally, EvalFuncs fall into one of the broad categories in table 11.1
We’ll take a look at implementing a simple EvalFunc in the following technique.
Simple These are functions that simply extend the EvalFunc class.
They operate individually on each tuple in a bag, and generate a single result for each input tuple.
Aggregate These functions also extend the EvalFunc class, but they differ from the simple functions due to the fact they operate over results generated by a GROUP operator.
If you recall from the overview earlier in the chapter, a group operator results in a single tuple per unique group field, where the first field in the tuple is the unique group field, and the second field is a bag of tuples containing all the tuples that matched the group field.
It’s advised that your functions be algebraic or accumulator if you are working with group tuples to help with memory utilization and to break up the work.
Called one time, once all the tuple chunks have been sent to the accumulate function.
You want to geolocate IPs in your log files for data analytics, data mining, or potentially security reasons.
Problem How do you write an evaluation function in Pig? And how do you join your log data with geolocation data? Solution Write an EvalFunc that uses the Distributed Cache to perform IP geolocation using the MaxMind library and data.
Learn how to register multiple UDFs that can support different input types.
Discussion This technique will highlight how you write a custom evaluation function, and also how you can use the distributed cache in Pig.
The distributed cache is a MapReduce mechanism that allows local files and files in HDFS to be made available to MapReduce code.
Pig supports the distributed cache, and we’ll use it to make your geolocation data available to your evaluation functions.
If you want to write your own function to manipulate some data, you’ll need to extend Pig’s EvalFunc class as shown in the following code.
You indicate the return type of your EvalFunc when you extend EvalFunc, and the same type is returned in the exec method.
To perform the actual geolocation, you use MaxMind’s GeoLite Country Lite binary file, which is free.
You need to somehow transport the geolocation data file to the DataNodes so that you can load the file with your MaxMind Java client.
Luckily, Pig has support for working with the distributed cache, and if you override the getCacheFiles method, you can specify what local files should be copied into the distributed cache.
Your exec method then can assume that the files are available locally on the DataNode on which they are operating.
Algebraic These implement the Algebraic interface and provide a mechanism to perform an operation over the entire bag.
The result must always be a scalar type, for example, the result of an algebraic function can’t be a tuple, bag, or map.
What’s special about these functions is that they can only work on an unordered subset of the overall data.
These functions are essentially invoked initially in the map phase, subsequently in the combine phase, and finally in the reduce phase to produce the final result.
Accumulator These implement the Accumulator interface, which provides a memory-efficient mechanism to operate over a bag of data.
It’s applicable to operations that typically can’t be expressed in algebraic form.
For example, this would apply if your function needs to run in the reducer so that all the input data is sorted.
Data is fed to the accumulator in chunks (typically one or more times for each reducer key)
Let’s take a look at your geolocation EvalFunc in this listing.7
Before getting into the PigLatin code to run the geolocation, let’s quickly recap where you’re at in the pipeline.
At this stage you have a relation called addrstatus_counts that contains a unique combination of IP address and HTTP status code, and a count of log entries for each combination:
Handle the case where you’re passed a NULL or empty tuple.
Get the first field in the tuple, which you assume to be the IP address.
Perform the geolocation and extract the country from the result.
MaxMind uses N/A to indicate no data for an IP.
Now let’s run your UDF against the IPs in your log file.
You’ll need to download theMaxMind binary geolocation file fromhttp://www.maxmind.com/app/geoip_country and extract the GeoIP.dat file into the local filesystem at /tmp/GeoIP.dat.
You’ll geolocate the IP address contained in the remoteAddr field, but you also want to make sure that you retain all the other fields in the resulting tuple too.
You’ll need to manually set a config setting to specify the file to load into the distributed cache:
With UDFs you can optionally define both input and output type information.
The advantage of doing this is that the Pig framework will perform validations, which will let the user know if there’s a type mismatch.
In your code you specified the output type as String, but you assumed that your input would be a tuple with a single String field.
What would youdo if youwanted yourUDF to specify what you expect your input type to be? And is there a way to support different input data types? In the example, youmay want to be able to support both the dot-separated string form of an IP, and the numeric form of the IP.
You have two options available: you can either determine the type (using instanceof) in your execmethod, or you can specify in your UDF type-specific alternative implementations, which is neater and removes ugly type-switching code.
If this function isn’t supplied, the original UDF will always be used regardless of input type.
In the example, you want to use your existing UDF to process IP addresses in string form, but use a different UDF to process IP addresses in numeric form:8
Update the Hadoop configuration indic where a file exists that should be copied.
If the file exi the local filesystem, you must use th.
If the file al exists in HDFS, you don’t need to spec scheme.
This indicates that you Hadoop to create a symbolic link with.
If you recall, in your UDF code e you use this link name to load the file.
Indicate that for the current class you expect a tuple with a single field, which is a.
Summary Are there any other approaches you could have used to geolocate your log data? You could have written a regular MapReduce job and called that directly from Pig.
But your Pig function was vastly simpler than your MapReduce code would have been.
Your geolocation database was around 1 MB in size, but if you tried to use this methodology to work with very large data files (hundreds of MBs or larger), you probably want to rethink your strategy and consider moving your data into HDFS and performing a native join.
Streaming is a process by which Pig will pipe relations to standard input of a process and collect the output relation from the process’s standard output.
It’s useful in situations where you have existing applications or utilities that you want to use to perform some manipulation on your data.
In this technique you’ll write a very simple Python script that will simulate calling out to an external security system.
Your simulated security system will return a score representing the riskiness of the IP address.
You’ll see how this script can be integrated into a Pig pipeline.
Problem How do you integrate Pig with existing scripts and utilities? Solution Use the STREAM THROUGH operator to stream your Pig tuples through some Python code.
Discussion Figure 11.7 shows how Pig streaming works in the context of MapReduce.
Pig has the PigToStream interface, which defines a method to marshall tuples into byte form, and StreamToPig to perform the reverse.
The power of Pig’s streaming is that you have complete control over what you output.
While with UDFs you’re limited to either filtering or evaluating, in streaming you can do both.
With Pig streaming, tuples are by default separated by lines, and tuple fields are separated by the tab character.
The tuple field delimiter can be overridden by specifying an alternate delimiter in the PigStreaming constructor for input and output.
In this example, you’ll specify that the comma character should be the input and output delimiter:
Your script must be available on the Hadoop cluster for Pig to call it during the MapReduce job.
If you use the DEFINE command, this activates Pig’s auto-ship mechanism, and it will copy the script from the local host to the cluster nodes.
If your script has multiple dependencies beyond the name of the script provided in the DEFINE statement, you can specify a SHIP option, which can take more filename arguments and will copy them to the cluster.
If you don’t use DEFINE, it’s your responsibility to ensure the scripts are available on your cluster hosts.
Summary Pig streaming is a useful way to manipulate data.
It allows any programming or scripting language to interact with your data in Pig.
Joins are one of the most important features in a data processing pipeline because they allow multiple relations to be combined together.
You’ll leverage Pig’s joining support to join your IP addresses with a aggregated dataset that includes details on how many times you’ve flagged the IP address as suspicious in the past.
Before you get started, let’s see where you are in your Pig pipeline, which you can see in figure 11.8
In this technique we’ll look at how to take data from multiple data sources and join them together.
Problem How do you combine data in Pig? Solution Use the JOIN operator to perform joins, and also learn how to perform full and outer joins.
Discussion You’ll load another relation from a file containing historical information about the number of times an IP has been classified as bad.
As you can see, the joined relation qualifies each field with the appropriate original relation name.
Summary By default the undecorated join keyword results in an inner join, which means that.
What if you wanted to include all the IPs in both relations? This is called a FULL join:
You can also use the LEFT and RIGHT keywords to always include one side of the relation, even if the other doesn’t have a matching field:
Joins in Pig require that all the tuples for a given instance of the field being joined can fit in memory.
This may not work well for cases where a relation contains many entries for the same field.
Pig supports a repartition map-side join by appending USING 'replicated' to the end of the JOIN statement.
It also supports a merge sort, where both relations are presorted by the join key.9
Pig supports ordering relations on any tuple fields, as long as the fields are scalar data types.
It orders numeric data numerically, Strings (chararray) lexicographically, and bytearray as binary data.
We’ll sort your data by the count of log entries for each IP address and HTTP status code combination.
Problem How do you sort data in Pig? Solution Use the ORDER BY operator to sort your tuples.
You want to sort your URL path counts in ascending order‚ which you would do as follows;
Summary There’s a gotcha to watch out for with sorting in Pig.
Pig breaks the normal MapReduce pattern and doesn’t guarantee that only a single reducer will receive all of the map values for a map key.
Pig attempts to optimize the reduce phase by balancing all the map output across all the reducers, which means a key can exist in multiple reducer output files.
This may be bad news if you have downstream MapReduce code which assumes that only a single part file contains a key.
The other gotcha to be aware of is that the ordering of tuples that have the same sort value is nondeterministic.
For example, if you were sorting on HTTP status code, and multiple log entries contained the same status code, their ordering in the result tuple may vary across different runs.
Sorting is the second-last item in your pipeline, and your final step is to store your results into HDFS.
After you’ve loaded data into Pig and performed some interesting operations on your data, you’ll want to store results.
We’ll take a look at how you can use Pig’s built-in storage capabilities, and also write your own storage function.
What if you wanted to store your Pig output in a SequenceFile? One reason that you may want to do this is because your output size would be more compact (due to SequenceFile’s binary data format and optional compression)
Therefore, if you’re looking for built-in support in Hadoop for writing SequenceFiles, you’re out of luck.
Solution Write a StoreFunc that will be used to write tuples in SequenceFile form in HDFS.
This technique also covers how to load them back into Pig.
They’re essentially a key/value-based binary serialization file format, where internally records or blocks can be compressed and split across multiple input splits.
The challenge with this technique is that you want to write out your log Pig tuples into your SequenceFile, and each tuple contains multiple fields.
The only builtin Writable array type in Hadoop, ArrayWritable, can only store one type of Writable, so it’s not a good fit for storing your tuple, which contains a variety of field types.
Luckily, all tuples in Pig are Writable classes, so you can use the DefaultTuple class to handle reading and writing your tuple and fields to the SequenceFile.
Let’s roll your own Pig StoreFunc, which can write data to SequenceFiles.
To do so you need to extend the StoreFunc class.
You want to allow users to specify if compression should be used, and the compression codec, so you provide a constructor which takes this information as input.
You do since the value will contain the entire tu.
You can’t specify the Tuple interface as your type because the type needs to be a concrete class with a default constructor.
Because you don’t know the concrete tuple class ahead of time, you specify the value type as DefaultTuple, and therefore have to convert between the relation tuple and a DefaultTuple when writing individual records to the SequenceFile.
Let’s look at how you’ll use your new StoreFunc to write your output:
Now that you’ve successfully written your SequenceFile, how can you read it back in?
The Piggybank SequenceFile LoadFunc unfortunately only supports simple built-in Writable types and won’t work with your values, which are DefaultTuple types.
I won’t show the code here, but it’s available on GitHub.
The DefaultTuple doesn’t store any information about the field names, so you need to specify them when you load your data:
Summary I chose to use SequenceFiles for data storage since they’re easy to use and offer a compressed binary file format.
Efficiency is important not only when you process data, but also when you decide how you store data, and this technique exemplifies the use of a compact data store for your data.
Avro would have been another contender for a compact data format.
Piggybank comes with AvroStorage, which could have been used for this technique.
In this section we covered various ways you can save your useful data that you’ve computed.
Saving data as text is easy, but often not the most efficient use of your storage tier, and the focus here was how to make better use of your valuable storage.
The next question you may have is, how do I streamline the process by which I work in Pig?
Working with large datasets in Pig frequently results in the following situation: write a number of data flow statements, execute a store or dump, and then wait for long period of time as the data pipeline is executed and produces results you can evaluate.
At this time it’s likely you’ll need to go back, make some tweaks, and repeat the process.
The above scenario begs the question, how can you work in a rapid manner and quickly iterate over your PigLatin with large datasets?
We’ll cover four keywords in Pig that will help you work effectively with your large datasets.
Problem Working with large datasets can result in long pauses in your workflow as you wait for results to be generated.
Solution Learn how the LIMIT, SAMPLE, and ILLUSTRATE operators can be used to speed-up your Pig workflow.
This technique also covers use of the EXPLAIN operator and looks at the output, and how it can be interpreted to understand how Pig is executing your pipelines in MapReduce.
Discussion The four Pig operators that we’ll cover are summarized below:
We’ve used it throughout this chapter to examine the schema of relations.
With the LIMIT operator you can reduce the size of a relation to a more manageable size, which is useful when you’re developing a new data pipeline in Pig and want to work quickly with your data.
The following series of PigLatin statements will serve as your starting point:
To drop the overall runtime of this pipeline for testing purposes, you’d inject a LIMIT as early on in the pipeline as possible.
Note that you applied the LIMIT immediately after the LOAD to cut down on the dataset being pushed through the pipeline.
The SAMPLE operator will randomly pick a subset of a relations tuple.
You need to indicate the percentage of records you want in your sample relation:
The first argument to the RandomSampleLoader is the class name of the Pig LoadFunc, and the second argument is the number of tuples that should be randomly extracted.
The ILLUSTRATE operator actually examines your pipeline and generates data to ensure that each pipe will have data fed through it.
It’s a useful mechanism to exercise your whole pipeline with a small generated dataset prior to running it against real data.
In the example your input data doesn’t have any logs with an HTTP status code of 400, as evidenced by the output of dump.
But using ILLUSTRATE will generate data for that data pipe:
You must supply relation field names in order for ILLUSTRATE to work, but it doesn’t need type details for fields.
Explains are a way to get insight into Pig’s execution plan for your data pipelines.
The output of explain contains three sections: a logical plan, which shows the pipeline and the sequence of operators that are applied to it; a physical plan, which shows the plan in the context of data sources and sinks; and, finally, the MapReduce plan which shows where operators are applied in MapReduce.
Explains are a very useful guide to gauge the efficiency of a data pipeline.
You saw an example of that in technique 69, where you used explain to determine whether an aggregate function was algebraic.
The first piece of information that the explain plan gives you is the logical plan.
The logical plan is focused on the operations, input and output types for each operation, and type conversions required for the operation to function.
The plan layout contains the end result at the top, and the starting point at the bottom-most node:
Next up is the physical plan, which looks at the pipeline execution from the perspective of input and output data sources for the operators.
Just like with the logical plan, you read it starting from the bottom:
The child odes are the logs relation which its input, and the equals perator, which is executed to erform the filter.
Therefore, Pig forms this cast operation to vert the log status field from earray to integer as a precursor the comparison.
This is a projection o status field in th relation for purpo.
It iden the type of the fie name, and also the.
You can see your fields, and the default Pig types for untyped fields, bytearray.
If you follow the tree edge to the parent you can see that this is.
Lastly, the MapReduce explain plan shows the various MapReduce jobs that are executed for your data pipeline, and how your operations are distributed across the various map, combine, and reduce phases.
With your simple Pig data flow, you only have a single MapReduce job, employing just the map phase:
Figure 11.9 shows the image produced in Graphiv for the MapReduce explain output.
Summary The four operators identified in this technique are useful tools to help expedite your workflow process.
For more robust and repeatable testing, I recommend taking a look at PigUnit, a Pig unit testing framework.
After you have a data pipeline that performs the function that you need, it’s time to turn your attention to performance aspects of Pig that can help reduce the execution time of your data flows.
Indicates the data source used by the LOAD operator, which is in HDFS.
In computer science you’re taught to avoid premature optimization, since many assumptions around what’s efficient and what’s not efficient are quite often incorrect.
With big data, efficiency and performance become a core part of your work and thought processes.
When you’re executing the same code on billions of records, even the smallest nonoptimal code can have big consequences.
The single act of using regular expressions, either directly or indirectly (by using String.split, for example) is an example of code use that at face value sounds reasonable, yet can have some negative performance results when applied to large datasets.
This was one of the reasons that your log file LoadFunc earlier in this chapter avoided the use of regular expressions (the use of regular expressions seems to be prevalent in most log file LoadFuncs)
Therefore, with MapReduce and Pig there are certain data and network patterns that are almost guaranteed to help reduce the runtime of your MapReduce jobs.
Let’s review some data and network performance patterns to help make your Pig jobs go from a trot to a sprint.
Problem When you’re working with very large datasets you’ll often find that your jobs take longer than you expect.
Solution Look at how filtering, reducer parallelism, sampling, and other patterns can be used to squeeze the maximum performance out of Pig and your Hadoop cluster.
Discussion The solution will be composed of bite-sized recipes that you should keep in mind when working with Pig.
FILTER, FILTER, FILTER A key pattern with MapReduce is to discard data as soon as you don’t need to work with it anymore.
This is exemplified by a map reducing the input data and emitting a subset to the reducer.
With Pig, the same effect can be achieved with use of the FILTER operator to reduce data that you’re working with.
Try to use FILTER as early in your data pipeline as possible, preferably immediately after a LOAD, if possible.
The following example shows discarding logs that are from the loopback address:
Another application of this pattern is as a join optimization.
Joins that operate on NULL fields are wasteful since they’re dropped.
You can apply your filter pattern to discard tuples that contain a NULL field value prior to a join:
You can use the RandomSampleLoader to load a random subset of your file if you want to perform work on a sample of your data.
You can also use the LIMIT keyword to reduce the number of tuples in your relation, as follows:
If your UDF is an aggregate function (meaning that it needs to operate on a relation that’s the result of a GROUP operation), then it’s highly encouraged that you define your function to be algebraic and/or accumulative.
By default, if you don’t and your group tuples are large, there may be overhead associated with spilling data to disk when working with your data.
Joins in Pig involve the reducer loading all the left-side relation tuples into memory, and then combining each left-side relation with each right-side relation to produce the joined relations.
If you know that one relation contains more tuples for a join than the other, you would want to have that as your right-side relation.
Pig has the notion of replicated joins, where if one of the relations fits into memory, it can load the entire relation and perform the join in the map phase.
The USING 'replicated' decoration can be added after the relation field name to enable this behavior.
Skewed joins can be used if the default MapReduce key partitioned distributes the majority of tuples to a small number of reducers.
Skewed joins employ some key sampling to predetermine a more even distribution of keys across all the reducers.
The USING 'skewed' decoration can be added after the relation field name to enable this behavior.
Some operators can be executed with multiple reducers in MapReduce, and it’s useful to understand how to control the parallelism of these reducers, and also which operators this applies to.
There are a number of operators11 in Pig that result in both a map and a reduce phase.
In such cases, it’s important that the reduce phase is parallelized; otherwise the reducer will run as a sequential operation in a single reduce task, which will be slow.
Pig will determine the number of reducers to be used for each operator usage in order of precedence, as follows:
For example, to run a group operator using 50 reducers you would do something like the following:
This is useful in situations where you have a default parallelism that you want applied to all operators that support parallelism.
An example of setting this to 50 would be as follows:
Summary I’ve outlined some approaches which will help optimize your data pipelines.
As with any performance optimizations, you should always prove to yourself that an optimization actually yielded an improved or more efficient result.
Pig is a tremendously useful tool for use by programmers, data analysts, and data scientists.
It offers a high level of abstraction on top of MapReduce, with the flexibility to bump down to the Java and even lower to the MapReduce layers if custom functions and/or performance improvements are required.
In this chapter I showed you a number of useful techniques related to loading, manipulating, and storing your data:
How to write your own Apache web server log file loader to simplify downstream data processing.
Aggregate and relational data manipulation techniques to mine basic analytics from your logs.
Our final foray into MapReduce abstractions is Crunch, which is a Java library that makes it easy to write and execute MapReduce jobs.
Much like Pig, it’s a pipeline-based framework but, because it’s a Java library, it offers a higher level of flexibility than you get with Pig.
Crunch is compelling in that it allows you to model MapReduce pipelines in Java without having to use MapReduce constructs such as Map/Reduce functions or Writables.
Crunch also benefits from not forcing its own type system onto programmers wishing to hook into the framework, unlike Pig and Hive, which impose their own data types if you want to write UDFs.
For programmers, this means you spend less time wrestling with MapReduce and Pig/Hive concepts, and instead focus on solving your actual problems.
It should be noted that Crunch shares a lot in common with Cascading, which we’ll briefly look at towards the end of this chapter.
Up until now we’ve looked at Pig and Hive, which are high-level MapReduce.
What is Crunch? Because Crunch is a newcomer to the MapReduce scene, we’ll spend a little time introducing the basic Crunch concepts before looking at some techniques.
In the techniques, we’ll look at how you can use Crunch to find popular URLs in your log files, and we’ll also look at some advanced Crunch use cases such as joining your logs with a user dataset.
After you have a handle on the fundamentals, you’ll apply them to a simple example of how Crunch can be used to tokenize strings.
Crunch is an implementation based on FlumeJava,1 a paper published by Google detailing their data pipeline library for working with MapReduce.
FlumeJava, like Crunch, is a Java library that makes it easy to create MapReduce pipelines using an abstracted MapReduce API.
Much like Pig and Hive, FlumeJava and Crunch were created to provide a low-cost entry into MapReduce.
Crunch was written by Josh Wills, a former Google employee who used FlumeJava and decided to write an implementation that would work with Hadoop.
A collection of data, representing data loaded from a data source such as HDFS, or a derivation of that data based on operations in Crunch.
A type mapping system, which allows data stored in HDFS (or any other data store) to be mapped to types used in Crunch.
Crunch comes bundled with a number of built-in operations to perform aggregate and join functions.
Crunch is extensible, so you can write and build a library of reusable custom functions for use in your Crunch pipelines.
Let’s look at the basic concepts in Crunch, including its type system and pipelined architecture.
The Crunch pipeline is represented with the Pipeline interface and MRPipeline implementation class, which you can see in figure 12.1
As you can see, the Pipeline class contains methods to read and write collections.
These collection classes have methods to perform operations on the contents of the.
Therefore, a pipeline consists of the definition of one or more input collections, a number of operations on these and intermediary collections, and the writing of the collections to data sinks.
The execution of all the actual pipeline operations is delayed until the run or done methods are called, at which point Crunch translates the pipeline into one or more MapReduce jobs and starts their execution.
In Crunch the collection interfaces represent a distributed set of elements.
A collection can be created in one of two ways: as a result of a read method invocation on the Pipeline class, or as a result of an operation on another collection.
There are three types of collections in Crunch, as seen in figure 12.2
The PGroupedTable is a special collection that’s a result of calling groupByKey on the PTable.
This results in a reduce phase being executed to perform the grouping.
What is Crunch? Collection classes contain a number of methods, which operate on the contents of the collections, as seen in figure 12.3
These operations are executed in either the map or reduce phase.
Functions can be applied to the collections that you just saw using the parallelDo method in the collection interfaces.
All the parallelDo methods take a DoFn implementation, which performs the actual operation on the collection in MapReduce.
Crunch comes with a number of built-in operations (such as joining, grouping, and counting), which, as you’ve seen throughout the book in various guises, represent.
Combine all the values for each group into a single value.
All members of the DoFn implementation must be Serializable, because Crunch serializes the function.
Because Crunch already has these operations defined, you don’t need to wrestle with MapReduce to write your own.
Crunch also lets you define your own custom operations, which you’ll use later in this chapter.
As you can see in figure 12.5, the parallelDo methods that you saw on the PCollection interface all take either a PType or PTableType argument, depending on whether the result was a PCollection or PTable.
These interfaces are used by Crunch to map between the data types used in the Crunch pipeline, and the serialization format used when reading or writing data in HDFS.
As you can see, Crunch has serialization support for both native Hadoop Writable classes as well as Avro types.
We’ll look at two simple examples, the first which results in a map-only job, and the second a full MapReduce job.
For your first example, imagine you have a bunch of text files containing multiple words per line, and you want to tokenize each line and produce output such that each word exists on a separate line.
Figure 12.6 shows a visual representation of the Crunch pipeline to perform this function.
PType represents how to convert a type T into the HDFS input and.
The MRP overall C of a num MRPipeli translate or more.
Informat output c Crunch h and Writ the Writ use the W serializat The code for this Crunch pipeline can be seen here:2
You didn’t use the groupBy method in your pipeline, which means that this will be a map-only job (groupBy is a trigger for the reduce phase to be used in a pipeline)
For the second example, you’ll look at a full-blown MapReduce job with Crunch.
You’ll take the inverted index example you wrote in chapter 1 and write a Crunch version of it.
As a reminder, the inverted index job takes a number of text files as input and produces output such that each line contains a unique word followed by a list of unique filenames that it occurs in, as seen in figure 12.7
Text file Figure 12.6 Crunch pipeline for text file tokenization.
The ne class knows how to the overall pipeline into one MapReduce jobs.
The first thing y to do with a pipel specify your input Crunch the input d.
The process method is called once per ent the input collection, which in your case con the lines from the input files.
Specify that the words collection should be written out to the specified path.
Let’s look at the Crunch code to create the inverted index:3
Let’s look at the two helper functions, starting with the first one, which creates a map of words to the filenames from which they originated.
The PTable is a multimap, which means that the same key can occur multiple times with different values:4
Use a helper method to convert collection of lines into a mult.
You call a helper function, which will produce a unique set of.
Extract the type information from the collection so that you can use it la.
This time you’re using the PTable version parallelDo method, since you want to.
In the previous example, you went a step further and overrode the setContext so that you could extract the input split filename from the context.
Let’s take a look at the uniqueValues helper method that you wrote, which calls groupByKey (which means the reducer is used) to group the keys in the PTable together, and uses a CombineFn to iterate over all the values for each unique key.
Your CombineFn anonymous inner class then produces a single output tuple for each unique key:5
In this section you learned the basics about the Crunch API and implemented two examples in Crunch.
Next we’ll look at some real-life usages of Crunch for log processing.
In this section we’ll look at a practical example of using Crunch to perform some simple analytics on Apache log files.
Working with log files is a common use of Hadoop, and by doing so you’ll gain additional familiarity with Crunch.
You use the groupBy method to group all the keys in the collection.
Crunch fulfills a groupBy with a reduce phase in a job.
A combine function is one where th process method is called once per uniqu.
Produce output such that the key is the word, and the value is all the filenames.
Writ aggr resu HDF highlight more advanced DoFn features, such as working with complex Writable structures, and implementing combiner functionality in Crunch.
In this technique you’ll look at how you can work with log file data to produce the most popular resources (URL paths)
Problem You want to work with complex data such as log file formats in Crunch.
Solution Learn how to parse Apache log files, and how filtering, projection, and combiners can be used in Crunch.
Discussion You’ll write a Crunch pipeline which will read data from Apache log files, extract the resource, and then count the number of occurrences of each unique URL to give you basic usage analytics for your website.
Call a helper function to engineer a collection of CommonLogEntry.
Call a method to filter ou records you want to ignore an also extract just the resourc.
Call a built-in Crunch aggregation helper method, which counts each.
For this technique you’ll leverage some code you wrote in chapter 11, which was about Pig.
You’ll leverage both of these classes in your first DoFn:7
You don’t want to abort processing the file if a line is malformed, so you update a counter and.
On a related note, Crunch doesn’t support throwing checked exceptions in the DoFn process method, but an.
If you hit a parsing issu update a counter and dum.
Next up is the second operation in your pipeline, which is a filtering and projection operation.
You ignore log entries that originated from the localhost IP address, and you also extract just the resource from the log entry:8
Finally, you leverage one of Crunch’s built-in functions, SUM_LONGS, to count each resource occurrence:
When a CombineFn is executed on a PGroupedTable (which is the result of calling the groupByKey), this is a signal to Crunch that the operation being performed is distributive and can be executed on both the map side (as a combiner) and in the reduce side.
If the function being implemented isn’t distributive (such as calculating a mean), you’d want to call the regular parallelDo method rather than call combineValues.
Let’s run the code against your Apache logs file and examine the output:
Convert a PCollection to a PTable by emitting a count.
Run a built-in combiner function which sums all the values together.
Summary In this technique we looked at how you could work with a complex data type representing log data and perform a basic analytics function.
In the next section we’ll look at how you can join together two separate datasets.
Imagine you have two datasets that you wish to combine, such as log data and user data, so that you can understand how your users are using your website.
Joining, which can be used to combine these datasets together, is a powerful capability you get with MapReduce.
In previous chapters, we looked at how you can join in native MapReduce, Pig, and Hive, and now it’s time to do the same with Crunch.
Crunch doesn’t require you to implement your own join logic, and it contains some code to make joins easy.
In this section we’ll look at how to leverage this code to join together the IP addresses in your logs with another dataset which contains IP and user details.
We’ll also look at how you can access the Crunch pipeline results directly from your Java code.
We’ll look at how you can use Crunch’s built-in joining capabilities to join logs and user data together.
Problem You have two distinct datasets that you want to join in Crunch.
Solution See how the Join.join method can be used to join datasets together.
Discussion Crunch has a Join class that provides support for a reduce-side repartition join.
In this technique you’ll join your log data with some user data.
The user data contains IP addresses and users that are associated with each IP address.
Figure 12.9 shows the pipeline that you’ll execute for this technique.
It’s important that the smaller dataset is supplied as the first argument in the join function because data from this dataset will be cached to perform the join.
The two functions that you wrote to convert the logs into a table and to load the users file into a table are shown here:10
Call a function to convert the log strings into a table where the key is the IP address and the.
Load the users file into a tab where the key is the IP addre and the value is the user nam.
The materialize method causes the pipeline to execute and also streams the collection contents.
You’ll copy the users file into HDFS and run your job:
Because you used the materialize method to get access to the result in the client side, you’re able to write the results to the console.
Summary There were two areas we covered in this technique: first, how to perform a join, and second, how Crunch can support accessing collection elements in the client process.
At the time of writing, the built-in support for joins in Crunch are limited to a twotable repartition join.
But Crunch is very powerful and writing joins such as semi-joins and replicated joins is feasible.
Cascading abstracts MapReduce into a logical model comprised of tuples, pipes, and taps.
Start execu data in the pipeline; the pipes represent operations being performed on tuples; and taps are data sources and data sinks.
TextLine represents a tuple stream, where each tuple represents a line of te The tuple consists of two fields: the byte offset in the file for the line t.
Define the data source that will be used to add tuples to the TextLine.
Define the field name that extract from the log lin.
Create a new pipeline where the first (operation) is Each, which is an operator th applied to each tuple in the input tuple stre.
Count is an aggregation operation that’s applied on grouped data.
A FlowConnector links together a data source, a data sink, and.
So how does Cascading compare to other pipeline-based technologies such as Crunch and Pig? The advantage of Pig is that it contains a shell that makes it easy for ad hoc usage.
But you could ostensibly use the Groovy or Scala shell to work with Crunch and Cascading too.
Cascading and Crunch can also be more easily incorporated into existing Java applications because they’re Java technologies.
From a feature perspective the differences between them aren’t significant other than the maturity of the project and the data model.
A comparison of the two projects can be seen in table 12.1
The goal with this chapter was to introduce Crunch, a recent and promising high-level MapReduce library.
Crunch’s emphasis is to lower the barrier to using MapReduce in Java, and abstract away MapReduce concepts in favor of working with more familiar Java built-in types.
We looked at how you could use Crunch to work with log data, and along the way you learned how to perform join operations and how to use the combiner.
We also briefly looked at Cascading and compared it to Crunch.
The next chapter looks at methods to help test and debug Java MapReduce applications.
Comprehensive set of functions to work with Cascading’s tuple model.
We’ll look at how to provide adequate unit testing for MapReduce code and examine some defensive coding techniques to minimize badly behaving code.
All the preparation and testing in the world doesn’t guarantee you won’t encounter any problems, and in the event that you do, we’ll look at how to debug your job to figure out what went wrong.
In this chapter we’ll focus on testing and debugging user space MapReduce.
In this section we’ll look at the best methods to test your MapReduce code, and also look at some design aspects to consider when writing MapReduce to help in your testing efforts.
When you’re running MapReduce in production you can guarantee that some day you’ll receive a call about a failing job.
The goal of this chapter is to help you put in.
It’s important to make sure unit tests are easy to write, and to ensure that they cover a good spectrum of positive and negative scenarios.
Let’s take a look at the impact that test-driven development, code design, and data have on writing effective unit tests.
When it comes to writing Java code, I’m a big proponent of test-driven development (TDD),1 and with MapReduce things are no different.
Test-driven development emphasizes writing unit tests ahead of writing the code, and recently has gained in importance as quick development turnaround times become the norm rather than the exception.
Applying test-driven development to MapReduce code is crucial, particularly when such code is part of a critical production application.
Writing unit tests prior to writing your code forces your code to be structured in a way that easily facilitates testing.
When you write code, it’s important to think about the best way to structure it so you can easily test it.
When you write MapReduce code, it’s a good idea to abstract away the code doing.
This is true not only for your map and reduce functions, but also for your InputFormats, OutputFormats, data serialization, and partitioner code.
Let’s look at a simple example to better illustrate this point.
The following code shows a reducer that calculates the mean for a stock:
If you were to structure the code to abstract away the work, you could easily test the user space code that’s doing your work, as the following code shows:
With this improved code layout you can now easily test the SMA class that’s adding and calculating the simple moving average, without the Hadoop code getting in your way.
When you write unit tests, you try to discover how your code handles both positive and negative input data.
In both cases it’s best if the data you’re testing with is a representative sample from production.
Often, no matter how hard you try, issues in your code in production will arise from unexpected input data.
Later, in section 13.2.2, we’ll look at how to identify when this occurs in production jobs.
It’s important that when you do discover input data that causes a job to blow up, you not only fix the code to handle the unexpected data, but you also pull the data that caused the blowup and use it in a unit test to prove that the code can now correctly handle that data.
MRUnit is a test framework you can use to unit test MapReduce code.
It was developed by Cloudera (a vendor with its own Hadoop distribution) and is currently an Apache project in incubator status.
In this technique we’ll look at writing unit tests that leverage each of the four types of tests provided by MRUnit:
A map test that only tests a map function (supported by the MapDriver class)
A reduce test that only tests a reduce function (supported by the ReduceDriver class)
Problem You want to test map and reduce functions, as well as MapReduce pipelines.
Discussion MRUnit has four types of unit tests—we’ll start with a look at the map tests.
Let’s kick things off by writing a test to exercise a map function.
Before starting, let’s look at what you need to supply to MRUnit to execute the test, and in the process learn about how MRUnit works behind the scenes.
Figure 13.1 shows the interactions of the unit test with MRUnit, and how, in turn, it interacts with the mapper you’re testing.
MRUnit is not tied to any specific unit testing framework, so if it finds an error it logs the error and throws an exception.
Let’s see what would happen if your unit test had specified output that didn’t match the output of the mapper, as in the following code:
If you run this test, your test will fail, and you’ll see the following log output:
One of the powerful features of JUnit and other test frameworks is that when tests fail, the failure message includes details on the cause of the failure.
This is the MapDriver, and as such you need to specify the key/value input and output types for the mapper you’re.
The withOutput method is used to specify the output key/value, which MRUnit will compare against the output generated by the mapper being tested.
If a failure’s encountered, it logs the discrepancy, and throws an exception.
If you call the withInput method more than once, it will overwrite the key and value from the previous call to withInput.
What if you wanted to leverage the power of MRUnit, and also leverage the informative errors that JUnit provides when assertions fail? You could modify your code4 to do that, and bypass MRUnit’s testing code:
With this approach, if there’s a mismatch between the expected and actual outputs, you get a more meaningful error message, which report-generation tools can use to easily describe what failed in the test:
To cut down on the inevitable copy-paste activities with this approach, I wrote a simple helper class5 to use JUnit asserts in combination with using the MRUnit driver.
The run method executes the map function and returns a list of all.
You’re calling withOutput because the helper function can extract the.
Call the helper function that us JUnit asserts to test the contents.
Now that we’ve looked at map function tests, let’s look at reduce function tests.
The MRUnit framework takes a similar approach for reduce testing.
Figure 13.2 shows the interactions of your unit test with MRUnit, and how it in turn interacts with the reducer you’re testing.
The following code6 is a simple unit test for testing the (identity) reducer class in Hadoop:
When testing the reducer specify a list of values t.
Now that we’ve completed our look at the individual map and reduce function tests, let’s look at how to test a map and reduce function together.
MRUnit also supports testing the map and reduce functions in the same test.
You feed MRUnit the inputs, which in turn are supplied to the mapper.
Figure 13.3 shows the interactions of your unit test with MRUnit, and how, in turn, it interacts with the mapper and reducer you’re testing.
Use the helper class you wrote earlier in the map section.
The following code7 is a simple unit test for testing the (identity) mapper and reducer classes in Hadoop:
Now we’ll look at our fourth and final type of test that MRUnit supports, pipeline tests, which are used to test multiple MapReduce jobs.
MRUnit supports testing a series of map and reduce functions—these are called pipeline tests.
You feed MRUnit one or more MapReduce functions, the inputs to the first map function, and the expected outputs of the last reduce function.
Figure 13.4 shows the interactions of your unit test with MRUnit pipeline driver.
The following code8 is a unit test for testing a pipeline containing two sets of (identity) mapper and reducer classes in Hadoop:
With the MapReduce driver you need t specify six types, the map input an.
Multiple MapReduce drivers are called in a sequence, creating a pipeline of jobs.
Compare the expected output to the actual output of the final MapReduce job in the pipeline.
Summary What type of test should you use for your code? Take a look at table 13.1 for some pointers.
MRUnit has a few limitations, some of which we touched upon in this technique:
The MapDriver and ReduceDriver support only a single key as input, which can make it more cumbersome to test map and reduce logic that requires multiple keys, such as those that cache the input data.
MRUnit isn’t integrated with unit test frameworks that provide rich error-reporting capabilities for quicker determination of errors.
The pipeline tests only work with the old MapReduce API, so MapReduce code that uses the new MapReduce API can’t be tested with the pipeline tests.
There’s no support for testing data serialization, or InputFormat, RecordReader, OutputFormat, or RecordWriter classes.
Notwithstanding these limitations, MRUnit is an excellent test framework to help you test at the granular level of individual map and reduce functions; MRUnit also can test a pipeline of MapReduce jobs.
And because it skips the InputFormat and OutputFormat steps, your unit tests will execute quickly.
Next we’ll look at how you can use the LocalJobRunner to test some MapReduce constructs that are ignored by MRUnit.
In the last section we looked at MRUnit, a great lightweight unit test library.
But what if you want to test not only your map and reduce functions, but also the InputFormat, RecordReader, OutputFormat, and RecordWriter code as well as the data serialization between the map and reduce phases? This becomes important if you’ve written your own input/output format classes because you want to make sure you’re testing that code, too.
Map You have a map-only job, and you want low-level unit tests where the framework takes care of testing the expected map outputs for your test map inputs.
Reduce Your job has a lot of complexity in the reduce function and you want to isolate your tests to only that function.
MapReduce You want to test the combination of the map and reduce functions.
Pipeline You have a MapReduce pipeline where the input of each MapReduce job is the output from the previous job.
You f Loca by se mapr to lo the d.
Wr inp fil Hadoop comes bundled with the LocalJobRunner class, which Hadoop and related projects (such as Pig and Avro) use to write and test their MapReduce code.
LocalJobRunner allows you to test all the aspects of a MapReduce job, including the reading and writing of data to and from the filesystem.
Tools like MRUnit are useful for low-level unit tests, but how can you be sure that your code will play nicely with the whole Hadoop stack? Problem You want to test the whole Hadoop stack in your unit test.
Solution Leverage the LocalJobRunner class in Hadoop to expand the coverage of your tests to include code related to processing job inputs and outputs.
Discussion Using the LocalJobRunner makes your unit tests start to feel more like integration tests, because what you’re doing is testing how your code works in combination with the whole MapReduce stack.
This is great because you can use this to test not only how your user space MapReduce code plays with MapReduce, but also to test InputFormats, OutputFormats, partitioners, and advanced sort mechanisms.
The code in the next listing9 shows an example of how you can leverage the LocalJobRunner in your unit tests.
Writing this test is more involved because you need to handle writing the inputs to the filesystem, and also reading them back out.
That’s a lot of boilerplate code to have to deal with for every test, and probably something that you want to factor out into a reusable helper class.
Here’s an example of a utility class to do that; the following code10 shows how IdentityTest code can be condensed into a more manageable size:
Delegate testing the expected results with the results to the utility class.
Summary So what are some of the limitations to be aware of when using LocalJobRunner?
Despite these limitations, LocalJobRunner is the most comprehensive way to test your MapReduce code, and as such will provide the highest level of assurance that your jobs will run the way you expect them to in Hadoop clusters.
Using the TDD approach, you wrote some unit tests using the techniques in this section.
You next wrote the MapReduce code and got it to the point where the unit tests were passing.
Before you break out the champagne, you still want assurances that the MapReduce code is working prior to running it in production.
The last thing you want is your code to fail in production and have to debug it over there.
But why, you ask, would my job fail if all of my unit tests pass? Good question, and it could be due to a variety of factors:
The data you used for your unit tests doesn’t contain all of the data aberrations and variances of the data used in production.
Because of these factors, when you build integration or QA test environments it’s crucial to ensure that the Hadoop version and configurations mirror those of the production cluster.
Different versions of Hadoop will behave differently, as will the same version of Hadoop configured in different ways.
When you’re testing changes in test environments, you want to ensure a smooth transition to production, so do as much as you can to make sure that version and configuration are as close as possible to production.
After your MapReduce jobs are successfully running in integration and QA, you can push them into production, knowing there’s a much higher probability that your jobs will work as expected.
We looked at some TDD and design principles to help write and test your Java code, and also covered some unit test libraries that make it easier to unit test MapReduce code.
Next we’ll move into the complex world of debugging problems in MapReduce jobs.
In this section we’ll walk through the steps to isolate and fix problems in your MapReduce user space code.
What’s meant by user space code is code that developers write.
Your MapReduce jobs can fail due to a number of problems, including the following:
You’ll need to take a structured approach to debugging a problem MapReduce job.
Figure 13.5 shows a decision tree you can use to narrow down a problem in your MapReduce code.
In the remainder of this section, we’ll address the three areas highlighted in figure 13.5 to help with your debugging efforts:
We’ll kick things off with a look at the task logs.
Accessing your task logs is the first step to figuring out what issues you’re having with your MapReduce job.
Depending on the exact issue, the logs in their current form may or may not help you.
For example, if there’s a subtle serialization bug in your code, unless the steps in section 13.2.4 were followed, there’s a good chance the logs won’t be much help in pinpointing serialization as the problem.
In this technique we’ll look at ways to access task logs in the event that you have a problem MapReduce job you want to debug.
Problem Your MapReduce job is failing, or generating unexpected outputs, and you want to determine if the logs can help you figure out the problem.
Solution Learn how to use the JobTracker UI to view task logs.
You will also look at how you can SSH to individual TaskTracker nodes and examine the logs directly.
Discussion So, a job has failed and you want to find out information about the cause of the failure.
When a job fails, it’s useful to look at the logs to see if they tell you anything about the failure.
Each map and reduce task has its own logs, so you need to identify the.
Reproduce problem in dev environment, ﬁx problem, and write unit test using problem.
Check the job inputs are correct, and also ensure that.
Are all items in section “Coding guidelines for eﬀective debugging”
Ensure adequate logging and exception handling, turn on verbose logging.
Do the contents of the output ﬁles line up with the reduce.
The easiest way to do this is to use the JobTracker UI.
Select the job that failed from the main JobTracker page, and you’ll be presented with some statistics about tasks, as shown in figure 13.6
If you click on the number of a failed task you’ll see a page containing all of the failed tasks and a stack trace for each task, an example of which is shown in figure 13.7
A number of scenarios under which tasks will fail include following:
Depending on the problem, you may find additional useful information in the logs, or in the standard out (stdout) or standard error (stderr) of the task process.
You can view all three outputs easily by selecting the All link under the Logs column, as shown in figure 13.8
This is all fine and dandy, but what if you don’t have access to the UI? How do you figure out the failed tasks and get at their output files? The job history command-line.
This output is informative: not only do you see the exception, but you also see the task name and the host on which the task was executed.
Our task didn't produce any output on standard out, just some Log4J warnings on standard error.
The syslog shows us the exception that's causing the job to fail.
Figure 13.8 TaskTracker page showing output from standard output, standard error, and the logs.
The last argument is the output directory of the job, which is used to extract the job history details.
The all option gives you verbose output for all tasks.
A URL that can be use retrieve all the outputs rel to the task.
It’ll be easier to parse the output by saving the HTML to a file (by adding -o [filename] to the curl command), copying that file to your local host, and using a browser to view the file.
What if you’re working in an environment where you don’t have access to the JobTracker or TaskTracker UI? This may be the case if you’re working in clusters that have firewalls blocking access to the UI ports from your laptop or desktop.
What if you only have SSH access to the cluster? One option is to run Lynx, a text-based web browser, from inside your cluster.
If you don’t have Lynx you’ll have to know how to access the task logs directly.
You know the hostname from the URL, so you’ll need to first SSH to that host.
The logs for each task are contained in the Hadoop logs directory.
Using curl to download the outputs from the TaskTracker URL.
For the curl command to work you’ll need to run it from a host that has access to the TaskTracker node.
Under this directory you’ll find at least the following three files:
You can use your favorite editor or simple tools like cat or less to view the contents of these files.
Summary Often, when things start going wrong in your jobs the task logs will contain details on the cause of the failure.
This technique looked at how you could use the JobTracker and, alternatively, the Linux shell to access your logs.
If the data in the logs suggests that the problem with your job is with the inputs (which can be manifested by a parsing exception), you need to figure out what kind of input is causing the problem.
In the previous section, you saw how to access failed task output files to help you figure out the root cause of the failure.
In the example, the outputs didn’t contain any additional information, which means that you’re dealing with some MapReduce code that wasn’t written to handle error conditions.
If it’s possible to easily modify the MapReduce code that’s failing, go ahead and skip to section 13.2.4 and look at the strategies to update your code to better handle and report on broken inputs.
Roll these changes into your code, push your code to the cluster, and rerun the job.
Your job outputs now will contain enough details for you to be able to update your code to better handle the unexpected inputs.
If this isn’t an option, read on; we’ll look at what to do to isolate the input data that’s causing your code to misbehave.
Imagine you have a job that’s reading Twitter tweets from a number of input files.
Some of the tweets aren’t formed correctly (could be a syntax problem or an unexpected value that you’re unaware of in your data dictionary), which leads to failure in your processing logic.
By examining the logs, you’re able to determine that there’s some data in your input files which is causing your parsing code to fail.
But your job has numerous input files and they’re all large, so your challenge is to narrow down where the problem inputs exist.
Problem You want to identify the specific input split that’s causing parsing issues.
Discussion Take the following three steps to fix the situation:
In this technique we’ll focus on the first item, because it will help you to fix your code.
We’ll cover future-proofing your code for debugging in section 13.2.4
The first step you need to do is determine what file contains the bad input record, and even better, find a range within that file, if the file’s large.
Unfortunately, Hadoop by default wipes out task-level details, including the input splits after the tasks have completed.
You’ll also have to rerun the job that failed, but this time you’ll be able to extract additional metadata about the failing task.
After rerunning the failed job you’ll once again need to use the hadoop job -history command discussed in the previous section to identify the host and job or task IDs.
With this information in hand, you’ll need to use the shell to log into the TaskTracker node, which ran the failed task, and then navigate to the task directory, which contains information about the input splits for the task.
This can be easily accomplished with a find, as follows:
When you’ve located this directory, you’ll see a number of files related to the task, including a file called split.info.
This file contains information about the location of the input split file in HDFS, as well as an offset that’s used to determine which of the input splits this task is working on.
Both the task and job split files are a mixture of text and binary content, so unfortunately, you can’t crack out your command-line editor to easily view their contents.
This is a subsection of the output, showing a failed task.
This directory contains a  binary ﬁle with input splits details.
To help with this situation, I’ve written a utility that can read the input split file for a task and use that information to open the job split file in HDFS, jump into the taskspecific offset, and read the task-specific split information.
Be warned that there’s a good chance this won’t work with versions other than Hadoop 0.20.x.
If you run this on the input.split file for your failed task you’ll gain some insight into the file and data about the start and end of the split (assuming the input for the task is a file):
At this point you have a couple of options: you can copy the file into your local box and write a unit test that runs the MapReduce job with the file.
You also can modify your code to catch an exception, which will allow you to set a breakpoint in your IDE and observe the input that’s causing your exception.
Alternatively, depending on the Hadoop distribution you’re running, Hadoop comes with a tool called IsolationRunner, which can re-execute a specific task with its input split.
Hadoop in Action by Chuck Lam contains an example of how to use the IsolationRunner, as does the Hadoop tutorial at http://goo.gl/FRv1H.
You can enable some options so that the task JVM runs with the Java debug agent enabled, and connect to the task via your IDE or jdb.
Summary We used this technique to identify the input splits for a task that’s failing due to a problem with some input data.
Next we’ll look at how you get at the JVM arguments you used to launch your task—useful when you suspect there’s an issue related to the JVM environment.
This technique steps somewhat outside of the realm of your user space MapReduce debugging, but it’s a useful technique in situations where you suspect there’s an issue with the startup JVM arguments for tasks.
For example, sometimes the classpath ordering of JARs is significant and issues with it can cause class loading problems.
The ability to examine the various arguments used to start a task can be helpful in debugging task failures.
For example, let’s say you’re trying to use a native Hadoop compression codec, but your MapReduce tasks are failing and the errors complain that the native compression libraries can’t be loaded.
In this case review the JVM startup arguments to determine if all of the required settings exist for native compression to work.
Problem You suspect that a task is failing due to missing arguments when a task is being launched, and want to examine the JVM startup arguments.
Discussion As the TaskTracker prepares to launch a map or reduce task, it also creates a shell script that’s subsequently executed to run the task.
The problem is that MapReduce by default removes these scripts after a job has completed.
Figure 13.11 shows all of the steps required to gain access to the task shell script.
If this is the case, you can remedy the problem by adding the native path, which you do by exporting JAVA_LIBRARY_PATH in hadoop-env.sh.
Summary This technique is useful in situations where you want to be able to examine the arguments used to launch the task JVM.
Next we’ll look at some coding practices that can help with debugging activities.
Debugging MapReduce code in production can be made a whole lot easier if you follow a handful of logging and exception-handling best practices.
Debugging a poorly written MapReduce job consumes a lot of time, and can be challenging in production environments where access to cluster resources is limited.
Problem You want to know the best practices to follow when writing MapReduce code.
Solution Look at how counters and logs can be used to enhance your ability to effectively debug and handle problem jobs.
Job ID Directory containing a shell script to launch a specific task attempt.
Determine the failed task attempt ID and the host it was running on.
Make a note of the job ID, the failed task attempt ID, and the host that it was.
This is a subsection of the output, showing a failed task.
You t or slow both ts so ailing his is ntial a, or asses.
You out Thi to r help the or p bet red.
Catch exceptions and provide meaningful logging output to help track down problem data inputs and logic errors.
In the following code,13 you’ll see applied a number of the previously described principles.
A mapper job with some best practices applied to assist debugging.
When the task starts, write the input spli details to the log.
Note that you enclose the key and value with square bracke you can easily identify leading and tr.
Also note that t important because it helps isolate pote.
Note that yo enclosed both strings wit square brackets to easi.
The reduce task should have similar debug log statements added to write out each reduce input key and value, and the output key and value.
You used counters to count the number of bad records you encountered.
The JobTracker UI can be used to view the counter values, as shown in figure 13.12
Depending on how you executed the job, you’ll see the counters dumped on standard out.
You have programmatic access to counters, and the job history command will also include the counters:
If you look at the logs for your tasks, you’ll also see some informative data related to the task:
Because you also updated the task status in your code, you can use the JobTracker UI to easily identify the tasks that had failed records, as shown in figure 13.13
Set that task status to indicate you hit an issue with.
Figure 13.12 Screenshot of the counter in JobTracker’s job summary page.
This tells you what file t task was working on, as w.
Note that because you used square brackets to encapsulate your strings, any leading or.
Summary We looked at a handful of simple yet useful coding guidelines for your MapReduce code.
If they’re applied and you hit a problem with your job in production, you’ll be in a great position to quickly narrow down possibilities on the root cause of the issue.
If the issue’s related to the input, your logs will contain details about how the input caused your processing logic to fail.
To complete this chapter we’ll examine some common missteps in MapReduce that often lead to hours of debugging.
The intent here is to learn by examining practices that should be avoided in MapReduce.
In the previous code example you caught any exception in your code and then made sure to write the exception to the logs along with as much contextual information as possible (such as the current key and value that the reducer was working on)
The big question is, should you rethrow the exception, or swallow it?
Rethrowing the exception is tempting because you’ll be immediately aware of any issues in your MapReduce code.
But if your code is running in production and fails every time it encounters a problem such as some input data that’s not handled correctly, ops, dev, and QA will be spending quite a few cycles addressing each issue as it comes along.
Writing code as you did to swallow exceptions has its own problems—for example, what if you encounter an exception on all inputs to the job? If you write code to swallow exceptions, the correct approach is to increment a counter (as in the code example), which the driver class should use after job completion to ensure that most of the input records within some tolerable threshold were successfully processed.
If they weren’t, the workflow being processed should probably be terminated and the appropriate alerts be sent to notify operations.
This is covered in more detail in Hadoop in Action by Chuck Lam.
Throughout this book I’ve covered a number of patterns to help you write and execute MapReduce jobs.
It can be just as useful to learn from anti-patterns, which are patterns that are commonly used but are either ineffective or worse, detrimental in practice.
Problem You want to learn some MapReduce anti-patterns so you’ll be aware of what practices you should avoid.
Solution Learn and laugh at mistakes that I’ve made in MapReduce on production clusters, which range from loading too much data into memory in tasks to going crazy with counters and bringing down the JobTracker.
Caching data in map and reduce tasks is required for many kinds of operations, such as data joins.
But in Java the memory overhead of caching is significant (see chapter 6 for specific details), and if your cache becomes too large to fit in Java’s heap, your task will fail with an OutOfMemoryError exception.
This is an approach worth considering, albeit it assumes the records are not overly large (bear in mind that even if you cap the number of records to a small size, it only takes a handful of large records to blow out your memory)
If you’re implementing some strategies (such as capping how many records are being cached), make sure you use counters to identify that you’re performing that capping, and ideally, by how much (count how many records aren’t being cached), so you can better understand data that’s being skipped.
If you’re working with variable-length records, it may be useful to log records over a certain size—again, to better understand your data and to help you make future caching decisions.
Stop to think about the input data to your MapReduce jobs.
If each input record isn’t a fixed size, there’s a chance you could encounter records that are possibly too large to fit into memory.
Take, for example, a simple case of a job that reads lines from a text file.
In either case there’s no cap on the maximum length of a line, so if you have a line that’s millions of characters in length, there’s a chance it won’t fit into memory (or if it does, any operation you attempt to perform on that string will exhaust your memory)
It will also log cases where it encountered lines that are over this length, including the byte offset in the input file.
If you’re working with other InputFormats, you should check to see if they have any mechanisms to limit the size of input records.
Similarly, if you’re writing an InputFormat, think about adding support for limiting the size of records you feed to a map task.
There’s nothing that can stop you from writing MapReduce jobs to pull data from databases, or web servers, or any other data source external to HDFS.
Keep in mind, though, the use of these external data sources, both by other users as well as by the MapReduce job.
It’s possible that the data source you’re working with doesn’t scale to support hundreds or thousands of concurrent reads or writes, and your single MapReduce job may bring it to its knees.
I recommend you limit the number of map and/ or reduce tasks to a small number to minimize the likelihood of this occurring.
Speculative execution is a mechanism used in MapReduce to guard against slow nodes in a cluster.
As the map and reduce phases of a job near completion, MapReduce will launch duplicate tasks that work off of the same inputs as the remaining tasks.
But what if your job is writing to a database or some other external resource, or directly to a file in HDFS? Now you have multiple tasks both writing the same data, which is probably not what you want.
If you’re using an OutputFormat that’s based on the FileOutputFormat, and you want to write additional output to HDFS, the best approach is to write into the task’s attempt directory.
Each task’s reduce (or map if a no-reduce job is being run) is written to a temporary attempt directory, and only if the task succeeds are the files moved into the job output directory.
The following code16 shows a map-only job that is writing output to a side-effect file:
Sqoop is a tool to import and export database data to and from HDFS.
Ask t FileO for t worki for t OutputStream sideEffectStream;
If you run this job you should observe two output files, one written to be the RecordWriter, and the other written by you directly from your map task:
Working with bad input is often the norm in MapReduce, but if you have code that doesn’t expect the unexpected, it may start failing when it sees data it doesn’t expect.
Ideally, the code should be able to handle these situations, but there is a workaround, without having to touch the code, via the SkipBadRecords class.17 Hadoop in Action by Chuck Lam has more details on how to use this class, but at a basic level this feature allows you to specify the tolerance for the number of records surrounding a bad record that can be discarded.
Extract the attempt ID to be used as your filename.
Create a file in the attempt’s working directory in HDFS.
Remember to close the file after your task has completed.
It’s not uncommon for code that works in unit tests to fail in a cluster.
But if you’re running multiple clusters, make an effort to ensure that the Hadoop versions, and Hadoop configurations, align as closely as possible.
Hadoop’s many configuration settings can cause jobs to behave differently, and keeping discrepancies down to a minimum will ensure that a job succeeding on one cluster will work on another cluster.
When you’re developing and testing MapReduce, Pig, or Hive scripts, it’s tempting to work directly with the full set of input data.
But doing so flies in the face of rapid development—rather than quickly iterating the development and test cycles, you’ll be sitting around waiting for the results of your job, and at the same time needlessly chewing up cluster resources.
We already covered this topic in section 13.2.4, but I want to reemphasize that a high percentage of problems you’ll encounter in your job are due to unexpected input, and can be as simple an issue as leading or trailing whitespace characters that cause parsing issues.
Including measures to be able to quickly debug these issues is crucial.
Counters are a great mechanism to communicate numerical data to some driver code that’s running your MapReduce job.
Be warned that each counter incurs some amount of memory overhead in the JobTracker.
For each individual counter the memory footprint may be small, but if you use counters carelessly this could lead to memory exhaustion in the JobTracker.
An example of this situation would be where you dynamically created a counter for each input record in the map task—it would only take a few million records for there to be a noticeable memory impact and overall slowdown in the JobTracker.
Summary We covered a few of the bumps you’ll face when you work with MapReduce.
You’ll never be able to foresee all of the potential problems you could encounter, but understanding some of the more common issues we’ve highlighted in this technique, coupled with a well-thought-out implementation of your MapReduce functions, can go a long way to avoiding those 2 a.m.
This chapter only scratched the surface when it comes to testing and debugging.
We laid the groundwork for how to test and debug user space MapReduce, but there’s much more to testing and debugging outside of the scope of user space MapReduce.
We also covered how the MapReduce-related parts of your code could be tested in both lightweight (without MapReduce stack involvement via MRUnit) and more heavyweight (with LocalTestRunner) setups.
We also emphasized how to debug issues that result in failing MapReduce jobs, as well as jobs that aren’t generating the results you’d expect.
We wrapped things up with some examples of badly written MapReduce jobs with the hope that we all can learn from the mistakes of others (including the author)
This appendix contains background information on all the related Hadoop technologies in the book.
Where applicable, I also include instructions on how to build, install, and configure related projects.
Hadoop in Action by Chuck Lam, geared to helping those that are new to Hadoop.
Michael Noll’s guide to install Hadoop 0.20.x on multiple hosts (a full cluster setup)
The following instructions are for users wanting to install the tarball versions of CDH or the vanilla Apache Hadoop distribution.
These instructions are for installing Hadoop on your desktop or laptop in pseudo-distributed mode, and not for a fullblown cluster.
First you’ll need to download the tarballs from their respective locations.
If you don’t have root permissions on your host, you can install Hadoop under a different directory, and substitute instances of /usr/local in the following instructions with your directory name.
If you don’t already have an SSH key pair, create one with the following command:
You’ll also need an SSH agent running so that you aren’t prompted to enter your password a bazillion times when starting and stopping Hadoop.
Different operating systems have different ways to run an SSH agent, and there are links for CentOS and other RedHat derivatives (http://goo.gl/Nffty) and OSX (http://goo.gl/dbdNb)
Google is your friend if you’re running on a different system.
To verify the agent is running and has your keys loaded, try opening an SSH connection to the local system:
If you’re prompted for a password, the agent’s not running or doesn’t have your keys loaded.
You’ll need to ensure that the system path includes the binary directory of your Java installation.
The following code shows what you can add to the bottom of your bash shell profile file in ~/.bash_profile (assuming you’re running bash):
The rest of the commands in this section assume that the Hadoop binary directory exists in your PATH, as per the instructions in the previous section:
After HDFS has been formatted, you can start Hadoop with the following script:
After running the start script, you should use the jps Java utility to check that all the processes are running.
You should see the same output as follows (with the exception of the process IDs, which will be different):
The most common error is that the HDFS formatting step, which I showed earlier, was skipped.
The first two commands create a directory in HDFS and upload the contents of the Hadoop configuration into HDFS.
The third command runs a MapReduce job, and the final command dumps the contents of the job output:
Here’s a list of them, including the ports that they run on, and URLs that assume they’re running on the local host (as is possibly the case if you have a pseudo-distributed installation running)
By default this is under $HADOOP_HOME/logs on each Hadoop node.
A.2 Flume Flume is a log collection and distribution system that can transport log files across a large number of hosts into HDFS.
It’s an Apache project in incubator status, originally developed and currently maintained and supported by Cloudera.
Chapter 2 contains a section on Flume and how it can be used.
The resource link titled “CDH3 Installation Guide” shows how to install Flume on CDH.
For the purpose of the book, the Flume Master and Flume Node packages should be installed.
Because Flume is currently in incubator status, there aren’t any installation instructions for non-CDH Hadoop distributions.
It’s a Hadoop workflow engine that manages data processing activities.
An excellent Flume user guide with a high level of detail.
Cloudera article on how to use Flume to collect Apache web server logs.
The previous resource link titled “CDH3 Installation Guide” shows how to install Oozie on CDH.
Oozie is currently in incubator status, so there aren’t any download packages or instructions from Apache.
A.4 Sqoop Sqoop is a tool for importing data from relational databases into Hadoop, and vice versa.
It can support any JDBC-compliant database, and also has native connectors for efficient data transport to and from mySQL and PostgreSQL.
Chapter 2 contains details on how imports and exports can be performed with Sqoop.
The resource link above titled “Sqoop Installation” shows how to install Sqoop on CDH.
If you’re planning on using Sqoop with MySQL, you’ll need to download the MySQL JDBC driver tarball from http://dev.mysql.com/downloads/connector/j/, explode it into a directory, and then copy the JAR file into the Sqoop lib directory:
You’ll need to set the following environment variables for Sqoop.
If you’re planning on using Sqoop with MySQL, you’ll need to download the MySQL JDBC driver tarball from http://dev.mysql.com/downloads/connector/j/, explode it into a directory, and then copy the JAR file into the Sqoop lib directory:
To run Sqoop there are a few environment variables that you may need to set.
If you have the Sun JDK installed on RedHat this would be /usr/java/latest.
HIVE_HOME Only required if you’re planning on using Hive with Sqoop.
HBASE_HOME Only required if you’re planning on using HBase with Sqoop.
Chapter 2 contains a number of techniques that show how the binaries are used for imports and exports.
A.5 HBase HBase is a real-time key/value distributed column-based database modeled after Google’s BigTable.
The resource link titled “CDH3 Installation Guide” shows how to install HBase on CDH.
For the purpose of the book, it’s sufficient to install HBase in standalone mode (http://goo.gl/1Y6Bi)
The previous resource links titled “Apache HBase getting started” and “Apache HBase Reference Guide” have comprehensive installation and configuration instructions for HBase.
A.6 Avro Avro is a data serialization system that provides features such as compression, schema evolution, and code generation.
It can be viewed as a more sophisticated version of a SequenceFile, with additional features such as schema evolution.
Chapter 3 contains details on how Avro can be used in MapReduce as well as with basic input/output streams.
Avro is a full-fledged Apache project, so you can download the binaries from the downloads link from the previous Apache project page link.
In this book we’ll use it in conjunction with Elephant Bird and Rhipe.
You’ll need a C++ compiler, which can be installed on 64-bit RHEL systems with the following command:
Avro 1.5.4 Javadocs with instructions on how Avro can be used in MapReduce.
Downloads page, containing a link for version 2.3.0 (required for use with Elephant Bird)
Thrift documentation is lacking, something which the project page attests to.
Downloads page, containing a link for version 0.5 (required for use with Elephant Bird)
Build and install the native and Java/Python libraries and binaries:
This step requires Ant to be installed, instructions for which are available at http://ant.apache.org/manual/index.html.
A.9 Snappy Snappy is a native compression codec developed by Google, which offers fast compression and decompression times.
In the book code examples where we don’t need splittable compression, we’ll use Snappy because of its time efficiency.
In this section we’ll cover how to build and set up your cluster to work with Snappy.
The following steps will walk you through the process to build, install, and configure Snappy compression.
Before you do this, I should mention a few key considerations as you’re performing these steps:
It’s highly recommended that you build the libraries on the same hardware that you have deployed in production.
All of the installation and configuration steps will need to be performed on any client hosts that will be using Snappy, as well as all the DataNodes in your cluster.
For other Hadoop distributions, please consult your vendor about how to install native Hadoop libraries.
You can also build the native libraries yourself by following the instructions at http://hadoop.apache.org/common/docs/r1.0.0/native_libraries.html.
If you don’t already have the GCC compiler installed, you can install it with the following command (on RHEL-based Linux systems):
You’ll also need to have Maven installed and the automake and libtool packages installed to build it:
After it’s built, copy the libraries into your Hadoop lib directory:
Make sure you remove the newlines and spaces such that there are no whitespace characters between the commas.
The following codecs also assume that you have the LZO/P codecs available.
A.10 LZOP LZOP is a compression codec that can be used to support splittable compression in MapReduce.
Chapter 5 has a section dedicated to working with LZOP.
In this section we’ll cover how to build and set up your cluster to work with LZOP.
The following steps walk you through the process to build, install, and configure LZOP compression.
Before you do this, I should mention a few key considerations as you’re performing these steps:
It’s highly recommended that you build the libraries on the same hardware that you have deployed in production.
All of the installation and configuration steps will need to be performed on any client hosts that will be using LZOP, as well as all the DataNodes in your cluster.
Blog post from Twitter about why they use LZO, including some statistics and setup instructions.
Todd Lipcon’s LZO GitHub repository, which is what we’ll use to build the native and Java libraries.
Todd Lipcon is a Cloudera employee (and Hadoop committer) who maintains this code for compatibility with CDH.
For other Hadoop distributions please consult your vendor about how to install native Hadoop libraries.
You can also build the native libraries yourself by following the instructions at http://hadoop.apache.org/common/docs/r1.0.0/native_libraries.html.
For the latest and greatest, go with the Twitter, unless you’re running CDH and you want a version that’s been baked and tested to work with CDH.
This code contains the sources for both a native and Java LZOP library.
Prebuilt versions of the libraries aren’t bundled, so you’ll need to build them yourself.
After you’ve downloaded and exploded the tarball, change into the directory that was just exploded and run the build to generate both the native and Java libraries:
After you’re done, copy the resulting libraries into the Hadoop library directives:
Make sure you remove the newlines and spaces such that there are no whitespace characters between the commas.
A.11 Elephant Bird Elephant Bird is a project that provides utilities for working with LZOP-compressed data.
It also provides a container format that supports working with Protocol Buffers and Thrift in MapReduce.
Please refer to their respective sections in this appendix for instructions.
After this is done, copy the Elephant Bird JAR from your Maven repository located at http://goo.gl/C6nPp and copy it into your Hadoop lib directory.
A.12 Hoop Hoop is an HTTP/S server which provides access to all the HDFS operations.
The “Getting started” resource link has comprehensive instructions on how to download, configure, and run the Hoop server.
The code in the book is compiled against version 2.0.5 of Elephant Bird.
The installation instructions above ensure that the 2.0.5 Elephant Bird JAR is placed into the Hadoop lib directory.
If a newer version of the Elephant Bird JAR is copied into the Hadoop lib directory there’s a chance that some of the examples in this book will fail.
A.13 MySQL MySQL (or another JDBC-compliant database) is needed by some projects such as Sqoop and Oozie.
MySQL packages are usually available for most Linux systems and can be downloaded using the normal package installation scripts.
For non-Linux systems‚ consult the MySQL project page at www.mysql.com for download and installation instructions.
A.14 Hive Apache Hive is a data warehouse project that provides a simplified and SQL-like abstraction on top of Hadoop.
The resource link titled “CDH3 Installation Guide” shows how to install Hive on CDH.
The resource link titled “Apache release and downloads” contains a download link to get to the Hive tarball.
The tarball can be extracted on any node that has access to a Hadoop instance.
The following shows instructions on installation into the /usr/local directory:
Out of the box, Hive is configured to use an embedded database (Derby) to store metadata.
This means that each individual user account would have their own Derby database and couldn’t share Hive database metadata.
To remedy this, it’s recommended to install a database such as MySQL to store the metadata.
This appendix includes a separate section with MySQL installation instructions.
After there’s a MySQL server accessible from the Hive installation, you need to configure Hive to use that database.
This involves updating the hive-site.xml file to add lines similar to the following example.
These instructions were modeled after the Hive wiki entry at http://goo.gl/IWioT.
Please replace these values with the actual values in your setup.
The username and password must have already been created in MySQL, using commands similar to those below:
You’ll also need to copy the MySQL JDBC driver JAR into the Hive lib directory.
The MySQL section in this appendix includes a link for obtaining the JAR.
In a multiuser environment, you need to make sure that the directories used by Hive have been opened up:
When you copy the following XML, you’ll need to remove any whitespace between the start and end tags and the actual value.
The following lines assume Hive is installed under /usr/local/hive, and that Hadoop is installed under /usr/local/hadoop:
A.15 Pig Apache Pig is a MapReduce pipeline project that provides a simplified abstraction on top of Hadoop.
The resource link titled “CDH3 Installation Guide” shows how to install Pig on CDH.
The resource link titled “Apache release and downloads” contains a download link to get to the Hive tarball.
Download the tarball and then extract to a local directory.
The following shows instructions on installation into the /usr/local directory:
There are many useful user-defined functions in PiggyBank, which is a Pig contrib.
With CDH, the PiggyBank JAR already exists, but if you’re working with Apache Hadoop, you’ll need to build it using the following instructions.
The instructions assume that Pig is installed under /usr/local/pig, and that the Oracle JDK is installed under /usr/java/latest:
The following lines assume Pig is installed under /usr/local/pig, and that Hadoop is installed under /usr/local/hadoop:
A.16 Crunch Crunch is a pure Java library that lets you write code that’s executed in MapReduce without having to actually use MapReduce specific constructs.
The following instructions can be used to download and build Crunch:
A.17 R R is an open source tool for statistical programming and graphics.
Chapter 8 contains details on how R can be used in conjunction with Hadoop.
Installing R from Yum makes things easy: it will figure out RPM dependencies and install them for you.
Go to http://www.r-project.org/, click on CRAN, select a download region that’s close to you, select RedHat, and pick the version and architecture appropriate for your system.
Replace the URL in baseurl in the following code and execute the command to add the R mirror repo to your Yum configuration:
A simple Yum command can be used to install R on 64-bit systems:
Go to http://www.r-project.org/, click on CRAN, select a download region that’s close to you, and select the binaries appropriate for your system.
A.18 RHIPE RHIPE is a library that improves the integration between R and Hadoop.
Chapter 8 contains details on how RHIPE can be used in conjunction with Hadoop.
Each node in your Hadoop cluster will require the following components:
Refer to section A.7 for details on how to build and install Protocol Buffers.
On CentOS, the Yum install may fail, complaining about a missing dependency.
Unfortunately, Rhipe isn’t integrated with CRAN, the R repository that’s used to quickly download and install R packages.
These instructions will need to be executed on all your Hadoop nodes and any client-side nodes using Rhipe.
Chapter 8 contains details on how RHadoop can be used in conjunction with Hadoop.
Each node in your Hadoop cluster will require the following components:
If you get an error installing rJava, you may need to set JAVA_HOME and reconfigure R prior to running the rJava installation:
These instructions will need to be executed on all your Hadoop nodes and any clientside nodes using rmr/rhdfs.
RHadoop comes with three packages, but we’ll focus on the rmr and rhdfs packages which provide MapReduce and HDFS integration with R.
The other package in RHadoop is rhbase for HBase integration.
A.20 Mahout Mahout is a predictive analytics project that offers in-JVM as well as MapReduce implementations for some of its algorithms.
Chapter 9 contains details on how Mahout can be used in conjunction with Hadoop.
Mahout should be installed on a node which has access to your Hadoop cluster.
Mahout is a client-side library and doesn’t need to be installed on your Hadoop cluster.
The following instructions will work on most Linux Operating Systems.
Download the Mahout distribution tarball and then execute the following instructions:
The following command shows how this can be performed on the command line (the same command can be copied into your bash profile file):
In this appendix, we’ll look at built-in mechanisms to read and write to HDFS, including the NameNode’s embedded HTTP server, and Hoop, a REST-based HDFS proxy.
This will help you understand what tools Hadoop provides out of the box.
Chapter 2 provides higher-level techniques and approaches for data ingress and egress.
B.1 Command line It’s easy to copy files to and from HDFS using the command-line interface (CLI)
The put and get options will perform these tasks for you.
The put option is more useful than the copyFromLocal option because it supports multiple file sources and it can also work with standard input.
For example, to read from standard input and write to a file in HDFS, you’d do the following:
There are also moveFromLocal and moveToLocal options that can be useful for ingress/ egress operations where you want to remove the source after the copy has completed.
It exposes basic filesystem operations such as create, open, and delete, among others.
Hadoop has a contrib project (contributed in JIRA ticket HADOOP-3754) that contains a Thrift server and bindings for various client languages including Python, Ruby, and Perl.
The Thrift client, Thrift server, and NameNode components can all exist on separate hosts.
For convenience sake, you would likely want to co-locate the Thrift server on the same node as the NameNode.
The Thrift interface is useful in situations where programming languages other than Java need access to HDFS.
Getting the Thrift HDFS interface to work is a twostep process: running the server, and then writing and running a client to perform filesystem operations.
Explode the tarball into a directory on your pseudo-distributed host.
There’s a script that’s supposed to launch the Thrift server, but it doesn’t work, so you’ll quickly write your own.
Now that your server’s running, you need to tackle the client side.
I wrote a simple Python script that uses the Thrift-generated HDFS client API to perform a simple read of a file in HDFS, which is available at GitHub at http://goo.gl/MYqFZ.
The main Python function that performs the copy from HDFS to local disk can be seen here:
When you run your Python client, you need to specify the Thrift server port that it dumped on standard out when it started.
You’ll write a file in HDFS and then use your Thrift Python client to copy it back to the local filesystem:
It worked! The disadvantage of using the Thrift server is that it adds another layer of indirection on top of HDFS, which means that your reads and writes won’t be as fast as they could be.
Also, because the Thrift server is the one performing all the interactions with HDFS, you’ve lost any client-side data locality that may result when a Thrift client is running on a DataNode.
B.4 Hadoop FUSE Hadoop comes with a component called FuseDFS, which allows HDFS to be mounted as a Linux volume via Filesystem in Userspace (FUSE)
The to b Because FUSE is a user space filesystem, there are quite a number of hops between the client application and HDFS.
The first step to getting things going is to install FuseDFS:
If you wanted to mount HDFS on /app/hdfs-fuse, you would do the following:
Now you can interact with HDFS with your normal filesystem utilities.
When you are finished using Hadoop FUSE, you can kill the hadoop-fuse-dfs application and then unmount the directory:
The convenience offered by using Hadoop FUSE is overshadowed by issues that make it not recommended for general use.
Hadoop FUSE is executed in user space and involves many layers between the client and the eventual HDFS operation, which results in poor performance.
It also can’t support use by any tools or applications that perform random writes.
And if you write a file, there’s no guarantee that the subsequent read will result in the contents you just wrote, so its consistency model is much worse than that of HDFS.
This is shown in the following example, where you write a file into HDFS and immediately read it, only to find that it seems to contain no contents:
You write a file to HDFS an immediately attempt to concatenat.
If you wait a second or two and reissue the cat command, you’ll now.
Issue comm the d cont that cont perm In conclusion, although Hadoop FUSE sounds like an interesting idea, it’s not ready for use in production environments.
B.5 NameNode embedded HTTP The advantage of using HTTP to access HDFS is that it relieves the burden of having to have the HDFS client code installed on any host that requires access.
Further, HTTP is ubiquitous and many tools and most programming languages have support for HTTP, which makes HDFS that much more accessible.
The NameNode has an embedded Jetty HTTP/HTTPS web server, which is used for the SecondaryNameNode to read images and merge them back.
It also supports the HTFP filesystem, which utilities such as distCp use to enable cross-cluster copies when Hadoop versions differ.
It supports a handful of operations and only read operations (HDFS writes aren’t supported)
Let’s look at a handful of basic filesystem operations that can be made with the embedded HTTP server.
The following shows an example of using curl, a client HTTP utility, to perform a directory listing:
For this exercise, you’ll create a directory in HDFS.te a small file in.
Files can be downloaded from the embedded HTTP NameNode server too.
That’s pretty much all that the embedded HTTP server currently supports in terms of file-level operations.
What’s interesting with the implementation of this servlet is that it redirects the actual file download to one of the DataNodes that contains the first block of the file.
That DataNode then streams back the entire file to the client.
There are a few other operations that can be performed via the HTTP interface, such as fsck for retrieving any issues with the filesystem, and contentSummary, which returns statistical information about a directory, such as quota limits, size, and more:
The response contains a listing element with two child elements, the first one being the element.
These three operations combined together can be used to read information from HDFS, but that’s about it.
B.6 HDFS proxy The HDFS proxy is a component in the Hadoop contrib that provides a web app proxy frontend to HDFS.
Its advantages over the embedded HTTP server are an access-control layer and support for multiple Hadoop versions.
Because the HDFS proxy leverages the embedded HTTP Jetty server in the NameNode, it has the same limitations that you saw in that section, primarily around only being able to support file reads.
Details about how to install and use the HDFS proxy are available at http://goo.gl/A9dYc.
Its advantage over the current Hadoop HTTP interface is that it supports writes as well as reads.
It’s a project created by Cloudera as a full replacement for the existing Hadoop HTTP service, and it’s planned for contribution into Hadoop.
After you have the Hoop server up and running, it’s simple to perform filesystem commands via curl:1
Let’s go through a sequence of basic filesystem manipulations, where you create a directory, write a file, and then list the directory contents:
Perform a director listing of /hoop-tes which shows the fi.
Hoop, as with any HDFS proxy, will suffer from adding hops between a client and HDFS, as well as circumventing data locality features available when using the Java HDFS client.
But it’s a huge improvement over the HDFS proxy, primarily because it can support writes due to its use of the Java HDFS client.
Figure B.6 shows that it coexists alongside the existing HDFS HTTP services.
You’ll use WebHDFS to create a directory, write a file to that directory, and finally remove the file.
Your first step is to create a directory, /whdfs, in HDFS.
Table B.1 shows the URL constructs, and optional parameters that can be supplied.
For example, the default in HDFS is the three digit octal 755, equivalent to -rwxr-xr-x.
Next you’ll create a file called test.txt under your newly created /whdfs directory.
You should quickly examine the options you have available when creating the file in table B.2
You first need to communicate your intent to create a file with the NameNode.
The NameNode replies with an HTTP redirect to a DataNode URL, which you must use to actually write the file content:
For example, the default in HDFS is the three digit octal 755, equivalent to -rwxr-xr-x.
The ampersand (&) in bash is a control character that’s used to launch a process in the background.
Because URLs frequently contain ampersands, it’s best to always enclose them in double quotes.
Notify the NameNode of your intent to create a file.
The response is an HTTP temporar redirect with a location field containin the DataNode URL to be used for th.
The options for APPEND are the same as for the creation operation; refer to table B.2 for more details:
Your next operation is to perform a directory listing in your directory:
Use the HDFS concatenate command to view the contents of the file.
A file status operation returns some statistics around a file or directory:
WebHDFS is a big step forward for HDFS in allowing rich client-side access to HDFS via HTTP.
B.9 Distributed copy Hadoop has a command-line tool for copying data between Hadoop clusters called distCp.
It performs the copy in a MapReduce job, where the mappers copy from one filesystem to another.
The following example shows a copy within the same cluster.
To copy between clusters running the same Hadoop version, change the URLs to point to the source and destination NameNode URLs:
One of the useful characteristics of distCp is that it can copy between multiple versions of Hadoop.
To support this, it uses the NameNode and DataNode HTTP interfaces to read data from the source cluster.
Because the Hadoop HTTP interfaces don’t support writes, when you’re running distCp between clusters of differing versions, you must run it on the destination cluster.
Notice in the following example that the source argument uses hftp as the scheme:
B.11 MapReduce MapReduce is a great mechanism to get data into HDFS.
Unfortunately, other than distCp, there’s no other built-in mechanism to ingest data from external sources.
Let’s look at how to write a MapReduce job to pull data from an HTTP endpoint:
You can run the MapReduce job and examine the contents of HDFS after it completes:
Get the job’s task ID, which is unique across all the tasks.
Get the connection timeout or use a default if not supplied.
Create the path to the fi that you’ll use to write th URL contents.
Copy the contents of the HTTP body into the HDFS file.
You emit the location of the URL file in HDFS, as well as the URL that was downloaded, so that they can be correlated.
Create a file in HDFS containing a list of URLs you want to download.
If you want multiple mappers to be run, then simply create separate input files, and each one will be processed by a separate mapper.
We’ve gone through a number of Hadoop built-in mechanisms to read and write data into HDFS.
If you were to use them, you’d have to write some scripts or code to manage the process of the ingress and egress because all the topics we covered are low-level.
If you’re using Hadoop you should have a solid understanding of HDFS so that you can make smart decisions about how to manage your data.
In this appendix we’ll walk through how HDFS reads and writes files to help you better understand how HDSF works behind the scenes.
Not many details about GFS are known beyond those published in the paper, but HDFS is a near clone2 of GFS, as described by the Google paper.
It was designed to avoid the overhead of network I/O and disk seeks by introducing the notion of data locality (the ability to read/write data that’s closest to the client), and by using large block sizes.
Files in HDFS are stored across one or more blocks, and each block is typically 64 MB or larger.
Blocks are replicated across multiple hosts in your cluster (as shown in figure C.1) to help with availability and fault tolerance.
Each block has a checksum associated with it, and if a discrepancy between the checksum and the block contents is detected (as in the case of bit rot), this information is sent to the HDFS master.
The HDFS master coordinates the creation of a new replica of the bad block as well as the deletion of the corrupted block.
C.2 How HDFS writes files A look at how HDFS writes files will bootstrap your HDFS knowledge and help you make smart decisions about your data and cluster, and how you work with your data.
The first step is to use the command-line interface (CLI) to copy a file from local disk to HDFS:
Now let’s look at how to achieve the same effect using Java:
The 2.x release will eventually include High Availability (HA) support for the NameNode.
Also included in 2.x are the additions of a Backup Node and Checkpoint Node, which serve as replacements of the SecondaryNameNode (although the SNN still exists)
The Checkpoint Node performs the same functions as the SecondaryNameNode; it downloads from the NameNode the current image file and subsequent edits, merges them together to create a new image file, and then uploads the new image to the NameNode.
The Backup Node is a superset of the Checkpoint Node, also providing that checkpointing mechanism, as well as acting as a NameNode in its own right.
This means if your primary NameNode goes down, you can immediately start using your Backup NameNode.
Copy the contents of the local file to the file in HDFS.
Now that you know how to copy a file using the CLI and Java, let’s look at what HDFS is doing behind the scenes.
Figure C.2 shows the components and how they interact when you write a file in HDFS.
If you run the previous example and don’t include the Hadoop configuration directory in your classpath, Hadoop uses default settings for all of its configuration.
When determining what DataNodes should be used for a block, the NameNode first attempts to pick the local node, if the client is running on a DataNode.
Local disk—Network I/O is expensive, so reading from local disk is always preferred over any network I/O.
Other—The data is resident on a separate rack, which makes it the slowest I/O of the three nodes (generally due to the additional network hops between the client and data)
In a rack-aware environment, the NameNode will ensure that at least one copy of the replica is on a rack separate from the other replicas.
If you’re using a high replication value, it also ensures that no one rack has too many replicas.
The list of DataNodes for the block are returned to the client ordered by proximity to the client, which means if the client is running on a DataNode, it will write to the local DataNode first.
There are two ways to enable rack awareness in Hadoop.
An alternative (and simpler) mechanism would be to write a script and locate it on your NameNode.
The following code is a brief example of how you’d write a shell script:
Prior to 2.x, Hadoop internally used its own RPC mechanism for components to communicate with each other.
This RPC mechanism uses Java’s proxy mechanism coupled with a simple marshalling framework (which marshalls Writable and Java primitives)
Acknowledgements from the DataNodes are also pipelined in reverse order.
The client only moves on to writing the next block after all acks for packets sent for the block have been received.
When each DataNode has completed writing the block locally, the blocks are moved from temporary storage into permanent storage, and each DataNode asynchronously notifies the NameNode of their block storage (steps A, B, and C)
Finally, when all the blocks have been written and the client closes the stream, the NameNode is notified that it should persist the blocks related to the file (last step, 8)
Walking through how HDFS writes files helps you to understand HDFS internals, which in turn will let you make better decisions related to your data and your environment.
We looked at how HDFS components work, and how they relate and communicate with each other.
You also learned how HDFS makes decisions about where to place your file blocks, and how you can set up your cluster to be rack aware.
Now that you have a handle on how HDFS writes, let’s move on to file reads.
C.3 How HDFS reads files Typically, you’ll read a file many more times than you’ll write it, so it’s equally important to know how HDFS reads your files.
Let’s first take a look at how you read (or view) the contents of a file using the CLI:
The interactions between Hadoop components for reading a file are detailed in figure C.3, which shows the interactions involved in reading a file from HDFS.
For each block, the NameNode orders the DataNodes by proximity to the client; a local DataNode (on the same physical host) is favored over a remote DataNode, which incurs the overhead of network I/O.
Rack-local DataNodes are also favored over nodes on different racks.
Th involves a request to the NameNode, whi sends back the block details for up to t first ten blocks of the file to the clien.
Reads the contents of the file and writes them t standard output.
As the client requests bytes from the stream, the DFSInputStream in turn reads from the DataNode’s stream until the end of the block is reached (in step 3 of the same figure)
If the file is larger than ten blocks, it once again asks the NameNode for block details on the next ten blocks (in step 6), and continues this pattern until all the blocks for the file have been read.
Now that you understand how HDFS reads data, you can see that it’s optimized for local reads, which is an important characteristic of a distributed system.
MapReduce leverages this knowledge and intelligently schedules its map tasks to execute on nodes such that the map reads the data from local disk and not over the network.5 You can also leverage this knowledge when making decisions about the architecture of your application.
Whether or not MapReduce can schedule a map task on a node that has all of its data local is dependent on a variety of factors.
The second is a framework provided to perform a replicated join, and you’ll build in some smarts that will allow you to cache the smaller of the datasets being joined.
D.1 An optimized repartition join framework The Hadoop contrib join package requires that all the values for a key be loaded into memory.
How can you implement a reduce-side join without that memory space overhead? In this optimization you’ll cache the dataset that’s smallest in size, and then perform a join as you iterate over data from the larger dataset.
This involves performing a secondary sort over the map output data so that the reducer will receive the data from the smaller dataset ahead of the larger dataset.
Figure D.2 shows a class diagram broken into two parts, with a generic framework, and some sample implementation classes.
The goal is to create a generic repartition join mechanism that can work with any datasets.
For the sake of brevity we’ll highlight the main elements of the package.
First we’ll look at the configure method, which is called at.
The goal here is to label each dataset so the reducer can distinguish between both datasets, and also to determine whether the input split you’re processing is the smaller or larger dataset:
Emit key/value where key is a composite key containing the join field and the dataset identifier.
Figure D.1 An optimized MapReduce implementation of a repartition join.
An abstract method that should return unique identifier representing the datase.
An abstract method tha must determine if th current dataset is th.
If re d re The map method first asks the implementing class to engineer an OutputValue object, which contains the value that the implementing class wants to use in the join (and presumably include in the final output), and a boolean indicating whether the value is being materialized from the smaller dataset.
If the map method then asks the implementing class to engineer the key that will be used for the join, this will make it the output key for the map:
Figure D.2 Class diagram showing main classes in the framework and a sample implementation.
An abstract method t create the map output key which is used to group dat.
Update the output value to indicate whether it originated from the smaller dataset.
Retrieve the group key, which is the map output key.
Ca th Figure D.3 shows the composite key and values emitted by the map.
The secondary sort will partition on the join key, but will order all the keys for a single join key using the whole composite key.
The composite key contains an integer indicating if the data source is from the small file, and as such can be used to ensure that values from the small file are passed to the reducer before records for the large file.
Because you have a guarantee that the small file values will arrive at the reducer ahead of the large file values, you can cache all of the values from the small dataset, and when you start seeing values from the large dataset, join each one with the cached values:
Create a structure to store the cached values from the.
Clone the value because it’s reused by the MapReduce code to store.
The joinAndCollect method combines values from the two datasets together and emits them:
Now you have the main guts of the framework uncovered.
D.2 A replicated join framework A replicated join is a map-side join, and gets its name from the fact that the smallest of the datasets is replicated to all the map hosts.
The implementation of the replicated join is straightforward and is demonstrated in Chuck Lam’s Hadoop in Action.
The goal in this section is to create a generic replicated join framework that can work with any datasets.
I’ll also provide an optimization that will dynamically determine if the distributed cache contents are larger than the input split, in which case you’d cache the map input and execute the join in the mapper cleanup method.
The class diagram for this framework is shown in figure D.4
But the join class can be extended to support any input and output formats.
An abstract method that must be implemented to perform th combination of dataset value.
Even if no data was collected from the small dataset, calling the combine method allows implementations to.
The mapper setup method determines if the map’s input split is larger than the distributed cache, in which case it loads the distributed cache into memory.
The map function either performs a join or caches the key/value pairs, based on whether the setup method loaded the cache.
If the input split is smaller than the distributed cache, the map cleanup method will read the records in the distributed cache and join them with the cache created in the map function.
It determines if the size of the files in the distributed cache are smaller than the input split, and if they are, loads them into a HashMap:
Your map method chooses its behavior based on whether the setup method cached the distributed cache.
If the distributed cache was loaded into memory, then it proceeds to join the tuple supplied to the map method with the cache.
Otherwise, it caches the map tuple for use later in the cleanup method:
If the input split is from a file, determi whether the distributed cache files a.
If the input split is not from a file, assume that the distributed cache is smaller, because you have no way.
After all of the records have been fed to the map method, the MapReduce framework will call the cleanup method.
If the contents of the distributed cache were larger than the input split, it is here where you perform the join between the map function’s cache of the input split tuples with the records contained in the distributed cache:
Ensure the join method is called with record in a predictable order: the record from th input split first, followed by the record b.
If the result of the join was a non-NULL object.
The default implementation of the joi which can be overridden to support othe.
InputFormat and OutputFormat classe concatenates the string forms of th.
Finally, the job driver code must specify the files that need to be loaded into the distributed cache.
The following code works with a single file, as well as a directory containing the results of a MapReduce job:
The assumption with this framework is that either the distributed cache or the input split contents can be cached in memory.
The advantage of this framework is that it will cache the smaller of the distributed cache and the input split.
In their optimization they further partition the distributed cache into N partitions, and likewise cache the map tuples into N hashtables, which process provides a more optimal join in the map cleanup method.
A downside to the replicated join is that each map task must read the distributed cache on startup.
A potential optimization suggested by the paper referenced in the previous paragraph is to override the FileInputFormat splitting such that input splits that exist on the same host are combined into a single split, thereby cutting down on the number of map tasks that need to load the distributed cache into memory.
But it requires that the input files of both datasets be sorted and distributed into identical partitions, which requires a good amount of preprocessing prior to leveraging their join mechanism.
It off ers developers handy ways to store, manage, and analyze data.
Hadoop in Practice collects 85 battle-tested examples and presents them in a problem/solution format.
It balances conceptual foundations with practical recipes for key problem areas like data ingress and egress, serialization, and LZO compression.
You’ll explore each technique step by step, learning how to build a specifi c solution along with the thinking that went into it.
As a bonus, the book’s examples create a well-structured and understandable codebase you can tweak to meet your own needs.
Th is book assumes the reader knows the basics of Hadoop.
Alex Holmes is a senior soft ware engineer with extensive expertise in solving big data problems using Hadoop.
He has presented at JavaOne and Jazoon and is a technical lead at VeriSign.
Interesting topics that    tickle the creative brain.”—Mark Kemna, Brillig “Ties together the Hadoop ecosystem.
Hadoop in Practice brief contents contents preface acknowledgments about this book Roadmap Code conventions and downloads Third-party libraries Datasets Getting help Author Online About the author About the cover illustration.
Technique 62 Using Mahout to train and test a spam classifier.
Technique 75 A four-step process to working rapidly with big data.
Technique 83 Figuring out the JVM startup arguments for a task.
