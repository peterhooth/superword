Special thanks to editor John Daily, to everyone who helped, and Basho Press.
Now imagine you could bet again, but only win if the wheel made a sequential 100 spins in your favor, otherwise you lose.
Would you still play? Winning a single bet might be easy, but over many trials the odds are not in your favor.
People make these sorts of bets with data all of the time.
A single server has a good chance of remaining available.
When you run a cluster with thousands of servers, or billions of requests, the odds of any one breaking down becomes the rule.
A once-in-a-million disaster is commonplace in light of a billion opportunities.
Riak is an open-source, distributed key/value database for high availability, fault-tolerance, and nearlinear scalability.
In short, Riak has remarkably high uptime and grows with you.
As the modern world stitches itself together with increasingly intricate connections, major shifts are occurring in information management.
The web and networked devices spur an explosion of data collection and access unseen in the history of the world.
The magnitude of values stored and managed continues to grow at a staggering rate, and in parallel, more people than ever require fast and reliable access to this data.
There’s a lot of discussion around what constitutes Big Data.
I have a 6 Terabyte RAID inmy house to store videos and other backups.
Does that count? On the other hand, CERN grabbed about 200 Petabytes looking for the Higgs boson.
Riak was built as a solution to real Big Data problems, based on the Amazon Dynamo design.
Dynamo is a highly available design—meaning that it responds to requests quickly at very large scales, even if your application is storing and serving terabytes of data a day.
It’s currently used by Github, Comcast, Voxer, Disqus and others, with the larger systems storing hundreds of TBs of data, and handling several GBs per node daily.
Erlang was chosen due to its strong support for concurrency, solid distributed communication, hot code loading, and fault-tolerance.
It runs on a virtual machine, so running Riak requires an Erlang installation.
So should you use Riak? A good rule of thumb for potential users is to ask yourself if every moment of downtime will cost you in some way (money, users, etc)
Not all systems require such extreme amounts of uptime, and if you don’t, Riak may not be for you.
Don’t feel compelled to have Riak, or even have a computer handy, when starting this book.
Youmay feel like installing at some point, and if so, instructions can be found in the Riak docs.
The existence of databases like Riak is the culmination of two basic trends: accessible technology spurring different data requirements, and gaps in the data management market.
First, as we’ve seen steady improvements in technology along with reductions in cost, vast amounts of computing power and storage are now within the grasp of nearly anyone.
Along with our increasingly interconnected world caused by the web and shrinking, cheaper computers (like smartphones), this has catalyzed an exponential growth of data, and a demand for more predictability and speed by savvier users.
In other words, more data is being created on the front-end, while more data is being managed on the backend.
These new databases are collected under the moniker NoSQL, and Riak is of its ilk.
Modern databases can be loosely grouped into the ways they represent data.
Unlike relational databases, but similar to document and columnar stores, objects cannot be joined by Riak.
Client code is responsible for accessing values and merging them, or by other code such as MapReduce.
The ability to easily join data across physical servers is a tradeoff that separates single node databases like relational and graph, from naturally partitionable systems like document, columnar, and key/value stores.
Relational normalization (organizing data to reduce redundancy) exists for systems that can cheaply join data together per request.
However, the ability to spread data across multiple nodes requires a denormalized approach, where some data is duplicated, and computed values may be stored for the sake of performance.
They excel inmodeling complex relationships betweennodes, andmany implementations can handlemultiple billions of nodes and relationships (or edges and vertices)
I tend to include triplestores and object DBs as specialized variants.
Document datastores model hierarchical values called documents, represented in formats such as JSON or XML, and do not enforce a document schema.
Key/Value, or KV stores, are conceptually like hashtables, where values are stored and accessed by an immutable key.
They range from single-server varieties like Memcached used for high-speed caching, to multi-datacenter distributed systems like Riak Enterprise.
Riak is a Key/Value (KV) database, built from the ground up to safely distribute data across a cluster of physical servers, called nodes.
A Riak cluster is also known as a ring (we’ll cover why later)
Depending on your background, you may call it hashtable, a map, a dictionary, or an object.
But the idea is the same: you store a value with an immutable key, and retrieve it later.
Retrieving Bob is as easy as going to his house.
Let’s say that poor old Bob dies, and Claire moves into this house.
The address remains the same, but the contents have changed.
Addresses in Riakville are more than a house number, but also a street.
For convenience, we call a bucket/key + value pair an object, sparing ourselves the verbosity of “X key in the Y bucket and its value”
Distributing data across several nodes is how Riak is able to remain highly available, tolerating outages and network partitions.
Riak combines two styles of distribution to achieve this: replication and partitions.
Replication is the act of duplicating data across multiple servers.
For example, imagine you have a list of country keys, whose values are those countries’ capitals.
The downside with replication is that you are multiplying the amount of storage required for every duplicate.
There is also some network overhead with this approach, since values must also be routed to all replicated nodes on write.
But there is a more insidious problem with this approach, which I will cover shortly.
A partition is how we divide a set of keys onto separate physical servers.
Rather than duplicate values, we pick one server to exclusively host a range of keys, and the other servers to host remaining nonoverlapping ranges.
With partitioning, our total capacity can increase without any big expensive hardware, just lots of cheap commodity servers.
For example, if we partition our countries into 2 servers, we might put all countries beginning with letters A-N into Node A, and O-Z into Node B.
There is a bit of overhead to the partition approach.
Some service must keep track of what range of values live on which node.
A requesting application must know that the key Spain will be routed to Node B, not Node A.
If one node goes down, that entire partition of data is unavailable.
Since partitions allow us to increase capacity, and replication improves availability, Riak combines them.
We partition data across multiple nodes, as well as replicate that data into multiple nodes.
Our server count has increased, but so has our capacity and reliability.
If you’re designing a horizontally scalable system by partitioning data, you must deal with replicating those partitions.
Riak applies consistent hashing to map objects along the edge of a circle (the ring)
If we visualize our 64 partitions as a ring, favorite falls here.
We count around the ring of vnodes in order, assigning each node to the next available vnode, until all vnodes are accounted for.
This places the object in physical nodes C, D, and E.
Once the write is complete, even if node C crashes, the value is still available on 2 other nodes.
We can visualize the Ring with its vnodes, managing nodes, and where favorite will go.
The Ring is more than just a circular array of hash partitions.
It’s also a system of metadata that gets copied to every node.
Each node is aware of every other node in the cluster, which nodes own which vnodes, and other system data.
Armed with this information, requests for data can target any node.
It will horizontally access data from the proper nodes, and return the result.
But when values are distributed, consistency might not be guaranteed.
In the middle of an object’s replication, two servers could have different results.
When we update favorite to cold pizza on one node, another node might contain the older value pizza, because of a network connectivity problem.
If you request the value of favorite on either side of a network partition, two different results could possibly be returned—the database is inconsistent.
If a network partition occurs between nodes, your servers still run.
Making Riak run faster by keeping R andW values low will increase the likelihood of temporarily inconsistent results (higher availability)
Setting those values higher will improving the odds of consistent responses (never quite reaching strict consistency), but will slow down those responses and increase the likelihood that Riak will fail to respond (in the event of a partition)
Currently, no setting can make Riak truly CP in the general case, but features for a few strict cases are being researched.
A question the CAP theorem demands you answer with a distributed system is: do I give up strict consistency, or give up ensured availability? If a request comes in, do I lock out requests until I can enforce consistency across the nodes? Or do I serve requests at all costs, with the caveat that the database may become inconsistent?
Riak’s solution is based on Amazon Dynamo’s novel approach of a tunable AP system.
It takes advantage of the fact that, though the CAP theorem is true, you can choose what kind of tradeoffs you’re willing to make.
Riak is highly available to serve requests, with the ability to tune its level of availability (nearing, but never quite reaching, full consistency)
Riak allows you to choose how many nodes you want to replicate an object to, and how many nodes must be written to or read from per request.
These values are settings labeled n_val (the number of nodes to replicate to), r (the number of nodes read from before returning), and w (the number of nodes written to before considered successful)
You can set other values (R,W) to equal the n_val number with the shorthand all.
In other words, setting w=all would help ensure your system was more likely to be consistent, at the expense of waiting longer, with a chance that your write would fail if fewer than 3 nodes were available (meaning, over half of your total servers are down)
A failed write, however, is not necessarily a true failure.
The client will receive an error message, but the write will typically still have succeeded on some number of nodes smaller than theW value, and will typically eventually be propagated to all of the nodes that should have it.
To ensure you have the most recent value, you can read from all 3 nodes containing objects (r=all)
Remember when Imentioned that RDBMS databases werewrite consistent? This is close to read consistency.
Just like w=all, however, the read will fail unless 3 nodes are available to be read.
Finally, if you only want to quickly read any value, r=1 has low latency, and is likely consistent if w=all.
In general terms, the N/R/W values are Riak’s way of allowing you to trade lower consistency for more availability.
If you’ve followed thus far, I only have one more conceptual wrench to throw at you.
I wrote earlier that with r=all, we can “compare all nodes against each other and choose the latest one.” But how do we know which is the latest value? This is where vector clocks (aka vclocks) come into play.
Vector clocks measure a sequence of events, just like a normal clock.
But since we can’t reasonably keep the clocks on dozens, or hundreds, or thousands of servers in sync (without really exotic hardware, like geosynchronized atomic clocks, or quantum entanglement), we instead keep a running history of updates.
Let’s use our favorite example again, but this time we have 3 people trying to come to a consensus on their favorite food: Aaron, Britney, and Carrie.
We’ll track the value each has chosen along with the relevant vector clock.
To illustrate vector clocks in action, we’ll cheat a bit.
By default, Riak no longer tracks vector clocks using client information, but rather via the server that coordinates a write request; nonetheless, the concept is the same.
We’ll cheat further by disregarding the timestamp that is stored with vector clocks.
When Aaron sets the favorite object to pizza, a vector clock could contain his name and the number of updates he’s performed.
Britney now comes along, and reads favorite, but decides to update pizza to cold pizza.
When using vclocks, she must provide the vclock returned from the request she wants to update.
This is how Riak can help ensure you’re updating a previous value, and not merely overwriting with your own.
At the same time as Britney, Carrie decides that pizza was a terrible choice, and tried to change the value to lasagna.
Nowwe are back to the simple case, where requesting the value of favoritewill just return the agreed upon pizza.
So why don’t we just distribute a standard relational database? MySQL has the ability to cluster, and it’s ACID (Atomic, Consistent, Isolated, Durable), right? Yes and no.
A single node in the cluster is ACID, but the entire cluster is not without a loss of availability and (often worse) increased latency.
When you write to a primary node, and a secondary node is replicated to, a network partition can occur.
To remain available, the secondary will not be in sync (eventually consistent)
Have you ever loaded from a backup on database failure, but the dataset was incomplete by a few hours? Same idea.
Or, the entire transaction can fail, making the whole cluster unavailable.
Unlike single node databases like Neo4j or PostgreSQL, Riak does not support ACID transactions.
Locking across multiple servers would can write availability, and equally concerning, increase latency.
As your server count grows—especially as you introduce multiple datacenters—the odds of partitions and node failures drastically increase.
We’ll cover other technical concepts as needed, including the gossip protocol, hinted handoff, and readrepair.
We’ll check out lookups, take advantage of write hooks, and examine alternative query options like secondary indexing, search, andMapReduce.
It’s worth mentioning that I use the word “node” a lot.
Realistically, this means a physical/virtual server, but really, the workhorses of Riak are vnodes.
Since Riak is a KV database, themost basic commands are setting and getting values.
We’ll use theHTTP interface, via curl, but we could just as easily use Erlang, Ruby, Java, or any other supported language.
The basic structure of a Riak request is setting a value, reading it, and maybe eventually deleting it.
The next command reads the value pizza under the bucket/key food/favorite.
This is the simplest formof read, respondingwith only the value.
Riak containsmuchmore information, which you can access if you read the entire response, including the HTTP header.
The anatomy of HTTP is a bit beyond this little book, but let’s look at a few parts worth noting.
Timings A block of headers represents different timings for the object or the request.
ETag - An entity tag which can be used for cache validation by a client.
X-Riak-Vclock - A logical clock which we’ll cover in more detail later.
Content These describe the HTTP body of the message (in Riak’s terms, the value)
Content-Length - The length, in bytes, of the message body.
Some other headers like Link will be covered later in this chapter.
All it requires is a bucket name, and it will generate a key for you.
Let’s add a JSON value to represent a person under the people bucket.
The response header is where a POST will return the key it generated for you.
Body You may note that no body was returned with the response.
For any kind of write, you can add the returnbody=true parameter to force a value to return, along with value-related headers like X-Riak-Vclock and ETag.
This detail isn’t normally important, except to understand two things:
In Riak, a delete is actually a read and a write, and should be considered as such when calculating read/write ratios.
Checking for the existence of a key is not enough to know if an object exists.
Youmight be reading a key after it has been deleted, so you should check for tombstone metadata.
The following will give us all of our buckets as a JSON object.
And this will give us all of our keys under the food bucket.
If we had verymany keys, clearly this might take a while.
So Riak also provides the ability to stream your list of keys.
When it has exhausted its list, it will close the connection.
You can see the details through curl in verbose (-v) mode (much of that response has been stripped out below)
You should note that list actions should not be used in production (they’re really expensive operations)
But they are useful for development, investigations, or for running occasional analytics at off-peak hours.
Although we’ve been using buckets as namespaces up to now, they are capable of more.
Different use-cases will dictate whether a bucket is heavily written to, or largely read from.
You may use one bucket to store logs, one bucket could store session data, while another may store shopping cart data.
Sometimes low latency is important, while other times it’s high durability.
And sometimes we just want buckets to react differently when a write occurs.
The basis of Riak’s availability and tolerance is that it can read from, or write to, multiple nodes.
Riak allows you to adjust these N/R/W values (which we covered under Concepts) on a per-bucket basis.
But we can set this n_val to less than the total number of nodes.
Any bucket property, including n_val, can be set by sending a props value as a JSON object to the bucket URL.
You can take a peek at the bucket’s properties by issuing a GET to the bucket.
If you have a command-line tool like jsonpp (or json_pp) installed, you can pipe the output there for easier reading.
The results below are a subset of all the props values.
But you may also have noticed that the cart props returned both r and w as quorum, rather than a number.
As long as r+w > n, in the absence of sloppy quorum (below), you’ll be able to get the newest values.
In other words, you’ll have a reasonable level of consistency.
If you were a strict quorum, you could merely refuse both drinks, since the required people (N=2) are unavailable.
But you’d rather be a sloppy drunk… erm, Imean sloppy quorum.
Rather than deny the drink, you take both, one accepted on her behalf (you also get to pay)
More than R’s and W’s Some other values you may have noticed in the bucket’s props object are pw, pr, and dw.
Per Request It’s worth noting that these values (except for n_val) can be overridden per request.
If any of the nodes currently responsible for the data cannot complete the request (i.e., hand off the data to the storage backend), the client will receive a failure message.
This doesn’t mean that the write failed, necessarily: if two of three primary vnodes successfully wrote the value, it should be available for future requests.
Thus trading availability for consistency by forcing a high dw or pw value can result in unexpected behavior.
Another utility of buckets are their ability to enforce behaviors on writes by way of hooks.
You can attach functions to run either before, or after, a value is committed to a bucket.
Functions that run before a write is called precommit, and has the ability to cancel a write altogether if the incoming data is considered bad in some way.
A simple precommit hook is to check if a value exists at all.
If you try and post to the cart bucket without a value, you should expect a failure.
You can also write precommit functions in JavaScript, though Erlang code will execute faster.
Post-commits are similar in form and function, albeit executed after the write has been performed.
The function’s return value is ignored, thus it cannot cause a failure message to be sent to the client.
In other words: although eventual consistency says a write will replicate to other nodes in time, there can be a bit of delay during which all nodes do not contain the same value.
That difference is entropy, and so Riak has created several anti-entropy strategies (abbreviated as AE)
We’ve already talked about how an R/W quorum can deal with differing values when write/read requests overlap at least one node.
Riak can repair entropy, or allow you the option to do so yourself.
The most basic, and least reliable, strategy for curing entropy is called last write wins.
It’s the simple idea that the last write based on a node’s system clock will overwrite an older one.
This is currently the default behavior in Riak (by virtue of the allow_mult property defaulting to false)
You can also set the last_write_wins property to true, which improves performance by never retaining vector clock history.
Realistically, this exists for speed and simplicity, when you really don’t care about true order of operations, or the possibility of losing data.
Since it’s impossible to keep server clocks truly in sync (without the proverbial geosynchronized atomic clocks), this is a best guess as to what “last” means, to the nearest millisecond.
A client writes a value using a stale (or missing) vector clock.
Two clients write at the same time with the same vector clock value.
First Casey (a vegan) places 10 orders of kale in the cart.
Note the opaque vector clock (via the X-Riak-Vclock header) returned by Riak.
That same value will be returned with any read request issued for that key until another write occurs.
In order to allow Riak to track the update history properly, Mark includes the most recent vector clock with his PUT.
If you look closely, you’ll notice that the vector clock changed with the second write request.
Now let’s consider a third roommate, Andy, who loves almonds.
Before Mark updates the shared cart with milk, Andy retrieved Casey’s kale order and appends almonds.
As with Mark, Andy’s update includes the vector clock as it existed after Casey’s original write.
Issuing a plain get on the /cart/fridge-97207 key will also return the vtags of all siblings.
Setting last_write_wins to true will optimize writes by assuming that previous vector clocks have no inherent value.
With Riak 1.3, Basho introduced active anti-entropy to proactively identify and repair inconsistent data.
This feature is also helpful for recovering data loss in the event of disk corruption or administrative error.
The overhead for this functionality isminimized bymaintaining sophisticated hash trees (“Merkle trees”) which make it easy to compare data sets between vnodes, but if desired the feature can be disabled.
The truth is, key-value is a pretty powerful mechanism that spans a spectrum of use-cases.
However, sometimes we need to lookup data by value, rather than key.
Sometimes we need to perform some calculations, or aggregations, or search.
Querying can be done in two forms: exact match and range.
What people own fridge-97207? It’s a quick lookup to receive the keys that have matching index values.
With those keys it’s a simple lookup to get the bodies.
It’s a basic form of 2i, with a decent array of utility.
MapReduce is a method of aggregating large amounts of data by separating the processing into two phases, map and reduce, that themselves are executed in parts.
Map will be executed per object to convert/extract some value, then those mapped values will be reduced into some aggregate result.
What do we gain from this structure? It’s predicated on the idea that it’s cheaper to move the algorithms to where the data lives, than to transfer massive amounts of data to a single server to run a calculation.
This method, popularized by Google, can be seen in a wide array of NoSQL databases.
In Riak, you execute a MapReduce job on a single node, which then propagates to the other nodes.
The results are mapped and reduced, then further reduced down to the calling node and returned.
This time we’ll go the easy route and write JavaScript.
You execute MapReduce by posting JSON to the /mapred path.
Both map and reduce phases should always return an array.
The map phase receives a single riak object, while the reduce phase received an array of values, either the result of multiple map function outputs, or of multiple reduce outputs.
I probably cheated a bit by using JavaScript’s reduce function to sum the values, but, well, welcome to the world of thinking in terms of MapReduce!
It would look like this to have the mapper just return matching object keys.
Pay special attention to the map function, and lack of reduce.
Another option when using MapReduce is to combine it with secondary indexes.
You can pipe the results of a 2i query into a MapReducer, simply specify the index you wish to use, and either a key for an index lookup, or start and end values for a ranged query.
Conceptually, a link is a one-way relationship from one object to another.
Link walking is a convenient query option for retrieving data when you start with the object linked from.
Let’s add a link to our people, by setting casey as the brother of mark using the HTTP header Link.
Any combination of these query values can be set to a wildcard _, meaning you want to match anything.
Link walking always returns a multipart/mixed, since a single key can contain any number of links, meaning any number of objects returned.
You can actually chain together linkwalks, whichwill follow the a followed link.
If casey has links, they can be followed by tacking another link triplet on the end, like so:
Now it may not seem so from what we’ve seen, but link walking is a specialized case of MapReduce.
If you have used Riak before, or have some older documentation, you may wonder what the difference is between Riak Search and Yokozuna.
In an attempt to make Riak Search user friendly, it was originally developed with a “Solr like” interface.
Sadly, due to the complexity of building distributed search engines, it was woefully incomplete.
Basho decided that, rather than attempting to maintain parity with Solr, a popular and featureful search engine in its own right, it made more sense to integrate the two.
Changes are to be expected, so please refer to the yokozuna project page for the most recent information.
Before using Yokozuna, you’ll have to have it installed and a bucket set up with an index (these details can be found in the next chapter)
Here we add ryan to the people table (with a default index)
To execute a search, request /search/[bucket] along with any distributed Solr parameters.
Here we query for documents that contain a word starting with zez, request the results to be in json format (wt=json), only return the Riak key (fl=_yz_rk)
With the matching _yz_rk keys, you can retrieve the bodies with a simple Riak lookup.
Another useful feature of Solr and Yokozuna is the tagging of values.
Riak is a distributed data store with several additions to improve upon the standard key-value lookups, like specifying replication values.
Since values in Riak are opaque, many of thesemethods either: require custom code to extract and give meaning to values, such asMapReduce; or allow for header metadata to provide an added descriptive dimension to the object, such as secondary indexes, link walking, or search.
Next we’ll peek further under the hood, and see how to set up and manage a cluster of your own, and what you should know.
In some ways, Riak is downright mundane in its role as the easiest NoSQL database to operate.
A network cable is cut at 2am? Deal with it after a few more hours of sleep.
Understanding this integral part of your application stack is still important, however, despite Riak’s reliability.
We’ve covered the core concepts of Riak, and I’ve provided a taste of how to use it, but there is more to the database than that.
There are details you should know if you plan on operating a Riak cluster of your own.
Up to this point you’ve conceptually read about “clusters” and the “Ring” in nebulous summations.
What exactly do we mean, and what are the practical implications of these details for Riak developers and operators?
A cluster in Riak is a managed collection of nodes that share a common Ring.
Secondly, the Ring is also used as a shorthand for describing the state of the circular hash ring I just mentioned.
This Ring (aka Ring State) is a data structure that gets passed around between nodes, so each knows the state of the entire cluster.
Which node manages which vnodes? If a node gets a request for an object managed by other nodes, it consults the Ring and forwards the request to the proper nodes.
It’s a local copy of a contract that all of the nodes agree to follow.
The gossip protocol is Riak’s method of keeping all nodes current on the state of the Ring.
If a node goes up or down, that information is propagated to other nodes.
Periodically, nodes will also send their status to a random peer for added consistency.
Propagating changes in Ring is an asynchronous operation, and can take a couple minutes depending on Ring size.
Currently, it is not possible to change the number of vnodes of a cluster.
This means that you must have an idea of how large you want your cluster to grow in a single datacenter.
A great deal of effort has been made toward being able to change the number of vnodes, so by the time you read this, it is entirely possible that Basho has released a version of Riak that allows it.
Even if you are not a programmer, it’s worth taking a look at this Ring example.
It’s also worth remembering that partitions are managed by vnodes, and in conversation are sometimes interchanged, though I’ll try to be more precise here.
Riak has the amazing, and dangerous, attach command that attaches an Erlang console to a live Riak node, with access to all of the Riak modules.
Just to illustrate that Erlang binary value is a real number, the next line makes it a more readable format, similar to the ring partition numbers.
So what does all this have to do with replication? With the above list, we simply replicate a write down the list N times.
If something has happened to one of those nodes, like a network split (confusingly also called a partition —the “P” in “CAP”), the remaining active nodes in the list become candidates to hold the data.
When a node goes down, data is replicated to a backup node.
This is not permanent; Riak will periodically examine whether each vnode resides on the correct physical node and hands them off to the proper node when possible.
As long as the temporary node cannot connect to the primary, it will continue to accept write and read requests on behalf of its incapacitated brethren.
Hinted handoff not only helps Riak achieve high availability, it also facilitates datamigrationwhen physical nodes are added or removed from the Ring.
Now that we have a grasp of the general concepts of Riak, how users query it, and how Riak manages replication, it’s time to build a cluster.
It’s so easy to do, in fact, I didn’t bother discussing it for most of this book.
The Riak docs have all of the information you need to install it per operating system.
Get Riak from a package manager (a la apt-get or Homebrew), or build from source (the results end up under rel/riak, with the binaries under bin)
Most Riak operations can be performed though the command line.
Simply typing the riak command will give a usage list, although not a terribly descriptive one.
Most of these commands are self explanatory, once you know what they mean.
The riak-admin command is the meat operations, the tool you’ll use most often.
This is where you’ll join nodes to the Ring, diagnose issues, check status, and trigger backups.
Many of these commands are deprecated, and many don’t make sense without a cluster, but a few we can look at now.
It’smostly the same information you can get from getting /stats via HTTP, although the coverage of information is not exact (for example, riak-admin status returns disk, and /stats returns some computed values like gossip_received)
Finally, top is an analysis command checking the Erlang details of a particular node in real time.
Different processes have different process ids (Pids), use varying amounts of memory, queue up so many messages at a time (MsgQ), and so on.
This is useful for advanced diagnostics, and is especially useful if you know Erlang or need help from other users, the Riak team, or Basho.
With several solitary nodes running—assuming they are networked and are able to communicate to each other—launching a cluster is the simplest part.
Executing the cluster command will output a descriptive set of commands.
To create a new cluster, you must join another node (any will do)
Taking a node out of the cluster uses leave or force-remove, while swapping out an old node for a new one uses replace or forcereplace.
I should mention here that using leave is the nice way of taking a node out of commission.
If a server happens to explode (or simply smoke ominously), you don’t need its approval to remove it from the cluster, but can instead mark it as down.
Once all changes are staged, you must review the cluster plan.
It will give you all of the details of the nodes that are joining the cluster, and what it will look like after each step or transition, including the member-status, and how the transfers plan to handoff partitions.
Below is a simple plan, but there are cases when Riak requires multiple transitions to enact all of your requested actions, such as adding and removing nodes in one stage.
NOTE: Applying these changes will result in 1 cluster transition.
Making changes to cluster membership can be fairly resource intensive, so Riak defaults to only performing 2 transfers at a time.
You can choose to alter this transfer-limit using riak-admin, but bear in mind the higher the number, the greater normal operations will be impinged.
Without any data, adding a node to a cluster is a quick operation.
However, with large amounts of data to be transferred to a new node, it can take quite a while before the new node is ready to use.
You can get a list of available services with the services command.
You can also see if the whole ring is ready to go with ringready.
If the nodes do not agree on the state of the ring, it will output FALSE, otherwise TRUE.
For a more complete view of the status of the nodes in the ring, you can check out member-status.
And for more details of any current handoffs or unreachable nodes, try ring-status.
Below I turned off the C node to show what it might look like.
WARNING: The cluster state will not converge until all nodes.
If all of the above information options about your nodes weren’t enough, you can list the status of each vnode per node, via vnode-status.
It’ll show each vnode by its partition number, give any status information, and a count of each vnode’s keys.
Finally, you’ll get to see each vnode’s backend typesomething I’ll cover in the next section.
The last command is diag, which leverages Riaknostic to give you more diagnostic tools.
I know this was a lot to digest, and probably pretty dry.
There are plenty of details behind many of the riak-admin commands, too numerous to cover in such a short book.
I encourage you to toy around with them on your own installation.
The name setting is the name of the currentRiak node.
This is where the magic happens, such as handling requests and coordinating them for redundancy and read repair.
It’s what makes Riak a KV store rather than something else like a Cassandra-style columnar data store.
Riak Pipe is an input/output messaging system that forms the basis of Riak’sMapReduce.
Thiswas not always the case, andMR used to be a dedicated implementation, hence some legacy options.
Like the ability to alter the KV path, you can also change HTTP from /mapred to a custom path.
Number of items the mapper will fetch in one request.
Riak KV’s MapReduce implementation (under riak_kv, though implemented in Pipe) is the primary user of the Spidermonkey JavaScript engine—the second user is precommit hooks.
It’s an integration of the distributed Solr search engine into Riak, and provides some extensions for extracting, indexing, and tagging documents.
The Solr server runs its own HTTP interface, and though your Riak users should never have to access it, you can choose which solr_port will be used.
Several modern databases have swappable backends, and Riak is no different in that respect.
Riak currently supports three different storage engines: Bitcask, eLevelDB, andMemory — and one hybrid calledMulti.
Using a backend is simply a matter of setting the storage_backend with one of the following values.
If you don’t have a compelling reason to not use it, this is my suggestion.
With the Multi backend, you can even choose different backends for different buckets.
This can make sense, as one bucket may hold user information that you wish to index (use eleveldb), while another bucket holds volatile session information that you may prefer to simply remain resident (use memory)
In any case, Riak API represents the client facing aspect of Riak.
Implementations handle how data is encoded and transferred, and this project handles the services for presenting those interfaces, managing connections, providing entry points.
Other projects add depth to Riak but aren’t strictly necessary.
Two of these projects are lager, for logging, and riak_sysmon, for monitoring.
If you wish to disable rotation, you can either set.
Whether to redirect error_logger messages into lager %% defaults to true.
Finding reasonable limits for a given workload is a matter.
InnoDB - The MySQL engine once supported by Riak, but now deprecated.
You may recall that we skipped the diag command while looking through riak-admin, but it’s time to circle back around.
Riaknostic exists separately from the core project but as of Riak 1.3 is included and installed with the standard database packages.
I’m a bit concerned that my disk might be slow, so I ran the disk diagnostic.
Riak Control is shipped with Riak as of version 1.1, but turned off by default.
You can enable it on one of your servers by editing app.config and restarting the node.
Then, you’ll have to enable Riak Control in your app.config, and add a user.
Yeah it sucks, so be careful to not open your Control web access to the rest of the world, or you risk giving away the keys to the kingdom.
If auth is set to 'userlist' then this is the.
After you log in using the user you set, you should see a snapshot page, which communicates the health of your cluster.
If something is wrong, you’ll see a huge red “X” instead of the green checkmark, along with a list of what the trouble is.
There is more in line for Riak Control, like performing MapReduce queries, stats views, graphs, and more coming down the pipe.
It’s not a universal toolkit quite yet, but it has a phenomenal start.
Riak CS is Basho’s open source extension to Riak to allow your cluster to act as a remote storage mechanism, comparable to (and compatiblewith) Amazon’s S3
There are several reasons youmaywish to host your own cloud storage mechanism (security, legal reasons, you already own lots of hardware, cheaper at scale)
This is not covered in this short book, though I may certainly be bribed to write one.
While the documentation is freely available, the source code is not.
If you reach a scale where keeping multiple Riak clusters in sync on a local or global scale is necessary, I would recommend considering this option.
