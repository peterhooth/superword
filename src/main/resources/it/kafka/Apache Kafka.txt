Set up Apache Kafka clusters and develop custom message producers and consumers using practical, hands-on examples.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Nishant has enjoyed working with recognizable names in IT services and financial industries, employing full software lifecycle methodologies such as Agile and SCRUM.
He has also undertaken many speaking engagements on Big Data technologies.
I would also like to thank my wife (Himani) and my kids (Nitigya and Darsh) for their never-ending support, which keeps me going.
Vijay—Director of Technology, Innovation Labs, Impetus—for having faith in me and giving me an opportunity to write.
Magnus Edenhill is a freelance systems developer living in Stockholm, Sweden, with his family.
He specializes in high-performance distributed systems but is also a veteran in embedded systems.
For ten years, Magnus played an instrumental role in the design and implementation of PacketFront's broadband architecture, serving millions of FTTH end customers worldwide.
Since 2010, he has been running his own consultancy business with customers ranging from Headweb—northern Europe's largest movie streaming service—to Wikipedia.
Iuliia Proskurnia is a doctoral student at EDIC school of EPFL, specializing in Distributed Computing.
Iuliia was awarded the EPFL fellowship to conduct her doctoral research.
She is a winner of the Google Anita Borg scholarship and was the Google Ambassador at KTH (2012-2013)
For her Master's thesis, she designed and implemented a unique real-time, low-latency, reliable, and strongly consistent distributed data store for the stock exchange environment at NASDAQ OMX.
This Master's thesis was about fuzzy portfolio management in previously uncertain conditions.
This period was productive for her in terms of publications and conference presentations.
Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at www.PacktPub.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
It is aimed at getting you started with a feel for programming with Kafka so that you will have a solid foundation to dive deep into its different types of implementations and integrations.
In addition to an explanation of Apache Kafka, we also offer a chapter exploring Kafka integration with other technologies such as Apache Hadoop and Storm.
Our goal is to give you an understanding of not just what Apache Kafka is, but also how to use it as part of your broader technical infrastructure.
What this book covers Chapter 1, Introducing Kafka, discusses how organizations are realizing the real value of data and evolving the mechanism of collecting and processing it.
Chapter 3, Setting up the Kafka Cluster, describes the steps required to set up a single/multibroker Kafka cluster.
Chapter 4, Kafka Design, discusses the design concepts used for building a solid foundation for Kafka.
Chapter 5, Writing Producers, provides detailed information about how to write basic producers and some advanced-level Java producers that use message partitioning.
Chapter 6, Writing Consumers, provides detailed information about how to write basic consumers and some advanced-level Java consumers that consume messages from the partitions.
Chapter 7, Kafka Integrations, discusses how Kafka integration works for both Storm and Hadoop to address real-time and batch processing needs.
Chapter 8, Kafka Tools, describes information about Kafka tools, such as its administrator tools, and Kafka integration with Camus, Apache Camel, Amazon cloud, and so on.
We assume you have some familiarity with command-line Linux; any modern distribution will suffice.
Some of the examples in this book need multiple machines to see things working, so you will require access to at least three such hosts.
You will generally need the big data technologies, such as Hadoop and Storm, to run your Hadoop and Storm clusters.
Who this book is for This book is for readers who want to know about Apache Kafka at a hands-on level; the key audience is those with software development experience but no prior exposure to Apache Kafka or similar technologies.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: "We can include other contexts through the use of the include directive."
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the color images of this book We also provide you with a PDF file that has color images of the screenshots used in  this book.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/ submit-errata, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
In today's world, real-time information is continuously getting generated by applications (business, social, or any other type), and this information needs easy ways to be reliably and quickly routed to multiple types of receivers.
Most of the time, applications that are producing information and applications that are consuming this information are well apart and inaccessible to each other.
This, at times, leads to redevelopment of information producers or consumers to provide an integration point between them.
Therefore, a mechanism is required for seamless integration of information of producers and consumers to avoid any kind of rewriting of an application at either end.
In the present big data era, the very first challenge is to collect the data as it is a huge amount of data and the second challenge is to analyze it.
This analysis typically includes following type of data and much more:
Message publishing is a mechanism for connecting various applications with the help of messages that are routed between them, for example, by a message broker such as Kafka.
Kafka is a solution to the real-time problems of any software solution, that is, to deal with real-time volumes of information and route it to multiple consumers quickly.
Kafka provides seamless integration between information of producers and consumers without blocking the producers of the information, and without letting producers know who the final consumers are.
Apache Kafka is an open source, distributed publish-subscribe messaging system, mainly designed with the following characteristics:
Persistent messaging: To derive the real value from big data, any kind of information loss cannot be afforded.
Apache Kafka is designed with O(1) disk structures that provide constant-time performance even with very large volumes of stored messages, which is in order of TB.
High throughput: Keeping big data in mind, Kafka is designed to work on commodity hardware and to support millions of messages per second.
Distributed: Apache Kafka explicitly supports messages partitioning over Kafka servers and distributing consumption over a cluster of consumer machines while maintaining per-partition ordering semantics.
Multiple client support: Apache Kafka system supports easy integration of clients from different platforms such as Java, .NET, PHP, Ruby, and Python.
Real time: Messages produced by the producer threads should be immediately visible to consumer threads; this feature is critical to event-based systems such as Complex Event Processing (CEP) systems.
Kafka provides a real-time publish-subscribe solution, which overcomes the challenges of real-time data usage for consumption, for data volumes that may grow in order of magnitude, larger that the real data.
Kafka also supports parallel data loading in the Hadoop systems.
At the production side, there are different kinds of producers, such as the following:
At the consumption side, there are different kinds of consumers, such as the following:
Offline consumers that are consuming messages and storing them in Hadoop or traditional data warehouse for offline analysis.
Near real-time consumers that are consuming messages and storing them in any NoSQL datastore such as HBase or Cassandra for near real-time analytics.
Real-time consumers that filter messages in the in-memory database and trigger alert events for related groups.
Need for Kafka A large amount of data is generated by companies having any form of web-based presence and activity.
Data is one of the newer ingredients in these Internet-based systems and typically includes user-activity events corresponding to logins, page visits, clicks, social networking activities such as likes, sharing, and comments, and operational and system metrics.
This data is typically handled by logging and traditional log aggregation solutions due to high throughput (millions of messages per second)
These traditional solutions are the viable solutions for providing logging data to an offline analysis system such as Hadoop.
However, the solutions are very limiting for building real-time processing systems.
According to the new trends in Internet applications, activity data has become a part of production data and is used to run analytics at real time.
Real-time usage of these multiple sets of data collected from production systems has become a challenge because of the volume of data collected and processed.
Apache Kafka aims to unify offline and online processing by providing a mechanism for parallel load in Hadoop systems as well as the ability to partition real-time consumption over a cluster of machines.
Kafka can be compared with Scribe or Flume as it is useful for processing activity stream data; but from the architecture perspective, it is closer to traditional messaging systems such as ActiveMQ or RabitMQ.
Few Kafka usages Some of the companies that are using Apache Kafka in their respective use cases are as follows:
This data powers various products such as LinkedIn news feed and LinkedIn Today in addition to offline analytics systems such as Hadoop.
It is used to  integrate Foursquare monitoring and production systems with Foursquare, Hadoop-based offline infrastructures.
On the consumer side, it outputs into Splunk, Graphite, or Esper-like real-time alerting.
In the next chapter we will look at the steps required to install Kafka.
Installing Kafka Kafka is an Apache project and its current Version 0.7.2 is available as a stable release.
Kafka Version 0.8 is available as beta release, which is gaining acceptance in many large-scale enterprises.
Prior to 0.8, any unconsumed partition of data within the topic could be lost if the broker failed.
This ensures that any committed message would not be lost, as at least one replica is available.
The previous feature also ensures that all the producers and consumers are replication aware.
By default, the producer's message send request is blocked until the message is committed to all active replicas; however, producers can also be configured to commit messages to a single broker.
Like Kafka producers, Kafka consumers' polling model changes to a long pulling model and gets blocked until a committed message is available from the producer, which avoids frequent pulling.
Additionally, Kafka 0.8 also comes with a set of administrative tools, such as controlled shutdown of cluster and Lead replica election tool, for managing the Kafka cluster.
Coming back to installing Kafka, as a first step, we need to download the available stable/beta release (all the commands are tested on CentOS 5.5 OS and may differ on other kernel-based OS)
Installing Kafka Now let us see what steps need to be followed in order to install Kafka:
Downloading Kafka Perform the following steps for downloading Kafka release 0.7.x:
Installing the prerequisites Kafka is implemented in Scala and uses the ./sbt tool for building Kafka binaries.
The following command will write the JAVA_HOME environment variable to the file /etc/profile, which contains system-wide environment configuration:
Building Kafka The following steps need to be followed for building and packaging Kafka:
On execution of the previous command, you should see the following output on the command prompt:
On execution of the previous command, you should see the following output on the command prompt:
The following additional command is only needed with Kafka 0.8 for producing the dependency artifacts:
On execution of the previous command, you should see the following output on the command prompt:
If you are planning to play with Kafka 0.8, you may experience lot of warnings with update and package commands, which can be ignored.
The following chapter discusses the steps required to set up single/multibroker Kafka clusters.
From here onwards, the book only focuses on Kafka 0.8
Setting up the Kafka Cluster Now we are ready to play with the Apache Kafka publisher-based messaging system.
With Kafka, we can create multiple types of clusters, such as the following:
All the commands and cluster setups in this chapter are based on Kafka 0.8
With Kafka 0.8, replication of clusters can also be established, which will be discussed in brief in the last part of this chapter.
Single node – single broker cluster This is the starting point of learning Kafka.
In the previous chapter, we built and installed Kafka on a single machine.
Now it is time to setup single node – single broker based Kafka cluster as shown in the following diagram.
Starting the ZooKeeper server Kafka provides the default and simple ZooKeeper configuration file used for launching a single local ZooKeeper instance.
Here, ZooKeeper serves as the coordination interface between the Kafka broker and consumers.
ZooKeeper (http://zookeeper.apache.org) allows distributed processes coordinating with each other through a shared hierarchical name space of data registers (znodes), much like a file system.
The main differences between ZooKeeper and standard filesystems are that every znode can have data associated with it and znodes are limited to the amount of data that they can have.
You should get an output as shown in the following screenshot:
Kafka comes with the required property files defining minimal properties required for a single broker – single node cluster.
For detailed information on how to set up multiple servers of ZooKeeper, visit http://zookeeper.apache.org/
Starting the Kafka broker Now start the Kafka broker using the following command:
You should now see the output as shown in the following screenshot:
This must be set to a unique integer for each broker.
The last section in this chapter defines few more important properties available for the Kafka broker.
For a complete list of Kafka broker properties, visit http://kafka.
Creating a Kafka topic Kafka provides a command-line utility for creating topics on the Kafka server.
You should get an output as shown in the following screenshot:
The previously mentioned utility will create a topic and show the successful creation message as shown in the previous screenshot.
Starting a producer for sending messages Kafka provides users with a command-line producer client that accepts inputs from the command line and publishes them as a message to the Kafka cluster.
By default, each new line entered is considered as a new message.
You should see an output as shown in the following screenshot:
While starting the producer's command-line client, the following parameters are required:
The topic Kafkatopic is a topic that was created in the Creating a Kafka topic section.
The topic name is required for sending a message to a specific group of consumers.
Now type the following message, This is single broker, and press Enter.
You should see an output as shown in the following screenshot:
Detailed information about how to write producers for Kafka and producer properties will be discussed in Chapter 5, Writing Producers.
Starting a consumer for consuming messages Kafka also provides a command-line consumer client for message consumption.
The following command is used for starting the console-based consumer that shows output at command line as soon as it subscribes to the topic created in Kafka broker:
On execution of the previous command, you should get an output as shown in the following screenshot:
Detailed information about how to write consumers for Kafka and consumer properties is discussed in Chapter 6, Writing Consumers.
By running all four components (zookeeper, broker, producer, and consumer) in different terminals, you will be able to enter messages from the producer's terminal and see them appearing in the subscribed consumer's terminal.
Usage information for both producer and consumer command-line tools can be viewed by running the command with no arguments.
Single node – multiple broker cluster Now we have come to the next level of Kafka cluster.
Let us now set up single node – multiple broker based Kafka cluster as shown in the following diagram:
Starting ZooKeeper The first step of starting ZooKeeper remains the same for this type of cluster.
Starting the Kafka broker For setting up multiple brokers on a single node, different server property files are required for each broker.
Each property file will define unique, different values for the following properties:
Now, we start each broker in a separate console using the following commands:
Similar commands are used for all brokers You will also notice that we have defined a separate JMX port for each broker.
The JMX ports are used for optional monitoring and troubleshooting with tools such as JConsole.
Creating a Kafka topic Using the command-line utility for creating topics on the Kafka server, let's create a topic named othertopic with two partitions and two replicas:
Starting a producer for sending messages If we use a single producer to get connected to all the brokers, we need to pass the initial list of brokers, and the information of the remaining brokers is identified by querying the broker passed within broker-list, as shown in the following command.
If we have a requirement to run multiple producers connecting to different combinations of brokers, we need to specify the broker list for each producer like we did in the case of multiple brokers.
Starting a consumer for consuming messages The same consumer client, as in the previous example, will be used in this process.
Just as before, it shows the output on the command line as soon as it subscribes to the topic created in the Kafka broker:
Multiple node – multiple broker cluster This cluster scenario is not discussed in detail in this book, but as in the case of multiple-node Kafka cluster, where we set up multiple brokers on each node, we should install Kafka on each node of the cluster, and all the brokers from the different nodes need to connect to the same ZooKeeper.
For testing purposes, all the commands will remain identical to the ones we used in the single node – multiple brokers cluster.
Kafka broker property list The following is the list of few important properties that can be configured for the Kafka broker.
Property name Description Default value broker.id Each broker is uniquely identified by an ID.
This ID serves as the broker's name, and allows the broker to be moved to a different host/port without confusing consumers.
Here, chroot is a base directory that is prepended to all path operations (this effectively namespaces all Kafka znodes to allow sharing with other applications on the same ZooKeeper cluster)
Summary In this chapter, we have learned how to set up a Kafka cluster with single/multiple brokers on a single node, run command-line producers and consumers, and exchange some messages.
We have also discussed some details about setting up a multinode – multibroker cluster.
In the next chapter, we will look at the internal design of Kafka.
Kafka Design Before we start getting our hands dirty by coding Kafka producers and consumers, let's quickly discuss the internal design of Kafka.
In this chapter we shall be focusing on the following topics:
While developing Kafka, the main focus was to provide the following:
An API for producers and consumers to support custom implementation.
Kafka design fundamentals In a very basic structure, a producer publishes messages to a Kafka topic, which is created on a Kafka broker acting as a Kafka server.
Consumers then subscribe to the Kafka topic to get the messages.
In the preceding diagram a single node – single broker architecture is shown.
This architecture considers that all three parties—producers, Kafka broker, and consumers—are running on different machines.
Here, each consumer is represented as a process and these processes are organized within groups called consumer groups.
A message is consumed by a single process (consumer) within the consumer group, and if the requirement is such that a single message is to be consumed by multiple consumers, all these consumers need to be kept in different consumer groups.
By Kafka design, the message state of any consumed message is maintained within the message consumer, and the Kafka broker does not maintain a record of what is consumed by whom, which also means that poor designing of a custom consumer ends up in reading the same message multiple times.
The fundamental backbone of Kafka is message caching and storing it on the filesystem.
In Kafka, data is immediately written to the OS kernel page.
Caching and flushing of data to the disk is configurable.
Kafka provides longer retention of messages ever after consumption, allowing consumers to reconsume, if required.
Kafka uses a message set to group messages to allow lesser network overhead.
Unlike most of the messaging systems, where metadata of the consumed messages are kept at server level, in Kafka, the state of the consumed messages is maintained at consumer level.
By default, consumers store the state in ZooKeeper, but Kafka also allows storing it within other storage systems used for Online Transaction Processing (OLTP) applications as well.
In Kafka, producers and consumers work on the traditional push-and-pull model, where producers push the message to a Kafka broker and consumers pull the message from the broker.
Kafka does not have any concept of a master and treats all the brokers as peers.
This approach facilitates addition and removal of a Kafka broker at any point, as the metadata of brokers are maintained in ZooKeeper and shared with producers and consumers.
In Kafka 0.7.x, ZooKeeper-based load balancing allows producers to discover the broker dynamically.
A producer maintains a pool of broker connections, and constantly updates it using ZooKeeper watcher callbacks.
But in Kafka 0.8.x, load balancing is achieved through Kafka metadata API and ZooKeeper can only be used to identify the list of available brokers.
Producers also have an option to choose between asynchronous or synchronous mode for sending messages to a broker.
Message compression in Kafka As we have discussed, Kafka uses message set feature for grouping the messages.
Here, data is compressed by the message producer using either GZIP or Snappy compression protocols and decompressed by the message consumer.
There is lesser network overhead for the compressed message set where it also puts very little overhead of decompression at the consumer end.
This compressed set of messages can be presented as a single message to the consumer who later decompresses it.
Hence, the compressed message may have infinite depth of messages within itself.
Within this compression byte, the lowest two bits are used to represent the compression codec used for compression and the value 0 of these last two bits represents an uncompressed message.
Message compression techniques are very useful for mirroring the data across datacenters using Kafka, where large amounts of data get transferred from active to passive datacenters in compressed format.
Cluster mirroring in Kafka The Kafka mirroring feature is used for creating the replica of an existing cluster, for example, for the replication of an active datacenter into a passive datacenter.
Kafka provides a mirror maker tool for mirroring the source cluster into target cluster.
The following diagram depicts the mirroring tool placement in architectural form:
In this architecture, the job of the mirror tool is to consume the messages from the source cluster and republish them on the target cluster.
Replication in Kafka Before we talk about replication in Kafka, let's talk about message partitioning.
In Kafka, message partitioning strategy is used at the Kafka broker end.
The decision about how the message is partitioned is taken by the producer, and the broker stores the messages in the same order as they arrive.
The number of partitions can be configured for each topic within the Kafka broker.
Kafka replication is one of the very important features introduced in Kafka 0.8
Though Kafka is highly scalable, for better durability of messages and high availability of Kafka clusters, replication guarantees that the message will be published and consumed even in case of broker failure, which may be caused by any reason.
Here, both producers and consumers are replication aware in Kafka.
In replication, each partition of a message has n replicas and can afford n-1 failures to guarantee message delivery.
Out of the n replicas, one replica acts as the lead replica for the rest of the replicas.
ZooKeeper keeps the information about the lead replica and the current in-sync follower replica (lead replica maintains the list of all in-sync follower replicas)
Each replica stores its part of the message in local logs and offsets, and is periodically synced to the disk.
This process also ensures that either a message is written to all the replicas or to none of them.
If the lead replica fails, either while writing the message partition to its local log or before sending the acknowledgement to the message producer, a message partition is resent by the producer to the new lead broker.
The process of choosing the new lead replica is that all followers' In-sync Replicas (ISRs) register themselves with ZooKeeper.
The very first registered replica becomes the new lead replica, and the rest of the registered replicas become the followers.
Synchronous replication: In synchronous replication, a producer first identifies the lead replica from ZooKeeper and publishes the message.
As soon as the message is published, it is written to the log of the lead replica and all the followers of the lead start pulling the message, and by using a single channel, the order of messages is ensured.
Each follower replica sends an acknowledgement to the lead replica once the message is written to its respective logs.
Once replications are complete and all expected acknowledgements are received, the lead replica sends an acknowledgement to the producer.
On the consumer side, all the pulling of messages is done from the lead replica.
Asynchronous replication: The only difference in this mode is that as soon as a lead replica writes the message to its local log, it sends the acknowledgement to the message client and does not wait for the acknowledgements from follower replicas.
But as a down side, this mode does not ensure the message delivery in case of broker failure.
Summary In this chapter, we have learned the design concepts used for building a solid foundation for Kafka.
In the next chapter, we shall be focusing on how to write Kafka producers using the API provided.
Writing Producers Producers are applications that create messages and publish them to the Kafka broker for further consumption.
These producers can be different in nature; for example, frontend applications, backend services, proxy applications, adapters to legacy systems, and producers for Hadoop.
These producers can also be implemented in different languages such as Java, C, and Python.
In this chapter we shall be focusing on the following topics:
At the end of the chapter, we will explore some of the important properties required for the Kafka producer.
The following diagram explains the Kafka API for message producers:
In the next few sections, we will discuss the API provided by Kafka for writing Java-based custom producers.
The Java producer API The following are the classes that are imported to write the Java-based basic producers for a Kafka cluster:
Producer: Kafka provides the Producer class (class Producer<K,V>) for creating messages for single or multiple topics with message partition as an optional feature.
Here, Producer is a type of Java generic (http://en.wikipedia.org/wiki/ Generics_in_Java) written in Scala where we need to specify the type of parameters; K and V specify the types for the partition key and message value, respectively.
KeyedMessage: The KeyedMessage class takes the topic name, partition key, and the message value that needs to be passed from the producer as follows: class KeyedMessage[K, V](val topic: String, val key: K, val message: V)
Here, KeyedMessage is a type of Java generic written in Scala where we need to specify the type of the parameters; K and V specify the type for the partition key and message value, respectively, and the topic is always of type String.
ProducerConfig: The ProducerConfig class encapsulates the values required for establishing the connection with brokers such as the broker list, message partition class, serializer class for the message, and partition key.
Simple Java producer Now we will start writing a simple Java-based producer to transmit the message to the broker.
This SimpleProducer class is used to create a message for a specific topic and transmit it.
Importing classes As the first step, we need to import the following classes:
Defining properties As the next step in writing the producer, we need to define properties for making a connection with Kafka broker and pass these properties to the Kafka producer:
Now let us see the major properties mentioned in the code:
In this example, we will be using the string encoder provided by Kafka.
By default, the serializer class for the key and message is the same, but we can change the serializer class for the key by using the key.
By default, the producer works in the "fire and forget" mode and is not informed in case of message loss.
Building the message and sending it As the final step, we need to build the message and send it to the broker as shown in the following code:
Compile the preceding program and use the following command to run it:
Here, kafkatopic is the topic that will be created automatically when the message Hello_There is sent to the broker.
Creating a simple Java producer with message partitioning The previous example is a very basic example of a Producer class and only uses a single broker with no explicit partitioning of messages.
Let's jump to the next level and write another program that connects to multiple brokers and uses message partitioning.
Importing classes This step remains the same for this program.
Defining properties As the next step, we need to define properties for making a connection with the Kafka broker, as shown in the following code, and pass these properties to the Kafka producer:
Kafka producers automatically find out the lead broker for the topic as well as partition it by raising a request for the metadata before it sends any message to the the broker.
If the key is null, Kafka uses random partitioning for message assignment.
Implementing the Partitioner class Next, we need to implement the Partitioner class as shown in the following code:
Building the message and sending it As the final step, we need to build the message and send it to the broker.
Before we run this program, we need to make sure our cluster is running as a multibroker cluster (either single or multiple nodes)
For more information on how to set up a single node – multibroker cluster, refer to Chapter 3, Setting up the Kafka Cluster.
Once your multibroker cluster is up, create a topic with five partitions and set the replication factor as 2 before running this program using the following command:
For verifying the data that is getting published to the Kafka broker, the Kafka console consumer can be used as follows:
The Kafka producer property list The following table shows the list of a few important properties that can be configured for Kafka producer.
The socket connections for sending the actual data will be established based on the broker information returned in the metadata.
The default encoder accepts a byte and returns the same byte.
The value 0 means the producer will not receive any acknowledgment from the broker.
The value 1 means the producer receives an acknowledgment once the lead broker has received the data.
The value -1 means the producer will receive the acknowledgment once all the in-sync replicas have received the data.
The default partitioner is based on the hash value of the key.
Summary In this chapter we have learned how to write basic producers and some advanced Java producers that use message partitioning.
In the next chapter, we will learn how to write Java-based consumers for message consumption.
Writing Consumers Consumers are the applications that consume the messages published by Kafka producers and process the data extracted from them.
Like producers, consumers can also be different in nature, such as applications doing real-time or near-real-time analysis, applications with NoSQL or data warehousing solutions, backend services, consumers for Hadoop, or other subscriber-based solutions.
These consumers can also be implemented in different languages such as Java, C, and Python.
In this chapter, we will focus on the following topics:
At the end of the chapter, we will explore some of the important properties required for a Kafka consumer.
In the next few sections, we will discuss the API provided by Kafka for writing Javabased custom consumers.
All the Kafka classes referred to in this book are actually written in Scala.
The high-level consumer API provides an abstraction over the lowlevel implementation of the consumer API, whereas the simple consumer API provides more control to the consumer by allowing it to override its default low-level implementation.
High-level consumer API The high-level consumer API is used when only data is needed and the handling of message offsets is not required.
Hence, most of the low-level details are abstracted during message consumption.
The high-level consumer stores the last offset read from a specific partition in ZooKeeper.
This offset is stored based on the consumer group name provided to Kafka at the beginning of the process.
Message offset is the position within the message partition to know where the consumer left off consuming the message.
The consumer group name is unique and global across the Kafka cluster and any new consumers with an in-use consumer group name may cause ambiguous behavior in the system.
When a new process is started with the existing consumer group name, Kafka triggers rebalance between the new and existing process threads for the consumer group.
Post rebalance, some of the messages that are intended for a new process may go to an old process, causing unexpected results.
To avoid this ambiguous behavior, any existing consumers should be shut down before starting new consumers for an existing consumer group name.
The following are the classes that are imported to write Java-based basic consumers using the high-level consumer API for a Kafka cluster:
This list of the KafkaStream objects is returned for each topic, which can further create an iterator shown as follows over messages in the stream: class KafkaStream[K,V]
Here, the parameters K and V specify the type for the partition key and message value, respectively.
This class is responsible for all the interaction of a consumer with ZooKeeper.
The following is the class diagram for the ConsumerConnector class:
Simple consumer API Features such as setting the initial offset when restarting the consumer are not provided by the high-level consumer API.
The simple consumer API provides low-level control to Kafka consumers for partition consumptions, for example, multiple reads for the same message or managing transactions, and so on.
Compared to high-level consumer API, developer needs to put in extra efforts to gain this low-level control within consumers, that is, consumers need to keep track of offsets and also need to figure out the lead broker for the topic and partition, and so on.
The following is the class diagram for the SimpleConsumer class:
A simple consumer class provides a connection to the lead broker for fetching the messages from the topic and methods to get the topic metadata and the list of offsets.
The following examples in this chapter are based on the high-level consumer API.
Simple high-level Java consumer Now, we will start writing a single-threaded simple Java consumer developed using high-level consumer API for consuming the messages from a topic.
This SimpleHLConsumer class is used to fetch a message from a specific topic and consume it, assuming that there is a single partition within the topic.
Importing classes As a first step, we need to import the following classes:
Defining properties As a next step, we need to define properties for making a connection with ZooKeeper and pass these properties to the Kafka consumer using the following code:
Now, let us see the major properties mentioned in the code:
Reading messages from a topic and printing them As a final step, we need to read the message using the following code:
So, the complete program will look like the following code:
Compile the previous program and use the following command to run it:
Here, kafkatopic is the topic where the Kafka producer places the messages for consumption.
Multithreaded consumer for multipartition topics The previous example is a very basic example of a consumer who consumes messages from a single broker with no explicit partitioning of messages within the topic.
Let's jump to the next level and write another program, which consumes messages from multiple partitions connecting to single/multiple topics.
A multithreaded high-level consumer-API-based design is usually based on the number of partitions in the topic and follows a one-to-one mapping approach between the thread and the partitions within the topic.
For example, if four partitions are defined for any topic, as a best practice, only four threads should be initiated with the consumer application to read the data; otherwise some conflicting behavior, such as threads never receiving a message or thread receiving messages from multiple partitions, may occur.
Also, receiving multiple messages will not guarantee that the messages will be placed in order.
For example, a thread may receive two messages from the first partition and three from the second partition, then three more from the first partition, followed by some more from the first partition, even if the second partition has data available.
Importing classes This step remains the same as the previous program.
Defining properties This step remains the same for this program as well.
Reading the message from threads and printing it The only difference in this section from the previous section is that we first create a thread pool and get the Kafka streams associated with each thread within the thread pool as shown in the following code:
The complete program listing for the multithread Kafka consumer based on the Kafka high-level consumer API is as follows:
Compile the previous program and, before running it, read the following information box.
Before we run this program, we need to make sure our cluster is running as a multibroker cluster (comprising either single or multiple nodes)
For more information on how to set up single node – multiple broker cluster, refer to Chapter 3, Setting up the Kafka Cluster.
Once your multibroker cluster is up, create a topic with four partitions and set the replication factor to 2 before running this program using the following command:
This program will print all the partitions of messages associated with each thread.
Kafka consumer property list The following is the list of a few important properties that can be configured for high-level consumer-API-based Kafka consumers.
Property name Description Default value group.id This property defines a.
Kafka uses ZooKeeper to store offsets of messages consumed for a specific topic and partition by the consumer group.
Summary In this chapter, we have learned how to write basic consumers and learned about some advanced levels of Java consumers that consume messages from partitions.
In the next chapter, we will learn how to integrate Kafka with Storm and Hadoop.
Kafka Integrations Consider a use case for a website where continuous security events, such as user authentication and authorization to access secure resources need to be tracked, and decisions need to be taken in real time for any security breach.
Using any typical batch-oriented data processing systems, such as Hadoop, where all the data needs to be collected first and then processed to find out patterns, will make it too late to decide whether there is any security threat to the web application or not.
Hence, this is the classical use case for real-time data processing.
Let's consider another use case, where raw clickstreams generated by customers through website usage are captured and preprocessed.
Processing these clickstreams provides valuable insight into customer preferences and these insights can be coupled later with marketing campaigns and recommendation engines to offer an analysis of consumers.
Hence, we can simply say that this large amount of clickstream data stored on Hadoop will get processed by Hadoop MapReduce jobs in batch mode, not in real time.
In this chapter, we shall be exploring how Kafka can be integrated with the following technologies to address different use cases, such as real-time processing using Storm and batch processing using Hadoop:
Introduction to Storm Storm is an open source, distributed, reliable, and fault-tolerant system for processing streams of large volumes of data in real time.
It supports many use cases, such as real-time analytics, online machine learning, continuous computation, and Extract Transformation Load (ETL) paradigm.
There are various components that work together for streaming data processing, which are as follows:
Spout: This is a source of stream, which is a continuous stream of log data.
Bolt: The spout passes the data to a component called bolt.
For example, emitting a stream of trend analysis by processing a stream of tweets.
The following diagram shows spout and bolt in the Storm architecture:
We can assume a Storm cluster to be a chain of bolt components, where each bolt performs some kind of transformation on the data streamed by the spout.
Next in the Storm cluster, jobs are typically referred to as topologies; the only difference is that these topologies run forever.
For real-time computation on Storm, topologies, which are nothing but graphs of computation, are created.
Typically, topologies define how data will flow from spouts through bolts.
The following section is useful if you have worked with Storm or have working knowledge of Storm.
Kafka Spout is available for integrating Storm with Kafka clusters.
The Kafka Spout is a regular spout implementation that reads the data from a Kafka cluster.
It requires the following parameters to get connected to the Kafka cluster:
Root path in ZooKeeper, where Spout stores the consumer offset.
The following code sample shows the KafkaSpout class instance initialization with the previous parameters:
The Kafka Spout uses ZooKeeper to store the states of the message offset and segment consumption tracking if it is consumed.
These offsets are stored at the root path specified for the ZooKeeper.
By default, Storm uses its own ZooKeeper cluster for storing the message offset, but other ZooKeeper clusters can also be used by setting it in Spout configuration.
As per the design, Spout works in a single-threaded model, as all the parallelism is handled by the Storm cluster.
It also has a provision to rewind to a previous offset rather than starting from the last saved offset.
Kafka Spout also provides an option to specify how Spout fetches messages from a Kafka cluster by setting properties, such as buffer sizes and timeouts.
To run Kafka with Storm, clusters for both Storm and Kafka need to be set up and should be running.
A Storm cluster setup is out of the scope of this book.
The following diagram shows the high-level integration view of what a Kafka Storm working model will look like:
Kafka integration with Hadoop Resource sharing, stability, availability, and scalability are a few of the many challenges of distributed computing.
Nowadays, an additional challenge is to process extremely large volumes of data in TBs or PBs.
Introduction to Hadoop Hadoop is a large-scale distributed batch processing framework which parallelizes data processing across many nodes and addresses the challenges for distributed computing, including big data.
Hadoop works on the principle of the MapReduce framework (introduced by Google), which provides a simple interface for the parallelization and distribution of large-scale computations.
Hadoop has its own distributed data filesystem called HDFS (Hadoop Distributed File System)
In any typical Hadoop cluster, HDFS splits the data into small pieces (called blocks) and distributes it to all the nodes.
The following diagram shows the high-level view of a multinode Hadoop cluster:
Name Node: This is a single point of interaction for HDFS.
A name node stores the information about the small pieces (blocks) of data that are distributed across the node.
Secondary Name Node: This node stores the edit logs, which are helpful to restore the latest updated state of HDFS in case of a name node failure.
Data Node: These nodes store the actual data distributed by the name node in blocks and also store the replicated copy of data from other nodes.
Job Tracker: This is responsible for splitting the MapReduce jobs into smaller tasks.
Task Tracker: The task tracker is responsible for the execution of tasks split by the job tracker.
The data nodes and the task tracker share the same machines, MapReduce jobs split, and execution of tasks is done based on the data store location information provided by the name node.
Integrating Hadoop This section is useful if you have worked with Hadoop or have working knowledge of Hadoop.
For real-time publish-subscribe use cases, Kafka is used to build a pipeline that is available for real-time processing or monitoring and to load the data into Hadoop, NoSQL, or data warehousing systems for offline processing and reporting.
Kafka provides the source code for both the Hadoop producer and consumer, under its contrib directory.
This section only discusses the source code provided with Kafka codebases for Hadoop; no other third-party solution for Kafka and Hadoop integrations is discussed.
Hadoop producer A Hadoop producer provides a bridge for publishing the data from a Hadoop cluster to Kafka, as shown in the following diagram:
For a Kafka producer, Kafka topics are considered as URIs, and to connect to a specific Kafka broker, URIs are specified as follows:
The Hadoop producer code suggests two possible approaches for getting the data from Hadoop:
Using the Pig script and writing messages in Avro format: In this approach, Kafka producers use Pig scripts for writing data in a binary Avro format, where each row signifies a single message.
For pushing the data into the Kafka cluster, the AvroKafkaStorage class (extends Pig's StoreFunc class) takes the Avro schema as its first argument and connects to the Kafka URI.
Using the AvroKafkaStorage producer, we can also easily write to multiple topics and brokers in the same Pig script-based job.
Using the Kafka OutputFormat class for jobs: In this approach, the Kafka OutputFormat class (extends Hadoop's OutputFormat class) is used for publishing data to the Kafka cluster.
This approach publishes messages as bytes and provides control over output by using low-level methods of publishing.
The Kafka OutputFormat class uses the KafkaRecordWriter class (extends Hadoop's RecordWriter class) for writing a record (message) to a Hadoop cluster.
For Kafka producers, we can also configure Kafka producer parameters and Kafka broker information under a job's configuration.
Hadoop consumer A Hadoop consumer is a Hadoop job that pulls data from the Kafka broker and pushes it into HDFS.
The following diagram shows the position of a Kafka consumer in the architecture pattern:
A Hadoop job performs parallel loading from Kafka to HDFS, and the number of mappers for loading the data depends on the number of files in the input directory.
The output directory contains data coming from Kafka and the updated topic offsets.
Individual mappers write the offset of the last consumed message to HDFS at the end of the map task.
If a job fails and jobs get restarted, each mapper simply restarts from the offsets stored in HDFS.
Summary In this chapter, we have learned how Kafka integration works for both Storm and Hadoop to address real-time and batch processing needs.
In the next chapter, which is also the last chapter of this book, we will look at some of the other important facts about Kafka.
Kafka Tools In this last chapter, we will be exploring tools available in Kafka and its integration with third-party tools.
We will also be discussing in brief the work taking place in the area of performance testing of Kafka.
Kafka administration tools There are a number of tools or utilities provided by Kafka 0.8 to administrate features such as replication and topic creation.
Kafka topic tools By default, Kafka creates the topic with a default number of partitions and replication factor (the default value is 1 for both)
But in real-life scenarios, we may need to define the number of partitions and replication factors more than once.
The following is the command for creating the topic with specific parameters:
Kafka also provides the utility for finding out the list of topics within the Kafka server.
The List Topic tool provides the listing of topics and information about their partitions, replicas, or leaders by querying ZooKeeper.
The following is the command for obtaining the list of topics:
On execution of the above command, you should get an output as shown in the following screenshot:
The above console output shows that we can get the information about the topic and partitions that have replicated data.
The output from the previous screenshot can be explained as follows:
Kafka replication tools For better management of replication features, Kafka provides tools for selecting a replica lead and controlling shut down of brokers.
As we have learned from Kafka design, in replication, multiple partitions can have replicated data, and out of these multiple replicas, one replica acts as a lead, and the rest of the replicas act as in-sync followers of the lead replica.
In case of non-availability of a lead replica, maybe due to broker shutdown, a new lead replica needs to be selected.
For scenarios such as shutting down of the Kafka broker for maintenance activity, election of the new leader is done sequentially, and this causes significant read/write operations at ZooKeeper.
In any big cluster with many topics/partitions, sequential election of lead replicas causes delay in availability.
To ensure high availability, Kafka provides tools for a controlled shutdown of Kafka brokers.
If the broker has the lead partition shut down, this tool transfers the leadership proactively to other in-sync replicas on another broker.
If there is no in-sync replica available, the tool will fail to shut down the broker in order to ensure no data is lost.
The ZooKeeper host and the broker ID that need to be shut down are mandatory parameters.
Next, in any big Kafka cluster with many brokers and topics, Kafka ensures that the lead replicas for partitions are equally distributed among the brokers.
However, in case of shutdown (controlled as well) or broker failure, this equal distribution of lead replicas may get imbalanced within the cluster.
Kafka provides a tool that is used to maintain the balanced distribution of lead replicas within the Kafka cluster across available brokers.
This tool retrieves all the topic partitions for the cluster from ZooKeeper.
We can also provide the list of topic partitions in a JSON file format.
It works asynchronously to update the ZooKeeper path for moving the leader of partitions and to create a balanced distribution.
Integration with other tools This section discusses the contributions by many contributors, providing integration with Apache Kafka for various needs such as logging, packaging, cloud integration, and Hadoop integration.
Under this project, a single MapReduce job performs the following steps for loading data to HDFS in a distributed manner:
As a first step, it discovers the latest topics and partition offsets from ZooKeeper.
Each task in the MapReduce job fetches events from the Kafka broker and commits the pulled data along with the audit count to the output folders.
After the completion of the job, final offsets are written to HDFS, which can be further consumed by subsequent MapReduce jobs.
Information about the consumed messages is also updated in the Kafka cluster.
Kafka performance testing Kafka contributors are still working on performance testing, and their goal is to produce a number of script files that help in running the performance tests.
Some of them are provided in the Kafka bin folder:
Some more scripts for pulling the Kafka server and ZooKeeper statistics are provided in the CSV format.
Once CSV files are produced, the R script can be created to produce the graph images.
Summary In this chapter, we have added some more information about Kafka, such as its administrator tools, its integration, and Kafka non-Java clients.
During this complete journey through Apache Kafka, we have touched upon many important facts about Kafka.
We have learned the reason why Kafka was developed, its installation, and its support for different types of clusters.
We also explored the design approach of Kafka, and wrote few basic producers and consumers.
In the end, we discussed its integration with technologies such as Hadoop and Storm.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cutting-edge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Realistic, simple code examples to solve problems at scale with Hadoop and related technologies.
Solutions to common problems when working in the Hadoop environment.
In depth code examples demonstrating various analytic models, analytic solutions, and common best practices.
Enhance your search with faceted navigation, result highlighting, relevancy ranked sorting, and more.
Comprehensive information on Apache Solr 3 with examples and tips so you can focus on the important parts.
Advice on data modeling, deployment considerations to include security, logging, and monitoring, and advice on scaling Solr and measuring performance.
Over 100 recipes to discover new ways to work with Apache's Enterprise Search Server.
Apache Solr to make your search engine quicker and more effective.
Deal with performance, setup, and configuration problems in no time.
Discover little-known Solr functionalities and create your own modules to customize Solr to your company's needs.
Over 60 recipes showing you how to design, configure, manage, monitor, and tune a Hadoop cluster.
Hands-on recipes to configure a Hadoop cluster from bare metal hardware nodes.
Easy-to-understand recipes for securing and monitoring a Hadoop cluster, and design considerations.
Chapter 3: Setting up the Kafka Cluster Single node – single broker cluster Starting the ZooKeeper server Starting the Kafka broker Creating a Kafka topic Starting a producer for sending messages Starting a consumer for consuming messages.
Single node – multiple broker cluster Starting ZooKeeper Starting the Kafka brokers Creating a Kafka topic Starting a producer for sending messages Starting a consumer for consuming messages.
Multiple node – multiple broker cluster Kafka broker property list Summary.
Creating a simple Java producer with message partitioning Importing classes Defining properties Implementing the Partitioner class Building the message and sending it.
Simple high-level Java consumer Importing classes Defining properties Reading messages from a topic and printing them.
Multithreaded consumer for multipartition topics Importing classes Defining properties Reading the message from threads and printing it.
